{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":222,"items":[{"title":"A single cluster for all environments?","url":"https://www.reddit.com/r/kubernetes/comments/1lqlr3i/a_single_cluster_for_all_environments/","date":1751537003,"author":"/u/ReverendRou","guid":182946,"unread":true,"content":"<p>My company wants to save costs. I know, I know. </p><p>They want Kubernetes but they want to keep costs as low as possible, so we've ended up with a single cluster that has all three environments on it - Dev, Staging, Production. The environments have their own namespaces with all their micro-services within that namespace. So far, things seem to be working fine. But the company has started to put a lot more into the pipeline for what they want in this cluster, and I can quickly see this becoming trouble. </p><p>I've made the plea previously to have different clusters for each environment, and it was shot down. However, now that complexity has increased, I'm tempted to make the argument again. We currently have about 40 pods per environment under average load. </p><p>What are your opinions on this scenario?</p>","contentLength":797,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1lqlp8p/weekly_this_week_i_learned_twil_thread/","date":1751536830,"author":"/u/gctaylor","guid":182945,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How are Go projects typically broken into parts? (Like how Java/C# have classes in separate files)","url":"https://www.reddit.com/r/golang/comments/1lqkot5/how_are_go_projects_typically_broken_into_parts/","date":1751532757,"author":"/u/Feldspar_of_sun","guid":183010,"unread":true,"content":"<p>I‚Äôm new to Go and starting my first real project, but I realized I don‚Äôt actually know the way Go projects are typically structured, nor what is idiomatic‚Ä¶ really just in general </p><p>My main question is, what is considered standard practice for separation of roles?</p>","contentLength":267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kubectl get pod doesnt show the pod, but it is still exists","url":"https://www.reddit.com/r/kubernetes/comments/1lqjvec/kubectl_get_pod_doesnt_show_the_pod_but_it_is/","date":1751529372,"author":"/u/learnamap","guid":182820,"unread":true,"content":"<p>cannot view the pod using kubectl get pod, but the pod is still pushing logs to elastic and the logs can be viewed in kibana.</p><p>from argocd, the 'missing' pod and replica set doesnt exist as well. but there is a separate existing replica set and pod.</p>","contentLength":247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bcachefs's time is running out ....probably out of kernel sooner than expected ...phew!","url":"https://lwn.net/Articles/1027289/#Comments","date":1751529255,"author":"/u/unixbhaskar","guid":182948,"unread":true,"content":"\nA reiserfs comparison... Not sure how I feel about that one :)<p>\nFirst: any project can look like chaos from afar. You have to dig deeper, understand the project priorities and if they're executing on those priorities.</p><p>\nWith reiserfs, reiserfs3 wasn't exactly known for robustness (speed yes, but there were known issues with repair) and I never got the impression reiserfs4 was focused on fixing those - I did hear about a lot of cool ambitious features they wanted to do, though!</p><p>\nIn contrast, bcachefs was started after the core - btree, IO paths were done, deployed and stable. And while it does have an ambitious featureset, the scope of that featureset was frozen years ago.</p><p>\nIt's also had users since before it went upstream; I did not submit it until it was looking stable for the userbase it had at the time. But of course that was not the end of stabilization and hardening; filesystems (especially today) are massive, so we need to do a gradual rollout, at every step learning more about what can go wrong, fixing the issues the current userbase is finding, and making sure that stabilization is keeping up with growing deployment.</p><p>\nIf we look back at the past two years of development, we also see that there hasn't exactly been a ton in the way of feature work: what feature work has happened has been limited in scope based on user feedback, things that were already planned and in the works, or scalability work. (There are now many bcachefs users with 100+ TB filesystems, and I think I can confidently say that we're good on scalability for now).</p><p>\nThe majority of the development time has gone to debugging, hardening, new debugging tools and better logging (you can't debug what you can't see and understand), and a lot of work on repair as we discover and learn how to cope with new failure modes.</p><p>\n- 6.7 (immediately prior to merge): upgrade/downgrade mechanisms, modern versioning (we don't do really do feature bits like other filesystems, we version everything, our forwards/backwards compatibility mechanisms are really nice; that made everything that came after much smoother (or even possible))\n- 6.8: per-device vector clocks, for split brain detection<p>\n- 6.9: repair by btree node scan, per-device superblock bitmaps of \"ranges with btree nodes\" to make btree node scan practical on large filesystems. (This one was motivated by reiserfs, but with a lot of lessons learned so that it works reliably - even if you've got a bcachefs image file on your filesystem).</p>\n- 6.11: disk accounting rewrite - this was a multi year project started before bcachefs was merged, which made our accounting extensible and scalable (it was fast before, but not extensible); this enabled e.g. per-snapshot accounting (not yet exposed).<p>\n- 6.12 or .13? - reflink improvements (the ability to put indirect extents in an error state), to ensure that transient errors don't cause data loss</p>\n- 6.14: major scalability work for backpointers check/repair, in response to larger and large filesystems becoming commonplace - this required an expensive and disruptive on disk format upgrade, but now we're good to 10+ PB, tested.<p>\n- 6.15: scalability work for device removal, snapshots removal: again done in response to actual usage</p>\n- 6.15: more hardening against actual IO/checksum errors, in the data move path (extent poisoning; this generated a kerfuffle among the block layer people - \"you want to do WHAT with FUA?\" \"it says it right here in the spec\" (and we've since determined that yes, read fua does work as advertised on scsi hard drives, anyone's guess what nvme devices are doing).<p>\n- 6.16: major logging improvements for data read errors, btree node read errors, and errors that trigger repair: grouping all errors and repair actions into a single error message, so we can follow the sequence of events</p></p><p>\nThrough it all, lots of lots of end user support and bug fixing. I am perpetually telling users: \"I don't care what broke or why, if you think it was the hardware's fault or pebcak - get me a metadata dump, get me the info I need, we'll get it working again and make it more robust for everyone.\"</p>","contentLength":4100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lqjud9/bcachefss_time_is_running_out_probably_out_of/"},{"title":"Hi guys I am getting timeout issue whenever I run exec or logs or top but when I run get it is working. fine.","url":"https://www.reddit.com/r/kubernetes/comments/1lqiz9t/hi_guys_i_am_getting_timeout_issue_whenever_i_run/","date":1751525845,"author":"/u/Chameleon_The","guid":182737,"unread":true,"content":"<div><p>I have like eks cluster there is 1 worker node when I try to use exec intothiss pod that is present in this pod it is throwing timeout, I am able to get pods only no exec no logs I checked TCP dump I am able to see the req from the apiserver buyt no response from the kubelet </p><p>I.want to know it is an issue with kubelet ornetworks issue.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Chameleon_The\"> /u/Chameleon_The </a>","contentLength":372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"JavaScript‚Ñ¢ Trademark Update","url":"https://deno.com/blog/deno-v-oracle4","date":1751518791,"author":"/u/LawfulKitten98","guid":182821,"unread":true,"content":"<p>On June 18, the Trademark Trial and Appeal Board (TTAB)\n<a href=\"https://ttabvue.uspto.gov/ttabvue/v?pno=92086835&amp;pty=CAN&amp;eno=15\" rel=\"noopener noreferrer\">dismissed</a> our\nfraud claim against Oracle. We disagree with this decision.</p><p>That claim alleged Oracle knowingly misled the USPTO in its 2019 renewal by\nsubmitting a screenshot of the Node.js website to show use of the ‚ÄúJavaScript‚Äù\ntrademark. As the creator of Node.js, I find that especially offensive. Node.js\nwas never an Oracle product or brand. Oracle didn‚Äôt create it, didn‚Äôt run it,\nand wasn‚Äôt authorized to use it to prop up its trademark. That they reached for\na third-party open source site suggests they had no better proof‚Äîand knew it.</p><p>But fraud was never the heart of this case.</p><p>We‚Äôre not amending the fraud claim. Doing so would delay the case by months, and\nour focus is on the claims that matter most:  and\n. Everyone uses ‚ÄúJavaScript‚Äù to describe a language‚Äînot a brand.\nNot an Oracle product. Just the world‚Äôs most popular programming language.</p><p>The case now proceeds quickly.</p><p>On , Oracle must respond to every paragraph of our cancellation\npetition‚Äîeither admitting or denying our claims about genericness and\nabandonment. I‚Äôm eager to see what they challenge.</p><p><strong>Discovery begins September 6.</strong></p><p>Everyone knows JavaScript isn‚Äôt an Oracle product‚Äîand 19,550 people at\n<a href=\"https://javascript.tm\" rel=\"noopener noreferrer\">javascript.tm</a> agree (at the time of writing). This\ntrademark doesn‚Äôt serve the public, the industry, or the purpose of trademark\nlaw. It‚Äôs just wrong.</p><p>If we win this cancellation‚Äîor if Oracle does the right thing and releases the\ntrademark‚ÄîJavaScript will be free. No more ‚Ñ¢ symbols. No more licensing fears.\nJust the name of the programming language that powers the web, belonging to\neveryone who uses it.</p>","contentLength":1671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lqh2wu/javascript_trademark_update/"},{"title":"I want to migrate from kong gateway to best alternative that has more adoption and community support as well.","url":"https://www.reddit.com/r/kubernetes/comments/1lqgugq/i_want_to_migrate_from_kong_gateway_to_best/","date":1751517955,"author":"/u/Wooden_Departure1285","guid":182613,"unread":true,"content":"<p>Can any one share their experience ?</p>","contentLength":36,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] AI/ML interviews being more like SWE interviews","url":"https://www.reddit.com/r/MachineLearning/comments/1lqgbdk/d_aiml_interviews_being_more_like_swe_interviews/","date":1751516135,"author":"/u/guohealth","guid":182681,"unread":true,"content":"<div><p>Have people noticed that AI/ML/DS job interviews now feel more SWE-like? For example, relying more on data structures and algorithms leetcode questions. I‚Äôve noticed in my professional friend groups more people are being asked these questions during the coding interview.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/guohealth\"> /u/guohealth </a>","contentLength":305,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Progress report on rustc_codegen_cranelift (June 2025)","url":"https://bjorn3.github.io/2025/06/30/progress-report-june-2025.html","date":1751511300,"author":"/u/yerke1","guid":182886,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/yerke1\"> /u/yerke1 </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lqet2y/progress_report_on_rustc_codegen_cranelift_june/"},{"title":"[D] Paper with code is completely down","url":"https://www.reddit.com/r/MachineLearning/comments/1lqedrt/d_paper_with_code_is_completely_down/","date":1751509982,"author":"/u/Striking-Warning9533","guid":182822,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust #606","url":"https://this-week-in-rust.org/blog/2025/07/02/this-week-in-rust-606/","date":1751509339,"author":"/u/b-dillo","guid":182614,"unread":true,"content":"<p>This week's crate is <a href=\"https://crates.io/crates/ansic\">ansic</a>, a proc macro providing a DSL to output ANSI escape strings with zero runtime overhead.</p><p>Thanks to <a href=\"https://users.rust-lang.org/t/crate-of-the-week/2704/1448\">Zeon</a> for the self-suggestion!</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.</p><p>If you are a feature implementer and would like your RFC to appear in this list, add a\n label to your RFC along with a comment providing testing instructions and/or\nguidance on which aspect(s) of the feature need testing.</p><p><a href=\"https://github.com/rust-lang/this-week-in-rust/issues\">Let us know</a> if you would like your feature to be tracked as a part of this list.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p><em>No Calls for participation were submitted this week.</em></p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p><em>No Calls for papers or presentations were submitted this week.</em></p><p>Lots of changes this week with results dominated by the 1-5% improvements from\n<a href=\"https://github.com/rust-lang/rust/pull/142941\">#142941</a> across lots of primary\nbenchmarks in the suite.</p><p>3 Regressions, 6 Improvements, 5 Mixed; 4 of them in rollups\n39 artifact comparisons made in total</p><ul><li><em>No RFCs were approved this week.</em></li></ul><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><p>Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.</p><ul><li><em>No New or Updated RFCs were created this week.</em></li></ul><p>Rusty Events between 2025-07-02 - 2025-07-30 ü¶Ä</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>I love Rust, so I was already biased to be positive about the Rust for Linux project, even before dabbling with it myself. I'm genuinely surprised to be even more optimistic now than before. The coding part was much easier than I imagined, thanks to the use of reference counting in the kernel.</p><p>And the promised benefits of Rust over C? They're absolutely real. The Rust version of the driver feels way more robust than the C code, not just regarding memory safety. It didn't have a single bug: Once it compiled, it worked. That's not a huge deal considering it was a direct rewrite, but it counts for something.</p></blockquote><p>Despite a lamentable lack of suggestions, llogiq is reasonably pleased with his choice.</p>","contentLength":2652,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lqe66f/this_week_in_rust_606/"},{"title":"Looking for shared auth solution for personal projects","url":"https://www.reddit.com/r/golang/comments/1lqdbdq/looking_for_shared_auth_solution_for_personal/","date":1751506741,"author":"/u/belak51","guid":182947,"unread":true,"content":"<p>The short version is that I've got a bunch of small personal projects I'd like to build but they all need some sort of login system. I'm very familiar with the concepts and I could definitely build a simple version for one project, but I'm a bit at a loss for how to share it with other projects.</p><p>Specifically, there's not a great way to have separate components which integrate with a migration system because most systems are designed around having a linear set of migrations, not multiple which get merged together. Before Go my background was in Python/Django where it was expected that you'd have multiple packages integrated in your app and they'd all provide certain routes and potentially migrations scoped to that package.</p><p>Even most recommended solutions like <a href=\"https://github.com/alexedwards/scs\">scs</a> are only half of the solution, and dealing with the complete end to end flow gets to be a fairly large solution, especially if you end up integrating with OIDC.</p><p>Am I missing something obvious? Is there a better way other than copying the whole thing between projects and merging all the migrations with your project's migrations? That doesn't seem very maintainable because making a bug fix with one would require copying it to all of your separate projects.</p><p>If anyone has library recomendations, framework recommendations, or even just good ways for sharing the implementation between separate projects that would be amazing. Bonus points if you can share the user database between projects.</p>","contentLength":1461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Year of the Linux Desktop? A Blog post","url":"https://www.reddit.com/r/linux/comments/1lqcebs/the_year_of_the_linux_desktop_a_blog_post/","date":1751503969,"author":"/u/freekun","guid":181726,"unread":true,"content":"<p>Is it finally time? Maybe, maybe not. 2025 has certainly been an exciting time for the OS we all love, so is it finally time to consider it *the year*?</p>","contentLength":151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Project] Distributed File system from scratch in Go","url":"https://www.reddit.com/r/golang/comments/1lqbhbj/project_distributed_file_system_from_scratch_in_go/","date":1751501266,"author":"/u/whathefuckistime","guid":181561,"unread":true,"content":"<p>I'm a mechanical engineer currently making the switch over to software engineering. I haven't received any job offerings yet, so for the past month I've been focusing my time on building this project to get more practical experience and have something solid to talk about in interviews.</p><p>As I've been interested in distributed systems recently, I decided to build a simple Distributed File System from scratch using Go.</p><p>The architecture is split into three services that talk to each other over gRPC:</p><ul><li><p>Coordinator: This is the controller node. It manages all the file metadata (like filenames and chunk lists), tracks which datanodes are alive via heartbeats, and tells the client which nodes to talk to for file operations.</p></li><li><p>Datanodes: These are simple storage nodes. Their main job is to store file chunks and serve them to clients via streams.</p></li><li><p>Client: The interface for interacting with the system. </p></li></ul><p>The main features are file upload, download, and replication. Here's the basic flow:</p><p>When you want to upload a file, the client first contacts the coordinator. The coordinator then determines where each chunk of the file should be stored given some selection algorithm (right now it just picks nodes with status: healthy) and returns this list of locations to the client. The client then streams the chunks directly to the assigned datanodes in parallel. Once a datanode receives a chunk, it runs a checksum and sends an acknowledgment back to the client, if it is a primary node (meaning it was the first to receive the chunk), it replicates the chunk to other datanodes, only after all replicates are stored the system returns a confirmation to the client. After all chunks are successfully stored and replicated, the client sends a confirmation back to the coordinator so that it can commit all the chunk storage locations in metadata tracker.</p><p>Downloads work in reverse: the client asks the coordinator for a file's locations, and then reaches out to the datanodes, who stream each chunk to the client. The client assembles the file in place by using a temp file and seeking to the correct offset by using the chunksize and index.</p><p>To make sure everything works together, I also built out a full end-to-end test environment using Docker that spins up the coordinator and multiple datanodes to simulate a small cluster. In the latest PR, I also added unit tests to most of the core components. This is all automated with Github Actions on every PR or commit to main.</p><p>I'd really appreciate any feedback, since I am still trying to get a position, I would like to know what you think my current level is, I am applying for both Jr and mid-level positions but it has been really difficult to get anything, I have reviewed my CV too many times for that to be an issue, I've also asked for the help of other engineers I know for their input and they thought it was fine. I think that it is the lack of work experience that is making it very hard, so I also have a personal projects section in there, where I list out these kinds of projects to prove that I actually know some stuff. </p>","contentLength":3069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It finally came!!","url":"https://www.reddit.com/r/kubernetes/comments/1lq7j7l/it_finally_came/","date":1751490800,"author":"/u/Agitatedndustry916","guid":181292,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Any professional rust folks get leetcoded in rust when interviewing?","url":"https://www.reddit.com/r/rust/comments/1lq7h2b/any_professional_rust_folks_get_leetcoded_in_rust/","date":1751490650,"author":"/u/Willing_Sentence_858","guid":181644,"unread":true,"content":"<p>any professional rust folks get leetcoded in rust when interviewing -- rust is rather difficult here but not impossible ... i wouldn't be surprised cognitively is 4x as much effort leetcoding then say python.</p><p>i need a new job and i don't know if i should just be leetcoding in python ...</p>","contentLength":286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"git-go: Git written in Go (sort of)","url":"https://www.reddit.com/r/golang/comments/1lq72go/gitgo_git_written_in_go_sort_of/","date":1751489667,"author":"/u/unknown_r00t","guid":182682,"unread":true,"content":"<p>Just finished a little side project:  - a basic Git implementation in Go.</p><p>Got the essentials working: , , , , , and . Nothing fancy (no push, pull), probably has bugs, definitely not production-ready or anything like that. This was purely for understanding how Git works under the hood (which was fun). Don't expect it to replace actual Git anytime soon /s, but figured I'd throw it out there in case anyone wants to poke around or add stuff to it.</p><p>Happy to answer questions about the implementation if anyone's curious about the internals.</p>","contentLength":538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Google SSO with Keycloak in a Go Application","url":"https://medium.com/@adityav170920/enabling-google-single-sign-on-with-keycloak-keycloak-26-2-0-4ba83866945f","date":1751487520,"author":"/u/Wide-Pear-764","guid":181294,"unread":true,"content":"<p>In today‚Äôs interconnected world, providing a smooth and secure user experience is paramount. Single Sign-On (SSO) plays a crucial role in achieving this, allowing users to access multiple applications with a single set of credentials. Keycloak, a powerful open-source Identity and Access Management solution, makes it easy to integrate various identity providers.</p><p>This article will walk you through the process of setting up Google as an identity provider in Keycloak, enabling your users to log in using their existing Google accounts. We‚Äôll be using Keycloak version 26.2.0 for this demonstration.</p><p>Before we begin, ensure you have:</p><ul><li>A running Keycloak instance (version 26.2.0 or compatible).</li><li>Administrative access to your Keycloak realm.</li><li>A Google Cloud Platform (GCP) account.</li></ul><p>The first step is to configure a new OAuth 2.0 Client ID in your Google Cloud Project. This will allow Keycloak to communicate with Google for authentication.</p><ol><li><strong>Navigate to Google Cloud Console:</strong> Go to the Google Cloud Console (<a href=\"https://console.cloud.google.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">console.cloud.google.com</a>). If you don‚Äôt have a project, create a new one.</li><li> In the left-hand navigation pane, search for ‚ÄúAPIs &amp; Services‚Äù and then click on ‚ÄúCredentials.‚Äù</li></ol><p> Click on ‚ÄúCreate credentials‚Äù at the top of the page.</p><ol><li> From the dropdown, choose ‚ÄúOAuth client ID.‚Äù</li><li><strong>Configure Consent Screen (if not already done):</strong> If this is your first time creating an OAuth client ID in this project, you might be prompted to configure the OAuth consent screen. Follow the on-screen instructions to set up your application‚Äôs name, user support email, and developer contact information.</li><li> Select ‚ÄúWeb application‚Äù as the application type.</li><li> Give your OAuth 2.0 client a descriptive name, e.g., ‚Äú.‚Äù</li><li><strong>Add Authorized Redirect URIs:</strong> This is a crucial step. You need to tell Google where to redirect users after they successfully authenticate. The Redirect URI for Keycloak‚Äôs Google Identity Provider will typically be in the format: <code>http://localhost:8080/realms/&lt;your-realm-name&gt;/broker/google/endpoint</code> This endpoint can be fetched from the keycloak page which we will cover in the later part of the article.</li></ol><p> Click the ‚ÄúCreate‚Äù button.</p><ol><li><strong>Note Down Client ID and Client Secret:</strong> After creation, a pop-up will display your  and . <strong>Copy these values immediately and store them securely.</strong> You will need them in Keycloak.</li></ol><p>Now, let‚Äôs configure Keycloak to use the Google OAuth client you just created.</p><ol><li> In the top-left corner, ensure you have selected the realm where you want to enable Google SSO.</li><li><strong>Navigate to Identity Providers:</strong> In the left-hand menu, under ‚ÄúConfigure,‚Äù click on ‚ÄúIdentity providers.‚Äù</li></ol><p><strong>4 . Copy the Redirect URI:</strong> On the ‚ÄúAdd Google provider‚Äù page, locate the ‚ÄúRedirect URI‚Äù field. This is the exact URL you need to ensure is present and correct in your Google OAuth client settings. Copy it now if you need to update it in Google.</p><p><strong>5. Paste Client ID and Client Secret:</strong></p><ul><li> Paste the  you obtained from Google Cloud Platform.</li><li> Paste the  you obtained from Google Cloud Platform.</li><li> (e.g., ) and  (e.g., ) will usually be pre-filled appropriately.</li></ul><p> Do not click ‚ÄúAdd‚Äù yet if you plan to immediately configure the flow. You can save it now and come back, or proceed to flow creation if you‚Äôre comfortable. For now, let‚Äôs assume you‚Äôve just pasted the credentials.</p><p>Keycloak uses ‚Äúflows‚Äù to define the authentication process. Creating a custom flow for your social logins is highly recommended, as it allows you to control how new users are handled and attributes are mapped.</p><ol><li><strong>Navigate to Authentication:</strong> In the left-hand menu, under ‚ÄúConfigure,‚Äù click on ‚ÄúAuthentication.‚Äù</li><li> Go to the ‚ÄúFlows‚Äù tab and click the ‚ÄúCreate flow‚Äù button.</li><li> Give your new flow a descriptive name, for example, .</li></ol><ul><li>You can add an optional description.</li><li>Keep the ‚ÄúFlow type‚Äù as ‚ÄúBasic flow.‚Äù</li></ul><ol><li><strong>Add Executions to the Flow:</strong> Click on your newly created  to configure its steps. These executions ensure a seamless user experience by detecting and automatically linking existing users.</li></ol><ul><li>Select ‚ÄúDetect existing broker user‚Äù and set its ‚ÄúRequirement‚Äù to ‚ÄúRequired.‚Äù This step checks if the user already exists in Keycloak based on the email provided by Google.</li><li>Click ‚ÄúAdd execution‚Äù again.</li><li>Select ‚ÄúAutomatically set existing user‚Äù and set its ‚ÄúRequirement‚Äù to ‚ÄúRequired.‚Äù If an existing user is detected, this step automatically links the social login to that Keycloak user.</li></ul><p>Now, you need to tell Keycloak to use your newly created flow for Google logins.</p><ol><li><strong>Navigate back to Identity Providers:</strong> In the left-hand menu, under ‚ÄúConfigure,‚Äù click on ‚ÄúIdentity providers.‚Äù</li><li> Click on the ‚ÄúGoogle‚Äù provider you configured earlier.</li><li> Toggle the ‚ÄúTrust Email‚Äù parameter to ‚ÄúOn.‚Äù This tells Keycloak to trust the email address provided by Google as verified.</li><li><strong>Set ‚ÄúFirst login flow override‚Äù:</strong> Scroll down to the ‚ÄúFirst login flow override‚Äù dropdown and select your custom flow (e.g., ). This ensures that when a new user logs in via Google for the first time, Keycloak uses your defined flow.</li></ol><p>6.  and your google login is ready to be used.</p><p>Let‚Äôs verify that your Google Single Sign-On integration is working as expected.</p><ol><li><strong>Locate the Google Login Option:</strong> You should now see a ‚ÄúGoogle‚Äù button or link on the login screen, typically below the standard username/password fields.</li></ol><ul><li> Click on the ‚ÄúGoogle‚Äù button.</li><li><strong>Google Authentication Prompt:</strong> You will be redirected to Google‚Äôs authentication page. If you‚Äôre already signed into a Google account, it might prompt you to confirm the sign-in for the application you‚Äôve configured (Keycloak). If not, you‚Äôll be asked to enter your Google credentials.</li></ul><p><strong>Successful Login and Redirection:</strong> After successful authentication with Google, you will be redirected back to your Keycloak application. You should now be logged into the system, and your Keycloak account will be connected to your Google identity provider profile.</p><p>nd there you have it! You‚Äôve successfully enabled Google Single Sign-On with Keycloak. This means your users can now enjoy the ease and familiarity of logging into your applications with their trusted Google accounts ‚Äî a win for convenience and a smoother experience all around. Keycloak truly shines with its flexibility, making it a powerful tool for bringing various identity providers under one roof.</p><p>If you hit any snags along the way, or just have a question, don‚Äôt hesitate to drop a comment below. Happy authenticating!</p>","contentLength":6440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lq66pr/integrating_google_sso_with_keycloak_in_a_go/"},{"title":"How Kelsey Hightower inspired a community to build Kubernetes [blog & fireside chat at CDS]","url":"https://www.containerdays.io/blog/how-kelsey-hightower-inspired-a-community-to-build-kubernetes/","date":1751485215,"author":"/u/Diligent-Respect-109","guid":181110,"unread":true,"content":"<p>With curiosity and no prior background in Kubernetes, I was excited to watch <a href=\"https://youtu.be/RkGa4WLugYc\">Kelsey‚Äôs panel</a> at ContainerDays Conference 2024 to gain a better understanding of how the movement began. Having organized conferences for over seven years with a primary focus on cloud-native and Kubernetes topics, I‚Äôve often found that most sessions are highly technical. This made me especially happy to learn more about the importance of this technology and how it all started.</p><p>These are my key takeaways from the session:</p><p>Today, <a href=\"https://kubernetes.io/\">Kubernetes</a> is a cornerstone of cloud-native applications. It has become an essential tool for developers and companies worldwide.</p><h2>But‚Ä¶ How did it start gaining traction?</h2><p>Kelsey Hightower played a crucial role in driving Kubernetes adoption and community growth from the early days.</p><p>When questioned about the beginning of his journey with Kubernetes, Kelsey attributes it to a stroke of luck.</p><p>While working at CoreOS, Kelsey was contacted by Google for their announcement of the release of Kubernetes. Recognizing the potential of this project, Kelsey quickly made Kubernetes run on CoreOS and prepared a detailed tutorial on how to do it. The next day, this tutorial became even more popular than Google‚Äôs announcement on Hacker News. This was an early indication that simple guides could be a powerful tool to drive Kubernetes‚Äô adoption.</p><p>Kelsey realized early on that Kubernetes solved a challenge that companies and system architects had been struggling with for a long time. This realization motivated him to start contributing independently to the project, even before CoreOS officially supported it. Like other passionate community members, Kelsey started contributing on nights and weekends, driven by his genuine interest in building a useful tool for the whole community.</p><h2>The turning point for Kubernetes</h2><p>For Kelsey, the moment when it became clear that Kubernetes was going to be the future was when Microsoft announced its support. Soon after, all the big vendors followed, solidifying the project‚Äôs status as the future of cloud infrastructure.</p><p>Despite his conviction about Kubernetes‚Äô potential from the start, Kelsey couldn‚Äôt have predicted it would become such a cornerstone for the cloud-native community. In fact, Kelsey initially approached Kubernetes as a fun, interesting project, which he wanted to share with the community.</p><p>In his early talks, Kelsey would simply (and enthusiastically) explain how Kubernetes worked, breaking down the difficult components into easy-to-grasp concepts. Eventually, his enthusiasm and approachable style ignited a spark that encouraged the community to not just adopt Kubernetes, but to actively contribute to its development.</p><p>While some players focused on building up credibility through white papers and highly technical content, Kelsey chose an educational path. By breaking down complex concepts into accessible information, and meeting people where they were in terms of knowledge, Kelsey gradually managed to build momentum and encourage more community involvement. This approach not only allowed the community to truly understand the technology, but it also inspired many to become contributors themselves.</p><p>Kelsey‚Äôs educational approach, combined with his previous contributions to the open-source community, allowed him to lend some of his own credibility to Kubernetes, which further fueled the project‚Äôs growth.</p><p>As it turns out - there was no master plan to grow Kubernetes. Instead, it was Kelsey‚Äôs accessible, educational approach that inspired a whole community to collectively build what is now a cornerstone of cloud computing.</p><p>In essence, from my personal perspective after watching this talk, the secret to Kubernetes‚Äô success can be summarized in three elements: demystifying complex concepts, an engaged community, and, as always, a dash of luck.</p>","contentLength":3838,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lq58ti/how_kelsey_hightower_inspired_a_community_to/"},{"title":"How to handle pre-merge testing without spinning up a full Kubernetes environment","url":"https://www.reddit.com/r/kubernetes/comments/1lq4op5/how_to_handle_premerge_testing_without_spinning/","date":1751483839,"author":"/u/krazykarpenter","guid":181291,"unread":true,"content":"<p>I wanted to share a pattern our team has been refining and get your thoughts, because I know the pain of testing microservices on Kubernetes is real.</p><p>For the longest time, the default was either a perpetually broken, shared \"staging\" or trying to spin up an entire environment replica for every PR. The first creates bottlenecks, and the second is slow and gets expensive fast, especially as your app grows.</p><p>We've been exploring a different approach: using a service mesh (Istio, linkerd etc) to create lightweight,  ephemeral environments within a single, shared cluster.</p><ol><li>You deploy  the one or two services they've changed into the shared dev/staging cluster.</li><li>When you (or a CI job) run a test, a unique HTTP header (e.g., <code>x-sandbox-id: my-feature-test</code>) is injected into the initial request.</li><li>The service mesh's routing rules are configured to inspect this header. If it sees the header, it routes the request to the new version of the service.</li><li>As that service makes downstream calls, the header is propagated, so the entire request path for that specific test is correctly routed through any other modified services that are part of that test. If a service in the chain wasn't modified, the request simply falls back to the stable baseline version.</li></ol><p>This gives an isolated test context that only exists for the life of that request, without duplicating the whole stack.</p><p> I'm a co-founder at <a href=\"https://www.signadot.com/\">Signadot</a>, and we've built our product around this concept. We actually just hit a 1.0 release with our Kubernetes Operator, which now supports Istio's new Ambient Mesh. It‚Äôs pretty cool to see this pattern work in a sidecar-less world, which makes the whole setup even more lightweight on the cluster.</p><p>Whether you're trying to build something similar in-house with Istio, Linkerd, or even just advanced Ingress rules, I'd be happy to share our learnings and exchange notes. Thanks</p>","contentLength":1865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"are we stuck with crate_name/crate-name/weird_crate-name inconsistency?","url":"https://www.reddit.com/r/rust/comments/1lq4obm/are_we_stuck_with_crate_namecratenameweird/","date":1751483813,"author":"/u/somnamboola","guid":181293,"unread":true,"content":"<p>IMO it's not only OCD triggering, It also opens a vector to supply chain attacks. Would be cool to brainstorm if there are some cool ideas. </p>","contentLength":140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How old is your PC?","url":"https://www.reddit.com/r/linux/comments/1lq4jl7/how_old_is_your_pc/","date":1751483502,"author":"/u/Kassebasse","guid":181295,"unread":true,"content":"<div><p>I was wondering on how many of the Linux users uses older hardware as their daily driver or maybe just as a spare computer. I am currently using a laptop that has a Intel i5 CPU 1:st generation, 8 GB of RAM and an SSD. My laptop is about 15 years old at this point as I bought is second hand.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Kassebasse\"> /u/Kassebasse </a>","contentLength":325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing tmux-rs","url":"https://richardscollin.github.io/tmux-rs/","date":1751481584,"author":"/u/Intelligent-Pear4822","guid":181112,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lq3qm4/introducing_tmuxrs/"},{"title":"kube_pod_info metrics not showing container label for one cluster","url":"https://www.reddit.com/r/kubernetes/comments/1lq364p/kube_pod_info_metrics_not_showing_container_label/","date":1751480239,"author":"/u/Next-Lengthiness2329","guid":181008,"unread":true,"content":"<div><p>I have 2 clusters , one cluster shows all necessary labels but another cluster named monitoring doesn't show some necessary labels like: endpoint namespace</p><p>I have setup kube-prometheus-stack with prometheus operator , and i am unable to create dashboards on grafana for my monitoring cluster due to this issue</p><p>what could be the issue ?</p><pre><code>prometheus: service: type: ClusterIP prometheusSpec: externalLabels: cluster: monitoring-eks enableRemoteWriteReceiver: true additionalScrapeConfigs: - job_name: 'kube-state-metric' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name] regex: kube-state-metrics action: keep - source_labels: [__meta_kubernetes_service_name] regex: kube-prometheus-stack-kube-state-metrics action: keep - source_labels: [__meta_kubernetes_namespace] regex: monitoring action: keep - source_labels: [__meta_kubernetes_endpoint_port_name] regex: http action: keep - target_label: cluster replacement: monitoring-eks </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/Next-Lengthiness2329\"> /u/Next-Lengthiness2329 </a>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Burn It With Fire: How to Eliminate an Industry-Wide Supply Chain Vulnerability","url":"https://medium.com/@jonathan.leitschuh/burn-it-with-fire-how-to-eliminate-an-industry-wide-supply-chain-vulnerability-12515516fb56","date":1751479908,"author":"/u/JLLeitschuh","guid":181113,"unread":true,"content":"<p>While Gradle, Bazel, and SBT responded with relatively swift and thoughtful fixes, Maven proved to be a far harder challenge.</p><p>In late 2020, I began a coordinated disclosure process to report a critical issue in Maven: <strong>transitive repository injection</strong>. Maven projects could inherit  repositories from dependencies without the developer even realizing it. It meant you could write a perfectly secure POM ‚Äî and still get compromised.</p><p>My colleague at Gradle, <a href=\"https://github.com/melix\" rel=\"noopener ugc nofollow\" target=\"_blank\">C√©dric Champeau</a>, authored a proof-of-concept, and I submitted it while engaging in the back-and-forth disclosure process with the Apache Security Team and the Maven PMC. The back-and-forth stretched across . Emails went unanswered. Conversations stalled. Multiple reminders were sent. At one point, I had to loop in <a href=\"https://en.wikipedia.org/wiki/CERT_Coordination_Center\" rel=\"noopener ugc nofollow\" target=\"_blank\">CERT/CC to</a> escalate.</p><p>Meanwhile, I wrote code to scan the entire Maven Central index ‚Äî parsing  to identify real-world usage and generate a list of affected coordinates. The results were staggering. Over  libraries were affected. Mend later published a public write-up summarizing the issue and its implications, which you can read here: <a href=\"https://www.mend.io/blog/maven-security-vulnerability-cve-2021-26291/\" rel=\"noopener ugc nofollow\" target=\"_blank\">Maven Security Vulnerability: CVE-2021‚Äì26291</a>.</p><p>Eventually, after significant pressure ‚Äî and after weeks of unanswered emails, stalled conversations, and repeated follow-ups ‚Äî I wrote to the ASF security team:</p><blockquote><p><em>‚ÄúWhat fundamentally disturbs me here is that all of us as individuals, and the companies we work for‚Ä¶ all play a critical role in protecting the supply chain of the JVM ecosystem. To that end, we‚Äôve all failed in some way here‚Ä¶ This game of ‚Äònot my problem, it‚Äôs yours‚Äô does nothing to protect users.‚Äù</em></p></blockquote><p>I believed this issue was real. And someone had to push to get it fixed.</p><p>CERT/CC eventually threatened to go over the Apache Security Team‚Äôs head and issue a CVE for the unpatched vulnerability. This threat finally spurred the Apache Maven team into action. Apache Maven issued <a href=\"https://nvd.nist.gov/vuln/detail/cve-2021-26291\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a> and shipped a fix in version 3.8.1, changing the default behavior to block HTTP repository resolution.</p><p>This was a hard-fought win ‚Äî and one that highlighted just how difficult it can be to drive coordinated change even when the risk is well-documented and easily fixed.</p>","contentLength":2188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lq3127/burn_it_with_fire_how_to_eliminate_an/"},{"title":"ZLUDA update Q2 2025 - bigger team, more groundwork, less bugs","url":"https://vosen.github.io/ZLUDA/blog/zluda-update-q2-2025/","date":1751477979,"author":"/u/vosen_l","guid":181111,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lq27el/zluda_update_q2_2025_bigger_team_more_groundwork/"},{"title":"k3s in dual-stack no ipv6","url":"https://www.reddit.com/r/kubernetes/comments/1lq2365/k3s_in_dualstack_no_ipv6/","date":1751477700,"author":"/u/G4rp","guid":180906,"unread":true,"content":"<p>I'm trying to building an on-prem dual-stack cluster with my RPi 5 for learning new stuff.</p><p>I'm currently working with ULA address space, to all my node is assigned an ipv6 address:</p><pre><code>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER\\_UP&gt; mtu 1500 qdisc pfifo\\_fast state UP group default qlen 1000 link/ether d8:3a:XX:XX:65:XX brd ff:ff:ff:ff:ff:ff inet 192.168.14.3/26 brd 192.168.14.63 scope global dynamic noprefixroute eth0 valid\\_lft 909sec preferred\\_lft 909sec inet6 fd12:3456:789a:14:3161:c474:a553:4ea1/64 scope global noprefixroute valid\\_lft forever preferred\\_lft forever inet6 fe80::98e6:ad86:53e5:ad64/64 scope link noprefixroute valid\\_lft forever preferred\\_lft forever </code></pre><p>There's no way that K3s will recognise it:</p><pre><code>kubectl get node cow -o json | jq '.status.addresses' [ { \"address\": \"192.168.14.3\", \"type\": \"InternalIP\" }, { \"address\": \"XXX\", \"type\": \"Hostname\" } ] </code></pre><p>And conseguence also Cilium:</p><pre><code>time=2025-07-02T17:20:25.905770868Z level=info msg=\"Received own node information from API server\" module=agent.controlplane.daemon nodeName=XXX labels=\"map[beta.kubernetes.io/arch:arm64 beta.kubernetes.io/os:linux kubernetes.io/arch:arm64 kubernetes.io/hostname:XXX kubernetes.io/os:linux node-role.kubernetes.io/control-plane:true node-role.kubernetes.io/master:true]\" ipv4=192.168.14.3 ipv6=\"\" v4Prefix=10.42.1.0/24 v6Prefix=fd22:2025:6a6a:42::100/120 k8sNodeIP=192.168.14.3 </code></pre><p>I'm installing my cluster with those switches: <code>--cluster-cidr=10.42.0.0/16,fd22:2025:6a6a:42::/104 --service-cidr=10.43.0.0/16,fd22:2025:6a6a:43::/112 --kube-controller-manager-arg=node-cidr-mask-size-ipv6=120</code> also tried with  but no way :(</p>","contentLength":1611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code Your Own Desktop GUI App With Rust Iced Crate","url":"https://youtu.be/2CQ4hLB2IMw","date":1751477147,"author":"/u/JonkeroTV","guid":181009,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lq1usf/code_your_own_desktop_gui_app_with_rust_iced_crate/"},{"title":"How to write Rust in the Linux kernel: part 1","url":"https://lwn.net/SubscriberLink/1024202/16bb2f1e0d67bb32/","date":1751476345,"author":"/u/CrankyBear","guid":181116,"unread":true,"content":"<blockquote><p>\nLWN.net is a subscriber-supported publication; we rely on subscribers\n       to keep the entire operation going.  Please help out by <a href=\"https://lwn.net/Promo/nst-nag4/subscribe\">buying a subscription</a> and keeping LWN on the\n       net.\n</p></blockquote><p>\nThe Linux kernel is seeing a steady accumulation of Rust code. As it becomes\nmore prevalent, maintainers may want to know how to read, review, and test the\nRust code that relates to their areas of expertise. Just as kernel C code is\ndifferent from user-space C code, so too is kernel Rust code somewhat different\nfrom user-space Rust code. That fact makes Rust's\n<a href=\"https://doc.rust-lang.org/stable/book/\">\nextensive documentation</a> of\nless use than it otherwise would be, and means that potential contributors with\nuser-space experience will need some additional instruction.\nThis article is the first in a multi-part series aimed at helping existing\nkernel contributors become familiar with Rust, and helping existing Rust\nprogrammers become familiar with what the kernel does differently from the\ntypical Rust project.\n</p><p>\nIn order to\nlay the groundwork for the rest of the articles in this series, this\nfirst article gives a high-level overview of installing and configuring Rust\ntooling, as well as an explanation of how Rust fits into the kernel's existing\nbuild system. Future articles will cover how Rust fits into the kernel's\nmaintainership model, what goes into writing a driver in Rust, the design of the\nRust interfaces to the rest of the kernel, and hints about specific things to\nlook for when reviewing Rust code.\n</p><p>\nWhile support for Rust on GCC is\n<a href=\"https://rust-gcc.github.io/2025/06/04/2025-05-monthly-report.html\">\ncatching up</a>, and the  code generation backend is\nnow capable of compiling the Rust components of the kernel,\nthe Rust for Linux project currently only supports building with plain .\nSince  uses LLVM, the project also recommends building the kernel\nas a whole with Clang while working on Rust code (although mixing GCC on the C\nside and LLVM on the Rust side does work).\nThe build also requires\n<a href=\"https://rust-lang.github.io/rust-bindgen/\">\nbindgen</a> to build the C/Rust API bindings, and\na copy of the Rust standard library so that it can be built with the flags that\nthe kernel requires. Building the kernel in the recommended way therefore requires Clang,\nlld, LLVM, the Rust compiler, the source form of the Rust standard library, and\nbindgen, at a minimum.\n</p><p>\nMany Linux distributions package\nsufficiently current versions of all of these; the\n<a href=\"https://docs.kernel.org/rust/quick-start.html\">Rust quick start documentation</a>\ngives distribution-specific installation instructions. The minimum version of\n required is 1.78.0, released in May 2024. The Rust for Linux\nproject has committed to not raising the minimum required version unnecessarily.\nAccording to Miguel Ojeda,\nthe current informal plan is to stick with the version included in Debian\nstable, once that catches up with the current minimum (likely later this year).\n</p><p>\nDevelopers working on Rust should probably also install\n<a href=\"https://doc.rust-lang.org/clippy/\">\nClippy</a> (Rust's linter),\n<a href=\"https://doc.rust-lang.org/rustdoc/index.html\">\nrustdoc</a> (Rust's documentation building tool),\nand\n<a href=\"https://rust-analyzer.github.io/\">\nrust-analyzer</a> (the Rust\n<a href=\"https://microsoft.github.io/language-server-protocol/\">\nlanguage server</a>), but these are not strictly required. The Rust for Linux\nproject tries to keep the code free of linter warnings, so patches that\nintroduce new warnings may be frowned upon by maintainers.\nInvoking  in the root of the kernel source\nwill check that the necessary tools have compatible versions installed.\nIndeed, all the commands discussed here should be run from the root of the\nrepository.\nThe  command will set\nup configuration files for rust-analyzer that should allow it to work seamlessly\nwith an editor that has language server support, such as Emacs or Vim.\n</p><p>\nRust code is controlled by two separate kernel configuration values.\n is automatically set when compatible tooling\nis available;  (available under \"General Setup ‚Üí Rust\nsupport\") controls whether any Rust code is actually built, and depends on the\nfirst option. Unlike the vast majority of user-space Rust projects, the kernel\ndoes not use Cargo, Rust's package manager and build tool. Instead, the kernel's\nmakefiles directly invoke the Rust compiler in the same way they would a C\ncompiler. So adding an\nobject to the correct make target is all that is needed to build a Rust module:\n</p><pre>    obj-$(CONFIG_RUST) += object_name.o\n</pre><p>\nThe code directly enabled by  is largely the support code\nand bindings between C and Rust, and is therefore not a representative sample of\nwhat most Rust driver code actually looks like. Enabling the Rust sample code\n(under \"Kernel hacking ‚Üí Sample kernel code ‚Üí Rust samples\") may provide a more\nrepresentative sample.\n</p><p>\nRust's testing and linting tools have also been integrated into the kernel's\n<a href=\"https://kernelnewbies.org/KernelBuild\">\nexisting build system</a>. To run Clippy, add  to the\n invocation; this performs a special build of the kernel with\ndebugging options enabled that make it unsuitable for production use,\nand so should be done with care.\n will build a local copy of the Rust documentation, which\nalso checks for some documentation warnings, such as missing documentation\ncomments or malformed intra-documentation links.\nThe tests can be run with\n<a href=\"https://docs.kernel.org/dev-tools/kunit/index.html\"></a>, the kernel's white-box unit-testing tool. The tool does\nneed additional arguments to set the necessary configuration variables for a\nRust build:\n</p><pre>    ./tools/testing/kunit/kunit.py run --make_options LLVM=1 \\\n        --kconfig_add CONFIG_RUST=y --arch=&lt;host architecture&gt;\n</pre><p>\nActually locating a failing test case could trip up people familiar with KUnit\ntests, though.\nUnlike the kernel's C code, which typically has KUnit tests written in separate files,\nRust code tends to have tests in the same file as the code that it is testing.\nThe convention is to use a separate\n<a href=\"https://doc.rust-lang.org/book/ch07-02-defining-modules-to-control-scope-and-privacy.html\">\nRust module</a> to keep the test code out of\nthe main namespace (and enable conditional compilation, so it's not included in\nrelease kernels). This module is often (imaginatively) called \"test\", and must\nbe annotated with the  macro.\nThat macro is implemented in\n<a href=\"https://elixir.bootlin.com/linux/v6.15.1/source/rust/macros/kunit.rs\"></a>; it looks through the\nannotated module for functions marked with  and sets up the\nneeded C declarations for KUnit to automatically recognize the test cases.\n</p><p>\nRust does have another kind of test that doesn't correspond directly to a C unit\ntest, however. A \"doctest\" is a test embedded in the documentation of a\nfunction, typically showing how the function can be used. Because it is a real\ntest, a doctest can be relied upon to\nremain current in a way that a mere example may not. Additionally, doctests are\nrendered as part of the automatically generated\n<a href=\"https://rust.docs.kernel.org/kernel/\">\nRust API documentation</a>.\nDoctests run as part of the KUnit test suite as well, but must be specifically\nenabled (under \"Kernel hacking ‚Üí Rust hacking ‚Üí Doctests for the `kernel` crate\").\n</p><p>\nAn example of a function with a doctest (lightly reformatted from the Rust\n<a href=\"https://elixir.bootlin.com/linux/v6.15.1/source/rust/kernel/str.rs#L35\">\nstring helper functions</a>) looks like this:\n</p><pre>    /// Strip a prefix from `self`. Delegates to [`slice::strip_prefix`].\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use kernel::b_str;\n    /// assert_eq!(\n    ///     Some(b_str!(\"bar\")),\n    ///     b_str!(\"foobar\").strip_prefix(b_str!(\"foo\"))\n    /// );\n    /// assert_eq!(\n    ///     None,\n    ///     b_str!(\"foobar\").strip_prefix(b_str!(\"bar\"))\n    /// );\n    /// assert_eq!(\n    ///     Some(b_str!(\"foobar\")),\n    ///     b_str!(\"foobar\").strip_prefix(b_str!(\"\"))\n    /// );\n    /// assert_eq!(\n    ///     Some(b_str!(\"\")),\n    ///     b_str!(\"foobar\").strip_prefix(b_str!(\"foobar\"))\n    /// );\n    /// ```\n    pub fn strip_prefix(&amp;self, pattern: impl AsRef&lt;Self&gt;) -&gt; Option&lt;&amp;BStr&gt; {\n        self.deref()\n            .strip_prefix(pattern.as_ref().deref())\n            .map(Self::from_bytes)\n    }\n</pre><p>\nNormal\n<a href=\"https://doc.rust-lang.org/reference/comments.html\">\ncomments</a> in Rust code begin with . Documentation\ncomments, which are processed by various tools, start with  (to\ncomment on the following item) or  (to comment on the containing\nitem). These are equivalent:\n</p><pre>    /// Documentation\n    struct Name {\n        ...\n    }\n\n    struct Name {\n        //! Documentation\n        ...\n    }\n</pre><p>\nDocumentation comments are analogous to the specially formatted\n comments used in the kernel's C code. In this doctest, the\n macro (an example of the\nother kind of macro invocation in Rust) is used\nto compare the return value of the  method to what it\nshould be.\n</p><div><pre># Check Rust tools are installed\nmake rustavailable\n# Build kernel with Rust enabled\n# (After customizing .config)\nmake LLVM=1\n# Run tests\n./tools/testing/kunit/kunit.py \\\n  run \\\n  --make_options LLVM=1 \\\n  --kconfig_add CONFIG_RUST=y \\\n  --arch=&lt;host architecture&gt;\n# Run linter\nmake LLVM=1 CLIPPY=1\n# Check documentation\nmake rustdoc\n# Format code\nmake rustfmt\n</pre></div><p>\nFinally, Rust code can also include\n<a href=\"https://docs.kernel.org/dev-tools/kselftest.html\">\nkernel selftests</a>, the kernel's third way to\nwrite tests. These need to be configured on an individual basis, using the\nkernel-configuration snippets in the  directory.\nKselftests are intended to be run on a machine booted with the corresponding\nkernel, and can be run with .\n</p><p>\nRust's syntax is complex. This has been one of several sticking points in\nadoption of the language, since people often feel that it makes the language\ndifficult to read. That problem cannot wholly be solved with formatting tools,\nbut they do help. Rust's canonical formatting tool is called ,\nand if it is installed, it can be run with  to reformat all\nthe Rust code in the kernel.\n</p><p>\nBuilding and testing Rust code is necessary, but not\nsufficient, to review Rust code. It may be enough to get one started\nexperimenting with the existing Rust code in the kernel, however.\nNext up, we will\nwill do an in-depth comparison between a simple driver module and its Rust\nequivalent, as an introduction to the kernel's Rust driver abstractions.\n</p>","contentLength":9470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lq1ib8/how_to_write_rust_in_the_linux_kernel_part_1/"},{"title":"What models say they're thinking may not accurately reflect their actual thoughts","url":"https://www.reddit.com/r/artificial/comments/1lq19kb/what_models_say_theyre_thinking_may_not/","date":1751475787,"author":"/u/MetaKnowing","guid":181385,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/MetaKnowing\"> /u/MetaKnowing </a>","contentLength":34,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A List Is a Monad","url":"https://alexyorke.github.io//2025/06/29/a-list-is-a-monad/","date":1751475586,"author":"/u/ketralnis","guid":182615,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lq169c/a_list_is_a_monad/"},{"title":"This influencer does not exist","url":"https://www.reddit.com/r/artificial/comments/1lq126v/this_influencer_does_not_exist/","date":1751475328,"author":"/u/MetaKnowing","guid":181114,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I wrote tutorials on interfacing RabbitMQ with Rust using amqprs library.","url":"https://www.reddit.com/r/rust/comments/1lq0uue/i_wrote_tutorials_on_interfacing_rabbitmq_with/","date":1751474852,"author":"/u/bassam_masry","guid":181204,"unread":true,"content":"<p>Long story: Some time ago in my previous job I was asked to write a microservice in Rust for receiving email content from RabbitMQ and then sending these emails. Unfortunately, RabbitMQ does not have an official client library for Rust, but it recommends amqprs and Lapin. Finding that Lapin was quite complicated, I decided to give amqprs a chance. I found no tutorials for using that library and the documentation was lacking, but I managed to do it since I have some experience with RabbitMQ internals. I then decided to write tutorials that cover using this library myself, so here they are: <a href=\"https://bassammasry.com/en/blog/rust-interface-rabbitmq/\">1- Connecting to RabbitMQ</a><a href=\"https://bassammasry.com/en/blog/rust-receive-rabbitmq/\">2- Receiving messages from RabbitMQ</a><a href=\"https://bassammasry.com/en/blog/rust-publish-rabbitmq/\">3- Publishing messages to RabbitMQ</a></p><p>I will appreciate feedback, if you have any. Also, there is a version of the tutorials in Russian.</p>","contentLength":788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lox is a parser and lexer generator for Go","url":"https://dcaiafa.github.io/lox/","date":1751473761,"author":"/u/dgwelder","guid":180799,"unread":true,"content":"<p>Heavily inspired on ANTLR on the surface (combined parser and lexer, action code separated from grammar), but more similar to yacc on the internals (LR(1), dependency-free parser). I'm especially proud of the type-safe Go action generation where the reduce-artifact's Go type is determined by the user-action's return type, and then used to match and verify its use in other productions.</p>","contentLength":387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lq0e12/lox_is_a_parser_and_lexer_generator_for_go/"},{"title":"Yet another ZIP trick","url":"https://hackarcana.com/article/yet-another-zip-trick","date":1751473237,"author":"/u/ketralnis","guid":181010,"unread":true,"content":"<p><a href=\"https://hackarcana.com/privacy-policy\" target=\"_blank\" trans-code=\"cookie_basic_info_link_title\">Privacy Policy</a></p>","contentLength":14,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lq061n/yet_another_zip_trick/"},{"title":"How could anyone use longhorn if you can‚Äôt secure the service? (Also request for alternatives)","url":"https://www.reddit.com/r/kubernetes/comments/1lpyeob/how_could_anyone_use_longhorn_if_you_cant_secure/","date":1751469151,"author":"/u/SnooPears7079","guid":180657,"unread":true,"content":"<p>EDIT: SOLVED! I had a really basic misunderstanding of how the UI works. I was under the impression that the UI pod served static assets, and then the browser talked to the backend through an ingress.</p><p>This isn‚Äôt the case. The UI pod serves the assets and proxies the requests to the cluster, so the backend pod does not need to be exposed. While it would help if the backend pod could be secured, it doesn‚Äôt need to be exposed anywhere but cluster local. </p><p>I really want to like longhorn! I‚Äôve used for a bit and it‚Äôs so nice. </p><p>Unfortunately, this issue: <a href=\"https://github.com/longhorn/longhorn/discussions/3031\">https://github.com/longhorn/longhorn/discussions/3031</a> is just totally unaddressed. You literally can‚Äôt add basic auth to the service or pod. You CAN add auth to the UI, but if my longhorn API is exposed to my home network (and you have to, for an out of cluster device like my iPad web browser to talk to the API), an attacker who‚Äôs compromised my home network can just raw http call the backend and delete volumes.</p><p>Am I missing something? Is this not a totally blocking security issue? I could just be totally misunderstanding - in fact, I hope I am!</p><p>Does anyone know any software that does similar things to longhorn? I really like how you can backup to s3, that‚Äôs my primary usecase. </p>","contentLength":1250,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feel Like a Full Member Now","url":"https://www.reddit.com/r/linux/comments/1lpx8mx/feel_like_a_full_member_now/","date":1751466361,"author":"/u/Terrible-Mobile2211","guid":181386,"unread":true,"content":"<p>A little context, I started to use linux VM's starting in 2021 for various things. In 2023, after I got sick of Windows 11 and the direction Microsoft is going, I made the transition to Linux complete. </p><p>Last night, I was trying to get something to work on github, and was exhausted from a couple days of not sleeping (I have insomnia, been a problem since I was a kid) and wasn't fully paying attention to the commands I was running. </p><p>Long story short, I completely shredded my system by accident. Not sure how, but pretty sure when I was removing a package I accidentally hit the up arrow in the console and included some main debian drivers. </p><p>It's been a fun few hours this morning after finally sleeping a little restoring everything.</p><p>Also, thank god for backups.</p>","contentLength":762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LibreOffice project and community recap: June 2025","url":"https://blog.documentfoundation.org/blog/2025/07/02/libreoffice-project-and-community-recap-june-2025/","date":1751465827,"author":"/u/themikeosguy","guid":180663,"unread":true,"content":"<p>Here‚Äôs our summary of updates, events and activities in the LibreOffice project in the last four weeks ‚Äì click the links to learn more‚Ä¶</p><ul><li>We started the month with Episode 3 of the LibreOffice Podcast ‚Äì this time looking at Quality Assurance (QA) in Free and Open Source Software. Watch it below ‚Äì or <a href=\"https://peertube.opencloud.lu/w/8319cGkkH1xoY8Ho6TjSWk\">on PeerTube</a>.</li></ul><div allowfullscreen=\"\" data-no-lazy=\"1\" data-skipgform_ajax_framebjll=\"\"><p><strong>Please confirm that you want to&nbsp;play a YouTube video.</strong> By accepting, you will be accessing content from YouTube, a service provided by an external third party.</p><p>If you accept this notice, your choice will be saved and the page will refresh.</p></div><ul><li>The end of Windows 10 is approaching, so it‚Äôs time to consider Linux and LibreOffice! That‚Äôs the message behind the ‚ÄúEnd of 10‚Äù campaign, <a href=\"https://blog.documentfoundation.org/blog/2025/06/11/the-end-of-windows-10/\">which we‚Äôre supporting</a>.</li></ul><ul><li>Before LibreOffice there was OpenOffice, and before OpenOffice there was StarOffice. And how was StarOffice developed? We <a href=\"https://blog.documentfoundation.org/blog/2025/06/18/before-libreoffice-there-was-openoffice-and-before-openoffice-there-was-staroffice/\">talked to Stefan Soyka</a>, who worked on the suite in the early ‚Äô90s, and has some entertaining stories to tell üòä</li></ul><ul><li>New LibreOffice merchandise is here! We <a href=\"https://blog.documentfoundation.org/blog/2025/06/24/new-libreoffice-merchandise-is-here/\">updated our Spreadshirt shop</a> with new designs and many extra items. Buy something and support LibreOffice ‚Äì some of the proceeds go back to the project!</li></ul><ul><li><a href=\"https://blog.documentfoundation.org/blog/2025/06/30/registration-open-for-the-libreoffice-conference-2025/\">Registration is now open</a> for the LibreOffice Conference 2025. Join us from 4 ‚Äì 6 September in Budapest ‚Äì we‚Äôll have technical talks, workshops, social events and more‚Ä¶</li></ul>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lpx0sk/libreoffice_project_and_community_recap_june_2025/"},{"title":"Markdowns in Go","url":"https://www.reddit.com/r/golang/comments/1lpx0s4/markdowns_in_go/","date":1751465826,"author":"/u/undercannabas","guid":181115,"unread":true,"content":"<p>Hi, I'm interested in Go. I can write basic CRUD operations, handle authentication, and work with databases. Right now, I'm really curious about markdown‚Äîhow it works and how I can easily use it in Go. </p><p>Has anyone written about this? I‚Äôd love to check out some repositories or useful articles if you have recommendations! </p>","contentLength":325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why is Linux on Snapdragon a problem if it isn't a problem on ARM chips like the Raspberry Pi?","url":"https://www.reddit.com/r/linux/comments/1lpwu7d/why_is_linux_on_snapdragon_a_problem_if_it_isnt_a/","date":1751465378,"author":"/u/Carbonga","guid":180664,"unread":true,"content":"<p>Pretty much the title: Why is Linux on Snapdragon a problem if it isn't a problem on ARM chips like the Raspberry Pi? How come one chip on one embedded system is so much better supported than another (like the Snapdragon X Elite)? Are they so different? Thank you for enlightening me!</p>","contentLength":284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We Just got 5 Malicious npm Packages Eliminated in a Cat and Mouse Game","url":"https://github.com/ossf/malicious-packages/pull/932","date":1751465222,"author":"/u/N1ghtCod3r","guid":180659,"unread":true,"content":"<p>Creator and maintainer of <a href=\"https://github.com/safedep/vet\">vet</a> here. We monitor public package registries, perform code analysis to identify malicious packages &amp; work towards getting them reported and eliminated.</p><p>We recently reported a bunch of malicious npm packages which finally got included in OSV and now hopefully all SCA tools and everyone else will identify and block these. Npm takes longer but got these removed from the registry as well.</p><p>We have been doing this for a while. We started with simple signature matching, then static code analysis and eventually dynamic analysis. Our systems are becoming complex, consuming resources and like any other complex systems, harder to extend. But we don't see any improvement in the overall ecosystems. We are still seeing the same type of malicious packages published every day. I am sure there are more sophisticated ones that we are yet to identify.</p><p>Intuitively it just seems like the problem of early 2000 where anyone would upload malicious executables in various  download sites. Eventually the AV and OS ecosystems improved in terms adopting signed executables, endpoint protection etc. With malicious open source packages, the attack is shifted towards developers, leveraging higher level scripting languages running within trusted processes like Node, Java, Python etc.</p><p>How do you see a solution emerging against malicious package sprawl?</p>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lpws1t/we_just_got_5_malicious_npm_packages_eliminated/"},{"title":"PSA: You can easily swap Rust‚Äôs allocator for better concurrency‚Äîhere‚Äôs a crate that automates it","url":"https://www.reddit.com/r/rust/comments/1lpwcmt/psa_you_can_easily_swap_rusts_allocator_for/","date":1751464159,"author":"/u/Working-Comfort-2514","guid":180658,"unread":true,"content":"<p> The default system allocator (like glibc malloc) can become a performance bottleneck under high concurrency due to global locking. Modern allocators such as  or  use thread-local caches and optimized strategies to improve throughput and reduce fragmentation.</p><p>Instead of manually configuring allocators with  and platform checks,  automatically picks the recommended allocator for your target platform:</p><ul><li>Linux/macOS/Windows: mimalloc</li><li>WASM: default allocator for compatibility</li><li>Embedded/no_std: embedded-alloc</li></ul><pre><code>[dependencies] auto-allocator = \"*\" </code></pre><pre><code>use auto_allocator; // Done! Memory allocation is now optimized fn main() { let info = auto_allocator::get_allocator_info(); println!( \"Using allocator: {:?}, reason: {}\", info.allocator_type, info.reason ); // your code here } </code></pre><p>If you‚Äôre curious about the performance impact, there are some references:</p><p>I‚Äôd be happy to hear any feedback or ideas. Thanks!</p>","contentLength":895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[EKS] How Many Ingress Resources Should I Use for 11 Microservices?","url":"https://www.reddit.com/r/kubernetes/comments/1lpvq5g/eks_how_many_ingress_resources_should_i_use_for/","date":1751462513,"author":"/u/Junior_Distance6875","guid":180478,"unread":true,"content":"<p>I‚Äôm deploying a demo <strong>microservices app with 11 services</strong> to , and I‚Äôm using:</p><ul><li> with an  for public-facing traffic.</li><li>Planning to use <strong>another NGINX Ingress Controller with a separate NLB</strong> (internal) for dashboards like , exposed via  + .</li></ul><p>Should I define one ingress resource for 2-3 microservices? </p><p>or consolidate all 11 services into one ingress resource?</p><p>It feels messy to cram 11  rules into one Ingress manifest, even if it technically works. </p><p>I'm <strong>planning to set up the internal ingress to try myself</strong>, but curious ‚Äî is having two ingress controllers (one public, one internal) ?</p><p>Thanks in advance for sharing how you‚Äôve handled similar setups!</p>","contentLength":643,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] The tabular DL model TabM now has a Python package","url":"https://www.reddit.com/r/MachineLearning/comments/1lpvn4q/p_the_tabular_dl_model_tabm_now_has_a_python/","date":1751462294,"author":"/u/_puhsu","guid":181458,"unread":true,"content":"<p>Hi! My colleagues have recently published a Python package for <a href=\"https://github.com/yandex-research/tabm\">TabM</a> -- a <strong>simple and powerful DL architecture</strong> for solving predictive tasks on  (classification, regression, etc.).</p><p>In a nutshell, TabM efficiently imitates an ensemble of MLPs (see the image below). This basically means that TabM has the power of an ensemble, but at the same time remains practical and scalable. Among the recent highlights: üèÜ <strong>TabM has been successfully used on Kaggle</strong>, including the winning solutions! The package provides the PyTorch implementation of TabM, as well as PyTorch layers and functions for building custom TabM-like models.</p>","contentLength":619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is learning linux with mint good for beginners?","url":"https://www.reddit.com/r/linux/comments/1lpvgf4/is_learning_linux_with_mint_good_for_beginners/","date":1751461806,"author":"/u/leebonakiss","guid":180488,"unread":true,"content":"<p>I want to learn to \"use\" linux, building packages, manually installing drivers, and so on, but I haven't even dabbled in the terminal. I started using mint because I hate windows 11 about 2 months ago, when I built a new PC.</p><p>I would eventually like to switch to Arch, but I don't want to go balls to the wall, when I don't even know the basics yet.</p><p>Is mint a good place to get my foot in the door, or should I set up a virtual machine with Arch and mess around?</p>","contentLength":459,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security researcher earns $25k by finding secrets in so called ‚Äúdeleted commits‚Äù on GitHub, showing that they are not really deleted","url":"https://trufflesecurity.com/blog/guest-post-how-i-scanned-all-of-github-s-oops-commits-for-leaked-secrets","date":1751459570,"author":"/u/ScottContini","guid":180481,"unread":true,"content":"<ol><li data-preset-tag=\"p\"><p>GitHub Archive logs every public commit, even the ones developers try to delete. Force pushes often cover up mistakes like leaked credentials by rewriting Git history. GitHub keeps these dangling commits, from what we can tell, forever. In the archive, they show up as ‚Äúzero-commit‚Äù .&nbsp;</p></li><li data-preset-tag=\"p\"><p>I scanned every force push event since 2020 and uncovered secrets worth .&nbsp;</p></li><li data-preset-tag=\"p\"><p>Together with Truffle Security, we're open sourcing a new tool to scan your own GitHub organization for these hidden commits (<a href=\"https://github.com/trufflesecurity/force-push-scanner\" target=\"_blank\" rel=\"noopener\">try it here</a>).</p></li></ol><p><a href=\"https://github.com/trufflesecurity/force-push-scanner\" target=\"_blank\" rel=\"noopener\"></a><em><strong>tool identifies secrets in dangling commits.</strong></em></p><p><em>This guest post by Sharon Brizinov, a white-hat hacker, was developed through </em><a href=\"https://trufflesecurity.com/cfp\" rel=\"noopener\"><em>Truffle Security‚Äôs Research CFP program</em></a><em>. We first connected with Sharon after his widely shared write-up, </em><a href=\"https://medium.com/@sharon.brizinov/how-i-made-64k-from-deleted-files-a-bug-bounty-story-c5bd3a6f5f9b\" rel=\"noopener\"><em>How I Made 64k From Deleted Files</em></a><em>, where he used TruffleHog to uncover high-value secrets in public GitHub repositories. In this follow-up, Sharon expanded his research to access 100% of deleted commits on GitHub. He takes a deeper dive into one of our favorite areas: secrets hidden in deleted GitHub commits.</em></p><ul><li data-preset-tag=\"p\"><p>What Does it Mean to Delete a Commit?</p></li><li data-preset-tag=\"p\"><p>Finding all Deleted Commits</p></li><li data-preset-tag=\"p\"><p>Hunting for Impactful Secrets</p></li><li data-preset-tag=\"p\"><p>Case Study - Preventing a Massive Supply-Chain Compromise</p></li></ul><p>My name is <a href=\"https://www.linkedin.com/in/sharonbrizinov/\" rel=\"noopener\">Sharon Brizinov</a>, and while I usually focus on low-level vulnerability and exploitation research in OT/IoT devices, I occasionally dive into bug bounty hunting.</p><p>I recently published a <a href=\"https://medium.com/@sharon.brizinov/how-i-made-64k-from-deleted-files-a-bug-bounty-story-c5bd3a6f5f9b\" rel=\"noopener\">blog post</a> about uncovering secrets hidden in dangling blobs within GitHub repositories, which sparked quite a lively discussion. After the post, I had several conversations with various people including Dylan, the CEO of Truffle Security, who gave me some intriguing ideas for continuing to explore new methods for large-scale secret hunting. I decided to create a mind map with everything I know related to this topic and try to come up with a new idea.&nbsp;</p><p>I‚Äôll spare you my messy sketch, but here‚Äôs a roundup of the projects, blogs, ideas, and resources I zeroed in on (highly recommended):</p><ul><li data-preset-tag=\"p\"><p><a href=\"https://neodyme.io/en/blog/github_secrets/\" rel=\"noopener\">Hidden GitHub Commits and How to Reveal Them by Neodyme.io</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github\" rel=\"noopener\">Anyone can Access Deleted and Private Repository Data on GitHub by TruffleHog</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://trufflesecurity.com/blog/trufflehog-now-finds-all-deleted-and-private-commits-on-github\" rel=\"noopener\">TruffleHog now finds all Deleted &amp; Private Commits on GitHub by TruffleHog</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://trufflesecurity.com/blog/trufflehog-scans-deleted-git-branches\" rel=\"noopener\">TruffleHog Scans Deleted Git Branches by TruffleHog</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://www.aquasec.com/blog/undetected-hard-code-secrets-expose-corporations/\" rel=\"noopener\">Phantom Secrets: Undetected Secrets Expose Major Corporations by Aqua Security</a></p></li></ul><p>Eventually, I came up with a simple idea - I will use the Github Event API alongside the <a href=\"https://www.gharchive.org/\" rel=\"noopener\">GitHub Archive</a> project to scan all Zero-Commit Push-Events (deleted commits) for secrets. Everything was known, I just glued it together and built automation at scale that hunted for secrets.</p><p>In this blog, I will describe my journey from understanding why you can never really delete a commit in GitHub to how to find all of them and build automation around it.</p><h2>What Does it Mean to Delete a Commit?</h2><p>In my previous <a href=\"https://medium.com/@sharon.brizinov/how-i-made-64k-from-deleted-files-a-bug-bounty-story-c5bd3a6f5f9b\" rel=\"noopener\">blog post</a>, I discussed how I discovered supposedly deleted files within GitHub repositories. Specifically, I was able to reconstruct dangling blobs - objects that had been deleted and were no longer referenced by any commit or tree‚Ä¶ Or so I thought. After chatting with the Truffle folks, it turns out these orphaned blobs actually had orphaned commits that went along with them. And with a little research, I was able to uncover 100% of those orphaned commits at scale, across all of GitHub.&nbsp;</p><p>Suppose you‚Äôve accidentally committed and pushed a secret to your repository. What‚Äôs the next step? Typically, you‚Äôd want to reset the HEAD to the previous commit and force-push the changes, effectively removing the current commit and making it unreferenced - essentially deleting it. Here‚Äôs how you do it:</p><p>But as <a href=\"https://neodyme.io/en/blog/github_secrets/\" target=\"_blank\" rel=\"noopener\">neodyme </a>and <a href=\"https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github\" target=\"_blank\" rel=\"noopener\">TruffleHog</a> discovered, even when a commit is deleted from a repository, GitHub never forgets. If you know the full commit hash, you can access the supposedly deleted content. Moreover, you don't even need the full commit has, as TruffleHog <a href=\"https://trufflesecurity.com/blog/trufflehog-now-finds-all-deleted-and-private-commits-on-github\" target=\"_blank\" rel=\"noopener\">discovered </a>- it's enough to brute-force just the first four hex-digits.</p><h3>Force Pushing: A Tutorial</h3><p>Let‚Äôs see this in action using my own repository: <a href=\"https://github.com/SharonBrizinov/test-oops-commit\" rel=\"noopener\">test-oops-commit</a>. Try to locate the deleted commit - <a href=\"https://github.com/SharonBrizinov/test-oops-commit/commit/9eedfa00983b7269a75d76ec5e008565c2eff2ef\" rel=\"noopener\">9eedfa00983b7269a75d76ec5e008565c2eff2ef</a>.&nbsp;</p><p>To help visualize our commits, I prepared a simple bash script that shows the <a href=\"https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\" target=\"_blank\" rel=\"noopener\">commit-tree-blob</a> objects, :</p><div><div><div><div><div><div><pre translate=\"no\"><code>- -- | - - | - -</code></pre></div></div></div></div></div></div><p>We start by creating a simple repository with a single commit (a  file):</p><p>Next, we create a new file named  containing our secret . We accidentally commit and push our secret to GitHub.</p><p>We look at the commit tree to see that we have a new commit ‚Ä¶ which is associated with a new tree and a new blob for the file . We see the same when we run , , or when we access it from the web on GitHub.</p><p>Oops! We discover our mistake and delete the commit by moving the HEAD of the branch to the previous commit and force-push it using:</p><p>Let's remove our local version of the repo, clone the repository again, and check the commit tree. Phew, no secrets; the commit was really deleted!</p><p>But we remember the commit hash, we we check online on GitHub and the commit can still be accessed - <a href=\"https://github.com/SharonBrizinov/test-oops-commit/commit/9eedfa00983b7269a75d76ec5e008565c2eff2ef\" rel=\"noopener\">9eedfa00983b7269a75d76ec5e008565c2eff</a> (even accessing using four hex digits is enough <a href=\"https://github.com/SharonBrizinov/test-oops-commit/commit/9eed\" target=\"_blank\" rel=\"noopener\">9eef</a>). However, this time we get a message saying that the commit is deleted or doesn't belong to any branch on this repository. </p><p>When you force-push after resetting (aka  followed by ), you remove Git‚Äôs reference to that commit from your branch, effectively making it unreachable through normal Git navigation (like ). However, the commit is still accessible on GitHub because GitHub stores these reflogs.&nbsp;</p><p>Why? I don‚Äôt know for sure, but GitHub does <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/what-happens-to-forks-when-a-repository-is-deleted-or-changes-visibility\" rel=\"noopener\">give some hints</a>. As I see it, GitHub is a much more complex beast than just a git server. It has many layers, including pull-requests, forks, private-public settings, and more.&nbsp;</p><p>My guess is that to support all of these features, GitHub stores all commits and never deletes them. Here are some cases to consider:</p><ol><li data-preset-tag=\"p\"><p>What are pull requests? These are just temporary branches, as <a href=\"https://www.aquasec.com/blog/undetected-hard-code-secrets-expose-corporations/\" rel=\"noopener\">Aqua Security</a> wrote about, and can be retrieved by fetching all refs using -</p><ol><li data-preset-tag=\"p\"><p><code>git -c \"remote.origin.fetch=+refs/*:refs/remotes/origin/*\" fetch origin</code></p></li></ol></li><li data-preset-tag=\"p\"><p>How does the GitHub fork network work? What happens when you ‚Äúfork‚Äù a repository? All the data is replicated, including commits you might delete.</p></li></ol><p>For these cases, and probably many others too (auditing? monitoring?) Github stores all the commits and won‚Äôt delete them, even if you force-push the head and ‚Äúdelete‚Äù the commit.</p><p>OK, so commits are not really deleted. Fine. But you‚Äôd still need to know the full commit hash, or at least the first four hex-digits ignoring collisions (). As it turns out, TruffleHog <a href=\"https://trufflesecurity.com/blog/trufflehog-now-finds-all-deleted-and-private-commits-on-github\" rel=\"noopener\">has a tool </a>to do just that, but it‚Äôs very slow, as you can imagine, going through all those. It doesn‚Äôt scale well beyond taking a day or two on a single repo.</p><p>But there‚Äôs another way. A faster way, I‚Äôm now happy to share with you. The GitHub Event API is part of GitHub's REST API, which allows users to retrieve information about events that occur within GitHub. Events represent various activities in GitHub, such as:</p><ul><li data-preset-tag=\"p\"><p>Opening or closing issues or pull requests</p></li></ul><ul><li data-preset-tag=\"p\"><p>No API token or auth is needed!</p></li><li data-preset-tag=\"p\"><p>You can see all the events that GitHub supports <a href=\"https://docs.github.com/en/rest/using-the-rest-api/github-event-types?apiVersion=2022-11-28\" rel=\"noopener\">here</a>.</p></li><li data-preset-tag=\"p\"><p>Events are recorded in near-real-time, but may be delayed by a few seconds.</p></li><li data-preset-tag=\"p\"><p>It‚Äôs only for public repositories.</p></li></ul><p>So, we could monitor commit data for all GitHub public repositories and store all the hashes. No more guessing commit hashes! Yeah, but it‚Äôs way too much. We are talking about millions of events per hour, and what about past events? Are they lost?</p><p>Luckily for us a great developer named <a href=\"https://github.com/igrigorik\" rel=\"noopener\">Ilya Grigorik</a> decided many years ago to start a project that listens to GitHub‚Äôs event stream and systematically archives it. The project is open-source and called <a href=\"https://github.com/igrigorik/gharchive.org\" rel=\"noopener\">GH Archive</a> and the website is <a href=\"http://gharchive.org/\" rel=\"noopener\">gharchive.org</a>. So, if we want, for example, to get the entire GitHub public activity around Jan 1st at 3pm UTC we just download this from here: <a href=\"https://data.gharchive.org/2015-01-01-15.json.gz\" rel=\"noopener\">https://data.gharchive.org/2015-01-01-15.json.gz</a>.</p><p>Here is a random sample of a  from that  archive:</p><h3>Finding Force Push Deleted Commits</h3><p>To identify only the deleted commits from force push events, we can look for push events that contain zero commits. Why would a Git push event have no commits? It indicates a force push that resets the branch - essentially just moving the HEAD without adding any new commits! I call this an  or a .</p><p>Let‚Äôs see a quick example. We will download a random archive and search for such an event.</p><p>If we randomly select one of the target event types, we will see that the  array is empty (zero commits). And if we look at the  commit - the one that was ‚Äúdeleted‚Äù (the HEAD before moving to HEAD^1, which is the ‚Äúafter‚Äù) - we see that Github still holds a record of it 10 years later!</p><p>Here it is - <a href=\"https://github.com/grapefruit623/gcloud-python/commit/e9c3d31212847723aec86ef96aba0a77f9387493\" rel=\"noopener\">https://github.com/grapefruit623/gcloud-python/commit/e9c3d31212847723aec86ef96aba0a77f9387493</a></p><p>And it‚Äôs not necessarily just the  commit that was deleted. Sometimes a force push overwrites many commits at once.&nbsp;</p><p>Given a Github organization (or user), repo name, and commit hash, it‚Äôs quite easy to scan the content of the ‚Äúdeleted‚Äù commit(s) for secrets using Git access:</p><ul><li data-preset-tag=\"p\"><p>Clones a repo in a minimal way.</p><ul><li data-preset-tag=\"p\"><p>: Omits file contents (blobs), only history/trees/commits.</p></li><li data-preset-tag=\"p\"><p>: Doesn't check out the working directory (no files appear yet).</p></li></ul></li><li data-preset-tag=\"p\"><p>Fetches a specific commit ().</p></li><li data-preset-tag=\"p\"><p>Scans for secrets using TruffleHog.</p><ul><li data-preset-tag=\"p\"><p>TruffleHog will automatically pull down the file contents (blobs) that need to be scanned.&nbsp;</p></li><li data-preset-tag=\"p\"><p>This command will search for secrets in all commits, starting with the  commit and working backward until the start of that branch. This ensures that all data from a force push overwriting more than one commit gets scanned; however, it will scan some non-dangling commits. The <a href=\"https://github.com/trufflesecurity/force-push-scanner\" rel=\"noopener\">open-source tool</a> we‚Äôve released is a bit more efficient and only scans the actual dangling (dereferenced) commits.</p></li></ul></li></ul><p>GitHub doesn't specify an exact rate limit for Git operations, but excessive cloning or fetching of repositories may trigger delaying or rate limiting (see <a href=\"https://github.com/orgs/community/discussions/44515\" rel=\"noopener\">here</a>).</p><p>In addition, we can use other methods to query a specific deleted/dangling commit with the GitHub API or simply with the Github web UI.</p><p>Query for the commit patch using&nbsp; GitHub‚Äôs REST API:&nbsp;</p><p><code><strong>https://api.github.com/repos/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commits/&lt;HASH&gt;</strong></code></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://api.github.com/repos/github/gitignore/commits/e9552d855c356b062ed82b83fcaacd230821a6eb\" rel=\"noopener\">https://api.github.com/repos/github/gitignore/commits/e9552d855c356b062ed82b83fcaacd230821a6eb</a></p></li></ul><p>Note: There‚Äôs a <a href=\"https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api?apiVersion=2022-11-28\" rel=\"noopener\">strict rate-limit</a> of 5,000 queries per hour for registered users and merely 60 for unregistered users. The server response header  indicates how many API calls users have left.</p><h4>Direct Web Access via <a href=\"https://github.com/\" rel=\"noopener\">Github.com</a></h4><p>You can also access the commit details directly from <a href=\"https://github.com/\" rel=\"noopener\">GitHub.com</a>.                                             </p><p>Here are three different examples of how to access any commit via the GitHub website:</p><p><strong>https://github.com/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commit/&lt;HASH&gt;</strong></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb\" target=\"_blank\" rel=\"noopener\">https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb</a></p></li></ul><p><strong>https://github.com/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commit/&lt;HASH&gt;.patch</strong></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.patch\" target=\"_blank\" rel=\"noopener\">https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.patch</a></p></li></ul><p><strong>https://github.com/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commit/&lt;HASH&gt;.diff</strong></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.diff\" rel=\"noopener\">https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.diff</a></p></li></ul><p>Although there is no documented rate limit, access is not guaranteed under heavy usage, and their WAF may block requests at any time without notice.</p><p>So we have all the ingredients - we can get all GitHub event data, search for all events, fetch the ‚Äúdeleted‚Äù commit (the  hash), and then scan for active secrets using TruffleHog. Let‚Äôs do this.&nbsp;</p><p>You know what? No need to build it, because together with Truffle Security‚Äôs Research team, we‚Äôre<a href=\"https://github.com/trufflesecurity/force-push-scanner\" rel=\"noopener\"> open-sourcing a new tool</a> to search the entire GH Archive for ‚ÄúOops Commits‚Äù made by your GitHub organization or user account. Since the entire GH Archive is available as a <a href=\"https://www.gharchive.org/#bigquery\" rel=\"noopener\">Google Big Query public dataset</a>, this tool scans GHArchive PushEvent data for zero-commit events, fetches the corresponding commits, and scans them for secrets using TruffleHog.&nbsp;</p><p>: We are releasing this tool to help blue teamers assess their potential exposure. Please use it responsibly.</p><p>Here‚Äôs a command to get started:</p><div><div><div><div><div><div><pre translate=\"no\"><code>. --- ///. -- &lt;/</code></pre></div></div></div></div></div></div><p>For this research, I used a custom version of our open-source tool to scan all of GitHub's  since 2020. And wow. There were lots of secrets!</p><h2>Hunting for Impactful Secrets</h2><p>After running the automation, I found thousands of active secrets. But how can I identify the most interesting secrets tied to the most impactful organizations? My three-step formula for success: manual search, a vibe-coded triage tool, and AI.</p><p>First, I manually explored and manipulated the data - essentially, got my hands dirty. The automation I built stores each newly discovered secret in a well-structured JSON file. Here's an example of what one of those files looks like:</p><p>During this stage, I manually looked over the files for interesting secrets. For example, I filtered out all commits made by authors with generic email addresses (e.g. <a href=\"https://gmail.com/\" rel=\"noopener\">gmail.com</a>, <a href=\"https://outlook.com/\" rel=\"noopener\">outlook.com</a>, <a href=\"http://mail.ru/\" rel=\"noopener\">mail.ru</a>, etc)&nbsp; and focused on commits pushed by authors with a corporate email. While not perfect, it was a good start, and I found some really impactful keys.</p><p>To understand the impact of specific tokens, I tried to figure out who owns the key and what access it has using open-source tools (e.g. <a href=\"https://github.com/NikhilPanwar/secrets-ninja\" rel=\"noopener\">secrets-ninja</a>) and a few custom scripts. During my research, I learned that the Truffle Security team launched an open-source tool to do just that - <a href=\"https://github.com/trufflesecurity/trufflehog?tab=readme-ov-file#mag-analyze\" rel=\"noopener\">TruffleHog Analyze</a>. It‚Äôs built into TruffleHog; you just have to run .&nbsp;</p><p>Note: I only did this additional secret enumeration when it was in-scope for specific Bug Bounty or Vulnerability Disclosure programs.</p><p>Once I found something relevant or interesting, I reported it via a bug-bounty program or directly via email.</p><h3>Vibe Coding for Secret Triage</h3><p>After a couple hundred manual checks, I had enough and decided to scale-up my secrets review. I used vercel v0 to vibe-code a whole platform for triaging these ‚ÄúOops Commit‚Äù secrets.&nbsp;</p><p>The platform was very simple. It was a front-end-only interface (no backend at all) that received a .zip file with JSON files created by the scanner. It then presented them in a very easy-to-use table so I could quickly review them and mark what I had already reviewed. This method proved very efficient, and I used a combination of filters to quickly find the hidden gems!</p><p>I also added some graphs and pie charts because why not? Looking at these graphs immediately revealed a few insights.</p><p>First, if you look at the time-series graph below, there‚Äôs clearly a direct correlation between the year and amount of  secrets - most likely because older secrets have already been revoked or expired - as they should!&nbsp;</p><p>Second, MongoDB secrets leaked the most. Based on my review of the data, this is because a lot of junior developers and CS students leaked mostly non-interesting side-project MongoDB credentials. The most interesting leaked secrets were GitHub PAT tokens and AWS credentials. These also generated the highest bounties!</p><p>Finally, I plotted the frequency of files leaking valid credentials, ahd the results are clear - your file needs extra protection!</p><p>Besides .env the most leaking filenames are: , , , , , , , , , , , , , , , , , , , , , , , ,, , , , , , , , , , , , , , , </p><p>I was quite satisfied with my vibe-coded secrets review platform. However, reviewing secrets is still a manual task. Ideally, the process should automatically resolve all secrets to extract basic information about the associated accounts wherever possible. This data could then be passed to a LLAMA-based agent that analyzes and identifies potentially valuable secrets. In essence, the goal is to build an offline agent capable of determining which secrets hold significance from a bug bounty or impact-driven perspective.</p><p>With the help of my friend <a href=\"https://il.linkedin.com/in/moti-harmats\" rel=\"noopener\">Moti Harmats</a>, I started working on it, but there‚Äôs still a lot more work to do, so I won‚Äôt release it at this time. But here‚Äôs a preview of what I started building:</p><h2>Case Study - Preventing a Massive Supply-Chain Compromise</h2><p>One of the secrets I found in a deleted commit was a <a href=\"https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\" rel=\"noopener\">GitHub Personal Access Token</a> (PAT) belonging to a developer. The developer accidentally leaked this secret when they committed their hidden configuration files (dot files). I analyzed this token and found it had admin access to ALL of <a href=\"https://github.com/istio/istio\" rel=\"noopener\">Istio</a> repositories.</p><p>Istio is an open-source service mesh that provides a transparent and language-independent way to flexibly and easily automate application network functions. It is designed to manage the communication between microservices in a distributed application, offering features such as traffic management, security, and observability without requiring changes to the application code.</p><p>The main <a href=\"https://github.com/istio/istio\" rel=\"noopener\">Istio</a> project has  stars and  forks. Istio is used by a wide range of organizations and teams that run complex, distributed applications, especially those adopting microservices architectures. This includes giant corporations like Google, IBM, Red Hat and many others.</p><p>And I had ADMIN level access to ALL of Itsio repositories (<a href=\"https://github.com/istio/\" rel=\"noopener\">there are many of them</a>). I could have read environment variables, changed pipelines, pushed code, created new releases, or even deleted the entire project. The potential for a mass supply-chain attack here was scary.&nbsp;</p><p>Fortunately, Istio has a well-maintained <a href=\"https://istio.io/latest/docs/releases/security-vulnerabilities/\" rel=\"noopener\">report page</a>, and the team acted quickly to revoke the GitHub PATs as soon as the issue was reported. Thank you!</p><p>This was a really fun project. I glued together some known discoveries and was able to create a reliable automation that scanned and found thousands of active secrets, even some that were buried for years. I also got the chance to vibe code a secret hunting platform with some nice features that allowed me to find needles in a haystack and earn approximately $25k of bounties and deep-thanks through the process.</p><p>The common assumption that deleting a commit is secure must change - once a secret is committed it should be considered compromised and must be revoked ASAP. It‚Äôs true for git blobs, git commits, and anything else that goes online.</p>","contentLength":18035,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lpun8i/security_researcher_earns_25k_by_finding_secrets/"},{"title":"10 features of D that I love","url":"https://bradley.chatha.dev/blog/dlang-propaganda/features-of-d-that-i-love/","date":1751456026,"author":"/u/BradleyChatha","guid":180660,"unread":true,"content":"<p>This is a beginner-friendly post exploring some of my favourite parts of the <a href=\"https://dlang.org/\">D programming language</a>, ranging from smaller quality of life stuff, to more major features.</p><p>I  talk much about D‚Äôs metaprogramming in this post as that topic basically requires its own dedicated feature list, but I still want to mention that D‚Äôs metaprogramming is world class - allowing a level of flexibility &amp; modelling power that few statically compiled languages are able to rival.</p><p>I‚Äôll be providing some minimal code snippets to demonstrate each feature, but this is by no means an in depth technical post, but more of an easy to read ‚Äúhuh, that‚Äôs neat/absolutely abhorrent!‚Äù sort of deal.</p><h2>Feature - Automatic constructors</h2><p>If you define a struct (by-value object) without an explicit constructor, the compiler will automatically generate one for you based on the lexical order of the struct‚Äôs fields.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Very handy for Plain Old Data types, especially with the semi-recent support for <a href=\"https://dlang.org/spec/expression.html#argument-parameter-matching\">named parameters</a>.</p><h2>Feature - Design by contract</h2><ul><li>‚Äúin‚Äù assertions to confirm that the function‚Äôs parameters are valid.</li><li>‚Äúout‚Äù assertions to confirm that the function‚Äôs return value is in a valid state.</li></ul><p>Additionally you can attach ‚Äúinvariants‚Äù onto structs and classes. Invariants are functions that run at the start and end of every  member function, and can be used to ensure that the type is always in a valid state.</p><p>Let‚Äôs start off with a contrived example of invariants:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Now let‚Äôs rewrite the above type to use ‚Äúin‚Äù contracts instead, with an extra function to show off ‚Äúout‚Äù contracts:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This can allow for an easy self-descriptive validation pattern for consumers/readers of your code, as well as an easy to implement self-checking mechanism for types that have complex internals.</p><p>Anecdotally I find this to be an underutilised feature of D, and it‚Äôs one I like to make use of a lot in my own code.</p><h2>Syntax - The dollar operator</h2><p>A lot of languages do not provide a shorthand syntax for referencing the length of an array, which can sometimes lead to awkward looking code when e.g. slicing arrays (any Go enjoyers here?).</p><p>D provides the dollar operator, which is a shorthand syntax for referencing the length of something.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Structs and classes can even <a href=\"https://dlang.org/spec/operatoroverloading.html#dollar\">overload</a> this operator.</p><p>D compilers provide an interpreter for the language which allows a very large amount of D code to be ran at compile time, as-is, without any special marking or other weirdness to go with it.</p><p>Generally, anywhere where the language requires a compile-time constant is a place where CTFE will transparently come into play.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This feature has a lot of different practical applications, and can allow for much cleaner, robust code than hardcoding precomputed values.</p><p>Since a lot of use cases relate to metaprogramming I‚Äôll leave the topic here, but CTFE is an extremely instant example of D‚Äôs unusual feature set.</p><h2>Feature - Built-in unittests</h2><p>D has direct support for defining unittests, and even allows you to override the built-in test runner for something more robust (such as with the <a href=\"https://code.dlang.org/packages/unit-threaded\">unit-threaded</a> library).</p><p>D code usually bundles unittests and normal code within the same file, rather than splitting them out into separate files as with most other languages:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This extremely low-friction barrier for writing tests is a godsend for motivating people to write even the most minimal of tests.</p><p>Of course if you have more complex needs then the option to have a proper testing framework + structure is still available to you, but the vast majority of D code I‚Äôve seen simply uses  blocks, optionally with a library that provides a better test runner.</p><h2>Feature - Exhaustive switch statements</h2><p>D provides a  statement which has an autogenerated  case that will immediately crash the program if its taken.</p><p>This allows you to define a switch that will always alert you if a new value needs to be added, or if an invalid value was somehow passed into it.</p><p>Additionally, if you use a  with an  value, then a compile-time check is triggered to ensure that every value within the  type has been declared, making it impossible to forget to add a new case when the enum is modified.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><h2>Syntax - Parenthesis omission</h2><p>D allows you to omit parentheses when calling functions in multiple contexts.</p><p>When calling a function with no parameters, you can omit them:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>(Marginally related) When calling a function with 1 parameter, you may use assignment syntax instead:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>When passing a single template parameter which consists of only 1 lexical token, you may omit the parenthesis:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This can do wonders for readability.</p><p>UFCS allows call chains to be ‚Äúinverted‚Äù by allowing freestanding functions to be used as if they were a member of their first parameter.</p><p>In other words:  can be rewritten as .</p><p>The two following snippets are completely equivalent in function, except the second snippet uses UFCS to provide a more clean look.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><h2>Feature - Scoped &amp; Selective Imports</h2><p>D supports limiting imports to a specific scope, whether that be a singular if-statement, an entire function, an entire struct/class, etc.</p><p>D will also allow you to selectively import symbols from other modules, instead of polluting your lookup scope with a ton of unrelated stuff - also helps increase comprehension of the codebase.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>While it may seem like clutter and extra effort, in the long run this allows for:</p><ol><li>Making it easy for newcomers to understand where certain functions are coming from.</li><li>Allows for code to become ‚Äúportable‚Äù between files since the code can carry most of its external dependencies inside of itself, making refactoring a bit easier.</li></ol><h2>Feature - Built-in documentation generator</h2><p>Finally, D has a built-in documentation generator with a relative standard, easy to read format.</p><p>There‚Äôs also a handful of documentation tools that are detached from the built-in one since the default generated output is a bit lacklustre ( I‚Äôm plugging my <a href=\"https://github.com/Juptune/marmos\">custom tool</a> here).</p><p>Here‚Äôs a relatively extreme example from one of my personal projects, to get an idea of the basic format:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Here‚Äôs an example from the standard library, which has minor usage of documentation macros:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>I tried to focus more on the more simpler day-to-day features, with only a splattering of the bigger more complicated stuff.</p><p>Hopefully this provides some insight on the wacky-yet-wonderful feature set that D provides.</p>","contentLength":6276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lpth7e/10_features_of_d_that_i_love/"},{"title":"AI girlfriends is really becoming a thing","url":"https://www.reddit.com/r/artificial/comments/1lpsts5/ai_girlfriends_is_really_becoming_a_thing/","date":1751453907,"author":"/u/Just-Grocery-2229","guid":180484,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"package-ui.nvim now supports multiple dependency managers, including Go module","url":"https://github.com/MonsieurTib/package-ui.nvim","date":1751452502,"author":"/u/TibFromParis","guid":180661,"unread":true,"content":"<p>I'm excited to share that package-ui.nvim has expanded its support to include a wide range of dependency managers. </p><p>Whether you're working with npm, pip, Cargo, Composer, Go modules, RubyGems, Mix, or Poetry, package-ui can now help you manage your project dependencies more efficiently.</p><p>Features include: - search for packages across supported managers - View available and installed packages - Inspect package details and versions - Install, update, or remove dependencies directly from the UI</p>","contentLength":493,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lpsg0h/packageuinvim_now_supports_multiple_dependency/"},{"title":"Anti-stale: A Go CLI tool to fight back against GitHub's stale bots","url":"https://github.com/KhashayarKhm/anti-stale","date":1751447963,"author":"/u/khashayar_khm","guid":180486,"unread":true,"content":"<p>Hey <a href=\"https://github.com/r/golang\">r/golang</a>! I built a CLI tool that automatically revives GitHub issues/PRs marked as \"stale\" before they get auto-closed. Would love to get your feedback if you're interested in checking it out!</p><p>Stale bots have become increasingly common, but they often do more harm than good: - They close legitimate bug reports that maintainers just haven't gotten to yet - They kill valuable feature discussions that are still relevant - They create busywork for contributors who have to \"bump\" issues manually - They can hurt project morale when contributors see their issues auto-closed</p><p>I found myself constantly having to comment \"still relevant\" on issues across different projects, so I decided to automate it.</p><p>anti-stale check --reply --interactive ```</p><p>go install github.com/KhashayarKhm/anti-stale@latest</p><h2>Configuration is straightforward</h2><p><code>json { \"token\": \"your_github_token\", \"userAgent\": \"your_username\", \"owners\": { \"golang\": { \"go\": { \"issues\": [12345, 67890] } } } } </code></p><p>I'm planning to add: - Support for multiple stale labels - Better GraphQL integration - Auto-reopening of recently closed issues - Custom messages per repository</p><p><strong>Would love to hear your thoughts!</strong> Have you dealt with aggressive stale bots? Any features you'd find useful? The codebase is pretty clean Go code, so contributions are very welcome.</p>","contentLength":1303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lpr8u2/antistale_a_go_cli_tool_to_fight_back_against/"},{"title":"Poor man's Backend-as-a-Service (BaaS) in 750 lines of code with zero dependencies","url":"https://github.com/zserge/pennybase","date":1751447485,"author":"/u/zserge","guid":180485,"unread":true,"content":"<div><p>Don't know why would anyone need it, but I've made a toy BaaS that supports:</p><ul><li>File-based storage using CSV files</li><li>Dynamic record schemas with validation</li><li>Uniform REST API with real-time SSE updates</li><li>Authentication and simple RBAC</li><li>Extensible with Hooks and Go tempaltes.</li></ul><p>Good enough to prototype a real-time chat app or a microblog engine. Not for production use, of course.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/zserge\"> /u/zserge </a>","contentLength":392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lpr4lz/poor_mans_backendasaservice_baas_in_750_lines_of/"},{"title":"Compute Freedom: Scale Your K8s GPU Cluster to 'Infinity' with Tailscale","url":"https://www.reddit.com/r/kubernetes/comments/1lpqmtw/compute_freedom_scale_your_k8s_gpu_cluster_to/","date":1751445487,"author":"/u/qingdi","guid":180373,"unread":true,"content":"<p>In today‚Äôs world, where the wave of artificial intelligence is sweeping the globe, GPU computing power is a key factor of production. However, a common pain point is that <strong>GPU resources are both scarce and expensive</strong>.</p><p>Take mainstream cloud providers as an example. Not only are GPU instances often hard to come by, but their prices are also prohibitive. Let‚Äôs look at a direct comparison:</p><ul><li>: The price of one H100 GPU is as high as .</li><li>: The price for equivalent computing power is only .</li><li><strong>Hyperstack / Voltage Park</strong>: The price is even as low as .</li></ul><p>The price difference is several times over! This leads to a core question:</p><p><strong>Can we design a solution that allows us to enjoy the low-cost GPUs from third-party providers while also reusing the mature and elastic infrastructure of cloud providers (such as managed K8s, object storage, load balancers, etc.)?</strong></p><p>The answer is yes. This article will detail a hybrid cloud solution based on  and  to cost-effectively build and scale your AI infrastructure.</p><p>A practical tutorial on how to extend GPU compute power at low cost using Tailscale and Kubernetes.</p><p>Learn to seamlessly integrate external GPUs into your K8s cluster, drastically cutting AI training expenses with a hybrid cloud setup.</p><p>Includes a guide to critical pitfalls like Cilium network policies and fwmark conflicts.</p>","contentLength":1307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"grep isn't what you think it means...","url":"https://youtu.be/iQZ81MbjKpU","date":1751445085,"author":"/u/MatchingTurret","guid":180379,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lpqje1/grep_isnt_what_you_think_it_means/"},{"title":"Exploiting the IKKO Activebuds \"AI powered\" earbuds, running DOOM, stealing their OpenAI API key and customer data","url":"https://blog.mgdproductions.com/ikko-activebuds/","date":1751444837,"author":"/u/Kok_Nikol","guid":180376,"unread":true,"content":"<p>So my journey with these earbuds started after I saw them on <a href=\"https://www.youtube.com/clip/UgkxPHxlV8Uo2L2k_v_RNloYS80CGql6CWH8?ref=blog.mgdproductions.com\" rel=\"noreferrer\">this</a> Mrwhosetheboss video about pointless tech. This device seems to be also popular on TikTok. My suspicions were confirmed, this runs android. So of course i went ahead and bought them.</p><p>245 euros later... and they finally arrived!</p><p>Before we dive further into this, unlike with <a href=\"https://x.com/MarcelD505/status/1785346490635878837?ref=blog.mgdproductions.com\" rel=\"noreferrer\">rabbit</a>, this issue has been properly reported and patched. This is also my first real blog post/disclosure so feedback is appreciated.</p><p>I like how they strapped a USB-c cable to the outside of the box while there is also a smaller one inside the box. They ran out of box space it seems...</p><p>I also wonder if they are legally allowed to use this OpenAI logo (probably not lol)</p><p>Anyways, we aren't here for fancy boxes, lets get to the main point. The device itself boots up to a screen with the time and ChatGPT front and center.</p><p>There are some other AI features available too like translations. But this isn't a review of the device, you can watch <a href=\"https://www.youtube.com/watch?v=p83t0qj9SFM&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">other YouTube videos</a> about that. The ChatGPT animation looks way too similar to the actual app and OpenAI could probably get them in legal trouble for stealing their brand identity. I will also mention that the audio quality is absolute shit if you use their EQ profiles but can be upped to a usable level by tweaking the EQ curves yourself.</p><p>There are also some apps available in the IKKO store, the reason that there is no google play store available is because these apps are modified specifically for the screen on the ActiveBuds, at least, that is what the CEO says about them. We will check that out in a bit. These apps include some music apps like Spotify, but also some gaming apps like, oh god, SUBWAY SURFERS BAYBEEEEE</p><p>Of course all of them unbearable to navigate due to the small screen. However we can now confirm that it most definitely runs android.</p><p>There is sadly no browser available to directly download other apps. And while you can open the native android settings app, clicking the build number 7 times does not enable developer mode. So i couldn't enable adb it seems. Is it locked that well? heh nope.</p><p>Let's just plug it into a pc and see what happens....</p><p>What the fuck, they left ADB enabled. Well, this makes it a lot easier. </p><p>After sideloading the obligatory DOOM, i began checking out how the ChatGPT integration works on the backend. I first started HTTP inspecting the device, however since i couldn't enable the proper system certificates without rooting the device, i couldn't see exactly to what URL it communicated. Fortunately that wasn't really needed.</p><p>Holy shit, holy shit, holy shit, it communicates DIRECTLY TO OPENAI. This means that a ChatGPT key must be present on the device!</p><p>I know that this device can be rooted to get the proper certificates installed because <a href=\"https://www.hovatek.com/forum/thread-32287.html?ref=blog.mgdproductions.com\" rel=\"noreferrer\">a tool</a> exists on all Spreadtrum/Unisoc devices which can be used to unlock the bootloader as long as companies use the default signing keys. This was indeed the case here too. However, i couldn't get past the confirmation screen as the device does not have a volume up key to confirm the unlock. I think you are able to sign your own partitions to make it flash them without an unlocked bootloader but that's a bit too advanced for my own liking.</p><p>So, i went back to the drawing board and just dumped all of the apps from it with an APK extractor tool. After popping the launcher app into JADX, things immediately became concerning.</p><p>The device can communicate to either of these domains.</p><ul><li>api.openai.com\nObvious, the OpenAI API</li><li>chat1.chat.iamjoy.cn\nSeems to be the API for the entire device, including features not related to ChatGPT like the app store. Loading it up in a browser gives a login page.</li><li>chat2.chat.iamjoy.cn\nSame thing as chat1, possibly a backup server?</li><li>openspeech.bytedance.com\nNo idea, might be a speech recogniser backup instead of whisper, haven't seen communication to this from the device.</li><li>www.airdimple.cn\nSeems like an OpenAI API mirror or proxy?</li></ul><p>Knowing this i went hunting for api endpoints and keys. I found a file called SecurityStringsAPI which contained encrypted endpoints and authentication keys. </p><p>You might think, hey that's just base64 idiot, the most basic encoding known to mankind. And well, yeah, it is.</p><p>However, there is a second stage which is handled by a native library which is obfuscated to hell. I am not going to even try to read that. Fortunately i didn't have to. I just sideloaded the app on a different device which was rooted, and well, <a href=\"https://x.com/MarcelD505/status/1785346490635878837?ref=blog.mgdproductions.com\" rel=\"noreferrer\">just like the rabbit apk</a>, it just works!</p><p>Yup, that's an OpenAI key.</p><p>Now, while having this access, we can also expose their (pretty funny) system prompt.</p><p>The device also has another few modes, which are Angry Dan and In-Love Dan. For the angry one you need to confirm you are 18+ because it actually swears a lot.</p><p>The system prompts for these are a bit more boring.</p><p>I also noticed that it logs the chat to another endpoint on the chat1 domain. This is probably just to keep a log of messages since the ChatGPT API does not allow that. Possibly for some Chinese espionage? Well, possibly but not entirely, we will get to that.</p><p>The headers for this request include the message, model, response and the device IMEI as the device id.</p><p>I also sideloaded the store app and found out that the apps seem to be mostly ripped straight from apkpure.com</p><p>After discovering this information, i sent an email to the security department of IKKObuds.</p><p>While waiting for their response i started to investigate their companion app. Wait i forgot to tell you about that? Yeah, these earbuds have a companion app with which you can also directly interface with ChatGPT and see your previous chats from the device. So that's what the logging endpoint is used for! You bind the app by <a href=\"https://youtu.be/IfyIV2oE-tE?feature=shared&amp;t=38&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">scanning a QR code</a> from the device in the \"Membership\" menu.</p><p>So, let's HTTP inspect this app and check out where it gets this information from.</p><p>Alright so it queries this API with your account token and your device id and returns all the chats you have ever had with the device. However, after removing the account token, the request still worked? So this api has no authentication apart from the device id. I feared the worst.</p><p>I found a frame in <a href=\"https://www.youtube.com/watch?v=IfyIV2oE-tE&amp;t=38s&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">the tutorial video</a> in which the device id wasn't properly blurred and plugged that into the api.</p><p>YUP, i now had their entire demo device chat history. And as the IMEI has a certain range, you would be able to figure out the chat history of all customers, which may include sensitive details.</p><p>I also added this new discovery to the email chain.</p><p>While that email was waiting for a reply i checked if i could fabricate a linking QR code from a known IMEI to bind the device. (The QR code is not the IMEI itself but something encrypted) I found the API endpoint by looking at the same SecurityStringsAPI, which was less secure than i initially thought because the variable names literally expose the encrypted api endpoints (lol)</p><p>Plugging in the getBindDevQrCode api in postman, i could fabricate a base64 image of the QR code with any IMEI.</p><p>However, using this QR code to try and bind the device to my app resulted in an error, saying that the device has already been bound to someone else. So that has been the only good security implementation up until now.</p><p>However, i lied, this is still a security/privacy issue. Why, you may ask? This exposes the username you set when creating the account for the app. However, there is no username field when creating your account. Only first and last name.</p><p>I created an account with the first name as \"Cheese2\" and the second name as \"Delight2\". Turns out that the username is equal to First name + Last name. When trying to bind that device to an app after it has already been bound to another app, the response includes the name \"Cheese2Delight2\". Great. Doxed.</p><p>So what we can do now is guess IMEI -&gt; generate QR code -&gt; Bind the device if not bound already, or get your full name when the device is already bound. -&gt; Get all your chat history either way if the device is bound or not.</p><p>There is an unbind_dev endpoint????</p><p>Unfortunately that one actually checks account token and does not allow to unbind a random device IMEI. Phew.</p><p>Hey, do you remember that logging endpoint that actually sent your chats you made with ChatGPT to their servers? This one?</p><p>Yeah, that also only used the device id as authentication, so we can send arbitrary text to the companion app of anyone....</p><p>I tried to send some HTML and JS through it to try and exploit the companion app, fortunately they use vue for their app and that has default HTML and JS injection security <a href=\"https://vuejs.org/guide/best-practices/security?ref=blog.mgdproductions.com#html-injection\" rel=\"noreferrer\">built in</a>. But we can still send scams or something to any user.</p><p>Oh hey a reply to my email!</p><p>First of all, from a gmail address? Come on, actually try to have at least some professionalism. Second, OK they are actually doing something about it. (The YouTube channel mentioned is because i said that i will be making a video about this. I have all the footage for it but i hate my voice with a passion so here we are on this blog post :))</p><p>Shortly after this email, they locked down the app and put out an announcement stating that the app will be in maintenance for a week.</p><p>They also wanted to become a sponsor of my empty YouTube channel? What? I don't think that they understood that i would be talking about their horrible security. Anyways.</p><p>The API was now non functional and displayed a maintenance message. After the service period they put out both an app update and a device update. What changed? The endpoint to get the chat history now needs a \"signature\" header. Which is composed of your account token, your device id, language and the current time encoded with a public/private key + a password. </p><p>Anyways, it is now impossible to fetch the chats without having a valid account token. Still doesn't fix the fact that i can generate a QR code with the guessable IMEI and bind the device to an app if it hasn't been bound already. That circumvents this all. </p><p>The device update broke the ChatGPT functionality from functioning on a device which is not the IkkoBuds itself. The keys remain on device and have not been rotated. So if anyone is able to figure out the broken app on another device or the key encryption system, you can still get your very own free OpenAI API key.</p><p>However i just gave up at this moment, also because they never replied with anything after my last email criticizing them for leaving the keys on device. This is now a month and a half ago.</p><p>So, that is it. You can still inject messages into apps of others, link devices that are not already bound to another companion app, thus leaking chat history. And leak first and last names of devices which are bound.</p><p>I am giving up, but if anyone else wants this company to fix this, be my guest.Also if you liked this deep dive, consider supporting me so i will be able to convince myself that buying more strange android devices is worth it lol<a href=\"https://ko-fi.com/mgdproductions?ref=blog.mgdproductions.com\">https://ko-fi.com/mgdproductions</a></p><p>I got this device rooted with help from <a href=\"https://x.com/haro7z?ref=blog.mgdproductions.com\">@haro7z</a></p><p>They are now checking the device's imei before it is able to use the chatgpt integration and are now using a proxy api instead of calling directly to openai. However this proxy api doesn't require any auth and only requires the User-Agent to be set to okhttp/4.9.0 LOL</p><p>They have also FINALLY rotated their old chatgpt api key!</p>","contentLength":11239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lpqh9c/exploiting_the_ikko_activebuds_ai_powered_earbuds/"},{"title":"[D] How to become fluent at modifying/designing/improving models?","url":"https://www.reddit.com/r/MachineLearning/comments/1lppyht/d_how_to_become_fluent_at/","date":1751442625,"author":"/u/total-expectation","guid":180798,"unread":true,"content":"<ol><li>Read a paper and and without much problem implement the techniques mentioned, whether it's building something from scratch using the paper as guidance (even in the absence of code), or modifying existing models.</li><li>Having an idea and being able to translate that into designing new architectures or modifying existing models.</li></ol><p>Think of people like <a href=\"https://github.com/lucidrains\">Phil Wang</a> who is very prolific at reproducing papers and or improving them. I'm very curious to know in your experience what made it \"click\" that unlocked your ability to be productive with these things. I suspect the boring answer is \"just reproduce papers, bro\", but I was hoping to learn about people's own experience/journey on this and if you guys have any specific insight/tricks that can be useful for others to know about. Like maybe you have a good workflow for this or a good pipeline that makes you 10x more productive, or you have some niche insight on designing/modifying/improving models that people don't usually talk about etc.</p>","contentLength":985,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] How will LLM companies deal with CloudFlare's anti-crawler protections, now turned on by default (opt-out)?","url":"https://www.reddit.com/r/MachineLearning/comments/1lppvk8/d_how_will_llm_companies_deal_with_cloudflares/","date":1751442290,"author":"/u/Endonium","guid":180483,"unread":true,"content":"<p>Yesterday, <a href=\"https://blog.cloudflare.com/content-independence-day-no-ai-crawl-without-compensation/\">Cloudflare had announced</a> that their protections against AI crawler bots will be turned on by default. Website owners can choose to opt out if they wish by charging AI companies for scraping their websites (\"pay per crawl\").</p><p>The era where AI companies simply recursively crawled websites with simple GET requests to extract data is over. Previously, AI companies simply disrespected robots.txt - but now that's not enough anymore.</p><p>Cloudflare's protections against crawler bots are now pretty sophisticated. They use generative AI to produce scientifically correct, but unrelated content to the website, in order to waste time and compute for the crawlers (\"<a href=\"https://blog.cloudflare.com/ai-labyrinth/\">AI Labyrinth</a>\"). This content is in pages that humans are not supposed to reach, but AI crawler bots should reach - invisible links with special CSS techniques (more sophisticated than ), for instance. These nonsense pages then contain links to other nonsense pages, many of them, to keep the crawler bots wasting time reading completely unrelated pages to the site itself and ingesting content they don't need.</p><p>Every possible way to overcome this, as I see it, would significantly increase costs compared to the simple HTTP GET request recursive crawling before. It seems like AI companies would need to employ a small LLM to check if the content is related to the site or not, which could be extremely expensive if we're talking about thousands of pages or more - would they need to feed every single one of them to the small LLM to make sure if it fits and isn't nonsense?</p><p>How will this arms race progress? Will it lead to a world where only the biggest AI players can afford to gather data, or will it force the industry towards more standardized \"pay-per-crawl\" agreements?</p>","contentLength":1740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes RKE Cluster Recovery","url":"https://www.reddit.com/r/kubernetes/comments/1lppqq0/kubernetes_rke_cluster_recovery/","date":1751441731,"author":"/u/Always_smile_student","guid":180374,"unread":true,"content":"<p>There is an RKE cluster with 6 nodes: 3 master nodes and 3 worker nodes.</p><p>Docker containers with RKE components were removed from one of the worker nodes.</p><p>How can they be restored?</p><p>kubectl get nodes -o wide</p><p>daf5a99691bf rancher/hyperkube:v1.26.6-rancher1 kube-proxy</p><p>daf3eb9dbc00 rancher/rke-tools:v0.1.89 nginx-proxy</p><p>2e99fa30d31b rancher/mirrored-pause:3.7 k8s_POD_coredns</p><p>5f63df24b87e rancher/mirrored-pause:3.7 k8s_POD_metrics-server</p><p>9825bada1a0b rancher/mirrored-pause:3.7 k8s_POD_rancher</p><p>93121bfde17d rancher/mirrored-pause:3.7 k8s_POD_fleet-controller</p><p>2834a48cd9d5 rancher/mirrored-pause:3.7 k8s_POD_fleet-agent</p><p>c8f0e21b3b6f rancher/nginx-ingress-controller k8s_controller_nginx-ingress-controller-wpwnk_ingress-nginx</p><p>a5161e1e39bd rancher/mirrored-flannel-flannel k8s_kube-flannel_canal-f586q_kube-system</p><p>36c4bfe8eb0e rancher/mirrored-pause:3.7 k8s_POD_nginx-ingress-controller-wpwnk_ingress-nginx</p><p>cdb2863fcb95 08616d26b8e7 k8s_calico-node_canal-f586q_kube-system</p><p>90c914dc9438 rancher/mirrored-pause:3.7 k8s_POD_canal-f586q_kube-system</p><p>c65b5ebc5771 rancher/hyperkube:v1.26.6-rancher1 kube-proxy</p><p>f8607c05b5ef rancher/hyperkube:v1.26.6-rancher1 kubelet</p><p>28f19464c733 rancher/rke-tools:v0.1.89 nginx-proxy</p>","contentLength":1187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"K3s or full Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1lposyz/k3s_or_full_kubernetes/","date":1751438040,"author":"/u/ReticularTen82","guid":180372,"unread":true,"content":"<p>So I just build a system on a supermicro x10dri. And I need help. Do I run K3S or full enterprise kubernetes?</p>","contentLength":109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Built Elasti ‚Äì a dead simple, open source low-latency way to scale K8s services to zero üöÄ","url":"https://www.reddit.com/r/kubernetes/comments/1lpluou/built_elasti_a_dead_simple_open_source_lowlatency/","date":1751427579,"author":"/u/ramantehlan","guid":179550,"unread":true,"content":"<p>We recently built <a href=\"https://github.com/truefoundry/elasti\"></a> ‚Äî a Kubernetes-native controller that gives your  true , without requiring major rewrites or platform buy-in.</p><p>If you‚Äôve ever felt the pain of idle pods consuming CPU, memory, or even licensing costs ‚Äî and your HPA or KEDA only scales down to 1 replica ‚Äî this is built for you.</p><p>Elasti adds a lightweight proxy + operator combo to your cluster. When traffic hits a scaled-down service, the proxy:</p><ul><li><strong>Forwards the request once the pod is ready</strong>.</li></ul><p>And when the pod is already running? The proxy just passes through ‚Äî  in the warm path.</p><p>It‚Äôs designed to be minimal, fast, and transparent. </p><ul><li><strong>Bursty or periodic workloads</strong>: APIs that spike during work hours, idle overnight.</li><li>: Tear everything down to zero and auto-spin-up on demand.</li><li>: Decrease infra costs by scaling unused tenants fully to zero.</li></ul><p>We did a deep dive comparing it with tools like Knative, KEDA, OpenFaaS, and Fission. Here's what stood out:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td align=\"left\"><strong>Works with any K8s Service</strong></td></tr><tr></tr><tr></tr></tbody></table><p>We kept things simple and focused:</p><ul><li> for now (TCP/gRPC planned).</li><li> metrics for triggers.</li><li><strong>Deployment &amp; Argo Rollouts only</strong> (extending support to other scalable objects).</li></ul><ul><li> CRD ‚Üí defines how the service scales</li><li>Elasti Proxy ‚Üí intercepts HTTP and buffers if needed</li><li>Resolver ‚Üí scales up and rewrites routing</li><li>Works with Kubernetes ‚â• 1.20, Prometheus, and optional KEDA for hybrid autoscaling</li></ul><p>More technical details in our blog:</p><ul><li> ‚Äî proxy just forwards.</li><li>: Helm + CRD, no big stack.</li><li> ‚Äî use your existing Deployments.</li></ul><p>If you're exploring serverless for <strong>existing Kubernetes services</strong> (not just functions), I‚Äôd love your thoughts:</p><ul><li>Does this solve something real for your team?</li><li>What limitations do you see today?</li><li>Anything you'd want supported next?</li></ul><p>Happy to chat, debate, and take ideas back into the roadmap.</p><p>‚Äî One of the engineers behind Elasti</p>","contentLength":1767,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is there a way to use strings.ReplaceAll but ignore terms with a certain prefix?","url":"https://www.reddit.com/r/golang/comments/1lpkpaj/is_there_a_way_to_use_stringsreplaceall_but/","date":1751423978,"author":"/u/god_gamer_9001","guid":180662,"unread":true,"content":"<p>For example, lets say I have the string <code>\"#number #number $number number &amp;number number #number\"</code>, and wanted to replace every \"number\" (no prefixes) with the string \"replaced\". I could do this through <code>strings.ReplaceAll(\"#number #number $number number &amp;number number #number\", \"number\", \"replaced\")</code>, but this would turn the string into <code>\"#replaced #replaced $replaced replaced &amp;replaced replaced #replaced\"</code>, when I would rather it just be <code>\"#number #number $number replaced &amp;number replaced #number\"</code>. Is there a way to go about this? I cannot just use spaces, as the example I'm really working with doesn't have them. I understand this is very hyper-specific and I apologize in advance. Any and all help would be appreciated. Thanks!</p>","contentLength":730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hidden complexity in software development","url":"https://purplesyringa.moe/blog/hidden-complexity-in-software-development/","date":1751421848,"author":"/u/imachug","guid":180909,"unread":true,"content":"<h2>Hidden complexity in software development</h2><a href=\"https://www.reddit.com/r/programming/comments/1lpk0fc/hidden_complexity_in_software_development/\"> Reddit</a><p>This is a tech phenomenon that I keep getting blindsided by no matter how much I try to anticipate it.</p><p>Physical work  difficult. You can look at someone and realize you don‚Äôt have nearly as much stamina, and even if you did, it still  demanding.</p><p>Research  difficult. You‚Äôre tasked with thinking about something no one else has considered yet. That rarely happens even outside of science ‚Äì try to tell a unique joke.</p><p>But non-algorithmic programming? You‚Äôre telling a machine that precisely follows instructions what you want it to do. At best, you‚Äôre a technical translator. You‚Äôre not working towards a PhD degree. You‚Äôre just wiring things together without creating anything intrinsically . It looks , and so it  easy.</p><p>Experience shows that it‚Äôs anything but easy, but it‚Äôs always been hard for me to pinpoint exactly why that is the case. And I think I‚Äôve finally found a good answer.</p><p>I‚Äôve recently started caring about <a href=\"https://github.com/iex-rs/lithium\">Lithium</a> again and did some work on rough edges.</p><p>At the API level, all Lithium does is provide  and  functions to simulate typed exceptions with panics or a more low-level mechanism. It‚Äôs not a good high-level construct, but it‚Äôs a useful tool nevertheless.</p><p>As a prototype, it could be implemented in 50 lines max. Obviously, optimizing for performance increases the LoC count significantly, but it still seems like it ought to be manageable.</p><p>But there‚Äôs 200 commits in this repo. There‚Äôs a ton of breakage and various small issues that crop up and suddenly I can‚Äôt just say this project is finished, forget about it, and use it as foundation for the next one.</p><p>Lithium relies on some low-level rustc mechanisms and nightly features, so it needs a CI to let me quickly react to things changing. I automatically <a href=\"https://github.com/iex-rs/lithium/blob/147520243e1eb9e2679e7ed612254055bd9c7c09/.github/workflows/ci.yml#L7\">run a CI job</a> every week, but I‚Äôm thinking about increasing the rate because I‚Äôm uncomfortable with breakage being found a bit later than I‚Äôd like.</p><p>You might think that breakage mostly happens due to changes to nightly features, but that isn‚Äôt the case. Lithium has quite a bit of platform-specific code, so I run CI on many targets. And oh boy, do they work .</p><ul><li>Windows <a href=\"http://www.emulators.com/docs/abc_arm64ec_explained.htm\">arm64ec</a> oscillates between working and breaking spectacularly. That‚Äôs surprising for a Tier 2 target. When I added CI, I stumbled upon <a href=\"https://github.com/rust-lang/rust/issues/138541\">a certain issue</a> that was fixed just a week ago, and less than a week later <a href=\"https://github.com/rust-lang/rust/issues/143253\">it broke again</a>.</li><li>Wasm exception handling support is‚Ä¶ spotty. I‚Äôll need to see if things have changed, but there have been <a href=\"https://github.com/rust-lang/rust/issues/132416\">several</a> different <a href=\"https://github.com/rust-lang/rust/issues/135665\">issues</a>, which I‚Äôve <a href=\"https://github.com/rust-lang/rust/pull/136200\">worked on for a bit</a>, but then I realized quite a few UI tests <a href=\"https://github.com/rust-lang/rust/pull/136199\">fail on Emscripten</a> and it‚Äôs all a bit too overwhelming for me to resolve in a structured manner.</li><li>Even on x86_64, LLVM <a href=\"https://github.com/llvm/llvm-project/issues/112943\">miscompiles</a> unwinding from a function that uses a foreign ABI (e.g. when an MS ABI function is invoked from a GNU ABI function).</li><li>Oh, and if that wasn‚Äôt enough, there‚Äôs also <a href=\"https://bugs.winehq.org/show_bug.cgi?id=57700\">a bug in Wine</a> that causes code compiled for  to have unaligned thread locals. There‚Äôs <a href=\"https://gitlab.winehq.org/wine/wine/-/merge_requests/7251\">an unmerged fix</a>. Also, <a href=\"https://github.com/rust-lang/rust/issues/135717\">just hangs</a> on  under Wine, but I can‚Äôt even tell if it‚Äôs a Rust bug or a Wine bug.</li></ul><p>So ultimately, the main reason Lithium is so unstable is external design deficiencies and bugs. It‚Äôs logically simple, but the lack of a reliable foundation forces me to use hacks or abandon otherwise good approaches.</p><p>Or I can desperately try to fix upstream. I can patch rustc, I can maybe touch unwinders and Cranelift. LLVM is where I draw the line, and it looks like that‚Äôs just not enough.</p><p>This fragility shows up anywhere you look. A significant chunk of complexity comes from setting up CI. You‚Äôd think people had perfected this by now, but apparently not:</p><ul><li>rustc does not support the  target.  and <code>aarch64-pc-windows-gnullvm</code> are both supported. I don‚Äôt know why.</li><li>Wine <a href=\"https://bugs.winehq.org/show_bug.cgi?id=58092\">doesn‚Äôt support</a> the arm64ec target out of the box yet, which means I can‚Äôt test it on CI and have to trust that running code on real Windows will catch all the bugs.</li><li><a href=\"https://github.com/cross-rs/cross\">Cross</a>, the Cargo wrapper for cross-platform building and testing via emulators like qemu, has odd bugs I don‚Äôt even know how to describe.</li><li>There don‚Äôt seem to be existing tools for automatically running tests under WASI or cross-compiling tests (which is necessary because I don‚Äôt want to run the whole compiler suite under Wine, just the tests themselves), so I had to <a href=\"https://github.com/iex-rs/lithium/tree/ad96472cecccb15fc6b2ba8639f37030d0796e69/ci\">make my own</a>.</li><li><code>cargo test --target &lt;target&gt;</code> has skipped over doctests without me knowing, and then started running them after an update, and that‚Äôs revealed problems like having to add rustc flags to both  and .</li></ul><p>In fact, I think that nightly-only and internal compiler features are  of my worries. They  just work, yet they absolutely do. Yes, there‚Äôs some code smell like <a href=\"https://github.com/iex-rs/lithium/blob/ad96472cecccb15fc6b2ba8639f37030d0796e69/src/backend/panic.rs#L60\">having to special-case Miri</a>, and I‚Äôm not  about relying on std or rustc internals, but them being less of a problem than anything else is telling.</p><p>It‚Äôs infuriating that the tools that are supposed to help us lead us to our demise. And I think this is a common trope in software development.</p><p>If you need to solve a complex algorithmic problem, or if you need to optimize a program, you can usually do that. It might be tricky, you might need to research something or write ugly  code, but then you implement it and it . You write a clever pile of code and it‚Äôs , until the requirements change significantly.</p><p>But reliable products are more than code snippets. They always make assumptions, like a frontend developer assuming the JavaScript runtime works correctly, a Rust programmer trusting LLVM, or the Docker runtime trusting Linux not to be stupid. And if this trust fails, you start to lose your sanity. Nothing you can do is  to solve the issue. It‚Äôs a vibes thing, it‚Äôs a ‚Äújust ship it and hotfix if something breaks‚Äù world, it‚Äôs pure madness.</p><p>And it‚Äôs . Only for societal reasons, only because someone didn‚Äôt consider an edge case somewhere and now fixing that requires more effort than anyone wants to invest, but that doesn‚Äôt make it any less real. It sucks, and it shouldn‚Äôt have happened, and it wouldn‚Äôt if we didn‚Äôt subscribe to the bollocks ‚Äúworse is better‚Äù ideology, but now we have to live with the consequences.</p><p>Computing is now obscure, unreliable sorcery. To program is to harness this inscrutable magic. And when you reframe it this way, it finally feels difficult.</p>","contentLength":6312,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lpk0fc/hidden_complexity_in_software_development/"},{"title":"Rust's C Dynamic Libs and static deallocation","url":"https://www.reddit.com/r/rust/comments/1lpj6t8/rusts_c_dynamic_libs_and_static_deallocation/","date":1751419391,"author":"/u/Sylbeth04","guid":180908,"unread":true,"content":"<p>It is about my first time having to make dynamic libraries in Rust, and I have some questions about this subject.</p><p>So, let's say I have a static as follows: <code>rust static MY_STATIC: Mutex&lt;String&gt; = Mutex::new(String::new()); </code></p><p>Afaik, this static is never dropped in a pure rust binary, since it must outlive the program and it's deallocated by the system when the program terminates, so no memory leaks.</p><p>But what happens in a dynamic library? Does that happen the same way once it's unloaded? Afaik the original program is still running and the drops are never run. I have skimmed through the internet and found that in C++, for example, destructors are called in DLLMain, so no memory leaks there. When targeting a C dynamic library, does the same happen for Rust statics?</p><p>How can I make sure after mutating that string buffer and thus memory being allocated for it, I can destroy it and unload the library safely?</p>","contentLength":907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How loosely coupled should I make my code???","url":"https://www.reddit.com/r/golang/comments/1lpirr4/how_loosely_coupled_should_i_make_my_code/","date":1751418149,"author":"/u/ShookethThySpear","guid":180377,"unread":true,"content":"<p>I am a relatively new Go developer so I'm still working my way around Go coding and best practices in Go development. I am currently creating a microservice for personal use now my question is that how loosely coupled do you guys make your code? I am currently using multiple external libraries one of which is widely used in my microservice. I used it widely due to the fact that the struct included in the package is massive and it contains many more nested structs of everything I need. I was thinking of decoupling code from 3rd party packages and also trying out dependency injection manually through interfaces and main() instantiation, but my worry is if I were to create an interface that my services can depend on, I have to create my own struct similar to the one provided by that 3rd party package just for the sake of abstraction.</p>","contentLength":842,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bazzite Linux Flare Request","url":"https://www.reddit.com/r/linux/comments/1lpi832/bazzite_linux_flare_request/","date":1751416528,"author":"/u/ATShields934","guid":180487,"unread":true,"content":"<p>Why does Pop!_OS get two flare icons (Pop!_OS and System 76 logo) but Bazzite and Fedora Silver blue don't get any love?</p><p>I'd like to request proper flare representation for my distro of choice!</p>","contentLength":192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Request for Career Advice ‚Äì ML PhD non hot topic","url":"https://www.reddit.com/r/MachineLearning/comments/1lphfhf/d_request_for_career_advice_ml_phd_non_hot_topic/","date":1751414233,"author":"/u/Hope999991","guid":179552,"unread":true,"content":"<p>I‚Äôm currently a PhD student in Machine Learning, working on a research topic that isn‚Äôt considered ‚Äúhot‚Äù in the current academic or industrial landscape. Despite this, I‚Äôve managed to publish as the lead author at ICML, NeurIPS. And twice at ECML. I also have two co-authored publications at ECAI.</p><p>I‚Äôve noticed that many PhD students in the U.S. seem to have much stronger publication records, often in trendier areas. This makes me question how competitive I really am in the current job market‚Äîespecially given the wave of layoffs and increasing demand for very specialized expertise in industry.</p><p>That said, I do have a strong foundation in core ML, Deep Learning, and LLMs (although LLMS aren‚Äôt the direct focus of my PhD research).</p><p>Given all of this, I‚Äôm trying to realistically assess: ‚Ä¢ What are my current chances of landing a demanding, high-quality job in industry or research after my PhD? ‚Ä¢ What could I do now to improve those chances? ‚Ä¢ Goal is FANNG.</p><p>I‚Äôd greatly appreciate any feedback.</p><p>Edit: My research focuses on anomaly detection, a less trendy area compared to the current popularity of large language models and reinforcement learning.</p>","contentLength":1177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"6 months ago didn't know how to code, now I launched my first app that actually has users","url":"https://www.reddit.com/r/artificial/comments/1lph92p/6_months_ago_didnt_know_how_to_code_now_i/","date":1751413751,"author":"/u/Sad_Mathematician95","guid":179553,"unread":true,"content":"<p>Kinda wild to see how far you can take the use of AI</p><p>A fully functional Photo restoration app that has a Gallery feature with sorting tools like folders and tags, Family tree builder and more!</p><p>If anyone is curious to try it's free!</p>","contentLength":229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you ship go?","url":"https://www.reddit.com/r/golang/comments/1lpgkn2/how_do_you_ship_go/","date":1751411892,"author":"/u/itsabdur_rahman","guid":179554,"unread":true,"content":"<p>I created a todo list app to learn go web development. I'm currently using templ, htmx, alpine and tailwind. Building the app was a breeze once I got used to the go sytanx and it's been fun.</p><p>After completing the app I decided to make a docker container for it, So it can run anywhere without hassle. Now the problem starts. I made a container as folows:</p><pre><code>FROM golang:1.24.4 WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . # Install tools RUN curl -L -o /usr/local/bin/tailwindcss https://github.com/tailwindlabs/tailwindcss/releases/latest/download/tailwindcss-linux-x64 &amp;&amp; chmod +x /usr/local/bin/tailwindcss RUN go install github.com/a-h/templ/cmd/templ@latest RUN go install github.com/sqlc-dev/sqlc/cmd/sqlc@latest # Produce Binary RUN tailwindcss -i ./static/css/input.css -o ./static/css/style.min.css RUN templ generate RUN sqlc --file ./internal/db/config/sqlc.yaml generate RUN go build -o /usr/local/bin/app ./cmd CMD [ \"app\" ] </code></pre><p>The problem I see here is that the build times are a lot longer none of the intall tool commands are cached (There is probably a way but I don't know yet). The produced go binary comes out to be just about 15 mb but we can see here that the containers are too big for such a small task</p><pre><code>$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE todo-app latest 92322069832a 2 minutes ago 2.42GB postgres 16-alpine d60bd50d7e2d 3 weeks ago 276MB </code></pre><p>I was considering shipping just the binary but that requires postgres so I bundle both postgres and my app to run using docker compose. There has to be a way to build and ship faster. Hence why I'm here. I know go-alpine has a smaller size that still wouldn't justify a binary as small as 15 mb</p><p>How do you guys ship go web applications. Whether it is just static sties of with the gothh stack.</p><p>Thank you everyone for replying giving amazing advice. I created a very minimalist multi-stage build process suggested by many people here.</p><pre><code>FROM scratch AS production COPY --from=builder /build/app / CMD [ \"/app\" ] </code></pre><p>I tried both  and  for the final image and the results are not what I expected:</p><pre><code>$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE todo-app-alpine latest e0f9a0767b87 11 minutes ago 15.1MB todo-app-scratch latest e0f9a0767b87 11 minutes ago 15.1MB </code></pre><p>I was expecting scratch be the bare minimum. However this is amazing because my image size went for 2.4 GB to 15mb that's incredible. Thanks to <a href=\"https://www.reddit.com/u/jefftee_\">/u/jefftee_</a> for suggesting mutlti-stage. Your commend thread helped me a lot. </p><p>Another change I made was to move  just before the production lines which now let's docker cache the tool installations making production faster. Thanks to <a href=\"https://www.reddit.com/u/BrenekH\">/u/BrenekH</a> in the comments for this tip.</p>","contentLength":2662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RFK Jr. Says AI Will Approve New Drugs at FDA 'Very, Very Quickly. \"We need to stop trusting the experts,\" Kennedy told Tucker Carlson.","url":"https://gizmodo.com/rfk-jr-says-ai-will-approve-new-drugs-at-fda-very-very-quickly-2000622778","date":1751410286,"author":"/u/esporx","guid":179393,"unread":true,"content":"<p>Robert F. Kennedy Jr. appeared on the latest episode of Tucker Carlson‚Äôs podcast on Monday and it‚Äôs filled with the ramblings of a man completely detached from reality. Kennedy <a href=\"https://gizmodo.com/cdc-to-re-investigate-vaccines-and-autism-despite-decades-of-evidence-showing-no-link-2000573773\">falsely</a> suggested vaccines cause autism, more or less endorsed the idea that Anthony Fauci should go to prison, and says that AI will allow the FDA to approve new drugs very quickly. It‚Äôs quite a mess.</p><p>These absolutely unhinged ideas wouldn‚Äôt be such a problem if this were any other fringe lunatic appearing on the podcast of a racist former Fox News host. But Kennedy happens to be the Secretary of Health and Human Services, a man who‚Äôs been given enormous power over America‚Äôs entire healthcare system thanks to President Donald Trump.</p><p>One of the most troubling moments in the new interview comes when Kennedy discusses the role that artificial intelligence is going to play in replacing or altering the VAERS system, which stands for Vaccine Adverse Event Reporting System. VAERS allows doctors to report incidents when they believe a patient has been harmed by vaccines, but Kennedy isn‚Äôt happy with it. The secretary <a href=\"https://youtu.be/Jx9pS1kFCrM\">insists</a> it was ‚Äúdesigned to fail,‚Äù suggesting it‚Äôs not registering enough people who in his mind have been harmed by vaccines over the years.</p><p>‚ÄúWe‚Äôre going to absolutely change VAERS and we‚Äôre going to make it, we‚Äôre going to create either within VAERS or supplementary to VAERS, a system that actually works,‚Äù Kennedy said. ‚ÄúAnd, you know, right now, even that system is antiquated because we have access to AI.‚Äù</p><p>Kennedy told Carlson he was creating an ‚ÄúAI revolution‚Äù at the Department of Health and Human Services and was attracting the top people from Silicon Valley who ‚Äúwalked away from billion dollar businesses.‚Äù But Kennedy says these people don‚Äôt want prestige or power, they just want to make the healthcare system better.</p><p>‚ÄúWe are at the cutting edge of AI,‚Äù Kennedy said. ‚ÄúWe‚Äôre implementing it in all of our departments. At FDA, we‚Äôre accelerating drug approvals so that you don‚Äôt need to use primates or even animal models. You can do the drug approvals very, very quickly with AI.‚Äù</p><p>Kennedy has previously talked about using AI to <a href=\"https://gizmodo.com/after-slashing-thousands-of-jobs-trumps-fda-wants-to-use-ai-to-approve-new-drugs-more-quickly-2000614144\">increase efficiency</a> at FDA but hasn‚Äôt provided details about what AI tools will be used and how they would be used to approve new drugs. But given generative AI‚Äôs instability and propensity for failing at some of the most basic tasks, the idea of putting drug approvals in the hands of robots is pretty terrifying.</p><p>Kennedy, who was the founder of an anti-vaccine group called the Children‚Äôs Health Defense, says repeatedly during the interview that vaccines have never been properly studied, which is just a flat-out lie. But he now has the power to demand investigations into vaccines that will get him the results he wants, no matter how much he insists his own opinion doesn‚Äôt matter.</p><p>‚ÄúWe need to stop trusting the experts, right?‚Äù Kennedy told Carlson. ‚ÄúWe were told at the beginning of COVID, don‚Äôt look at any data yourself, don‚Äôt do any investigation yourself, just trust the experts. And trusting the experts is not a feature of science, it‚Äôs not a feature of democracy, it‚Äôs a feature of religion, and it‚Äôs a feature of totalitarianism.‚Äù</p><p>Kennedy went on to insist that it was important for everyone to ‚Äúdo your own research,‚Äù a common refrain among those in the so-called Make America Healthy Again movement. But Kennedy is intentionally misrepresenting the role of experts in an informed society. Listening to experts isn‚Äôt about abandoning all critical thinking. It‚Äôs about recognizing that there are areas where you may not have expertise and taking the opinions of medical professionals more seriously than random people on shows like Joe Rogan and Tucker Carlson who are just self-proclaimed experts.</p><p>Kennedy was asked several leading questions from Carlson, including whether the covid-19 vaccine has killed more people than it saved. And Kennedy is skilled enough as a communicator (his father was Attorney General during his uncle‚Äôs presidency, as he frequently mentions) that he can avoid directly answering in the affirmative while subtly telling you that he believes it‚Äôs the case.</p><p>Notice, for instance, how Kennedy initially responds to Carlson‚Äôs question while eventually working his way to sowing doubt about trust in vaccines.</p><blockquote><p> Do you think overall the COVID vaccine killed more than it saved?</p><p> My opinion about that is irrelevant. What we‚Äôre going to try to do is make that science available so the public can look at the science.</p><p> And I would not say one way or the other. And the truth is, I don‚Äôt know. And the reason I don‚Äôt know is because the studies that were done by my agency were substandard. And they were not designed to answer that question. And there‚Äôs been a lot of obfuscation about covering up, as you know, about suppressing any kind of discussion of vaccine injuries.</p></blockquote><p>Kennedy is often effective at manipulating an audience, but also says things that don‚Äôt make any sense, even if you agree with his worldview. At one point during his interview with Carlson he said that when Pfizer‚Äôs covid-19 vaccine was studied there were two people who died in the control group and one person who died in the vaccine group.</p><p>‚ÄúYou remember they were saying the vaccine is 100% effective? Well, that‚Äôs why they were saying it because there was‚Ä¶ there was‚Ä¶ two is 100% of one,‚Äù Kennedy said.</p><p>That‚Äôs not how anyone is measuring the efficacy of vaccines. Yes, some of the early studies were admittedly too rosy in their projections, especially those in early 2021 as the vaccines were first released. But nobody was claiming that two people dying in a control group and one person dying in the vaccine group showed the vaccine was 100% effective. That math isn‚Äôt anything that was actually presented in any study Gizmodo is aware of.</p><p>Kennedy was also asked about whether Anthony Fauci, the nation‚Äôs most visible public health expert during the covid-19 pandemic, would be prosecuted for some unspecified crimes. Again, the secretary danced around a bit with his language but then heavily suggested Fauci should be tried for criminal acts. Kennedy said there should be some kind of ‚Äútruth commission‚Äù for covid-19 vaccines like the truth and reconciliation commissions in South Africa and Central America in the 20th century under repressive governments.</p><p>‚ÄúAnybody who comes and volunteers to testify truthfully is then given immunity from prosecution. And, but, so that at least the public knows who did what,‚Äù Kennedy said. ‚ÄúAnd people who are called and don‚Äôt take that deal and purge themselves, they then can be, they can be prosecuted criminally.‚Äù</p><p>Kennedy believes that Fauci was involved in some kind of weaponization of covid-19 and in cahoots with the Chinese government. ‚ÄúI think he had a lot of liability on creating coronavirus,‚Äù Kennedy said. ‚ÄúYou know, he was funding precisely that research at the Wuhan lab. And he was giving them the technology.‚Äù</p><p>When Kennedy notes that Fauci no longer has protection from the Secret Service since President Trump withdrew it, Carlson responds ‚Äúgood.‚Äù Fauci received countless death threats from lunatics over the years.</p><p>Kennedy didn‚Äôt really get into the spiritual side of his MAHA movement during his latest interview, something that‚Äôs previously been top of mind. In fact, Kennedy was very focused on the role of a higher power when he last appeared on Carlson‚Äôs show back in August 2024, shortly after abandoning his own bid for president. </p><p>Casey Means, Kennedy‚Äôs pick to be Surgeon General, has also appeared on podcasts like Joe Rogan to spout many of the same <a href=\"https://gizmodo.com/maga-crackpots-turn-on-trumps-crackpot-surgeon-general-nominee-2000599701\">crazy talking points</a> and emphasize how important spirituality is for health. But it remains to be seen whether Means will be confirmed by the U.S. Senate. Kennedy recently said he‚Äôs going to push for all Americans to get a <a href=\"https://gizmodo.com/rfk-jr-wants-every-american-to-be-sporting-a-wearable-within-four-years-2000619672\">wearable device</a> to monitor their health, and as luck would have it, Means sells a wearable for monitoring glucose. The device is targeted at consumers who aren‚Äôt even diabetic, the people who do actually need glucose monitoring.</p><p>The entire episode of Tucker Carlson is available on <a href=\"https://youtu.be/w_fzlwxJZAA\">YouTube</a> but it‚Äôs a frustrating thing to sit through for any halfway intelligent person. At one point, Kennedy insists Trump is a smart guy, calling him ‚Äúimmensely knowledgeable‚Äù and ‚Äúencyclopedic in certain areas.‚Äù Kennedy even referred to Trump as ‚Äúone of the most empathetic people that I‚Äôve ever met.‚Äù The only point where Kennedy seems to disagree with Trump is on tariffs, with the secretary saying that ‚Äúbusinesses are hurting because of the tariffs.‚Äù But it‚Äôs the kind of quick dissent that will likely go unnoticed given how Kennedy praises the fascist president incessantly throughout.</p><p>You‚Äôve been warned. Listen at the risk of your own sanity.</p><p><em> An earlier version of this article incorrectly stated that Kennedy had founded a group called the Children‚Äôs Defense Fund. The group started by Kennedy is called Children‚Äôs Health Defense. Our sincerest apologies to the Children‚Äôs Defense Fund, which is not against childhood vaccinations.</em></p>","contentLength":9203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lpfzgx/rfk_jr_says_ai_will_approve_new_drugs_at_fda_very/"},{"title":"Any deaf Linux users here?","url":"https://www.reddit.com/r/linux/comments/1lpfpf5/any_deaf_linux_users_here/","date":1751409548,"author":"/u/Macdaddyaz_24","guid":179422,"unread":true,"content":"<p>Who here is Deaf? Been wanting to create a deaf only Linux user subreddit. Please comment here if you‚Äôre deaf and use linux, plus interested in creating a deaf Linux subreddit. This way we can work with like minded users :)</p>","contentLength":225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux breaks through 5% share in USA desktop OS market (Statcounter)","url":"https://www.reddit.com/r/linux/comments/1lpepvq/linux_breaks_through_5_share_in_usa_desktop_os/","date":1751407028,"author":"/u/MrHighStreetRoad","guid":179363,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/MrHighStreetRoad\"> /u/MrHighStreetRoad </a>","contentLength":39,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I've been writing Rust for 5 years and I still just .clone() everything until it compiles","url":"https://www.reddit.com/r/rust/comments/1lpc85o/ive_been_writing_rust_for_5_years_and_i_still/","date":1751401062,"author":"/u/kruseragnar","guid":179266,"unread":true,"content":"<p>That's it. That's the post. Then I go back and fix it later. Sometimes I don't.</p>","contentLength":79,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"i made csv-parser 1.3x faster (sometimes)","url":"https://blog.jonaylor.com/i-made-csv-parser-13x-faster-sometimes","date":1751400698,"author":"/u/ProGloriaRomae","guid":180479,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lpc2i0/i_made_csvparser_13x_faster_sometimes/"},{"title":"Tips for landing a Rust job","url":"https://www.reddit.com/r/rust/comments/1lpbdjy/tips_for_landing_a_rust_job/","date":1751399065,"author":"/u/PuzzleheadedAd9587","guid":180907,"unread":true,"content":"<p>Hi there! I've been looking for an opportunity to get a Rust developer job for the past 8 months, but I have to say it‚Äôs far from easy. Usually, all Rust job openings require at least 3 years of professional experience (in my case, I‚Äôve used Rust for only 6 months professionally, plus 18 months on academic and side projects). Unfortunately, there are barely any positions for less experienced Rustaceans, which creates a vicious circle: I can‚Äôt get a Rust job because I don‚Äôt have experience, and I can‚Äôt get experience because I don‚Äôt have a Rust job.</p><p>What would be your advice for increasing the chances of getting hired for a Rust position while not having much professional experience with this awesome programming language?</p>","contentLength":741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Libreboot 25.06 released","url":"https://libreboot.org/news/libreboot2506.html","date":1751398959,"author":"/u/libreleah","guid":179394,"unread":true,"content":"<p>Article published by: Leah Rowe</p><p>Date of publication: 30 June 2025</p><p>There  a Libreboot 25.04 release in April 2025, but that is retroactively regarded as an RC of 25.06. The original 25.06 release announcement showed changes since 25.04, but the changelog is now relative to December 2024. This reflects the <a href=\"https://libreboot.org/news/revisions.html\">revised release schedule</a>. It means that the changelog is much bigger, and also includes the changes that went in Libreboot 25.04.</p><p>Today‚Äôs Libreboot 25.06 revision is a , whereas the previous stable release was Libreboot 20241206. This revised release log lists all changes as of today, 30 June 2025, since the Libreboot 20241206 release of December 2024.</p><div><h2>Open source BIOS/UEFI firmware</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot2506.html#open-source-biosuefi-firmware\">[link]</a></div><p>Libreboot is a free/open source BIOS/UEFI replacement on x86 and ARM, providing boot firmware that initialises the hardware in your computer, to then load an operating system (e.g.&nbsp;Linux/BSD). It is specifically a , in the same way that Debian is a Linux distribution. It provides an automated build system to produce coreboot ROM images with a variety of payloads such as GRUB or SeaBIOS, with regular well-tested releases to make coreboot as easy to use as possible for non-technical users. From a project management perspective, this works in  the same way as a Linux distro, providing a source-based package manager (called lbmk) which patches sources and compiles coreboot images. It makes use of <a href=\"https://www.coreboot.org/\">coreboot</a> for hardware initialisation, and then a payload such as <a href=\"https://www.seabios.org/SeaBIOS\">SeaBIOS</a> or GRUB to boot your operating system; on ARM(chromebooks) and certain x86 mainboards, we provide  (as a coreboot payload), which provides a lightweight UEFI implementation..</p><div><h2>Sumarised list of changes</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot2506.html#sumarised-list-of-changes\">[link]</a></div><p>This section provides a brief overview, summarising all of the changes. The next sections (after this) show  changes in detail.</p><p>The most important changes are, thus:</p><ul><li>Acer Q45T-AM support added (similar to G43T-AM3 mainboard)</li><li>Dell Precision T1700 SFF and MT</li></ul><ul><li>GRUB, SeaBIOS, Untitled, flashprog, U-Boot, uefitool have all been updated to newer revisions, from ~April 2025.</li></ul><ul><li>GRUB has has  of security fixes applied to it from upstream, including a very large series of  major security fixes, and a few minor tweaks after the fact.</li><li>Globbing issues fixed in the Libreboot build system, lbmk. Better error handling in general.</li><li>ThinkPad T480/3050micro: Disable hyperthreading by default</li></ul><ul><li>Better, more reliable caching of Git repositories and files during download. Re-builds of sources make better use of local caching, instead of downloading from scratch every time (e.g.&nbsp;coreboot and GRUB sources).</li><li>Handling of vendor files is more reliable, caching everything more aggressively and even verifying checksums of  files, from inside update archives.</li><li>Non-root USB hub support added to GRUB, for xHCI devices</li><li>GRUB: better LVM scanning, for auto-boot especially with encrypted ; Linux distros are easier to handle, in general.</li><li>Safer handling of vendor files; release images padded to prevent flashing, where such files are needed, until they have been inserted.</li><li>Better MAC address handling, on IFD-based systems. Insertion of MAC addresses is done by default, randomised by default, unless overridden by the user.</li></ul><ul><li>Removed unnecessary sources by default, to make source archives smaller. Only the sources needed to build the binaries are included, in many cases.</li><li>Updated various dependencies configs, for installing build dependencies in various Linux distros (for the  command which installs them in your distro).</li></ul><ul><li>Better checksum verification for project files, when deciding whether to re-build a given upstream source.</li><li>General build system fixes, making the build process more reliable, with much stricter error handling (and some false error conditions have also been removed).</li><li>GRUB payload: Mark E820 reserved for cbmem, which means that you no longer need  (kernel option) at boot time, to access the cbmem console.</li><li>Use  instead of  in nvmutil, as a character for randomness in MAC addresses, to work around a design quirk in ZSH.</li><li>Where files are operated on post-build, e.g.&nbsp;coreboot images, more strictly operate on them first, erroring out more reliably when a fault occurs; prevent bad files from being copied to final build destinations. This reduces the chance of bad/corrupt build artifacts being present in release builds.</li><li>HP EliteBook 820 G2 images now included in releases, because handling of the refcode files was corrected so that checksum verification passes during insertion.</li></ul><p>This, and more, has all been done. There was also a general focus on heavily auditing the build system, lbmk, so as to clean up the code. The amount of overall code in lbmk was , without removing functionality.</p><p>These next sections will repeat many of the above items, but in more detail.</p><div><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot2506.html#detailed-list-of-changes\">[link]</a></div><p>The priority for the first half of 2025 has been on further auditing the Libreboot build system, so fewer board ports were added. More board ports will be added instead in the December 2025 release (a lot more).</p><p>The following boards have been added since the Libreboot 20241206 release:</p><ul><li>Acer Q45T-AM support added (similar to G43T-AM3 mainboard)</li><li>Dell Precision T1700 SFF and MT</li></ul><p>Board ports were low priority for this release; now it shall be the focus, between June 2025 and October 2025, ready for the 25.12 release cycle leading into December 2025.</p><p>Dell Precision T1700 is essentially the OptiPlex 9020 but with a slightly different, code-compatible PCH that also supports ECC memory features when an Intel Xeon processor is installed.</p><p>In descending order from latest changes to earliest changes:</p><ul><li>GRUB: Update to revision 73d1c959e (14 March 2025)</li><li>Bump SeaBIOS to to rev 9029a010, 4 March 2025</li><li>Updated Untitled to newer LBSSG repository.</li><li>Bump flashprog to rev e060018 (1 March 2025)</li><li>Bump U-Boot on ARM64 boards to U-Boot v2025.04. Patching courtesy of Alper Nebi Yasak.</li><li>Bump uefitool to rev a072527, 26 Apr 2025 to fix CMake compatibility issue since CMake 4 no longer supports version 3.5, whereas the old uefitool had an earlier version as the minimum supported. This fixed a minor build error.</li><li>Merged coreboot/next with coreboot/default</li><li>Bump coreboot/next to rev c247f62749b as of 20 April 2025</li><li>Bump coreboot/default to rev c247f62749b as of 20 April 2025</li><li>Bump flashprog to revision eb2c041 (14 Nov 2024).</li></ul><p>The GRUB revision includes a number of critical CVE fixes, and regression fixes, that were also included in Libreboot 20241206 rev11. Some later fixes are also present, such as wiping LUKS keys from memory after successfully booting Linux (Linux handles LUKS itself, and starts the process again).</p><p>The NASM version was updated to version 2.16.03 on coreboot/fam15h, to prevent build errors, instead of fixing the old NASM 2.14.02. Tested on Debian Sid Experimental, with GCC15-based toolchain, and on Fedora 42.</p><p>PICO support: Reverted to the old pico serprog/sdk repositories used in Libreboot 20240612. This is temporary, because pico2 support is currently broken, so this release only has pico1 support, when dealing with Rpi Pico devices. Upstream pico-serprog works fine on pico2, so this will be fixed in and re-updated again in a future revision release. The pico2 update images were retroactively removed from the 20241206 release on rsync.</p><p>A patch from upstream was backported to the old pico-sdk version, so that it builds correctly on newer GCC15 (tested on Debian Sid with ‚ÄúExperimental‚Äù packages enabled).</p><ul><li>Added SPDX license headers to almost every configuration file in lbmk.</li></ul><p>These can be considered bug fixes, but these are special fixes that are of massive concern to users.</p><ul><li>This GRUB change was merged, in the aforementioned revision update: <code>dbc0eb5bd disk/cryptodisk: Wipe the passphrase from memory</code> - this wipes the LUKS key from memory, after GRUB exits, where one was created by GRUB while unlocking a given volume.</li><li>Merged  critical CVE fixes into the GNU GRUB source code, from upstream.</li><li>Stricter use of pledge and unveil in the nvmutil source code.</li><li> safer . It used to be that the tarballs were extracted and files inserted into the extracted images, but the tarballs were left unmodified; many users thought then that they should extract the tarball and flash that, which lead to bricks. And it was easy to flash uninjected images, where files (e.g.&nbsp;Intel ME) are needed, so now ROM images are padded by one byte, to prevent flashing, and the user is strongly reminded to inject files first; upon running the  commands, these images are then safe to flash.</li><li>Fix globbing issues in lbmk by double-quoting variables everywhere, and generally making sure that certain characters are escaped properly when necessary. To reduce the chance of bad commands being run by mistake or intentionally.</li><li>Removed auto-confirm on  commands, to mitigate the risk of a buggy package manager on the user‚Äôs distro possibly removing many packages. Now the user must confirm their choice, e.g.&nbsp;when a conflict occurs, instead of the package manager already deciding for the user.</li><li>ThinkPad T480 / OptiPlex 3050: Disable HyperThreading/SMT by default, for security, to reduce the attack vector of certain speculative execution-based exploits.</li></ul><p>In descending order from latest changes to earliest changes:</p><ul><li>: looser  validation; correct it on child instances, if it‚Äôs not set, or set incorrectly.</li><li>: use subshells on  functions, wrapped in an error handler so as to provide more verbose output under fault conditions. This makes it easier to debug when a download fails.</li><li>: Re-implement redundant git downloads, more reliably than before; all repositories are now cached, reliably, including submodules, even when upstream repo links differ wildly. This reduces the amount of internet bandwidth used, when handling multiple builds.</li><li>: build in tmp directory first, leaving old files behind under fault conditions, for further analysis</li><li>: re-add mac address confirmation, for user-friendliness, when running the inject commands.</li><li>: Resolve  via readlink</li><li>: Use  in , with realpath only as fallback. This makes the function more redundant, working on more systems by default.</li><li>: support any command on  (later renamed); this is a generic function, that implements a while loop for a given set of files, based on the output a command that generates those paths. This is operated on by a function, defined when calling find_exec. This unifies all use of while loops on lists of files and directories, throughout xbmk, rather than re-implementing the for/while loops each time.</li><li>: simplify kconfig scanning by using the  with a new function, . This new function checks  coreboot configs for a given target, whereas the old behaviour only resulted in the  config being checked. In practise, this causes no real behaviour changes.</li><li>: Print the rom image path being generated</li><li>: Add warning if x_ is called without args</li><li>: More verbose error info, on non-zero exits.</li><li>: Within each 4KB part, only handle 4KB, even if the block size is bigger. This means using less memory, and modification of anything past 4KB is not required.</li><li>: Support 16KB and 128KB GbE files, in addition to the usual 8KB files. The size is based on the block size of the flash you use.</li><li>Added non-root USB3 hub support to GRUB on the xHCI implementation, courtesy of a patch from Nitrokey.</li><li>GRUB: Scan LUKS inside  LVM, to support the uncommon use case where LUKS is inside LVM, instead of LVM inside LUKS. It is theoretically possible, even if ill advised.</li><li>GRUB: Scan  LVM device, where available, as a fallback at boot time when all else fails.</li><li>Release ROMs prefixed with a ‚ÄúDO NOT FLASH‚Äù warning and padded by one byte, where vendor files are required. The  commands remove this prefix/padding, after vendor files are inserted and checksums verified.</li><li>Better detecting of whether vendor files are needed, and confirmation to the user while running  commands.</li><li>Allow restoring the default MAC address on  commands, by using the  arguments.</li><li>Randomise the MAC address by default, where applicable, when running the  commands, because lots of users previously flashed without changing it, so lots of users had generic MAC addresses. The  argument prevents this from happening, where desired.</li><li>: More user-friendly debug messages, for the user to know what‚Äôs going on.</li><li>: Add uninstall command to the Makefile</li><li>: Add distclean command to the Makefile</li><li>: Nicer hexdump display, similar to .</li><li>Support a  argument in  Fedora commands, for re-installation of packages as desired.</li><li>Support  in the  command, when the user wants to re-install dependencies.</li><li>Put temporary  directory in the normal  directory, and clear it whenever a new parent instance of the build system is executed. This is used for the GCC/GNAT matched symlinks, for example, or the python symlink created at startup.</li><li>Pico 2 support briefly added, but was a bit buggy for now, so it‚Äôs removed in this release, and was retroactively removed in rsync for the Libreboot 20241206 release; this will be re-added in a future release.</li><li>Added GRUB-first payload setups as an option, but not enabled by default. The user can add  in the  file for a given mainboard.</li><li>Support automatically downloading Lenovo ThunderBolt firmware for the ThinkPad T480, automatically padding it for installation. This update fixes a charging bug that affected some earlier launch models.</li><li>Insert GRUB backgrounds in CBFS instead of GRUB memdisk, which makes GRUB background images easier to replace.</li></ul><p>In descending order from the latest changes to the earliest changes:</p><ul><li>ifd/hp8300usdt: set the HAP bit by default; it was previously not set, but the  config was nonetheless used, and ME Soft Temporary Disable was also used. As a result, this change is basically redundant, but otherwise technically correct (more so than the previous behaviour).</li><li>coreboot: Remove unused vboot tests (futility tests), to shrink the size of release tarballs.</li><li>coreboot/default: Remove unneeded FSP modules when downloading, because only the Kabylake version is needed at this time. This is done, using the  function via  files. This shrinks the size of release tarballs.</li><li>: add </li><li>HP 820 G2: Use fam15h cbfstool tree for refcode; this avoids the need to clutter the source code with an entire additional coreboot tree, thus reducing the size of releases.</li><li>A GRUB configuration change was made, fixing auto-scanning of LVMs when doing cryptomount.</li><li>T480/3050micro: Removed the  targets, because we only need the  targets.</li><li>Added  to Fedora 41 dependencies.</li><li>Added  to Arch dependencies, needed for the  utility.</li><li>Added  to Arch dependencies, because it‚Äôs needed for certain commands e.g.&nbsp;git commands.</li><li>GRUB: Use the codeberg mirror first, to mitigate GNU mirrors often being slow or rate limited, e.g.&nbsp;for gnulib downloads.</li><li>fedora41/dependencies: add libuuid-devel</li><li>Added  to fedora41 dependencies</li><li>flashprog: Disable  to prevent minor warnings being treated as errors.</li></ul><p>This combines both build system fixes, and changes to upstream sources (e.g. coreboot and various payloads like SeaBIOS/GRUB, utilities like flashprog, and so on).</p><p>The following bug fixes have been merged (in descending order from the latest changes to the earliest changes):</p><ul><li>: add sha512 error for . Handle errors in  and ; also check that  exists and error out if it doesn‚Äôt, when checking a given project hash. We know that the project hash file should always exist, and always be read; technically, find might not yield results, but then an empty file would be produced. the empty file edge-case scenario would already have resulted in an error exit inside , so that‚Äôs already covered.</li><li>: add error checking in , when reading the  variable; we need to error out where a read error occurs. such an error is extremely unlikely, so this fix is largely theoretical and preventative.</li><li>: more reliable clean in ; don‚Äôt do a no-op if it fails, instead fall back to the  method, and throw an error if  fails. The no-op existed because not all projects have distclean, but we always intend for them to be cleaned. This therefore prevents further unhandled error conditions, in such edge cases.</li><li>put coreboot utils in , to prevent old binaries from still being used when a code change is made.</li><li>: use printf to create version files, instead of copying the version files, because they don‚Äôt exist in some cases, so this prevents an error condition.</li><li>: error out if .git/ is a symlink; this is a preventative bug fix, to prevent future unknown bugs in such a scenario.</li><li>: Properly error out if  fails, where it previously failed to throw an error under certain fault conditions.</li><li>: Don‚Äôt auto-run make-oldconfig; it now must be applied permanently, via e.g.&nbsp; commands. Otherwise, undesirable changes can sometimes be made during build time, especially on projects that don‚Äôt use scons quite as reliably, as in the U-Boot build system.</li><li>: re-generate remotes every time, on cached Git repositories, so that configuration changes in  are automatically applied when dealing with multiple versions of a given upstream project.</li><li>: copy version files to  (release source directory), otherwise an  version number is erroneously created. This fixes a regression caused by previous optimisation to </li><li>xbmk: add fake config makefile args to , and , to prevent  (without additional arguments) from erroneously exiting with error status. otherwise, an error can occur in such conditions if a Makefile has not yet been created.</li><li>: skip running  on dry builds, otherwise running  without argument will cause an error.</li><li>: Don‚Äôt run make-clean on dry runs (), to prevent error conditions while building GRUB, if  is passed without additional argument, since the latter requiires running autoconf to get a Makefile in the GRUB build system.</li><li>: add missing check in ; we were checking the main URL on a download, but not the backup URL.</li><li>: stricter URL check in ; throw an error if a URL is empty, rather than skipping to the next. If a URL is set but fails, then falling back to the next is OK (or throw an error if the backup is set, and also failed).</li><li>: Make  always throw an error upon exiting the loop check; it was previously throwing an error if the for loop returned with zero status. Depending on the sh implementation, or changes made in the future, this could cause unpredictable buggy behaviour. Therefore, the error exit is much stricter now, and less ambiguous, to prevent future bugs, because it is imperative that execution must never continue under fault conditions. If a file or repository is successfully handled, a return (zero) occurs, otherwise the loop exits and a non-zero exit occurs.</li><li>: fix up , or specifically fix a bad  loop, because shorthand conditionals are used and the way they were used can be buggy on some sh implementations, so they are terminated more explicitly.</li><li>xbmk: stricter handling of files on while loops, to prevent instances where execution continues under fault conditions. This prevents other, less predictable bugs in the future.</li><li>: Hardcode  for integrity; this is a bug fix, because there‚Äôs too much that can be wrong with this being configurable, so now it is hardcoded at runtime. It was never intended to be configurable anyway.</li><li>: check/validate version/versiondate once read, in child instances of xbmk, to further verify that they were previously set, and set correctly. This is theoretically a preventative bug fix.</li><li>: force an error condition if the xbmk version was not read. This prevents further erroneous state within xbmk.</li><li>: check the  file BEFORE , to prevent erroneous initialisation while another xbmk parent instance is running.</li><li>: return from xbmk child instances in  instead. This is easier than the previous check, preventing the initialisation of a git repo and/or recreation of xbmktmp and xbmklocal by erroneoues parent executions of xbmk while another parent is running - the latter of which could have caused a massively unpredictable build failure, so this is also a preemptive bug fix, fixing and preventing all kinds of weird unknown bugs.</li><li>: Remove  if it‚Äôs bad; this complements a bug fix, in the bug fix section above, that caches the extracted files and hashes them. On a subsequent run where the given file is needed, it is  if the final file exists. This mitigates the possibility that corruption may have occured, under unhandled fault conditions. Therefore, this is a preventative bug fix.</li><li>: don‚Äôt move  to  inside release archives, because otherwise  will fail, inside release archives.</li><li>: Properly verify SHA512SUM on extraction. This is performed on the actual extracted files, alongside the existing check on downloaded files. This mitigates against future fault conditions in the extraction process, thus fixing a major design flaw. <em>This change also caches those files, thus speeding up extractions when they‚Äôre done multiple times.</em></li><li> submodules: Don‚Äôt delete files recursively. Use  instead of , on files.</li><li>: Only create destination repo on success; don‚Äôt leave a broken cache laying around, which would otherwise break the build system under certain conditions.</li><li>: removed an unnecessary  variable</li><li>: delete tmp/cache from release tarballs</li><li>: Remove confusing path on tar creation; that is, don‚Äôt print said path, because temporary paths are printed during this, when creating tarballs. In this file, the correct path is printed at the end of the process, when handling an images tarball.</li><li>: only create elfdir in , to prevent empty directories being created where a project provides , but where no actual configs are being built on a given target name.</li><li>: operate on refcode in tmp area first, to prevent bad files from being saved to the final destination under fault conditions. This pertains to the change made at build time that enables GbE devices from the refcode.</li><li>: use subshell to speed up  (this is a bug fix, because slowness is a bug)</li><li>: add missing error handli for  (when doing releases)</li><li>: hard fail if git am fails (regression fix)</li><li>: Hard fail if reset fails; allowing re-try when cloning fails, but the reset-fail scenario didn‚Äôt cause any exit at all. This is fixed now.</li><li>: Only check  if it exists</li><li>: fix trying to boot all logical volumes after unlocking an encrypted volume; this makes booting LVMs more reliable, on encrypted boot setups.</li><li>: also allow  or , not just  and , because some people use uppercase here. This is considered a bug fix, but could just as easily have been in the features section.</li><li>: check  is a directory instead of a file.</li><li>: run , to prevent a future situation where the version is not set correctly. In general, the version should always be set as early as poessible when running xbmk.</li><li>: clean up tmp me file before extract; this is a preventative fix, to ensure that cross-flashing does not occur.</li><li>: re-add missing break in fe/fx_, that caused improper exits (or non exits) in some cases.</li><li>: use , not ; this is a less strict test, to prevent certain errors under specific edge-case conditions.</li><li>: Safer ; don‚Äôt insert special files like GRUB keymaps AFTER copying the system ROM to the final destination; do it BEFORE, instead, to ensure that bad images aren‚Äôt left in place under fault conditions.</li><li>: specifically check keymaps in ; it previously checked whether a setup is  seauboot, which was valid, but future conditionals would break this check. the code has been changed in advance, to prevent bugs in a future revision of xbmk.</li><li>: Fix bad error handling for ; I accidentally mixed and/or in a shorthand conditional statement, which leads to buggy behaviour in various implementations of sh.</li><li>GRUB: Mark E820 reserved on coreboot memory, to fix cbmem when running with strict  access; otherwise, restrictions on access to memory below 1MB will cause an error when trying to access the cbmem console.</li><li>: set  in  in case they were set  in other parts of xbmk.</li><li>: Silence the output of git config ‚Äìglobal</li><li>: Run git name/email check before init; otherwise, it returns if init is already done, which could lead to an error later when building coreboot.</li><li>: stricter  check in </li><li>: simplify err-not-set handling</li><li> err: add missing redirect to stderr</li><li>xbmk: MUCH safer  function; make it an actual function, instead of a variable. Initially, this function was made to then check a variable, that refers to a function, and a fallback was provided for non-zero exit in case the pointed function didn‚Äôt, but it was later made to be just a simple function that exits with a message. Code equals bugs, so fewer lines of code will yield fewer bugs.</li><li>: Make x_ err if first arg is empty; this is a preventative bug fix, to make the build system still exit under such conditions, but it would result in an empty error message.</li><li>: Make err_ always exit no matter what; this is a preventative bug fix, just making the exit stricter in all cases.</li><li>: re-make gnupath/ after handling crossgcc, rather than deleting files within. This makes the creation of it more reliable.</li><li>: re-make gnupath/ for each cross compiler, to ensure that no stagnant build artifacts are re-used</li><li>: Stricter TBFW handling; don‚Äôt copy it until it has been properly padded to the correct size.</li><li>:  tmpdirs on parent instance, to ensure that they are not cluttered with old files that might cause weird bugs in the future; this is a preventative bug fix.</li><li>: Always create xbmklocal, to prevent errors in the case when it isn‚Äôt created automatically in certain child instances, like when running a  copy of the build system, during release builds.</li><li>: Fix bad touch command</li><li>: always re-build nvmutil, so that changes to it are automatically re-applied when running the build system again. (and only build it once, for a given instance of xbmk)</li><li>: use , not , for random characters, while still supporting  for backwards compatibility. This is because ZSH errors out when providing the old characters, in some setups. Use of  is more reliable, across several implementations of sh, e.g.&nbsp; would be a full random MAC address.</li><li> find_ex: explicitly create the tmp file, to prevent errors, which were nonetheless unlikely to begin with.</li><li>: Explicitly create the xbmktmp directory (make sure to do this when creating this which is a temporary directory).</li><li>: add fe_ which is fx_ but err on find</li><li>xbmk: unified execution on  commands. Handle it with a new special function that is common across the build system.</li><li>: Download vendorfiles before building release, to mitigate intermittent internet connectivity during release builds, otherwise a release build could fail. This way, all downloads are done simultaneously, since downloads are the fastest part, even on a crap internet connection.</li><li>Revert AHCI reset patch for SeaBIOS, which caused AHCI not to work in SeaBIOS on the 25.04 release; the latter was also revised, to fix this. SeaBIOS has since added a new release, which includes a fix that delays AHCI reset, to mitigate in cases where the controller isn‚Äôt ready sooner. However, this release simply reverts the AHCI reset patch for now. The AHCI reset plus delay will be present in Libreboot‚Äôs next release, after 25.06.</li><li>lenovo/t420: Add missing text-mode configuration</li><li>coreboot (all trees): Added patch fixing GMP build errors on modern GCC15 hostcc.</li><li>coreboot (all trees): Fixed building of crossgcc with newer GCC15. Patches courtesy of Alper Nebi Yasak.</li><li>coreboot (all trees): Added a patch to fix building coreboot utils with newer GCC15.</li><li>dependencies/debian: Fixed the libusb package name for newer Debian releases, courtesy of Alper Nebi Yasak.</li><li>SeaBIOS: Fixed  function pointers in the  patch, courtesy of Alper Nebi Yasak. Fix build errors on GCC 15.</li><li>: Force use of System Python e.g.&nbsp;, when a python venv is detected. This prevents the build system from hanging.</li><li>coreboot : Fixed the  path.</li><li>Alper Nebi Yasak fixed the Python 2/3 detection in some edge cases when the  command is python2.</li><li> (later ): Do root check , right after the dependencies check, whereas it previously did the python check before checking for root user.</li><li>lbmk: Don‚Äôt use TMPDIR directly, use another variable containing its value, and make sure it doesn‚Äôt get changed wrongly. This reduces the possibility of accidentally leaving old tmp files laying around.</li><li>:  commands now return an exit with error, if a fault occurs, whereas it didn‚Äôt before, due to piped output. This is done using the  wrapper on tar commands, to provide error exits.</li><li>: function  now returns an error, if the sha512sum command fails. It previously didn‚Äôt, due to piped outputs. It‚Äôs now mitigated by using  on piped commands, for error exits.</li><li>Forking of lbmk parent instance to child instance isno longer handled by variables. It‚Äôs been simplified, to only be based on whether TMPDIR is set, and it‚Äôs generally more robust now in this release. The old code sometimes broke under certain edge cases.</li><li> (later renaming to ): General code cleanup, about 100 sloc removed without reducing features.</li><li>lbmk: Initialise  to a standard string if not set, on the parent instance of lbmk.</li><li>lbmk: Use  instead of the  variable, resetting the latter safely as lbmk runs. This prevents lbmk from changing directory to an erroneous system path, if  wasn‚Äôt properly set for some reason. This is a preventative bug fix, because no actual issue ever occured in practise.</li><li>Much safer Python version check at lbmk startup, using data structures that are provided universally by all Python implementations, instead of relying on the output of .</li><li>Fixed T480 backlight controls, courtesy of a patch from Mate Kukri.</li><li>Set up Python in  when lbmk starts, to ensure that it is always version 3. This is checked at startup.</li><li>: Prevent double-nuke, where a given tarball already had vendor files removed prior to release.</li><li>: Allow setting a MAC address even if vendor files aren‚Äôt needed.</li><li>: Download utils even if  is not set, in case the user is also setting a MAC address.</li><li>: Honour the  variable, if set by the user, otherwise it is set to  by default.</li><li>: Don‚Äôt do  when running .</li><li>: Proper DESTDIR/PREFIX handling, whereas it was not handled properly at all before.</li><li>: Only set CC/CFLAGS if unset, and use sensible defaults.</li><li>Fixed various shellcheck errors in lbmk.</li><li>HP EliteBook 820 G2: Fixed vendor file insertion and set . The insertion of Intel MRC and refcode previously didn‚Äôt pass checksum validation.</li><li>ThinkPad T480 / OptiPlex 3050: Force power-off state upon recovery from power loss, otherwise the system always turns on as soon as a charger is plugged in. This is configured by hardcoding, due to a current lack of any option table on the T480.</li><li>Debian dependencies: replace liblz4-tool with lz4 and liblz4-dev. The latter is also available in Debian Trixie and Sid, at this time, in addition to Debian Bookworm, so it works on all of them.</li><li>U-Boot (x86): Fixed a bug since Swig 4.3.0 changed the syntax for its language-specific AppendOut functions. A patch from upstream was backported, and the patch is also compatible with older versions of Swig.</li><li>In lbmk scripts, use  instead of , to find the locations of certain binaries. This is a bug fix, since  is non-standard and so could break on some setups.</li><li>Crossgcc: when building it for coreboot, fix mismatching GCC/GNAT versions so that they match, if multiple versions are present. This was done because Debain Trixie initially had GCC 14 and GNAT 13, whereas we need GNAT to build the Intel video init code on many mainboards.</li><li>T480/T480: Disable TPM2 to mitigate a hang in SeaBIOS due to buggy drivers.</li><li>: Fix the  package, renamed it to , which works on bookworm  newer, but the former did not.</li><li>: don‚Äôt initialise the  variable globally, reset it per target instead, to prevent some repositories from being wrongly re-cloned.</li><li>Thinkpad T480 / Dell OptiPlex 3050: Handle FSP insertion post-release, rather than providing FSP images directly in release images. It is now handled by the  command, copying the reference image from coreboot and splitting it upp and rebasing it, to mitigate certain technicalities of Intel‚Äôs FSP license, which otherwise permits free redistribution.</li><li>Safer, more reliable exit when handling vendor files, because in some cases lbmk was leaving the  file in place (erroneously).</li><li>Safer exit when running the  commands, so that lbmk is more likely to exit, because it was theoretically possible that it might not under certain edge cases.</li><li>Disable nvme hotplug on Dell OptiPlex 3050 Micro, to prevent replugging in Linux, which would otherwise lead to possible data corruption.</li><li>T480: Fix coreboot SPD size to 512 instead of 256 (it was already auto-corrected to 512 at build time, but the original configs were 256 which is wrong).</li><li>Add tarballs and gpg signatures to </li></ul><p>Another bug focus in this release was to clean up the logic of Libreboot‚Äôs build system, and fix several bugs, especially those relating to error handling.</p><p>A lot of cleanup was done on the init functions used by the build system, to initialise common variables, such as environmental variables, and temporary files and/or directories; such logic was moved to a new script called .</p><p>In descending order from the latest changes to the earliest changes:</p><ul><li>: simplify </li><li>: simplify </li><li>: simplify </li><li>: simplify </li><li>: tidy up </li><li>: simplify </li><li>general cleanup in  and </li><li>xbmk: rename / variables (shorten them)</li><li>: consolidate printf statements</li><li>: remove redundant printf in </li><li>: remove superfluous command in </li><li>: simplify </li><li>: simplify </li><li>: rename , which actually handles general init tasks, including the processing of vendor files where appropriate.</li><li>: simplify ccache handling for coreboot; make-oldconfig wasn‚Äôt needed at all, when cooking configs to enable ccache, so the  function became much smaller and was then merged with </li><li>: simplify u-boot payload handling, by using a single variable name that defines the type of U-Boot tree. This allows several other U-Boot-related checks to be greatly simplified, as they were.</li><li>: add a colon at the end of a  loop</li><li>: make  easier to understand, by not using shorthand conditional statements in the for loop handling a repository or file download.</li><li>: merge  with </li><li>: set pyver from  instead of the main function.</li><li>: merge  with </li><li>: only update version files on parent, to speed up xbmk</li><li>: simplify unknown version creation, where none was created and no Git metadata exists.</li><li>: only set xbmk version on parent instance; we only need to read what was set, on child instances. In other words, apply the principle of least privelege.</li><li>: initialise variables AFTER path, to avoid unnecessary work inside child instances of xbmk.</li><li>: merge  with </li><li>: Set python version only on parent instances of xbmk, to speed up operation of the xbmk build system.</li><li>:  to </li><li>: move  creation to </li><li>: move PATH init to </li><li>: shorten the  variable name</li><li>: simplify </li><li>: simplify </li><li>: use  on find command for , so as to remove the need for a more complicated while loop inside said function.</li><li>: move  to  and only run it on releases; don‚Äôt do it on normal xbmk Git. It‚Äôs only needed in the former context, because that has to do with distribution by the project, and this makes development easier. Therefore, files are only purged within the release archives, but not during development.</li><li>: simplify </li><li>: merge  with </li><li>: simplify </li><li>: simplify  by using  for the file loop</li><li>: simplify </li><li>: simplify  config check</li><li>: simplify  by using  for everything, instead of implementing redundant logic in the build system.</li><li>: reduce indendation in ; simplify the for loop by replacing it with a call to  instead.</li><li>: simplify </li><li>: clean up the  after release</li><li>: removed an unnecessary  command</li><li>: split up  into smaller functions</li><li>: remove the unnecessary  function</li><li>: move  to </li><li>: move  to </li><li>: split up  into smaller functions</li><li>: move  to </li><li>: remove the  variable</li><li>: define  here instead</li><li>: remove </li><li>: simplify </li><li>: simplify </li><li>: split up  into smaller functions</li><li>: only compile nvmutil if needed</li><li>: simplified serprog check</li><li>: tidy up variables</li><li>: split up  into smaller functions</li><li>: further cleanup for , such that all vendor-download functions are only defined in ; this means that the Canoeboot version of the file can remain in much closer sync, with fewer differences.</li><li>: simplified srcdir check on make-clean</li><li>: split download functions to a new file, </li><li>: split up the inject functions into smaller functions for each specific task.</li><li>xbmk: use  instead of , where appropriate, because it handles globbing perfectly these days, and  is cleaner in most cases.</li><li>: fix outdated info in a comment</li><li>: use direct comparison for metmp, to speed up checking so many files.</li><li>: remove unnecessary line break</li><li>: re-split tree logic to new file, </li><li>: move release functions to </li><li>: use  for fail variables</li><li>: remove useless export; variables that are y/n can just be reset to  if not set to , for simplicity.</li><li>: export  in  instead of .</li><li>: simplify </li><li>: simplified MAC address handling</li><li>: Simplify </li><li>: Remove useless command in </li><li>: rename  and  functions (make the names shorter).</li><li>: Simplified  and removed ; fe didn‚Äôt prefix  to a given command, but fx did. Now, it is prefix manually, for greater control, on commands that need stricter error handling, while it can be avoided on commands where strict error handling is unfeasible.</li><li>: Create serprog tarballs here instead;  was simplified to use mkhelp when building actual images.</li><li>build serprog images using , to tidy up xbmk</li><li>: build serprog images with , rather than implementing a specific for loop.</li><li>: insanely optimise the me bruteforce, by operating on files recursively via the  function, instead of manually implementing a recursive file search, when bruteforce-extracting  images.</li><li>: Simplify git am handling by using the new  or  function, instead of making a specific while loop.</li><li>: remove an unused function</li><li>: New function  to execute path files; this is used instead of for loops all around xbmk, to simplify operations where the output of a file search is used as argument to a function.</li><li>: Further simplified FSP extraction</li><li>: Write sort errors to </li><li>: Remove warning of empty args; it‚Äôs really not required, since it‚Äôs obvious anyway in the resulting final error message.</li><li>xbmk: Replace  with much simpler implementation, for reliability and bug prevention.</li><li>: simplify </li><li>: simplify </li><li>: move  to </li><li>: simplify <code>extract_intel_me_bruteforce()</code></li><li>: Remove unnecessary check</li><li>: reduce indentation</li><li>: Move FSP extraction only to , since that‚Äôs the only place where it‚Äôs needed.</li><li>: tidy up intel me handling</li><li>: tidy up the deguard command</li><li>: single-quote xbmklock in </li><li>: define lock file in a variable instead; this makes it more flexible, because the path can be checked and then re-used nicely.</li><li>: tidy up ; make the command style more consistent</li><li>: rename errx to xmsg</li><li>: tidy up TBFW handling</li><li>: remove useless comment block</li><li>: tidy up the python version check</li><li>: move non-init functions to </li><li>: simplify dependencies handling</li><li>: tidy up </li><li>: tidy up xgccargs handling</li><li>: generally removed dead code</li><li>: tidy up pathdir creation</li><li>: tidy up </li><li>: reduce indentation in </li><li>: Allow use of x_ on prefix functions</li><li>: tidy up  sha512sum check</li><li>: simplify </li><li>: general code cleanup</li><li>: simplify </li><li>: simplified fsp extraction</li><li>: Remove redundant code in copy_tbfw</li><li>xbmk: Unified local ./tmp handling</li><li>: redirect find errors to  to prevent clutter on the user‚Äôs terminal</li><li>: unified handling of ./tmp</li><li>: include rom.sh directly</li><li>: support multiple arguments in remkdir()</li><li>: simplify remkdir()</li><li>: move setvars/err_ to lib.sh</li><li>: Generally modularised it, moving separate tasks into separate functions, rathher than having it be one big monolith.</li><li> was renamed to , so that future changes can be in better sync between lbmk and cbmk on this file, because the cbmk version has the MAC address changer (but no vendorfile handling). In the future, this will be split so that  exists again, containing only the vendorfile handling, and  will only handle MAC addresses.</li><li>: Several variables were moved out of this file and elsewwhere in lbmk.</li><li>Moved the  function to  instead of </li><li>Moved the  function from  to .</li><li>: Use a more top-down function order, more clear, and it was split into an extra file  that does the most basic lbmk initialisation at startup, whereas what remains in  really are generic library functions used throughout lbmk.</li><li>: Removed unused crossgcc linking feature, because we don‚Äôt use it anymore (coreboot trees have their own crossgcc and never link to another these days). Libreboot used to have many more coreboot trees, some of which re-used crossgcc from another tree. Similarly, the accompanying variable  is no longer handled. The  variable is still handled, because projects like U-Boot use that to configure crossgcc.</li><li>include/vendor.sh: Removed unnecessary check against the ROM image size. Generally simplified the processing of release images.</li><li>include/git.sh`: Removed many redundant functions, merging several of them.</li><li>: Fixed a bad print, making proper use of a string inside a printf statement.</li><li>Simplified many file checks in lbmk, by using the  function.</li><li>Removed a bunch of useless  commands in general, throughout lbmk, making the code much cleaner.</li><li>lbmk: the  function is now used much more aggressively, for error handling, simplifying error handling in lbmk overall.</li><li> main script: Merged the  script with it, so now it‚Äôs all one script. The  script is now the only executable script in lbmk.</li><li> (main script): The  command is removed (legacy / obsolete).</li><li>The version/versiondate files are now dotfiles, to hide during operation.</li><li>: Hardcoded projectname/projectsite variables, instead of storing them in a file.</li><li> script: Unified handling of flags (same string used in error output), to ensure that error(usage) messages always match.</li><li> script (later merged into ): Removed a lot of old bloat.</li><li>: Make the checksum word position a define. Generally cleaned up a lot of code to make it clearer for the reader. Added more verbose messages to the user, confirming things such as how much was read or written on the user‚Äôs file system. Various miscallaneous bug fixes (edge cases that were unlikely to ever be triggered).</li><li>: More efficient use of memory when handling files.</li><li>: Much cleaner handling of user input.</li><li>`util/nvmutil: More granular MAC address parsing errors, easy for debugging.</li><li>: Make the Gbe Checksum a define, for readibility.</li><li>: Obey the 79-character-per-line limit, as per lbmk coding style.</li><li>: Tidied up several pledge calls</li><li>Removed use of several unnecessary subshells and  statements in lbmk.</li><li>: Later, the GCC/GNAT matching feature was rewritten to work both ways, where an older GCC was matched to GNAT and vice versa, whereas it previously only went one way.  and  are manipulated in  to ensure that the user has a consistent version of both.</li><li> later merged into the  script (which later merged into the main  script). This  is what contained the first implementation of the GNAT/GCC version matching feature.</li><li>: Remove unnecessary shebang, and the same on other  scripts. NOTE:  was later merged into , which then became split into  in later changes (see above).</li><li>Removed legacy build system commands e.g.&nbsp; and ; now only the newer  commands are supported. This and the change below was briefly reverted, for the 20241206 revisions, but then re-introduced.</li><li>Removed the deprecated  command; now only  commands are used. The  commands are used, for downloading vendor files.</li><li>Removed unused patch that was for the original deguard implementation, prior to Mate Kukri‚Äôs re-write of it.</li></ul><p>This log shows all changes in today‚Äôs release, from 30 June 2025, ever since the Libreboot 20241206 release of 6 December 2025:</p><pre><code>* c46a71138c7 Libreboot 25.06 release \n* b1ef562b767 tree.sh: add sha512 error for check_project_hashes \n* 04bee3834d0 tree.sh: add error check in check_project_hashes() \n* 677dfc4d103 tree.sh: more reliable clean in run_make_command \n* 267d4c90341 inject.sh: add missing semicolons \n* 974bdbb3815 vendor.sh: fix bad cbfstool path \n* dc6996252a0 put coreboot utils in elf/coreboot/TREE \n* b77154640de release.sh: use printf to create version files \n* dee6997d0cc lib.sh: simplify setvars() \n* 79ded40f3d0 lib.sh: simplify chkvars() \n* 5036a0bc501 mk: simplify main() \n* 41308ee9244 get.sh: simplify fetch_project() \n* b5867be214d get.sh: simplify try_copy() \n* 495098d6a71 get.sh: tidy up bad_checksum() \n* 671e3aa27b4 get.sh: simplify fetch_targets() \n* 09b6e91803d general cleanup in get.sh and vendor.sh \n* 18dacd4c22b xbmk: rename xbmklocal/xbmktmp variables \n* e981132c829 get.sh: consolidate printf statements \n* afc36754b13 get.sh: remove redundant printf in fetch_project \n* ffe387ac6b9 get.sh: remove superfluous command in try_git() \n* ba7c49c090b vendor.sh: simplify fetch() \n* 30bc3732c39 init.sh: error out if .git/ is a symlink \n* 2493203ee53 get.sh: Properly error out if tmpclone fails \n* ad333ae2481 tree.sh: Don't auto-run make-oldconfig \n* 97ce531c341 rom.sh: simplify mkcoreboottar() \n* a47e9811723 rom.sh: rename mkvendorfiles \n* d2e148fdd9d rom.sh: simplify ccache handling for coreboot \n* 8c3f10ba402 rom.sh: simplify u-boot payload handling \n* 3e28873532b ifd/hp8300usdt: set the HAP bit by default \n* 452aeb6001a coreboot: Remove unused vboot tests \n* 64cc91bca33 coreboot/default: Remove unneeded FSP modules \n* 0216a3104a5 get.sh: Always update git remotes \n* 419733d3073 get.sh: re-generate remotes every time \n* 231b320e63b release.sh: copy version files to rsrc \n* fc0720184d9 xbmk: add fake config makefile args to flashprog \n* f9266601b8c vendor.sh: add colon at the end of a for loop \n* 8e0c6059d15 rom.sh: skip copyps1bios on dry builds \n* a3250d14474 tree.sh: Don't run make-clean on dry runs \n* 24b8e633e03 GRUB: Update to revision 73d1c959e (14 March 2025) \n* f6b77822835 Revert \"vendor.sh: optimise find_me()\" \n* fb7aaa78bb0 vendor.sh: optimise find_me() \n* 903f78bf080 get.sh: add missing check in fetch_project() \n* f15bb8153a3 get.sh: stricter URL check in xbmkget() \n* cdc0fb49e1c get.sh: make xbmkget() easier to understand \n* 620c1dd6fae get.sh: Make xbmkget err on exiting the loop check \n* 900da04efa9 tree.sh: fix up copy_elf(), bad for loop \n* 8aaf404ddea lib.sh: Use while, not for, to process arguments \n* d9c64b26754 xbmk: stricter handling of files on while loops \n* b25a4876434 init.sh: looser XBMK_THREADS validation \n* 769a97aed5a init.sh: Hardcode XBMK_CACHE for integrity \n* 265ec0b7673 dependencies/debian: add libx86 \n* 2702a43a86d init.sh: merge xbmk_lock() with xbmk_set_env() \n* fc4006ce877 init.sh: move xbmk_set_version \n* 962902a1c4a init.sh: set pyver from set_env \n* 158c56072c0 init.sh: merge xbmk_mkdirs with set_env \n* 5f022acbf47 init.sh: check version/versiondate once read \n* 485a60e2f6a init.sh: error if version not read \n* 99f09f25ef3 init.sh: only update version files on parent \n* 94437278dc7 init.sh: simplify unknown version creation \n* 6b603b9fbf4 init.sh: only set xbmk version on parent instance \n* ac36ea7f950 init.sh: initialise variables AFTER path \n* 484afcb9196 init.sh: merge create_pathdirs with set_pyver \n* d0bee6b4ebb init.sh: Set python version only on parent \n* 4aa69a7d1f0 init.sh: remove useless command \n* 36ffe6ef501 init.sh: remove useless comment \n* 0343081d905 init.sh: xbmk_create_tmpdir to xbmk_mkdirs \n* c75bc0449d0 init.sh: move gnupath creation to create_tmpdir \n* 253aa81a3f9 init.sh: move PATH init to set_env \n* e05a18d3513 init.sh: check the lock file BEFORE git init \n* cde3b7051e4 init.sh: return from child in set_env instead \n* 7ec9ee42283 inject.sh: shorten the nukemode variable name \n* b48eb161e49 vendor.sh: simplify mksha512sum() \n* ac609d5aae4 vendor.sh: Remove _dest if it's bad \n* a3e1ed9823d release.sh: rename relsrc to rsrc \n* 44df3b2bff8 release.sh: tidy up nuke() \n* 3c58181f69e get.sh: remove useless message \n* 01a0217c1e3 get.sh: simplify bad_checksum() \n* 4ca57943d70 release.sh: simplify nuke() EVEN MORE, yet again \n* 47a3982bbea release.sh: use x_ on find command for nuke() \n* 6dc71cc0246 release.sh: simplify nuke() EVEN MORE \n* 05c07f7401b get.sh: move nuke() to release.sh \n* 587d245cafa release.sh: simplify prep_release_bin() \n* 136bd66c280 mrc.sh: merge extract_mrc with extract_shellball \n* dbe109d7b54 release.sh: don't move src/docs/ \n* 840d6a1d277 get.sh: FURTHER simplify nuke() \n* d2564fd9457 get.sh: simplify tmpclone() \n* 6dea381614d get.sh: fix bad mkdir command \n* 6a2ed9428b7 vendor.sh: Fix broken KBC1126 insertion \n* 4313b474a59 vendor.sh: additional safety check \n* d668f3a3529 vendor.sh: Properly verify SHA512SUM on extraction \n* a191d22bd6d get.sh: add missing eval to dx_ in nuke() \n* c8813c9a144 properly exit 1 when calling fx_ \n* 208dfc89bd5 get.sh: simplify nuke() \n* 46f42291d3c get.sh: fix broken printf statement \n* f29aa9c8d59 get.sh: use subshells on try_ functions \n* e62886dedae get.sh: simplify try_copy() \n* d9ed03f9ea5 get.sh submodules: Don't delete files recursively \n* 8d5475ed5b5 get.sh: simplify fetch_submodules() config check \n* 21867b7d805 get.sh: simplify fetch_submodules() \n* e9fe5a74a2e get.sh: fix caching of crossgcc tarballs \n* 6089716f07c release.sh: Don't run prep_release with fx_ \n* b04c86e5740 git.sh: rename to get.sh \n* 3c23ff4fa18 git.sh: Only create destination repo on success \n* ed8a33d6fb1 git.sh: cleanup \n* 1ca26c5d238 git.sh: Re-implement redundant git downloads \n* e38805a9448 rom.sh: reduce indendation in check_coreboot_utils \n* 6bf24221e60 release.sh: simplify release() \n* 66f7ecdb2d7 release.sh: clean up the vdir after release \n* d4c0479093a release.sh: remove src_dirname variable \n* 6d3a6347c3e release.sh: build in tmp directory first \n* a0105e1ab44 release.sh: remove unnecessary mkdir command \n* f4871da9bca release.sh: split up build_release() \n* c85aff5c54e release.sh: delete tmp/cache from the tarball \n* 92954eeb38f lib.sh: remove rmgit() \n* 05b5914b354 lib.sh: remove mk() \n* c9696e23338 lib.sh: move xbmkget() to git.sh \n* 23913bb8d2a lib.sh: move mksha512sum() to vendor.sh \n* 80f0562e8d1 lib.sh: split up try_file() \n* 89cd828e87c lib.sh: move _ua to try_file() \n* 308a9ab1e17 mrc.sh: minor cleanup \n* 40163dcfa4e mrc.sh: update copyright year to include 2025 \n* ef800b652c8 inject.sh: remove the hashfiles variable \n* 311ae2f8df2 inject.sh: define xchanged here instead \n* 76f81697e6e vendor.sh: remove check_vcfg() \n* 97d4d020d97 vendor.sh: simplify getvfile() \n* 57f896ac016 vendor.sh: simplify setvfile() \n* 3879f6c4d8f lib.sh: use fx_ in rmgit() \n* 0911a5a5aed lib.sh: split up xbmkget() \n* a449afb287f inject.sh: only compile nvmutil if needed \n* 2bbf2ae80b7 inject.sh: simplified serprog check \n* 9c27b7437cf vendor.sh: tidy up variables \n* 0cc816167bb vendor.sh: split up setvfile() \n* 7d90d434252 remove another confusing message \n* a0c436ad4ba inject.sh: Remove confusing path on tar creation \n* dcfd3e632e2 inject.sh: re-add mac address confirmation \n* e5af201060e inject.sh: further cleanup for vendor.sh \n* 0aa99f4bf8b tree.sh: only create elfdir in copy_elf() \n* a8e374020c0 tree.sh: simplified srcdir check on make-clean \n* 0f931b508a8 inject.sh: split to vendor.sh the download parts \n* 3554b5aad9c inject.sh: split up the inject functions \n* 81dbde7e09f lbmk: use x_ instead of err, where appropriate \n* 14d46abceda mrc.sh: operate on refcode in tmp area first \n* 6e521c2e1ea mrc.sh: fix outdated info in the comment \n* 23486abef3a inject.sh: use direct comparison for metmp \n* 91220ce1833 inject.sh: use subshell to speed up find_me() \n* ff33ec3352b mk: use zero exit instead, to run trees \n* c2b627dc6d0 remove useless comment \n* 066402b7e7a mk: remove unnecessary line break \n* 7012c00ed11 mk: re-split tree logic to include/tree.sh \n* 50ce1ac9b22 mk: move release functions to idnclude/release.sh \n* 1ce3e7a3d39 mk: add missing error handli for mk -f \n* 0d876622fcb git.sh: re-write tmpclone without caching \n* 454f11bdd7b git.sh: use setvars for fail variables \n* 6bdb15fd329 git.sh: hard fail if git am fails \n* 93d4eca04ae git.sh: Hard fail if reset fails \n* a3ba8acface init.sh: Only check XBMK_CACHE if it exists \n* 021e7615c84 HP 820 G2: Use fam15h cbfstool tree for refcode \n* fe926052441 also fix the other grub trees \n*   a8594762d27 Merge pull request 'fix trying to boot all logical volumes after unlocking an encrypted volume' (#330) from cqst/lbmk:master into master \n|\\  \n| * e084b06dc76 fix trying to boot all logical volumes after unlocking an encrypted volume \n|/  \n* 2cea8517f3b init.sh: remove useless export \n* 1b0afdcea22 init.sh: also allow XBMK_RELEASE=Y or N \n* 570f1417a80 init.sh: Resolve XBMK_CACHE via readlink \n* e1af1055ed1 init.sh: check XBMK_CACHE is a directory instead \n* e1628ad8f3e init.sh: export LOCALVERSION in set_env \n* 40a944118f2 init.sh: run set_version before set_env \n* cba04aa74b8 init.sh: Use readlink in pybin() \n* a94bd3c0939 inject.sh: simplify extract_kbc1126ec() \n* e3098c61f43 inject.sh: simplified MAC address handling \n* d530e68594d inject.sh: Simplify patch_release_roms() \n* 7f71328f0e2 lib.sh: Remove useless command in err() \n* 394b4ea7a59 inject.sh: rename copytb and preprom functions \n* ec5c954337b lib.sh: Simplified fx_() and removed fe_() \n* 1390f7f8007 mk: Create serprog tarballs here instead \n* 0ef77e65832 build serprog using fe_ *defined inside mkhelper* \n* d2e6f989d7e rom.sh: build serprog images with fe_ \n* 0faef899469 lib.sh: support any command on find_exec() \n* 2b7f6b7d7ce inject.sh: Simplify extract_intel_me_bruteforce() \n* 485d785d331 inject.sh: clean up tmp me file before extract \n* fac99aa2d44 lib.sh: re-add missing break in fe/fx_ \n* 03300766d14 inject.sh: tidy up extract_intel_me_bruteforce \n* 4781dbd2a05 inject.sh: fix oversight in me bruteforce \n* cf78583a6d8 inject.sh: remove unnecessary check \n* 5657cc1afb3 inject.sh: don't use subshell for me bruteforce \n* 5686f35e0f1 inject.sh: insanely optimise the me bruteforce \n* e8be3fd1d41 git.sh: Simplify git am handling \n* 4c1de1ad126 inject.sh: remove unused function \n* 282b939d9da init.sh: New function dx_ to execute path files \n* 73074dedee3 inject.sh: Further simplified FSP extraction \n* 7585336b914 inject.sh: simplify kconfig scanning \n* ef38333f8b0 lib.sh find_ex: Write sort errors to /dev/null \n* c275f35e7e2 lib.sh x_(): Remove warning of empty args \n* 17d826d3a96 lbmk: Replace err with much simpler implementation \n* f98e34a24dd singletree/elfcheck: use fx_, not fe_ \n* 8ca06463ebc rom.sh: Print the rom image path being generated \n* dc9fe517cb0 rom.sh: Safer cprom() \n* 2be8d1c7982 rom.sh: specifically check keymaps in cprom() \n* 89a8cd4936a rom.sh: simplify mkseagrub() \n* c2182d82193 mk: simplify elfcheck() \n* 437ac2454c1 lib.sh: simplify singletree() \n* 62ec3dac075 git.sh: move singletree() to lib.sh \n* 6b247c93e25 mk: Fix bad error handling for gnu_setver \n* ee8bb28ba21 GRUB: Mark E820 reserved on coreboot memory \n* 61ec396ef6d inject.sh: simplify extract_intel_me_bruteforce() \n* e4edc2194d3 inject.sh: Remove unnecessary check \n* f4057d7daab inject.sh extract_intel_me(): reduce indentation \n* b7ca59debe6 inject.sh: Move FSP extraction only to extract_fsp \n* eb882de94cb inject.sh: tidy up intel me handling \n* 153dd76a82e inject.sh: tidy up the deguard command \n* 428c46ca2b1 lib.sh: set -u -e in err() \n* 20c87308587 lib.sh: Provide error message where none is given \n* 35265731c5b init.sh: Silence the output of git config --global \n* 5e3aaa1eb8b init.sh: Run git name/email check before init \n* a3b5626f53d lib.sh: stricter xbmk_err check in err() \n* 51b2a1159d0 lib.sh: simplify err-not-set handling \n* 61e5fd1a0b2 lib.sh: Add warning if x_ is called without args \n* 4020fb43280 lib.sh: simplify err() \n* b51846da6de init.sh: single-quote xbmklock in xbmk_lock() \n* 8b7bd992f66 init.sh: define lock file in a variable instead \n* 9611c19e7ed init.sh: tidy up xbmk_child_exec() \n* 37ca0c90e1c lib.sh err: add missing redirect to stderr \n* 54291ebb720 lbmk: MUCH safer err function \n* 3f7dc2a55f5 lib.sh: rename errx to xmsg \n* 59c94664e3e lib.sh: Make x_ err if first arg is empty \n* 91bb6cbede0 lib.sh: Make err_ always exit no matter what \n* b19c4f8f674 inject.sh: tidy up TBFW handling \n* 439020fbda5 inject.sh: remove useless comment block \n* 6e447876cca init.sh: tidy up the python version check \n* 7392f6fc8ec init.sh: move non-init functions to lib.sh \n* 7acec7a3a1d init.sh: simplify dependencies handling \n* 93ba36ae456 rom.sh: tidy up copyps1bios() \n* fc71e52fdfc mk: tidy up xgccargs handling \n* 184871bc17c mk: remove useless code \n* b6a2dc4ea3c init.sh: tidy up pathdir creation \n* f5b2bdb8868 mk: re-make gnupath/ after handling crossgcc \n* 1b7a9fd637d mk: tidy up check_cross_compiler \n* 488d52e784f mk: re-make gnupath/ for each cross compiler \n* c33467df1e6 mk: reduce indentation in check_cross_compiler() \n* aa4083443b1 mk: Allow use of x_ on prefix functions \n* 8f828e6cd35 mk: tidy up check_project_hashes() sha512sum check \n* 7a2f33264d7 mk: simplify check_gnu_path() \n* 46b968a6e85 inject.sh: minor code cleanup \n* 5499ae66bd8 inject.sh: simplify extract_archive() \n* 72f4412a52d inject.sh: simplified fsp extraction \n* bf569d2b4dc inject.sh: Remove redundant code in copy_tbfw \n* 8de0ed811fb inject.sh: Stricter TBFW handling \n* 530e4109a2b init.sh: *Re-create* tmpdirs on parent instance \n* 498f5a26cc8 init.sh: Always create xbmklocal \n* 00d22f20829 lbmk: Unified local ./tmp handling \n* 0f7b3691aba lib.sh: redirect find errors to /dev/null \n* 7fadb17fd9e lib.sh: Fix bad touch command \n* 0b09d970732 inject.sh: Only build nvmutil once \n* 308df9ca406 inject.sh: always re-build nvmutil \n* 44a1cc9ef85 util/nvmutil:  use x, not ?, for random characters \n* a17875c3459 lib.sh find_ex: explicitly create the tmp file \n* 0ffaf5c7331 init.sh: Explicitly create the xbmktmp directory \n* fcc52b986e7 init.sh: unified handling of ./tmp \n* 47762c84ad0 lib.sh: add fe_ which is fx_ but err on find \n* d18d1c2cae2 lbmk: unified execution on find commands \n* 773d2deaca0 NEW MAINBOARD: Dell Precision T1700 SFF and MT \n* 9b11e93686c mk: include rom.sh directly \n* 1f7e4b35cb2 mk: Download vendorfiles before building release \n* acb0ea202f2 lib.sh: Simplify rmgit() \n* 15b76bc202f lib.sh: support multiple arguments in remkdir() \n* f3ae3dbbbe4 lib.sh: simplify remkdir() \n* 6c4d88f2686 move x_() to lib.sh \n* 2ae565ba93a init.sh: move setvars/err_ to lib.sh \n* c073ee9d4fc Restore SeaBIOS 9029a010 update, but with AHCI fix \n* 8245f0b3211 Revert \"seabios: bump to rev 9029a010, 4 March 2025\" \n* 4c50157234d coreboot/t420_8mb: add missing txtmode config \n* f21749da8b1 Libreboot 25.04 Corny Calamity \n* bb5f5cd5763 add pico-sdk backport patch fixing gcc 14.x \n* 4f77125066d coreboot/fam15h: update submodule for nasm \n* 0f2202554ab coreboot/fam15h: update nasm to 2.16.03 \n* 2009c26f0aa serprog: Remove pico2 support for the time being \n* a08b8d94fc5 seabios: bump to rev 9029a010, 4 March 2025 \n* 342eca6f3d1 update untitled \n* b0a6d4711a3 coreboot413: add alper's fix to cbfstool for gcc15 \n* 628ae867c9a flashprog: bump to rev e060018 (1 March 2025) \n* 5e96db5a2b4 further gcc-15 fix for gmp on -std=23 \n* 9a9cd26b2d5 coreboot/default and fam15h: gmp fix, gcc15 hostcc \n* 80007223c85 lib.sh: Provide printf for mktarball \n*   a16c483e5fd Merge pull request 'coreboot: fam15h: Add patches to fix build with GCC 15 as host compiler' (#318) from alpernebbi/lbmk:coreboot-fam15h-gcc15 into master \n|\\  \n| * 685685ab0e4 coreboot: fam15h: Add patches to fix build with GCC 15 as host compiler \n|/  \n*   02110f2bc1d Merge pull request 'coreboot: Add patch to fix build with GCC 15 as host compiler' (#317) from alpernebbi/lbmk:coreboot-gcc15-nonstring into master \n|\\  \n| * 5ad1de3931a coreboot: Add patch to fix build with GCC 15 as host compiler \n|/  \n*   9e7bceb7fa9 Merge pull request 'seabios: Fix malloc_fn function pointer in romfile patch' (#313) from alpernebbi/lbmk:seabios-romfile-malloc-fptr into master \n|\\  \n| * 35c853f8b33 seabios: Fix malloc_fn function pointer in romfile patch \n* |   686e136f150 Merge pull request 'dependencies/debian: Fix libusb package name' (#315) from alpernebbi/lbmk:debian-libusb-dependency into master \n|\\ \\  \n| * | 6f120f01588 dependencies/debian: Fix libusb package name \n| |/  \n* / d8b0e749983 init.sh: fix yet another double quote for dotfiles \n|/  \n*   780844112ae Merge pull request 'Update U-Boot to v2025.10' (#305) from alpernebbi/lbmk:uboot-v2025.04 into master \n|\\  \n| * 1265927ca38 u-boot: gru: Disable INIT_SP_RELATIVE \n| * 5bea1fade9a u-boot: arm64: Expand our modified defconfigs to full configs \n| * fd56d8ada13 u-boot: arm64: Merge our modifications into new defconfigs \n| * ed9ddd7415f u-boot: arm64: Add new upstream defconfigs \n| * b1fa44858cb u-boot: arm64: Rebase to v2025.04 \n| * 976fc6890ae u-boot: arm64: Save our modifications to the upstream defconfigs \n| * 418570a6172 u-boot: arm64: Turn configs into defconfigs \n|/  \n* 093a86d9c09 init.sh: don't use eval to read version files \n* 3045079947b init.sh: use backslash for dotfiles in eval \n* da108d1c045 mk: Don't run mkhelpers if mode is set \n* 71a58a38ab4 mk: condense main() again \n* f3882b9bf21 init.sh: make git name/email error more useful \n* 9cebda333d5 init.sh: move git name/mail check to xbmk_git_init \n* ea081adc4ca init.sh: tidy up the git name/email check \n* 3292bded692 mk: make main() more readable \n* 97a5e3d15ed mk: move git check to init.sh xbmk_set_version \n* 11cd952060d init.sh: tidy up xbmk_init() \n* f6c5c8d396d mk: move git_init to init.sh \n* ec1c92238cc init.sh: minor cleanup \n* e009f09e7fa init.sh: clean up setvars \n* 9ec72153408 init.sh setvars: make err a printf for eval \n* 18ad654a1f7 init.sh: merge xbmk_child_init with xbmk_init \n* 15268202478 init.sh: split xbmk_child_init into functions \n* 0280cd4c0e7 init.sh: move parent fork to new function \n* a0e1d42ff74 init.sh: Provide more complete error info \n* a8f0623efbb update uefitool to rev a072527, 26 Apr 2025 \n* c698972130f rename include/vendor.sh to inject.sh \n* 24e488aae56 lib.sh: move _ua to the xbmkget function \n* 6779d3f9915 move variables out of init.sh to others \n* 848159fa0eb lib.sh: rename vendor_checksum \n* 1de77c6558c lib.sh: move singletree() to git.sh \n* 703fe444312 lib.sh: move cbfs() to rom.sh \n* b57952e90d2 re-split include/init.sh to lib.sh \n* 8ecb62c6628 rename include/lib.sh to init.sh \n* ce4381169fa lib.sh: introduce more top-down function order \n* 15b64cfebe8 mk/git.sh: remove tree_depend variable \n* 9b8179c0e5d git.sh: remove unused xgcc linking feature \n* 4624c6e536c mk: remove unused variables (ser/xp) \n* aba5b3a3532 mk: simplify main() \n* 0ab7c6ff9cf lib.sh: use realpath to get sys python on venv \n* 8edea026c58 lib.sh: Force use of System Python to prevent hang \n* b1b964fa5c3 lib.sh: further condense the python check \n* 9543a325acb lib.sh: further simplify the python check \n* 9baabed7186 lib.sh: condense the python check \n* 0c5c5ffc873 lib.sh: simplify mk() \n* 83022b6ba83 lib.sh: simplify cbfs() \n* 13ad839691d lib.sh: simplify the python check \n* b1ea4165754 mk: remove mkhelp() and use x_() instead \n* 4cf64e59ed0 mk: simplify handling of trees() \n* d0581914c74 coreboot/hp8300cmt: purge xhci_overcurrent_mapping \n* cb52fc4ba82 Fix VBT path on HP Elite desktops \n* 2bee87cfc26 lib.sh: add missing copyright year \n* 4b7ab403c65 ifd/q45t_am: unlock regions by default \n* 564155277ea coreboot/g43t_am3: use ifd-based setup \n* 0ddd1963751 coreboot/q45t_am3: use ifd-based setup \n* 3b2d933842a coreboot/default: add missing submodules \n* a10d81399c7 NEW MAINBOARD: Acer Q45T-AM (G43T-AM3 variant) \n* d114e0a765c mk: don't print confirmation of git pkg.cfg \n* f59c24f12aa coreboot/g43t_am3: fix data.vbt path \n* 21020fa319a add missing config/data/coreboot/0 \n*   2b4629d790b Merge pull request 'lib.sh: Fix python3 detection when 'python' is python2' (#290) from alpernebbi/lbmk:python3-detection-fix into master \n|\\  \n| * a18d287a81e lib.sh: Fix python3 detection when 'python' is python2 \n|/  \n* c7569a67145 coreboot/next: merge with coreboot/default \n* 762c7ff43eb coreboot/default: Update, c247f62749b (8 Feb 2025) \n* 86e7aa80c51 Update the GRUB revisions \n* 8d57bf6009e Revert \"git.sh: minor cleanup\" \n* a2898771f6e lib.sh: perform root check even earlier \n* 779f6003421 lib.sh: tidy up opening logic (put it together) \n* bac4be99c20 lib.sh: do root check before python check \n* e63d8dd20d9 git.sh: minor cleanup \n* 11078508a25 lib.sh: simplify mktarball() \n* 087bbedc5f8 vendor.sh: tidy up vendor_download() \n* e11fd52d958 mk: tidy up check_gnu_path() \n* 3442f4278ed mk: simplify check_project_hashes() \n* 6b6a0fa607c lib.sh: fix missing s/TMPDIR/xbmktmp \n* e07a2adb130 lbmk: don't handle TMPDIR directly \n* 9d3b52cd1d2 rom.sh: minor cleanup \n* b4402c54258 vendor.sh: yet even more code cleanup \n* fe5bdc7633d vendor.sh: even more cleanup \n* fcedb17a9a1 vendor.sh: more cleanup \n* 4e2b59ed3ff vendor.sh: minor cleanup \n* a3acf4c3f95 vendor.sh: simplify process_release_roms \n* 30213a96883 vendor.sh: remove unnecessary check \n* 38df7275f12 git.sh: remove unnecessary comment \n* f5891fb6991 git.sh: remove link_crossgcc() \n* a685654b90f git.sh: remove move_repo() \n* e4aa62f79a8 git.sh: remove prep_submodule() \n* 2839feb9e43 git.sh: make git_prep command clearer \n* 410fa702c9c mrc.sh: Make proper use of variable inside printf \n* 075902c3ea7 simplify a few file checks \n* b2255425eba rom.sh: remove unnecessary check \n* 39640d76a75 lbmk: minor cleanup \n* c8dc701f3eb lib.sh mktarball: stricter tar error handling \n* 58a53d7046f vendor.sh: don't err on bruteforce me extract \n* 958fa34832a mk check_project_hashes: handle error on sha512sum \n* 8b4b069e3f6 vendor.sh: remove unnecessary xchanged=\"y\" \n* 166dbb04c92 vendor.sh: set need_files=\"n\" if skipping patch \n* e90657cc734 vendor.sh: Don't handle vendor files if not needed \n* 2e10a45fa36 Revert \"lib.sh: use eval for the command in x_\" \n* 738d4bb6b6d lib.sh: fix bad eval writing resized file \n* eb9e5d2d5d4 lib.sh: fix bad eval writing version/versiondate \n* 3bfdecdc75b lib.sh: use eval for the command in x_ \n* 4fa3bb9e5b1 mk: use eval to run mkhelp commands \n* 9b3635718a8 mk: tidy up the switch/case block in main() \n* 0c381028abc mk: tidier error handling \n* 023f9cf0498 lib.sh: tidy up the error handling \n* cb3253befb9 rom.sh: tidy up error handling \n* 7af46721bcb vendor.sh: tidy up error handling \n* 04ebb3b91a0 vendor.sh: tidy up decat_fspfd() \n* 0c87fdf96ad git.sh: clean up fetch_project() \n* 9eb8856b3c5 mk: Remove unnecessary argument checks on trees() \n* 52f3d54116f vendor.sh: properly call err_ in fail_inject \n* c4c6692b761 remove xbmk_parent, handle forking in lib.sh \n* fd5431db05d lib.sh: define x_ right after err_ \n* 972681a127b mk: minor cleanup \n* b41cd39b686 lib.sh: minor cleanup \n* 49939502648 mrc.sh: minor cleanup \n* c158d82298b rom.sh: minor cleanup \n* cb36248c8c0 vendor.sh: tidy up check_release() \n* 409cab39c56 vendor.sh: tidy up vendor_inject() \n* 12b1623e473 vendor.sh: tidy up readcfg() \n* 0d85f061e2e vendor.sh: tidy up patch_release_roms() \n* 61f20141028 vendor.sh: tidy up process_release_roms() \n* 5901f36e49d vendor.sh: tidy up patch_rom() \n* 082930ce0e7 vendor.sh: tidy up inject() \n* e1f91f30372 vendor.sh: tidy up modify_mac_addresses() \n* 3181ac50126 script/trees: merge with mk and delete script/ \n* 3d03dd1a507 mk: remove the legacy \"roms\" command \n* f0c629dcc6c lib.sh: write version/versiondate to dotfiles \n* 23b942c83e9 lib.sh: hardcode projectname/projectsite \n* a03bb793aea remove update/vendor symlinks \n* d7f80ebe71e move build to mk \n* 57d58527fd0 trees: unify the execution of mkhelper commands \n* e5262da4be7 trees: tidy up configure_project() \n* 51798278397 build: make coreboot building an else in \"roms\" \n* c189257888a trees: don't build dependencies if dry=\":\" \n* 115a66fddd3 trees: unified handling of flags \n* 3ea633cc791 trees: simplified handling of badhash/do_make \n* 9be40e94a2b trees: don't set mode on ./mk -b \n* 67ad7c2635c trees: don't set mod on ./mk -d \n* 24448948419 trees: don't initialise mode to \"all\" \n* 97c50a39a60 trees: clean up some comments \n* cfb14fd8dd8 vendor.sh: simplified readkconfig() \n* 5b697b93a2d lib.sh: double-quote pwd to prevent globbing \n* 5a0a24f5559 lbmk: unified PWD handling (work directory) \n* a25a29cfbb7 lib.sh: initialise PATH if it's unset \n* 1022abf6991 move XBMKPATH to include/lib.sh \n* 0764c969a29 lbmk: use pwd util, not PWD environmental variable \n* f98b9b01107 clean up a few semicolons in the build system \n* 8ccb61cc718 trees: err if first argument is not a flag \n* 947c3e1a176 trees: err if no argument given \n* edbbde0b12d trees: set dry=\":\" on ./mk -f \n* 33bb0ecf764 trees: clean up initialisation of the dry variable \n* c7636ff1dfc trees: initialise mode to \"all\", not \"\" \n* d0bd12631a6 trees: don't abuse the mode variable on -f \n* c4cd876c609 trees: Add missing flag to error output \n* 5ebcae5235f lbmk: minor code formatting cleanup \n* 70cef71dbab grub/xhci: Remove unused patch \n* 3f14a470a2e remove _fsp targets (keep _vfsp) \n* d7312260e7e util/nvmutil: remove excessive comments \n* e348ea0381a Bump GRUB revision to add 73 security patches \n*   4b228c11f9f Merge pull request 'Update pico-serprog revision' (#271) from Riku_V/lbmk:master into master \n|\\  \n| * a8359e30b27 Update pico-serprog revision \n|/  \n* d2cb954933b util/nvmutil: Fix bad error messages on R/W \n* e1e515bd22a util/nvmutil: hardened pledge on help output \n*   ada057a865c Merge pull request 'Simplify the README' (#269) from runxiyu/lbmk:readme-simplification into master \n|\\  \n| * 9ced146b47c README.html: Use newlines instead of bulleted list for docs/support links \n| * 266122592cd README.html: Use the EFF's page on Right to Repair \n| * e36aa8c5a5c README.html: Vastly simplify it \n| * c17f4381ce5 README.html: Mention SeaBIOS and U-Boot instead of Tianocore as payloads \n|/  \n*   47eb049cb47 Merge pull request 'deps/arch: genisoimage belongs to cdrtools' (#267) from runxiyu/lbmk:master into master \n|\\  \n| * fa9a0df2458 deps/arch: genisoimage belongs to cdrtools \n|/  \n* a98490573be util/nvmutil: only set mac_updated at the end \n* 6b9cf09ca21 restore old x230 gbe file \n* 8a435355135 util/nvmutil: Fix bad comparison \n* a65a0c2f963 util/nvmutil: allow ./nvm gbe MAC \n* 96356ce94f6 util/nvmutil: move \"e\" to swap() \n* b1d8975959d util/nvmutil: Only read up to 4KB on larger gbe \n* 6821659bcb2 util/nvmutil: fix minor mistake (line break) \n* 3bb7520f6d9 util/nvmutil: do setmac if only filename given \n* d94b274fd9f vendor.sh: don't error if grep -v fails \n* 6ebdd3c72ba vendor.sh: Don't show gbe filename on inject \n* a08748a9eda util/nvmutil: don't say write not needed if errno \n* 6841a351ebc util/nvmutil: print dump *after* modification \n* da0a6c216cf util/nvmutil: verbosely print the written MAC \n* db5879c6b5a util/nvmutil: minor cleanup in cmd_dump \n* bd7215d1eb7 util/nvmutil: show nvm words written on writeGbe \n* c70117c79c4 util/nvmutil: clean up readonly check on writeGbe \n* cf5a63e65ca util/nvmutil: Remove useless gbeFileChanged var \n* 83601aa524b util/nvmutil: reset errno if any MAC updated \n* 3e86bf5ce25 util/nvmutil: reset errno when writing a MAC \n* bcf53cc2cc0 util/nvmutil: show total number of bytes read \n* c91cc329cf8 util/nvmutil: rename tbw/bw to tnw/nw \n* 90607108330 util/nvmutil: err if bytes read lower than nf \n* c72f699d368 util/nvmutil: err if fewer bytes written \n* d666f67ebe5 util/nvmutil: Show bytes written in writeGbe \n* b2d6393ed5f util/nvmutil swap(): ensure that no overflow occurs \n* 063fef14d34 util/nvmutil: make swap() a bit clearer \n* fd1bbdc96cb util/nvmutil: make 0x3f checksum position a define \n* 5ddf7f251d6 util/nvmutil: make 128 (nvm area) a define \n* 8850acc7da6 util/nvmutil swap(): Only handle the nvm area \n* 49506a88328 util/nvmutil: move write checks to writeGbe \n* 948377b0e7e util/nvmutil: make cmd_swap its own function again \n* 6e134c9f4bf util/nvmutil: minor cleanup \n* 98e105ac4f1 util/nvmutil: allocate less memory for setchecksum \n* 52e8ea57f7b util/nvmutil: Further reduce memory usage \n* 7a7d356824e util/nvmutil: Remove unnecessary buf16 variable \n* cdf23975bc1 util/nvmutil: Only allocate needed memory for file \n* ed45da9cae5 util/nvmutil: Remove unnecessary buffer \n* ec3148dc3b5 util/nvmutil: Show specific error for bad cmd argc \n* 073420d3056 util/nvmutil: cleaner argument handling \n* a6c18734e70 util/nvmutil: extreme pledge/unveil hardening \n* deb307eaf63 util/nvmutil: more minor cleanup \n* c14eccaf153 util/nvmutil: more granular MAC parsing errors \n* 88fb9cc90ea util/nvmutil: more cleanup \n* 5aaf27f80c3 remove errant comment in nvmutil \n* c829b45c17c util/nvmutil: support 16kb and 128kb gbe files \n* a98ca5bf65c util/nvmutil: Prevent unveil allowing dir access \n* 68c32034a00 typo: nvme should say nvm in nvmutil.c \n* c944c2bbac7 util/nvmutil: General code cleanup \n* 8c65e64e398 snip \n* f666652fe15 snip \n* 64d3c7b5150 grub/xhci: Add xHCI non-root-hub fixes from Nitrokey \n* 7bf0d4c2ed5 add gnults-devel to fedora 41 dependencies \n* 66d084e7f7c grub.cfg: scan luks *inside lvm* \n* 5a3b0dab966 grub.cfg: Scan *every* LVM device \n* 3c9f4be76f6 Libreboot 20241206, 8th revision \n* d4cc94d6b44 rom.sh: don't run mkpicotool on dry builds \n* de6d2f556f1 pico-sdk: Import picotool as a dependency \n* 4210ee68ea2 lib.sh: Much safer python version check \n* 8c7ba6131cc coreboot/next uprev: Fix T480 backlight keys \n* 411fb697dfc set up python in PATH, ensuring that it is python3 \n* e8336bcc3ca vendor.sh: Proper semantics on prefix file names \n* 63f45782638 vendor.sh: Confirm if need_files=n \n* 13b06ae130f vendor.sh: Allow restoring the default GbE file \n* ab8feff92e0 vendor.sh: set random MAC address *by default* \n* 0ceaa01d45d vendor.sh: add clarification to nogbe warning \n* 4d5caf1dcfc vendor.sh: check that the vcfg file exists \n* fc4ee88e167 vendor.sh: error out if nuking failed \n* 8819a93d89b add line break, part 3 \n* 8ce1a00f517 add line break, part 2 \n* bc2c14e76a8 add line break \n* c762850311a vendor.sh: prevent double-nuke \n* 68299ad05ca vendor.sh: much more verbose errors/confirmation \n* b8e6d12f3d9 add libx86 to arch dependencies \n* cf8ad497b4e vendor.sh: Remove unnecessary return \n* c858099b359 vendor.sh: Download utils even if vcfg unset \n* ce16856a242 vendor.sh: Allow setmac if vendorfiles not needed \n* 4b51787d078 add less to arch dependencies \n* 8bd028ec153 lib.sh: Set python after dependencies \n* 44b6df7c24c update my copyright years on modified scripts \n* 818f3d630c2 vendor.sh: Don't error if vcfg is unset \n* 432a1a5bca7 lib.sh: Fix unescaped quotes in chkvars() \n* a73b0fd910a Revert \"fix more unescaped quotes in eval\" \n* ec6bcc1fba5 fix more unescaped quotes in eval \n* 5284f20b981 fix ./mk dependencies build issue \n* d825f9a9683 rom.sh: Remove errant GRUB modules check \n* 4149f3dc81a submodule/grub: use codeberg for 1st gnulib mirror \n* 0305975e705 util/nvmutil: Update AUTHORS and COPYING files \n* 20b192e13bd util/nvmutil: Describe nvmutil in help output \n* d1ca21628cb util/nvmutil: Remove the correct binary on uninstall \n* e63fe256dfc util/spkmodem-recv: More correct Makefile \n* efd50ee548b util/nvmutil: Honour the INSTALL variable \n* 8008838abbc util/nvmutil: Don't clean when doing uninstall \n* 982f257f58a util/nvmutil: Proper DESTDIR/PREFIX handling \n* 3f85ae5f853 util/nvmutil: Set CC and CFLAGS only if unset \n* 2c7b9fb9412 util/nvmutil: Capitalise BABA \n* 57f9906f6d1 util/nvmutil: Add uninstall to Makefile \n* 4defe2c6085 util/nvmutil: Add distclean to Makefile \n* 033e4cd9d50 util/nvmutil: Make the GbE checksum a define \n* 874317c4e59 util/nvmutil: nicer hexdump display \n* a338e585eed util/nvmutil: show the correct hexdump order \n* b032e483ef1 lib.sh mktarball: cleaner if statement \n* 0cf58c22734 fix lbmk shellcheck errors \n* 8276560cc99 lib.sh and rom.sh: update my header \n* 08e86d2218c vendor.sh inject: reset err upon return \n* 41275d699ca vendor.sh: MUCH, MUCH, MUCH safer ./mk inject \n* ed7293494e3 util/nvmutil: Obey the 79-character per line limit \n* 637b5e36fd2 util/nvmutil: Tidy up copyright header \n* cd28db883e2 vendor.sh: fix comment \n* 57971ceb227 util/nvmutil: Fix another straggler \n* 15b37b2a1ab util/nvmutil: Tidy up pledge calls \n* e8799310db2 hp820g2: fix vendorfile inject and set release=y \n* f9ab082ec19 fedora41/dependencies: add libuuid-devel \n* 661591f9f0b add uuid-devel to fedora41 dependencies \n* 1a46c047386 support ./mk dependencies fedora reinstall \n* d58d63569f1 fix missing semicolon in grub nvme patch \n* 95ea3293df5 bump seabios to rev 1602647f1 (7 November 2024) \n* 6d7e6c361b3 Bump GRUB revision to 6811f6f09 (26 November 2024) \n* 09a01477df6 t480/3050micro: force power off post power failure \n* d344cd95eac flashprog: Disable -Werror \n* dc95e912bfe bump flashprog to revision eb2c041 (14 Nov 2024) \n* 27c8c1c16ba replace liblz4-tool with lz4 and liblz4-dev \n* d3a732a64db lib.sh dependencies: support --reinstall argument \n* 466ada423dd move xbmkpath to XBMK_CACHE/ \n* b0a23840327 Revert \"Remove legacy update/vendor commands\" \n* 3d7dd4aa9fe Fix U-Boot build issue with Swig 4.3.0 \n* 0c810747469 use command -v instead of which \n* 6c7e3ce2d6e trees: remove unnecessary subshell \n* ad137eae89d trees: only symlink host gcc/gnat to build xgcc \n* cfb6de94c33 trees: correction on check_gnu_path \n* ec2f0716662 trees: match gcc/gnat versions both ways \n* f64b5996279 Merge path.sh into script/trees \n* 295463d281e path.sh: Further cleanup \n* 5b24e0a5a96 path.sh: More thorough gcc/gnat version check \n* 7849a075886 path.sh: minor cleanup \n* 17168a87dbf path.sh: remove unnecessary shebang \n* e565df94fd7 Fix globbing issue in lbmk \n* c80cc0a00b6 remove auto-confirm on distro dependencies \n* 01fc65a0a9d Mitigate Debian Trixie/Sid GCC/GNAT version mismatch \n* 424b0c7103b t480/3050micro: disable hyperthreading \n* 603105f3b4e t480/t480s: Disable TPM2 to mitigate SeaBIOS lag \n* 754bd1e6ca3 rom.sh: Name pico directory serprog_pico \n* db22308eba5 add 2024 to Riku's copyright header on rom.sh \n*   4fa5f696db8 Merge pull request 'rp2530' (#258) from Riku_V/lbmk:rp2530 into master \n|\\  \n| * a5e0360992d pico-sdk: update to 2.1.0 \n| * e2f8cc7f3ee pico-serprog: enable building for multiple pico chips \n|/  \n* ccc2b4d589f add spdx headers to dependencies configs \n* a3969701e6b dependencies/debian: fix debian sid \n* 8f370cb60d9 add spdx headers to various config files \n* d591ea4c5dc git.sh: don't initialise livepull globally \n* b5da9feba3b vendor.sh: Print useful message on ./mk inject \n* 12c6259cb2f vendor.sh: Handle FSP insertion post-release \n* 78132051462 Remove legacy update/vendor commands \n* 07037561bd6 lbmk: remove use of deprecated ./vendor command \n* 5d1f1823067 vendor.sh: Safer exit when vendorfiles not needed \n* a18175a5df9 data/deguard: Remove unused patch \n* ee8f53b96ff lib.sh: Safer exit from ./mk dependencies \n* a8b35c88cf1 remove geteltorito and mtools from lbmk \n* 1dd32ea5487 rom.sh: support grub-first setups \n* f7801ef4770 vendor.sh: delete old tb.bin first, just in case \n* 02cbf8a729d vendor.sh: make TBFW pad size configurable \n* 9884e5ed1b0 T480/T480S: Support fetching ThunderBolt firmware \n* 36b42dd1c11 also de-rainbow the u-boot menu \n* eafc82028a4 Revert \"use rainbow deer on the grub background\" \n* 44969c73bd2 rom.sh: insert grub background in cbfs not memdisk \n* 401efb24b22 use rainbow deer on the grub background \n* dc27cb91784 add some scripts to .gitignore \n* 3b6b283eabe disable 3050micro nvme hotplug \n* c2023921893 fix t480 spd size (512, not 256) \n* da527459b68 add tarballs and signatures to gitignore \n* b910424b5df fix another very stupid mistake \n* e3b77b132e6 fix the stupidest bug ever </code></pre><p>This is about 650 changes.</p><p>When certain bugs are found, releases may be re-built and re-uploaded. When this happens, the original release is replaced with a .</p><p>Revisions are numbered; for example, the first post-release revision is .</p><p>No revisions, thus far. The original 25.06 release is the current revision, so it could be considered  (revision zero).</p><p>This release was built on the latest Debian 12.10 Bookworm release, as of this day. It was also build-tested successfully on the latest Arch Linux updates as of 26 June 2025.</p>","contentLength":79134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lpbbwj/libreboot_2506_released/"},{"title":"A Pro-Russia Disinformation Campaign Is Using Free AI Tools to Fuel a ‚ÄòContent Explosion‚Äô","url":"https://www.wired.com/story/pro-russia-disinformation-campaign-free-ai-tools/","date":1751398321,"author":"/u/wiredmagazine","guid":179309,"unread":true,"content":"<p> campaign is leveraging consumer artificial intelligence tools to fuel a ‚Äúcontent explosion‚Äù focused on exacerbating existing tensions around global elections, Ukraine, and immigration, among other controversial issues, according to <a data-offer-url=\"https://checkfirst.network/wp-content/uploads/2025/06/Overload%C2%A02_%20Main%20Draft%20Report_compressed.pdf\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://checkfirst.network/wp-content/uploads/2025/06/Overload%C2%A02_%20Main%20Draft%20Report_compressed.pdf&quot;}\" href=\"https://checkfirst.network/wp-content/uploads/2025/06/Overload%C2%A02_%20Main%20Draft%20Report_compressed.pdf\" rel=\"nofollow noopener\" target=\"_blank\">new research published last week</a>.</p><p>The campaign, known by many names including <a data-offer-url=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election&quot;}\" href=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" rel=\"nofollow noopener\" target=\"_blank\">Operation Overload</a> and <a href=\"https://www.wired.com/story/kremlin-backed-accounts-trying-to-destroy-yulia-navalnaya/\">Matryoshka</a> (other researchers have also tied it to <a data-offer-url=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/&quot;}\" href=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" rel=\"nofollow noopener\" target=\"_blank\">Storm-1679</a>), has been operating since 2023 and has been aligned with the Russian government by multiple groups, including <a data-offer-url=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/&quot;}\" href=\"https://blogs.microsoft.com/on-the-issues/2024/06/02/russia-cyber-bots-disinformation-2024-paris-olympics/\" rel=\"nofollow noopener\" target=\"_blank\">Microsoft</a> and the <a data-offer-url=\"https://www.isdglobal.org/digital_dispatches/stolen-voices-russia-aligned-operation-manipulates-audio-and-images-to-impersonate-experts/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.isdglobal.org/digital_dispatches/stolen-voices-russia-aligned-operation-manipulates-audio-and-images-to-impersonate-experts/&quot;}\" href=\"https://www.isdglobal.org/digital_dispatches/stolen-voices-russia-aligned-operation-manipulates-audio-and-images-to-impersonate-experts/\" rel=\"nofollow noopener\" target=\"_blank\">Institute for Strategic Dialogue</a>. The campaign disseminates false narratives by impersonating media outlets with the apparent aim of sowing division in democratic countries. While the campaign targets audiences around the world, <a data-offer-url=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election&quot;}\" href=\"https://www.recordedfuture.com/research/operation-overload-impersonates-media-influence-2024-us-election\" rel=\"nofollow noopener\" target=\"_blank\">including in the US</a>, its main target has been Ukraine. Hundreds of AI-manipulated videos from the campaign have tried to fuel pro-Russian narratives.</p><p>The report outlines how, between September 2024 and May 2025, the amount of content being produced by those running the campaign has increased dramatically and is receiving millions of views around the world.</p><p>In their report, the researchers identified 230 unique pieces of content promoted by the campaign between July 2023 and June 2024, including pictures, videos, QR codes, and fake websites. Over the last eight months, however, Operation Overload churned out a total of 587 unique pieces of content, with the majority of them being created with the help of AI tools, researchers said.</p><p>The researchers said the spike in content was driven by consumer-grade AI tools that are available for free online. This easy access helped fuel the campaign‚Äôs tactic of ‚Äúcontent amalgamation,‚Äù where those running the operation were able to produce multiple pieces of content pushing the same story thanks to AI tools.</p><p>‚ÄúThis marks a shift toward more scalable, multilingual, and increasingly sophisticated propaganda tactics,‚Äù researchers from Reset Tech, a London-based nonprofit that tracks disinformation campaigns, and Check First, a Finnish software company, wrote in the report. ‚ÄúThe campaign has substantially amped up the production of new content in the past eight months, signalling a shift toward faster, more scalable content creation methods.‚Äù</p><p>Researchers were also stunned by the variety of tools and types of content the campaign was pursuing. \"What came as a surprise to me was the diversity of the content, the different types of content that they started using,‚Äù Aleksandra Atanasova, lead open-source intelligence researcher at Reset Tech, tells WIRED. ‚ÄúIt's like they have diversified their palette to catch as many like different angles of those stories. They're layering up different types of content, one after another.‚Äù</p><p>Atanasova added that the campaign did not appear to be using any custom AI tools to achieve their goals, but were using AI-powered voice and image generators that are accessible to everyone.</p><p>While it was difficult to identify all the tools the campaign operatives were using, the researchers were able to narrow down to one tool in particular: Flux AI.</p><p>Flux AI is a text-to-image generator developed by Black Forest Labs, a German-based company founded by former employees of Stability AI. Using the SightEngine image analysis tool, the researchers found a 99 percent likelihood that a number of the fake images shared by the Overload campaign‚Äîsome of which claimed to show Muslim migrants rioting and setting fires in Berlin and Paris‚Äîwere created using image generation from Flux AI.</p>","contentLength":3543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lpb1z5/a_prorussia_disinformation_campaign_is_using_free/"},{"title":"How to safely change StorageClass reclaimPolicy from Delete to Retain without losing existing PVC data?","url":"https://www.reddit.com/r/kubernetes/comments/1lpavii/how_to_safely_change_storageclass_reclaimpolicy/","date":1751397896,"author":"/u/kiroxops","guid":179361,"unread":true,"content":"<p>Hi everyone, I have a StorageClass in my Kubernetes cluster that uses reclaimPolicy: Delete by default. I‚Äôd like to change it to Retain to avoid losing persistent volume data when PVCs are deleted.</p><p>However, I want to make sure I don‚Äôt lose any existing data in the PVCs that are already using this StorageClass.</p>","contentLength":314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Logging to HTTP vs Syslog","url":"https://www.reddit.com/r/kubernetes/comments/1lpafer/logging_to_http_vs_syslog/","date":1751396853,"author":"/u/Stock_Wish_3500","guid":180797,"unread":true,"content":"<div><p>Can someone explain to me pros and cons of using HTTP vs syslog for logging sidecar? I understand that HTTP is higher overhead, but should I be choosing one specifically over another if I want to use it for logging stdout/stderr for infra.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Stock_Wish_3500\"> /u/Stock_Wish_3500 </a>","contentLength":277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A virtual pet site written in Rust, inspired by Neopets - 2 years later!","url":"https://www.reddit.com/r/rust/comments/1lp9f6g/a_virtual_pet_site_written_in_rust_inspired_by/","date":1751394570,"author":"/u/lemphi","guid":180375,"unread":true,"content":"<p>Just about two years ago <a href=\"https://old.reddit.com/r/rust/comments/15c0j97/a_virtual_pet_site_written_in_rust_inspired_by/\">I posted here</a> about how I was looking to get better with Rust and remembered how much I liked Neopets as a kid, so I ended up making a virtual pet site in Rust as a fun little project! Well, I've still been working on it daily ever since then, and it's not quite so little anymore, so I thought I'd provide an update here in case anyone was curious about how everything's going.</p><p>It uses Rust on the backend and TypeScript on the frontend. The only frontend dependencies are TypeScript, Solid JS, and mutative. The backend server runs on a $5 / month monolithic server and mostly uses axum, sqlx, Postgres, strum, tokio, tungstenite, rand, and oauth2.</p><p>I've also really optimized the code since then. Previously, user requests would use JSON, but I've since migrated to binary websockets. So now most requests and responses are only a few bytes each (variable length binary encoding).</p><p>I also wrote some derive macro crates that convert Rust data types to TypeScript. So I can annotate my Rust structs / enums and it generates interface definitions in TypeScript land for them, along with functions to decode the binary into the generated interface types. So Rust is my single source of truth (as opposed to having proto files or something). Some simple encoding / decoding crates then serialize my Rust data structures into compact binary (so much faster and smaller than JSON). Most of the data sent (like item IDs, quantities, etc.) are all small positive integers, and with this encoding protocol each is only 1 byte (variable length encoding).</p><p>So now I can write something like this:</p><pre><code>#[derive(BinPack, FromRow, ToTS, DecodeTS)] pub struct ResponseProfile { pub person_id: PersonID, pub handle: String, pub created_at: UnixTimestamp, pub fishing_casts: i32 } </code></pre><p>and the following TypeScript is automatically generated:</p><pre><code>export interface ResponseProfile { person_id: PersonID; handle: string; created_at: UnixTimestamp; fishing_casts: number; } export function decodeResponseProfile(dv: MyDecoder): ResponseProfile { return { person_id: decodePersonID(dv), handle: dv.getStr(), created_at: decodeUnixTimestamp(dv), fishing_casts: dv.getInt(), }; } </code></pre><p>Another design change was that previously I used a lot of  for many things in the web server (like everyone's current luck, activity feed, rate limiters, etc.) I never liked this and after a lot of thinking I finally switched towards an approach where each player is a separate actor, and channels are used to send messages to them, and they own their own state in their own tokio task. So each player actor now owns their activity feed, game states, current luck, arena battle state, etc. This has led to a much cleaner (and much more performant!) architecture and I was able to delete a ton of mutexes / rwlocks, and new features are much easier to add now.</p><p>With these changes, I was able to be much more productive and added a ton of new locations, activities, items, etc. I added new puzzles, games, dark mode, etc. And during all of this time, the Rust server has still never crashed in the entire 3 years it's been running (compared to my old Node JS days this provides me  much peace of mind). The entire codebase (frontend + backend) has grown to be around 130,000 lines of code, but the code is still so simple and adding new features is still really trivial. And whenever I refactor anything, the Rust compiler tells me everything I need to change. It's so nice because I never have to worry about breaking anything because the compiler always tells me anything I need to update. If I had to do everything over again I would still choose Rust 100% because it's been nothing but a pleasure.</p><p>But yeah, everything is still going great and it's so much fun coming up with new stuff to add all the time. Here's some screenshots and a trailer I made recently if you want to see what it looks like (also, almost every asset is an SVG since I wanted all the characters and locations to look beautiful at every browser zoom level). Also, I'd love to hear any feedback, critique, thoughts, or ideas if you have any!</p>","contentLength":4076,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Theory Applications in Video Games","url":"https://utk.claranguyen.me/talks.php?id=videogames","date":1751393678,"author":"/u/ketralnis","guid":180480,"unread":true,"content":"\n\t\t\tMaze Generation is an element in games that can give players a unique\n\t\t\texperience every time they play. Doing them efficiently can be\n\t\t\taccomplished via Graph Theory. If you attended\n\t\t\t<a href=\"https://utk.claranguyen.me/talks.php?id=gextract\">my previous talk</a>, you will know\n\t\t\thow powerful the Disjoint-Set data structure is for object detection.\n\t\t\tIn this talk, however, we're going to use it to generate mazes...\n\t\t\tthe right way.\n\n\t\t\t<h3>Observing Properties of Mazes</h3>\n\t\t\tLet's see what we got here... There are a few things about mazes that\n\t\t\twe should pay attention to prior to making one ourselves:\n\n\t\t\t<ul><li>\n\t\t\t\t\tCells are \"matched\" with a select few adjacent ones. Cells that\n\t\t\t\t\thave been matched do not have a wall between them.\n\t\t\t\t\t</li><li>\n\t\t\t\t\tAll cell pairs that are not \"matched\" have a wall separating\n\t\t\t\t\tthem.\n\t\t\t\t\t</li><li>\n\t\t\t\t\tMazes can be represented as graphs. Depending on the properties\n\t\t\t\t\tof the maze, it can be a minimum spanning tree.\n\t\t\t\t\t</li><li>\n\t\t\t\t\tWe can use typical graph algorithms to find solutions to mazes.\n\t\t\t\t\tPopular choices include DFS (Depth-First Search) and BFS\n\t\t\t\t\t(Breadth-First Search). We can use them to find a solution from\n\t\t\t\t\tany  to any , easily.\n\t\t\t\t\t</li></ul>\n\n\t\t\tNow then, let's talk about Disjoint-Sets...\n\n\t\t\t<h3>The Disjoint-Set Data Structure</h3>\n\t\t\tInitially, treat this data structure as if we have  disjoint\n\t\t\tsets (hence the name), not connected to each other at all. When\n\t\t\tthinking about a maze, treat this as a maze where  walls are\n\t\t\tup, and you can't go from any cell to any other cell. Then, we can use\n\t\t\toperations to manipulate this data structure and connect cells\n\t\t\ttogether.\n\t\t\t<p>\n\t\t\tWe have two operations we can use: </p> and <ul><li> Join two disjoint sets together.\n\t\t\t\t\t</li><li> Get the vertex that is at the \"root\" of a disjoint\n\t\t\t\t\tset. This is the \"ID\" of that set.\n\t\t\t\t\t</li></ul>\n\n\t\t\tLet's discuss them... For now, let's only talk about .\n\t\t\t has a neat optimisation that'll come in handy later.\n\n\t\t\t\n\t\t\tThis one is trivial. Join two disjoint sets together. For this\n\t\t\tpart, I'm going to notate it as  where\n\t\t\tS = S ‚ãÉ S.  we merge the two sets S and\n\t\t\tS into S. Then, S is\n\t\t\tobliterated.\n\n\t\t\t<p>\n\t\t\tLet's show a visual example of this... It might make some things\n\t\t\tclick more easily.\n\t\t\t</p> Assume a graph  where  (there\n\t\t\tare 16 vertices) and  (there are 0 edges). Each\n\t\t\tseparate vertex is part of its own set S(\n\t\t\t\tv ‚àà S,\n\t\t\t\tv ‚àà S,\n\t\t\t\t...,\n\t\t\t\tv ‚àà S,\n\t\t\t). Shown as a 4 √ó 4 grid, we have the following:\n\n\t\t\t<p>\n\n\t\t\tNow let's say we performed </p>. The figures below\n\t\t\tshow the selection of 1 and 2 (left) as well as the result of the\n\t\t\tunion operation (right), visually:\n\n\t\t\t = { 1, 2 }. S is empty and\n\t\t\tobliterated. How about we try a  now?\n\n\t\t\t<p>\n\t\t\tTo properly generate a maze, we can just keep repeating this\n\t\t\tprocedure until there is only one set left.\n\t\t\t</p><p>\n\n\t\t\tAt this point, the only remaining set is S</p> = {\n\t\t\t\tv,\n\t\t\t\tv,\n\t\t\t\t...,\n\t\t\t\tv\n\t\t\t}.\n\n\t\t\tWe are unable to merge anymore sets (and break anymore walls)\n\t\t\tbecause they all belong in the same set already. Any other walls\n\t\t\tbeing broken down will force a  to appear in the\n\t\t\tgraph. Let's break down the kind of graph we have just created:\n\n\t\t\t<ul><li>\n\t\t\t\t\tThe maze generation algorithm we just used is known as\n\t\t\t\t\t<b>Randomised Kruskal's Algorithm</b>.\n\t\t\t\t\t</li><li>\n\t\t\t\t\tThere are no cycles in this graph.\n\t\t\t\t\t</li><li>\n\t\t\t\t\tThere is exactly one path from every  to every\n\t\t\t\t\t.\n\t\t\t\t\t</li><li>\n\t\t\t\t\tIf drawn as a graph, it is a minimal spanning tree.\n\t\t\t\t\t</li><li>\n\t\t\t\t\tIt tends to generate mazes with patterns that are easy to\n\t\t\t\t\tsolve.\n\t\t\t\t\t</li></ul>\n\n\t\t\tThough, this wouldn't be a talk by me though if I didn't say we can\n\t\t\tdo better, now would it? Let's expand on this concept:\n\t\t\t<h3>A simple square maze is boring. We can do better.</h3>\n\t\t\tWe can connect 2 mazes together by breaking down a wall between\n\t\t\tthem. We can even add a \"hallway\" between them if we wanted. This\n\t\t\tis only possible because there exists a path from every  to\n\t\t\tevery .\n\t\t\t<p>\n\t\t\tThus, if we broke down a wall on the outer end of the maze and\n\t\t\tmerged two together, there will always exist a path from one maze\n\t\t\tto the other, as there will always be a path to the cell with the\n\t\t\touter wall we broke. Here's what I mean:\n\n\t\t\t</p><p>\n\n\t\t\tThis kind of \"horizontal expansion\" is possible with mazes.\n\t\t\tWe can also do this vertically. Notice, though, that there is a\n\t\t\tvalid path from anywhere in the left maze to anywhere in the right\n\t\t\tmaze. To take this to the extreme, we can </p> do better,\n\t\t\tand expand the maze by an  (or a few, if we\n\t\t\treally wanted to). Let's give it a second floor...\n\n\t\t\t<p>\n\n\t\t\tWe can go on, but I think this gets the point across. We can also\n\t\t\tcombine the horizontal/vertical expansion with this \"elevator\" to\n\t\t\tmake some pretty unique (but also tedious) puzzles!\n\n\t\t\t</p>\n\t\t\tThe \"find\" operation is used to find the ID of the set a vertex\n\t\t\tbelongs in. I'll denote it as . If S = {\n\t\t\tv, v } and I call , it'll\n\t\t\treturn 0, because v ‚àà S. By the time the\n\t\t\tmaze generation algorithm above is done, calling  on\n\t\t\tany vertex will return 0, as they are all in S.\n\t\t\t of two sets, in a graph theory sense,\n\t\t\tis simply connecting an edge between two points, the \n\t\t\toperation is simply going up to the  of the tree and\n\t\t\treturning that value. Let's go though the previous maze generation\n\t\t\texample once more. This time, let's see how a  is built\n\t\t\tfrom all of this.\n\n\t\t\t<p>\n\n\t\t\tNow that we've constructed the graph, let's order it to where the\n\t\t\tcoloured node (the root) is at the top. It'll look like this:\n\n\t\t\t</p><p>\n\t\t\tThere's something bad about this... Take a look at the deepest node\n\t\t\tin that tree. Since </p> just goes up to the top and returns\n\t\t\tthe root node, it has to go through  vertices before it\n\t\t\treturns. That's an O(n) operation right there.\n\t\t\t<p>\n\t\t\tNow, I'm not going to make a huge deal out of a linear-time lookup.\n\t\t\tA maze of size 2048 √ó 2048 would speak for itself. But, like\n\t\t\tI said, we can do better... </p>.\n\n\t\t\t\n\t\t\tThere are two techniques we can apply to our operations to make\n\t\t\tfind() perform much better:  and ...\n\n\t\t\t<ul><li> - When merging two sets, attach the\n\t\t\t\t\tshorter tree to the taller one. This forces minimal (or no)\n\t\t\t\t\tgrowth of the tree. In fact, at most, the tree can only\n\t\t\t\t\tgrow in height by 1 from this.\n\t\t\t\t\t</li><li> - Make every node point straight to\n\t\t\t\t\tthe root node.\n\t\t\t\t\t</li></ul>\n\n\t\t\tThe visuals of these two would get messy quite quickly, so I\n\t\t\tdecided to not draw them out. But I think those explanations make\n\t\t\tit obvious how these improve on what we had before.\n\t\t\t<p>\n\t\t\tNow then... with these optimisations in place, our O(n) lookup time\n\t\t\tsuddenly becomes lg</p> n (iterated logarithm base 2). In\n\t\t\tthe world of Computer Science, this is essentially . For the record, if x = 2, then\n\t\t\tlg(x) = 5. Here's a table of values just to show\n\t\t\thow slow the equation grows...\n\n\t\t\ta is not a typo. It's known as\n\t\t\t<a href=\"https://en.wikipedia.org/wiki/Tetration\">tetration</a>,\n\t\t\tand is a step up from exponents. If I said 2, that's\n\t\t\tthe same as 2. 2 =\n\t\t\t2. You get the idea.\n\t\t","contentLength":6846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lp9176/graph_theory_applications_in_video_games/"},{"title":"Websites used to be simple","url":"https://simplesite.ayra.ch/","date":1751392243,"author":"/u/AyrA_ch","guid":179227,"unread":true,"content":"<p><i>\n\t\t\t\tThis website is a trip down memory lane.\n\t\t\t\tI'm not trying to tell you to stop modern web development.\n\t\t\t\tThis website uses technologies not available at the time the content here is about.\n\t\t\t\tIt works on mobile (tested in Firefox for Android) but you miss out on the background image.\n\t\t\t</i></p><p>\n\t\t\tI created my first website somewhere in the early 2000s,\n\t\t\tand like most websites back then, it was very simple.\n\t\t\tNot surprising, considering most people (including me) were likely using notepad to create those websites,\n\t\t\twhich puts a limit on their complexity.\n\t\t\tIt was either that or  editors that would chain you to themselves\n\t\t\tbecause there was no chance the generated HTML would be maintainable at all without the tool,\n\t\t\tand if you did manual edits it could outright break your editor.\n\t\t\tThere were no iPhones, there was barely any SEO, and JavaScript really was optional, and so was CSS.<p>\n\t\t\tThe color representation on early LCD screens was bad, so you better picked a color scheme with high contrast.\n\t\t</p></p><p>\n\t\t\tThe resolution of choice was 1024√ó768 (or 1280√ó1024 if you could afford it),\n\t\t\tand yet with the window frame and the toolbars we had back then your website should better also work on 800√ó600.\n\t\t\tYou didn't want to have content right up to the edge of the left or right screen border anyways.\n\t\t</p><div>\n\t\t\tFor reference, this image shows the resolutions 1920√ó1080 (HD), 1280√ó1024, 1024√ó768,\n\t\t\tand 800√ó600 in relation to each other.<img src=\"https://simplesite.ayra.ch/?file=VGA\" alt=\"Various colored rectangles stacked on top of each other comparing display resolutions\"></div><p>\n\t\t\tThe simplest sites were just plain HTML without anything beyond the basic formatting.\n\t\t\tSometimes a  and  if we felt fancy.\n\t\t\tPeople that cared would change the font to a sans-serif type,\n\t\t\toften using a  tag that went around the entire page.\n\t\t\tThere weren't a lot of safe fonts out there, and your choices were as follows:\n\t\t</p><ul><li>:\n\t\t\t\tSerif fonts being the default made this look like a mistake or low effort,\n\t\t\t\tbut in the right places could add some nostalgia.\n\t\t\t\tAnd it does look professional, considering it's the \"newspaper font\".\n\t\t\t\tBeing the default, it is also what you got if the font of choice\n\t\t\t\tby the website owner was not installed on your system.\n\t\t\t</li><li>:\n\t\t\t\tThe font of choice for when you don't know what else to pick.\n\t\t\t\tA sensible option, suitable for pretty much all applications.\n\t\t\t</li><li>:\n\t\t\t\tNerds. Nerds picked this font.\n\t\t\t\tThe choice for when text had to be green on black background.\n\t\t\t</li><li>:\n\t\t\t\tNow likely the most hated font, back then a good choice for your personal website.\n\t\t\t\tIt may look goofy but its readability is not to be underestimated.\n\t\t\t\tIt's a windows-only font, but most people were using Windows back then.\n\t\t\t</li></ul><p>\n\t\t\tAnimated gif images were always an option, but they eat up valuable bandwidth.\n\t\t\tFor simple move or blink animations,\n\t\t\tthe  and  tags had you covered.\n\t\t\tThe marquee is deprecated but still works. Blink was removed\n\t\t\tbut a bit of JS .\n\t\t\tIn that regards, <a href=\"https://simplesite.ayra.ch/anim.php\">please enjoy the most important animation ever in pure HTML</a>.\n\t\t</p><p>\n\t\t\tIn absence of most CSS features we now take for granted,\n\t\t\ttiny images were common to achieve things like color gradient, rounded border edges\n\t\t\tand fancy looking buttons. It wasn't uncommon for a site to be made up of 10 or more images for this purpose.\n\t\t</p><p>\n\t\t\tWe also occasionally added background images.\n\t\t\tThese were usually very small and would tile across the page.\n\t\t\tYou could steal them from an existing website, or make one yourself.\n\t\t\tIn fact, I proudly present the literal \"wall\" paper on this page, created in mspaint in two minutes.\n\t\t\tIt's around 130 bytes. Not as fancy as an animated image but better than a solid color.\n\t\t\tIf the wallpaper made text unreadable we would either go into mspaint and floodfill it with a different color,\n\t\t\tor just increase the font size.\n\t\t\tWe weren't in a rush, and didn't need to cram as much information as possible into the viewport either.\n\t\t</p><p>\n\t\t\tThe fancy people would make the background image stay put when scrolling the content, just like on this page.\n\t\t</p><h2>Partitioning your website</h2><p>\n\t\t\tModern websites have it easy.\n\t\t\tIf you want your header or a menu to stay visible you can just use \n\t\t\tand voil√†, your element will stay inside of the viewing area when scrolling.\n\t\t</p><p>\n\t\t\tWe didn't had this luxury, but what we had were framesets.\n\t\t\tA frameset was (actually still is) a way of partitioning the browser window,\n\t\t\tand displaying individual pages inside of those partitions.\n\t\t\tMany websites would have the following layout:\n\t\t</p><p>\n\t\t\tUnless blocked with an attribute,\n\t\t\tusers could drag the frame border around to change the partition ratio.\n\t\t\tThe border could also be made invisible, creating a seamless experience.\n\t\t\tIf the background color of each frame was the same you could usually not tell that frames were there,\n\t\t\tunless you were looking closely during the page load.\n\t\t</p><p>\n\t\t\tThe frames could interact with each other.\n\t\t\tYou could give them an identifier using the  attribute,\n\t\t\tand links could change the URL of a given frame using the  attribute.\n\t\t\tMy first background music changer worked this way.\n\t\t\tA frame could contain a website, or another frameset, allowing to partition the window multiple times.\n\t\t</p><p>\n\t\t\tYes this technology still works. It is marked as deprecated but\n\t\t\tit will still render and behave like it did over 20 years ago.\n\t\t</p><p>\n\t\t\tTables were pretty much the only universal means of creating a responsive web layout.\n\t\t\tIf you fixed the width of all but one column the unfixed column would adjust in width automatically\n\t\t\tto the current window size if the table width was set to 100%.\n\t\t</p><p>\n\t\t\tTables were also the first universal way of vertically centering content\n\t\t\tusing the  attribute.\n\t\t\tThe frameset layout shown above would occasionally be made with tables\n\t\t\trather than frames. People would shove entire websites into single table cells.\n\t\t</p><p>\n\t\t\tResponsive meant it would adjust to the screen size of a computer.\n\t\t\tNobody cared about the modern doomscrolling rectangles because they didn't exist yet.\n\t\t\tWe did not have flexbox, but <a href=\"https://simplesite.ayra.ch/noflex.php\">float:left</a> gets you quite far.\n\t\t</p><p>\n\t\t\tThe early solution to mobile devices was a completely separate website,\n\t\t\toptimized for small screens. People would be redirected based on the user agent string.\n\t\t</p><p>\n\t\t\tJavaScript is a language designed to be just barely good enough to make animated gif images\n\t\t\tdance around on the page and for the content around your mouse cursor to sparkle\n\t\t\twhen you move it.\n\t\t\tAnd this is exactly how we used it. This and drop down menus.\n\t\t\tAnything beyond that would get you into the realm of browser compatibility problems.\n\t\t\tThose problems are what made jQuery popular because it detected your browser\n\t\t\tand abstracted all the differences it had with other browsers away,\n\t\t\tbut for a very long time, conditional HTML comments were the norm to make IE behave.\n\t\t\tPractically every page had a <code>&lt;!--[if IE lt 7]&gt;...&lt;![endif]--&gt;</code> section.\n\t\t\tFor most small scripts, you would go to a site like  (defunct),\n\t\t\tsearch for the script you wanted, and copy paste it into your website.\n\t\t\tThis behavior hasn't significantly changed although now you go to stackoverflow or ask an AI.\n\t\t\tBack then copy pasting scripts was your best bet\n\t\t\tbecause JS debugging tools were virtually non-existent at that time.\n\t\t\tScript loading would (and still does) block the page from rendering anything below it,\n\t\t\tso script tags were traditionally put at the bottom of the page rather than the header,\n\t\t\tand small scripts were inlined.\n\t\t\tToday we have the  attribute.\n\t\t</p><p>\n\t\t\tA script at the bottom of course meant users with slow connections\n\t\t\tcould try to interact with your page before it fully loaded in.\n\t\t\tThis was often solved by making the main page element hidden initially\n\t\t\tand displaying a \"Loading\" text (or gif if you were fancy) instead.\n\t\t\tA small script that was jammed into the  attribute of the body\n\t\t\twould then hide the loading banner and show the main content.\n\t\t</p><p>\n\t\t\tIf you ever need to make a page interactable before it is fully loaded,\n\t\t\tuse a mutation observer on the document root element that monitors added nodes.\n\t\t\tThen simply add the events to the relevant elements as they're streamed in.\n\t\t\tWe didn't had that back then. Instead we would register a setInterval function\n\t\t\tthat frequently added events, and unregistered said function in the onload event.\n\t\t</p><p>\n\t\t\tRegardless of the amount of scripts on a page,\n\t\t\tserious websites would work to some extent without it,\n\t\t\tsimply because JS was a security problem, and so was disabled in many corporate environments.\n\t\t\tThe  tag can be used to render any content (including  tags)\n\t\t\tif scripts are disabled. This is usually used to inform the user that JS is necessary,\n\t\t\tor to provide a link to a less interactive version of the site.\n\t\t\tHowever JS is now considered a base requirement for most websites.\n\t\t\tSites that use JS based UI rendering will just remain blank if JS is disabled.\n\t\t</p><h2>Dynamic Server Side Content</h2><p>\n\t\t\tA super fancy page would show dynamic content from the server and update it.\n\t\t\tThe simplest solution that worked in all browsers was an iframe with a meta refresh tag inside.\n\t\t\tIt would unconditionally reload the iframe. By making the border invisible people wouldn't even be able to tell.\n\t\t\tThis of course is kinda bad because you reload the page regardless of whether there is new content to show or not.\n\t\t</p><p>\n\t\t\tLong polling must be among the dirtiest, nastiest tricks we used to get content to dynamically update from the server.\n\t\t\tWhen you load a website, your browser actually renders the HTML elements as they're streamed from the network.\n\t\t\tThis is why on some slow sites the width of various elements changes as the site loads.\n\t\t\tHTTP was strictly a client to server initiated protocol.\n\t\t\tWe figured out however that you can abuse the HTML streaming behavior\n\t\t\tto implement a server to client initiated protocol.<p>\n\t\t\tYou would do this by loading a page in an iframe that was purposefully designed to never stop loading.\n\t\t\tIt would occasionally send an invisible HTML comment to keep the connection open,\n\t\t\tbut would otherwise remain silent until it was necessary to push new content to the client.\n\t\t\tYou would then simply send a </p> tag with new JavaScript instructions inside,\n\t\t\tor if the content was purely for display purposes,\n\t\t\ta  with the content inside, plus a piece of CSS code to hide the previous div.\n\t\t\tWebsites would grow indefinitely with this but you could simply solve this with a meta refresh\n\t\t\tthat triggered when the connection ended.<p>\n\t\t\tIt was crude but it was dynamic content without JS.\n\t\t\tWe built entire live chat systems around this.\n\t\t</p></p><p>\n\t\t\tThe first true way to replace long polling are websockets.\n\t\t\tHTTP 2 and 3 have the ability to push events to the client without waiting for a client request\n\t\t\tin what is known as \"server push\" but I've never seen it in the wild.\n\t\t</p><p>\n\t\t\tAjax stands for \"Asynchronous javascript and XML\".\n\t\t\tIt was invented by Microsoft to streamline communication between web browsers (only Internet Explorer actually)\n\t\t\tand the Exchange Server web interface. The technology is actually quite old.\n\t\t\tInternet Explorer 5 was the first browser to ship with this, but others were quickly to adopt it too.\n\t\t\tMicrosoft products intensively use XML, which is why XML is contained in \"Ajax\",\n\t\t\twhy the JS object to make requests is named ,\n\t\t\tand why there's a dedicated  property in it.\n\t\t</p><p>\n\t\t\tAnything not covered by web standards could be extended using ActiveX.\n\t\t\tThis was basically a way to load and call functions from native DLLs that registered themselves as an ActiveX component.\n\t\t\tThis was necessary to play video. It was also needed for audio if you desired any control over the playback whatsoever\n\t\t\tbecause  would not allow you to control playback\n\t\t\tbeyond replacing its value with \"about:blank\" using JS to stop it entirely.\n\t\t\tNow deprecated,  was replaced with .\n\t\t\tAnd thankfully, fully automated audio playback on page load is not permitted by modern browsers\n\t\t\tbecause that is certainly something I don't want back.\n\t\t</p><p>\n\t\t\tBack then, ActiveX was also a way to bypass some system restrictions.\n\t\t\tAt school they would block the remote desktop client, but that block was only for the executable.\n\t\t\tI would just load the MSTSCAX library into a html file and then use the web browser to connect to my home computer\n\t\t\twhenever I found a website being blocked.\n\t\t</p><p>\n\t\t\tThe entire system was a nightmare but Microsoft could basically do whatever they desired\n\t\t\tbecause of the massive market share that Internet Explorer had.\n\t\t\tEvery browser implemented this differently, and you had to update these components all the time.\n\t\t</p><p>\n\t\t\tFlash (see: \"Vulnerability as a Service\") allowed you to do many things that initially weren't possible without,\n\t\t\tincluding but not limited to video and audio playback without depending on locally installed codecs,\n\t\t\tlive streaming, file uploads with a progress bar, and networked multiplayer games.\n\t\t\tAt some point it was basically mandatory to have it installed\n\t\t\tbecause most websites would to some extent depend on it.\n\t\t</p><p>\n\t\t\tBandwidth was obviously at a premium.\n\t\t\tAnd while times were slower back then,\n\t\t\twe would not want to wait for ages either.\n\t\t\tDSL was just getting popular,\n\t\t\tand I was lucky enough to start out with a 5000/500 kbps connection.\n\t\t\tA good website however would load in an acceptable manner on a dialup connection.\n\t\t\tThese were commonly known as \"56k\" because that was their speed under ideal conditions,\n\t\t\t56 kilobits per second. This amounts to 7 kilobytes per second.<p>\n\t\t\tYou often would get a bit more because the connection supported compression,\n\t\t\tand HTML is fairly simple to compress.\n\t\t</p></p><p>\n\t\t\tImages were considered high Bandwidth media back then, and you would not fill your page with them\n\t\t\twithout some serious modifications.\n\t\t\tThe image further below is a new version of the Windows XP default wallpaper known as \"bliss\"\n\t\t\tthat Microsoft recently released. The colors in their version are more muted than the original,\n\t\t\tbut I found this \"corrected\" version that's more in spirit with the original.\n\t\t\tThe image is 2'345'199 bytes in size.\n\t\t\tThis would load 5.5 minutes given a 56k connection.</p><p>\n\t\t\tTo improve load speeds we first drop the resolution.\n\t\t\tIf our site should work with 800√ó600 displays\n\t\t\tthere is no need to have this image in its original 4089√ó2726 size,\n\t\t\tand because we likely don't have the full size of the screen available due to the frameset menu,\n\t\t\twe can scale it down to fit 640√ó480 (the OG VGA resolution).\n\t\t</p><p>\n\t\t\tNext is the quality. By setting the jpeg to 75% quality we can further reduce the size.\n\t\t\tThis quality value is good for noisy pictures like this but will show artifacts around hard borders,\n\t\t\tof which there are none.\n\t\t\tWith those limits the image will load in about 6 seconds.\n\t\t</p><p>\n\t\t\tWe can simulate a faster image load by saving it as a progressive image.\n\t\t\tProgressive images don't store the pixels in the normal order, but rather store them in groups.<i>(This is a simplification)</i>\n\t\t\tThe first group will only store every 8th pixel horizontally and vertically.\n\t\t\tThe next group will store every 4th pixel (minus those already contained in the first group),\n\t\t\tand so on until all pixels are represented.\n\t\t\tDepending on how good your eyes are you may not even notice the last pass.\n\t\t\tImages stored this way will slightly increase in size,\n\t\t\tbut rather than having to wait for it to slowly appear line by line,\n\t\t\twe can fairly quickly render an initial (blurry) version that progressively (hence the name) gets better.<p>\n\t\t\tThis is also possible with PNG images.\n\t\t</p></p><p>\n\t\t\tThe image below has been saved with all the given constraints and settings mentioned above.\n\t\t\tIt is now about 35 KB. To see it load progressively,\n\t\t\trefresh the page or open the image in a new tab.\n\t\t</p><img src=\"https://simplesite.ayra.ch/?file=bliss\" ismap=\"\" alt=\"Windows XP default wallpaper\"><p>\n\t\t\tIf you did not want to compromise on the quality you would\n\t\t\tcreate a thumbnail that when clicked, would navigate the user to the raw image.\n\t\t</p><p>\n\t\t\tHere's the PHP code that converted this image\n\t\t</p><pre>$bliss = ImageCreateFromJpeg('bliss.jpg');\n// \"-1\" means to calculate height to keep aspect ratio\n$scaled = ImageScale($bliss, 640, -1, IMG_BICUBIC);\nImageDestroy($bliss);\n// Enable progressive scan\nImageInterlace($scaled, TRUE);\n// Save with 75% quality\nImageJpeg($scaled, 'bliss_scaled.jpg', 75);\nImageDestroy($scaled);</pre><p>\n\t\t\tIf the client had JavaScript enabled,\n\t\t\tyou could load the image using JS only when the user scrolls the image into view.\n\t\t\tThis could save substantial bandwidth on image heavy sites.\n\t\t\tYou can still do this, but the browser does it automatically if you add the  attribute.\n\t\t\tThis also stops the image from blocking the page load event.\n\t\t</p><p>\n\t\t\tImages could be made interactive to some extent.\n\t\t\tIf the image was inside of an  tag, the attribute \n\t\t\tcould be added to the image. When clicking, the browser would navigate to the URL in the link,\n\t\t\tbut adds the X and Y coordinate as  query string to the URL.\n\t\t\tThis allows a server to check where in the image the user clicked. This allowed for image based menus,\n\t\t\tor a \"Where's Waldo\" (or \"Wally\" in some countries) style game.\n\t\t\tLater came client side image maps, which are invisible clickable regions you could overlay over an image.\n\t\t</p><p>\n\t\t\tBoth of these technologies still work,\n\t\t\tnone of them are marked as deprecated either.\n\t\t</p><p>\n\t\t\tYolo driven development was the norm back then for personal sites and small company pages.\n\t\t\tYou changed a few lines, then uploaded your changes to your webserver with an FTP client,\n\t\t\tlikely not even encrypted.\n\t\t\tYour choices were PHP on an Apache web server and ASP on an IIS.\n\t\t\tStaging environment? What staging environment?\n\t\t</p><p>\n\t\t\tIt was crude, but oh boy was it fast to get something going.\n\t\t\tPHP is easy to get started, more forgiving than other languages, available practically everywhere,\n\t\t\tand still actively maintained.\n\t\t\tAt this point it has probably gained cockroach status and will be around for ages.\n\t\t</p><p>\n\t\t\tThis is also how this website is deployed. Except instead of FTP I use syncthing\n\t\t\tbecause I want it to upload automatically when I change something,\n\t\t\tbut straight to prod it goes.\n\t\t\tIt doesn't needs any form of compilation or build process whatsoever.\n\t\t</p><p>\n\t\t\tSimpler times. Not necessarily better, but simpler.\n\t\t\tWe achieved a lot with less. We optimized our media, and used very little scripting.\n\t\t\tNow we don't. Nobody cares anymore if your website is 10 or 20 megabytes.\n\t\t\tDo I miss creating websites in a plain text editor? No. I want to keep my IDE with syntax highlighting,\n\t\t\tsyntax checking, and code completion.\n\t\t\tI also want to keep the libraries that make my life easier.\n\t\t\tI haven't written an SQL statement in my backend code for a long time now.\n\t\t\tSQL ORM mappers are great.<p>\n\t\t\tDo I wish we would create simpler pages again? Yes.\n\t\t\tI believe that although websites back then were simpler and technology more limited,\n\t\t\tit was not necessarily worse than today, but the average website now feels bloated and overengineered.\n\t\t\tThe internet is no longer a place of creation; it is a place of consumption.\n\t\t\tI want the world wide web wild west back (call it W5 or whatever).\n\t\t\tI want search engines to find wacky websites again.\n\t\t\tI don't want to please algorithms of our corporate overlords.</p>\n\t\t\tWriting this page, I just found out that in 2021, Warner Bros has taken down the original Space Jam website from 1996.\n\t\t\tI would have liked to put it here,\n\t\t\t<a rel=\"noopener noreferrer nofollow\" href=\"https://web.archive.org/web/20200404221000/https://www.spacejam.com/\" target=\"_blank\">but I guess the web archive is your only hope now</a>.\n\t\t\tIf you have time, check out how the main menu on that site was made.\n\t\t</p><p>\n\t\t\tIn any case, my tribute to those times is a wordle clone.\n\t\t\tIf you have Internet Explorer 1.5 or later, or any modern browser (I heard this Mozilla thing is taking off),\n\t\t\tyou can play <a href=\"https://wordle.ayra.ch/\">a game of wordle here</a>.\n\t\t\tI recommend you use at least IE 2.0 because 1.5 lacks color support, which makes the clues less visible.\n\t\t</p><p>\n\t\t\tIf you want to go super oldschool and can get a copy of NCSA Mosaic running (the first ever web browser),\n\t\t\tyou can play <a href=\"http://wordle.ayra.ch:8081/mosaic.php\">the game here instead</a>.\n\t\t\tI can confirm the browser does run on Windows XP, and possibly later versions of Windows,\n\t\t\tbut likely only on 32 bit versions.</p><p>\n\t\t\tNote: The WWW was designed to share crudely formatted scientific documents and link them,\n\t\t\twhich explains why it's so hostile towards making screen oriented applications\n\t\t\tbut has a plethora of features what work well for print media.\n\t\t\tThis early feature set is exactly what Mosaic 1.0 provides.\n\t\t\tIt lacks color support and doesn't even has means for text input,\n\t\t\tbut I managed to cobble together something for it anyways.\n\t\t\tIts HTTP support is best described as \"somewhat there\", which explains why this wordle version runs on its own port.\n\t\t</p><p>\n\t\t\tYou're experiencing it right now.\n\t\t\tThis website is looped through a RS-232 serial connection at 56k baud rate\n\t\t\t(actually a little bit extra to handle protocol overhead).\n\t\t\tI disabled the server cache so you can experience the scrollbar shrinking as content slowly loads in.\n\t\t\tBut some things are worth the time. People may not even notice that your website is slow\n\t\t\tif you give them a bit of content to read before shoving an AI generated title image down their throat.<p>\n\t\t\tYou don't need a serial loopback for this,\n\t\t\ta tool like </p><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://github.com/AyrA/SlowPipe\">SlowPipe</a> in front of your server does the same.\n\t\t</p>","contentLength":21279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lp8efe/websites_used_to_be_simple/"},{"title":"The Senate Just Put Clean Energy for AI in the Crosshairs","url":"https://www.wired.com/story/the-senate-just-put-clean-energy-for-ai-in-the-crosshairs/","date":1751390804,"author":"/u/wiredmagazine","guid":179228,"unread":true,"content":"<p>Even without the industry-ending excise tax, experts still <a data-offer-url=\"https://x.com/tylerhnorris/status/1940016653036580883\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/tylerhnorris/status/1940016653036580883&quot;}\" href=\"https://x.com/tylerhnorris/status/1940016653036580883\" rel=\"nofollow noopener\" target=\"_blank\">say</a> that the forced retirement of the tax credits blows up valuable investment in projects already in the pipeline. Since the beginning of the year, the clean energy industry has felt the pressure of looming IRA rollbacks. According to an <a data-offer-url=\"https://e2.org/releases/may-25-clean-economy-works/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://e2.org/releases/may-25-clean-economy-works/&quot;}\" href=\"https://e2.org/releases/may-25-clean-economy-works/\" rel=\"nofollow noopener\" target=\"_blank\">analysis</a> from energy NGO E2, around $15.5 billion in investment in new clean energy projects and factories has been lost since the start of the year, including more than $9 billion in Republican congressional districts.</p><p>The intense hostility for solar and wind coming from the Trump administration may seem, to a logical person, to be at odds with its goal of ‚Äúenergy dominance.‚Äù Energy experts say that renewables‚Äîparticularly when paired with batteries‚Äîare helping to bolster the US grid as energy needs soar. Texas, for instance, added more solar and battery storage than any other type of energy to its grid last year. As of this spring, wind and solar combined made up <a href=\"https://docs.house.gov/meetings/IF/IF03/20250325/118040/HHRG-119-IF03-Wstate-VegasP-20250325.pdf\">42 percent</a> of Texas‚Äôs installed generation capacity, more than any other state in the US. All that new solar and storage has, in turn, helped the grid stay stable during peak use, <a href=\"https://insideclimatenews.org/news/28062025/texas-battery-storage-solar-reduces-summer-blackout-risk/\">lowering the risk of blackouts</a> during the first heatwaves of the summer‚Äîeven as Texas faces <a href=\"https://www.axios.com/local/dallas/2025/06/27/texas-energy-demand-record-data-center-ercot\">never-before-seen summer demand</a> this year, thanks to hot temperatures and the addition of energy-thirsty data centers. Yet in an <a data-offer-url=\"https://nypost.com/2025/06/27/opinion/how-the-big-beautiful-bill-will-lower-energy-costs-bolster-the-electric-grid-and-unleash-us-prosperity/?utm_medium=social&amp;utm_campaign=nypost_opinion&amp;utm_source=twitter\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://nypost.com/2025/06/27/opinion/how-the-big-beautiful-bill-will-lower-energy-costs-bolster-the-electric-grid-and-unleash-us-prosperity/?utm_medium=social&amp;utm_campaign=nypost_opinion&amp;utm_source=twitter&quot;}\" href=\"https://nypost.com/2025/06/27/opinion/how-the-big-beautiful-bill-will-lower-energy-costs-bolster-the-electric-grid-and-unleash-us-prosperity/?utm_medium=social&amp;utm_campaign=nypost_opinion&amp;utm_source=twitter\" rel=\"nofollow noopener\" target=\"_blank\">op-ed</a> published in the New York Post last week, Energy Secretary Chris Wright said that wind and solar contribute to a ‚Äúless stable grid.‚Äù</p><p>Doug Lewin, an energy analyst based in Austin, points out that solar and batteries are particularly <a data-offer-url=\"https://x.com/douglewinenergy/status/1921986007966372187\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/douglewinenergy/status/1921986007966372187&quot;}\" href=\"https://x.com/douglewinenergy/status/1921986007966372187\" rel=\"nofollow noopener\" target=\"_blank\">well-positioned</a> to help out with grid demand during heatwaves, when the sun is shining‚Äîand people turn on their air conditioners.</p><p>‚ÄúWe‚Äôre just in this situation where we are going to need massive amounts of power to deal with the heat,‚Äù he says. ‚ÄúWe‚Äôve gotta have air conditioning to keep people healthy and safe during these hellacious summers, which are getting worse. That‚Äôs just an objective matter.‚Äù</p><p>It‚Äôs particularly ironic to see these kinds of pushbacks as the Trump administration goes all in on artificial intelligence, which, by some projections, could comprise nearly <a data-offer-url=\"https://www.mckinsey.com/featured-insights/sustainable-inclusive-growth/charts/ais-power-binge\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.mckinsey.com/featured-insights/sustainable-inclusive-growth/charts/ais-power-binge&quot;}\" href=\"https://www.mckinsey.com/featured-insights/sustainable-inclusive-growth/charts/ais-power-binge\" rel=\"nofollow noopener\" target=\"_blank\">12 percent of US power demand</a> by the end of the decade. Right now, a global backlog in gas turbines is spelling trouble for those looking to scale up fast. Turbine producers like GE Vernova <a data-offer-url=\"https://naturalgasintel.com/news/higher-for-longer-natural-gas-power-with-equipment-sold-out-through-2027-says-ge-vernova-ceo/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://naturalgasintel.com/news/higher-for-longer-natural-gas-power-with-equipment-sold-out-through-2027-says-ge-vernova-ceo/&quot;}\" href=\"https://naturalgasintel.com/news/higher-for-longer-natural-gas-power-with-equipment-sold-out-through-2027-says-ge-vernova-ceo/\" rel=\"nofollow noopener\" target=\"_blank\">say</a> they‚Äôve already filled orders for the next few years, and project it may take several years for new customers to get their hands on a completed turbine. In April, the CEO of renewable and utility giant NextEra Energy <a data-offer-url=\"https://www.investor.nexteraenergy.com/~/media/Files/N/NEE-IR/reports-and-fillings/quarterly-earnings/2025/Q1%202025/Final_Q1%202025%20Script_vF.pdf\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.investor.nexteraenergy.com/~/media/Files/N/NEE-IR/reports-and-fillings/quarterly-earnings/2025/Q1%202025/Final_Q1%202025%20Script_vF.pdf&quot;}\" href=\"https://www.investor.nexteraenergy.com/~/media/Files/N/NEE-IR/reports-and-fillings/quarterly-earnings/2025/Q1%202025/Final_Q1%202025%20Script_vF.pdf\" rel=\"nofollow noopener\" target=\"_blank\">told shareholders</a> that he expects renewables to act as a ‚Äúbridge,‚Äù helping to bolster the grid and buy time until bigger gas projects can come online.</p><p>But even with the promise of AI using up every spare electron on the grid, the cultural backlash to renewables is as strong as ever‚Äîand it isn‚Äôt isolated to the White House. Despite Texas‚Äôs reliance on renewables, the state legislature <a data-offer-url=\"https://www.utilitydive.com/news/anti-renewables-bills-die-texas-legislature-power-sector-energy/749709/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.utilitydive.com/news/anti-renewables-bills-die-texas-legislature-power-sector-energy/749709/&quot;}\" href=\"https://www.utilitydive.com/news/anti-renewables-bills-die-texas-legislature-power-sector-energy/749709/\" rel=\"nofollow noopener\" target=\"_blank\">battled</a> over several bills this past session that would have seriously kneecapped solar and wind development in the state. Oklahoma, which relies on wind energy for a third of its energy needs, faces a <a data-offer-url=\"https://heatmap.news/plus/the-fight/spotlight/renewable-energy-ban-oklahoma\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://heatmap.news/plus/the-fight/spotlight/renewable-energy-ban-oklahoma&quot;}\" href=\"https://heatmap.news/plus/the-fight/spotlight/renewable-energy-ban-oklahoma\" rel=\"nofollow noopener\" target=\"_blank\">growing movement</a> to ban renewables altogether. Across the country, local governments, responding to grassroots movements, are pushing back against wind and solar projects on their land. (It‚Äôs important to note that many of these movements often include Democrats.)</p><p>Lewin, who wrote about Texas‚Äôs legislative drama in detail this year in his <a data-offer-url=\"https://www.douglewin.com/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.douglewin.com/&quot;}\" href=\"https://www.douglewin.com/\" rel=\"nofollow noopener\" target=\"_blank\">newsletter</a>, says it‚Äôs too simplistic to ascribe the hostility towards renewables as simply being funded by Big Oil. According to <a data-offer-url=\"https://www.politico.com/live-updates/2025/07/01/congress/senate-bill-to-ease-wind-and-solar-phaseout-00434983\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.politico.com/live-updates/2025/07/01/congress/senate-bill-to-ease-wind-and-solar-phaseout-00434983&quot;}\" href=\"https://www.politico.com/live-updates/2025/07/01/congress/senate-bill-to-ease-wind-and-solar-phaseout-00434983\" rel=\"nofollow noopener\" target=\"_blank\">Politico</a>, Alaska Senator Lisa Murkowski, who has received <a href=\"https://www.opensecrets.org/members-of-congress/lisa-murkowski/summary?cid=N00026050\">hundreds of thousands of dollars in campaign donations</a> from oil and gas interests over the course of her career, was an instrumental figure in changing the final Senate language to remove the excise tax. In Texas, the oil and gas lobby <a data-offer-url=\"https://www.dallasnews.com/news/politics/2025/05/01/renewable-energy-developers-oil-lobby-and-manufacturers-unite-against-bill/\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.dallasnews.com/news/politics/2025/05/01/renewable-energy-developers-oil-lobby-and-manufacturers-unite-against-bill/&quot;}\" href=\"https://www.dallasnews.com/news/politics/2025/05/01/renewable-energy-developers-oil-lobby-and-manufacturers-unite-against-bill/\" rel=\"nofollow noopener\" target=\"_blank\">united</a> with renewables to defeat a bill that would have made energy prices higher by increasing costs for wind and solar.</p><p>‚ÄúIt feels like you‚Äôve got a large number of really powerful folks who have just decided, or been convinced‚Äîand then had that belief reinforced by algorithms over and over‚Äîthat somehow, wind and solar are the root of all evil and are causing every problem,‚Äù Lewin says. ‚ÄúIt's bizarre. It's really hard to kind of understand this animus for technologies that have had a huge benefit.‚Äù</p>","contentLength":4518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lp7rj5/the_senate_just_put_clean_energy_for_ai_in_the/"},{"title":"Hayro: An experimental, work-in-progress PDF rasterizer in pure Rust.","url":"https://github.com/LaurenzV/hayro","date":1751390187,"author":"/u/Frexxia","guid":179551,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lp7hlz/hayro_an_experimental_workinprogress_pdf/"},{"title":"Why does Rust feel so well designed?","url":"https://www.reddit.com/r/rust/comments/1lp7562/why_does_rust_feel_so_well_designed/","date":1751389421,"author":"/u/Glum-Psychology-6701","guid":179134,"unread":true,"content":"<p>I'm coming from Java and Python world mostly, with some tinkering in fsharp. One thing I notice about Rust compared to those languages is everything is well designed. There seems to be well thought out design principles behind everything. Let's take Java. For reasons there are always rough edges. For example List interface has a method called add. Immutable lists are lists too and nothing prevents you from calling add method on an immutable list. Only you get a surprise exception at run time. If you take Python, the zen contradicts the language in many ways. In Fsharp you can write functional code that looks clean, but because of the unpredictable ways in which the language boxes and unboxes stuff, you often get slow code. Also some decisions taken at the beginning make it so that you end up with unfixable problems as the language evolves. Compared to all these Rust seems predictable and although the language has a lot of features, they are all coherently developed and do not contradict one another. Is it because of the creator of the language doing a good job or the committee behind the language features has a good process?</p>","contentLength":1142,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Recommended preparation material for ML interviews.","url":"https://www.reddit.com/r/MachineLearning/comments/1lp6n1r/d_recommended_preparation_material_for_ml/","date":1751388293,"author":"/u/South-Conference-395","guid":180482,"unread":true,"content":"<p>Below I am gathering some interview preparation tools for ML research positions. People who had been in the job market recently, which one would you recommend/ find more relevant? Any other resources that I might be missing?</p>","contentLength":224,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A black box full of dangers","url":"https://www.reddit.com/r/rust/comments/1lp68zo/a_black_box_full_of_dangers/","date":1751387405,"author":"/u/WanderingCID","guid":179225,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/WanderingCID\"> /u/WanderingCID </a>","contentLength":35,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pluto is a unique dialect of Lua with a focus on general-purpose programming","url":"https://pluto-lang.org/docs/Introduction","date":1751385145,"author":"/u/ketralnis","guid":179392,"unread":true,"content":"<p>Pluto is a superset of Lua 5.4 designed to assist with general-purpose programming &amp; facilitate cleaner, more streamlined development via:</p><p>Despite the immense additions, Pluto remains highly compatible with Lua:</p><ul><li>(Mostly) compatible with Lua 5.4 source code.\n<ul><li>Our only breakage is the addition of new keywords, which causes conflicts when those keywords are used as identifiers. However, Pluto leverages parser heuristics and ‚Äî in cases where parser heuristics fail ‚Äî <a href=\"https://pluto-lang.org/docs/Compatibility#compatibility-mode\">Compatibility Mode</a> to eliminate this concern. Most Lua 5.4 source code will execute flawlessly on Pluto.</li></ul></li><li>Reads and writes Lua 5.4 bytecode meaning it's forwards- and backwards-compatible.\n<ul><li>Only some Pluto features generate backwards-incompatible bytecode, but they will say so in their documentation.</li></ul></li><li>Actively rebases with Lua's main repository. We are not a time-frozen dialect. When Lua 5.5 releases, we intend on updating to that.</li></ul><p>With Compatibility Mode, Pluto has been dropped into large communities and did not break any existing scripts.</p><h2>What does Pluto aspire to be?<a href=\"https://pluto-lang.org/docs/Introduction#what-does-pluto-aspire-to-be\" aria-label=\"Direct link to What does Pluto aspire to be?\" title=\"Direct link to What does Pluto aspire to be?\">‚Äã</a></h2><p>Pluto aspires to be a version of Lua with a larger feature-set, that is all. Pluto is not a Lua-killer, an attempted successor, or any of that. Many people (rightly so) love Lua precisely because of the design philosophy. And fundamentally, Pluto is a major deviation from Lua's design philosophy. Some may prefer this, some may not.</p>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lp59bm/pluto_is_a_unique_dialect_of_lua_with_a_focus_on/"},{"title":"Protesters accuse Google of violating its promises on AI safety: 'AI companies are less regulated than sandwich shops'","url":"https://www.businessinsider.com/protesters-accuse-google-deepmind-breaking-promises-ai-safety-2025-6","date":1751384536,"author":"/u/MetaKnowing","guid":179229,"unread":true,"content":"<p>A full-blown courtroom drama ‚Äî complete with a gavel-wielding judge and an attentive jury, played out in London's King's Cross on Monday, mere steps away from <a target=\"_self\" href=\"https://www.businessinsider.com/google-deepmind-ai-talent-war-aggressive-noncompetes-2025-4\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Google DeepMind</a>'s headquarters.</p><p>Google was on trial for allegations of breaking its promises on AI safety.</p><p>The participants of this faux-production were protesters from PauseAI, an activist group concerned that tech companies are racing into AI with little regard for safety. On Monday, the group congregated near King's Cross station to demand that Google be more transparent about the safety checks it's running on its most cutting-edge AI models.</p><p>PauseAI argues that <a target=\"_self\" href=\"https://www.businessinsider.com/gen-z-shoppers-google-amazon-2025-6\" data-track-click=\"{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}\" rel=\"\">Google</a> broke a promise it made during the 2024 AI Safety Summit in Seoul, Korea, when the company agreed to consider external evaluations of its models and publish details about how external parties, including governments, were involved in assessing the risks.</p><p>When Google launched Gemini 2.5 Pro, its latest frontier model, in April, it did neither of those things. The company said it was because the model was still \"experimental.\" A few weeks later, it released a \"model card\" with some safety details, which some experts criticized for being too thin on details, <a target=\"_blank\" href=\"https://storage.googleapis.com/model-cards/documents/gemini-2.5-pro-preview.pdf\" data-track-click=\"{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}\" rel=\" nofollow\">TechCrunch</a> previously reported. While the safety report made reference to third-party testers, it did not specify who they were.</p><p>\"We are committed to developing AI safely and securely to benefit society,\" a Google DeepMind spokesperson told BI. \"We continue to evolve our model testing and reporting to respond to rapid changes in the technology, and will continue to provide information that supports the responsible use of our AI models.\"</p><p>For PauseAI, this isn't good enough. More importantly, the organization said, it's about not letting any lapse slip by and allowing Google to set a precedent.</p><p>\"If we let Google get away with breaking their word, it sends a signal to all other labs that safety promises aren't important and commitments to the public don't need to be kept,\" said PauseAI organizing director Ella Hughes, addressing the crowd, which had gradually swelled to around 60 people.</p><p>\"Right now, AI companies are less regulated than sandwich shops.\"</p><p>Focusing on the specific issue of the Google safety report is a way for PauseAI to push for a specific and attainable near-term change.</p><p>About 30 minutes into the protest, several intrigued passers-by had joined the cause. After a rousing speech from Hughes, the group proceeded to Google DeepMind's offices, where the fake courtroom production played out. Some Google employees leaving for the day looked bemused as chants of \"Stop the race, it's unsafe\" and \"Test, don't guess\" rang out.</p><p>\"AI regulation on an international level is in a very bad place,\" PauseAI founder Joep Meindertsma told Business Insider, pointing to how US Vice President JD Vance <a target=\"_blank\" href=\"https://www.presidency.ucsb.edu/documents/remarks-the-vice-president-the-artificial-intelligence-action-summit-paris-france\" data-track-click=\"{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}\" rel=\"\">warned against over-regulating AI</a> at the AI Action Summit.</p><p>Monday was the first time PauseAI had gathered over this specific issue, and it's not clear what comes next. The group is engaging with members of UK parliament who will run these concerns up the flagpole, but Meindertsma is reticent to say much about how Google is engaging with the group and their demands.</p><p>Meindertsma hopes support will grow and references polls that suggest the public at large is <a target=\"_blank\" href=\"https://time.com/7213096/uk-public-ai-law-poll/\" data-track-click=\"{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}\" rel=\" nofollow\">concerned that AI is moving too fast</a>. The group on Monday was made up of people from different backgrounds, including some who work in tech. Meindertsma himself runs a software development company and regularly uses AI tools from Google, OpenAI, and others.</p><p>\"Their tools are incredibly impressive,\" he said, \"which is the thing that worries me so much.\"</p>","contentLength":3617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lp4zvi/protesters_accuse_google_of_violating_its/"},{"title":"Is os.Executable() reliable?","url":"https://www.reddit.com/r/golang/comments/1lp4rce/is_osexecutable_reliable/","date":1751383990,"author":"/u/1oddbull","guid":180378,"unread":true,"content":"<div><p>The documentation says no guarantee that the path is pointing to the right executable. But then how do you ship other applications files with your Go executable? eg an Electron app</p></div>   submitted by   <a href=\"https://www.reddit.com/user/1oddbull\"> /u/1oddbull </a>","contentLength":211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"(Ab)using channels to implement a 3D pipe game","url":"https://jro.sg/go-chan.html","date":1751380987,"author":"/u/jroo1","guid":179181,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lp3gtq/abusing_channels_to_implement_a_3d_pipe_game/"},{"title":"Who's Hiring - July 2025","url":"https://www.reddit.com/r/golang/comments/1lp3e6p/whos_hiring_july_2025/","date":1751380806,"author":"/u/jerf","guid":179016,"unread":true,"content":"<p>This post will be stickied at the top of until the last week of July (more or less).</p><p>: It seems like Reddit is getting more and more cranky about marking external links as spam. A good job post obviously has external links in it. If your job post does not seem to show up please send modmail. Or wait a bit and we'll probably catch it out of the removed message list.</p><p>Please adhere to the following rules when posting:</p><ul><li>Don't create top-level comments; those are for employers.</li><li>Feel free to reply to top-level comments with on-topic questions.</li><li>Meta-discussion should be reserved for the distinguished mod comment.</li></ul><ul><li>To make a top-level comment you must be hiring directly, or a focused third party recruiter with <strong>specific jobs with named companies</strong> in hand. No recruiter fishing for contacts please.</li><li>The job must be currently open. It is permitted to post in multiple months if the position is still open, especially if you posted towards the end of the previous month.</li><li>The job must involve working with Go on a regular basis, even if not 100% of the time.</li><li>One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.</li><li>Please base your comment on the following template:</li></ul><p><em>[Company name; ideally link to your company's website or careers page.]</em></p><p><em>[Full time, part time, internship, contract, etc.]</em></p><p><em>[What does your team/company do, and what are you using Go for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.]</em></p><p><em>[Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.]</em></p><p><em>[Please attempt to provide at least a rough expectation of wages/salary.If you can't state a number for compensation, omit this field. Do not just say \"competitive\". Everyone says their compensation is \"competitive\".If you are listing several positions in the \"Description\" field above, then feel free to include this information inline above, and put \"See above\" in this field.If compensation is expected to be offset by other benefits, then please include that information here as well.]</em></p><p><em>[Do you offer the option of working remotely? If so, do you require employees to live in certain areas or time zones?]</em></p><p><em>[Does your company sponsor visas?]</em></p><p><em>[How can someone get in touch with you?]</em></p>","contentLength":2351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strudel: a programming language for writing music","url":"https://strudel.cc/workshop/getting-started/","date":1751379998,"author":"/u/pimterry","guid":179362,"unread":true,"content":"<div data-astro-cid-j75b3yus=\"\"><p>Welcome to the Strudel documentation pages!\nYou‚Äôve come to the right place if you want to learn how to make music with code.</p><p>With Strudel, you can expressively write dynamic music pieces.\nIt is an official port of the <a href=\"https://tidalcycles.org/\">Tidal Cycles</a> pattern language to JavaScript.\nYou don‚Äôt need to know JavaScript or Tidal Cycles to make music with Strudel.\nThis interactive tutorial will guide you through the basics of Strudel.<p>\nThe best place to actually make music with Strudel is the </p><a href=\"https://strudel.cc/\">Strudel REPL</a></p><h2>What can you do with Strudel?<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://strudel.cc/workshop/getting-started/#what-can-you-do-with-strudel\"></a></h2><ul><li>live code music: make music with code in real time</li><li>algorithmic composition: compose music using tidal‚Äôs unique approach to pattern manipulation</li><li>teaching: focussing on a low barrier of entry, Strudel is a good fit for teaching music and code at the same time.</li><li>integrate into your existing music setup: either via MIDI or OSC, you can use Strudel as a really flexible sequencer</li></ul><p>Here are some examples of how strudel can sound:</p><p>These examples cannot fully encompass the variety of things you can do, so <a href=\"https://strudel.cc/intro/showcase/\">check out the showcase</a> for some videos of how people use Strudel.</p><p>The best way to start learning Strudel is the workshop.\nIf you‚Äôre ready to dive in, let‚Äôs start with your <a href=\"https://strudel.cc/workshop/first-sounds/\">first sounds</a></p></div>","contentLength":1199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lp32gt/strudel_a_programming_language_for_writing_music/"},{"title":"Kamune, secure communication over untrusted networks","url":"https://www.reddit.com/r/golang/comments/1lp2cup/kamune_secure_communication_over_untrusted/","date":1751378254,"author":"/u/hossein1376","guid":178968,"unread":true,"content":"<p>EDIT: This is an experimental project, and is not intended to be used for critical purposes.</p><p>Two weeks ago, Internet access in Iran was shut down nationwide. The remaining services were government-controlled or affiliated. So, I started writing something that allowed for secure communication over untrusted networks. I learned a lot, and it helped me to keep myself busy. I'm curious to know what you guys think about it, and I'm looking forward to your thoughts and suggestions. <a href=\"https://github.com/hossein1376/kamune\">Link</a></p><p>Fun fact: Initially, I named it as such because KƒÅmune (in Persian means truck) have always reminded me of the word communication. Later on, my sister mentioned that the word can also be read as Kamoon-e, which means ricochet; and now I think it makes more sense to call it that.</p>","contentLength":764,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sniffnet: a free, open source network monitoring app","url":"https://www.reddit.com/r/linux/comments/1lp2288/sniffnet_a_free_open_source_network_monitoring_app/","date":1751377497,"author":"/u/GyulyVGC","guid":178969,"unread":true,"content":"<p>Sniffnet (<a href=\"https://sniffnet.net\">website</a> | <a href=\"https://github.com/GyulyVGC/sniffnet\">GitHub</a>) is a powerful yet intuitive network analysis tool to enable everyone comfortably monitor their Internet traffic.</p><p>I‚Äôve been working on Sniffnet as a side-project for almost 3 years, and its development is today supported by the European Union‚Äôs  program.</p><p>The most recent major version of the app was published just a couple days ago and, among the other features, it finally makes Sniffnet available as a Docker image for Linux.</p><p>The latest release also introduces the ability to import data from Packet Capture files in addition to network interfaces, and it turned out Sniffnet is 2x faster than Wireshark at processing them.</p>","contentLength":654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] The Bitter Lesson is coming for Tokenization","url":"https://www.reddit.com/r/MachineLearning/comments/1lp1lfb/r_the_bitter_lesson_is_coming_for_tokenization/","date":1751376255,"author":"/u/lucalp__","guid":179076,"unread":true,"content":"<p>New to the sub but came across <a href=\"https://www.reddit.com/r/MachineLearning/comments/1hli20i/d_in_byte_latent_transformer_how_is_the_decoded/\">discussion posts</a> on BLT so I figured everyone might appreciate <a href=\"https://lucalp.dev/bitter-lesson-tokenization-and-blt/\">this new post</a>! In it, I highlight the desire to replace tokenization with a general method that better leverages compute and data.</p><p>For the most part, I summarise tokenization's role, its fragility and build a case for removing it. I do an overview of the influential architectures so far in the path to removing tokenization so far and then do a deeper dive into the Byte Latent Transformer to build strong intuitions around some new core mechanics.</p><p>Hopefully it'll be of interest and a time saver for anyone else trying to track the progress of this research effort.</p>","contentLength":659,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do I setup backup & restore for CloudNativePG such that it works with an \"ephemeral\" cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1lp15y2/how_do_i_setup_backup_restore_for_cloudnativepg/","date":1751375119,"author":"/u/TemporalChill","guid":179133,"unread":true,"content":"<p>LIKELY ALREADY RESOLVED: I didn't take a bloody backup to begin with. I knowww. Fresh pair of eyes could've saved my entire night.</p><p>I love how easy it is to setup cnpg, but as a new user, the backup/restore bit is sending me. Perusing the docs, I figured this was possible:</p><ol><li><p>Create my cnpg clusters (initdb), with s3 backup configured.</p></li><li><p>After the initdb job has succeeded and the wal backups show up in s3, alter the cnpg cluster manifest to replace initdb bootstrap with the SAME s3 cluster as restore source.</p></li><li><p>Now I can teardown the k8s cluster and rebuild it. Given there are backups in s3, the restoration should be automated and straightforward, no matter how many k8s resets I have.</p></li></ol><pre><code>apiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: uno-postgres spec: storage: size: 5Gi backup: barmanObjectStore: endpointURL: https://REDACTED destinationPath: s3://development/db s3Credentials: accessKeyId: name: s3 key: accessKeyId secretAccessKey: name: s3 key: accessKeySecret bootstrap: recovery: source: clusterBackup externalClusters: - name: clusterBackup barmanObjectStore: endpointURL: https://REDACTED destinationPath: s3://development/db s3Credentials: accessKeyId: name: s3 key: accessKeyId secretAccessKey: name: s3 key: accessKeySecret </code></pre><p>Note that I comment out the bootstrap section for init to succeed and do I see the wal/000... files in my obj store, so it's not a connection problem. I figure the bootstrap section only needs to be commented out once for initdb to run and place the initial backup files in s3, after which I'd never have to comment it out again.</p><p>The \"full recovery\" pod fails with:</p><pre><code>\"msg\":\"Error while restoring a backup\",\"logging_pod\":\"uno-postgres-1-full-recovery\",\"error\":\"no target backup found\",\"stacktrace\": </code></pre>","contentLength":1742,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulnerability Advisory: Sudo chroot Elevation of Privilege","url":"https://www.stratascale.com/vulnerability-alert-CVE-2025-32463-sudo-chroot","date":1751372419,"author":"/u/FryBoyter","guid":178936,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lp0784/vulnerability_advisory_sudo_chroot_elevation_of/"},{"title":"[Media] Rust + Svelte for single binary web apps","url":"https://www.reddit.com/r/rust/comments/1lozkov/media_rust_svelte_for_single_binary_web_apps/","date":1751370540,"author":"/u/HugoDzz","guid":178932,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I created an open-source tool to analyze 1.5M medical AI papers on PubMed","url":"https://www.reddit.com/r/MachineLearning/comments/1lozfbp/p_i_created_an_opensource_tool_to_analyze_15m/","date":1751370063,"author":"/u/Avienir","guid":178934,"unread":true,"content":"<p>I've been working on a personal project to understand how AI is actually being used in medical research (not just the hype), and thought some of you might find the results interesting.</p><p>After analyzing nearly 1.5 million PubMed papers that use AI methods, I found some intersting results:</p><ul><li><strong>Classical ML still dominates</strong>: Despite all the deep learning hype, traditional algorithms like logistic regression and random forests account for 88.1% of all medical AI research</li><li><strong>Algorithm preferences by medical condition</strong>: Different health problems gravitate toward specific algorithms </li><li><strong>Transformer takeover timeline</strong>: You can see the exact point (around 2022) when transformers overtook LSTMs in medical research</li></ul><p>I built an interactive dashboard where you can:</p><ul><li>Search by medical condition to see which algorithms researchers are using</li><li>Track how algorithm usage has evolved over time</li><li>See the distribution across classical ML, deep learning, and LLMs</li></ul><p>One of the trickiest parts was filtering out false positives (like \"GAN\" meaning Giant Axonal Neuropathy vs. Generative Adversarial Network).</p><p>The tool is completely free, hosted on Hugging Face Spaces, and open-source. I'm not trying to monetize this - just thought it might be useful for researchers or anyone interested in healthcare AI trends.</p><p>Happy to answer any questions or hear suggestions for improving it!</p>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A guide to fine-grained permissions in MCP servers","url":"https://www.cerbos.dev/blog/dynamic-authorization-for-ai-agents-guide-to-fine-grained-permissions-mcp-servers","date":1751369099,"author":"/u/West-Chard-1474","guid":179075,"unread":true,"content":"<p>AI Agents are rapidly evolving beyond simple Retrieval-Augmented Generation (RAG) and are now expected to take action. This is made possible through standards like the Model Context Protocol (MCP), which allows agents to interact with external tools and APIs. However, this new capability introduces a critical challenge: implementing fine-grained permissions and access controls based on ‚Äúwho can do what?‚Äù.</p><p>Hardcoding  statements for user roles is not a scalable or secure solution. Modern applications require a dynamic authorization model that can make decisions based on a rich set of attributes - a model often referred to as Policy-Based Access Control or Attribute-Based Access Control.</p><p>This guide will walk you through building a secure MCP server where AI Agent tool access is managed by Cerbos, a decoupled, policy-driven authorization service. You will learn how to enforce fine-grained authorization by externalizing access controls into human-readable policies.</p><blockquote><p>See how to implement dynamic authorization for AI agents, and fine-grained permissions in MCP servers, using Cerbos - <a href=\"https://www.cerbos.dev/secure-your-mcp-server-workshop\">speak with an engineer</a>.</p></blockquote><h3>The challenge. Static permissions in a dynamic AI world</h3><p>When an AI Agent acts on behalf of a user, it must be subject to delegated (or an attenuated form of) permissions as that user. The challenge is that these Permissions are often complex and context-dependent. For example:</p><ul><li>A user might be able to  an expense but not  it.</li><li>A manager might be able to  expenses, but only for their own team.</li><li>An admin might be the only one who can  records.</li></ul><p>Implementing this logic directly in the MCP server creates brittle, hard-to-manage code. A change in your authorization policy requires a code change and a full redeployment.</p><h3>The solution. Decoupled authorization with Cerbos and MCP</h3><p>The Model Context Protocol (MCP) is a specification that standardizes communication between AI Agents and external tools. An MCP server exposes a list of available tools, which any MCP client, be it a human in a chat application or a native AI agent, can then invoke to perform actions in your system.</p><p>Cerbos is a stateless, <a href=\"https://www.cerbos.dev/product-cerbos-pdp\">open source authorization</a> service that externalizes access controls into declarative YAML policies. Your application queries the Cerbos Policy Decision Point with a question like, \"Can this principal perform this action on this resource?\" Cerbos evaluates the relevant policies and returns a simple allow/deny decision in milliseconds. This enables powerful <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/pbac\">PBAC</a> and <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/abac\">ABAC</a> without complicating your application logic.</p><p>By combining MCP and Cerbos, you build a system where the MCP server defines , but dynamically enables only the ones the user has permission to use for a given request.</p><h3>Step-by-step implementation guide</h3><h4>Step 1: Declarative policy authoring</h4><p>First, define your access controls in a Cerbos policy. This policy will govern which roles have permission to use which tools (actions).</p><p>Create a  directory and add the following  file.</p><p>File: <code>policies/mcp_expenses.yaml</code></p><pre><code>apiVersion: \"api.cerbos.dev/v1\"\nresourcePolicy:\n  version: \"default\"\n  resource: \"mcp::expenses\"\n  rules:\n    - actions: [\"list_expenses\"]\n      effect: EFFECT_ALLOW\n      roles: [\"admin\", \"manager\", \"user\"]\n\n    - actions: [\"add_expense\"]\n      effect: EFFECT_ALLOW\n      roles: [\"user\"]\n\n    - actions: [\"approve_expense\", \"reject_expense\"]\n      effect: EFFECT_ALLOW\n      roles: [\"admin\", \"manager\"]\n\n    - actions: [\"delete_expense\", \"superpower_tool\"]\n      effect: EFFECT_ALLOW\n      roles: [\"admin\"]\n</code></pre><h4>Step 2: Deploying the Cerbos PDP</h4><p>Run the Cerbos PDP in Docker, mounting your policies directory. This makes your authorization policies live and ready to be queried.</p><pre><code>docker run --rm -it -p 3593:3593 \\\n  -v \"$(pwd)/policies\":/policies \\\n  ghcr.io/cerbos/cerbos:latest\n</code></pre><h4>Step 3: Integrating the MCP server</h4><p>Create a Node.js Express server that connects to the Cerbos PDP.</p><pre><code>npm install express @modelcontextprotocol/sdk @cerbos/grpc\n</code></pre><ol start=\"2\"><li>Create the server: The code below defines every tool but uses  to perform a central authorization check. Based on the response, it dynamically enables only the permitted tools for the session. How the identity gets passed to this is out of scope, but with the recent OAuth improvements in the MCP spec, you will be able to token with the user's identity from an OAuth2 authorization server and pass it through to the MCP server.</li></ol><pre><code>import express from \"express\";\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StreamableHTTPServerTransport } from \"@modelcontextprotocol/sdk/server/streamableHttp.js\";\nimport { GRPC } from \"@cerbos/grpc\";\nimport { randomUUID } from \"node:crypto\";\n\nconst cerbos = new GRPC(\"localhost:3593\", { tls: false });\n\nasync function getServer({ user, sessionId }) {\n  const server = new McpServer({ name: \"CerbFinance MCP Server\" });\n\n  // Example tools - actual implementation is out of scope\n  const tools = {\n    list_expenses: server.tool(\n      \"list_expenses\",\n      \"Lists expenses.\",\n      {},\n      { title: \"List Expenses\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    add_expense: server.tool(\n      \"add_expense\",\n      \"Adds an expense.\",\n      {},\n      { title: \"Add Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    approve_expense: server.tool(\n      \"approve_expense\",\n      \"Approves an expense.\",\n      {},\n      { title: \"Approve Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    reject_expense: server.tool(\n      \"reject_expense\",\n      \"Rejects an expense.\",\n      {},\n      { title: \"Reject Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    delete_expense: server.tool(\n      \"delete_expense\",\n      \"Deletes an expense.\",\n      {},\n      { title: \"Delete Expense\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n    superpower_tool: server.tool(\n      \"superpower_tool\",\n      \"Grants superpowers.\",\n      {},\n      { title: \"Superpower Tool\" },\n      async () =&gt; ({ content: [{ type: \"text\", text: \"...\" }] })\n    ),\n  };\n\n  const toolNames = Object.keys(tools);\n\n  // Central Authorization Check\n  const authorizedTools = await cerbos.checkResource({\n    principal: { id: user.id, roles: user.roles },\n    resource: { kind: \"mcp::expenses\", id: sessionId },\n    actions: toolNames,\n  });\n\n  for (const toolName of toolNames) {\n    if (authorizedTools.isAllowed(toolName)) {\n      tools[toolName].enable();\n    } else {\n      tools[toolName].disable();\n    }\n  }\n\n  server.sendToolListChanged();\n  return server;\n}\n\nconst app = express();\napp.use(express.json());\n\n// Middleware to simulate user authentication - use OAuth in production\napp.use((req, res, next) =&gt; {\n  req.user = { id: \"user-123\", roles: [\"user\"] }; // Test different roles here\n  next();\n});\n\napp.post(\"/mcp\",  async (req, res) =&gt; {\n  const transport = new StreamableHTTPServerTransport({\nsessionIdGenerator: undefined,\n});\n  const server = await getServer({\n    user: req.user,\n    sessionId: req.sessionId || randomUUID(),\n  });\n  await server.connect(transport);\n  await transport.handleRequest(req, res, req.body);\n});\napp.listen(3000, () =&gt; console.log(\"MCP Server running on port 3000\"));\n</code></pre><h3>Testing your policy-driven AI agent</h3><p>You can test your server using the MCP Client extension in VS Code.</p><ol><li>Open the Command Palette (Ctrl+Shift+P) and select \"MCP: Add Server\".</li><li>Enter your server URL: <code>http://localhost:3000/mcp</code>.</li><li>Run in Copilot to open a chat window. The available tools will be listed.</li></ol><p>Example prompts by role: Change the  in  to simulate different users.</p><ul><li>As a  ():\n<ul><li><code>\"Add an expense for $100\"</code> -&gt; Succeeds</li><li> -&gt; Fails (The agent reports it doesn't have the tool).</li></ul></li><li>As a  (<code>roles: ['manager', 'user']</code>):\n<ul><li> -&gt; Succeeds</li><li> -&gt; Fails</li></ul></li><li>As an  ():\n<ul><li> -&gt; Succeeds</li></ul></li></ul><h3>Beyond roles - the power of ABAC</h3><p><a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/rbac\">Role-Based Access Control</a> is just the beginning. The real power of a decoupled authorization system is implementing ABAC. With Cerbos, you can write policies that use attributes from the user (), the resource, or the request itself.</p><p>For example, to restrict managers to approving expenses only up to a certain amount, you could pass the amount as an attribute and write a condition, then do an additional check inside the tool implementation:</p><p> (snippet of the Cerbos call):</p><pre><code>await cerbos.checkResource({\n  principal: { id: user.id, roles: user.roles },\n  resource: {\n    kind: \"mcp::expenses\",\n    id: sessionId,\n    attr: { amount: 150 } // Pass resource attributes\n  },\n  actions: [\"approve_expense\"],\n});\n</code></pre><p><code>policies/mcp_expenses.yaml</code> (snippet of the policy rule):</p><pre><code>- actions: [\"approve_expense\"]\n  effect: EFFECT_ALLOW\n  roles: [\"manager\"]\n  condition:\n    match:\n      # The manager can only approve if the expense amount is less than 1000\n      expr: request.resource.attr.amount &lt; 1000\n</code></pre><p>This demonstrates true fine-grained authorization that goes far beyond simple roles.</p><p>By decoupling your authorization logic using Cerbos, you can build powerful, secure, and scalable AI Agents. This architecture allows you to manage Permissions through declarative policies, enabling you to implement everything from simple role-based rules to sophisticated ABAC without touching your application code. As AI agents become more integrated into our workflows, a robust, policy-driven approach to access controls is a necessity.</p><p>For further details on mastering dynamic authorization for MCP servers with Cerbos, <a href=\"https://www.cerbos.dev/features-benefits-and-use-cases/dynamic-authorization-for-MCP-servers\">check out this piece</a>.</p>","contentLength":9414,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1loz4jd/a_guide_to_finegrained_permissions_in_mcp_servers/"},{"title":"Lies we tell ourselves to keep using Golang","url":"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang","date":1751368552,"author":"/u/Nekuromento","guid":179015,"unread":true,"content":"<div><p>\n                    üëã This page was last updated ~3 years ago. Just so you know.\n                </p></div><p data-bo=\"117\">In the two years since I‚Äôve posted <a href=\"https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride\">I want off Mr Golang‚Äôs Wild\nRide</a>, it‚Äôs made the rounds time and\ntime again, on Reddit, on Lobste.rs, on HackerNews, and elsewhere.</p><p data-bo=\"329\">And every time, it elicits the same responses:</p><ul><li>You talk about Windows: that‚Äôs not what Go is good at! (Also, who cares?)</li><li>This is very one-sided: you‚Äôre not talking about the  sides of Go!</li><li>You don‚Äôt understand the compromises Go makes.</li><li>Large companies use Go, so it can‚Äôt be  bad!</li><li>Modelling problems ‚Äúcorrectly‚Äù is too costly, so caring about correctness is moot.</li><li>Correctness is a spectrum, Go lets you trade some for development speed.</li><li>Your go-to is Rust, which also has shortcomings, so your argument is invalid.</li></ul><p data-bo=\"878\">There‚Äôs also a vocal portion of commenters who wholeheartedly agree with the\nrant, but let‚Äôs focus on unpacking the apparent conflict here.</p><p data-bo=\"1019\">I‚Äôll first spend a short amount of time pointing out clearly disingenuous\narguments, to get them out of the way, and then I‚Äôll move on to the fairer\ncomments, addressing them as best I can.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#the-author-is-a-platypus\"></a><p data-bo=\"1239\">When you don‚Äôt want to hear something, one easy way to <em>not have to think about\nit at all</em> is to convince yourself that whoever is saying it is incompetent, or\nthat they have ulterior motives.</p><p data-bo=\"1433\">For example, the top comment on HackerNews right now starts like this:</p><blockquote><p data-bo=\"1507\">The author fundamentally misunderstands language design.</p></blockquote><p data-bo=\"1565\">As an impostor syndrome enthusiast, I would normally be sympathetic to such\ncomments. However, it is a lazy and dismissive way to consider any sort of\nfeedback.</p><p data-bo=\"1727\">It doesn‚Äôt take much skill to notice a problem.</p><p data-bo=\"1776\">In fact, as developers get more and more senior, they tend to ignore more and\nmore problems, because they‚Äôve gotten so used to it. That‚Äôs the way it‚Äôs always\nbeen done, and they‚Äôve learned to live with them, so they‚Äôve stopped questioning\nit any more.</p><p data-bo=\"2029\">Junior developers however, get to look at everything again with a fresh pair of\neyes: they haven‚Äôt learned to ignore all the quirks yet, so it feels\n to them, and they tend to question it (if they‚Äôre made to feel\nsafe enough to voice their concerns).</p><p data-bo=\"2296\">This alone is an extremely compelling reason to hire junior developers, which I\nwish more companies would do, instead of banking on the fact that ‚Äúseniors can\nget up-to-speed with our current mess faster‚Äù.</p><p data-bo=\"2503\">As it happens, I am  a junior developer, far from it. Some way or another,\nover the past 12 years, seven different companies have found an excuse to pay me\nenough money to cover rent and then some.</p><p data-bo=\"2707\">I did, in fact, <a href=\"https://ooc-lang.github.io/\">design a language</a> all the way back in\n2009 (when I  a wee programmer baby), focused mainly on syntactic sugar\nover C. At the time it was deemed interesting enough to warrant an invitation to\nOSCON (my first time in Portland Oregon, the capital of grunge, coffee, poor\nweather and whiteness), where I got to meet other young and not-so-young\nwhippersnappers (working on Io, Ioke, Wren, JRuby, Clojure, D, Go, etc.)</p><p data-bo=\"3174\">It was a very interesting conference: I‚Äôm still deeply ashamed by the\npresentation I gave, but I remember fondly the time an audience member asked the\nGo team <strong>‚Äúwhy did you choose to ignore any research about type systems since the\n1970s‚Äù</strong>? I didn‚Äôt fully understand the implications at the time, but I sure do\nnow.</p><p data-bo=\"3493\">I have since thoroughly lost interest in my language, because I‚Äôve started\ncaring about semantics a lot more than syntax, which is why I also haven‚Äôt\nlooked at Zig, Nim, Odin, etc: I am no longer interested in ‚Äúa better C‚Äù.</p><p data-bo=\"3718\">But <em>all of that is completely irrelevant</em>. It doesn‚Äôt matter who points out that\n‚Äúmaybe we shouldn‚Äôt hit ourselves in the head with a rake repeatedly‚Äù: that\nfeedback ought to be taken under advisement no matter who it comes from.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#mom-smokes-so-it-s-probably-okay\"><h2>Mom smokes, so it‚Äôs probably okay</h2></a><p data-bo=\"3988\">One of the least effective way to shop for technologies (which CTOs, VPs of\nengineering, principals, senior staff and staff engineers need to do regularly)\nis to look at what other companies are using.</p><p data-bo=\"4191\">It is a great way to  technologies to evaluate (that or checking\nThoughtWorks‚Äô <a href=\"https://www.thoughtworks.com/radar\">Tech Radar</a>), but it‚Äôs far\nfrom enough.</p><p data-bo=\"4358\">A piece from company X on ‚Äúhow they used technology Y‚Äù, will \nreflect the true cost of adopting that technology. By the point the engineers\nbehind the post have been bullied into filling out the company‚Äôs tech blog after\nmonths of an uphill battle, the decision has been made, and there‚Äôs no going\nback.</p><p data-bo=\"4676\">This kind of blog doesn‚Äôt lend itself to coming out and admitting that mistakes\nwere made. It‚Äôs supposed to make the company look good. It‚Äôs supposed to attract\nnew hires. It‚Äôs supposed to help us stay .</p><p data-bo=\"4891\">Typically, scathing indictments of technologies come from , who\nhave simply decided that they, as a person, can afford making a lot of people\nangry. Companies typically cannot.</p><p data-bo=\"5431\">You can be impressed, that  are using Go, right now, and that\nthey have gone all the way to Davy Jones‚Äô Locker and back to solve complex\nproblems that ultimately helps deliver value to customers.</p><p data-bo=\"5646\">Or you can be , as you realize that those complex problems only exist\n. Those complex problems would not exist in other\nlanguages, not even in C, which I can definitely not be accused of shilling for\n(and would not recommend as a Go replacement).</p><p data-bo=\"5931\">A lot of the pain in the  article is caused by:</p><ul><li>Go not having sum types ‚Äî making it really awkward to have a type that is\n‚Äúeither an IPv4 address or an IPv6 address‚Äù</li><li>Go choosing which data structures you need ‚Äî in this case, it‚Äôs the\none-size-fits-all slice, for which you pay 24 bytes on 64-bit machines.</li><li>Go not letting you do operator overloading, harkening back to the Java days\nwhere  isn‚Äôt the same as </li><li>Go‚Äôs lack of support for immutable data ‚Äî the only way to prevent something\nfrom being mutated is to only hand out copies of it, and to \nto not mutate it in the code that actually has access to the inner bits.</li><li>Go‚Äôs unwillingness to let you make an opaque ‚Äúnewtype‚Äù. The only way to do\nit is to make a separate package and use interfaces for indirection, which is\ncostly  awkward.</li></ul><p data-bo=\"6806\">Unless you‚Äôre out for confirmation bias, that whole article is a very compelling\nargument against using Go for that specific problem.</p><p data-bo=\"6941\">And yet Tailscale is using it. Are they wrong? Not necessarily! Because their\nteam is made up of a bunch of . As evidenced by the  article,\nabout the Go linker.</p><p data-bo=\"7122\">Because they‚Äôre Go experts, they know the cost of using Go upfront, and they‚Äôre\nequipped to make the decision whether or not it‚Äôs worth it. They know how Go\nworks deep down (something Go marketing pinky-swears you <em>never need to worry\nabout, why do you ask?</em>), so if they hit edge cases, they can dive into it, fix\nit, and wait for their fix to be upstreamed (if ever).</p><p data-bo=\"7493\">But chances are, . This is not your org. You are not Google\neither, and you cannot afford to build a whole new type system on top of Go just\nto make your project (Kubernetes) work at all.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#the-good-parts\"></a><p data-bo=\"7718\">But okay - Tailscale‚Äôs usage of Go is pretty  still. Just like my\n2020 piece about Windows raised an army of ‚Äúbut that‚Äôs not what Go is good for‚Äù\nobjections, you could dismiss Tailscale‚Äôs posts as ‚Äúwell that‚Äôs on you for\nwanting to ship stuff on iOS / doing low-level network stuff‚Äù.</p><p data-bo=\"8014\">Fair enough! Okay. Let‚Äôs talk about what makes Go compelling.</p><p data-bo=\"8077\">Go is a pretty good async runtime, with opinionated defaults, a\n<a href=\"https://go.dev/blog/ismmkeynote\">state-of-the-art garbage collector</a> with two\nknobs, and tooling that would make C developers jealous, if they bothered\nlooking outside their bubble.</p><p data-bo=\"8325\">This also describes <a href=\"https://nodejs.org/en/\">Node.js</a> from the very start (which\nis essentially libuv + V8), and I believe it also describes ‚Äúmodern Java‚Äù, with\nAPIs like NIO. Although I haven‚Äôt checked what‚Äôs happening in Java land too\nclosely, so if you‚Äôre looking for an easy inaccuracy to ignore this whole\narticle, there you go: that‚Äôs a freebie.</p><p data-bo=\"8678\">Because the async runtime is core to the language, it comes with tooling that\n make Rust developers jealous! I talk about it in <a href=\"https://fasterthanli.me/articles/request-coalescing-in-async-rust\">Request coalescing in\nasync Rust</a>, for example.</p><p data-bo=\"8906\">Go makes it easy to dump backtraces (stack traces) for all running goroutines in\na way <a href=\"https://tokio.rs/\">tokio</a> doesn‚Äôt, at this time. It is also able to\ndetect deadlocks, it comes with its own profiler, it seemingly lets you not\nworry about <a href=\"https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/\">the color of\nfunctions</a>, etc.</p><p data-bo=\"9258\">Go‚Äôs tooling around package management, refactoring, cross-compiling, etc., is\neasy to pick up and easy to love ‚Äî and certainly feels at first like a definite\nimprovement over the many person-hours lost to the whims of pkg-config,\nautotools, CMake, etc. Until you reach some of the arbitrary limitations that\nsimply do not matter to the Go team, and then you‚Äôre on your own.</p><p data-bo=\"9636\">All those and more explains why many, including me, were originally enticed by\nit: enough to write piles and piles of it, until its shortcomings have finally\nbecome impossible to ignore, by which point it‚Äôs too late. You‚Äôve made your bed,\nand now you‚Äôve got to make yourself feel okay about lying in it.</p><p data-bo=\"9941\">But one  does not a platform make.</p><p data-bo=\"9994\">The really convenient async runtime is not the only thing you adopted. You also\nadopted a  toolchain, a build system, a calling convention, a\nsingle GC (whether it works for you or not), the set of included batteries, some\nof which you CAN swap out, but the rest of the ecosystem won‚Äôt, and most\nimportantly, you adopted a language that happened by accident.</p><p data-bo=\"10367\">I will grant you that caring  about something is grounds for\nsuspicion. It is no secret that a large part of what comes out of academia is\nwoefully inapplicable in the industry at this time: it is easy to lose oneself\nin the abstract, and come up with convoluted schemes to solve problems that do\nnot really exist for anyone else.</p><p data-bo=\"10709\">I imagine this is the way some folks feel about Rust.</p><p data-bo=\"10764\">But caring  about something is dangerous too.</p><p data-bo=\"10823\">Evidently, the Go team didn‚Äôt  to design a language. What they really\nliked was their async runtime. And they wanted to be able to implement TCP, and\nHTTP, and TLS, and HTTP/2, and DNS, etc., on top of it. And then web services on\ntop of all of that.</p><p data-bo=\"11081\">And so they didn‚Äôt. They didn‚Äôt design a language. It sorta just ‚Äúhappened‚Äù.</p><p data-bo=\"11159\">Because it needed to be familiar to ‚ÄúGooglers, fresh out of school, who probably\nlearned some Java/C/C++/Python‚Äù (Rob Pike, Lang NEXT 2014), it borrowed from all\nof these.</p><p data-bo=\"11332\">Just like C, it doesn‚Äôt concern itself with error handling . Everything\nis a big furry ball of mutable state, and it‚Äôs on you to add ifs and elses to\nVERY CAREFULLY (and very manually) ensure that you do not propagate invalid\ndata.</p><p data-bo=\"11573\">Just like Java, it tries to erase the distinction between ‚Äúvalue‚Äù and\n‚Äúreference‚Äù, and so it‚Äôs impossible to tell from the callsite if something is\ngetting mutated or not:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"11746\"></figure><p data-bo=\"11886\">Depending on whether the signature for change is this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"11942\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"12000\"></figure><p data-bo=\"12049\">‚Ä¶the local  in  will either get mutated or not.</p><p data-bo=\"12109\">And since, just like C  Java, you do not get to decide what is mutable and\nwhat is immutable (the  keyword in C is essentially advisory,\n<a href=\"https://twitter.com/jckarter/status/1520059601277792256\">kinda</a>), passing a\nreference to something (to avoid a costly copy, for example) is fraught with\nrisk, like it getting mutated from under you, or it being held somewhere\nforever, preventing it from being freed (a lesser, but very real, problem).</p><p data-bo=\"12562\">Go fails to prevent many other classes of errors: it makes it easy to\naccidentally <a href=\"https://fasterthanli.me/articles/a-rust-match-made-in-hell\">copy a mutex</a>, rendering it\ncompletely ineffective, or leaving struct fields uninitialized (or rather,\ninitialized to their zero value), resulting in countless logic errors.</p><p data-bo=\"12858\">Taken in isolation, each of these and more can be dismissed as ‚Äújust a thing to\nbe careful about‚Äù. And breaking down an argument to its smallest pieces,\nrebutting them one by one, is a self-defense tactic used by those who cannot\nafford to adjust their position in the slightest.</p><p data-bo=\"13139\">Which makes perfect sense, because Go is really hard to move away from.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#go-is-an-island\"></a><p data-bo=\"13232\">Unless you use cgo, (but <a href=\"https://dave.cheney.net/2016/01/18/cgo-is-not-go\">cgo is not\nGo</a>), you are living in the\nPlan 9 cinematic universe.</p><p data-bo=\"13375\">The Go toolchain does not use the assembly language everyone else knows about.\nIt does not use the linkers everyone else knows about. It does not let you use\nthe debuggers everyone knows about, the <a href=\"https://valgrind.org/\">memory checkers</a>\neveryone knows about, or the calling conventions everyone else has agreed to\nsuffer, in the interest of interoperability.</p><p data-bo=\"13737\">Go is closer to\n<a href=\"https://en.wikipedia.org/wiki/Open-world_assumption\">closed-world</a> languages\nthan it is to C or C++. Even Node.js, Python and Ruby are not as hostile to\n<a href=\"https://en.wikipedia.org/wiki/Foreign_function_interface\">FFI</a>.</p><p data-bo=\"13973\">To a large extent, this is a feature: being different is . And it\ncomes with its benefits. Being able to profile the internals of the TLS and HTTP\nstacks the same way you do your business logic is fantastic. (Whereas in dynamic\nlanguages, the stack trace stops at OpenSSL). And that code takes full advantage\nof the lack of function coloring: it can let the  worry about\nnon-blocking I/O and scheduling.</p><p data-bo=\"14398\">But it comes at a terrible cost, too. There is excellent tooling out there for\nmany things, which you cannot use with Go (you can use it for the cgo parts,\nbut again, you should not use cgo if you want the Real Go Experience). All the\n‚Äúinstitutional knowledge‚Äù there is lost, and must be relearned from scratch.</p><p data-bo=\"14711\">It also makes it extremely hard to integrate Go with anything else, whether it‚Äôs\nupstream (calling C from Go) or downstream (calling Go from Ruby). Both these\nscenarios involve cgo, or, if you‚Äôre unreasonably brave, <a href=\"https://words.filippo.io/rustgo/\">a terrifying\nhack</a>.</p><p data-bo=\"15081\">Making Go play nice with another language (any other language) is really hard.\nCalling C from Go, nevermind the cost of crossing the FFI boundary, involves\n<a href=\"https://pkg.go.dev/cmd/cgo#hdr-Passing_pointers\">manual descriptor tracking</a>,\nso as to not break the GC. (WebAssembly had the same problem before <a href=\"https://github.com/WebAssembly/reference-types/blob/master/proposals/reference-types/Overview.md\">reference\ntypes</a>!)</p><p data-bo=\"15503\">Calling Go from  involves shoving the whole Go runtime (GC included)\ninto whatever you‚Äôre running: expect a very large static library and all the\noperational burden of running Go code as a regular executable.</p><p data-bo=\"15723\">After spending years doing those FFI dances in both directions, I‚Äôve reached the\nconclusion that <strong>the only good boundary with Go is a network boundary</strong>.</p><p data-bo=\"15879\">Integrating with Go is  if you can afford to pay the\nlatency cost of doing RPC over TCP (whether it‚Äôs a REST-ish HTTP/1 API,\nsomething like JSON-RPC, a more complicated scheme like GRPC, etc.). It‚Äôs also\nthe only way to make sure it doesn‚Äôt ‚Äúinfect‚Äù your whole codebase.</p><p data-bo=\"16172\">But even that is costly: you need to maintain invariants on both sides of the\nboundary. In Rust, one would typically reach for something like\n<a href=\"https://serde.rs/\">serde</a> for that, which, combined with sum types and the lack\nof zero values, lets you make  that what you‚Äôre holding is what\nyou think you‚Äôre holding: if a number is zero, it was meant to be zero, it\nwasn‚Äôt just missing.</p><p data-bo=\"16573\">(All this goes out the window if you use a serialization format like\n<a href=\"https://developers.google.com/protocol-buffers\">protobuf</a>, which has all the\ndrawbacks of Go‚Äôs type system and none of the advantages).</p><p data-bo=\"16780\">That still leaves you with the Go side of things, where unless you use some sort\nof <a href=\"https://go-ozzo.github.io/ozzo-validation/\">validation package</a> religiously,\nyou need to be ever vigilant not to let bad data slip in, because the compiler\ndoes  to help you maintain those invariants.</p><p data-bo=\"17076\">And that brings us to the larger overall problem of the Go .</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#all-or-nothing-so-let-s-do-nothing\"><h2>All or nothing (so let‚Äôs do nothing)</h2></a><p data-bo=\"17188\">I‚Äôve mentioned ‚Äúleaving struct fields uninitialized‚Äù. This happens easily when\nyou make a code change from something like this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"17317\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"17520\"></figure><p data-bo=\"17718\">That second program prints this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"17752\"></figure><p data-bo=\"17804\">We‚Äôve essentially changed the function signature, but forgot to update a\ncallsite. This doesn‚Äôt bother the compiler at all.</p><p data-bo=\"17929\">Oddly enough, if our function was structured like this:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"17986\"></figure><p data-bo=\"18133\">Then we‚Äôd get a compile error:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"18165\"></figure><p data-bo=\"18310\">Why does the Go compiler suddenly care if we provide explicit values now? If the\nlanguage was self-consistent, it would let me omit both parameters, and just\ndefault to zero.</p><p data-bo=\"18486\">Because one of the tenets of Go is that zero values are good, actually.</p><p data-bo=\"18559\">See, they let you go fast. If you  mean for  to be zero, you can just\nnot specify it.</p><p data-bo=\"18654\">And sometimes it works fine, because zero values  mean something:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"18725\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"18933\"></figure><p data-bo=\"18995\">This is fine! Because the  slice is actually a reference type, and its\nzero value is , and  just returns zero, because ‚Äúobviously‚Äù, a\nnil slice is empty.</p><p data-bo=\"19174\">And sometimes it‚Äôs  fine, because zero values don‚Äôt mean what you think\nthey mean:</p><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"go\" data-bo=\"19263\"></figure><figure role=\"region\" aria-label=\"Code block\" translate=\"no\" data-lang=\"\" data-bo=\"19475\"></figure><p data-bo=\"19678\">In that case, you should‚Äôve initialized the map first (which is also \na reference type), with , or with a map literal.</p><p data-bo=\"19814\">That alone is enough to cause incidents and outages that wake people up at\nnight, but everything gets worse real fast when you consider the <a href=\"https://dave.cheney.net/2014/03/19/channel-axioms\">Channel\nAxioms</a>:</p><ul><li>A send to a  channel blocks forever</li><li>A receive from a  channel blocks forever</li><li>A send to a closed channel panics</li><li>A receive from a closed channel returns the zero value immediately</li></ul><p data-bo=\"20221\">Because <em>there had to be a meaning</em> for nil channels, this is what was picked.\nGood thing there‚Äôs pprof to find those deadlocks!</p><p data-bo=\"20351\">And because there‚Äôs no way to ‚Äúmove‚Äù out of values, there has to be meaning for\nreceiving and sending to closed channels, too, because even after you close them\n<em>you can still interact with them</em>.</p><p data-bo=\"20549\">(Whereas in a language like Rust, a channel closes when its\n<a href=\"https://doc.rust-lang.org/stable/std/sync/mpsc/struct.Sender.html\">Sender</a> is\ndropped, which only happens when <em>nobody can touch it again, ever</em>. The same probably\napplies to C++ and a bunch of other languages, this is not new stuff).</p><p data-bo=\"20846\">‚ÄúZero values have meaning‚Äù is naive, and clearly untrue when you consider the\ninputs of, like‚Ä¶ almost everything. There‚Äôs so many situations when values\nneed to be ‚Äúone of these known options, and nothing else‚Äù, and that‚Äôs where\nsum types come in (in Rust, that‚Äôs <a href=\"https://fasterthanli.me/articles/peeking-inside-a-rust-enum\">enums</a>).</p><p data-bo=\"21161\">And Go‚Äôs response to that is: just be careful. Just like C‚Äôs response before it.</p><p data-bo=\"21243\">Just don‚Äôt access the return value if you haven‚Äôt checked the error value. Just\nhave a half-dozen people carefully review each trivial code change to make sure\nyou‚Äôre not accidentally propagating a nil, zero, or empty string way too deep\ninto your system.</p><p data-bo=\"21500\">It‚Äôs just another thing watch out for.</p><p data-bo=\"21540\">It‚Äôs not like you can prevent  problems anyway.</p><p data-bo=\"21761\">And you can write logic errors in just about every language! And if you try hard\nenough I‚Äôm sure you can drive a train straight into a tree! It‚Äôs just much\neasier with a car.</p><p data-bo=\"21937\">The fallacy here is that because it is impossible to solve , we\nshouldn‚Äôt even attempt to solve . By that same logic, it‚Äôs always\nworthless to support any individual financially, because it does nothing to help\nevery  individual who‚Äôs struggling.</p><p data-bo=\"22216\">And this is another self-defense tactic: to refuse to consider anything but the\nmost extreme version of a position, and point out how ridiculous it is (ignoring\nthe fact that nobody is actually defending that ridiculous, extreme position).</p><p data-bo=\"22457\">So let‚Äôs talk about that position.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#rust-is-perfect-and-you-re-all-idiots\"><h2>‚ÄúRust is perfect and you‚Äôre all idiots‚Äù</h2></a><p data-bo=\"22537\">I  that was how I felt, because it would be so much simpler to explain.</p><p data-bo=\"22619\">That fantasy version of my argument is so easy to defeat, too. ‚ÄúHow come you use\nLinux then? That‚Äôs written in C‚Äù. ‚ÄúUnsafe Rust is incredibly hard to write\ncorrectly, how do you feel about that?‚Äù</p><p data-bo=\"22816\">The success of Go is due in large part to it having batteries included and\nopinionated defaults.</p><p data-bo=\"22914\">The success of Rust is due in large part to it being easy to adopt piecemeal\nand .</p><p data-bo=\"23024\">They are both success stories, just very different ones.</p><p data-bo=\"23082\">If the boogeyman is to be believed, ‚ÄúRust shills‚Äù would have everyone\nimmediately throw away everything, and replace it with The Only Good Language\nOut there.</p><p data-bo=\"23242\">This is  from what‚Äôs happening in the real world, it‚Äôs tragic.</p><p data-bo=\"23904\">None of these are without challenges, and none of the people involved are\ndenying said challenges. But all of these are incremental and pragmatic, very\nprogressively porting parts to a safer language .</p><p data-bo=\"24129\">We are very far from a ‚Äúthrowing the baby out with the bathwater‚Äù approach. The\nRust codegen backend  is a mountain of C++ code (LLVM).\nThe alternatives are not competitors by any stretch of the imagination, except\nmaybe for <a href=\"https://github.com/rust-lang/rustc_codegen_gcc\">another mountain of C++\ncode</a>.</p><p data-bo=\"24460\">The most hardcore Rust users are the most vocal about issues like build times,\nthe lack of certain language features (I just want\n<a href=\"https://blog.rust-lang.org/2021/08/03/GATs-stabilization-push.html\">GATs</a>!),\nand all the other shortcomings everyone else is also talking about.</p><p data-bo=\"24737\">And they‚Äôre also the first to be on the lookout for other, newer languages, that\ntackle the same kind of problems, but do it .</p><p data-bo=\"24878\">But as with the ‚Äúquestioning your credentials‚Äù angle, .\nThe current trends could be dangerous snake oil and we could have literally no\ndecent alternative, and <em>it would still be worth talking about</em>. No matter who\nraises the point!</p><p data-bo=\"25135\">Creating false dichotomies isn‚Äôt going to help resolve any of this.</p><p data-bo=\"25204\">Folks who develop an allergic reaction to ‚Äúbig balls of mutable state without\nsum types‚Äù tend to gravitate towards languages that gives them control over\nmutability, lifetimes, and lets them build abstractions. That those languages\nhappen to often be Go and Rust is immaterial. Sometimes it‚Äôs C and Haskell.\nSometimes it‚Äôs ECMAScript and Elixir. I can‚Äôt speak to those, but they do\nhappen.</p><p data-bo=\"25595\">You don‚Äôt have to choose between ‚Äúgoing fast‚Äù and ‚Äúmodelling literally every\nlast detail of the problem space‚Äù. And you‚Äôre not stuck doing one or the other\nif you choose Go or Rust.</p><p data-bo=\"25778\">You can, at great cost, write extremely careful Go code that stays far away from\nstringly-typed values and constantly checks invariants ‚Äî you just get no help\nfrom the compiler whatsoever.</p><p data-bo=\"25970\">And you can, fairly easily, decide not to care about a whole bunch of cases when\nwriting Rust code. For example, if you‚Äôre not writing a low-level command-line\nutility like , you can decide to only care about paths that are valid UTF-8\nstrings by using <a href=\"https://lib.rs/crates/camino\">camino</a>.</p><p data-bo=\"26271\">When handling errors, it is extremely common to list a few options we  care\nabout and want to do special handling for, and shove everything else into an\n‚ÄúOther‚Äù or ‚ÄúInternal‚Äù or ‚ÄúUnknown‚Äù variant, which we can flesh out later as\nneeded, when reviewing logs.</p><p data-bo=\"26534\">The ‚Äúcorrect‚Äù way to assume an optional value is set, is to ,\nnot to use it regardless. That‚Äôs the difference between calling \nand crossing your fingers, and calling\n<a href=\"https://doc.rust-lang.org/stable/std/option/enum.Option.html#method.unwrap\">unwrap()</a>\non an .</p><p data-bo=\"26842\">And it‚Äôs so much easier to do it correctly when the type system lets you spell\nout what the options are ‚Äî even when it‚Äôs as simple as ‚Äúok‚Äù or ‚Äúnot ok‚Äù.</p><p data-bo=\"26997\">Which brings me to the next argument, by far the most reasonable of the bunch.</p><a href=\"https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang#go-as-a-prototyping-starter-language\"><h2>Go as a prototyping/starter language</h2></a><p data-bo=\"27118\">We‚Äôve reached the fifth stage of grief: acceptance.</p><p data-bo=\"27171\">. It may well be that Go is not adequate for production services unless\nyour shop is literally made up of Go experts (Tailscale) or you have infinite\nmoney to spend on engineering costs (Google).</p><p data-bo=\"27374\">But surely there‚Äôs still a place for it.</p><p data-bo=\"27416\">After all, Go is an easy language to pick up (because it‚Äôs so small, right?),\nand a lot of folks have learned it by now, so it‚Äôs easy to recruit Go\ndevelopers, so we can get lots of them on the cheap and just uhhh prototype a\nfew systems?</p><p data-bo=\"27656\">And then later when things get hard (as they always do at scale) we‚Äôll either\nrewrite it to something else, or we‚Äôll bring in experts, we‚Äôll figure something\nout.</p><p data-bo=\"27820\">Except there is no such thing as throwaway code.</p><p data-bo=\"27870\">All engineering organizations I‚Äôve ever seen are EXTREMELY rewrite-averse, and\nfor good reason! They take time, orchestrating a seamless transition is hard,\ndetails get lost in the shuffle, you‚Äôre not shipping new features while you‚Äôre\ndoing that, you have to retrain your staff to be effective at the new thing,\netc.</p><p data-bo=\"28189\">Tons of good, compelling reasons.</p><p data-bo=\"28224\">So very few things eventually end up being rewritten. And as more and more\ncomponents get written in Go, there‚Äôs more and more reason to  doing that:\nnot because it‚Äôs working particularly well for you, but because interacting with\nthe existing codebases from  is so painful (except over\nthe network, and even then.. see ‚ÄúGo is an island‚Äù above).</p><p data-bo=\"28602\">So things essentially never improve. All the Go pitfalls, all the things the\nlanguage and compiler , are an issue for everyone,\nfresh or experienced. Linters help some, but can never do quite as much as\ncompiler for languages that took these problems seriously to begin with. \nthey slow down development, cutting into the ‚Äúfast development‚Äù promise.</p><p data-bo=\"28984\">All the complexity that doesn‚Äôt live in the language now lives in your codebase.\nAll the invariants you don‚Äôt have to spell out using types, you now have to\nspell out using code: the signal-to-noise ratio of your (very large) codebases\nis extremely poor.</p><p data-bo=\"29240\">Because it has been decided that abstractions are for academics and fools, and\nall you  need is slices and maps and channels and funcs and structs, it\nbecomes extremely hard to follow what any program is doing at a high level,\nbecause everywhere you look, you get bogged down in imperative code doing\ntrivial data manipulation or error propagation.</p><p data-bo=\"29598\">Because function signatures don‚Äôt tell you much of anything (does this mutate\ndata? does it hold onto it? is a zero value there okay? does it start a\ngoroutine? can that channel be nil? what types can I really pass for this\n param?), you rely on documentation, which is costly to update, and\ncostlier still  to update, resulting in more and more bugs.</p><p data-bo=\"29969\">The very reason I don‚Äôt consider Go a language ‚Äúsuitable for beginners‚Äù is\nprecisely that its compiler accepts so much code that is very clearly wrong.</p><p data-bo=\"30122\">It takes a lot of experience about everything  the language, everything\nGo willfully leaves as an exercise to the writer, to write semi-decent Go code,\nand even then, I consider it more effort than it‚Äôs worth.</p><p data-bo=\"30341\">The ‚Äúworse is better‚Äù debate was never about some people wanting to feel\nsuperior by adding needless complexity, then mastering it.</p><p data-bo=\"30474\">Quite the contrary, it‚Äôs an admission that humans suck at maintaining\ninvariants. All of us. But we are capable of building tools that can help us\ndoing that. And focusing our efforts on that has an upfront cost, but that cost\nis well worth it.</p><p data-bo=\"30720\">I thought we‚Äôd moved past the notion that ‚Äúprogramming is typing on a keyboard‚Äù\nlong ago, but when I keep reading ‚Äúbut it‚Äôs fast to write lots of Go!‚Äù, I‚Äôm not\nso sure.</p><p data-bo=\"30890\">Inherent complexity does not go away if you close your eyes.</p><p data-bo=\"30952\">When you choose not to care about complexity, you‚Äôre merely pushing it onto\nother developers in your org, ops people, your customers, . Now\n have to work around your assumptions to make sure everything keeps\nrunning smoothly.</p><p data-bo=\"31194\">And nowadays, I‚Äôm often that , and I‚Äôm tired of it.</p><p data-bo=\"31256\">Because there is a lot to like in Go at first, because it‚Äôs so easy to pick up,\nbut so hard to move away from, and because the cost of choosing it in the first\nplace reveals itself slowly over time, and compounds, only becoming unbearable\nwhen it‚Äôs much too late, this is not a discussion we can afford to ignore as an\nindustry.</p><p data-bo=\"31586\">Until we demand better of our tools, we are doomed to be woken up in the middle\nof the night, over and over again, because some  value slipped in where it\nnever should have.</p><div data-bo=\"31911\"><p data-bo=\"31924\">Here‚Äôs a list of lies we tell ourselves to keep using Golang:</p><ul><li>Others use it, so it must be good for us too</li><li>Everyone who has concerns about it is an elitist jerk</li><li>Its attractive async runtime and GC make up for everything else</li><li>Every language design flaw is ok in isolation, and ok in aggregate too</li><li>We can overcome these by ‚Äújust being careful‚Äù or adding more linters/eyeballs</li><li>Because it‚Äôs easy to write, it‚Äôs easy to develop production software with</li><li>Because the language is simple, everything else is, too</li><li>We can do just a little of it, or just at first, or we can move away from it easily</li><li>We can always rewrite it later</li></ul></div><div data-context=\"end-of-page\">\n            (JavaScript is required to see this. Or maybe my stuff broke)\n        </div>","contentLength":27436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1loyyr1/lies_we_tell_ourselves_to_keep_using_golang/"},{"title":"Windows User Base Shrinks By 400 Million In Three Years","url":"https://www.reddit.com/r/linux/comments/1loy6zj/windows_user_base_shrinks_by_400_million_in_three/","date":1751365842,"author":"/u/Or0ch1m4ruh","guid":178872,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Or0ch1m4ruh\"> /u/Or0ch1m4ruh </a>","contentLength":34,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"That Crossplane did not land. So... where to?","url":"https://www.reddit.com/r/kubernetes/comments/1loxvu2/that_crossplane_did_not_land_so_where_to/","date":1751364696,"author":"/u/IngwiePhoenix","guid":178897,"unread":true,"content":"<p>But this feedback paired with the Domino's provider () had me left wondering what other mechanisms are out there to \"unify\" resources.</p><p>...This requires a bit of explaining. I run a little homelab with three k3s nodes on Radxa Orion O6'es - super nice, although I don't have the full hw available, the compute is plenty, powerful and good! Alpine Linux is my base here - it just boots and works (in ACPI mode). But, I have a few auxiliary servers and services that are not kube'd; a FriendlyElec NANO3 that handles TVHeadend, a NAS that handles more complex services like Jellyfin, PaperlessNGX and Home Assistant, a secondary \"random crap that fits together\" NAS with an Athlon 3000G that runs Kasm on OpenMediaVault - and soon, I will have an AI server backed by LocalAI. That's a lot of potential API resources and I would love to take advantage of them. Probably not all of them, to be fair and honest. However, this is why I really liked the basic idea of Crossplane; I can use the HTTP provider to define CRUD ops and then use Kubernetes resources to manage and maintain them - kind of centralizing them, and perhaps opting into GitOps also (which I have not done yet entirely - my stuff  in a private Git repo but no ArgoCD is configured).</p><p>So... Since Crossplane hit such a nerve (oh my god the emotions were  xD) and OpenTofu seems absurdly overkill for a lil' homelab like this, what are some other \"orchestration\" or \"management\" tools that come to your mind?</p><p>I might still try CrossPlane, I might try Tekton at some point for CI/CD or see if I can make Concourse work... But it's a homelab, there's always something to explore. And, one of the things I would really like to get under control, is some form of central management of API-based resources.</p><p>So in other words; rather than the absolute moment that is the Crossplane post's comment section, throw out the things you liked to use in it's stead or something that you think would kinda go there!</p><p>And, thanks for the feedback on that post. Couldn've asked for a cleaner opinion at all. XD</p>","contentLength":2048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monthly: Who is hiring?","url":"https://www.reddit.com/r/kubernetes/comments/1loxpea/monthly_who_is_hiring/","date":1751364035,"author":"/u/gctaylor","guid":178871,"unread":true,"content":"<div><p>This monthly post can be used to share Kubernetes-related job openings within  company. Please include:</p><ul><li>Location requirements (or lack thereof)</li><li>At least one of: a link to a job posting/application page or contact details</li></ul><p>If you are interested in a job, please contact the poster directly. </p><p>Common reasons for comment removal:</p><ul><li>Not meeting the above requirements</li><li>Recruiter post / recruiter listings</li><li>Negative, inflammatory, or abrasive tone</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Questions and advice","url":"https://www.reddit.com/r/kubernetes/comments/1loxpe3/weekly_questions_and_advice/","date":1751364034,"author":"/u/gctaylor","guid":179391,"unread":true,"content":"<p>Have any questions about Kubernetes, related tooling, or how to adopt or use Kubernetes? Ask away!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"crd-to-sample-yaml now has an intellij and vscode plugin","url":"https://www.reddit.com/r/kubernetes/comments/1lox6m9/crdtosampleyaml_now_has_an_intellij_and_vscode/","date":1751361948,"author":"/u/skarlso","guid":178931,"unread":true,"content":"<p>I have a tool I wrote a while ago called <a href=\"https://github.com/Skarlso/crd-to-sample-yaml\">crd-to-sample-yaml</a> that does a bunch of things, but its main purpose is to be able to take anything that has an openAPI schema in it, and generate a valid YAML for it.</p><p>Now, I created a vscode and an intellij plugin for it. They are both registered and your can find them here: <a href=\"https://marketplace.visualstudio.com/items?itemName=GergelyBrautigam.crd-to-sample-yaml\">VSCode Extension</a> and here <a href=\"https://plugins.jetbrains.com/plugin/27800-crd-to-sample-yaml\">IntelliJ Plugin</a>. The intellij plugin is still under review officially, but you can also install it from the repository through File ‚Üí Settings ‚Üí Plugins ‚Üí Install Plugin from Disk.</p><p>Enjoy, and if you find any problems, please don't hesitate to create an issue. :) Thank you so much for the great feedback and usage already.</p>","contentLength":670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Compiling 10,000+ Go CLI Packages Statically","url":"https://blog.pkgforge.dev/cross-compiling-10000-go-cli-packages-statically","date":1751361833,"author":"/u/Azathothas","guid":178900,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lox5lq/crosscompiling_10000_go_cli_packages_statically/"},{"title":"Linux managed to save me almost 50 gigs after a windows 11 install managed to somehow take up half my entire SSD.","url":"https://www.reddit.com/r/linux/comments/1lowp0y/linux_managed_to_save_me_almost_50_gigs_after_a/","date":1751359998,"author":"/u/gloombert","guid":178765,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alternative Blanket Implementations for a Single Rust Trait (blog post)","url":"https://www.greyblake.com/blog/alternative-blanket-implementations-for-single-rust-trait/","date":1751355159,"author":"/u/greyblake","guid":178793,"unread":true,"content":"<p>Rust's trait system is famously powerful - and famously strict about avoiding ambiguity.</p><p>One such rule is that <strong>you can't have multiple blanket implementations of the same trait</strong> that  apply to the same type.</p><h2>What Is a Blanket Implementation?</h2><p>A  is a trait implementation that applies to  type meeting certain constraints, typically via generics.</p><p>A classic example from the standard library is how <a rel=\"noopener\" target=\"_blank\" href=\"https://doc.rust-lang.org/std/convert/trait.From.html\"></a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://doc.rust-lang.org/std/convert/trait.Into.html\"></a> work together:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Thanks to this, when you implement  for , you automatically get  for . Very ergonomic!</p><p>However, Rust enforces a key rule: <strong>no two blanket implementations may overlap</strong> - even . Consider:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Even if no type currently implements both  and , the compiler will reject this. The reason? Some type  satisfy both in the future, and that would make the implementation ambiguous.</p><p>While working on <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb\">Joydb</a>, I ran into this exact problem.</p><p>I have an <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb/blob/2aea9e11b1bc96e108e6c4478e599aaa587ffa8c/joydb/src/adapters/mod.rs#L50-L67\"></a> trait responsible for persisting data.</p><p>In practice, there are two common ways to implement it:</p><ul><li>A  that stores all data in a single file (e.g., JSON). In Joydb, this is <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb/blob/2aea9e11b1bc96e108e6c4478e599aaa587ffa8c/joydb/src/adapters/mod.rs#L107-L117\"></a>.</li><li>A  that stores each relation in a separate file (e.g., one CSV per relation), called <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/greyblake/joydb/blob/2aea9e11b1bc96e108e6c4478e599aaa587ffa8c/joydb/src/adapters/mod.rs#L119-L136\"></a>.</li></ul><p>Ideally, users would only need to implement one of those and get the  trait \"for free\".</p><p>But Rust won't let me define two conflicting blanket implementations. So... is there a workaround? ü§î</p><h2>The Trait Definitions in Joydb</h2><p>Here are the relevant traits in Joydb:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>So the question becomes: <strong>how can I let someone implement either  or , and then get  automatically?</strong></p><h2>The Workaround: Associated Type + Marker Structs</h2><ol><li> like  and  to wrap adapter types.</li><li>A helper trait, , implemented for each marker type.</li><li>An  in the  trait to delegate behavior.</li></ol><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>These zero-sized types are used solely for type-level dispatch.</p><h3>Step 2: The  Trait</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Now we have <em>non-conflicting blanket impls</em> because they apply to  ( vs. ).</p><h3>Step 3: The  Trait with Associated Type</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The key piece: the associated type  tells  whether to delegate to  or .</p><p>Let's say we need to implement a  that writes everything to a single file.\nIt can be implemented as a :</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>No code duplication.\nNo conflicts.\nThe only overhead is 3 extra lines to link things together</p><p>This pattern - using <strong>marker types + associated types</strong> - gives you the flexibility of <em>alternative blanket implementations</em> while staying within Rust's coherence rules.</p><p>It's especially useful when you want to support mutually exclusive behaviors under a unified interface, without compromising on ergonomics.</p><h2>Psss! Are you looking for a passionate Rust dev?</h2><p>My friend is looking for a job in Berlin or remote.\nReach out to <a rel=\"noopener\" target=\"_blank\" href=\"https://www.linkedin.com/in/zekefast/\"></a>.</p>","contentLength":2497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lovjej/alternative_blanket_implementations_for_a_single/"},{"title":"It‚Äôs harder to read code than to write it","url":"https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/","date":1751352712,"author":"/u/abooishaaq","guid":178933,"unread":true,"content":"<p>Netscape 6.0 is finally going into its first public beta. There never was a version 5.0. The last major release, version 4.0, was released almost three years ago. Three years is an  long time in the Internet world. During this time, Netscape sat by, helplessly, as their market share plummeted.</p><p>It‚Äôs a bit smarmy of me to criticize them for waiting so long between releases. They didn‚Äôt do it , now, did they?</p><p>Well, yes. They did. They did it by making the <b>single worst strategic mistake</b> that any software company can make:</p><p><img data-recalc-dims=\"1\" decoding=\"async\" hspace=\"4\" align=\"right\" src=\"https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2000/04/Upper_West_Side_Brownstones_2.jpg?w=730&amp;ssl=1\">They decided to rewrite the code from scratch.</p><p>Netscape wasn‚Äôt the first company to make this mistake. Borland made the same mistake when they bought Arago and tried to make it into dBase for Windows, a doomed project that took so long that Microsoft Access ate their lunch, then they made it again in rewriting Quattro Pro from scratch and astonishing people with how few features it had. Microsoft almost made the same mistake, trying to rewrite Word for Windows from scratch in a doomed project called Pyramid which was shut down, thrown away, and swept under the rug. Lucky for Microsoft, they had never stopped working on the old code base, so they had something to ship, making it merely a financial disaster, not a strategic one.</p><p>We‚Äôre programmers. Programmers are, in their hearts, architects, and the first thing they want to do when they get to a site is to bulldoze the place flat and build something grand. We‚Äôre not excited by incremental renovation: tinkering, improving, planting flower beds.</p><p>There‚Äôs a subtle reason that programmers always want to throw away the code and start over. The reason is that they think the old code is a mess. And here is the interesting observation:  The reason that they think the old code is a mess is because of a cardinal, fundamental law of programming:</p><p>It‚Äôs harder to read code than to write it.</p><p>This is why code reuse is so hard. This is why everybody on your team has a different function they like to use for splitting strings into arrays of strings. They write their own function because it‚Äôs easier and more fun than figuring out how the old function works.</p><p><img data-recalc-dims=\"1\" decoding=\"async\" hspace=\"4\" align=\"left\" src=\"https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2000/04/Columbus_Ave_Barber_Shop.jpg?w=730&amp;ssl=1\">As a corollary of this axiom, you can ask almost any programmer today about the code they are working on. ‚ÄúIt‚Äôs a big hairy mess,‚Äù they will tell you. ‚ÄúI‚Äôd like nothing better than to throw it out and start over.‚Äù</p><p>‚ÄúWell,‚Äù they say, ‚Äúlook at this function. It is two pages long! None of this stuff belongs in there! I don‚Äôt know what half of these API calls are for.‚Äù </p><p>Before Borland‚Äôs new spreadsheet for Windows shipped, Philippe Kahn, the colorful founder of Borland, was quoted a lot in the press bragging about how Quattro Pro would be much better than Microsoft Excel, because it was written from scratch. All new source code! As if source code .</p><p>The idea that new code is better than old is patently absurd. Old code has been . It has been .  of bugs have been found, and they‚Äôve been . There‚Äôs nothing wrong with it. It doesn‚Äôt acquire bugs just by sitting around on your hard drive. Au contraire, baby! Is software supposed to be like an old Dodge Dart, that rusts just sitting in the garage? Is software like a teddy bear that‚Äôs kind of gross if it‚Äôs not made out of ?</p><p>Back to that two page function. Yes, I know, it‚Äôs just a simple function to display a window, but it has grown little hairs and stuff on it and nobody knows why. Well, I‚Äôll tell you why: those are bug fixes. One of them fixes that bug that Nancy had when she tried to install the thing on a computer that didn‚Äôt have Internet Explorer. Another one fixes that bug that occurs in low memory conditions. Another one fixes that bug that occurred when the file is on a floppy disk and the user yanks out the disk in the middle. That LoadLibrary call is ugly but it makes the code work on old versions of Windows 95.</p><p>Each of these bugs took weeks of real-world usage before they were found. The programmer might have spent a couple of days reproducing the bug in the lab and fixing it. If it‚Äôs like a lot of bugs, the fix might be one line of code, or it might even be a couple of characters, but a lot of work and time went into those two characters.</p><p>When you throw away code and start from scratch, you are throwing away all that knowledge. All those collected bug fixes. Years of programming work.</p><p>You are throwing away your market leadership. You are giving a gift of two or three years to your competitors, and believe me, that is a  time in software years.</p><p>You are putting yourself in an extremely dangerous position where you will be shipping an old version of the code for several years, completely unable to make any strategic changes or react to new features that the market demands, because you don‚Äôt have shippable code. You might as well just close for business for the duration.</p><p>You are wasting an outlandish amount of money writing code that already exists.</p><p>Is there an alternative? The consensus seems to be that the old Netscape code base was bad. Well, it might have been bad, but, you know what? It worked pretty darn well on an awful lot of real world computer systems.</p><p>When programmers say that their code is a holy mess (as they always do), there are three kinds of things that are wrong with it.</p><p>First, there are architectural problems. The code is not factored correctly. The networking code is popping up its own dialog boxes from the middle of nowhere; this should have been handled in the UI code. These problems can be solved, one at a time, by carefully moving code, refactoring, changing interfaces. They can be done by one programmer working carefully and checking in his changes all at once, so that nobody else is disrupted. Even fairly major architectural changes can be done without . On the Juno project we spent several months rearchitecting at one point: just moving things around, cleaning them up, creating base classes that made sense, and creating sharp interfaces between the modules. But we did it carefully, with our existing code base, and we didn‚Äôt introduce new bugs or throw away working code.</p><p>A second reason programmers think that their code is a mess is that it is inefficient. The rendering code in Netscape was rumored to be slow. But this only affects a small part of the project, which you can optimize or even rewrite. You don‚Äôt have to rewrite the whole thing. When optimizing for speed, 1% of the work gets you 99% of the bang.</p><p>Third, the code may be doggone ugly. One project I worked on actually had a data type called a FuckedString. Another project had started out using the convention of starting member variables with an underscore, but later switched to the more standard ‚Äúm_‚Äù. So half the functions started with ‚Äú_‚Äù and half with ‚Äúm_‚Äù, which looked ugly. Frankly, this is the kind of thing you solve in five minutes with a macro in Emacs, not by starting from scratch.</p><p>It‚Äôs important to remember that when you start from scratch there is  to believe that you are going to do a better job than you did the first time. First of all, you probably don‚Äôt even have the same programming team that worked on version one, so you don‚Äôt actually have ‚Äúmore experience‚Äù. You‚Äôre just going to make most of the old mistakes again, and introduce some new problems that weren‚Äôt in the original version. </p><p><img data-recalc-dims=\"1\" decoding=\"async\" hspace=\"4\" align=\"left\" src=\"https://i0.wp.com/www.joelonsoftware.com/wp-content/uploads/2008/01/Lincoln_Center_Trees.jpg?w=730&amp;ssl=1\">The old mantra  is dangerous when applied to large scale commercial applications. If you are writing code experimentally, you may want to rip up the function you wrote last week when you think of a better algorithm. That‚Äôs fine. You may want to refactor a class to make it easier to use. That‚Äôs fine, too. But throwing away the whole program is a dangerous folly, and if Netscape actually had some adult supervision with software industry experience, they might not have shot themselves in the foot so badly.</p>","contentLength":7843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1loux7h/its_harder_to_read_code_than_to_write_it/"},{"title":"I want to build a TUI-based game (player movement, collisions, basic enemies). Is Go a good choice?","url":"https://www.reddit.com/r/golang/comments/1louv5a/i_want_to_build_a_tuibased_game_player_movement/","date":1751352489,"author":"/u/Feldspar_of_sun","guid":178764,"unread":true,"content":"<p>I had a silly idea to make an extreme demake of one of my favorite games (Ikachan) with an ASCII art style. I thought it would be fun to make it purely as a TUI </p><p>Is Go a good choice for this? I have a little experience with it and have enjoyed what I‚Äôve done so far, but I also have some experience in C/C++ and Python, and I‚Äôm wondering if those may be better </p><p>If Go is a good choice, what package(s) would be best for something like this? If not, how come? And do you have a different recommendation?</p>","contentLength":504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Is Why You Can't Trust AI to Review Your Mission-Critical Code","url":"https://medium.com/p/456c47ce7e81","date":1751351799,"author":"/u/goated_ivyleague2020","guid":178966,"unread":true,"content":"<div><h2>The Unspoken Dangers of Relying on Language Models</h2></div><p>AI makes it too easy to use too little brain power. And if I was lazy and careless, I would‚Äôve paid the price.</p><p>My name is Austin Starks, and I‚Äôm building an app called NexusTrade.</p><p>NexusTrade is like if ChatGPT had a baby with Robinhood, who grew up to have a baby with QuantConnect. In short, it‚Äôs a platform that allows everybody to create, test, and deploy their own algorithmic trading strategies.</p><p>While exceptionally good at testing complex strategies that operate at open and close, the platform has a major flaw‚Ä¶ if you want to test out an intraday strategy, you‚Äôre cooked. <a href=\"https://medium.com/p/f77652e42217\" rel=\"noopener\">I‚Äôm working diligently to fix this.</a></p><p>In a previous article, I described the different milestones with the implementation. For , my objective is to implement ‚Äúintraday-ness‚Äù within my indicators for my platform. And, if you observe from the surface-level (i.e, using AI tools), you might assume it‚Äôs already implemented! In fact, <a href=\"https://github.com/austin-starks/ta-rs-improved/\" rel=\"noopener ugc nofollow\" target=\"_blank\">you can check it out yourself</a>, and see the ingenuity of my implementation.</p><p>And if I trusted the surface-level (i.e, the  AI tools on the planet), I would‚Äôve proceeded with the WRONG implementation. Here‚Äôs how Claude Opus 4 outright failed me on a critical feature.</p><p>To implement my intraday indicators, the first step was seeing if the implementation for it that exists is correct. To do this, I asked Claude the following:</p><blockquote><p>What do you think of this implementation? Is it correct? Any edge cases?</p></blockquote><p>For our overly ambitious engineering reader, here‚Äôs the implementation of the Simple Moving Average. See if you can spot the bug yourself.</p><p>The way this implementation works is by taking a Duration as the input. This parameter helps us maintain a sliding window and works for any period ‚Äî 30 days, 30 minutes, or even 30 seconds.</p><p>At the surface, the implementation looks correct. Even Claude Opus 4, the most powerful coding LLM of our time, only pointed out nitpicks and unrealistic edge cases.</p><p>However, <strong>solely because I implemented the technical indicator library,</strong> I knew that there existed a hidden weakness. Allow me to explain.</p><p>On the surface level, the implementation of the intraday indicator looks sound. And it is!</p><p>If you make the following assumption: <strong>the data is ingested at regular intervals.</strong></p><p>Take this graph for example. It will correctly compute the 14-day SMA across Apple‚Äôs closed price because we‚Äôre assuming one data-point per day. But what happens if that assumption is violated?</p><p>Let‚Äôs say we ‚Äúwarmed up‚Äù our indicators using open/close data (i.e, computed our moving averages), and now we‚Äôre running our backtest on intraday data, which requires ingesting new data points at the minutely granularity.</p><p><strong>If we use the current implementation of the indicator, that introduces a major bug.</strong></p><p>This graph shows the impact of ingesting just 4 minutes of minutely data into our system. The SMA shoots up rapidly, approaching the current price of Apple.</p><p>The current implementation  that each ingested data point should be weighted the same as every other datapoint in the window. This is wrong!</p><p>In reality, we need to implement a time-weighted moving average. After some immense brainpower, I ended up developing the following algorithm.</p><p>This new implementation is an improvement over the original because:</p><ul><li>It should regress to the original implementation if we‚Äôre just ingesting open and closed data</li><li>It automatically resets the minutely averages at the start of the day for stocks</li><li>It maintains the minutely averages for cryptocurrency, which is tradeable all day</li></ul><p>Our unchecked assumption would‚Äôve caused a major bug in such a critical feature. What can we learn from this?</p><p>I‚Äôm sharing this story for really one reason: as a cautionary tale for tech executives and software engineers.</p><p>And this should go without saying, but I am not some anti-AI evangelist. I literally developed a no-code, AI-Powered trading platform. If there‚Äôs anybody having sermons about the power and value of AI, it would be me!</p><p>But this article clearly demonstrates something immensely important: you can ask the literal best AI models of our time point blank if an implementation is wrong, and it will tell you no.</p><p>Now, this is not the model‚Äôs fault.  if I prompted it in such a way that listed every single assumption that can be made, then maybe it would‚Äôve caught it!</p><p>But that‚Äôs not how people use AI in the real-world. I know it and you know it too.</p><p>For one, many assumptions that we make in software are implicit. We‚Äôre not even fully aware that we‚Äôre making them!</p><p>But also, just imagine if I didn‚Äôt even write the technical indicator library, and I trusted the authors to handle this automatically. Or, imagine if AI wrote the library entirely, and I never wondered about how it worked under the hood.</p><p>The implementation would‚Äôve yielded outright incorrect values forever. Unless someone raised the issue because something seemed off, the bug would‚Äôve laid dormant for months or even longer.</p><p>Debugging the issue would‚Äôve been a nightmare on its own. I would‚Äôve checked if the data was right or if the event emitter was firing correctly, and everything else within the core of the trading platform‚Ä¶ I mean, why would I double-check the external libraries it depended on?</p><p>Catastrophically-silent bugs like this are going to become rampant. Not only do AI tools dramatically increase the output of engineers, but they are notoriously bad at understanding the larger picture.</p><p>Moreover, more and more non-technical folks are ‚Äúvibe-coding‚Äù their projects into existence. They‚Äôre developing software based on intuition, requirements, and AI prompts, and don‚Äôt have a deep understanding of the actual code that‚Äôs being generated.</p><p>I‚Äôve seen it first-hand, on LinkedIn, Reddit, and even TikTok! Just Google ‚Äúvibe-coding‚Äù and see how popular it has become.</p><p>What happens when a ‚Äúvibe-coded‚Äù library is used by thousands of developers, and these issues start infesting all of our software? <strong>If I nearly missed a critical bug and I actually wrote the code, how many bugs will exist because code wasn‚Äôt written by engineers with domain expertise?</strong></p><p>I shudder to think of that future.</p><p>So if you‚Äôre a tech executive, don‚Äôt fire your engineering team yet. They may be more critical now than ever before.</p><p>Maybe my brain is overreacting.</p><p>Maybe I would‚Äôve caught this issue well before I launched. I‚Äôm just having trouble figuring out </p><p>In this case, <strong>I knew of the limitation because I wrote the library. </strong>But there are hundreds of libraries now being created and reviewed purely by AI. Engineers are looking at less and less of the code that is brought into the world.</p><p>And this should terrify you.</p><p>For backtesting software, the consequences of this bug would‚Äôve been an improper test. Users would be annoyed and leave bad reviews. I would suffer reputational harm. But I would survive.</p><p>But imagine such a bug for other, mission-critical software, like rocket ships and self-driving cars.</p><p>This article demonstrates why human beings still need to be in the loop when developing complex software systems. <strong>It is imperative, that human-beings sanity-check LLM-generated code with domain-aware unit tests.</strong> Even the best AI models don‚Äôt fully grasp exactly what we‚Äôre building.</p><p>So before you ship that feature (whose code you barely glanced at), ask yourself this question: what assumption did you and Gemini miss?</p>","contentLength":7382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1louou5/this_is_why_you_cant_trust_ai_to_review_your/"},{"title":"[D] Any path for a mid career/mid aged MLE to do ML research in the industry","url":"https://www.reddit.com/r/MachineLearning/comments/1lotkac/d_any_path_for_a_mid_careermid_aged_mle_to_do_ml/","date":1751347501,"author":"/u/LastAd3056","guid":178727,"unread":true,"content":"<p>I've seen some flavor of questions here about whether they should do a PhD to join a research lab. I have a slightly different question. I did a non-CS PhD almost a decade ago, failed to get a faculty position after a bunch of postdocs and then meandered through FANG jobs, first in DS and then in MLE. I did some applied research in my last job, but more stats heavy than ML. But through a bunch of layoffs and restructuring, currently I am in a more traditional MLE role, think recommendation systems, A/B tests, move metrics...</p><p>But at my heart, I still want to do research. I've dabbled with writing a single author paper in on the top ML conferences in my own time, but its kinda hard, with job, family etc.. Even if I do manage to pull it off, will the one off Neurips paper (lets say) help me get an entry card to a more research-y ML job, like a Research Scientist/ Research Engineer in a ML lab? I am competing with ML PhDs with multiple papers, networks etc.</p><p>I also think that I don't have a lot of time, most of my friends have moved on to management after a decade of IC roles, and thats sort of the traditional path. But part of me is still holding on and wants to give it a shot and see if I can break into research this late, without an ML PhD. I know I will be much more fulfilled as a research scientist, compared to a regular SWE/M job,. I am currently trying to use my weekends and nights to write a single author paper to submit to one of the top conferences. Worst case I get rejected.</p><p>Some thoughts in my mind: (1) I have also thought of writing workshop papers, which are easier to get accepted, but I doubt they have a similar value in the RS job market.<p> (2) Research Engineer will likely be easier than Research Scientist. But how should I strategize for this?</p></p><p>I'd be grateful if I get thoughts on how I should strategize a move. Feel free to also tell me its impossible, and I should cut my losses and move on.</p>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-Minute Daily AI News 6/30/2025","url":"https://www.reddit.com/r/artificial/comments/1lot1gv/oneminute_daily_ai_news_6302025/","date":1751345620,"author":"/u/Excellent-Target-847","guid":178967,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Claude collaborative platform","url":"https://www.reddit.com/r/kubernetes/comments/1losxzq/my_claude_collaborative_platform/","date":1751345283,"author":"/u/MuscleLazy","guid":178667,"unread":true,"content":"<p>I've been using Claude Desktop a lot and wanted a better way to manage different collaboration styles, like having it act as an engineer vs researcher vs creative partner.</p><p> (the default) forgets everything between conversations. You start fresh every time, explain your preferences, coding style, whatever. Gets old fast.</p><p> (with memory) actually remembers your working style, project context, and collaboration preferences. Game changer for long-term work.</p><p>I've been using this setup for about 3 months now with the engineer profile and it dramatically improved my workflow.</p><p>: Every conversation started with me explaining \"I need root cause analysis first, minimal code changes, focus on production safety, don't over-engineer solutions.\" Then spending the first 10 messages training Claude to give me direct technical responses instead of hand-holding explanations.</p><p>: Claude immediately knows I want systematic troubleshooting, that I prefer infrastructure optimization over quick fixes, and that I need definitive technical communication without hedging language.</p><p>The platform tracks our  from incident reviews and  where it documents lessons learned from outages, alternative approaches we considered but didn't implement, and insights about our infrastructure.</p><p>I've thoroughly tested the  profile for production incidents, while spending a lot less time on \"tuning\" the other profiles, you are welcome to contribute. It is striking to see how Claude transforms from a junior engineer, constantly performing unauthorized commands or file edits, into a \"cold\", \"precise like a surgeon's scalpel\" engineer. No more \"You're right!\" messages, Claude will actually tell you where you're wrong, straight up! Claude's <a href=\"https://github.com/axivo/claude/blob/main/docs/images/profile-user-drift.png\"></a> to <a href=\"https://github.com/axivo/claude/blob/main/docs/profile-engineer.md#profile-drift-correction\"></a>. üßë‚Äçüíª</p><p>The most spectacular improvements are the conversation logs and Claude's diary, Claude will not be shy to write any dumb mistakes you did, priceless.</p><p>The repo has all the details, examples, and documentation. Worth checking out if you're tired of re-training Claude on every conversation.</p>","contentLength":2012,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Inference-Time Scaling and Collective Intelligence for Frontier AI","url":"https://www.reddit.com/r/MachineLearning/comments/1los6wj/r_inferencetime_scaling_and_collective/","date":1751342705,"author":"/u/iwiwijp","guid":178629,"unread":true,"content":"<p>TL;DR: our AB-MCTS lets multiple frontier models work together at inference time, outperforming each model running alone on the ARC-AGI-2 benchmark.</p><p>Our new inference-time scaling algorithm enables collective intelligence for AI by allowing multiple frontier models (like Gemini 2.5 Pro, o4-mini, DeepSeek-R1-0528) to cooperate.</p><p>Inspired by the power of human collective intelligence, where the greatest achievements arise from the collaboration of diverse minds, we believe the same principle applies to AI. Individual frontier models like ChatGPT, Gemini, and DeepSeek are remarkably advanced, each possessing unique strengths and biases stemming from their training, which we view as valuable resources for collective problem-solving.</p><p>AB-MCTS (Adaptive Branching Monte Carlo Tree Search) harnesses these individualities, allowing multiple models to cooperate and engage in effective trial-and-error, solving challenging problems for any single AI. Our initial results on the ARC-AGI-2 benchmark are promising, with AB-MCTS combining o4-mini + Gemini-2.5-Pro + R1-0528, current frontier AI models, significantly outperforming individual models by a substantial margin.</p><p>This research builds on our 2024 work on evolutionary model merging, shifting focus from ‚Äúmixing to create‚Äù to ‚Äúmixing to use‚Äù existing, powerful AIs. At Sakana AI, we remain committed to pioneering novel AI systems by applying nature-inspired principles such as evolution and collective intelligence. We believe this work represents a step toward a future where AI systems collaboratively tackle complex challenges, much like a team of human experts, unlocking new problem-solving capabilities and moving beyond single-model limitations.</p><p>If you have any questions, please ask them below or feel free to get in touch, any discussion is more than welcome :)</p>","contentLength":1830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"probemux: When you need more than 1 {liveness, readiness}Probe","url":"https://www.reddit.com/r/kubernetes/comments/1lor7jp/probemux_when_you_need_more_than_1_liveness/","date":1751339521,"author":"/u/thockin","guid":178698,"unread":true,"content":"<div><p>There was an issue recently where someone argued that they REALLY DO need more than 1 livenessProbe, so I cobbled this together from bits of other programs:</p><p>NAME probemux - multiplex many HTTP probes into one.</p><p>SYNOPSIS probemux --port=&lt;port&gt; [OPTIONS]... BACKENDS...</p><pre><code>When the / URL is read, execute one HTTP GET operation against each backend URL and return the composite result. If all backends return a 2xx HTTP status, this will respond with 200 \"OK\". If all backends return valid HTTP responses, but any backend returns a non-2xx status, this will respond with 503 \"Service Unavailable\". If any backend produced an HTTP error, this will respond with 502 \"Bad Gateway\". Backends are probed synchronously when an incoming request is received, but backends may be probed in parallel to each other. </code></pre><pre><code>Probemux has exactly one required flag. --port The port number on which to listen. Probemux listens on the unspecified address (all IPs, all families). All other flags are optional. -?, -h, --help Print help text and exit. --man Print this manual and exit. --pprof Enable the pprof debug endpoints on probemux's port at /debug/pprof/... --timeout &lt;duration&gt; The time allowed for each backend to respond, formatted as a Go-style duration string. If not specified this defaults to 3 seconds (3s). -v, --verbose &lt;int&gt;, $GITSYNC_VERBOSE Set the log verbosity level. Logs at this level and lower will be printed. --version Print the version and exit. </code></pre><pre><code>probemux \\ --port=9376 \\ --timeout=5s \\ http://localhost:1234/healthz \\ http://localhost:1234/another \\ http://localhost:5678/a-third </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/thockin\"> /u/thockin </a>","contentLength":1606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graphite (now a top-100 Rust project) turns Rust into a functional, visual scripting language for graphics operations ‚Äî REQUESTING HELP to implement compiler bidirectional type inference","url":"https://www.reddit.com/r/rust/comments/1lor3b4/graphite_now_a_top100_rust_project_turns_rust/","date":1751339166,"author":"/u/Keavon","guid":177127,"unread":true,"content":"<p>Just now, <a href=\"https://graphite.rs\">Graphite</a> has broken into the top 100 Rust projects on GitHub by <a href=\"https://github.com/GraphiteEditor/Graphite/stargazers\">star</a> count, and it has been today's #1 <a href=\"https://github.com/trending?since=daily\">trending repo</a> on all of GitHub regardless of language.</p><p>It's a community-driven open source project that is a comprehensive 2D content creation tool for graphic design, digital art, and interactive real-time motion graphics. It also, refreshingly, has a high-quality UI design that is modern, intuitive, and user-friendly. The vision is to become the Blender equivalent of 2D creative tools. <a href=\"https://static.graphite.rs/content/index/sizzle-compilation.mp4\">Here's a 1-minute video</a> showing the cool, unique, visually snazzy things that can be made with it.</p><p>Graphite features a node-based procedural editing environment using a bespoke functional programming language, Graphene, that we have built on top of Rust itself such that it uses Rust's data types and  to transform artist-created documents into portable, standalone programs that can procedurally generate parametric artwork. Think: something spanning the gamut from Rive to ImageMagick.</p><p>For the juicy technical deets, give the <a href=\"https://www.youtube.com/watch?v=ZUbcwUC5lxA\">podcast episode</a> a listen where we were interviewed about how our Graphene engine/language lets even nontechnical artists \"paint with Rust\", sort of like if Scratch used Rust as its foundation. We go into detail on the unique approach of turning a graphics editor into a compiled programming language where the visual editor is like an IDE for Rust code.</p><h3>Here's the ask: help implement bidirectional type inference in our language's compiler</h3><p>The Graphene language ‚Äî while it is built on top of Rust and uses Rust's compiler, data types, traits, and generics ‚Äî also has its own type checker. It supports generics, but is somewhat rudimentary and needs to be made more powerful, such as implementing <a href=\"https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system\">Hindley‚ÄìMilner</a> or similar, in order for Graphene types to work with contextual inference just like Rust types do.</p><p>This involves the Graphene compiler internals and we <strong>only have one developer with a compilers background</strong> and he's a student with limited free time spread across all the crucial parts of the Graphite project's engineering. But we know that <a href=\"https://www.reddit.com/r/rust\">/r/rust</a> is ‚Äî well... ‚Äî naturally a place where many talented people who love building compilers and hobby language implementations hang out.</p><p>This type system project should last a few weeks for someone with the right background‚Äî but for more than a year, working around having full type inference support has been a growing impediment that is impacting how we can keep developing ergonomic graphics tooling. For example, a graphics operation can't accept two inputs and use the type of the first to pick a compatible generic type for the second. This results in painful workarounds that confuse users. Even if it's just a short-term involvement, even temporarily expanding our team beyond 1 knowledgeable compiler developer would have an outsized impact on helping us execute our mission to bring programmatic graphics (and Rust!) into the hands of artists.</p><p>If you can help, we will work closely with you to get you up to speed with the existing compiler code. If you're up for the fun and impactful challenge, the best way is to <a href=\"https://discord.graphite.rs\">join our project Discord</a> and say you'd like to help in our  channel. Or you can <a href=\"https://github.com/GraphiteEditor/Graphite/issues/1621\">comment on the GitHub issue</a>.</p><p>Besides compilers, we also need general help, especially in areas of our bottlenecks: code quality review, and helping design API surfaces and architecture plans for upcoming systems. If you're an experienced engineer who could help with any of those for a few hours a week, or with general feature development, please also come get involved! Graphite is one of the easiest open source projects to start contributing to according to many of our community members; we really strive to make it as frictionless as possible to <a href=\"https://graphite.rs/volunteer/guide/\">start out</a>. Feel free to drop by and leave a code review on any <a href=\"https://github.com/GraphiteEditor/Graphite/pulls\">open PRs</a> or ask what kind of task best fits your background (graphics, algorithm design, application programming, bug hunting, and of course most crucially: programming language compilers).</p><p>Thank you! Now let's go forth and get artists secretly addicted to Rust üòÄ In no time at all, they will be writing custom Rust functions to do their own graphical operations.</p><p>P.S. If you are attending <a href=\"https://opensauce.com/\">Open Sauce</a> in a few weeks, come visit our booth. We'd love to chat (and give you swag).</p>","contentLength":4288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reflections on Haskell and Rust","url":"https://academy.fpblock.com/blog/rust-haskell-reflections/","date":1751338698,"author":"/u/sibip","guid":178839,"unread":true,"content":"<p>For most of my professional experience, I have been writing production\ncode in both Haskell and Rust, primarily focusing on web services,\nAPIs, and HTTP stack development. My journey started with Haskell,\nfollowed by working with Rust, and most recently returning to the\nHaskell ecosystem.</p><p>This experience has given me perspective on both languages' strengths\nand limitations in real-world applications. Each language has aspects\nthat I appreciate and miss when working with the other. This post\nexamines the features and characteristics that stand out to me in each\nlanguage.</p><p>Rust's ability to shadow variables seamlessly is something I came to\nappreciate. In Rust, you can write:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This pattern is common and encouraged in Rust, making code more\nreadable by avoiding the need for intermediate variable names. In\nHaskell, you would typically need different names:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Haskell's approach is slightly harder to read, while Rust's shadowing\nmakes transformation pipelines more natural.</p><p>Rust's enum system, particularly when combined with pattern matching,\nfeels more robust than Haskell's sum types of records. When defining\nsum types of records in Haskell, there is a possibility of introducing\npartial record accessors which can cause runtime crashes, though\nrecent versions of GHC now produce compile-time warnings for this\npattern:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust eliminates this class of errors by design:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Rust allows multiple enum types to have the same variant names within\nthe same module, while Haskell's constructor names must be unique\nwithin their scope. This leads to different patterns in the two\nlanguages.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The same approach in Haskell would cause a compile error:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>In Haskell, you need unique constructor names:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Alternatively, Haskell developers often use qualified imports syntax\nto achieve similar namespacing:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>The  syntax in Rust makes the intent clearer at the\nusage site. You immediately know which enum type you're working with,\nwhile Haskell's approach can sometimes require additional context or\nprefixing to achieve the same clarity.</p><p>Rust provides granular visibility control for struct fields, allowing\nyou to expose only specific fields while keeping others private. This\nfine-grained control is built into the language:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Rust offers even more granular visibility control beyond simple \nand private fields. You can specify exactly where a field should be\naccessible:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>These granular visibility modifiers (, ,\n) allow you to create sophisticated access patterns that\nmatch your module hierarchy and architectural boundaries. Some of\nthese patterns are simply not possible to replicate in Haskell's\nmodule system.</p><p>In Haskell, record field visibility is controlled at the type level,\nnot the field level. This means you typically either export all of a\nrecord's fields at once (using ) or none of them. To achieve\nsimilar granular control, you need to use more awkward patterns:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>The Haskell approach requires writing boilerplate accessor functions\nand losing the convenient record syntax for the fields you want to\nkeep private. Rust's per-field visibility eliminates this awkwardness\nwhile maintaining the benefits of direct field access for public\nfields.</p><h2>Purity and Referential Transparency</h2><p>One of Haskell's most significant strengths is its commitment to\npurity. Pure functions, which have no side effects, are easier to\nreason about, test, and debug. Referential transparency‚Äîthe principle\nthat a function call can be replaced by its resulting value without\nchanging the program's behavior‚Äîis a direct benefit of this purity.</p><p>In Haskell, the type system explicitly tracks effects through monads\nlike , making it clear which parts of the code interact with the\noutside world. This separation of pure and impure code is a powerful\ntool for building reliable software.</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>While Rust encourages a similar separation of concerns, it does not\nenforce it at the language level in the same way. A function in Rust\ncan perform I/O or mutate state without any explicit indication in its\ntype signature (beyond  for mutable borrows). This means that\nwhile you  write pure functions in Rust, the language doesn't\nprovide the same strong guarantees as Haskell.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This lack of enforced purity in Rust means you lose some of the strong\nreasoning and refactoring guarantees that are a hallmark of Haskell\ndevelopment.</p><p>Rust's explicit error handling through  removes the\ncognitive overhead of exceptions. Compare these approaches:</p><p>Haskell (with potential exceptions):</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust (explicit throughout):</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>In Rust, the  operator makes error propagation clean while keeping\nthe flow clear.</p><h2>Unit tests as part of source code</h2><p>Rust's built-in support for unit tests within the same file as the\ncode being tested is convenient:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>In Haskell, tests are typically in separate files:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust's co-location makes tests harder to forget and easier to\nmaintain. Additionally, in Haskell you often need to export internal\ntypes and functions from your modules just to make them accessible for\ntesting, which can pollute your public API.</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Rust's  attribute means test code has access to private\nfunctions and types without exposing them publicly.</p><p>Rust's  provides a standard formatting tool that the entire\ncommunity has adopted:</p><pre data-lang=\"bash\"><code data-lang=\"bash\"></code></pre><p>In Haskell, while we have excellent tools like  and\n, the lack of a single standard has led to configuration\ndebates:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>I have witnessed significant time spent on style discussions when team\nmembers are reluctant to adopt formatting tools.</p><p>While Haskell Language Server (HLS) has improved significantly, it\nstill struggles with larger projects. Basic functionality like\nvariable renaming can fail in certain scenarios, particularly in\nTemplate Haskell heavy codebases.</p><p>rust-analyzer provides a more reliable experience across different\nproject sizes, with features like \"go to definition\" working\nconsistently even in large monorepos. One feature I'm particularly\nfond of is rust-analyzer's ability to jump into the definitions of\nstandard library functions and external dependencies. This seamless\nnavigation into library code is something I miss when using HLS, even\non smaller projects, where such functionality is not <a href=\"https://github.com/haskell/haskell-language-server/issues/708\">currently</a>\npossible.</p><p>Another feature I extensively use in rust-analyzer is the ability to\nrun tests inline directly from the editor. This functionality is\ncurrently missing in HLS, though there is an <a href=\"https://github.com/haskell/haskell-language-server/issues/3357\">open issue</a> tracking\nthis feature request.</p><p>Despite Rust's reputation for slow compilation, I've found it\nconsistently faster than Haskell for equivalent services. The Rust\nteam has made significant efforts to optimize the compiler over the\nyears, and these improvements are noticeable in practice. In contrast,\nHaskell compilation times have remained slow, and newer GHC versions\nunfortunately don't seem to provide meaningful improvements in this\narea.</p><h2>Interactive development experience</h2><p>I appreciate Haskell's REPL (Read-eval-print loop) for rapid\nprototyping and experimentation. Not having a native REPL in Rust\nnoticeably slows down development when you need to try things out\nquickly or explore library APIs interactively. In GHCi, you can load\nyour existing codebase and experiment around it, making it easy to\ntest functions, try different inputs, and explore how your code\nbehaves.</p><p>As an alternative, I have been using <a href=\"https://github.com/emacs-rustic/rustic?tab=readme-ov-file#org-babel\">org babel in rustic mode</a> for\ninteractive Rust development. While this provides some level of\ninteractivity within Emacs, it feels more like a band-aid than an\nactual solution for quickly experimenting with code. The workflow is\nmore cumbersome compared to the direct approach of typing expressions\ndirectly into GHCi and seeing results instantly.</p><h2>Lists as first-class citizens</h2><p>Haskell's treatment of lists as first-class citizens through special\nsyntax can be problematic. The convenient  syntax defaults to\nlinked lists:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>Better alternatives exist but require more verbose syntax:</p><pre data-lang=\"haskell\"><code data-lang=\"haskell\"></code></pre><p>While Haskell has the  extension to make this more\nconvenient, it hasn't enjoyed the same widespread adoption as\n.</p><p>Rust also has <a href=\"https://doc.rust-lang.org/std/collections/struct.LinkedList.html\"></a> in its standard library, but it comes\nwith clear documentation discouraging its use:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><h2>Configuration file experience</h2><p>Cabal and TOML represent different approaches to project\nconfiguration. While cabal's tool support has improved greatly with\nHLS integration, which now supports features like automatic formatting\nusing cabal-gild, TOML enjoys broader ecosystem support.</p><p>TOML benefits from independent language servers like <a href=\"https://github.com/tamasfe/taplo\">taplo</a> and\nthe recent <a href=\"https://github.com/tombi-toml/tombi\">tombi</a>, providing a superior editing\nexperience. Features like jumping to different Cargo.toml files across\na workspace or seeing descriptions of specific Cargo.toml fields on\nhover work seamlessly.</p><p>Additionally, TOML has upstream support in editors like Emacs through\ntoml-ts-mode, which takes advantage of tree-sitter. No equivalent\nexists for cabal files, likely due to the format's limited popularity\noutside the Haskell ecosystem.</p><p>It's worth noting that when Cabal was created, there was likely no\nwidely adopted standard configuration format available. Rust had the\nbenefit of coming later and choosing an existing, well-established\nformat rather than inventing something new.</p><p>I handle production operations for both Rust and Haskell services,\nincluding parts of the Stackage infrastructure at FP Complete (which\nFP Complete later donated to the Haskell Foundation). I have\nencountered significantly more operational challenges with Haskell\nservices.</p><p>Haskell services often require tweaking GHC's RTS (Runtime system)\nparameters to avoid memory issues and achieve stable performance. This\nremains an ongoing challenge - recently, Bryan from the Haskell\nFoundation team opened a <a href=\"https://github.com/commercialhaskell/stackage-server/issues/354\">similar issue</a> on the Stackage server,\nencountering the same type of issue I dealt with. In contrast, Rust\nservices have been much easier to operate.</p><p>Building static binaries is trivial in Rust, and the ecosystem\nactively works to reduce non-Rust dependencies. Recent efforts like\nthe <a href=\"https://trifectatech.org/blog/bzip2-crate-switches-from-c-to-rust/\">bzip2 crate switching from C to Rust</a> further simplify the\nbuild process by removing C library dependencies wherever possible.</p><p>Cross-compilation to ARM is particularly straightforward. For one of\nour clients, we run all production services on ARM\narchitecture. Setting up the entire cross-compilation pipeline took\nless than a day. I would not be confident about achieving the same\nwith GHC and the various commonly used dependencies in Hackage.</p><p>The build process is quite simple:</p><pre data-lang=\"bash\"><code data-lang=\"bash\"></code></pre><p>My experience has been that Rust services typically consume less\nmemory and CPU cycles than the Haskell ones in general.</p><p>Both languages have their place in the software development\necosystem. However, I find that Rust's larger user base brings\ntangible benefits through more robust tooling and actively maintained\nlibraries. I used to reach for Haskell when writing CLI tools,\nappreciating its expressiveness. These days, I find myself choosing\nRust instead, largely due to how polished libraries like  have\nbecome for command-line parsing.</p><p>While I appreciate that Haskell's theoretical foundations remain\nstrong for exploring effect systems and advanced type-level\nprogramming, I observe that its library ecosystem has become less\nactive compared to Rust's rapidly evolving landscape.</p><p>For the web services and API development I work on, I find that Rust's\npractical decisions around error handling, tooling, and operational\naspects often result in more maintainable and deployable systems.\nWhile language choice always depends on numerous factors like team\nexpertise and project requirements, Rust's powerful combination of\ndesign and ecosystem makes it a compelling option. For me, it has\nbecome the pragmatic choice for building reliable software,\nspecifically when a greenfield project is involved.</p><p><a href=\"https://blogtrottr.com/?subscribe=https://www.fpcomplete.com/feed/\" target=\"_blank\">\n      Subscribe to our blog via email\n    </a><small>Email subscriptions come from our <a target=\"_blank\" href=\"https://academy.fpblock.com/feed/\">Atom feed</a> and are handled by <a target=\"_blank\" href=\"https://blogtrottr.com\">Blogtrottr</a>. You will only receive notifications of blog posts,\n      and can unsubscribe any time.</small></p><p>Do you like this blog post and need help with Next Generation Software Engineering, Platform\n    Engineering or Blockchain &amp; Smart Contracts? <a href=\"https://academy.fpblock.com/contact-us/\">Contact us</a>.</p>","contentLength":11920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1loqxty/reflections_on_haskell_and_rust/"},{"title":"Heap Management with Go & Cgo","url":"https://www.reddit.com/r/golang/comments/1loqmgt/heap_management_with_go_cgo/","date":1751337712,"author":"/u/winwaed","guid":178700,"unread":true,"content":"<p>I think I know the answer, but a bit of a sanity check,,,</p><p>I'm a relative Go Newbie. We have a Go app running in a Docker (Ubuntu) container. This calls a C/C++ library (C interface, but C++ under the hood) via cgo. Yes I am aware of the dangers of that, but this library depends on a 3rd party C++ library and uses x64 intrinsics. The 3rd party library is slowly being ported to Go but it isn't ready yet for prime time; and of course there's the time to port our library to Golang: I've seen worse, but not trivial either!</p><p>Memory allocation is a potential issue, and I will investigate the latest GC options. Most of the memory allocation is in the C++ library (potentially many GB). Am I right in thinking that the C++ memory allocation will be separate from Golang's heap? And Golang does not set any limits on the library allocations? (other than OS-wide ulimit settings of course)</p><p>In other words, both Golang and the C++ library will take all the physical memory they can? And allocate/manage memory independently of each other?</p>","contentLength":1030,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TIL features can't be used like this and I think it's nuking my incremental build times","url":"https://www.reddit.com/r/rust/comments/1loqbol/til_features_cant_be_used_like_this_and_i_think/","date":1751336805,"author":"/u/Tiflotin","guid":178726,"unread":true,"content":"<p>I'm making a mmorpg that has several shared types between client and server. I have 3 crates, client, server, shared.</p><p>Item, is one of those types for example. Item has 2 fields `id` and `amount`. I want to change the type of the field based off what crate is using the shared crate. Eg the client, the `id` field should be of type u16. But in the server, I want the `id` type to be a `ItemId` (enum repr u16 type for item constants). The client can't use the same type since `ItemId` wont always be up to date with the latest items in game (we can add items without releasing new client updates).</p><p>This is what I've got so far, and its fine when building the specific crate (eg client or server) but if u try to build the workspace itself, it fails.</p><pre><code>pub struct Item { pub id: ItemIdType, } #[cfg(not(feature = \"server\"))] pub type ItemIdType = u16; #[cfg(feature = \"server\")] pub type ItemIdType = String; </code></pre><p>Example use of client with u16 type: ```rust use shared::Item;</p><p>fn main() { // Client project, NO server feature let test = Item { id: 123 }; println!( \"We are a u16 since we're NOT using the server feature! {:?}\", test.id ); } ```</p><p>Example use of server with String type: ```rust use shared::Item;</p><p>fn main() { // Server project, YES server feature let test = Item { id: String::from(\"Hello\"), }; println!( \"We are a String since we ARE using the server feature! {:?}\", test.id ); } ```</p><p>My issue is when running cargo build in the workspace, it gives an error saying client was failed to build due to incorrect item id type. But if I run cargo build inside the client directory it works fine.</p><p><code> error[E0308]: mismatched types --&gt; client/src/main.rs:5:27 | 5 | let test = Item { id: 123 }; | ^^^- help: try using a conversion method: `.to_string()` | | | expected `String`, found integer </code></p><p>I don't really understand why this isnt valid, and not quite sure how to achieve what I want without duplicating the struct inside server and client. It's got me stumped!</p>","contentLength":1951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Tech Schools Backend MasterClass outdated or worth it?","url":"https://www.reddit.com/r/golang/comments/1looc0q/is_tech_schools_backend_masterclass_outdated_or/","date":1751330875,"author":"/u/stewrat1","guid":179077,"unread":true,"content":"<p>I am starting to learn Go and I found this course from Tech School: </p><p>What interested me about this course was the AWS, Docker and Kubernetes usage in it too- as it seems quite industrious and valuable. My only concern is if it is outdated as I saw on YouTube, the original series was made 5 years ago. </p><p>Anyone take this course recently or have other suggestion for learning? </p>","contentLength":373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] best chunking method for financial reports?","url":"https://www.reddit.com/r/MachineLearning/comments/1loob3z/d_best_chunking_method_for_financial_reports/","date":1751330799,"author":"/u/Wickkkkid","guid":177069,"unread":true,"content":"<p>Hey all, I'm working on a RAG (Retrieval-Augmented Generation) pipeline focused on financial reports (e.g. earnings reports, annual filings). I‚Äôve already handled parsing using a combo of PyMuPDF and a visual LLM to extract structured info from text, tables, and charts ‚Äî so now I have the content clean and extracted.</p><p>My issue: I‚Äôm stuck on choosing the right chunking strategy. I've seen fixed-size chunks (like 500 tokens), sliding windows, sentence/paragraph-based, and some use semantic chunking with embeddings ‚Äî but I‚Äôm not sure what works best for this kind of data-heavy, structured content.</p><p>Has anyone here done chunking specifically for financial docs? What‚Äôs worked well in your RAG setups?</p><p>Appreciate any insights üôè</p>","contentLength":740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] How far are we from LLM pattern recognition being as good as designed ML models","url":"https://www.reddit.com/r/MachineLearning/comments/1loo8yl/d_how_far_are_we_from_llm_pattern_recognition/","date":1751330623,"author":"/u/chrisfathead1","guid":178899,"unread":true,"content":"<p>LLMs are getting better quickly. It seems like every time a new release comes out, they have moved faster than I anticipated. </p><p>Are they great at abstract code, integrating systems, etc? Not yet. But I do find that they are excellent at data processing tasks and machine learning code, especially for someone who knows and understands those concepts and is able to understand when the LLM has given a wrong or inefficient answer.</p><p>I think that one day, LLMs will be good enough to perform as well as a ML model that was designed using traditional processes. For example, I had to create a model that predicted call outcomes in a call center. It took me months to get the data exactly like I needed it from the system and identify the best transformation, combinations of features, and model architecture to optimize the performance.</p><p>I wonder how soon I'll be able to feed 50k records to an LLM, and tell it look at these records and teach yourself how to predict X. Then I'll give you 10k records and I want to see how accurate your predictions are and it will perform as well or better than the model I spent months working on. </p><p>Again I have no doubt that we'll get to this point some day, I'm just wondering if you all think that's gonna happen in 2 years or 20. Or 50? </p>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AerynOS: Mid-year update","url":"https://aerynos.com/blog/2025/06/30/mid-year-update/","date":1751328580,"author":"/u/NomadicCore","guid":178701,"unread":true,"content":"<p>As we hit the middle of the year, it‚Äôs time for another update for those of you following along with AerynOS‚Äôs development.</p><p>Over the last few months, things may have seemed unusually quiet, however rest assured that there has been A LOT going on in the background. As such, we are preparing a short series of blog posts to go over the relevant topics in the coming weeks.</p><p>For this blog post, we are going to cover our infrastructure port, along with the process of rebuilding our entire package repository.</p><ul><li>All core AerynOS tooling is now written in Rust</li><li>Every recipe in the repository has been rebuilt (twice!) with many packages then having been updated to newer versions after the rebuilds were completed</li><li>A CDN has been implemented for faster package installation and ISO downloads</li></ul><p>When delivering a Linux distribution, its infrastructure and associated processes effectively act as the ‚Äúspine‚Äù of the project. But spine surgery can be a delicate affair, particularly when it comes to rehabilitation after successful surgery.</p><p>For us, this cycle has been particularly demanding, as we have completed an MVP (Minimum Viable Product) port of our infrastructure tooling code to Rust, meaning that all core AerynOS tooling has now fully transitioned away from DLang.</p><p>We have covered the reasons for this transition <a href=\"https://aerynos.com/blog/2023/09/06/oxidised-moss/\">previously</a>, and it‚Äôs fair to say that we are already feeling the benefits of easy and native reuse of code in our tooling repositories and welcoming more Rust contributors into our community.</p><p>Earlier this year, our existing DLang build infrastructure started showing signs of instability and required more and more manual intervention to successfully land packages.</p><p>Given our prior decision to transition our tooling over to Rust, we had already stopped further development of the DLang based infrastructure. Hence, we decided to accelerate our transition timeline for the infrastructure re-write to Rust with tarkah and ermo leading the development activity, which began at the end of March.</p><p>Towards the end of May, we put the first infrastructure prototype to the test, and then iteratively fixed bugs and built out missing functionality to the point of being able to put our MVP into production on our build infrastructure.</p><p>This MVP will serve as the development base of the code that will be used for all future package builds.</p><p>We have some cool features planned in AerynOS that we envision will make package maintenance a lot easier to manage through smart use of automation.</p><p>Until these features are implemented, however, maintaining the AerynOS repository will remain somewhat human resource intensive. This is the main reason why the repository is consciously being kept ‚Äúsmall‚Äù for now, with us deliberately focusing on having packages that will help developers and contributors improve AerynOS, while still delivering a nice Daily Driver experience.</p><p>Until the new features are implemented, this will necessarily be a balancing act between maintaining the package repository so it doesn‚Äôt go stale vs. having the development time to implement the new features.</p><p>Aside from porting the infrastructure code to Rust, proper testing was required to yield confidence that packages were both successfully built on the new infrastructure and that they worked as expected.</p><p>The end goal was to prove that we were able to rebuild the full AerynOS recipes repository (currently at ~950 recipes) from start to finish without infra-related build errors on the new infra.</p><p>To enable the rebuild, ermo set up a distributed build cluster of four builders of varying hardware specifications. A separate branch of the ‚Äòrecipes‚Äô repository was created, and was used to test both the Rust infrastructure and to land packages for internal testing without them being seeded to user installs.</p><p>In addition, compared to the old infra, we made it simpler to add new avalanche build agents to the build cluster, thus making it very simple to scale out our build cluster as required.</p><p>To summarise the infrastructure Rust re-write and testing effort, we have:</p><ul><li>Completed more than 3k recipe builds</li><li>Deployed the new Rust infrastructure on the AerynOS builders and continue to use it on ermo‚Äôs build cluster</li><li>Validated that the new infrastructure code is more stable and performant at runtime than the previous DLang version</li></ul><h3>How did testing add value?</h3><p>The full rebuild of the recipes repository has also served to ensure ABI sanity for dependencies. Additionally, we can now say that at this point in time, the whole AerynOS repository is known to be buildable and works with all the latest toolchains.</p><p>A special thanks goes to Reilly Brogan, who worked diligently with ermo to not only drive the rebuild process, but also to ensure that some longstanding repository issues were corrected as part of the rebuild process.</p><p>During this process, we have delivered updates to our os-tools (Boulder and Moss), toolchains and build systems. A selection of the updates and additions include (but is certainly not limited to):</p><ul><li>Linux 6.14.11 (6.15.x on the way)</li><li>Distrobox added at v1.8.1.2</li><li>Exfatprogs added at v1.2.9</li></ul><p>As mentioned earlier, the testing work was <a href=\"https://github.com/orgs/AerynOS/discussions/47\">conducted</a> on a separate branch of the <a href=\"https://github.com/AerynOS/recipes\">recipes repository</a>. Consequently, those of you on the old <code dir=\"auto\">packages.aerynos.com/volatile/</code> repository, have not received any updates over the last 10-12 weeks.</p><p>This was a conscious decision to ensure that the mostly untested packages built during the infrastructure testing process did not reach end users immediately. Even though AerynOS is in Alpha and under continuous development, we still do our best not to break user systems if we can avoid it!</p><p>Now that we have a level of testing in place, with this blog post, we are announcing a new rolling  package repository for users. The old  package repository has received one final update to Moss that fixes an important bug when transitioning to the new  repository.</p><p>To ease the transition to the new repository for existing users, we are working on a script that can automatically modify the active repository on the system.</p><p>Once this script has been sufficiently productized, the next time existing users update their systems, they will notice that every single package will show an update available.</p><p>The exact number will vary from system to system depending on how many other packages are installed from the repository but for context, on a base AerynOS GNOME install, this is around 500 packages.</p><p>In the meantime, we have created a manual guide on how to transition existing installs to the new repository in our GitHub Discussions forum <a href=\"https://github.com/orgs/AerynOS/discussions/53\">here</a>. The process is fairly simple, but if you do have any issues transitioning manually, do get in touch via a comment under the GitHub Discussions <a href=\"https://github.com/orgs/AerynOS/discussions/53\">post</a> or via <a href=\"https://matrix.to/#/#aerynos:matrix.org\">Matrix</a>.</p><h3>Content Delivery Network for Packages and ISOs</h3><p>A common bit of feedback we have been receiving relates to the download speed of our repository, namely that it is not fast or even acceptable, especially if you live outside of Europe. This became more evident for those using the rebuild repository on ermo‚Äôs rebuild testing server, which felt noticeably faster for people in Europe in particular.</p><p>To remedy this, we have implemented CDN caching for our new  hosted assets. This means there will be synced copies of our ISOs and package repository on CDN servers around the world, which should help improve download speeds.</p><p>In particular, the new rolling  package repository mentioned above will be served via this CDN for the benefit of our users.</p><p>Please let us know how you get on with AerynOS ISO and package downloads in the coming weeks, as we would love to validate the improvement outside of our own internal testing.</p><h2>Future infrastructure development targets</h2><p>So far, we have only outlined what we have already accomplished since late March.</p><p>The next part of this blog post is going to be a brief outline of where we are going from here in terms of infrastructure and repository development.</p><p>With the transition to the new infrastructure and the new  repository, we have been freed up to begin planning out the necessary steps to be able to deliver versioned repositories and versioned Moss format upgrades.</p><p>These topics have been mentioned in a previous blog <a href=\"https://aerynos.com/blog/2025/02/06/hello-2025/#-versioned-repositories\">post</a>.</p><h3>How do versioned repositories add value?</h3><p>Versioned Repositories will enable us to deploy new Boulder and Moss features in a seamless fashion. This will enable us to introduce breaking code and on-disk format changes, that would otherwise cause installed systems to require manual intervention for them to continue to receive updates.</p><p>Once versioned repositories are in place, the goal is that users will be able to simply update and sync their system as normal via the  command.</p><ul><li>Users will be upgraded to the new versions of Moss that uses a new repository format, without having to pay special attention.</li><li>It will enable AerynOS to iteratively expand the capability of Moss and Boulder on existing systems without breaking user systems in the process.</li></ul><p>We consider versioned repositories a pre-requisite for what we call ‚Äútry-builds‚Äù and eventually multi-arch support.</p><ul><li>Automated try-builds denotes the process whereby the infrastructure discovers an update to the upstream source repository of a package, attempts to auto-update the recipe and then attempts to build the updated package recipe in question.</li><li>We think this will be a useful tool for contributors as it will automate some of the packaging tedium related to simple package version updates. It will also help enable automated regression testing and build flag optimisation in a future workstream.</li><li>Included under the multi-arch umbrella is our ability to target ARM, RISC-V, and different x86 architecture levels such as x86-64-v3 or v4.</li></ul><p>Within the previous 3 month period, we have rebuilt a brand new Rust version of the infrastructure tooling that is robust enough to run in production on AerynOS servers, delivering packages to our contributors and users. This new version has proven to be more stable and performant than the old DLang version we were previously using.</p><p>From a day to day perspective, unlocking the infrastructure means that we can get back to reviewing and landing recipe PRs for our <a href=\"https://github.com/orgs/AerynOS/discussions/52\">package maintainers</a> or accepting new contributors into our AerynOS ecosystem. For those wishing to <a href=\"https://aerynos.dev/packaging/workflow/basic-workflow/\">contribute</a> to AerynOS, please make sure that you have manually <a href=\"https://github.com/orgs/AerynOS/discussions/53\">switched over</a> to our new repositories before making submissions to ensure you are using all the latest tooling.</p><p>Alternatively, you can wait until the automatic transition script is functional and have it make the change for you.</p><h3>Where to get in touch with us</h3><p>If you want to engage with the team, feel free to drop by our GitHub <a href=\"https://github.com/orgs/AerynOS/discussions\">Discussions</a>, raise issues across our various repositories or if you‚Äôre interested in contributing, feel free to raise PRs where you think our code can be improved or where you want to submit recipes for our repo.</p><p>We also have our matrix space that you can access via this <a href=\"https://matrix.to/#/#aerynos:matrix.org\">link</a>:</p><ul><li>The Development room in particular is a great place for discussions around our code.</li><li>The General room is a great place to drop by and get to know the team.</li><li>The Packaging room is where you want to be if you‚Äôre interested in building packages for yourself and/or submitting them to the repository.</li></ul><p>Concurrently to our work around the infrastructure re-write and repository rebuild, there has been several additional workstreams running in the background.</p><p>The team has been refactoring our existing Rust code, mainly focused on our os-tools (Moss and Boulder) and we are working on several additional improvements that we want to get over the finish line before our next ISO release.</p><p>We will be sharing details of this work in upcoming blog posts over the next few weeks.</p>","contentLength":11671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lonjn2/aerynos_midyear_update/"},{"title":"Getting started with Go","url":"https://www.reddit.com/r/golang/comments/1lonfsi/getting_started_with_go/","date":1751328270,"author":"/u/Brunoo_1013","guid":178631,"unread":true,"content":"<p>I have been programming for a while now, and I have built some projects including an IRC server in C++. Back then I had to choose between an IRC or web server, but now I wanted to learn Go and thought of building a web server as a way to start learning Go. This would allow me to explore how HTTP works and get started in the language.</p><p>Would this be a good idea, or should I start smaller and learn basic concepts first? If so, what specific Go concepts should I look into?</p>","contentLength":472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Writing Code Was Never The Bottleneck","url":"https://ordep.dev/posts/writing-code-was-never-the-bottleneck","date":1751325665,"author":"/u/ordepdev29","guid":176987,"unread":true,"content":"<p>For years, I‚Äôve felt that writing lines of code  the bottleneck in software engineering.</p><p>The actual bottlenecks were, and still are, ,  through mentoring and pairing, , , and the human overhead of <strong>coordination and communication</strong>. All of this wrapped inside the labyrinth of tickets, planning meetings, and agile rituals.</p><p>These processes, meant to drive quality, often slow us down more than the act of writing code itself because they require thought, shared understanding, and sound judgment.</p><p>Now, with LLMs making it easy to generate working code faster than ever, a new narrative has emerged: that writing code  the bottleneck, and we‚Äôve finally cracked it.</p><p>But that‚Äôs .</p><p>The marginal cost of adding new software is approaching , especially with LLMs. But what is the price of , , and  that code? .</p><h2>LLMs shift the workload ‚Äî they don‚Äôt remove it</h2><p>Tools like Claude can speed up initial implementation. Still, the result is often more code flowing through systems and more pressure on the people responsible for reviewing, integrating, and maintaining it.</p><p>This becomes especially clear when:</p><ul><li>It‚Äôs unclear whether the author fully understands what they submitted.</li><li>The generated code introduces unfamiliar patterns or breaks established conventions.</li><li>Edge cases and unintended side effects aren‚Äôt obvious.</li></ul><p>We end up in a situation where code is more straightforward to produce but more complex to verify, which doesn‚Äôt necessarily make teams move faster overall.</p><p>It‚Äôs not a new challenge. Developers have long joked about , but the velocity and scale that LLMs enable have <strong>amplified those copy-paste habits</strong>.</p><h2>Understanding code is still the hard part</h2><blockquote><p><em>‚ÄúThe biggest cost of code is understanding it ‚Äî not writing it.‚Äù</em></p></blockquote><p>LLMs reduce the time it takes to produce code, but they haven‚Äôt changed the amount of effort required to reason about behavior, identify subtle bugs, or ensure long-term maintainability. That work can be even more challenging when reviewers struggle to distinguish between generated and handwritten code or understand why a particular solution was chosen.</p><h2>Teams still rely on trust and shared context</h2><p>Software engineering has always been collaborative. It depends on , , and . However, when code is generated faster than it can be discussed or reviewed, teams risk falling into a mode where <strong>quality is assumed rather than ensured</strong>. That creates stress on reviewers and mentors, potentially slowing things down in more subtle ways.</p><h2>LLMs are powerful ‚Äî but they don‚Äôt fix the fundamentals</h2><p>There‚Äôs real value in faster prototyping, scaffolding, and automation. But LLMs don‚Äôt remove the need for , , and . If anything, those become even more important as more code gets generated.</p><p>Yes, the cost of writing code has indeed dropped. But the cost of making sense of it together as a team .</p><p><strong>That‚Äôs still the bottleneck. Let‚Äôs not pretend it isn‚Äôt.</strong></p>","contentLength":2868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lomhlq/writing_code_was_never_the_bottleneck/"},{"title":"What is the purpose of setting the container port field?","url":"https://www.reddit.com/r/kubernetes/comments/1lomcdg/what_is_the_purpose_of_setting_the_container_port/","date":1751325269,"author":"/u/MaxJ345","guid":176985,"unread":true,"content":"<pre><code>apiVersion: v1 kind: Pod metadata: name: mysql-server spec: containers: - name: mysql image: mysql:8 env: - name: MYSQL_ROOT_PASSWORD value: \"...\" ports: - containerPort: 3306 </code></pre><p>Even if I remove the  section, everything will work just fine. The MySQL database server will continue listening on port 3306 and function without issue.</p><p>I'll still be able to reference the port using a service:</p><pre><code>apiVersion: v1 kind: Service metadata: name: mysql-service spec: selector: ... ports: - protocol: TCP port: 12345 targetPort: 3306 type: ClusterIP </code></pre><p>I'll still be able to access the database via port forwarding:</p><pre><code>kubectl port-forward pod/mysql-server --address=... 55555:3306 </code></pre><p>So what is the purpose of setting the container port field?</p>","contentLength":717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DarkDiskz ‚Äì a simple open-source Linux GUI for disks, RAID, bcache, and SMART (early version, feedback welcome!)","url":"https://www.reddit.com/r/linux/comments/1lom3vr/darkdiskz_a_simple_opensource_linux_gui_for_disks/","date":1751324641,"author":"/u/Dark_ant007","guid":178901,"unread":true,"content":"<p>I wanted to share a project I‚Äôve been working on called .</p><p>It‚Äôs an <strong>open-source Python/GTK4 GUI tool</strong> that combines several disk-related utilities in one place. The goal is to make it easier to see drive information and manage storage setups without juggling a bunch of separate commands.</p><ul><li>View detailed disk information (, )</li><li>Set up and monitor bcache</li></ul><p><strong>‚ö†Ô∏è Important Notice (Please Read):</strong> This is an <strong>early project by an amateur coder</strong>, so:</p><ul><li>Some functions may not work perfectly.</li><li><strong>You could lose data if you use destructive operations like wiping drives or re configuring RAID.</strong></li></ul><p><strong>üí° Please back up all important data before testing or using any of the write/format functions. Use at your own risk.</strong></p><p> I‚Äôm not much of a programmer‚Äîthis is my first serious attempt at making something useful for the Linux community. I‚Äôm hoping others might try it out, give feedback, report issues, or even contribute improvements. I probably wont change or edit the program any farther maybe the community enjoys this I hope so. </p><p>If you‚Äôre interested, I‚Äôd really appreciate:</p><ul><li>Testing on different distros (I did all testing on Linux Mint)</li><li>Bug reports and suggestions</li><li>Contributions to help make it better and more reliable</li></ul><p>Thanks for taking the time to check it out!</p>","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made my VM think it has a CPU fan","url":"https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html","date":1751323952,"author":"/u/ketralnis","guid":179014,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lolubp/i_made_my_vm_think_it_has_a_cpu_fan/"},{"title":"At what age did you guys instal Linux?","url":"https://www.reddit.com/r/linux/comments/1lolpeb/at_what_age_did_you_guys_instal_linux/","date":1751323592,"author":"/u/angelaanahi","guid":176990,"unread":true,"content":"<p>Hi guys! A reel I saw on Instagram made me notice that a lot of people installed their first Linux distro when they were 12, I also installed it when I was 12 (Ubuntu 10), so I was generally curious on this, at what age did you install Linux? And why? </p>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] BIG-Bench Extra Hard","url":"https://arxiv.org/abs/2502.19187","date":1751323299,"author":"/u/EducationalCicada","guid":176988,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1lollc0/r_bigbench_extra_hard/"},{"title":"Tips & Tricks‚ÄîSecuring Kubernetes with network policies","url":"https://www.reddit.com/r/kubernetes/comments/1lolcao/tips_trickssecuring_kubernetes_with_network/","date":1751322650,"author":"/u/wineandcode","guid":177052,"unread":true,"content":"<p>Understanding what each network policy does individually, and how they all work together, is key to having confidence that only the workloads needing access are allowed to communicate and that we are are restrictive as possible, so if a hacker takes control of a container in our cluster it can not communicate freely with the rest of the containers running on the cluster. This post by Guillermo Quiros shares some tips and tricks for securing kubernetes with network policies:</p>","contentLength":478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"React Still Feels Insane And No One Is Talking About It","url":"https://mbrizic.com/blog/react-is-insane/","date":1751321638,"author":"/u/mbrizic","guid":177053,"unread":true,"content":"<p>Recently, I did a side project that I wrote about in <a href=\"https://mbrizic.com/blog/receipt-scanner/\">the other post</a>. As part of it, I had what was supposed to be just a few paragraphs on how React sucks - but I just couldn't stop writing about it.</p><p>So here it is a full, standalone blog post, even bigger than the one it sprang from, all about how React sucks. And how it might not even be its own fault.</p><p>In my junior days, down on the streets, I used to do Angular.JS for money. At the time, it was a seriously good piece of technology. Definitely the biggest JS framework of its time, and what's most important to its legacy, probably the first time web development has had a \"framework\". Prior to that, they were all \"libraries\", so this was the first one that gave not only you a set of functions to use, but the actual framework in which you built your web app.</p><p>But things are always only good in relative, and Angular was good because it's predecessors were not. At the time, we had other SPA frameworks like Backbone and Knockout, but they didn't leave as much of an impact. No, the real enemy that Angular had beaten was jQuery.</p><p>Even though jQuery was only a wrapper over (at the time admittedly very shoddy) HTML DOM APIs, it still became a de-facto standard if you wanted to build complex web applications. How it worked was pretty straightforward: you create HTML elements in JS, manually and imperatively, then modify them, move them around, do whatever it takes to make the website interactive like it's an app.</p><p>This all works completely fine for simple apps, but you can imagine it becoming a maintenance nightmare in case of anything bigger. And that is exactly what started happening around. You can't really blame jQuery, but only the appetites of modern users who needed that kind of interactivity everywhere. So developers were blindsided to keep using jQuery even though it was not a good fit for the job anymore.</p><p>Then Angular arrived and had it all sorted out. You could focus your energy on writing the UI and app logic instead of manually assembling individual pieces of HTML. It truly was a game-changing  framework, as you finally had a proper tool to make  interactive applications. Some magic things it had:</p><p>A) Components. Ok, it had a weird naming so these were actually called \"directives\", but in any case you could define a simple HTML and JS file combo representing a piece of UI and then reuse it across multiple places in the app.</p><p>B) Two-way binding. You define a variable, and whenever it changes, all the places in the UI are updated. This worked really well. Later, people started nagging that this omnidirectional data flow is bad, so there was a push to use one-way (top-bottom) bindings instead, which does sound technically better, but in practice made everything more complicated and started a strand of discussion which ended with us all having to use Redux today. So thanks.</p><p>On my first job I worked on exactly one such rewrite of an huge, unwieldy jQuery app into an Angular app. Both the process and the end results were pretty good.</p><p>What was not good, though, was having to rewrite the exact same screens in Angular 2 a few years later, and I'm just happy I left that company early enough before they made me rewrite it for the third time in React later. </p><p>I did get a chance later to get to learn React and even use it professionally on a project or two.</p><p>I still remember seeing how fresh it looked on first glance. At the time, the contrast was with the framework of the day, which was Angular 2 - a complete rewrite of the original, but now with twice the boilerplate, Typescript-out-of-the-box, one-way binding, reactive/observable patterns - all good things on their own, but god damn was it complicated, slow to work on, slow to build, slow to run.</p><p>React swung the pendulum back to the simplicity and people were up all for it. And for a while, simplicity remained, React gained popularity to become the #1  for making SPAs.</p><p>Yes, now we were using the term \"library\" again, showing how simpler it really was. But you can't reasonably build a complex app with just a library. You need few of them to handle all of the app's concerns, and you also need some code structure. React's \"bring your own beer\" approach meant you basically built a framework yourself, with all the downsides it had.</p><p>The end result - no two React apps were the same. Each of them had a bespoke \"framework\" built out of random libraries found on the internet.</p><p>The apps I had the misfortune to work on at the time all made me think the same thing - even Angular 2 would be better than The JSX \"core\" always seemed solid, but everything around it was just plain mess.</p><p>So I got out and went writing some Java backends, which I believe says it all.</p><p>They say a man can never really learn anything - you either know something or you don't. I apparently don't, so I dragged myself back into React recently.</p><p>Granted, it was a hobby project, so I didn't experience it \"in full\" like I would if it was a serious production app. But still, even this experience both confirmed and greatly exceeded my low expectations for it. React feels insane and I don't know how no one else is talking about it.</p><p>First, let's start with the architecture React enforces for you. As said before, React is only a library, so it's not forcing you on anything, but still, the implicit constraints of having JSX make some patterns surface on their own. Eons ago, we used to talk about MVC, MVVM, MVP, all of which only a variations on the same theme, so which one is React? None, I believe this is a new-ish paradigm - I think we could literally call it \"components-based architecture\".</p><p>On first glance, it's all logical. You have components, you build a top-down tree of them, and bam there's your app. React does some internal magic to make sure it's up to date with the data you give it. Simple enough.</p><p>But sometime along the way, it all started acting smarter than it should really be. For a simple \"UI library\", React sure has a lot of loaded terminology. And for a library that doesn't have anything to do with \"functional programming\", it sure has a lot of functional programming names inside.</p><p>Let's start from the state. If you have a top-down tree of components, it's logical you'd want to pass the state top-down too. But in practice, with components very numerous and small, this is very messy, as you spend a lot of time and code just wiring the various pieces of data to get them where you need them.</p><p>This was solved by \"sideloading\" state into components using React hooks. I haven't heard anyone complain about this, but are you guys serious? You're saying that any component can use any piece of app state? And even worse, any component can emit a state change, that can then update in any other component.</p><p>How did this ever pass a code review? You are basically using a global variable, just with more elaborate state mutation rules. They're not even rules, but merely a ceremony, because nothing is really preventing you from mutating state from anywhere. People really think if you give something a smart name like a reducer it suddenly becomes Good Architecture‚Ñ¢?</p><p>So if both top-down and sideloading approaches suck, what would be the solution for this? I honestly don't know. In fact, the only thing I can think of is: if we can't solve this nicely, then maybe the entire \"components architecture\" was a mistake and we shouldn've called it a paragon of Nice Design and stopped innovating. Maybe this time for a change we really did need yet another JS framework that would try something better.</p><p>Next on in the \"things we're unsure how they passed a code review\", let's riff on React Hooks. There's no denying they're useful, but their existence even to this day raises question marks above my head.</p><p>I won't even mention how people talk about components as \"pure functions\" but then have hooks as a tiny stateful black boxes inside of them. And given their composable nature, it's more like layers and layers of tiny stateful black boxes. </p><p>But no, I'd mostly like to roast  here. It's simple what a \"side effect\" would be. You change a state and then you need to do some external action, like post the results to an API. This split between the \"important app stuff\" and \"side effects\" makes sense - in theory. But in practice, can you ever split it cleanly like that?</p><p>My biggest gripe, for starters, is that  is used as a \"run something after the component mounts\". I understand when React migrated from classes to hooks, this was the closest alternative to , but come on - how is this not considered a huge hack? </p><p>You're using a \"side effect\" hook to  the component? Ok, if you have to make an API call from there, I'd agree that would be a side effect. But then that API call... it... it sets the state too. So a completely innocous \"side effect\" hook actually manages a state of the component. Why is no one talking about how crazy this is?</p><p>Moreover, if you wanted to depend on that state and do something after it, then you... you... define yet another  with a dependency on what the first one sets. </p><img src=\"https://mbrizic.com/images/use-effect-01.png\" alt=\"Screenshot showing hard-to-parse useEffect chain\" loading=\"lazy\"><p>This is a code that I have taken from a production app of company recently acquired for several tens of millions of US dollars. I slightly redacted it here to use a simpler  and  entities instead of what is actually there. But go take a look and try to parse in which order is this code executed. When you're ready, the answer is in an image below:</p><img src=\"https://mbrizic.com/images/use-effect-02.png\" alt=\"Screenshot showing hard-to-parse useEffect chain, with diagram showing how it's actually executed \" upside=\"\" down\"\"=\"\" loading=\"lazy\"><p>So something like that, a series of state mutations that would otherwise be a simple imperative code is now... spread out across two asynchronous functions, where the only hint of the order of their execution is the \"dependency array\" at the bottom of each. And the way that you actually mentally parse it is, in fact, from the bottom to the top.</p><p>I remember how Javascript promises were considered unwieldy with their s, and even before them we had \"callback hell\" - but literally anything would be better than this. </p><p>I understand these issues could be solved a) by moving them into a separate file, which is just hiding the problem, or b) probably with some Redux or something, but I really don't have enough mileage with it to know for sure. </p><p>All of this combined looks ugly, and betrays the simplicity that React promised in its \"Hello world\" example. But wait, I'm not done yet. I read a blog post from an acquaintance called <a href=\"https://www.mensurdurakovic.com/the-most-common-react-design-patterns/\">\"The Most Common React Design Patterns\"</a>. Expecting I don't know what, I was still shocked at how complicated these are and how much mental overhead there is to simply figure out what is happening - and all of that just to render a list of items on a screen.</p><p>The most jarring thing: the article doesn't even acknowledge it. All this complexity is taken from granted. People apparently really build their UIs like this and no one bats an eye.</p><p>Then, as if that isn't enough, some of you go as far as to write \"CSS-in-JS\", and then get paid for it. I agree that JSX initially showed that \"separation of concerns\" is not \"separation of files\" and that it's actually okay to write your HTML and JS in a same file. But chucking in CSS in there too and making it strongly typed? Isn't this a step too far? </p><p>It would be too easy to just say React is, well, downright insane, and go on with our lives. But as reasonable primates, I believe we can do better. We can try to understand it.</p><p>I am once again trippin' down the memory superhighway to get reminded of my first job and a colleague from the mentioned \"jQuery migration\" project. A super-experienced backend engineer, an architect type, and overall a very respected guy when it came to all things software.</p><p>What I remember the most about him are not his technical solutions, but the amount of judgement he'd have shown on anything we did on frontend. Looking anything on the Angular app, he was like - what the hell are you guys doing here? Why does this have to be so complicated?</p><p>And it's not that we sucked - we too were a no-nonsense crew about software. It's just that at the time, through the eyes of a classical backend developer, the entire Angular setup seemed absolutely insane.</p><p>Today, I'm roughly the same age as he was then, and I am here writing a blog post about how  React is insane. Some things are inevitable, I guess.</p><p>But let's rise a step above and try to understand why it could be so.</p><p>Firstly, I think we can all agree that most web apps shouldn't even be web apps in the first place. People go the way of SPA even if they don't need a SPA but they might need it later, so it apparently doesn't cost as much to go with SPA from scratch.</p><p>But I'd argue here that such a move, in fact, does cost you. It's just that we're so entrenched in the \"SPA-by-default\" way that we forgot how simpler the alternatives are. Having a simple dumb server-side-rendered page is orders of magnitude simpler than even thinking about React. There's no overhead with API communication, frontend is very lightweight, your UI code can be strongly typed (if your backend is strongly typed), you can do refactors across the full stack, everything will load faster, you can cache it better because some components are very static and remain the same for all the users so you can render them only once, etc, etc.</p><p>You do lose the flexibility to have complex interactive logic at your product manager's whim, though. But that's maybe only partially true, because I'd wager you could go a pretty long way with just plain Javascript \"progressive enhancement\" before you really get to a state management complex enough to warrant adding React in there.</p><p>Ok, so I'm saying we use React simply because we've used it before. No wonder, inertia is a hell of a drug, but it still doesn't explain why this code ends up being so unthinkably complex.</p><p>My answer to that question, surprisingly, stops roasting React and goes the opposite way, defending not only React, but also Angular and jQuery and everything that came before them. I think this code is bad because <strong>making a interactive UI where any component can update any other component is simply one of the most complicated things you could do in software</strong>.</p><p>Think of any other system you use in your everyday life. Your kitchen sink has two inputs, hot and cold, and one output, a water running. Your kitchen mixer or a power drill might have a button or two, and still whatever you do, it only affects the action on the spinning part. An oven might have three or four or five knobs and maybe the same number of outputs, and already that is starting to sound pretty dangerous.</p><p>In contrast, an interactive UI that we have on web can have <strong>potentially infinite number of inputs, and potentially infinite number of outputs</strong>. How could you even expect to have a \"clean code\" for this?</p><p>So, this entire rant about React... it's not even React's fault. Neither is Angular's, or jQuery's. Simply, whichever tech you choose will inevitably crumble down under the impossible complexity of building a reactive UI.</p><p>How could we fix this? I'm not smart or in-the-weeds enough to really solve this problem, but I can spitball some ideas. If we adopt this input/output mental model of a webpage as a real thing, then maybe we could start working on reducing a number of its inputs and outputs. </p><p>On the inputs side, yeah, this is me saying: \"go have fewer buttons\", which may not always, or ever, be enforceable. But certainly, the less features you have, the more manageable your codebase is. </p><p>It's straightforward enough to need no mentioning - or is it? Do product managers know that adding three buttons instead of two will cause 5% more bugs and make any future work on that screen 30% more complicated to design and implement? No one is even measuring those things, but I believe they could be true. </p><p>Why is it that, if I told you we need to add Redis on backend, you will tell me \"no, we need to curb the technical complexity\" - but if a product manager asks to add a global app-wide filter that could be applied from anywhere and to anything, you'd just get your head down and write some monstruosity that people will spend the next 10 years trying to get out.</p><p>In short - please, stop adding so many buttons, I beg you. You could even, I know, crazy, try to remove some of them?</p><p>On the outputs side, however, the story is a bit different. Writing this makes me realize that having a server-side rendered page is basically reducing the page to a single output. Anything you interact with, it just rebuilds the entire page. This means that, ironically, removing FP-inspired React from the mix makes a server-side rendered page an <strong>actually a pure function of the state</strong>. No frontend state = big simplicity wins, if you could afford it.</p><p>Inevitably, when you do need some scripting logic in your server-side rendered \"app\", maybe the smart move would be to add it only on the most necessary places. The smallest you could go with, the better.</p><p>I thought a good name for this would be \"islands of interactivity\". Then I Googled it and <a href=\"https://deno.com/blog/intro-to-islands\">turns out that's already a thing</a>. Although, that post still mentions Preact, SSR, manifest files, so I'm not sure we're really on a same page. People will overcomplicate everything.</p><p>But I do believe we have enough bandwidth today that you can load a small React app that only renders an island of interactivity inside of what is a classic server-side rendered page. I don't believe that mix would be that abominable, but I've yet to try it, and for my next project, I just might.</p><p>So, my untested approach to having clean and maintanable frontend code is: go render it all on server and plop in React or whatever only where you really need it.</p><p>It really can't be any worse than</p><p>(Side note, I'm trying something new this time (no, it's not Patreon) - here are the \"official\" comment threads for this blog post on <a href=\"https://news.ycombinator.com/item?id=44428489\">HackerNews</a> and on <a href=\"https://www.reddit.com/r/programming/comments/1lokxnv/react_still_feels_insane_and_no_one_is_talking/\">Reddit</a>. To keep up-to-date, you can also <a href=\"https://buttondown.com/mbrizic\">subscribe to my newsletter</a>.)</p>","contentLength":17778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lokxnv/react_still_feels_insane_and_no_one_is_talking/"},{"title":"Why the Linux hate is a thing?","url":"https://www.reddit.com/r/linux/comments/1lok98x/why_the_linux_hate_is_a_thing/","date":1751319983,"author":"/u/Guilty_Bird_3123","guid":176937,"unread":true,"content":"<p>Lately I had installed Linux after years of Windows experience and wanted to open a thread on <a href=\"https://www.reddit.com/r/FACEITcom\">r/FACEITcom</a> for awareness only.</p><p>Couple of negative comments have been sent, why people are so mad about using Linux instead of Windows?</p>","contentLength":228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI's evolution: From Nonprofit to Corporate","url":"https://www.reddit.com/r/artificial/comments/1lojuee/openais_evolution_from_nonprofit_to_corporate/","date":1751318967,"author":"/u/MrKoyunReis","guid":178935,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Should we petition for requiring reviewers to state conditions for improving scores?","url":"https://www.reddit.com/r/MachineLearning/comments/1lohh1u/d_should_we_petition_for_requiring_reviewers_to/","date":1751313372,"author":"/u/Able-Entertainment78","guid":176989,"unread":true,"content":"<p>I‚Äôve been thinking about how opaque and inconsistent peer reviews can be, especially in top ML conferences. What if we made it a requirement for reviewers to explicitly state the conditions under which they would raise their scores? For example, ‚ÄúIf the authors add experiments on XYZ‚Äù or ‚ÄúIf the theoretical claim is proven under ABC setup.‚Äù</p><p>Then, area chairs (ACs) could judge whether those conditions were reasonably met in the rebuttal and updated submission, rather than leaving it entirely to the whims of reviewers who may not revisit the paper properly.</p><p>Honestly, I suspect many reviewers don‚Äôt even know what exactly would change their mind.</p><p>As an added bonus, ACs could also provide a first-pass summary of the reviews and state what conditions they themselves would consider sufficient for recommending acceptance.</p><p>What do you think? Could this improve transparency and accountability in the review process?</p>","contentLength":926,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OPNSense firewall in front of kubernetes cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1logkav/opnsense_firewall_in_front_of_kubernetes_cluster/","date":1751311195,"author":"/u/bykof","guid":176882,"unread":true,"content":"<p>I want to ask you if an OPNSense firewall is a good idea in front of a kubernetes cluster. </p><ol><li>Managing Wireguard in OPNSense</li><li>Access the whole cluster only via Wireguard VPN</li><li>Allow only specific IPs to access the cluster without Wireguard VPN</li></ol><p>Are there any benefits or drawbacks from this idea, that I don't see yet?</p><p>Thank you for your ideas!</p>","contentLength":333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chromium/V8 implementing Temporal API via Rust (temporal_rs and ICU4X)","url":"https://www.reddit.com/r/rust/comments/1logjzt/chromiumv8_implementing_temporal_api_via_rust/","date":1751311174,"author":"/u/Manishearth","guid":176883,"unread":true,"content":"<p>In the last two months I've been working on adding support for the (rather large) <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal\">Temporal</a> datetime API to V8, Chromium's JS engine. The meat of this implementation is all Rust. </p><p>Firefox already has an implementation using <a href=\"https://github.com/unicode-org/ICU4X\">ICU4X</a>. For V8 we're using <a href=\"https://github.com/boa-dev/temporal\">temporal_rs</a>, which builds on top of ICU4X but does more of the spec-specific stuff. This wouldn't be the first Rust in Chromium, but it's a significant chunk of code! You can see most of the glue code in V8 in <a href=\"https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-temporal-objects.cc\">here</a>, and you can look at all of the CLs <a href=\"https://chromium-review.googlesource.com/q/hashtag:%22temporal%22+(status:open%20OR%20status:merged\">here</a>).</p><p>There's still a bunch of work to do on test conformance, but now is a point where we can at least say it is fully implemented API-wise.</p><p>I'm happy to answer any questions people may have! I'm pretty excited to see this finally happen, it's a long-desired improvement to the JS standard library, and it's cool to see it being done using Rust.</p>","contentLength":844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Here Is Everyone Mark Zuckerberg Has Hired So Far for Meta's ‚ÄòSuperintelligence‚Äô Team","url":"https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/","date":1751307207,"author":"/u/wiredmagazine","guid":176885,"unread":true,"content":"<p> Meta staff today to introduce them to the new superintelligence team. The memo, which WIRED obtained, lists names and bios for the recently hired employees, many of whom came from rival AI firms like OpenAI, Anthropic, and Google.</p><p>Over the past few months, Meta CEO Mark Zuckerberg has been on a recruiting frenzy to poach some of the most sought-after talent in AI. The social media giant has invested $14.3 billion in Scale AI and hired Alexandr Wang, its CEO, to <a data-offer-url=\"https://x.com/alexandr_wang/status/1933328165306577316\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://x.com/alexandr_wang/status/1933328165306577316&quot;}\" href=\"https://x.com/alexandr_wang/status/1933328165306577316\" rel=\"nofollow noopener\" target=\"_blank\">run Meta‚Äôs Superintelligence Labs</a>. News of the memo was <a data-offer-url=\"https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires&quot;}\" href=\"https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires\" rel=\"nofollow noopener\" target=\"_blank\">first reported by Bloomberg</a>.</p><p>‚ÄúWe‚Äôre going to call our overall organization Meta Superintelligence Labs (MSL). This includes all of our foundations, product, and FAIR teams, as well as a new lab focused on developing the next generation of our models,‚Äù Zuckerberg wrote in the memo on Monday. Meta declined to comment.</p><p>Zuckerberg introduced Wang, who will be the company‚Äôs ‚Äúchief AI officer‚Äù and leader of MSL, as well as former GitHub CEO Nat Friedman. Friedman will colead the new lab with Wang, with a focus on AI products and applied research.</p><p>Here‚Äôs the list of all the new hires as seen in Zuckerberg's memo. It notably doesn‚Äôt include the employees who joined from OpenAI‚Äôs Zurich office.</p><ul><li>Trapit Bansal: pioneered RL on chain of thought and cocreator of o-series models at OpenAl.</li><li>Shuchao Bi: cocreator of GPT-4o voice mode and o4-mini. Previously led multimodal post-training at OpenAl.</li><li>Huiwen Chang: cocreator of GPT-4o's image generation, and previously invented MaskIT and Muse text-to-image architectures at Google Research.</li><li>Ji Lin: helped build 03/o4-mini, GPT-4o, GPT-4.1, GPT-4.5, 40-imagegen, and Operator reasoning stack.</li><li>Joel Pobar: inference at Anthropic. Previously at Meta for 11 years on HHVM, Hack, Flow, Redex, performance tooling, and machine learning.</li><li>Jack Rae: pre-training tech lead for Gemini and reasoning for Gemini 2.5. Led Gopher and Chinchilla early LLM efforts at DeepMind.</li><li>Hongyu Ren: cocreator of GPT-4o, 4o-mini, o1-mini, o3-mini, 03 and o4-mini. Previously leading a group for post-training at OpenAl.</li><li>Johan Schalkwyk: former Google Fellow, early contributor to Sesame, and technical lead for Maya.</li><li>Pei Sun: post-training, coding, and reasoning for Gemini at Google Deepmind. Previously created the last two generations of Waymo's perception models.</li><li>Jiahui Yu: cocreator of 03, 04-mini, GPT-4.1 and GPT-4o. Previously led the perception team at OpenAl, and co-led multimodal at Gemini.</li><li>Shengjia Zhao: cocreator of ChatGPT, GPT-4, all mini models, 4.1 and 03. Previously led synthetic data at OpenAl.</li></ul>","contentLength":2565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1loeu16/here_is_everyone_mark_zuckerberg_has_hired_so_far/"},{"title":"Test orchestration anyone?","url":"https://www.reddit.com/r/kubernetes/comments/1locgxp/test_orchestration_anyone/","date":1751301898,"author":"/u/Dmitry_Fon","guid":176814,"unread":true,"content":"<p>Almost by implication of Kubernetes, we're having more and more microservices in our software. If you are doing test automation for your application (APIs, End-to-End, Front-End, Back-End, Load testing, etc.) - How are you orchestrating those test? - CI/CD - through Jenkins, GitHub Actions, Argo Workflows? - A dedicated Test orchestration tool?</p>","contentLength":346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to manage configuration settings in Go web applications","url":"https://www.alexedwards.net/blog/how-to-manage-configuration-settings-in-go-web-applications","date":1751301728,"author":"/u/alexedwards","guid":176936,"unread":true,"content":"<p>When I'm building a web application in Go, I prefer to use command-line flags to pass configuration settings to the application at runtime. But sometimes, the client I'm working with wants to use environment variables to store configuration settings, or the nature of the project means that storing settings in a TOML, YAML or JSON file is a better fit. And of course that's OK ‚Äî it makes sense to be flexible and vary how configuration is managed based on the specific needs of a project and/or client.</p><p>So, in this tutorial, I want to share the patterns that I use for parsing configuration settings ‚Äî whether they come from flags, environment variables or files ‚Äî and explain how I pass the settings onwards to where they are needed in the rest of the web application code. I'll also end with a short discussion about the relative pros and cons of the different approaches.</p><p>It's a fairly detailed post, so here are the shortcut links for quick reference:</p><p>To illustrate the patterns in the rest of this tutorial, let's pretend that we have a web application where we want to configure the following five settings:</p><table><tbody><tr><td>The port number the web application listens on</td></tr><tr><td>Enables detailed request and error logging</td></tr><tr><td>Maximum duration to wait for a request to complete</td></tr><tr><td>Username required for HTTP Basic Authentication</td></tr><tr><td>Password required for HTTP Basic Authentication</td></tr></tbody></table><p>Regardless of where the configuration settings are coming from (flags, environment variables or a file), I'm quite strict about keeping all the code related to configuration settings isolated in one place, and reading in the configuration setting values <em>right at the start of the program, before doing almost anything else</em>.</p><p>Most of the time, I prefer to store all the configuration setting values in a single  struct, like so:</p><figure><code><pre>type config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n</pre></code></figure><p>I like this because it feels very clear ‚Äî all the configuration settings are contained in a single struct, along with their appropriate Go type, and you can easily see at a glance what configuration settings the application expects and supports.</p><p>As I mentioned at the start of this tutorial, using command-line flags with the standard library <a href=\"https://pkg.go.dev/flag\"></a> package is my preferred approach to managing configuration settings. With this approach, you explicitly pass the configuration values as part of the command when running the program. For example:</p><figure><code><pre>$ go run main.go -port=9999 -verbose-logging=true -request-timeout=10s -basic-auth-username=admin -basic-auth-password=\"secr3tPa55word\"\n</pre></code></figure><p>In your Go code, you define a specific command-line flag using syntax like this:</p><figure><code><pre>flag.IntVar(&amp;cfg.port, \"port\", 4000, \"The port number the web application listens on\")`\n</pre></code></figure><p>In this example code, we define a command-line flag named  that accepts an integer value and stores it at the location pointed to by the  pointer. It will have a default value of  if no corresponding  flag is provided when starting the application, and the final parameter is a description that will be displayed when a user runs the program with the  flag.</p><p>Importantly, after you've defined all the command-line flags for your application, you need to call the <a href=\"https://pkg.go.dev/flag#Parse\"></a> function to actually read in the values from the command-line arguments.</p><p>Let's put this together in a very simple application that reads the command-line flag values into a  struct, and then prints them out.</p><figure><code><pre>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"time\"\n)\n\n// The config struct holds all configuration settings for the application.\ntype config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n\nfunc main() {\n    // Create a new config instance.\n    var cfg config\n\n    // Define the command-line flags. Notice that we define these so that the values \n    // are read directly into the appropriate config struct field, and set sensible default \n    // values for each of them.\n    flag.IntVar(&amp;cfg.port, \"port\", 4000, \"The port number the web application listens on\")\n    flag.BoolVar(&amp;cfg.verboseLogging, \"verbose-logging\", false, \"Enables detailed request and error logging\")\n    flag.DurationVar(&amp;cfg.requestTimeout, \"request-timeout\", 5*time.Second, \"Maximum duration to wait for a request to complete\")\n    flag.StringVar(&amp;cfg.basicAuth.username, \"basic-auth-username\", \"\", \"Username required for HTTP Basic Authentication\")\n    flag.StringVar(&amp;cfg.basicAuth.password, \"basic-auth-password\", \"\", \"Password required for HTTP Basic Authentication\")\n\n    // Parse the flags with the flag.Parse function. This is important!\n    flag.Parse()\n\n    // Print all configuration settings.\n    fmt.Printf(\"Port: %d\\n\", cfg.port)\n    fmt.Printf(\"Verbose Logging: %t\\n\", cfg.verboseLogging)\n    fmt.Printf(\"Request Timeout: %v\\n\", cfg.requestTimeout)\n    fmt.Printf(\"Basic Auth Username: %s\\n\", cfg.basicAuth.username)\n    fmt.Printf(\"Basic Auth Password: %s\\n\", cfg.basicAuth.password)\n}\n</pre></code></figure><p>If you're following along, go ahead and run the application with your own values in the command-line flags. You should see the same values printed out by the application, like so:</p><figure><code><pre>$ go run main.go -port=9999 -verbose-logging=true -request-timeout=30s -basic-auth-username=admin -basic-auth-password=\"secr3tPa55word\"\n<samp>Port: 9999\nVerbose Logging: true\nRequest Timeout: 30s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>If you don't provide a value for a specific flag, the application will revert to using the default value you specified. For example, if you don't provide a  flag it will default to the value of , like so:</p><figure><code><pre>$ go run main.go -basic-auth-username=admin -basic-auth-password=\"secr3tPa55word\"\n<samp>Port: 4000\nVerbose Logging: false\nRequest Timeout: 5s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>One of the great things about the standard library  package is the support for automatic help text. If you run your application with the flag , it will list all the available flags for the application, along with their accompanying help text and default values if appropriate. Like so:</p><figure><code><pre>$ go run main.go -help\n<samp>Usage of /tmp/go-build2103583960/b001/exe/main:\n  -basic-auth-password string\n        Password required for HTTP Basic Authentication\n  -basic-auth-username string\n        Username required for HTTP Basic Authentication\n  -port int\n        The port number the web application listens on (default 4000)\n  -request-timeout duration\n        Maximum duration to wait for a request to complete (default 5s)\n  -verbose-logging\n        Enables detailed request and error logging</samp></pre></code></figure><p>For boolean flags, if you want to pass a value of  you can simply include the flag name without assigning a value. The following two commands are equivalent:</p><figure><code><pre>$ go run main.go -verbose-logging=true\n$ go run main.go -verbose-logging\n</pre></code></figure><p>In contrast, you always need to use  if you want to set a boolean flag value to .</p><p>You can use one or two dashes in front of a flag name, both work identically. The standard library  package does not support 'short' flags, and the number of dashes has no effect on the behavior or any special meaning. So it's just a matter of personal taste which you use. The following two commands are equivalent:</p><figure><code><pre>$ go run main.go -verbose-logging -request-timeout=30s\n$ go run main.go --verbose-logging --request-timeout=30s\n</pre></code></figure><p>If you try to pass an invalid value as a command-line flag, the application will automatically exit with an error message and the help text for reference. For example, if you try to pass a non-integer value in the  flag, the parsing would fail and the output would look like this:</p><figure><code><pre>$ go run main.go -port=foobar\n<samp>invalid value \"foobar\" for flag -port: parse error\nUsage of /tmp/go-build2103583960/b001/exe/main:\n  -basic-auth-password string\n        Password required for HTTP Basic Authentication\n  -basic-auth-username string\n        Username required for HTTP Basic Authentication\n  -port int\n        The port number the web application listens on (default 4000)\n  -request-timeout duration\n        Maximum duration to wait for a request to complete (default 5s)\n  -verbose-logging\n        Enables detailed request and error logging\nexit status 2</samp></pre></code></figure><p>Similarly, if you try to use a flag that as not been defined, the application will automatically exit with an error message and the help text. For example:</p><figure><code><pre>$ go run main.go -foobar=baz\n<samp>flag provided but not defined: -foobar\n...etc</samp></pre></code></figure><p>The  package provides functions for reading command-line flag values into the following Go types: , , , , , ,  and .</p><p>If you want to parse a command-line flag value into another Go type (such as  or ), you have a few different options. The simplest approach is to use the   function, which I've written about <a href=\"https://www.alexedwards.net/blog/custom-command-line-flags\">here</a>. Or you can also make your own custom type that implements the  or  interfaces, and define the flag using either the  or  functions respectively. I've shared a gist demonstrating how to do this <a href=\"https://gist.github.com/alexedwards/7838faf5f4936e2024657d6e306723e1\">here</a>.</p><p>Alternatively, there are third-party packages (such as <a href=\"https://github.com/spf13/viper\"></a>) that you can use, which automatically support parsing command-line flags into a wider range of Go types. Personally, I've never felt it necessary to use these, but YMMV.</p><p>Lastly, if you want you can create , which act like a 'container' for a distinct set of command-line flags. It's rare that I need to use flagsets in a web application, but I do often use them when building CLI applications with multiple subcommands. There's a good tutorial about how to use flagsets <a href=\"https://www.digitalocean.com/community/tutorials/how-to-use-the-flag-package-in-go#using-flagset-to-implement-sub-commands\">here</a>.</p><h2>Using environment variables</h2><p>First, I'll start by saying that you can use environment variables in conjunction with command-line flags if you want. Simply set your environment variables as normal, and use them in the command when starting your application. Like so:</p><figure><code><pre>$ export VERBOSE_LOGGING=\"true\"\n$ export REQUEST_TIMEOUT=\"30s\"\n$ go run main.go -verbose-logging=$VERBOSE_LOGGING -request-timeout=$REQUEST_TIMEOUT\n</pre></code></figure><p>But if you don't want to do this, you can read the values from environment variables directly into your Go code using the <a href=\"https://pkg.go.dev/os#Getenv\"></a> function. This will return the value of the environment variable as a , or the empty string  if the environment variable doesn't exist. You can also use the <a href=\"https://pkg.go.dev/os#LookupEnv\"></a> function to check whether a specific environment variable exists or not.</p><p>To help read values from environment variables, I like to create an  package containing some helper functions that convert the environment variable  to the appropriate Go type, and optionally set a default value for if the environment variable doesn't exist (just like command-line flags). For example:</p><figure><figcaption>File: internal/env/env.go</figcaption><code><pre>package env\n\nimport (\n    \"fmt\"\n    \"os\"\n    \"strconv\"\n    \"time\"\n)\n\nfunc GetInt(key string, defaultValue int) int {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n\n    intValue, err := strconv.Atoi(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to an int\", key, value))\n    }\n    return intValue\n}\n\nfunc GetBool(key string, defaultValue bool) bool {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n\n    boolValue, err := strconv.ParseBool(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to a bool\", key, value))\n    }\n    return boolValue\n}\n\nfunc GetDuration(key string, defaultValue time.Duration) time.Duration {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n\n    durationValue, err := time.ParseDuration(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to a time.Duration\", key, value))\n    }\n    return durationValue\n}\n\nfunc GetString(key string, defaultValue string) string {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        return defaultValue\n    }\n    return value\n}\n</pre></code></figure><p>In some projects, I use a twist on these helper functions and panic if a specific environment variable isn't set, rather than returning a default value. For example:</p><figure><code><pre>func MustGetInt(key string) int {\n    value, exists := os.LookupEnv(key)\n    if !exists {\n        panic(fmt.Errorf(\"environment variable %s must be set\", key))\n    }\n\n    intValue, err := strconv.Atoi(value)\n    if err != nil {\n        panic(fmt.Errorf(\"environment variable %s=%q cannot be converted to an int\", key, value))\n    }\n    return intValue\n}\n</pre></code></figure><p>Using those helper functions in your application then looks a bit like this:</p><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n    \"time\"\n\n    \"your-project/internal/env\"\n)\n\ntype config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n\nfunc main() {\n    var cfg config\n\n    cfg.port = env.GetInt(\"PORT\", 4000)\n    cfg.verboseLogging = env.GetBool(\"VERBOSE_LOGGING\", false)\n    cfg.requestTimeout = env.GetDuration(\"REQUEST_TIMEOUT\", 5*time.Second)\n    cfg.basicAuth.username = env.GetString(\"BASIC_AUTH_USERNAME\", \"\")\n    cfg.basicAuth.password = env.GetString(\"BASIC_AUTH_PASSWORD\", \"\")\n\n    fmt.Printf(\"Port: %d\\n\", cfg.port)\n    fmt.Printf(\"Verbose Logging: %t\\n\", cfg.verboseLogging)\n    fmt.Printf(\"Request Timeout: %v\\n\", cfg.requestTimeout)\n    fmt.Printf(\"Basic Auth Username: %s\\n\", cfg.basicAuth.username)\n    fmt.Printf(\"Basic Auth Password: %s\\n\", cfg.basicAuth.password)\n}\n</pre></code></figure><p>If you'd like to try this out, go ahead and add the necessary environment variables to your  or  files, or  them in your shell, and try running the application again. You should see the configuration settings reflected in the output, or any default values for ones that you didn't set.</p><figure><code><pre>$ export PORT=\"9999\"\n$ export VERBOSE_LOGGING=\"false\"\n$ export BASIC_AUTH_USERNAME=\"admin\"\n$ export BASIC_AUTH_PASSWORD=\"secr3tPa55word\"\n$ go run main.go \n<samp>Port: 9999\nVerbose Logging: false\nRequest Timeout: 5s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>If you're working on multiple projects on the same development machine (and not using separate containers for each project), it can become awkward to manage environment variables and avoid clashes across the projects. Rather than setting environment variables in  or , a fairly common workaround is to create an  file in your project containing the environment variables, like so:</p><figure><code><pre>export PORT=5000\nexport VERBOSE_LOGGING=true\nexport REQUEST_TIMEOUT=10s\nexport BASIC_AUTH_USERNAME=admin\nexport BASIC_AUTH_PASSWORD=secr3tPa55word\n</pre></code></figure><p>Then you can  the  file to export the variables in the current terminal session and run your Go application:</p><figure><code><pre>$ source .env \n$ go run main.go \n<samp>Port: 5000\nVerbose Logging: true\nRequest Timeout: 10s\nBasic Auth Username: admin\nBasic Auth Password: secr3tPa55word</samp></pre></code></figure><p>Alternatively, if you don't want to keep running the  command, you can use the <a href=\"https://github.com/joho/godotenv\"></a> package to automatically load the values from the  file into the environment when your application starts up.</p><h2>Using configuration files</h2><p>The third option that I sometimes use is configuration files, which store all the settings in a single file on-disk. I normally only use these in projects where there are  of configuration settings, and loading them all via command-line flags would be onerous and error-prone. Or also, if the configuration settings are complex, with a deeply nested 'structure' to them.</p><p>There are a lot of different formats that you can use for configuration files, such as TOML or YAML ‚Äî or even JSON. They all have different advantages and disadvantages, and you'll be hard-pressed to find one that everybody agrees is 'perfect'. But whatever format you choose, there is probably a Go package that you can use to automatically parse values from the file into a  struct for you.</p><p>For example, let's say that you want to use TOML and have a configuration file that looks like this:</p><figure><code><pre># Server configuration\nport = 4000\nverbose_logging = true\nrequest_timeout = \"10s\"\n\n# Basic authentication settings\n[basic_auth]\nusername = \"admin\"\npassword = \"secr3tPa55word\"\n</pre></code></figure><p>You can use the <a href=\"https://github.com/BurntSushi/toml\"></a> package to read the file and unpack the contents to a  struct like so:</p><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"time\"\n\n    \"github.com/BurntSushi/toml\"\n)\n\n// Make sure the struct fields are exported, so that the BurntSushi/toml package\n// can write to them, and use struct tags to map the TOML key/value pairs to the\n// appropriate struct field.\ntype config struct {\n    Port           int           `toml:\"port\"`\n    VerboseLogging bool          `toml:\"verbose_logging\"`\n    RequestTimeout time.Duration `toml:\"request_timeout\"`\n    BasicAuth      struct {\n        Username string `toml:\"username\"`\n        Password string `toml:\"password\"`\n    } `toml:\"basic_auth\"`\n}\n\nfunc main() {\n    var cfg config\n\n    // Load configuration settings from the config.toml file.\n    metadata, err := toml.DecodeFile(\"config.toml\", &amp;cfg)\n    if err != nil {\n        log.Fatalf(\"error loading configuration: %v\", err)\n    }\n\n    // Check for any undecoded keys in the config.toml file.\n    if len(metadata.Undecoded()) &gt; 0 {\n        log.Fatalf(\"unknown configuration keys: %v\", metadata.Undecoded())\n    }\n\n    fmt.Printf(\"Port: %d\\n\", cfg.Port)\n    fmt.Printf(\"Verbose Logging: %t\\n\", cfg.VerboseLogging)\n    fmt.Printf(\"Request Timeout: %v\\n\", cfg.RequestTimeout)\n    fmt.Printf(\"Basic Auth Username: %s\\n\", cfg.BasicAuth.Username)\n    fmt.Printf(\"Basic Auth Password: %s\\n\", cfg.BasicAuth.Password)\n}\n</pre></code></figure><p>Notice that in this code we're making use of the metadata returned by the  function to check if any settings were not decoded successfully ‚Äî which should help to catch typos or invalid keys in the TOML file.</p><h2>Passing settings to where they are needed</h2><p>Getting the configuration settings into the  struct, wherever they come from, is the first half of the puzzle. The second part is getting those settings to where you need them in your Go code. There are many different ways to approach this, and no single 'right' way. </p><p>For small or medium sized web applications, I often use a pattern of creating an  struct which contains all the dependencies that my HTTP handlers need, and I implement the handlers as methods on the  struct. To make the configuration settings available to the HTTP handlers, I simply include the  struct as a field in . </p><figure><code><pre>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"log/slog\"\n    \"net/http\"\n    \"os\"\n    \"time\"\n)\n\ntype config struct {\n    port           int\n    verboseLogging bool\n    requestTimeout time.Duration\n    basicAuth      struct {\n        username string\n        password string\n    }\n}\n\n// The application struct contains the dependencies for the handlers, including \n// the config struct\ntype application struct {\n    config config\n    logger *slog.Logger\n}\n\nfunc main() {\n    logger := slog.New(slog.NewTextHandler(os.Stdout, nil))\n\n    var cfg config\n    flag.IntVar(&amp;cfg.port, \"port\", 4000, \"The port number the web application listens on\")\n    flag.BoolVar(&amp;cfg.verboseLogging, \"verbose-logging\", false, \"Enables detailed request and error logging\")\n    flag.DurationVar(&amp;cfg.requestTimeout, \"request-timeout\", 5*time.Second, \"Maximum duration to wait for a request to complete\")\n    flag.StringVar(&amp;cfg.basicAuth.username, \"basic-auth-username\", \"\", \"Username required for HTTP Basic Authentication\")\n    flag.StringVar(&amp;cfg.basicAuth.password, \"basic-auth-password\", \"\", \"Password required for HTTP Basic Authentication\")\n    flag.Parse()\n\n    app := &amp;application{\n        config: cfg,\n        logger: logger,\n    }\n\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/\", app.home)\n\n    // Use the port configuration setting\n    logger.Info(\"starting server\", \"port\", cfg.port)\n\n    err := http.ListenAndServe(fmt.Sprintf(\":%d\", cfg.port), mux)\n    if err != nil {\n        logger.Error(err.Error())\n        os.Exit(1)\n    }\n}\n\nfunc (app *application) home(w http.ResponseWriter, r *http.Request) {\n    // Use the verboseLogging configuration setting\n    if app.config.verboseLogging {\n        app.logger.Info(\"handling request\", \"method\", r.Method, \"path\", r.URL.Path)\n    }\n\n    fmt.Fprintf(w, \"Hello!\")\n}\n</pre></code></figure><p>If you run this application with the  flag, and make a HTTP request to , you should see the details of the request in the log output, similar to below ‚Äî demonstrating that the config setting is correctly available to the handler.</p><figure><code><pre>$ go run main.go -verbose-logging\n<samp>time=2025-06-27T14:15:40.230+02:00 level=INFO msg=\"starting server\" port=4000\ntime=2025-06-27T14:15:48.705+02:00 level=INFO msg=\"handling request\" method=GET path=/</samp></pre></code></figure><p>In larger applications where I want to define my handlers outside of , or pass the config struct to functions in other packages, I normally define an exported  struct in an  package, and pass this around as necessary. For example, let's say that you have a project structure like so:</p><figure><code><pre>‚îú‚îÄ‚îÄ go.mod\n‚îú‚îÄ‚îÄ go.sum\n‚îú‚îÄ‚îÄ main.go\n‚îî‚îÄ‚îÄ internal\n    ‚îú‚îÄ‚îÄ config\n    ‚îÇ   ‚îî‚îÄ‚îÄ config.go\n    ‚îî‚îÄ‚îÄ handlers\n        ‚îî‚îÄ‚îÄ home.go\n</pre></code></figure><p>Then the contents of those  files would look something like this:</p><figure><figcaption>File: internal/config/config.go</figcaption><code><pre> \npackage config\n\nimport \"time\"\n\ntype Config struct {\n    Port           int\n    VerboseLogging bool\n    RequestTimeout time.Duration\n    BasicAuth      struct {\n        Username string\n        Password string\n    }\n}\n</pre></code></figure><figure><figcaption>File: internal/handlers/home.go</figcaption><code><pre>package handlers\n\nimport (\n    \"fmt\"\n    \"log/slog\"\n    \"net/http\"\n\n    \"your-project/internal/config\"\n)\n\nfunc Home(cfg config.Config, logger *slog.Logger) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        if cfg.VerboseLogging {\n            logger.Info(\"handling request\", \"method\", r.Method, \"path\", r.URL.Path)\n        }\n\n        fmt.Fprintf(w, \"Hello!\")\n    }\n}\n</pre></code></figure><figure><code><pre>package main\n\nimport (\n    \"flag\"\n    \"fmt\"\n    \"log/slog\"\n    \"net/http\"\n    \"os\"\n    \"time\"\n\n    \"your-project/internal/config\"\n    \"your-project/internal/handlers\"\n)\n\nfunc main() {\n    logger := slog.New(slog.NewTextHandler(os.Stdout, nil))\n\n    var cfg config.Config\n    flag.IntVar(&amp;cfg.Port, \"port\", 4000, \"The port number the web application listens on\")\n    flag.BoolVar(&amp;cfg.VerboseLogging, \"verbose-logging\", false, \"Enables detailed request and error logging\")\n    flag.DurationVar(&amp;cfg.RequestTimeout, \"request-timeout\", 5*time.Second, \"Maximum duration to wait for a request to complete\")\n    flag.StringVar(&amp;cfg.BasicAuth.Username, \"basic-auth-username\", \"\", \"Username required for HTTP Basic Authentication\")\n    flag.StringVar(&amp;cfg.BasicAuth.Password, \"basic-auth-password\", \"\", \"Password required for HTTP Basic Authentication\")\n    flag.Parse()\n\n    mux := http.NewServeMux()\n    mux.HandleFunc(\"/\", handlers.Home(cfg, logger))\n\n    // Use the port configuration setting\n    logger.Info(\"starting server\", \"port\", cfg.Port)\n\n    err := http.ListenAndServe(fmt.Sprintf(\":%d\", cfg.Port), mux)\n    if err != nil {\n        logger.Error(err.Error())\n        os.Exit(1)\n    }\n}\n</pre></code></figure><p>Obviously I'm using command-line flags in these examples, but the same patterns work for environment variables or config files too ‚Äî once the  struct is loaded with the data, it doesn't matter where it originally came from and the code patterns are the same.</p><p>If you've been in the web development world for a long time and buy into the <a href=\"https://12factor.net/config\">12-factor app principles</a> (which I generally do), you might think that the correct approach is <em>\"just use environment variables\"</em>. But over the years I've come to the conclusion that they have some drawbacks:</p><ul><li>I've been bitten more times than I want by bugs that were ultimately a result of an unset or unexpected value in an environment variable ‚Äî and I think that part of the problem here is that environment variables aren't <em>readily and easily observable</em> in the same way that the values in command-line flags or a configuration file are. </li><li>If you're working on multiple projects on the same development machine (rather than working in separate containers for each project), you have to manage the lack of natural isolation between environment variables... you need to make sure that there aren't any naming clashes, and that (for example) application A isn't accidentally using the  setting intended for application B.</li><li>I've also seen a lot of Go codebases where configuration settings are read in using <em>at the point in the code where they are needed</em>. This makes discoverability difficult ‚Äî it's hard to look at an application's code and easily see <em>what the expected configuration settings are</em>. </li></ul><p>You can mitigate these issues with some of the techniques that we've discussed in this tutorial. If you're strict about reading all the settings into a single  struct at application startup, that addresses the discoverability problem. If you create helpers like  which panic if an environment variable isn't set, that helps to eliminate bugs that exist due to missing environment variables. And you can work around some of the environment variable isolation problems in development by using a  file ‚Äî but at that point, it might be worth considering whether a configuration file might be more appropriate.</p><p>One of the big reasons that I like to use command-line flags is that you get a lot of stuff for free. You get automatic  text, automatic type conversions, the ability to set defaults, and it handles invalid inputs and undefined flags nicely. Also, it's always very clear what configuration values are being used ‚Äî you either explicitly pass the values when starting the application, or the default values hardcoded into your Go codebase are used. On top of that, most other gophers will be familiar with the  package and you don't need any third-party dependencies.</p><p>When I'm using command-line flags, I typically set the default values to things that are appropriate for a development environment. This is mainly so I don't have to keep typing long commands to run the application when actively developing it.</p><p>In terms of application secrets, like I <a href=\"https://www.alexedwards.net/blog/how-to-manage-configuration-settings-in-go-web-applications#using-environment-variables\">mentioned earlier</a>, there's nothing stopping you from storing a specific secret in an environment variable and using it in conjunction with a command-line flag if you want. For example, if you store a password for your database user in a  environment variable, you can include it as a command-line flag value when starting the application like so:</p><figure><code><pre>$ go run main.go -db-user=web -db-password=$DB_PASSWORD\n</pre></code></figure><p>Or, although it is a bit more 'magical', you could even use the environment variable as the default value:</p><figure><code><pre>flag.StringVar(&amp;cfg.db.password, \"db-password\", os.Getenv(\"DB_PASSWORD\"), \"Database user password\")\n</pre></code></figure><p>So, for all these reasons, I tend to prefer using command-line flags for configuration. The big exception to this is when there are  of configuration settings, and it would be awkward to pass them all via command-line flags, or the settings have a deeply nested 'structure' to them. In these cases, I think it can be more practical and maintainable to store the settings in a TOML or JSON configuration file, and load them on application startup like we demonstrated earlier.</p>","contentLength":26965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1loceah/how_to_manage_configuration_settings_in_go_web/"},{"title":"Built a geospatial game in Go using PostGIS where you plant seeds at real locations","url":"https://www.reddit.com/r/golang/comments/1loboyo/built_a_geospatial_game_in_go_using_postgis_where/","date":1751300091,"author":"/u/SoaringSignificant","guid":176781,"unread":true,"content":"<p>So I built this thing where you plant virtual seeds at real GPS locations and have to go back to water them or they die. Sounds dumb but I had fun making it and it's kinda fun to use.</p><p>Like you plant a seed at your gym, and if you don't go back within a few days your plant starts losing health. I've got a bunch of plants that I'm trying to get to level 10.</p><p>Built the main logic in Go, TypeScript + React for the frontend, and PostgreSQL with PostGIS for all the geospatial queries, though a bunch of that stuff happens in the service layer too. The geospatial stuff was interesting to work out, I ended up implementing plants and soils as circles since it makes the overlap detection and containment math way simpler. Figuring out when a plant fits inside a soil area or when two plants would overlap becomes basic circle geometry instead of dealing with complex polygons.</p><p>Plants decay every 4 hours unless you water them recently (there's a grace period system). Got a bunch of other mechanics like different soil types and plant tempers that are not fully integrated into the project right now. Just wanted to get the core loop working first and see how people actually use it.</p><p>You just need to get within like 10 meters of your plant to water it, but I'm still playing with these values to see what ends up being a good fit. Used to have it at 5 metres before but it made development a pain. The browser's geolocation api is so unreliable that I'd avoid it in future projects.</p><p>Been using it during development and it's actually getting me to go places more regularly but my plant graveyard is embarrassingly large though.</p><p>Here's a link to the repo and the live site for anyone interested in trying it out: <a href=\"https://github.com/jasonuc/moota\">GitHub</a> | <a href=\"https://moota.app\">Live Site</a></p>","contentLength":1722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Text Classification: Is Golang Viable or Should I Use Pytho","url":"https://www.reddit.com/r/golang/comments/1lobi9d/exploring_text_classification_is_golang_viable_or/","date":1751299659,"author":"/u/Fit_Honeydew4256","guid":178630,"unread":true,"content":"<p>Hi everyone, I‚Äôm still in the early stages of exploring a project idea where I want to classify text into two categories based on writing patterns. I haven‚Äôt started building anything yet ‚Äî just researching the best tools and approaches.</p><p>Since I‚Äôm more comfortable with Go (Golang), I‚Äôm wondering:</p><p>Is it practical to build or run any kind of text classification model using Go?</p><p>Has anyone used Go libraries like Gorgonia, goml, or onnx-go for something similar?</p><p>Would it make more sense to train the model in Python and then call it from a Go backend (via REST or gRPC)?</p><p>Are there any good examples or tutorials that show this kind of hybrid setup?</p><p>I‚Äôd appreciate any tips, repo links, or general advice from folks who‚Äôve mixed Go with ML. Just trying to figure out the right path before diving in.</p>","contentLength":806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made a functional 8-bit adder/subtractor circuit that works natively within MS Paint","url":"https://github.com/RRTogunov/MSPaintComputer","date":1751299598,"author":"/u/jkjkjij22","guid":176884,"unread":true,"content":"<p>I built all logic gates using the bucket/fill tool. These were combined to make an 8-bit ripple-carry adder as well as an 8-bit adder/subtractor circuit. </p><ol><li>Define inputs A and B (white = 0, black = 1) using bucket fill.</li><li>To run the circuit/computation, use the colour picker and fill tool to cycle through a sequence of colour changes from the ‚ÄúBus‚Äù and ‚ÄúProbe‚Äù squares on the left and apply them to the circuit leads on the right.</li></ol><p>This is where my knowledge of computer science ends, and I'm not sure how far this could theoretically be taken. </p><p>There are a few quirks that make this particularly challenging. For example, all logical components of the circuit are single-use (i.e., at the end of the computation, the entire circuit is black/white, and all the colour pixel logic is lost). Also, because this is in 2-dimensions it's not possible to cross/bridging/tunnel \"wires\" to make complex compound logic gates (XOR and XNOR). There's also a challenge with back-propagation, where colour fills don't just go forward down the circuit, but travel back and affect other parts of the circuit.<a href=\"https://www.reddit.com/r/mspaint/comments/1le7vqo/i_built_a_working_8bit_ripple_carry_adder_in_ms/\"></a></p>","contentLength":1095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lobham/i_made_a_functional_8bit_addersubtractor_circuit/"},{"title":"Don‚Äôt Be Ashamed to Say \"I Don‚Äôt Know\"","url":"https://www.thecoder.cafe/p/i-dont-know","date":1751298911,"author":"/u/teivah","guid":176816,"unread":true,"content":"<p><em>Hello! Today, let‚Äôs discuss the power of ‚ÄúI don‚Äôt know‚Äú with a personal story.</em></p><p>Last month, I was at the hospital with my partner for the birth of our newborn. During our stay, my partner experienced a specific symptom, and we wanted to understand what could be causing it. So we asked the midwife.</p><p>We immediately noticed the hesitation in her eyes. When she finally gave an answer, it came with a kind of forced confidence, and we both felt she wasn‚Äôt sure about it.</p><p>At our hospital, midwives do 12-hour shifts. So a few hours later, we asked the exact same question to the next midwife. Same hesitation, but this time, a different answer.</p><p>And so it went on. Every shift, we asked again. Every time, a different answer. Eventually, it even became a game between my partner and me: trying to guess what the next answer would be.</p><p>Until‚Ä¶ The one. The one who broke this cycle.</p><p>We asked her the same question. She paused. Thought about it. And then said something unexpected:</p><p>Twenty minutes later, she even came back to our room and said:</p><div><p>I asked the doctor, the answer is because of [X]. Thanks for asking, I learned something.</p></div><p>That brief exchange resonated with me.</p><p>In our field, we often put a lot of weight on posture. We build up our position as the go-to person for a codebase, a data model, or a framework. The more we know, the more we are seen as the one to consult or include in any related discussion.</p><p>But from that posture, admitting we don‚Äôt know something can feel like pulling out the bottom card in a house of cards. Suddenly, it feels like everything we built to earn that status might collapse.</p><p>Yet, if we take a step back, admitting we don‚Äôt know shouldn‚Äôt be seen as something shameful or embarrassing. In fact, it‚Äôs often the most responsible thing we can do.</p><ul><li><p><strong>Pretending to know can lead to bad decisions.</strong></p></li><li><p><strong>Authority isn‚Äôt built on knowing everything.</strong></p></li><li><p><strong>Teams work better when people feel safe to admit uncertainty.</strong></p></li><li><p><strong>Curiosity + humility = real learning. </strong></p></li></ul><p>Whether it‚Äôs for us or others, next time we don‚Äôt know something, let‚Äôs be like that midwife: let‚Äôs just admit it. Without shame.</p><p><em>How comfortable are you with saying ‚ÄúI don‚Äôt know‚Äú?</em></p><p><em>If you made it this far and enjoyed the post, please consider giving it a like.</em></p>","contentLength":2247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lob6fa/dont_be_ashamed_to_say_i_dont_know/"},{"title":"4 of the top 10 YouTube channels are AI-generated","url":"https://sherwood.news/tech/ai-created-videos-are-quietly-taking-over-youtube/","date":1751295343,"author":"/u/Alone-Competition-77","guid":176749,"unread":true,"content":"<p>YouTube has never stayed still for long.&nbsp;</p><p>The video powerhouse owned by  started its life as a place to upload home movies, only to use its early scale to morph into a user-generated MTV, birthing an entire universe of homegrown celebrities. It‚Äôs since evolved in two competing directions: incentivizing longer, more professional videos to compete with Netflix but also shorter, more ephemeral videos to compete with TikTok.</p><p>And it looks as if it‚Äôs changing again. The age of AI has finally arrived for YouTube, and it could be its most existential shift yet.</p><p>Over the past few months, Garbage Day has tracked how a range of AI-generated videos have found increasing success on the platform, taking up attention and space over more professional creators. More than a few of them seemingly rely on inauthentic engagement to boost their attention ‚Äî though, in a platform overrun with AI, it‚Äôs worth asking if that even matters.</p><p>At the same time, though, some of YouTube‚Äôs biggest success stories have shown less and less interest in the platform, focusing their efforts and promotion on places like TikTok and Instagram. All of this suggests a watershed moment for the internet‚Äôs biggest video site.&nbsp;</p><p>In May, four of the top 10 YouTube channels with the most subscribers featured AI-generated material in every video. Not all the channels are using the same AI programs, and there are indications that some contain human-made elements, but none of these channels has ever uploaded a video that was made entirely without AI ‚Äî and each has uploaded a constant stream of videos, which is crucial to their success.</p><p>While not all of the videos from these AI channels are identical, the most successful examples tend to find a theme and stick to it. Some, like ‚Äú<a href=\"https://www.youtube.com/@ChickofHonor-t3i/shorts\" target=\"_blank\" rel=\"noopener\">Chick of Honor</a>,‚Äù use tools like <a href=\"https://hailuoai.video/\" target=\"_blank\" rel=\"noopener\">Hailuo</a> for the instantly established format of <a href=\"https://www.bbc.com/future/article/20240819-why-these-ai-cat-videos-may-be-the-internets-future\" target=\"_blank\" rel=\"noopener\">cute animals</a> in visibly dangerous or tragic situations. Others, like ‚Äú<a href=\"https://www.youtube.com/@MastersOfProphecy\" target=\"_blank\" rel=\"noopener\">Masters of Prophecy</a>,‚Äù upload AI-generated music videos for AI-generated songs, made with <a href=\"https://suno.ai\" target=\"_blank\" rel=\"noopener\">Suno</a> to evoke ‚Äô80s synth nostalgia.</p><p>This is a profound change in how YouTube looked even just six months ago. In January, for instance, the most popular account making videos with AI got 2.5 million subscribers and 220 million views ‚Äî barely in the top 20 for the month. In June, the top four AI channels combined to get more than 23 million subscribers and 800 million views. The algorithm clearly favors AI channels now, enough that they‚Äôre getting a much better ratio of views to subscriptions.</p><p>This does make some sense. Generative-AI-driven channels do fix one core problem YouTube has struggled with ever since it started pushing more professional video content: not every creator has the time, resources, or skills to make Netflix-level content. Google‚Äôs ad revenue split doesn‚Äôt exactly pay for the costs that come with making your channel more professional, while AI videos all tend to be a certain standard and cost sometimes cents to produce.</p><p>Generating content like this allows YouTubers to keep up with increasingly high demand from the platform‚Äôs algorithms, especially where YouTube Shorts are concerned. As the definition of Shorts has <a href=\"https://sherwood.news/tech/does-youtube-have-a-future-if-its-creators-have-to-make-most-of-their-money/\" target=\"_blank\" rel=\"noopener\">changed constantly</a> to keep up with competitors like TikTok, YouTube has focused on their profitability much more than their appeal to creators. Starting in March, the platform <a href=\"https://support.google.com/youtube/thread/333869549/a-change-to-how-we-count-views-on-shorts?hl=en\" target=\"_blank\" rel=\"noopener\">changed the qualification</a> of viewing a Short from watching it for a few seconds to any time the video starts or loops.&nbsp;</p><p>According to <a href=\"https://digiday.com/media/youtube-shorts-view-count-update-wins-over-brands-but-creators-arent-sold/\" target=\"_blank\" rel=\"noopener\">DigiDay</a>, this was done to make tracking engagement metrics for marketers easier, while individual creators still see the same revenue. Meanwhile, Shorts has also become a testing ground for Google‚Äôs many AI tools, whether that‚Äôs <a href=\"https://lifehacker.com/tech/youtube-shorts-veo-ai-generator\" target=\"_blank\" rel=\"noopener\">making clips with Veo</a> or searching through them with Google Lens. Each popular AI-filled channel uploaded at least one Short in the month of May. Some, like ‚ÄúChick of Honor,‚Äù uploaded entirely Shorts rather than full videos; others just made shorter clips of their videos and streams.</p><p>This all feeds into YouTube‚Äôs aspirations for TV domination, as well. A <a href=\"https://techcrunch.com/2025/05/27/youtube-tops-disney-and-netflix-in-tv-viewing-nielsen-finds/\" target=\"_blank\" rel=\"noopener\">report from Nielsen</a> last month shows that it‚Äôs maintained the highest share of all TV viewing for several months straight, and it‚Äôs been the <a href=\"https://techcrunch.com/2024/02/20/youtube-dominates-tv-streaming-in-u-s-per-nielsens-latest-report/\" target=\"_blank\" rel=\"noopener\">top streaming service</a> for more than a year. This enormous share is reflected in its profits, as <a href=\"https://www.hollywoodreporter.com/business/business-news/youtube-value-revenue-1236176556/\" target=\"_blank\" rel=\"noopener\">recent estimates</a> say the platform is set to surpass Disney as the world‚Äôs most profitable media company. YouTube has tried to present itself as a competitor to streaming services for years, <a href=\"https://adage.com/article/special-report-newfronts/youtube-puts-shorts-center-newfronts-pitch/2491851/\" target=\"_blank\" rel=\"noopener\">showcasing users</a> like Alan Chikin Chow, whose YouTube videos are made in a <a href=\"https://variety.com/2024/digital/news/alan-chikin-chow-production-space-los-angeles-1236218119/\" target=\"_blank\" rel=\"noopener\">massive production studio</a>. But AI-generated music channels can play just as easily on a TV without any of the production costs.</p><p>Masters of Prophecy is currently the fastest-growing channel across all of YouTube, going from a few hundred subscribers in February to over 30 million in June, and all of its content is AI-generated. But that growth looks suspicious, especially looking at how it began by going from less than 300 subscribers to more than 100,000 in a single day without any new videos, Shorts, or comments in that time. But again, on a platform increasingly powered ‚Äî and populated ‚Äî by AI, what does inauthentic growth even mean? At the end of the day, an AI bot isn‚Äôt going to buy a product from an advertiser.</p><p>It‚Äôs clear that YouTube doesn‚Äôt want to answer that question. In many ways, it feels like it‚Äôs hoping no one notices how popular AI content is. With a quiet misdirect, and if users (and more importantly, advertisers) don‚Äôt complain, everyone will just keep making money.&nbsp;</p><p>But it also means that at some point in the near future, you‚Äôll open up the app and suddenly realize there aren‚Äôt any humans on it anymore.</p><p><a href=\"https://www.garbageday.email/\" target=\"_blank\" rel=\"noopener\"></a><i> is an award-winning newsletter that focuses on web culture and technology, covering a mix of memes, trends, and internet drama. We also run a program called Garbage Intelligence, a monthly report tracking the rise and fall of creators and accounts across every major platform on the web. We‚Äôll be sharing some of our findings here on Sherwood News. </i><a href=\"https://www.garbageday.email/subscribe\" target=\"_blank\" rel=\"noopener\"><i>You can subscribe to Garbage Day here.</i></a></p>","contentLength":6126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lo9mqg/4_of_the_top_10_youtube_channels_are_aigenerated/"},{"title":"Writing Toy Programs is a great way to remember why you started programming","url":"https://blog.jsbarretto.com/post/software-is-joy","date":1751295103,"author":"/u/Tech_User_Station","guid":176747,"unread":true,"content":"<p>I am a huge fan of Richard Feyman‚Äôs famous quote:</p><blockquote><p>‚ÄúWhat I cannot create, I do not understand‚Äù</p></blockquote><p>I think it‚Äôs brilliant, and it remains true across many fields (if you‚Äôre willing to be a little creative with the\ndefinition of ‚Äòcreate‚Äô). It is to this principle that I believe I owe everything I‚Äôm truly good at. Some will tell you\nto avoid reinventing the wheel, but they‚Äôre wrong: you  build your own wheel, because it‚Äôll teach you more about\nhow they work than reading a thousand books on them ever will.</p><p>In 2025, the beauty and craft of writing software is being eroded. AI is threatening to replace us (or, at least, the\nmost joyful aspects of our craft) and software development is being increasingly commodified, measured, packaged, and\nindustrialised. Software development needs more simple joy, and I‚Äôve found that creating toy programs is a great way to\nremember why I started working with computers again.</p><p>Toy programs follow the 80:20 rule: 20% of the work, 80% of the functionality. The point is  to build\nproduction-worthy software (although it is true that some of the best production software began life as a toy).\nAggressively avoid over-engineering, restrict yourself to only whatever code is necessary to achieve your goal. Have\nevery code path panic/crash until you‚Äôre forced to implement it to make progress. You might be surprised by just how\neasy it is to build toy versions of software you might previously have considered to be insummountably difficult to\ncreate.</p><p>I‚Äôve been consistently surprised by just how often some arcane nugget of knowledge I‚Äôve acquired when working on a toy\nproject has turned out to be immensely valuable in my day job, either by giving me a head-start on tracking down a\nproblem in a tool or library, or by recognising mistakes before they‚Äôre made.</p><p>Understanding the constraints that define the shape of software is vital for working with it, and there‚Äôs no better way\nto gain insight into those constraints than by running into them head-first. You might even come up with some novel\nsolutions!</p><p>Here is a list of toy programs I‚Äôve attempted over the past 15 years, rated by difficulty and time required. These\nratings are estimates and assume that you‚Äôre already comfortable with at least one general-purpose programming language\nand that, like me, you tend to only have an hour or two per day free to write code. Also included are some suggested\nresources that I found useful.</p><h3>Regex engine (difficulty = 4/10, time = 5 days)</h3><p>A regex engine that can read a POSIX-style regex program and recognise strings that match it. Regex is simple yet\nshockingly expressive, and writing a competent regex engine will teach you everything you need to know about using the\nlanguage too.</p><h3>x86 OS kernel (difficulty = 7/10, time = 2 months)</h3><p>A multiboot-compatible OS kernel with a simple CLI, keyboard/mouse driver, ANSI escape sequence support, memory manager,\nscheduler, etc. Additional challenges include writing an in-memory filesystem, user mode and process isolation, loading\nELF executables, and supporting enough video hardware to render a GUI.</p><h3>GameBoy/NES emulator (difficulty = 6/10, time = 3 weeks)</h3><p>A crude emulator for the simplest GameBoy or NES games. The GB and the NES are classics, and both have relatively simple\ninstruction sets and peripheral hardware. Additional challenges include writing competent PPU (video) and PSG (audio)\nimplementations, along with dealing with some of the more exotic cartridge formats.</p><h3>GameBoy Advance game (difficulty = 3/10, time = 2 weeks)</h3><p>A sprite-based game (top-down or side-on platform). The GBA is a beautiful little console to write code for and there‚Äôs\nan active and dedicated development community for the console. I truly believe that the GBA is one of the last game\nconsoles that can be fully and completely understood by a single developer, right down to instruction timings.</p><h3>Physics engine (difficulty = 5/10, time = 1 week)</h3><p>A 2D rigid body physics engine that implements Newtonian physics with support for rectangles, circles, etc. On the\nsimplest end, just spheres that push away from one-another is quite simple to implement. Things start to get complex\nwhen you introduce more complex shapes, angular momentum, and the like. Additional challenges include making collision\nresolution fast and scaleable, having complex interactions move toward a steady state over time, soft-body interactions,\netc.</p><h3>Dynamic interpreter (difficulty = 4/10, time = 1-2 weeks)</h3><p>A tree-walking interpreter for a JavaScript-like language with basic flow control. There‚Äôs an unbounded list of extra\nthings to add to this one, but being able to write programs in my own language still gives me child-like elation. It\nfeels like a sort of techno-genesis: once you‚Äôve got your own language, you can start building the universe within it.</p><h3>Compiler for a C-like (difficulty = 8/10, time = 3 months)</h3><p>A compiler for a simply-typed C-like programming language with support for at least one target archtecture. Extra\nchallenges include implementing some of the most common optimisations (inlining, const folding, loop-invariant code\nmotion, etc.) and designing an intermediate representation (IR) that‚Äôs general enough to support multiple backends.</p><h3>Text editor (difficulty = 5/10, time = 2-4 weeks)</h3><p>This one has a lot of variability. At the blunt end, simply reading and writing a file can be done in a few lines of\nPython. But building something that‚Äôs closer to a daily driver gets more complex. You could choose to implement the UI\nusing a toolkit like QT or GTK, but I personally favour an editor that works in the console. Properly handling unicode,\nsyntax highlighting, cursor movement, multi-buffer support, panes/windows, tabs, search/find functionality, LSP support,\netc. can all add between a week or a month to the project. But if you persist, you might join the elite company of those\ndevelopers who use an editor of their own creation.</p><h3>Async runtime (difficulty = 6/10, time = 1 week)</h3><p>There‚Äôs a lot of language-specific variability as to what ‚Äòasync‚Äô actually means. In Rust, at least, this means a\nlibrary that can ingest  tasks and poll them concurrently until completion. Adding support for I/O waking\nmakes for a fun challenge.</p><h3>Hash map (difficulty = 4/10, time = 3-5 days)</h3><p>Hash maps (or sets/dictionaries, as a higher-level language might call them) are a programmer‚Äôs bread &amp; butter. And yet,\nsurprisingly few of us understand how they really work under the bonnet. There are a plethora of techniques to throw\ninto the mix too: closed or open addressing, tombstones, the robin hood rule, etc. You‚Äôll gain an appreciation for when\nand why they‚Äôre fast, and also when you should just use a vector + linear search.</p><h3>Rasteriser / texture-mapper (difficulty = 6/10, time = 2 weeks)</h3><p>Most of us have played with simple 3D graphics at some point, but how many of us truly understand how the graphics\npipeline works and, more to the point, how to fix it when it doesn‚Äôt work? Writing your own software rasteriser will\ngive you that knowledge, along with a new-found appreciation for the beauty of vector maths and half-spaces that have\napplications across many other fields. Additional complexity involves properly implementing clipping, a Z-buffer, N-gon\nrasterisation, perspective-correct texture-mapping, Phong or Gouraud shading, shadow-mapping, etc.</p><h3>SDF Rendering (difficulty = 5/10, time = 3 days)</h3><p>Signed Distance Fields are a beautifully simple way to render 3D spaces defined through mathematics, and are perfectly\nsuited to demoscene shaders. With relatively little work you can build yourself a cute little visualisation or some\nmoving shapes like the graphics demos of the 80s. You‚Äôll also gain an appreciation for shader languages and vector\nmaths.</p><h3>Voxel engine (difficulty = 5/10, time = 2 weeks)</h3><p>I doubt there are many reading this that haven‚Äôt played Minecraft. It‚Äôs surprisingly easy to build your own toy voxel\nengine cut from a similar cloth, especially if you‚Äôve got some knowledge of 3D graphics or game development already. The\nsimplicity of a voxel engine, combined with the near-limitless creativity that can be expressed with them, never ceases\nto fill me with joy. Additional complexity can be added by tackling textures, more complex procedural generation,\nfloodfill lighting, collisions, dynamic fluids, sending voxel data over the network, etc.</p><h3>Threaded Virtual Machine (difficulty = 6/10, time = 1 week)</h3><p>Writing interpreters is great fun. What‚Äôs more fun? . If you keep pushing interpreters as far as\nthey can go without doing architecture-specific codegen (like AOT or JIT), you‚Äôll eventually wind up (re)discovering\n (not to be confused with multi-threading, which is a very different beast). It‚Äôs a beautiful way of\nweaving programs together out highly-optimised miniature programs, and a decent implementation can even give an AOT\ncompiler a run for its money in the performance department.</p><h3>GUI Toolkit (difficulty = 6/10, time = 2-3 weeks)</h3><p>Most of us have probably cobbled together a GUI program using tkinter, GTK, QT, or WinForms. But why not try writing\nyour GUI toolkit? Additional complexity involves implementing a competent layout engine, good text shaping (inc.\nunicode support), accessibility support, and more. Fair warning: do not encourage people to use your tool unless it‚Äôs\n - the world has enough GUIs with little-to-no accessibility or localisation support.</p><h3>Orbital Mechanics Sim (difficulty = 6/10, time = 1 week)</h3><p>A simple simulation of Newtonian gravity can be cobbled together in a fairly short time. Infamously, gravitational\nsystems with more than two bodies cannot be solved analytically, so you‚Äôll have to get familiar with iterative\n methods. Additional complexity comes with implementing more precise and faster integration methods,\naccounting for relativistic effects, and writing a visualiser. If you‚Äôve got the maths right, you can even try plugging\nin real numbers from NASA to predict the next high tide or full moon.</p><h3>Bitwise Challenge (difficulty = 3/10, time = 2-3 days)</h3><p>Here‚Äôs one I came up with for myself, but I think it would make for a great game jam: write a game that only persists 64\nbits of state between subsequent frames. That‚Äôs 64 bits for everything: the entire frame-for-frame game state should be\nreproducible using only 64 bits of data. It sounds simple, but it forces you to get incredibly creative with your game\nstate management. Details about the rules can be found on the GitHub page below.</p><h3>An ECS Framework (difficulty = 4/10, time = 1-2 weeks)</h3><p>For all those game devs out there: try building your own <a href=\"https://en.wikipedia.org/wiki/Entity_component_system\">ECS</a>\nframework. It‚Äôs not as hard as you might think (you might have accidentally done it already!). Extra points if you can\nbuild in safety and correctness features, as well as good integration with your programming language of choice‚Äôs type\nsystem features.</p><p>I built a custom ECS for my <a href=\"https://www.youtube.com/watch?v=nS5rj80L-pk\">Super Mario 64 on the GBA</a> project due to the\nunique performance and memory constraints of the platform, and enjoyed it a lot.</p><h3>CHIP-8 Emulator (difficulty = 3/10, time = 3-6 days)</h3><p>The <a href=\"https://en.wikipedia.org/wiki/CHIP-8\">CHIP-8</a> is a beautifully simple virtual machine from the 70s. You can write\na fully compliant emulator in a day or two, and there are an enormous plethora of fan-made games that run on it.\n<a href=\"https://github.com/zesterer/emul8/raw/refs/heads/master/test/test.ch8\">Here‚Äôs</a> a game I made for it.</p><h3>Chess engine (difficulty = 5/10, time = 2-5 days)</h3><p>Writing a chess engine is great fun. You‚Äôll start off with every move it makes being illegal, but over time it‚Äôll get\nsmart and smarter. Experiencing a loss to your own chess engine really is a rite of passage, and it feels magical.</p><h3>POSIX shell (difficulty = 4/10, time = 3-5 days)</h3><p>We interact with shells every day, and building one will teach you can incredible amount about POSIX - how it works, and\nhow it doesn‚Äôt. A simple one can be built in a day, but compliance with an existing shell language will take time and\nteach you more than you ever wanted to know about its quirks.</p><h2>A note on learning and LLMs</h2><p>Perhaps you‚Äôre a user of LLMs. I get it, they‚Äôre neat tools. They‚Äôre useful for certain kinds of learning. But I might\nsuggest resisting the temptation to use them for projects like this. Knowledge is not supposed to be fed to you on a\nplate. If you want that sort of learning, read a book - the joy in building toy projects like this comes from an\nexploration of the unknown, without polluting one‚Äôs mind with an existing solution. If you‚Äôve been using LLMs for a\nwhile, this cold-turkey approach might even be painful at first, but persist. There is no joy without pain.</p><p>The runner‚Äôs high doesn‚Äôt come to those that take the bus.</p>","contentLength":12598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lo9j7g/writing_toy_programs_is_a_great_way_to_remember/"},{"title":"Won at a Hackathon","url":"https://www.reddit.com/r/linux/comments/1lo968z/won_at_a_hackathon/","date":1751294233,"author":"/u/MasterBach","guid":176700,"unread":true,"content":"<p>Internal corporate hackathon. Red hat guys were onsite for the duration of it. </p>","contentLength":79,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenTelemetry is Great, But Who the Hell is Going to Pay For It?","url":"https://www.adatosystems.com/2025/02/10/who-the-hell-is-going-to-pay-for-this/","date":1751291923,"author":"/u/finallyanonymous","guid":176696,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lo88h3/opentelemetry_is_great_but_who_the_hell_is_going/"},{"title":"I built a label-aware PostgreSQL proxy for Kubernetes ‚Äì supports TLS, pooling, dynamic service discovery (feedback + contributors welcome!)","url":"https://www.reddit.com/r/kubernetes/comments/1lo81ev/i_built_a_labelaware_postgresql_proxy_for/","date":1751291434,"author":"/u/dewelopercloud","guid":176653,"unread":true,"content":"<p>I've been working on a Kubernetes-native PostgreSQL proxy written in Go, built from scratch with a focus on dynamic routing, TLS encryption, and full integration with K8s labels.</p><ul><li>TLS termination with auto-generated certificates (via cert-manager)</li><li>Dynamic service discovery via Kubernetes labels</li><li>Deployment-based routing (usernames like )</li><li>Optional connection pooling support (e.g. PgBouncer)</li><li>Works with any PostgreSQL deployment (single, pooled, cluster)</li><li>Super lightweight (uses ~0.1-0.5 vCPU / 18-60MB RAM under load)</li></ul><p>This is currently production-tested in my own hosting platform. I'd love your feedback ‚Äî and if you're interested in contributing, the project could easily be extended to support MySQL or MongoDB next.</p><p>Looking forward to any ideas, improvements, or contributions üôå</p>","contentLength":779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Ansic; a blazing fast, proc macro - zero overhead way to style with ansi!","url":"https://www.reddit.com/r/rust/comments/1lo7q9e/introducing_ansic_a_blazing_fast_proc_macro_zero/","date":1751290629,"author":"/u/Pitiful-Run983","guid":178699,"unread":true,"content":"<p> the new crate solving the pain of building ansi styled applications with  and the magic of proc macros -- with üöÄ  runtime overhead and  support</p><p>Most ansi crates uses  syntax for styling and display types and calculates the ansi style at runtime, which for applications with alot of styles or optimizations isn't ideal. Other crates also don't have a clean reusable model for ansi strings and alot of weird chaining and storing methods are used for it to be used.</p><p>Ansic solves those problems with a clean and reusable proc macro which uses a clean and convenient DSL which outputs raw string literals at compile time for  runtime overhead.</p><p>The  macro is the foundation of ansic, in here you write all your DSL expressions to define a style and it spits out the raw &amp;str literal.</p><p>Ansic has two different types of expressions separated by a space for each; Styles and colors.</p><p>Colors: Colors are simply written with their names (like \"green\" and \"red) but every single color supports extra arguments prefixed or postfixed by writing the format:  where each argument is with a dot before the color. There are two arguments for colors:</p><ul></ul><p>(by default if you dont provide the bg argument to a color its treated as a foreground color)</p><p>so let's say you want to make a ansi style which is a bright red foreground, then you can write , and it will output the string literal for the ansi equivalent, and if you want a bright red foreground with a bright green background you can do <code>ansi!(br.red bg.br.green)</code>. We also support 24bit rgb with the color syntax .</p><p>We also have styles (they don't take arguments) with for example the  and  styles with <code>ansi!(br.red underline bg.green bold)</code> for example (bright red foreground underline green background and bold ansi).</p><p>Ansic also encourages a consistent and reusable and simple architecture for styling with const:</p><pre><code>use ansic::ansi; const ERROR: &amp;'static str = ansi!(red bold underline); const R: &amp;'static str = ansi!(reset); fn main() { println!(\"{ERROR}ERROR: something wrong happened!{R}\"); } </code></pre><p>This encourages a reuseable, elegant and very easy way to style which muss less overhead (contradicting other crates with much less readable and elegant styles of styling)</p><p>üõ†Ô∏è I built  because I was frustrated with other ansi crates:</p><p>It was hard to read, reuse and manage weirdly chained styles, and I hated that every time I used it there was a runtime calculation to make the styles even work. I love  because it solves all those problems with a clean reuseable model, DSL which is easy to read and maintain, and a proc macro which does everything at compile time.</p><p>I'm also 100% open to feedback, reviews, discussions and criticism!</p><p>üé® Add ansic to style with ansi with !</p><p>üåü If you like the project, please consider leaving a star on our <a href=\"https://github.com/zeonzip/ansic\">GitHub page</a>!</p>","contentLength":2772,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Says Its New AI System Diagnosed Patients 4 Times More Accurately Than Human Doctors","url":"https://www.wired.com/story/microsoft-medical-superintelligence-diagnosis/","date":1751290307,"author":"/u/wiredmagazine","guid":176657,"unread":true,"content":"<p> ‚Äúa genuine step toward medical superintelligence,‚Äù says Mustafa Suleyman, CEO of the company‚Äôs <a href=\"https://www.wired.com/tag/artificial-intelligence/\">artificial intelligence</a> arm. The tech giant says its powerful new AI tool can <a href=\"https://www.wired.com/story/ai-diagnose-illnesses-country-rich/\">diagnose disease</a> four times more accurately and at significantly less cost than a panel of human physicians.</p><p>The experiment tested whether the tool could correctly diagnose a patient with an ailment, mimicking work typically done by a human doctor.</p><p>The Microsoft team used 304 case studies sourced from the New England Journal of Medicine to devise a test called the Sequential Diagnosis Benchmark. A language model broke down each case into a step-by-step process that a doctor would perform in order to reach a diagnosis.</p><p>Microsoft‚Äôs researchers then built a system called the MAI Diagnostic Orchestrator (MAI-DxO) that queries several leading AI models‚Äîincluding OpenAI‚Äôs GPT, Google‚Äôs Gemini, Anthropic‚Äôs Claude, Meta‚Äôs Llama, and xAI‚Äôs Grok‚Äîin a way that loosely mimics several human experts working together.</p><p>In their experiment, MAI-DxO outperformed human doctors, achieving an accuracy of 80 percent compared to the doctors‚Äô 20 percent. It also reduced costs by 20 percent by selecting less expensive tests and procedures.</p><p>\"This orchestration mechanism‚Äîmultiple agents that work together in this chain-of-debate style‚Äîthat's what's going to drive us closer to medical superintelligence,‚Äù Suleyman says.</p><p>The company poached several Google AI researchers to help with the effort‚Äîyet another sign of <a href=\"https://www.wired.com/story/four-openai-researchers-leave-meta/\">an intensifying war for top AI expertise</a> in the tech industry. Suleyman was previously an executive at Google working on AI.</p><p>AI is already widely used in some parts of the US health care industry, including helping radiologists interpret scans. The latest multimodal AI models have the potential to act as more general diagnostic tools, though the use of AI in health care raises its own issues, particularly related to bias from training data that‚Äôs skewed toward particular demographics.</p><p>Microsoft has not yet decided if it will try to commercialize the technology, but the same executive, who spoke on the condition of anonymity, said the company could integrate it into Bing to help users diagnose ailments. The company could also develop tools to help medical experts improve or even automate patient care. ‚ÄúWhat you'll see over the next couple of years is us doing more and more work proving these systems out in the real world,‚Äù Suleyman says.</p><p>The project is the latest in a growing body of research showing how AI models can diagnose disease. In the last few years, both Microsoft and Google have published papers showing that large language models can accurately diagnose an ailment when given access to medical records.</p><p>The new Microsoft research differs from previous work in that it more accurately replicates the way human physicians diagnose disease‚Äîby analyzing symptoms, ordering tests, and performing further analysis until a diagnosis is reached. Microsoft describes the way that it combined several frontier AI models as ‚Äúa path to medical superintelligence‚Äù in a blog post about the project today.</p><p>The project also suggests that AI could help lower health care costs, a critical issue, particularly in the US. \"Our model performs incredibly well, both getting to the diagnosis and getting to that diagnosis very cost effectively,\" says Dominic King, a vice president at Microsoft who is involved with the project.</p>","contentLength":3445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lo7lv8/microsoft_says_its_new_ai_system_diagnosed/"},{"title":"Changing max pods limit in already established cluster - Microk8s","url":"https://www.reddit.com/r/kubernetes/comments/1lo7cxk/changing_max_pods_limit_in_already_established/","date":1751289647,"author":"/u/BunkerFrog","guid":176744,"unread":true,"content":"<p>Hi, I do have quite beefy setup. Cluster of 4x 32core/64thread with 512GB RAM. Nodes are bare metal. I used stock setup with stock config of microk8s and while there was no problem I had reached limit of 110 pods/node. There are still plenty of system resources to utilize - for now using like 30% of CPU and RAM / node.</p><p>Question #1: Can I change limit on already running cluster? (there are some posts on internet that this change can only be done during cluster/node setup and can't be changed later)</p><p>Question #2: If it is possible to change it on already established cluster, will it be possible to change it via \"master\" or need to be changed manually on each node</p><p>Question #3: What real max should I use to not make my life with networking harder? (honestly I would be happy if 200 would pass)</p>","contentLength":795,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rust-analyzer changelog #292","url":"https://rust-analyzer.github.io/thisweek/2025/06/30/changelog-292.html","date":1751289477,"author":"/u/WellMakeItSomehow","guid":176986,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lo7ap3/rustanalyzer_changelog_292/"},{"title":"Here is My 3D Pool Simulator for Linux","url":"https://github.com/sysrpl/Raylib.4.0.Pascal/blob/master/examples/table/README.md","date":1751286276,"author":"/u/sysrpl","guid":176659,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lo65wi/here_is_my_3d_pool_simulator_for_linux/"},{"title":"I'm getting an error after certificate renewal please help","url":"https://www.reddit.com/r/kubernetes/comments/1lo509h/im_getting_an_error_after_certificate_renewal/","date":1751282635,"author":"/u/Known_Wallaby_1821","guid":176568,"unread":true,"content":"<p>Hello, My Kubernetes cluster was running smoothly until I tried to renew the certificates after they expired. I ran the following commands:</p><blockquote><p>sudo kubeadm certs renew all</p><p>echo 'export KUBECONFIG=/etc/kubernetes/admin.conf' &gt;&gt; ~/.bashrc</p></blockquote><p>After that, some abnormalities started to appear in my cluster. Calico is completely down and even after deleting and reinstalling it, it does not come back up at all.</p><p>When I check the daemonsets and deployments in the kube-system namespace, I see:</p><blockquote><p>kubectl get daemonset -n kube-system</p><p>NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE</p><p>kubectl get deployments -n kube-system</p><p>NAME READY UP-TO-DATE AVAILABLE AGE</p><p>calico-kube-controllers 0/1 0 0 4m19s</p></blockquote><p>Before this, I was also getting \"unauthorized\" errors in the kubelet logs, which started after renewing the certificates. This is definitely abnormal because the pods created from deployments are not coming up and remain stuck.</p><p>There is no error message shown during deployment either. Please help.</p>","contentLength":986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"After nine years, Ninja has merged support for the GNU Make jobserver","url":"https://thebrokenrail.com/2025/06/30/ninja-jobserver.html","date":1751281215,"author":"/u/TheBrokenRail-Dev","guid":176654,"unread":true,"content":"<p>And I promise you: this is awesome!</p><p><a href=\"https://ninja-build.org/\">Ninja</a> is a build system like <a href=\"https://www.gnu.org/software/make/\">GNU Make</a>: you give it the list of files you want created (the outputs), how it should create them (the rules), and what each one‚Äôs dependencies are (the inputs). But you need to ensure it creates files in parallel when possible. It must not re-create files that already exist, but must re-create files when inputs change. Many other tiny annoying details need to be handled as well.</p><p>Let‚Äôs focus on one specific detail: parallelism. Almost all build systems run in parallel to maximize resource utilization and minimize execution time. Ninja specifically defaults to running one process per CPU thread. This works great most of the time!</p><p>Except‚Ä¶ what if one of those processes is another instance of Ninja? Now in the worst-case scenario (assuming an 8-core/16-thread CPU), you have the parent instance of Ninja managing 16 processes and another child instance managing 16 more threads! That adds up to 32 processes total on an eight-core CPU. You can see how this can quickly get out of hand and lead to massive resource over-utilization and even system freezes.</p><p>There are ways to work around this. For instance, you can disable parallelism on the child Ninja instance(s), but while that will fix over-utilization, it will lead to resource under-utilization and slower execution times. You can also try manually tweaking parallelism levels. However, that will cause inconsistent and inefficient behavior when using multiple build machines.</p><p>And this is a real problem affecting real projects. Features like <a href=\"https://cmake.org/cmake/help/latest/module/ExternalProject.html\">CMake‚Äôs </a> often lead to recursive Ninja calls. These recursive calls can easily cause the parallelism problem described above. For reference, see <a href=\"https://groups.google.com/g/ninja-build/c/AAuTlZp57f0\">these</a><a href=\"https://discourse.cmake.org/t/efficiency-issues-with-externalproject-sub-builds-under-ninja/3241\">discussion</a><a href=\"https://gitlab.kitware.com/cmake/cmake/-/issues/21597#note_1208049\">posts</a>.</p><p>But what if I told you this problem has already been solved‚Ä¶ back in <a href=\"https://cgit.git.savannah.gnu.org/cgit/make.git/commit/?id=fc0fe4103ac983d88b83dad0daf97664ffa8e04b\">1999</a>?</p><h2>Introducing: The GNU Make Jobserver</h2><p>This was created because GNU Make experienced the same problem as Ninja: recursive GNU Make calls could easily cause resource over-utilization and system freezes. Previously, this had been ‚Äúsolved‚Äù by disabling parallelism in child GNU Make calls, but as I mentioned earlier, this led to resource under-utilization and longer build times.</p><p>The jobserver was a proper solution. The parent GNU Make instance would act as the server. All child instances would be clients. Anytime a client wanted to launch a process, it would ask the server to tell it when it was allowed to. This allowed the server to ensure resources were not over- or under-utilized.</p><p>So, if GNU Make had this problem solved for such a long time, why did people experiencing it keep using Ninja? Well, unfortunately many tools like <a href=\"https://mesonbuild.com/\">Meson</a> did not support GNU Make. This meant developers still needed Ninja.</p><p>Over the years, this has led to a few soft forks of Ninja where the sole change was merging jobserver support (including <a href=\"https://github.com/Kitware/ninja\">one</a> from <a href=\"https://www.kitware.com/\">Kitware</a>, the company behind CMake).</p><p>Thankfully, as of this blog post, Ninja now supports the exact same tried-and-tested jobserver protocol. That means the problem is completely solved, right? ‚Ä¶Right?</p><p>As you might have guessed, there are multiple catches.</p><p>For one, Ninja only implements support for the jobserver . Without a corresponding server, it cannot actually do anything. This means you either need to run your Ninja instance inside an instance of GNU Make or <a href=\"https://github.com/ninja-build/ninja/blob/656412538b6fc102b809a61e0efce422e5a20534/misc/jobserver_pool.py\">some other server implementation</a>.</p><p>Another issue is that on Linux it only supports the <a href=\"https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html\">named pipe/FIFO implementation</a> of the jobserver. This was released with <a href=\"https://lists.gnu.org/archive/html/help-make/2022-10/msg00020.html\">GNU Make 4.4</a> back in October 2022, which was only added to <a href=\"https://wiki.debian.org/DebianTrixie\">Debian Trixie</a> just a few months ago, in <a href=\"https://metadata.ftp-master.debian.org/changelogs//main/m/make-dfsg/make-dfsg_4.4.1-2_changelog\">December 2024</a>. This means you will need an extremely recent OS. Otherwise, GNU Make‚Äôs jobserver will not be compatible with Ninja.</p><p>And finally, as of writing this post, <a href=\"https://github.com/ninja-build/ninja/releases/tag/v1.13.0\">Ninja v1.13.0</a> (the version containing jobserver support) is less than a week-old. It will take a while before it is included in major package repositories. Until then, you will probably have to compile it from source code or use third-party binaries.</p><p>While I might have been nitpicky, this is a major improvement. This will give many Ninja-based projects an immediate performance improvement with minimal required changes. I certainly cannot complain too much about that.</p>","contentLength":4215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lo4krm/after_nine_years_ninja_has_merged_support_for_the/"},{"title":"After nine years, Ninja has merged support for the GNU Make jobserver","url":"https://thebrokenrail.com/2025/06/30/ninja-jobserver.html","date":1751281187,"author":"/u/TheBrokenRail-Dev","guid":176605,"unread":true,"content":"<p>And I promise you: this is awesome!</p><p><a href=\"https://ninja-build.org/\">Ninja</a> is a build system like <a href=\"https://www.gnu.org/software/make/\">GNU Make</a>: you give it the list of files you want created (the outputs), how it should create them (the rules), and what each one‚Äôs dependencies are (the inputs). But you need to ensure it creates files in parallel when possible. It must not re-create files that already exist, but must re-create files when inputs change. Many other tiny annoying details need to be handled as well.</p><p>Let‚Äôs focus on one specific detail: parallelism. Almost all build systems run in parallel to maximize resource utilization and minimize execution time. Ninja specifically defaults to running one process per CPU thread. This works great most of the time!</p><p>Except‚Ä¶ what if one of those processes is another instance of Ninja? Now in the worst-case scenario (assuming an 8-core/16-thread CPU), you have the parent instance of Ninja managing 16 processes and another child instance managing 16 more threads! That adds up to 32 processes total on an eight-core CPU. You can see how this can quickly get out of hand and lead to massive resource over-utilization and even system freezes.</p><p>There are ways to work around this. For instance, you can disable parallelism on the child Ninja instance(s), but while that will fix over-utilization, it will lead to resource under-utilization and slower execution times. You can also try manually tweaking parallelism levels. However, that will cause inconsistent and inefficient behavior when using multiple build machines.</p><p>And this is a real problem affecting real projects. Features like <a href=\"https://cmake.org/cmake/help/latest/module/ExternalProject.html\">CMake‚Äôs </a> often lead to recursive Ninja calls. These recursive calls can easily cause the parallelism problem described above. For reference, see <a href=\"https://groups.google.com/g/ninja-build/c/AAuTlZp57f0\">these</a><a href=\"https://discourse.cmake.org/t/efficiency-issues-with-externalproject-sub-builds-under-ninja/3241\">discussion</a><a href=\"https://gitlab.kitware.com/cmake/cmake/-/issues/21597#note_1208049\">posts</a>.</p><p>But what if I told you this problem has already been solved‚Ä¶ back in <a href=\"https://cgit.git.savannah.gnu.org/cgit/make.git/commit/?id=fc0fe4103ac983d88b83dad0daf97664ffa8e04b\">1999</a>?</p><h2>Introducing: The GNU Make Jobserver</h2><p>This was created because GNU Make experienced the same problem as Ninja: recursive GNU Make calls could easily cause resource over-utilization and system freezes. Previously, this had been ‚Äúsolved‚Äù by disabling parallelism in child GNU Make calls, but as I mentioned earlier, this led to resource under-utilization and longer build times.</p><p>The jobserver was a proper solution. The parent GNU Make instance would act as the server. All child instances would be clients. Anytime a client wanted to launch a process, it would ask the server to tell it when it was allowed to. This allowed the server to ensure resources were not over- or under-utilized.</p><p>So, if GNU Make had this problem solved for such a long time, why did people experiencing it keep using Ninja? Well, unfortunately many tools like <a href=\"https://mesonbuild.com/\">Meson</a> did not support GNU Make. This meant developers still needed Ninja.</p><p>Over the years, this has led to a few soft forks of Ninja where the sole change was merging jobserver support (including <a href=\"https://github.com/Kitware/ninja\">one</a> from <a href=\"https://www.kitware.com/\">Kitware</a>, the company behind CMake).</p><p>Thankfully, as of this blog post, Ninja now supports the exact same tried-and-tested jobserver protocol. That means the problem is completely solved, right? ‚Ä¶Right?</p><p>As you might have guessed, there are multiple catches.</p><p>For one, Ninja only implements support for the jobserver . Without a corresponding server, it cannot actually do anything. This means you either need to run your Ninja instance inside an instance of GNU Make or <a href=\"https://github.com/ninja-build/ninja/blob/656412538b6fc102b809a61e0efce422e5a20534/misc/jobserver_pool.py\">some other server implementation</a>.</p><p>Another issue is that on Linux it only supports the <a href=\"https://www.gnu.org/software/make/manual/html_node/POSIX-Jobserver.html\">named pipe/FIFO implementation</a> of the jobserver. This was released with <a href=\"https://lists.gnu.org/archive/html/help-make/2022-10/msg00020.html\">GNU Make 4.4</a> back in October 2022, which was only added to <a href=\"https://wiki.debian.org/DebianTrixie\">Debian Trixie</a> just a few months ago, in <a href=\"https://metadata.ftp-master.debian.org/changelogs//main/m/make-dfsg/make-dfsg_4.4.1-2_changelog\">December 2024</a>. This means you will need an extremely recent OS. Otherwise, GNU Make‚Äôs jobserver will not be compatible with Ninja.</p><p>And finally, as of writing this post, <a href=\"https://github.com/ninja-build/ninja/releases/tag/v1.13.0\">Ninja v1.13.0</a> (the version containing jobserver support) is less than a week-old. It will take a while before it is included in major package repositories. Until then, you will probably have to compile it from source code or use third-party binaries.</p><p>While I might have been nitpicky, this is a major improvement. This will give many Ninja-based projects an immediate performance improvement with minimal required changes. I certainly cannot complain too much about that.</p>","contentLength":4215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lo4kev/after_nine_years_ninja_has_merged_support_for_the/"},{"title":"Result in C++","url":"https://github.com/Jarsop/cpp_result","date":1751280470,"author":"/u/Jarsop","guid":176745,"unread":true,"content":"<p>Rust developer since more than 8 years ago, I really annoyed when I use other languages without  API. In C++ we have  (since c++17) and  (since c++23) but I don‚Äôt think it‚Äôs really convenient. This how I decided to create , a more ergonomic API which try to mimic Rust  type. Macros are also provided to mimic the  operator. Any feedback is very welcomed.</p>","contentLength":359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lo4deu/result_in_c/"},{"title":"Bardcore Portfolio - Powered by Go","url":"https://www.reddit.com/r/golang/comments/1lo45qd/bardcore_portfolio_powered_by_go/","date":1751279692,"author":"/u/ArinjiBoi","guid":176658,"unread":true,"content":"<p>Hey Everyone! I just finished working on a portfolio site themed around \"bardcore\", its a site i made for my music friend to showcase her songs. I am using Pocketbase for the backend with a golang proxy to have the music stored in google drive be playable on the site</p>","contentLength":267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Evolution of Caching Libraries in Go","url":"https://maypok86.github.io/otter/blog/cache-evolution/","date":1751278826,"author":"/u/Ploobers","guid":176528,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lo3x9k/the_evolution_of_caching_libraries_in_go/"},{"title":"Exception handling in rustc_codegen_cranelift","url":"https://tweedegolf.nl/en/blog/157/exception-handling-in-rustc-codegen-cranelift","date":1751278609,"author":"/u/Expurple","guid":176746,"unread":true,"content":"<p>We will use the following example to illustrate the various cases that commonly occur:</p><pre><code>// For do_catch\n#![feature(rustc_attrs, core_intrinsics)]\n#![allow(internal_features)]\nstruct Droppable;\nimpl Drop for Droppable {\n    fn drop(&amp;mut self) {}\n}\n// Unwind without running any drops\n#[no_mangle]\nfn do_panic() {\n    std::panic::panic_any(());\n}\n// Unwind while running a drop on the cleanup path\n#[no_mangle]\nfn some_func() {\n    let _a = Droppable;\n    do_panic();\n}\n// Catch a panic\n#[no_mangle]\nfn do_catch_panic() {\n    // This has a simplified version of std::panic::catch_unwind inlined for ease of understanding\n    unsafe {\n        if std::intrinsics::catch_unwind(do_call, 0 as *mut _, do_catch) == 0 {\n            std::process::abort(); // unreachable\n        } else {\n            // Caught panic\n        };\n    }\n    #[inline]\n    fn do_call(_data: *mut u8) {\n        some_func();\n    }\n    #[inline]\n    #[rustc_nounwind] // `intrinsic::catch_unwind` requires catch fn to be nounwind\n    fn do_catch(_data: *mut u8, _panic_payload: *mut u8) {}\n}\nfn main() {\n    do_catch_panic();\n}\n</code></pre><p>Let's first compile this using a version of  with unwinding enabled:</p><pre><code>dist/rustc-clif panic_example.rs -Cdebuginfo=2 --emit link,mir,llvm-ir\n</code></pre><p>This command enables debuginfo, and emits three artifacts:  emits the normal executable,  emits MIR, and  is repurposed with  to emit Cranelift IR (clif ir for short). In any case with the executable now compiled, let's run it in a debugger:</p><pre><code>$ gdb ./panic_example\nReading symbols from ./panic_example...\n</code></pre><p>We begin by setting a breakpoint in :</p><pre><code>(gdb) break do_panic\nBreakpoint 1 at 0x38794: file panic_example.rs, line 13.\n</code></pre><pre><code>(gdb) run\nDownloading separate debug info for system-supplied DSO at 0xfffff7ffb000\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/aarch64-linux-gnu/libthread_db.so.1\".\nBreakpoint 1, panic_example::do_panic () at panic_example.rs:13\n13          std::panic::panic_any(());\n(gdb) backtrace\n#0  panic_example::do_panic () at panic_example.rs:13\n#1  0x0000aaaaaaad87b4 in panic_example::some_func () at panic_example.rs:20\n#2  0x0000aaaaaaad8868 in panic_example::do_catch_panic::do_call () at panic_example.rs:37\n#3  0x0000aaaaaaad8820 in panic_example::do_catch_panic () at panic_example.rs:28\n#4  0x0000aaaaaaad888c in panic_example::main () at panic_example.rs:46\n[...]\n</code></pre><pre><code>(gdb) break _Unwind_RaiseException\nBreakpoint 2 at 0xfffff7f975e8: file ../../../src/libgcc/unwind.inc, line 93\n(gdb) continue\nContinuing.\nthread 'main' panicked at panic_example.rs:13:5:\nBox&lt;dyn Any&gt;\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nBreakpoint 2, _Unwind_RaiseException (exc=0xaaaaaace1ce0) at ../../../src/libgcc/unwind.inc:93\nwarning: 93     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) bt\n#0  _Unwind_RaiseException (exc=0xaaaaaace1ce0) at ../../../src/libgcc/unwind.inc:93\n#1  0x0000aaaaaabde42c in panic_unwind::imp::panic () at library/panic_unwind/src/gcc.rs:72\n#2  0x0000aaaaaabdde3c in panic_unwind::__rust_start_panic () at library/panic_unwind/src/lib.rs:103\n#3  0x0000aaaaaaae977c in std::panicking::rust_panic () at library/std/src/panicking.rs:894\n#4  0x0000aaaaaaae959c in std::panicking::rust_panic_with_hook () at library/std/src/panicking.rs:858\n#5  0x0000aaaaaaad7bcc in std::panicking::begin_panic::{closure#0}&lt;()&gt; () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panicking.rs:770\n#6  0x0000aaaaaaad7b20 in std::sys::backtrace::__rust_end_short_backtrace&lt;std::panicking::begin_panic::{closure_env#0}&lt;()&gt;, !&gt; ()\n    at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/sys/backtrace.rs:168\n#7  0x0000aaaaaaad7b70 in std::panicking::begin_panic&lt;()&gt; () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panicking.rs:769\n#8  0x0000aaaaaaad7b4c in std::panic::panic_any&lt;()&gt; () at /home/gh-bjorn3/cg_clif/build/stdlib/library/std/src/panic.rs:260\n#9  0x0000aaaaaaad87a0 in panic_example::do_panic () at panic_example.rs:13\n#10 0x0000aaaaaaad87b4 in panic_example::some_func () at panic_example.rs:20\n#11 0x0000aaaaaaad8868 in panic_example::do_catch_panic::do_call () at panic_example.rs:37\n#12 0x0000aaaaaaad8820 in panic_example::do_catch_panic () at panic_example.rs:28\n#13 0x0000aaaaaaad888c in panic_example::main () at panic_example.rs:46\n[...]\n</code></pre><p>We can validate that the exception is in fact a Rust exception by running:</p><pre><code>(gdb) print exc\n$1 = (struct _Unwind_Exception *) 0xaaaaaace1ce0\n(gdb) print *exc\n$2 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 &lt;panic_unwind::imp::panic::exception_cleanup&gt;, private_1 = 0, private_2 = 0}\n(gdb) print (char[8])(exc.exception_class)\n$3 = \"MOZ\\000RUST\"\n</code></pre><p>That looks a lot like a Rust exception to me. The rest of the exception data is located directly after the  struct.</p><h2>Unwinding ABI crash course</h2><p>There are nowadays two major unwinder ABIs still in use for C++ exceptions and Rust panics. These are:</p><ul><li>SEH (Structured Exception Handling) on Windows</li><li>Itanium unwinding ABI (originating from the infamous Intel <a href=\"https://en.wikipedia.org/wiki/Itanium\">cpu architecture</a>) on most Unix systems.</li></ul><p>SEH and Itanium unwinding have a similar architecture: there is a table that indicates, for each instruction from which an exception may be thrown, which registers need to be restored to unwind the stack to the caller as well as contains a reference to a function (the so called personality function) which interprets a language-specific data format and a reference to some data in this format (called LSDA or language-specific data area for Itanium unwinding).</p><p>In most cases there is a single personality function for each language. Rust generally<a href=\"https://tweedegolf.nl/en/blog/157/exception-handling-in-rustc-codegen-cranelift#1\"></a> uses  as personality function.  For the LSDA, Rust uses the exact same format as GCC and Clang use for C++ despite not needing half its features because LLVM doesn't allow frontends to specify a custom format.</p><p>Both SEH and Itanium unwinding implement two-phase unwinding. In other words, they first do a scan over the stack to see if any function catches the exception (phase one) before actually unwinding (phase two). For this in the first phase SEH and Itanium unwinders call the personality function to check if there is a catch for the exception around the given call site. To do this, the personality function parses the LSDA looking up the entry for the current instruction pointer.</p><p>In the second phase the personality function is called again and this time it is given the chance to divert execution to an exception handler. In the case of SEH this exception handler is a so-called \"funclet\": a function which gets the stack pointer of the stack frame currently being unwound as argument, and unwinding resumes when this funclet returns.</p><p>For Itanium unwinding on the other hand, execution gets diverted to a \"landingpad\" which runs in the context of the stack frame being unwound. Unwinding either resumes when the landingpad calls  or in the case of a catch, the landingpad just continues execution as usual.</p><p>With the SEH method, all stack frames remain on the stack until unwinding has finished. It is also possible to unwind without removing any stack frames. Itanium unwinding instead removes each stack frame from the stack after it has been unwound, so effectively throwing an exception is an alternative return of the function. Cranelift currently only supports unwinding mechanisms that use landingpads, which is why cg_clif doesn't support unwinding on Windows.</p><p>While dwarfdump can be used to show part of the unwind info in a human-readable way, I'm not aware of any tool that is capable of showing the entire unwind info in a human-readable way: dwarfdump does not parse the LSDA, and there is no option to interleave assembly instructions and unwind instructions. As such I wrote my own tool for this, which I will use to show how exactly the unwinder sees our functions:</p><pre><code>$ git clone https://github.com/bjorn3/rust_unwind_inspect.git\n$ cd rust_unwind_inspect\n$ cargo build\n$ cp target/debug/rust_unwind_inspect ../\n</code></pre><h2>Unwinding without exception handlers</h2><p>Now on to showing how Itanium unwinding support is actually implemented in cg_clif. I'm going to skip ahead to the second phase of the unwinding process -- the actual unwinding -- for the sake of simplicity.</p><p>Let's start with the  function:</p><pre><code>//- panic_example.mir\n\n// [snip]\nfn do_panic() -&gt; () {\n    let mut _0: ();\n    let _1: !;\n\n    bb0: {\n        _1 = panic_any::&lt;()&gt;(const ()) -&gt; unwind continue;\n    }\n}\n// [snip]\n</code></pre><p>This is a simple function which consists of nothing other than a  call which never returns, and when it unwinds, it continues to the caller.</p><pre><code>;- panic_example.clif/do_panic.unopt.clif\nfunction u0:28() system_v {\n    gv0 = symbol colocated userextname0\n    sig0 = (i64) system_v\n    fn0 = colocated u0:6 sig0 ; Instance { def: Item(DefId(1:5518 ~ std[a023]::panic::panic_any)), args: [()] }\nblock0:\n    jump block1\nblock1:\n; _1 = std::panic::panic_any::&lt;()&gt;(const ())\n    v0 = global_value.i64 gv0\n    call fn0(v0)\n    trap user1\n}\n</code></pre><p>Nothing too exciting here.  gets lowered to a regular call of . The argument is an implicit argument of type  because  is marked with . When unwinding out of a  clif ir instruction, this will continue unwinding out of the current function.  shows the following:</p><pre><code>$ ./rust_unwind_inspect panic_example do_panic\n000000000003878c &lt;do_panic&gt;:\n  personality: 0x38db0 &lt;rust_eh_personality+0x0&gt;\n  LSDA: 0x1fd900 &lt;.gcc_except_table+0x1e4&gt;\n  0x3878c: stp x29, x30, [sp, #-0x10]!\n    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)\n  0x38790: mov x29, sp\n  0x38794: adrp x0, #0x235000\n  0x38798: ldr x0, [x0, #0x578]\n  0x3879c: bl #0x37b40\n    call site 0x3879f..0x387a0 action=continue\n</code></pre><p>Here personality and LSDA are as explained in the <a href=\"https://tweedegolf.nl/en/blog/157/exception-handling-in-rustc-codegen-cranelift#Unwinding-ABI-crash-course\">previous section</a>. The <code>CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)</code> line tells us that</p><ul><li> can be found at offset -16 from </li><li> can be found at offset -8 from </li></ul><p>This information is all coming from the language independent half of the unwind tables which is found in . This is what the unwinder itself parses. In addition there is a line <code>call site 0x317a3..0x317a4 action=continue</code> which indicates that the previous instruction is a call which, if it throws an exception, should cause unwinding to continue to the caller of . This information comes from the LSDA found in  at offset 0x1e0. If no call site is found for a call that threw an exception, the personality function will indicate to the unwinder that unwinding should abort.</p><p>Now to see it in action in the debugger:</p><p>First we define a macro that allows us to set a breakpoint for the personality function getting executed for a given call site:</p><pre><code>(gdb) define break_on_personality_for\nset language c\nb rust_eh_personality if ((struct _Unwind_Context *)$x4).ra == $arg0\nset language auto\nend\n</code></pre><p>And now we can set a breakpoint for  and continue:</p><pre><code>(gdb) break_on_personality_for panic_example::do_panic+20\nBreakpoint 4 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.\n(gdb) continue\nContinuing.\nBreakpoint 4, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                            rust_eh_personality_impl(\n(gdb) up\n#1  0x0000fffff7f972d8 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628)\n    at ../../../src/libgcc/unwind.inc:64\nwarning: 64     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) print context.ra\n$4 = (void *) 0xaaaaaaad87a0 &lt;panic_example::do_panic+20&gt;\n(gdb) down\n#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                            rust_eh_personality_impl(\n</code></pre><p>And to show the return value:</p><pre><code>(gdb) finish\nRun till exit from #0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628) at ../../../src/libgcc/unwind.inc:66\nwarning: 66     ../../../src/libgcc/unwind.inc: No such file or directory\nValue returned is $5 = \"\\b\\000\\000\"\n(gdb) print (_Unwind_Reason_Code)$x0\n$6 = _URC_CONTINUE_UNWIND\n</code></pre><p>We had to explicitly read the return value from register x0 because cg_clif currently doesn't emit debuginfo for arguments and return types. We also had to use <code>((struct _Unwind_Context *)$x4).ra == $arg0</code> as condition for the breakpoint for this reason.</p><h2>Unwinding with an exception handler</h2><p>More exciting is the case where there is an exception handler in scope like our  function.</p><pre><code>//- panic_example.mir\n\n// [snip]\nfn some_func() -&gt; () {\n    let mut _0: ();\n    let _1: Droppable;\n    let _2: ();\n    scope 1 {\n        debug _a =&gt; const Droppable;\n    }\n\n    bb0: {\n        _2 = do_panic() -&gt; [return: bb1, unwind: bb3];\n    }\n\n    bb1: {\n        drop(_1) -&gt; [return: bb2, unwind continue];\n    }\n\n    bb2: {\n        return;\n    }\n\n    bb3 (cleanup): {\n        drop(_1) -&gt; [return: bb4, unwind terminate(cleanup)];\n    }\n\n    bb4 (cleanup): {\n        resume;\n    }\n}\n// [snip]\n</code></pre><p>This function first calls  and then, no matter if it unwinds or not, it runs the drop glue for the  value in . If the drop glue unwinds when called within the unwind path, the function will abort, otherwise it will unwind. And finally if the drop glue succeeds within the unwind path, unwinding will resume thanks to the  terminator.</p><pre><code>; panic_example.clif/some_func.unopt.clif\nfunction u0:29() system_v {\n    sig0 = () system_v\n    sig1 = (i64) system_v\n    sig2 = (i64) system_v\n    sig3 = () system_v\n    sig4 = (i64) system_v\n    fn0 = colocated u0:28 sig0 ; Instance { def: Item(DefId(0:7 ~ panic_example[4533]::do_panic)), args: [] }\n    fn1 = colocated u0:14 sig1 ; Instance { def: DropGlue(DefId(2:3040 ~ core[390d]::ptr::drop_in_place), Some(Droppable)), args: [Droppable] }\n    fn2 = colocated u0:14 sig2 ; Instance { def: DropGlue(DefId(2:3040 ~ core[390d]::ptr::drop_in_place), Some(Droppable)), args: [Droppable] }\n    fn3 = u0:47 sig3 ; \"_ZN4core9panicking16panic_in_cleanup17hda9d23801310caf7E\"\n    fn4 = u0:36 sig4 ; \"_Unwind_Resume\"\nblock0:\n    jump block1\nblock1:\n; _2 = do_panic()\n    try_call fn0(), sig0, block6, [ tag0: block7(exn0) ]\nblock7(v0: i64) cold:\n    v4 -&gt; v0\n    jump block4\nblock6:\n    jump block2\nblock2:\n; drop(_1)\n    v1 = iconst.i64 1\n    call fn1(v1)  ; v1 = 1\n    jump block3\nblock3:\n    return\nblock4 cold:\n; drop(_1)\n    v2 = iconst.i64 1\n    try_call fn2(v2), sig2, block5, [ tag0: block9(exn0) ]  ; v2 = 1\nblock9(v3: i64) cold:\n; panic _ZN4core9panicking16panic_in_cleanup17hda9d23801310caf7E\n    call fn3()\n    trap user1\nblock5 cold:\n; lib_call _Unwind_Resume\n    call fn4(v4)\n    trap user1\n}\n</code></pre><p>This is the clif ir produced for . The  call gets lowered to a  rather than a regular  because this time we want to divert execution to another code path in case of unwinding. In the <code>try_call fn0(), sig0, block6, [ tag0: block7(exn0) ]</code>,  is where execution continues if the call returns normally, while  is where execution will continue when unwinding. The  part indicates that  will get the first register set by the personality function as a block argument. In the case of Rust, this will be a pointer to the exception itself. Other languages may use additional \"landingpad arguments\". The  part is some opaque metadata that Cranelift will forward to cg_clif together with the position of all call sites and landingpads. cg_clif uses  to indicate a cleanup block and  to indicate that an exception should be caught. Once all cleanup code has run, a call to  will be made with the exception pointer as argument to resume unwinding.  will pop the stack frame of the caller and then continue unwinding as usual.</p><pre><code>$ ./unwind_inspect/target/debug/rust_unwind_inspect ./panic_example some_func\n00000000000387a4 &lt;some_func&gt;:\n  personality: 0x38db0 &lt;rust_eh_personality+0x0&gt;\n  LSDA: 0x1fd910 &lt;.gcc_except_table+0x1f4&gt;\n  0x387a4: stp x29, x30, [sp, #-0x10]!\n    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)\n  0x387a8: mov x29, sp\n  0x387ac: str x20, [sp, #-0x10]!\n    CFA=X29+0x10 X29=Offset(-16) X30=Offset(-8) X20=Offset(-32)\n  0x387b0: bl #0x3878c\n    call site 0x387b3..0x387b4 landingpad=0x387c8 action=continue\n  0x387b4: mov x0, #1\n  0x387b8: bl #0x37dc0\n    call site 0x387bb..0x387bc action=continue\n  0x387bc: ldr x20, [sp], #0x10\n  0x387c0: ldp x29, x30, [sp], #0x10\n  0x387c4: ret\n  0x387c8: mov x20, x0\n  0x387cc: mov x0, #1\n  0x387d0: bl #0x37dc0\n    call site 0x387d3..0x387d4 landingpad=0x387e8 action=continue\n  0x387d4: adrp x1, #0x23f000\n  0x387d8: ldr x1, [x1, #0xdc8]\n  0x387dc: mov x0, x20\n  0x387e0: blr x1\n    call site 0x387e3..0x387e4 action=continue\n</code></pre><p>Our  call has <code>call site 0x387b3..0x387b4 landingpad=0x387c8 action=continue</code> as unwind info. This indicates that if the call unwinds, execution should jump to address . The Rust personality function will also set  (aka  in clif ir) to the exception pointer.</p><pre><code>(gdb) break_on_personality_for panic_example::some_func+16\nBreakpoint 5 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.\n(gdb) continue\nBreakpoint 5, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) up\n#1  0x0000fffff7f972d8 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628)\n    at ../../../src/libgcc/unwind.inc:64\nwarning: 64     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) print context.ra\n$7 = (void *) 0xaaaaaaad87b4 &lt;panic_example::some_func+16&gt;\ndown\n#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n</code></pre><p>We got to the personality function call for . Now let's set a couple of breakpoints to see how the personality function causes execution to jump to the landingpad:</p><pre><code>(gdb) break _Unwind_SetGR\nBreakpoint 6 at 0xfffff7f9494c: file ../../../src/libgcc/unwind-dw2.c, line 275.\n(gdb) break _Unwind_SetIP\nBreakpoint 7 at 0xfffff7f949e0: file ../../../src/libgcc/unwind-dw2.c, line 369.\n</code></pre><p> and  are functions called by the personality function to tell the unwinder how to run the landingpad.</p><pre><code>(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffe9f0, index=0, val=187649986796768) at ../../../src/libgcc/unwind-dw2.c:275\nwarning: 275    ../../../src/libgcc/unwind-dw2.c: No such file or directory\n(gdb) print *(struct _Unwind_Exception *)val\n$8 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 &lt;panic_unwind::imp::panic::exception_cleanup&gt;, private_1 = 0, private_2 = 281474976706176}\n(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffe9f0, index=1, val=0) at ../../../src/libgcc/unwind-dw2.c:275\n275     in ../../../src/libgcc/unwind-dw2.c\n</code></pre><p>The first thing the personality function does is use  to set the aformentioned \"landingpad arguments\". x0 is set to the exception pointer, while x1 is set to zero. The latter isn't needed for cg_clif, but cg_llvm generates landingpads that take an additional i32 argument even though it doesn't do anything with it. I believe C++ uses it for the exception type. I suspect at some point LLVM didn't handle landingpads which are missing this extra argument.</p><pre><code>(gdb) continue\nBreakpoint 7, _Unwind_SetIP (context=0xffffffffe9f0, val=187649984661448) at ../../../src/libgcc/unwind-dw2.c:369\n369     in ../../../src/libgcc/unwind-dw2.c\n(gdb) set $landingpad=val\n</code></pre><p>Next up  is used to set the address of the landingpad. We save this address here to set a breakpoint on it later on.</p><pre><code>(gdb) up 2\n#2  0x0000aaaaaaad8dc0 in std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) finish\n_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffe9f0, frames_p=frames_p@entry=0xffffffffe628) at ../../../src/libgcc/unwind.inc:66\nwarning: 66     ../../../src/libgcc/unwind.inc: No such file or directory\nValue returned is $9 = \"\\a\\000\\000\"\n(gdb) p (_Unwind_Reason_Code)$x0\n$10 = _URC_INSTALL_CONTEXT\n</code></pre><p>The personality function returns  to indicate that there is a landingpad.</p><pre><code>(gdb) break *$landingpad\nBreakpoint 8 at 0xaaaaaaad87c8: file panic_example.rs, line 21.\n(gdb) continue\nBreakpoint 8, 0x0000aaaaaaad87c8 in panic_example::some_func () at panic_example.rs:21\n21      }\n(gdb) disassemble\nDump of assembler code for function panic_example::some_func:\n   0x0000aaaaaaad87a4 &lt;+0&gt;:     stp     x29, x30, [sp, #-16]!\n   0x0000aaaaaaad87a8 &lt;+4&gt;:     mov     x29, sp\n   0x0000aaaaaaad87ac &lt;+8&gt;:     str     x20, [sp, #-16]!\n   0x0000aaaaaaad87b0 &lt;+12&gt;:    bl      0xaaaaaaad878c &lt;panic_example::do_panic&gt;\n   0x0000aaaaaaad87b4 &lt;+16&gt;:    mov     x0, #0x1                        // #1\n   0x0000aaaaaaad87b8 &lt;+20&gt;:    bl      0xaaaaaaad7dc0 &lt;_ZN4core3ptr45drop_in_place$LT$panic_example..Droppable$GT$17hb62d62884fcb8d11E&gt;\n   0x0000aaaaaaad87bc &lt;+24&gt;:    ldr     x20, [sp], #16\n   0x0000aaaaaaad87c0 &lt;+28&gt;:    ldp     x29, x30, [sp], #16\n   0x0000aaaaaaad87c4 &lt;+32&gt;:    ret\n=&gt; 0x0000aaaaaaad87c8 &lt;+36&gt;:    mov     x20, x0\n   0x0000aaaaaaad87cc &lt;+40&gt;:    mov     x0, #0x1                        // #1\n   0x0000aaaaaaad87d0 &lt;+44&gt;:    bl      0xaaaaaaad7dc0 &lt;_ZN4core3ptr45drop_in_place$LT$panic_example..Droppable$GT$17hb62d62884fcb8d11E&gt;\n   0x0000aaaaaaad87d4 &lt;+48&gt;:    adrp    x1, 0xaaaaaacdf000\n   0x0000aaaaaaad87d8 &lt;+52&gt;:    ldr     x1, [x1, #3528]\n   0x0000aaaaaaad87dc &lt;+56&gt;:    mov     x0, x20\n   0x0000aaaaaaad87e0 &lt;+60&gt;:    blr     x1\n   0x0000aaaaaaad87e4 &lt;+64&gt;:    udf     #49439\n   0x0000aaaaaaad87e8 &lt;+68&gt;:    adrp    x3, 0xaaaaaacdf000\n   0x0000aaaaaaad87ec &lt;+72&gt;:    ldr     x3, [x3, #2688]\n   0x0000aaaaaaad87f0 &lt;+76&gt;:    blr     x3\n   0x0000aaaaaaad87f4 &lt;+80&gt;:    udf     #49439\nEnd of assembler dump.\n</code></pre><p>And finally if we set a breakpoint on the registered landingpad value and continue execution, we indeed see that execution jumped to the landingpad.</p><p>And finally to finish it up, let's catch a panic using :</p><pre><code>fn do_catch_panic() -&gt; () {\n    let mut _0: ();\n    let mut _1: i32;\n    let mut _2: fn(*mut u8);\n    let mut _3: *mut u8;\n    let mut _4: fn(*mut u8, *mut u8);\n    let _5: !;\n\n    bb0: {\n        _2 = do_catch_panic::do_call as fn(*mut u8) (PointerCoercion(ReifyFnPointer, Implicit));\n        _3 = const 0_usize as *mut u8 (PointerWithExposedProvenance);\n        _4 = do_catch_panic::do_catch as fn(*mut u8, *mut u8) (PointerCoercion(ReifyFnPointer, Implicit));\n        _1 = std::intrinsics::catch_unwind(move _2, copy _3, move _4) -&gt; [return: bb1, unwind unreachable];\n    }\n\n    bb1: {\n        switchInt(move _1) -&gt; [0: bb2, otherwise: bb3];\n    }\n\n    bb2: {\n        _5 = std::process::abort() -&gt; unwind continue;\n    }\n\n    bb3: {\n        return;\n    }\n}\n</code></pre><p> calls the first function pointer with the second argument as argument. If this function unwinds, it will call the second function pointer with the same argument and additionally the exception pointer. And finally it returns 1 if an exception was caught and 0 otherwise.</p><pre><code>function u0:30() system_v {\n    sig0 = (i64) system_v\n    sig1 = (i64, i64) system_v\n    sig2 = (i64) system_v\n    sig3 = (i64, i64) system_v\n    sig4 = () system_v\n    fn0 = colocated u0:31 sig0 ; Instance { def: Item(DefId(0:10 ~ panic_example[4533]::do_catch_panic::do_call)), args: [] }\n    fn1 = colocated u0:32 sig1 ; Instance { def: Item(DefId(0:11 ~ panic_example[4533]::do_catch_panic::do_catch)), args: [] }\n    fn2 = u0:44 sig4 ; Instance { def: Item(DefId(1:6188 ~ std[a023]::process::abort)), args: [] }\nblock0:\n    jump block1\nblock1:\n; _2 = do_catch_panic::do_call as fn(*mut u8) (PointerCoercion(ReifyFnPointer, Implicit))\n    v0 = func_addr.i64 fn0\n; _3 = const 0_usize as *mut u8 (PointerWithExposedProvenance)\n    v1 = iconst.i64 0\n; _4 = do_catch_panic::do_catch as fn(*mut u8, *mut u8) (PointerCoercion(ReifyFnPointer, Implicit))\n    v2 = func_addr.i64 fn1\n; _1 = std::intrinsics::catch_unwind(move _2, copy _3, move _4)\n    try_call_indirect v0(v1), sig2, block5, [ tag1: block6(exn0) ]  ; v1 = 0\nblock5:\n    v3 = iconst.i32 0\n    jump block2(v3)  ; v3 = 0\nblock6(v4: i64) cold:\n    call_indirect.i64 sig3, v2(v1, v4)  ; v1 = 0\n    v5 = iconst.i32 1\n    jump block2(v5)  ; v5 = 1\nblock2(v6: i32):\n; switchInt(move _1)\n    brif v6, block4, block3\nblock3 cold:\n; _5 = std::process::abort()\n    call fn2()\n    trap user1\nblock4:\n    return\n}\n</code></pre><p>In Cranelift IR this is implemented using a  with  rather than  for the cleanup block. And additionally it won't call  in the end, but rather continue execution after the intrinsic call. In the unwind tables the exception catching is represented using:</p><pre><code>$ cargo run -- ../panic_example do_catch_panic\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.04s\n     Running `target/debug/rust_unwind_inspect ../panic_example do_catch_panic`\n00000000000387f8 &lt;do_catch_panic&gt;:\n  personality: 0x38db0 &lt;rust_eh_personality+0x0&gt;\n  LSDA: 0x1fd930 &lt;.gcc_except_table+0x214&gt;\n  LSDA actions:\n    0x0: catch 0x0 next=None\n  0x387f8: stp x29, x30, [sp, #-0x10]!\n    CFA=SP+0x10 X29=Offset(-16) X30=Offset(-8)\n  0x387fc: mov x29, sp\n  0x38800: stp x20, x22, [sp, #-0x10]!\n    CFA=X29+0x10 X29=Offset(-16) X30=Offset(-8) X20=Offset(-32) X22=Offset(-24)\n  0x38804: adrp x11, #0x235000\n  0x38808: ldr x11, [x11, #0x4b8]\n  0x3880c: mov x0, #0\n  0x38810: mov x22, x0\n  0x38814: adrp x20, #0x235000\n  0x38818: ldr x20, [x20, #0x4c0]\n  0x3881c: blr x11\n    call site 0x3881f..0x38820 landingpad=0x38838 action=0\n  0x38820: mov w8, #0\n  0x38824: mov w15, w8\n  0x38828: cbz x15, #0x3884c\n  0x3882c: ldp x20, x22, [sp], #0x10\n  0x38830: ldp x29, x30, [sp], #0x10\n  0x38834: ret\n  0x38838: mov x1, x0\n  0x3883c: mov x0, x22\n  0x38840: blr x20\n    call site 0x38843..0x38844 action=continue\n  0x38844: mov w8, #1\n  0x38848: b #0x38824\n  0x3884c: adrp x0, #0x23e000\n  0x38850: ldr x0, [x0, #0xab0]\n  0x38854: blr x0\n    call site 0x38857..0x38858 action=continue\n</code></pre><p>where  references the  LSDA action. In C++ the  would instead be the typeid of the caught exception and  optionally representing another  block for the same  block.</p><p>And finally one last debugger step through for completeness. It is not much different from the  step through, so I won't discuss it in detail.</p><pre><code>(gdb) break_on_personality_for panic_example::do_catch_panic+40\nBreakpoint 9 at 0xaaaaaaad8dbc: file library/std/src/sys/personality/gcc.rs, line 307.\n(gdb) continue\nBreakpoint 9, std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) up\n#1  0x0000fffff7f97354 in _Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffea90, frames_p=frames_p@entry=0xffffffffe6c8)\n    at ../../../src/libgcc/unwind.inc:64\nwarning: 64     ../../../src/libgcc/unwind.inc: No such file or directory\n(gdb) print context.ra\n$11 = (void *) 0xaaaaaaad8820 &lt;panic_example::do_catch_panic+40&gt;\n(gdb) down\n#0  std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffea90, index=0, val=187649986796768) at ../../../src/libgcc/unwind-dw2.c:275\nwarning: 275    ../../../src/libgcc/unwind-dw2.c: No such file or directory\n(gdb) print *(struct _Unwind_Exception *)val\n$12 = {exception_class = 6076294132934528845, exception_cleanup = 0xaaaaaabde440 &lt;panic_unwind::imp::panic::exception_cleanup&gt;, private_1 = 0, private_2 = 281474976706176}\n(gdb) continue\nBreakpoint 6, _Unwind_SetGR (context=0xffffffffea90, index=1, val=0) at ../../../src/libgcc/unwind-dw2.c:275\n275     in ../../../src/libgcc/unwind-dw2.c\n(gdb) continue\nBreakpoint 7, _Unwind_SetIP (context=0xffffffffea90, val=187649984661560) at ../../../src/libgcc/unwind-dw2.c:369\n369     in ../../../src/libgcc/unwind-dw2.c\n(gdb) set $landingpad=val\n(gdb) up 2\n#2  0x0000aaaaaaad8dc0 in std::sys::personality::gcc::rust_eh_personality () at library/std/src/sys/personality/gcc.rs:307\n307                             rust_eh_personality_impl(\n(gdb) finish\n_Unwind_RaiseException_Phase2 (exc=exc@entry=0xaaaaaace1ce0, context=context@entry=0xffffffffea90, frames_p=frames_p@entry=0xffffffffe6c8) at ../../../src/libgcc/unwind.inc:66\nwarning: 66     ../../../src/libgcc/unwind.inc: No such file or directory\nValue returned is $13 = \"\\a\\000\\000\"\n(gdb) print (_Unwind_Reason_Code)$x0\n$14 = _URC_INSTALL_CONTEXT\n(gdb) break *$landingpad\nBreakpoint 10 at 0xaaaaaaad8838: file panic_example.rs, line 43.\n(gdb) continue\nBreakpoint 10, 0x0000aaaaaaad8838 in panic_example::do_catch_panic () at panic_example.rs:43\n43      }\n(gdb) disassemble\nDump of assembler code for function panic_example::do_catch_panic:\n   0x0000aaaaaaad87f8 &lt;+0&gt;:     stp     x29, x30, [sp, #-16]!\n   0x0000aaaaaaad87fc &lt;+4&gt;:     mov     x29, sp\n   0x0000aaaaaaad8800 &lt;+8&gt;:     stp     x20, x22, [sp, #-16]!\n   0x0000aaaaaaad8804 &lt;+12&gt;:    adrp    x11, 0xaaaaaacd5000\n   0x0000aaaaaaad8808 &lt;+16&gt;:    ldr     x11, [x11, #1208]\n   0x0000aaaaaaad880c &lt;+20&gt;:    mov     x0, #0x0                        // #0\n   0x0000aaaaaaad8810 &lt;+24&gt;:    mov     x22, x0\n   0x0000aaaaaaad8814 &lt;+28&gt;:    adrp    x20, 0xaaaaaacd5000\n   0x0000aaaaaaad8818 &lt;+32&gt;:    ldr     x20, [x20, #1216]\n   0x0000aaaaaaad881c &lt;+36&gt;:    blr     x11\n   0x0000aaaaaaad8820 &lt;+40&gt;:    mov     w8, #0x0                        // #0\n   0x0000aaaaaaad8824 &lt;+44&gt;:    mov     w15, w8\n   0x0000aaaaaaad8828 &lt;+48&gt;:    cbz     x15, 0xaaaaaaad884c &lt;panic_example::do_catch_panic+84&gt;\n   0x0000aaaaaaad882c &lt;+52&gt;:    ldp     x20, x22, [sp], #16\n   0x0000aaaaaaad8830 &lt;+56&gt;:    ldp     x29, x30, [sp], #16\n   0x0000aaaaaaad8834 &lt;+60&gt;:    ret\n=&gt; 0x0000aaaaaaad8838 &lt;+64&gt;:    mov     x1, x0\n   0x0000aaaaaaad883c &lt;+68&gt;:    mov     x0, x22\n   0x0000aaaaaaad8840 &lt;+72&gt;:    blr     x20\n   0x0000aaaaaaad8844 &lt;+76&gt;:    mov     w8, #0x1                        // #1\n   0x0000aaaaaaad8848 &lt;+80&gt;:    b       0xaaaaaaad8824 &lt;panic_example::do_catch_panic+44&gt;\n   0x0000aaaaaaad884c &lt;+84&gt;:    adrp    x0, 0xaaaaaacde000\n   0x0000aaaaaaad8850 &lt;+88&gt;:    ldr     x0, [x0, #2736]\n   0x0000aaaaaaad8854 &lt;+92&gt;:    blr     x0\n   0x0000aaaaaaad8858 &lt;+96&gt;:    udf     #49439\nEnd of assembler dump.\n</code></pre><p>We've now seen how exception handling works in cg_clif. Currently, this feature is still disabled by default because I'm still in the process of finishing the implementation and fixing a performance regression caused by enabling it. Follow the <a href=\"https://github.com/rust-lang/rustc_codegen_cranelift/issues/1567\">tracking issue</a> to stay up to date!</p><p>The following gdb script can be used to reproduce the debugger session:</p><pre><code>set debuginfod enabled on\nset pagination off\nb do_panic\nrun\nbt\nb _Unwind_RaiseException\nc\nbt\np exc\np *exc\np (char[8])(exc.exception_class)\nb _Unwind_RaiseException_Phase2\nc\ndel 3\ndefine break_on_personality_for\nset language c\nb rust_eh_personality if ((struct _Unwind_Context *)$x4).ra == $arg0\nset language auto\nend\necho \\ndo_panic\\n===========================\\n\nbreak_on_personality_for panic_example::do_panic+20\nc\nup\np context.ra\ndown\nfinish\np (_Unwind_Reason_Code)$x0\necho \\nsome_func\\n===========================\\n\nbreak_on_personality_for panic_example::some_func+16\nc\nup\np context.ra\ndown\nb _Unwind_SetGR\nb _Unwind_SetIP\nc\np *(struct _Unwind_Exception *)val\nc\nc\nset $landingpad=val\nup 2\nfinish\np (_Unwind_Reason_Code)$x0\nb *$landingpad\nc\ndisassemble\necho \\ndo_catch_panic\\n===========================\\n\nbreak_on_personality_for panic_example::do_catch_panic+40\nc\nup\np context.ra\ndown\nc\np *(struct _Unwind_Exception *)val\nc\nc\nset $landingpad=val\nup 2\nfinish\np (_Unwind_Reason_Code)$x0\nb *$landingpad\nc\ndisassemble\n</code></pre>","contentLength":31660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lo3v9h/exception_handling_in_rustc_codegen_cranelift/"},{"title":"Ask r/kubernetes: What are you working on this week?","url":"https://www.reddit.com/r/kubernetes/comments/1lo3lj1/ask_rkubernetes_what_are_you_working_on_this_week/","date":1751277620,"author":"/u/gctaylor","guid":176472,"unread":true,"content":"<p>What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell <a href=\"https://www.reddit.com/r/kubernetes\">/r/kubernetes</a> what you're up to this week!</p>","contentLength":195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I just want to express my appreciation for golang","url":"https://www.reddit.com/r/golang/comments/1lo2b64/i_just_want_to_express_my_appreciation_for_golang/","date":1751272434,"author":"/u/sebastianstehle","guid":176427,"unread":true,"content":"<p>Hi, I am from the .NET world and I really hate that more and more features are added to the language. But I am working with it since a 15 years, so I know every single detail and the code is easy to understand for me.</p><p>But at the moment I am also in a kotlin project. And I don't know if kotlin has more or less features but I have the impression that in every code review I see something new. A weird language construct or function from the runtime library that should improve something by getting rid of a few characters. If you are familiar with a programming language you do not see the problems so clearly, but know I am aware how much kotlin (and probably C#) can suck.</p><p>When I work with go, I just understand it. There is only one way to do something and not 10. I struggle with generics a little bit, but overall it is a great experience.</p>","contentLength":842,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Linux journey so far","url":"https://www.reddit.com/r/linux/comments/1lo27j5/my_linux_journey_so_far/","date":1751272006,"author":"/u/onelostalien777","guid":176475,"unread":true,"content":"<p>I started with Manjaro like 10 years ago and used it for a few months and then switched back to windows, tried ubuntu and a few others and then forgot about linux for many years, at the start if the year i started checking distros out again, i started with mint for nearly a month and it was ok, then i went to arch and i liked it but i don't have that much time to configure a lot of things ( even tho its pretty fun and i do enjoy it but i don't have time to fix things ) so i went to manjaro and yeah i really liked it ( i am biased as it is the one i used many years ago ) it had customization and i didnt find many bugs and i really liked it but then got on reddit and saw everyone hates it and saying endeavour is better and the manjaro team is poopy so i will give it a try starting today ( my favorite one was arch but i found myself breaking it every other day and reinstalling it again and i don't have much time for it, i 100% prefer arch based distros and maybe one day i'll go full arch if i find the time ( or not, depends how endeavour goes )</p>","contentLength":1057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fedora: Proposal for the removal of i686 withdrawn","url":"https://discussion.fedoraproject.org/t/f44-change-proposal-drop-i686-support-system-wide/156324/400","date":1751270726,"author":"/u/FryBoyter","guid":176428,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lo1w8e/fedora_proposal_for_the_removal_of_i686_withdrawn/"},{"title":"Freelens v1.4.0 is just released","url":"https://github.com/freelensapp/freelens/releases/tag/v1.4.0","date":1751268564,"author":"/u/dex4er","guid":176392,"unread":true,"content":"<p>I'm happy to share with you the newest release of free UI for Kubernetes with a lot of minor improvements for UX and handling extensions. This version also brings full support for Jobs, CronJobs, and EndpointSlices. </p><p>Extensions can now use the JSX runtime and many more React components. The new version is more developer-friendly, and I hope we'll see some exciting extensions soon.</p><p>Finally Windows arm64 version is bug-free and can install extensions at all. Of course, all other versions are first citizens too: Windows x64 (exe, msi, and WinGet), MacOS arm64 and Intel (pkg, dmg, and brew), Linux for all variants (APT, deb, rpm, AppImage, Flatpak, Snap, and AUR).</p>","contentLength":666,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lo1dgs/freelens_v140_is_just_released/"},{"title":"Service Binding for K8s in Spring Boot cloud-native applications","url":"https://medium.com/cloudnativepub/service-binding-for-k8s-in-spring-boot-cloud-native-applications-3717d3486886?sk=346dc534327888ca805aad94e5d0f1b5","date":1751266991,"author":"/u/zarinfam","guid":176358,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lo0zg3/service_binding_for_k8s_in_spring_boot/"},{"title":"Donate More by Donating Less","url":"https://www.reddit.com/r/linux/comments/1lo0x5p/donate_more_by_donating_less/","date":1751266754,"author":"/u/Fluid-Pirate646","guid":176359,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is the Rusty Approach to Distributed Systems?","url":"https://www.reddit.com/r/rust/comments/1lo0dr4/what_is_the_rusty_approach_to_distributed_systems/","date":1751264610,"author":"/u/skwyckl","guid":176815,"unread":true,"content":"<p>I have thickened my skin in the Erlang / Elixir world when starting out, which kind of ruined concurrency for me in all other languages, but still, I am building an application in Rust and was thinking how to replicate the features that make Erlang-style concurrency so great. So, for starting out, the Actor Model can be implemented using e.g. Actix, so all good, but AFAIK I can't have two Actix actors communicate across difference instances of my application. What link is missing there Rust-wise? Thank you in advance.</p>","contentLength":523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Kubernetes] Backend pod crashes with Completed / CrashLoopBackOff, frontend stabilizes ‚Äî what‚Äôs going on?","url":"https://www.reddit.com/r/kubernetes/comments/1lo0168/kubernetes_backend_pod_crashes_with_completed/","date":1751263268,"author":"/u/erudes91","guid":176321,"unread":true,"content":"<p>New to building K clusters, only been a user of them not admin.</p><ul><li>Running local K8s cluster with 2 nodes (node1: control plane, node2: worker).</li><li>Built and deployed a full app manually (no Helm).</li><li>Backend: Python Flask app (alternatively tested with Node.js).</li><li>Frontend: static HTML + JS on Nginx.</li><li>Services set up properly ( for backend,  for frontend).</li></ul><ul><li>Backend pod status starts as , then goes to , and finally ends up in .</li><li> for backend shows nothing.</li><li>Flask version works  when run with Podman on node2: it starts, listens, and responds to POSTs.</li><li>Frontend pod goes through multiple restarts, but after a few minutes finally stabilizes ().</li><li>Frontend can't reach the backend () ‚Äî because backend isn‚Äôt running.</li></ul><ul><li>Verified backend image runs fine with <code>podman run -p 5000:5000 backend:local</code>.</li><li>Described pods: backend shows , , no crash trace.</li><li>Checked YAML: nothing fancy ‚Äî single container, exposing correct ports, no health checks.</li><li>Logs: totally empty (), no Python traceback or indication of forced exit.</li><li>Frontend works but obviously can‚Äôt POST since backend is unavailable.</li></ul><ul><li>The pod exits cleanly after handling the POST and terminates.</li><li>Kubernetes thinks it crashed because it exits too early.</li></ul><p><strong>node1@node1:/tmp$ kubectl get pods</strong></p><p>NAME READY STATUS RESTARTS AGE</p><p>backend-6cc887f6d-n426h 0/1 CrashLoopBackOff 4 (83s ago) 2m47s</p><p>frontend-584fff66db-rwgb7 1/1 Running 12 (2m10s ago) 62m</p><p>Why does this pod \"exit cleanly\" and not stay alive?</p><p>Why does it behave correctly in Podman but fail in K8s?</p><p>Any files you wanna take a look at?</p><pre><code>FROM node:18-slim WORKDIR /app COPY package*.json ./ RUN npm install COPY server.js ./ EXPOSE 5000 CMD [\"node\", \"server.js\"] FROM node:18-slim WORKDIR /app COPY package*.json ./ RUN npm install COPY server.js ./ EXPOSE 5000 CMD [\"node\", \"server.js\"] </code></pre><pre><code>const express = require('express'); const app = express(); app.use(express.json()); app.post('/register', (req, res) =&gt; { const { name, email } = req.body; console.log(`Received: name=${name}, email=${email}`); res.status(201).json({ message: 'User registered successfully' }); }); app.listen(5000, () =&gt; { console.log('Server is running on port 5000'); }); const express = require('express'); const app = express(); app.use(express.json()); app.post('/register', (req, res) =&gt; { const { name, email } = req.body; console.log(`Received: name=${name}, email=${email}`); res.status(201).json({ message: 'User registered successfully' }); }); app.listen(5000, () =&gt; { console.log('Server is running on port 5000'); }); </code></pre>","contentLength":2463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We are rewriting the message queue in Rust and would like to hear your suggestions.","url":"https://www.reddit.com/r/rust/comments/1lnzkjq/we_are_rewriting_the_message_queue_in_rust_and/","date":1751261497,"author":"/u/wenqiang_lobo","guid":176473,"unread":true,"content":"<p>We are a group of developers who love Rust, message queues, and distributed storage. We are trying to write a message queue using Rust. Its name is: RobustMQ. It follows the Apache-2.0 license. We hope that it can eventually be contributed to the Apache community and become a top project of the Apache community, contributing our own share of strength to the Apache and Rust communities. </p><p>Some information about RobustMQ: </p><ol><li>Original intention: To explore the possibility of combining Rust with message queues, and solve the existing problems of the message queue components in the current community.</li><li>Positioning: An All In One open-source message queue developed 100% based on the Rust language.</li><li>Goal: To deliver a message queue that supports multiple mainstream messaging protocols, has a completely Serverless architecture, is low-cost, and elastic.</li></ol><ul><li>100% Rust: A message queue engine implemented entirely based on the Rust language.</li><li>Multiple protocols: Supports MQTT 3.1/3.1.1/5.0, AMQP, Kafka Protocol, RocketMQ Remoting/GRPC, OpenMessing, JNS, SQS, etc., the mainstream messaging protocols.</li><li>Hierarchical architecture: A three-layer architecture with completely independent computing, storage, and scheduling, with clear responsibilities and independence.</li><li>Serverless: All components have distributed cluster deployment capabilities and the ability to quickly scale up and down.</li><li>Plugin-based storage: An independent plugin-based storage layer implementation, supporting both independent deployment and shared storage architectures.</li><li>Comprehensive functions: Fully aligns with the functions and capabilities of the mainstream MQ products in the corresponding communities. </li></ul><p>For more detailed information, please visit our Github homepage and official website: </p><p>We have currently completed the development of the first release version, including the overall architecture and the adaptation of the MQTT protocol. Next, we plan to further improve the MQTT, refine the stability, and then prepare for compatibility with the Kafka protocol. </p><p>At this stage, we would like to hear your suggestions. We hope to know whether this action makes sense and what areas for improvement there are. So that we can stay on the right track and do this well. </p><p>We know this is a difficult task, but we think it's a really cool thing and we want to give it a try. We are looking forward to the community's suggestions. </p><p>Cool! Let's do something fun together~. </p><p>At the same time, we also hope to find students who are interested in implementing infrastructure components, message queues, and distributed storage systems using Rust, and together explore the unlimited possibilities of Rust in the field of infrastructure. </p>","contentLength":2681,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zoi: A Universal Package Manager (Seeking Contributors!)","url":"https://www.reddit.com/r/golang/comments/1lnzfp3/zoi_a_universal_package_manager_seeking/","date":1751260987,"author":"/u/ZilloweZ","guid":176474,"unread":true,"content":"<p>Zoi is a project I recently started working on, its main goal is to provide a universal package manager for all operating systems and architectures. It's currently in beta, and it's has a lot of problems, please don't hesitate to report an issue.</p><p>It fetches the packages from a git repo and sync it locally, the packages are in yaml format. The yaml file has options to download the file, either a binary, installer script or build from source. Also it has runtime dependencies and build dependencies.</p><p>I'm currently looking for contributors, idk if this post is correct sharing it here, I hope so.</p>","contentLength":595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feadback/Support: Inkube CLI app - Helps to Develop Inside Kubernetes Environment","url":"https://www.reddit.com/r/kubernetes/comments/1lnzazc/feadbacksupport_inkube_cli_app_helps_to_develop/","date":1751260519,"author":"/u/abdheshnayak","guid":176320,"unread":true,"content":"<p>I felt hectic to setup and manage local development with kubernetes cluster access. i was thinking solution for easy setup for each project with added env mirroring and packages locking. so built one tools for it inkube which helps to connect with cluster, mirror env and also provides package manager.</p><p>please have a look and leave your thoughts and feed back on it.</p>","contentLength":365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go makes sense in air-gapped ops environments","url":"https://www.reddit.com/r/golang/comments/1lnz0w2/go_makes_sense_in_airgapped_ops_environments/","date":1751259520,"author":"/u/Resource_account","guid":176323,"unread":true,"content":"<p>Been doing Linux ops in air-gapped environments for about a year. Mostly RHEL systems with lots of automation. My workflow is basically 75% bash and 25% Ansible.</p><p>Bash has been solid for most of my scripting needs. My mentor believes Python scripts are more resilient than bash and I agree with him in theory but for most file operations the extra verbosity isn't worth it.</p><p>So far I've only used Python in prod in like 2-3 situations. First I wrote an inventory script for Ansible right around the time I introduced the framework itself to our shop. Later I wrote a simple script that sends email reminders to replace certain keys we have. Last thing I built with it was a PyGObject GUI though funny story there. Took a week to build in Python then rewrote it in bash with YAD in an afternoon.</p><p>Python's stdlib is honestly impressive and covers most of what I need without external dependencies. But we've got version management headaches. Desktops run 3.12 for Ansible but servers are locked to 3.8 due to factory requirements. System still depends on 3.6 and most of the RPM's are built against 3.6 (RHEL 8).</p><p>Started exploring Go recently for a specific use case. Performance-critical stuff with our StorNext CVFS. In my case with venv and dependencies on CVFS performance has been a little rough. The compiled binary approach seems ideal for this. Just rsync the binary to the server and it runs. Done.</p><p>The other benefit I've noticed is the compiler feedback. Getting LSPs and linters through security approval is a long exhausting process so having the compiler catch issues upfront, and so quickly, helps a lot. Especially when dealing with the constant firefighting.</p><p>Not saying Python is bad or Go is better. Just finding Go fits this particular niche really well.</p><p>Wondering if other devops or linux sysadmins have found themselves in a similar spot.</p>","contentLength":1847,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Procedural city generation in go with ebitengine","url":"https://hopfenherrscher.itch.io/union-station","date":1751258433,"author":"/u/oliver-bestmann","guid":176293,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lnypwm/procedural_city_generation_in_go_with_ebitengine/"},{"title":"Would love some feedback on a library I‚Äôm writing!","url":"https://www.reddit.com/r/golang/comments/1lnyfjq/would_love_some_feedback_on_a_library_im_writing/","date":1751257417,"author":"/u/Far_Solution_1784","guid":176394,"unread":true,"content":"<p>Hello, I am working on a library that wraps go docker sdk and also wraps over the docker compose cli to allow you to programmatically create docker compose files and or run directly from go code. I‚Äôm aiming to solve a solution for go devs that want programmatic control over docker in a declarative fashion. </p><p>Would love some feedback, or if you‚Äôre willing to contribute that would be sick! </p>","contentLength":393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Should I be Looking into Custom Metrics or External Metrics?","url":"https://www.reddit.com/r/kubernetes/comments/1lnydqd/should_i_be_looking_into_custom_metrics_or/","date":1751257242,"author":"/u/zangetsuMG","guid":176291,"unread":true,"content":"<p>I am not completely sure if I am even asking the right kind of questions, so please feel free to offer guidance. I am hoping to learn how I can use either Custom Metrics or External Metrics to solve some problems. I'll put the questions up front, but also provide some background that might help people understand what I am thinking and trying to do.</p><p>Thank you and all advice is welcome.</p><p><strong>Is there some off the shelf solution that can run an SQL Query, and provide the result as a metric?</strong></p><p>This feels like it is a problem others have had and is probably already solved. I feel like there should be some kind of existing service I can run, and with appropriate configuration it should be able to connect to my database, run a query and return that value as a metric in a form that K8s can use. Is there something like that?</p><p><strong>If I have to implement my own, Should I be looking at Custom Metrics or External Metrics?</strong></p><p>I can go down the path of building my own metrics service, but if I do, should I be doing Custom Metrics, or External Metrics? Is there some documentation about Custom Metrics or External Metrics that is more than just a generated description of the data types? I would love to find something that explains things like what the different parts of the URI path mean, and all the little pieces of the data types so that if I do implement something, I can do it right.</p><p><strong>Is it really still a beta API after at least 4 years?</strong></p><p>I'm kind of surprised by the v1beta1 and v1beta2 in the names after all this time.</p><p> (feel free to stop reading here)</p><p>I am working with a system that is composed of various containers. Some containers have a web service inside of them, while others have a non-interactive processing service inside them, and both types communicate with a database (Microsoft SQL Server). </p><p>The web servers are actually Asp.Net Core web servers and we have been able to implement a basic web API that returns an HTTP 200 OK if the web server thinks it is running correctly, or an HTTP error code if it is not. We've been able to configure K8s to probe this API and do things like terminate and restart the container. For the web servers we've been able to setup some basic horizontal auto-scaling based on CPU usage. (If they have high sustained CPU usage, scale up).</p><p>For our non-interactive services (Also .Net code), they mostly connect to the database periodically and do some work (this is way over-simplified, but I suspect the details aren't important.)In the past we have had some cases where these processes may get into a broken state, but from the container management tools they look like they are running just fine. This is one problem I would like to be able to detect and have k8's report and maybe fix. Another issue is that I would like for these non-interactive services to be able to auto-scale, but the catch here is that the out of the box metrics like CPU and Memory aren't actually a good indicator if the container should be scaled.</p><p>I'm not too worried about the web servers, but I am worried about the non-interactive services. I am reasonably sure I could add a very small web API that could be probed, and that we could configure K8s to check the container and terminate and restart. In fact I am almost sure that we'll be adding that functionality in the near future.</p><p>I think for our non-interactive services in order to get a smart horizontal auto-scaling, we need some kind of metrics server, but I am having trouble determining what that metrics service should look like. I have found the external metrics documentation at <a href=\"https://kubernetes.io/docs/reference/external-api/\">https://kubernetes.io/docs/reference/external-api/</a> but I find it a bit hard to follow.</p><p>Because of the way my non-interactive services work, I am thinking that there is some amount of available work in our database. The unit-of-work has a time value for when the unit of work was added, so I should be able to look at the work, and calculate how long the work has been waiting before being processed, and if that time span is too long, that would be the signal to scale up. I am reasonably sure I could distill that question down to an SQL query that returns a single number, that could be returned as a metric.</p>","contentLength":4160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a PC for AI Workloads + Kubernetes, Need Advice on CPU, GPU, RAM & Upgradability","url":"https://www.reddit.com/r/kubernetes/comments/1lny37h/building_a_pc_for_ai_workloads_kubernetes_need/","date":1751256230,"author":"/u/root0ps","guid":176290,"unread":true,"content":"<p>I‚Äôm planning to build a PC mainly to learn and run AI workloads and also set up Kubernetes clusters locally. I already have some experience with Kubernetes and now want to get into training and running AI models on it.</p><p>I‚Äôm based in India, so availability and pricing of parts here is also something I‚Äôll need to consider.</p><p>I need help with a few things:</p><p>CPU ‚Äì AMD or Intel? I want something powerful but also future-proof. I‚Äôd like to upgrade the CPU in the future, so I‚Äôm looking for a motherboard that will support newer processors.</p><p>GPU ‚Äì NVIDIA or AMD? My main goal is running AI workloads. Gaming is a secondary need. I‚Äôve heard NVIDIA is better for AI (CUDA, etc.), but is AMD also good enough? Also, is it okay to start with integrated graphics for now and add a good GPU 6‚Äì8 months later? Has anyone tried this?</p><p>RAM ‚Äì 32 GB or 64 GB? Is 32 GB enough for running AI stuff and Kubernetes? Or should I go for 64 GB from the start?</p><p>Budget: I don‚Äôt have a strict budget, but I‚Äôm thinking around $2000. I‚Äôm okay with spending a bit more if it means better long-term use.</p><p>I want to build something I can upgrade later instead of replacing everything. If anyone has built a PC for similar use cases or has suggestions, I‚Äôd really appreciate your input!</p>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Donate More by Donating Less (further explanation from Steve Deobald)","url":"https://www.reddit.com/r/linux/comments/1lnxslo/donate_more_by_donating_less_further_explanation/","date":1751255241,"author":"/u/pr0fic1ency","guid":176275,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Free access to an H100. What can I build?","url":"https://www.reddit.com/r/MachineLearning/comments/1lnvjin/r_free_access_to_an_h100_what_can_i_build/","date":1751247987,"author":"/u/cringevampire","guid":176655,"unread":true,"content":"<p>My company is experimenting with new hardware and long story short, there's an idling H100 with a 2TB RAM and 27TB of storage and I'm allowed to play with it!</p><p>I really want to do some cool AI research to publish at a decent conference but I'm not well caught up with the research frontier and I could really use some help (and collaborators?).</p><p>I understand neural networks, CNNs, transformer models etc. to a reasonable depth but understanding what SOTA is will probably take more time than how long I have access to the GPU</p>","contentLength":522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Duke Nukem 3D code review by Tariq10x","url":"https://m.youtube.com/watch?v=F9lOJlC_kQs","date":1751242022,"author":"/u/r_retrohacking_mod2","guid":175519,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lntlhj/duke_nukem_3d_code_review_by_tariq10x/"},{"title":"Flecs v4.1, an Entity Component System for C/C++/C#/Rust is out!","url":"https://ajmmertens.medium.com/flecs-4-1-is-out-fab4f32e36f6","date":1751241621,"author":"/u/ajmmertens","guid":176292,"unread":true,"content":"<p><a href=\"https://github.com/SanderMertens/flecs\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs</a> is an Entity Component System (<a href=\"https://github.com/SanderMertens/ecs-faq\" rel=\"noopener ugc nofollow\" target=\"_blank\">FAQ</a>) for C and C++ that helps with building games, simulations and more. The core features of Flecs are:</p><ul><li>Store data for  in data structures optimized for CPU cache efficiency and composition-first design</li><li>Builtin support for hierarchies, prefabs and more with <a href=\"https://www.flecs.dev/flecs/md_docs_2Relationships.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>which speed up game code and reduce boiler plate</li><li>An <a href=\"https://www.flecs.dev/explorer\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a><a href=\"https://www.flecs.dev/flecs/md_docs_2Quickstart.html#addons]\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>to profile, visualize, document and debug projects</li></ul><p>Flecs is fully open source and licensed under the MIT license. If you‚Äôd like to support the project, consider giving it a Ô∏èÔ∏è‚≠êÔ∏è on the Github page!</p><p>Since v4.0 there have been exciting updates from games that use Flecs!</p><p>Congrats to the Tempest Rising team on the successful release of their game! Command distinct factions in a desperate struggle for power and resources in Tempest Rising ‚Äî a classic RTS set on Earth after a nuclear war (Unreal Engine 5).</p><h2>Announced: Resistance is Brutal</h2><p>Resistance is Brutal is Vampire Survivors meets Running Man with a bit of Rick and Morty thrown in. Try to survive as you battle the enemy hordes with brutal abilities before going out in a blaze of glory (Unreal Engine 5).</p><h2>Announced: Age of Respair</h2><p>Age of Respair is a medieval strategy castle-builder. Build massive castles with fortifications while managing resources and production chains. Assemble a large army and lay siege to enemy castles. Lead your people and bring respair to your kingdom (Unreal Engine 5).</p><p>You‚Äôve been hired by an elite organization that cooks for gods. Farm, automate, and cook gourmet meals to appease divine beings or face extinction (Unreal Engine 5).</p><p>Ascendant is a large scale open world voxel RPG inspired by Minecraft and Daggerfall. it spawns the player into a procedurally generated landscape with various biomes and features such as cities and ruins. It creates animals and enemies in the map, and you can gather resources, craft them into other ones, and fight enemies with sword, bow, or magic.</p><p>The developer behind this project is also the author of <a href=\"https://vkguide.dev/\" rel=\"noopener ugc nofollow\" target=\"_blank\">vkguide</a>, which is one of the best resources for learning Vulkan. There is a <a href=\"https://vkguide.dev/docs/ascendant\" rel=\"noopener ugc nofollow\" target=\"_blank\">chapter</a> dedicated specifically to Ascendant, make sure to check it out!</p><p>A vampire survivor like game built in Flecs. What‚Äôs awesome is that the developer of the game has made its source code available! Check it out here: <a href=\"https://github.com/ptidejteam/ecs-survivors\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/ptidejteam/ecs-survivors</a></p><p>There are still more projects cooking that I haven‚Äôt listed here. If you want to stay up to date on the progress of Flecs projects, check out the <a href=\"https://discord.com/invite/caR2WmY\" rel=\"noopener ugc nofollow\" target=\"_blank\">showcase channel</a> of the Flecs Discord!</p><p>The v4.1 release builds on the new architecture laid out by v4.0 and comes with significant performance improvements across many parts of the library, in addition to new features that are already proving popular with developers.</p><p>Here are some of the most notable performance improvements (as measured by the <a href=\"https://github.com/SanderMertens/ecs_benchmark\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs benchmarking suite</a>):</p><ul><li>: 5x faster than v4.0</li><li>: 5x faster than v4.0</li><li>: 5‚Äì10x faster than v4.0</li><li>: 1.5‚Äì2x faster than v4.0</li><li>: 2‚Äì4x faster than v4.0</li><li>: 5‚Äì10x faster than v4.0</li><li>: 40x faster than v4.0, 6x faster than v3.2.12</li></ul><p>Flecs now also uses a lot less memory. The minimum footprint of a Flecs world has decreased 5x, and overall RAM consumption can be as much as two times lower depending on the application! Scroll down to the ‚ÄúPerformance‚Äù section to see how these improvements were achieved.</p><p>As with every release, a lot of effort has gone into testing and bugfixing. Flecs now has 11.000 test cases, an increase of 2500 test cases since v4.0!</p><p>Here‚Äôs an overview of the highlights since v4.0:</p><h2>Non-fragmenting components</h2><p>Flecs is an <a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-2-archetypes-and-vectorization-fe21690805f9\" data-discover=\"true\">archetype-style ECS</a>, which in short means it optimizes the storage at runtime to allow for fast iteration of multiple components at the same time. The tradeoff that comes with this design however is that adding and removing components to entities can be expensive.</p><p>The alternative to archetypes is a sparse set or independent-storage based design. This design has the opposite tradeoff: adding/removing components is cheap, but iterating multiple components at the same time is more expensive.</p><p>Flecs v4.1 is the first* open source ECS that <a href=\"https://www.flecs.dev/flecs/md_docs_2ComponentTraits.html#dontfragment-trait\" rel=\"noopener ugc nofollow\" target=\"_blank\">supports both</a>! Switching between storages is easy, just add the  trait to a component during registration:</p><pre></pre><p><em>* many people have pointed out to me that Bevy ECS has had sparse components for a long time. That‚Äôs correct, but adding a sparse component to an entity in Bevy changes its archetype, so they are *not* non-fragmenting!</em></p><h2>Flecs Script improvements</h2><p>Many new features and performance improvements have been added to Flecs Script that allow for the creation of more complex scenes. Here‚Äôs an overview of the most notable changes:</p><ul><li>A new (much faster) expression parser</li><li>A new  addon</li><li>Entity name expressions ()</li></ul><p>An example script that uses some of the new features:</p><pre></pre><p>The following scene is constructed entirely from primitive rectangle and box shapes (link to the code below the image):</p><p>Often when working with hierarchies the order in which children are processed is important. This is especially true for UIs, where the order of widgets can determine how they appear on screen.</p><p>Because of how the Flecs storage works internally, the order in which children are iterated can change when components are added or removed. To improve the usability of builtin hierarchies when working with UIs and similar use cases, a new  trait has been introduced:</p><pre></pre><p>An additional operation has been introduced that makes it possible to change the order of children after they have been created:</p><pre></pre><h2>World local component ids</h2><p>A common annoyance when working with multiple Flecs worlds was that if components were shared between worlds, their ids had to match. This could sometimes lead to unexpected and confusing errors, and was not great for developer UX.</p><p>Since v4.0.4 component ids in the C++ API are now fully local to a world, which means that different worlds can have different ids for the same component.</p><h2>get()/try_get() API redesign</h2><p>The C++  APIs have been redesigned to use the following pattern:</p><pre></pre><p>This expresses intent more clearly and can reduce boilerplate as code no longer has to include defensive checks on whether  returns a . It also allows most code to work with references, which many users prefer.</p><p>: this is a breaking change that affects a lot of code. An easy way to migrate is to replace all occurrences of  in a project with .</p><p>A new  API has been added to the C++ API which can be used as an alternative to . The  operation will add a component to an entity if the entity didn‚Äôt have it yet, whereas  only assigns existing components and will panic if the entity didn‚Äôt have the component yet.</p><p>In addition to expressing intent more clearly,  also guarantees that it will never move entities to different archetypes, which is something that can happen when calling .</p><p>Using  can also significantly improve performance. This is partly because the operation to get an existing component is faster than the operation to ensure that a component exists.</p><p>A larger improvement comes from another optimization, which is that Flecs no longer inserts  commands if there are no  hooks/observers. In applications that use  frequently this can make a huge difference: it cut frame times in half in one of the Flecs demos.</p><p>Some games can have large variations in the number of objects they are simulating. Flecs holds on to memory once it‚Äôs been allocated to avoid constantly freeing and reallocating memory. This can however cause a game to use a lot more memory than what is required for the number of objects in a scene.</p><p>Usually this is not a problem. When someone is playing a game it is typically the only thing that‚Äôs happening on a machine, so as long we don‚Äôt exceed the maximum amount of RAM the ECS is allowed to use we‚Äôre good.</p><p>But what if the game is  the only thing that‚Äôs running on the machine? Maybe the process is a game server that coexists with other processes on a server. Maybe the application launches other processes, and needs to run in the background until those other processes finish.</p><p>For those scenarios where the simulation still needs to run but with a greatly reduced number of entities, applications can now shrink the world:</p><p>This frees memory where possible by reclaiming memory from arrays, cleaning up unused tables and more.</p><p>In multithreaded applications it can be difficult to know when the ECS world can be accessed safely, especially if you have a large team of developers. A new feature has been added to help track down scenarios where more than one thread is trying to (illegally) access the world:</p><pre></pre><p>When a thread is done it can release the world so others can use it again:</p><pre></pre><p>When releasing the world a thread can choose to lock it for writing. This only allows threads to read the world, and effectively allows threads to write to a world only when they have exclusive access:</p><pre></pre><h2>Performance tracing hooks</h2><p>Flecs now provides hooks for better integration with profiling tools such as <a href=\"https://github.com/wolfpld/tracy\" rel=\"noopener ugc nofollow\" target=\"_blank\">Tracy</a>. This provides fine-grained visibility in how much time different parts of Flecs applications such as observers and systems are performing.</p><p>The new hooks can be set as callbacks on the OS API:</p><pre></pre><p>There is now <a href=\"https://github.com/flecs-hub/traffic\" rel=\"noopener ugc nofollow\" target=\"_blank\">a new Flecs demo</a> that showcases large numbers of objects with complex behavior. The demo is still a work in progress but it‚Äòs already pretty satisfying to look at:</p><p>This project proved to be as much of an exercise in designing ECS components for optimal CPU cache efficiency, as well as coming up with a set of behaviors that keep traffic flowing at all times. I might do a writeup at some point as there‚Äôs fun takeaways from both.</p><p>Because such a large part of this release was dedicated to performance improvements, I thought it‚Äôd be fun to share a few details on what changed. This will be more technical than these blogs usually are, so if you‚Äôre not interested in nitty-gritty details you can safely skip this.</p><p>In ‚Äú<a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-1-where-are-my-entities-and-components-63d07c7da742\" data-discover=\"true\">where are my entities and components</a>‚Äù I talked about how we can find components on entities using the component index. This allows us to do operations like , which are common and thus performance critical:</p><pre></pre><p>The component index provides us with a general purpose, constant time solution for finding components. In short, it works like this:</p><pre></pre><p>This approach has a big downside: it needs to access at least three distinct memory locations which can easily cause CPU cache misses.</p><p>Flecs v4.0.1 introduced a component lookup array to each table which provides a much more direct path to obtain the component column:</p><pre></pre><p>The size of  is bound to 256 by default to avoid spending too much memory, so if component ids are larger than that we still revert to the old method of going through the component index.</p><p>This simple change sped up  and related operations by 3x!</p><p>Uncached queries use the component index to quickly find all tables that have the components in a query. In v4.1.0 a new bloom filter got introduced that significantly speeds up uncached query evaluation.</p><p>The existing approach uses the component index to match tables. For a  query, the query engine does the following:</p><pre></pre><p>Map lookups are fast enough for many use cases, but they start adding up when they‚Äôre done millions of times per second.</p><p>The new bloom filter avoids having to do many of these lookups. A bloom filter is a bit pattern that can tell us one of two things:</p><ul><li>this table  matches the query</li><li>this table  match the query</li></ul><p>Evaluating the bloom filter looks like this and is super fast:</p><pre></pre><p>We can‚Äôt use the bloom filter for all queries, as we can‚Äôt express things like operators or more advanced query features. Where it can be used though speedups are significant: observer evaluation got faster because the bloom filter in many cases prevented observer query evaluation entirely!</p><h2>Faster flecs::ref validation</h2><p>The  API provides an even faster way to get a component pointer than . It does this by storing a bit of state about the component so that the next time it is fetched we have to do less work. An example:</p><pre></pre><p>Previously this worked by caching the table record (see ‚Äúfaster component lookups‚Äù) on the . This was faster than a , but it does require accessing the table record which could cause cache misses.</p><p>A colleague came up with a clever way to prevent having to do this, and speed up the implementation of  by 2x:</p><ul><li>A ref now directly stores a pointer to the component</li><li>It also stores a ‚Äútable version‚Äù</li><li>Whenever something happens that invalidates a component pointer we increase the table version, which is stored in an array in the world:</li></ul><pre></pre><ul><li>When fetching the component, we check the table version in the ref with the table version in the array:</li></ul><pre></pre><p>Note how we‚Äôre only accessing the  and  here. Because the latter is likely to be hot in the cache, we avoid the vast majority of cache misses, which greatly speeds up  performance!</p><p>Fun fact:  is now as fast as  was before this optimization!</p><h2>Faster component fetching when iterating cached queries</h2><p>A major factor that determines how fast queries evaluate is how quickly we can fetch the iterated components. Flecs has gone through a number of iterations that progressively sped this up.</p><p>Flecs v4.1 introduces a new change that significantly speeds this up for cached queries, with a mechanism that is very similar to the new mechanism used by !</p><p>In short, the query now caches component pointers for each matched table. If a cached query matches , then for each matched table the cache entry will store a pointer to the  column and to the  column.</p><p>Column pointers can become invalid however if a column is resized. To address this each cache entry stores a table version (just like ) that increases when column pointers change. We then do the exact same thing we did for references to make sure the pointers are still valid before returning them to the application.</p><p>This is super efficient because:</p><ul><li>Table columns rarely grow, so we almost never need to revalidate the cached column pointers.</li><li>Table columns always grow together, which means we can use a single version number to check all columns.</li></ul><p>This change by itself can speed up cached query iteration by 2x, but it‚Äôs not the only thing that changed for cached queries:</p><p>Flecs cached queries are packed with features, and over time these increased the size of the cache elements. Things like wildcards, relationship traversal, grouping and sorting all added fields to cache elements. Most queries don‚Äôt use these features however, and so these fields just add dead weight to cache elements.</p><p>Another source of inefficiency is that the query cache was based on a linked list. Iterating the query cache meant doing this (pseudo):</p><pre></pre><p>The nice thing about this was that the iteration code could remain entirely agnostic to query features such as grouping and sorting, which just rewired the  pointers of query cache elements.</p><p>Sadly linked lists come with significant performance drawbacks. Combined with a less than optimal allocation strategy for cache elements, iterating this linked list all but guaranteed tons of cache misses.</p><p>Flecs v4.1 completely refactored the query cache to address these issues:</p><ul><li>Queries now use much (3x) smaller cache elements for simple queries</li><li>The linked list has been replaced with an array</li></ul><p>Combined these changes added up to another 2x performance improvement, and a big reduction in query cache size! These improvements also enabled me to write a much simpler iterator function which also accounted for a large part of the speedup.</p><p>A simple yet effective performance improvement was to force inlining commonly used ECS operations. Compilers use complicated logic to decide which functions should be inlined, and it can be hit or miss on whether they get it right. Since Flecs v4.0.5 performance critical operations now are annotated with <code>__attribute__((always_inline))</code> on clang and gcc.</p><p>This improved performance of operations like  by another 2x.</p><p>I can‚Äôt go over every single improvement in detail or this blog post would become way too long, so here‚Äôs a short callout to some of the other notable performance improvements:</p><ul><li>Uncached queries are 2‚Äì4x faster to create.</li><li>Pipelines run up to 2x faster.</li><li>World creation is 1.4‚Äì2.5x faster</li><li>The performance of empty table cleanup has improved up to 5x.</li><li>Change detection overhead has been reduced by 2x for trivial queries</li><li>C++ systems no longer rely on C code to do query iteration. This avoids function pointer indirection, and makes it much easier for the compiler to reason about and inline C++ system code.</li><li>Empty tables are no longer stored separately from normal tables in the component index and query caches. This speeds up spawn performance considerably (don‚Äôt have to emit empty table events anymore) and greatly simplifies some of the internals.</li><li>Creating and deleting entities can no longer cause query rematching, which eliminates a source of lag spikes in applications that are heavy users of queries with relationship traversal.</li></ul><p>The ink on the v4.1 release isn‚Äôt dry yet but work on the next batch of improvements is already in full swing! Here‚Äôs a few things to look forward to in upcoming releases:</p><ul><li>A new storage optimized for asset hierarchies which in early benchmarks has shown to be an order of magnitude faster than the current implementation.</li><li>Performance improvements to the core data structures used in the component index to speed up uncached query iteration.</li><li>New features and performance improvements for Flecs Script and its reactivity implementation.</li><li>The new non-fragmenting component storage has teased out a nice interface for abstracting component storage. Pluggable storages are again on the horizon!</li><li>More robust reflection/explorer support for complex component types (think reflection for map types &amp; inspector support for collections).</li></ul>","contentLength":17579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lntgg1/flecs_v41_an_entity_component_system_for_cccrust/"},{"title":"Flecs v4.1, an Entity Component System for C/C++/C#/Rust is out!","url":"https://ajmmertens.medium.com/flecs-4-1-is-out-fab4f32e36f6","date":1751241257,"author":"/u/ajmmertens","guid":175479,"unread":true,"content":"<p><a href=\"https://github.com/SanderMertens/flecs\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs</a> is an Entity Component System (<a href=\"https://github.com/SanderMertens/ecs-faq\" rel=\"noopener ugc nofollow\" target=\"_blank\">FAQ</a>) for C and C++ that helps with building games, simulations and more. The core features of Flecs are:</p><ul><li>Store data for  in data structures optimized for CPU cache efficiency and composition-first design</li><li>Builtin support for hierarchies, prefabs and more with <a href=\"https://www.flecs.dev/flecs/md_docs_2Relationships.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>which speed up game code and reduce boiler plate</li><li>An <a href=\"https://www.flecs.dev/explorer\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a><a href=\"https://www.flecs.dev/flecs/md_docs_2Quickstart.html#addons]\" rel=\"noopener ugc nofollow\" target=\"_blank\"></a>to profile, visualize, document and debug projects</li></ul><p>Flecs is fully open source and licensed under the MIT license. If you‚Äôd like to support the project, consider giving it a Ô∏èÔ∏è‚≠êÔ∏è on the Github page!</p><p>Since v4.0 there have been exciting updates from games that use Flecs!</p><p>Congrats to the Tempest Rising team on the successful release of their game! Command distinct factions in a desperate struggle for power and resources in Tempest Rising ‚Äî a classic RTS set on Earth after a nuclear war (Unreal Engine 5).</p><h2>Announced: Resistance is Brutal</h2><p>Resistance is Brutal is Vampire Survivors meets Running Man with a bit of Rick and Morty thrown in. Try to survive as you battle the enemy hordes with brutal abilities before going out in a blaze of glory (Unreal Engine 5).</p><h2>Announced: Age of Respair</h2><p>Age of Respair is a medieval strategy castle-builder. Build massive castles with fortifications while managing resources and production chains. Assemble a large army and lay siege to enemy castles. Lead your people and bring respair to your kingdom (Unreal Engine 5).</p><p>You‚Äôve been hired by an elite organization that cooks for gods. Farm, automate, and cook gourmet meals to appease divine beings or face extinction (Unreal Engine 5).</p><p>Ascendant is a large scale open world voxel RPG inspired by Minecraft and Daggerfall. it spawns the player into a procedurally generated landscape with various biomes and features such as cities and ruins. It creates animals and enemies in the map, and you can gather resources, craft them into other ones, and fight enemies with sword, bow, or magic.</p><p>The developer behind this project is also the author of <a href=\"https://vkguide.dev/\" rel=\"noopener ugc nofollow\" target=\"_blank\">vkguide</a>, which is one of the best resources for learning Vulkan. There is a <a href=\"https://vkguide.dev/docs/ascendant\" rel=\"noopener ugc nofollow\" target=\"_blank\">chapter</a> dedicated specifically to Ascendant, make sure to check it out!</p><p>A vampire survivor like game built in Flecs. What‚Äôs awesome is that the developer of the game has made its source code available! Check it out here: <a href=\"https://github.com/ptidejteam/ecs-survivors\" rel=\"noopener ugc nofollow\" target=\"_blank\">https://github.com/ptidejteam/ecs-survivors</a></p><p>There are still more projects cooking that I haven‚Äôt listed here. If you want to stay up to date on the progress of Flecs projects, check out the <a href=\"https://discord.com/invite/caR2WmY\" rel=\"noopener ugc nofollow\" target=\"_blank\">showcase channel</a> of the Flecs Discord!</p><p>The v4.1 release builds on the new architecture laid out by v4.0 and comes with significant performance improvements across many parts of the library, in addition to new features that are already proving popular with developers.</p><p>Here are some of the most notable performance improvements (as measured by the <a href=\"https://github.com/SanderMertens/ecs_benchmark\" rel=\"noopener ugc nofollow\" target=\"_blank\">Flecs benchmarking suite</a>):</p><ul><li>: 5x faster than v4.0</li><li>: 5x faster than v4.0</li><li>: 5‚Äì10x faster than v4.0</li><li>: 1.5‚Äì2x faster than v4.0</li><li>: 2‚Äì4x faster than v4.0</li><li>: 5‚Äì10x faster than v4.0</li><li>: 40x faster than v4.0, 6x faster than v3.2.12</li></ul><p>Flecs now also uses a lot less memory. The minimum footprint of a Flecs world has decreased 5x, and overall RAM consumption can be as much as two times lower depending on the application! Scroll down to the ‚ÄúPerformance‚Äù section to see how these improvements were achieved.</p><p>As with every release, a lot of effort has gone into testing and bugfixing. Flecs now has 11.000 test cases, an increase of 2500 test cases since v4.0!</p><p>Here‚Äôs an overview of the highlights since v4.0:</p><h2>Non-fragmenting components</h2><p>Flecs is an <a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-2-archetypes-and-vectorization-fe21690805f9\" data-discover=\"true\">archetype-style ECS</a>, which in short means it optimizes the storage at runtime to allow for fast iteration of multiple components at the same time. The tradeoff that comes with this design however is that adding and removing components to entities can be expensive.</p><p>The alternative to archetypes is a sparse set or independent-storage based design. This design has the opposite tradeoff: adding/removing components is cheap, but iterating multiple components at the same time is more expensive.</p><p>Flecs v4.1 is the first* open source ECS that <a href=\"https://www.flecs.dev/flecs/md_docs_2ComponentTraits.html#dontfragment-trait\" rel=\"noopener ugc nofollow\" target=\"_blank\">supports both</a>! Switching between storages is easy, just add the  trait to a component during registration:</p><pre></pre><p><em>* many people have pointed out to me that Bevy ECS has had sparse components for a long time. That‚Äôs correct, but adding a sparse component to an entity in Bevy changes its archetype, so they are *not* non-fragmenting!</em></p><h2>Flecs Script improvements</h2><p>Many new features and performance improvements have been added to Flecs Script that allow for the creation of more complex scenes. Here‚Äôs an overview of the most notable changes:</p><ul><li>A new (much faster) expression parser</li><li>A new  addon</li><li>Entity name expressions ()</li></ul><p>An example script that uses some of the new features:</p><pre></pre><p>The following scene is constructed entirely from primitive rectangle and box shapes (link to the code below the image):</p><p>Often when working with hierarchies the order in which children are processed is important. This is especially true for UIs, where the order of widgets can determine how they appear on screen.</p><p>Because of how the Flecs storage works internally, the order in which children are iterated can change when components are added or removed. To improve the usability of builtin hierarchies when working with UIs and similar use cases, a new  trait has been introduced:</p><pre></pre><p>An additional operation has been introduced that makes it possible to change the order of children after they have been created:</p><pre></pre><h2>World local component ids</h2><p>A common annoyance when working with multiple Flecs worlds was that if components were shared between worlds, their ids had to match. This could sometimes lead to unexpected and confusing errors, and was not great for developer UX.</p><p>Since v4.0.4 component ids in the C++ API are now fully local to a world, which means that different worlds can have different ids for the same component.</p><h2>get()/try_get() API redesign</h2><p>The C++  APIs have been redesigned to use the following pattern:</p><pre></pre><p>This expresses intent more clearly and can reduce boilerplate as code no longer has to include defensive checks on whether  returns a . It also allows most code to work with references, which many users prefer.</p><p>: this is a breaking change that affects a lot of code. An easy way to migrate is to replace all occurrences of  in a project with .</p><p>A new  API has been added to the C++ API which can be used as an alternative to . The  operation will add a component to an entity if the entity didn‚Äôt have it yet, whereas  only assigns existing components and will panic if the entity didn‚Äôt have the component yet.</p><p>In addition to expressing intent more clearly,  also guarantees that it will never move entities to different archetypes, which is something that can happen when calling .</p><p>Using  can also significantly improve performance. This is partly because the operation to get an existing component is faster than the operation to ensure that a component exists.</p><p>A larger improvement comes from another optimization, which is that Flecs no longer inserts  commands if there are no  hooks/observers. In applications that use  frequently this can make a huge difference: it cut frame times in half in one of the Flecs demos.</p><p>Some games can have large variations in the number of objects they are simulating. Flecs holds on to memory once it‚Äôs been allocated to avoid constantly freeing and reallocating memory. This can however cause a game to use a lot more memory than what is required for the number of objects in a scene.</p><p>Usually this is not a problem. When someone is playing a game it is typically the only thing that‚Äôs happening on a machine, so as long we don‚Äôt exceed the maximum amount of RAM the ECS is allowed to use we‚Äôre good.</p><p>But what if the game is  the only thing that‚Äôs running on the machine? Maybe the process is a game server that coexists with other processes on a server. Maybe the application launches other processes, and needs to run in the background until those other processes finish.</p><p>For those scenarios where the simulation still needs to run but with a greatly reduced number of entities, applications can now shrink the world:</p><p>This frees memory where possible by reclaiming memory from arrays, cleaning up unused tables and more.</p><p>In multithreaded applications it can be difficult to know when the ECS world can be accessed safely, especially if you have a large team of developers. A new feature has been added to help track down scenarios where more than one thread is trying to (illegally) access the world:</p><pre></pre><p>When a thread is done it can release the world so others can use it again:</p><pre></pre><p>When releasing the world a thread can choose to lock it for writing. This only allows threads to read the world, and effectively allows threads to write to a world only when they have exclusive access:</p><pre></pre><h2>Performance tracing hooks</h2><p>Flecs now provides hooks for better integration with profiling tools such as <a href=\"https://github.com/wolfpld/tracy\" rel=\"noopener ugc nofollow\" target=\"_blank\">Tracy</a>. This provides fine-grained visibility in how much time different parts of Flecs applications such as observers and systems are performing.</p><p>The new hooks can be set as callbacks on the OS API:</p><pre></pre><p>There is now <a href=\"https://github.com/flecs-hub/traffic\" rel=\"noopener ugc nofollow\" target=\"_blank\">a new Flecs demo</a> that showcases large numbers of objects with complex behavior. The demo is still a work in progress but it‚Äòs already pretty satisfying to look at:</p><p>This project proved to be as much of an exercise in designing ECS components for optimal CPU cache efficiency, as well as coming up with a set of behaviors that keep traffic flowing at all times. I might do a writeup at some point as there‚Äôs fun takeaways from both.</p><p>Because such a large part of this release was dedicated to performance improvements, I thought it‚Äôd be fun to share a few details on what changed. This will be more technical than these blogs usually are, so if you‚Äôre not interested in nitty-gritty details you can safely skip this.</p><p>In ‚Äú<a rel=\"noopener\" href=\"https://ajmmertens.medium.com/building-an-ecs-1-where-are-my-entities-and-components-63d07c7da742\" data-discover=\"true\">where are my entities and components</a>‚Äù I talked about how we can find components on entities using the component index. This allows us to do operations like , which are common and thus performance critical:</p><pre></pre><p>The component index provides us with a general purpose, constant time solution for finding components. In short, it works like this:</p><pre></pre><p>This approach has a big downside: it needs to access at least three distinct memory locations which can easily cause CPU cache misses.</p><p>Flecs v4.0.1 introduced a component lookup array to each table which provides a much more direct path to obtain the component column:</p><pre></pre><p>The size of  is bound to 256 by default to avoid spending too much memory, so if component ids are larger than that we still revert to the old method of going through the component index.</p><p>This simple change sped up  and related operations by 3x!</p><p>Uncached queries use the component index to quickly find all tables that have the components in a query. In v4.1.0 a new bloom filter got introduced that significantly speeds up uncached query evaluation.</p><p>The existing approach uses the component index to match tables. For a  query, the query engine does the following:</p><pre></pre><p>Map lookups are fast enough for many use cases, but they start adding up when they‚Äôre done millions of times per second.</p><p>The new bloom filter avoids having to do many of these lookups. A bloom filter is a bit pattern that can tell us one of two things:</p><ul><li>this table  matches the query</li><li>this table  match the query</li></ul><p>Evaluating the bloom filter looks like this and is super fast:</p><pre></pre><p>We can‚Äôt use the bloom filter for all queries, as we can‚Äôt express things like operators or more advanced query features. Where it can be used though speedups are significant: observer evaluation got faster because the bloom filter in many cases prevented observer query evaluation entirely!</p><h2>Faster flecs::ref validation</h2><p>The  API provides an even faster way to get a component pointer than . It does this by storing a bit of state about the component so that the next time it is fetched we have to do less work. An example:</p><pre></pre><p>Previously this worked by caching the table record (see ‚Äúfaster component lookups‚Äù) on the . This was faster than a , but it does require accessing the table record which could cause cache misses.</p><p>A colleague came up with a clever way to prevent having to do this, and speed up the implementation of  by 2x:</p><ul><li>A ref now directly stores a pointer to the component</li><li>It also stores a ‚Äútable version‚Äù</li><li>Whenever something happens that invalidates a component pointer we increase the table version, which is stored in an array in the world:</li></ul><pre></pre><ul><li>When fetching the component, we check the table version in the ref with the table version in the array:</li></ul><pre></pre><p>Note how we‚Äôre only accessing the  and  here. Because the latter is likely to be hot in the cache, we avoid the vast majority of cache misses, which greatly speeds up  performance!</p><p>Fun fact:  is now as fast as  was before this optimization!</p><h2>Faster component fetching when iterating cached queries</h2><p>A major factor that determines how fast queries evaluate is how quickly we can fetch the iterated components. Flecs has gone through a number of iterations that progressively sped this up.</p><p>Flecs v4.1 introduces a new change that significantly speeds this up for cached queries, with a mechanism that is very similar to the new mechanism used by !</p><p>In short, the query now caches component pointers for each matched table. If a cached query matches , then for each matched table the cache entry will store a pointer to the  column and to the  column.</p><p>Column pointers can become invalid however if a column is resized. To address this each cache entry stores a table version (just like ) that increases when column pointers change. We then do the exact same thing we did for references to make sure the pointers are still valid before returning them to the application.</p><p>This is super efficient because:</p><ul><li>Table columns rarely grow, so we almost never need to revalidate the cached column pointers.</li><li>Table columns always grow together, which means we can use a single version number to check all columns.</li></ul><p>This change by itself can speed up cached query iteration by 2x, but it‚Äôs not the only thing that changed for cached queries:</p><p>Flecs cached queries are packed with features, and over time these increased the size of the cache elements. Things like wildcards, relationship traversal, grouping and sorting all added fields to cache elements. Most queries don‚Äôt use these features however, and so these fields just add dead weight to cache elements.</p><p>Another source of inefficiency is that the query cache was based on a linked list. Iterating the query cache meant doing this (pseudo):</p><pre></pre><p>The nice thing about this was that the iteration code could remain entirely agnostic to query features such as grouping and sorting, which just rewired the  pointers of query cache elements.</p><p>Sadly linked lists come with significant performance drawbacks. Combined with a less than optimal allocation strategy for cache elements, iterating this linked list all but guaranteed tons of cache misses.</p><p>Flecs v4.1 completely refactored the query cache to address these issues:</p><ul><li>Queries now use much (3x) smaller cache elements for simple queries</li><li>The linked list has been replaced with an array</li></ul><p>Combined these changes added up to another 2x performance improvement, and a big reduction in query cache size! These improvements also enabled me to write a much simpler iterator function which also accounted for a large part of the speedup.</p><p>A simple yet effective performance improvement was to force inlining commonly used ECS operations. Compilers use complicated logic to decide which functions should be inlined, and it can be hit or miss on whether they get it right. Since Flecs v4.0.5 performance critical operations now are annotated with <code>__attribute__((always_inline))</code> on clang and gcc.</p><p>This improved performance of operations like  by another 2x.</p><p>I can‚Äôt go over every single improvement in detail or this blog post would become way too long, so here‚Äôs a short callout to some of the other notable performance improvements:</p><ul><li>Uncached queries are 2‚Äì4x faster to create.</li><li>Pipelines run up to 2x faster.</li><li>World creation is 1.4‚Äì2.5x faster</li><li>The performance of empty table cleanup has improved up to 5x.</li><li>Change detection overhead has been reduced by 2x for trivial queries</li><li>C++ systems no longer rely on C code to do query iteration. This avoids function pointer indirection, and makes it much easier for the compiler to reason about and inline C++ system code.</li><li>Empty tables are no longer stored separately from normal tables in the component index and query caches. This speeds up spawn performance considerably (don‚Äôt have to emit empty table events anymore) and greatly simplifies some of the internals.</li><li>Creating and deleting entities can no longer cause query rematching, which eliminates a source of lag spikes in applications that are heavy users of queries with relationship traversal.</li></ul><p>The ink on the v4.1 release isn‚Äôt dry yet but work on the next batch of improvements is already in full swing! Here‚Äôs a few things to look forward to in upcoming releases:</p><ul><li>A new storage optimized for asset hierarchies which in early benchmarks has shown to be an order of magnitude faster than the current implementation.</li><li>Performance improvements to the core data structures used in the component index to speed up uncached query iteration.</li><li>New features and performance improvements for Flecs Script and its reactivity implementation.</li><li>The new non-fragmenting component storage has teased out a nice interface for abstracting component storage. Pluggable storages are again on the horizon!</li><li>More robust reflection/explorer support for complex component types (think reflection for map types &amp; inspector support for collections).</li></ul>","contentLength":17579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lntbxt/flecs_v41_an_entity_component_system_for_cccrust/"},{"title":"[P] Code for Fine-Tuning FLUX.1-dev Explained Step by Step With Comments","url":"https://www.reddit.com/r/MachineLearning/comments/1lnt9za/p_code_for_finetuning_flux1dev_explained_step_by/","date":1751241096,"author":"/u/FallMindless3563","guid":176907,"unread":true,"content":"<p>I was having trouble finding a simple, self contained example of Fine-Tuning FLUX.1-dev with explanation of all the components, so I decided to create one. </p><p>There were examples in HuggingFace diffusers <a href=\"https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_flux.py\">examples/dreambooth/train_dreambooth_lora_flux.py</a> (which didn't work out of the gate for me) and <a href=\"https://github.com/ostris/ai-toolkit\">AI-Toolkit</a> which worked well, but had way too many nested if-statements to fully see what was going on under the hood. I took inspiration from both, but cleaned up the code so it was easier to read and worked out of the gate.</p><p>The code was written in a <a href=\"https://marimo.io/\">Marimo Notebook</a> which I'm enjoying lately for developing simple training scripts. </p>","contentLength":631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] How should I respond to reviewers when my model is worse than much larger models?","url":"https://www.reddit.com/r/MachineLearning/comments/1lnsph5/d_how_should_i_respond_to_reviewers_when_my_model/","date":1751239455,"author":"/u/AdministrativeRub484","guid":176656,"unread":true,"content":"<p>I got a review asking to compare my submission paper with more recent models. The models were not even out 3 months before the submission so by ACL rules I should not have to compare them with my model because it is contemporary.</p><p>Nevertheless I have ran comparisons and my model is much much worse... Why? I'm using a model doing the same thing but 32x smaller, used almost 1/10 of the data they used, etc... I am severely resource constrained and cannot compete in terms of scale, but I still think that my paper makes an important contribution that if we were to match the other models scale we would get better results.</p><p>What should I do? Should I report results that show other models are better and risk the reviewers lower their scores? I kinda just want to explain the authors that the scale is completely different and other factors make it a very unfair comparison, but they might just not care...</p><p>I have a 2.5 average score and really wanted to try to raise it to make it at least into findings, but I honestly don't know how to defend against not having as many resources as top labs/unis...</p>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Karpenter NodePool Strategies: Balancing Cost, Reliability & Tradeoffs","url":"https://www.reddit.com/r/kubernetes/comments/1lnsns8/karpenter_nodepool_strategies_balancing_cost/","date":1751239320,"author":"/u/Separate-Welcome7816","guid":175446,"unread":true,"content":"<li><p> Best for stability and predictability, but comes with higher costs. Ideal for critical workloads that cannot afford interruptions or require guaranteed compute availability.</p></li><li><p> Great for cost savings ‚Äî often 70-90% cheaper than On-Demand. However, the tradeoff is reliability. Spot capacity can be reclaimed by AWS with little warning, which means workloads must be resilient to node terminations.</p></li><li><p><strong>Mixed Strategy (80% Spot / 20% On-Demand)</strong> The sweet spot for many production environments. This setup blends the cost savings of Spot with the fallback reliability of On-Demand. Karpenter can intelligently schedule critical pods on On-Demand nodes and opportunistic workloads on Spot instances, minimizing risk while maximizing savings.</p></li>","contentLength":733,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"colormatrix\". A very colored cmatrix close that uses a random array of colors.","url":"https://www.reddit.com/r/linux/comments/1lns8om/colormatrix_a_very_colored_cmatrix_close_that/","date":1751238157,"author":"/u/Beautiful_Crab6670","guid":175447,"unread":true,"content":"<p>Eh...just a little \"something\" I came up with in my free time. It picks a \"true\" random color and use it to draw a drop. Should be compatible with terminals that use true color. So expect a \"puke\" of colors if you use it on such terminal.<a href=\"https://gitlab.com/gee.8ruhs/writteninc/-/raw/main/colormatrix.c\">Click here</a> to grab the code. Then compile it with \"gcc colormatrix.c -o colormatrix -static -O3 -Wall\".</p>","contentLength":341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"godump - v1.3.0 - New Release","url":"https://i.postimg.cc/FF0k8Fyk/godump.png","date":1751237481,"author":"/u/cmiles777","guid":175428,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lnrzwx/godump_v130_new_release/"},{"title":"Create Jobs and CronJobs via Ui using kube composer","url":"https://www.reddit.com/r/kubernetes/comments/1lnrofd/create_jobs_and_cronjobs_via_ui_using_kube/","date":1751236623,"author":"/u/same7ammar","guid":175426,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/same7ammar\"> /u/same7ammar </a>","contentLength":33,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"my first open-source project","url":"https://www.reddit.com/r/golang/comments/1lnqgqj/my_first_opensource_project/","date":1751233384,"author":"/u/MoonOwlMage","guid":175429,"unread":true,"content":"<p>Hey all, I've been working on a service monitoring tool called Heimdall and wanted to share it with the community. It's a lightweight service health checker written in pure Go with zero external dependencies. More information you can find in README.</p><p>It is my first project, that I want to be an open-source, so I'm looking forward for your feedback, feature offers and pull requests. It was started as personal project for my job, but I thought, that it can be useful for others.</p><p>p.s project in dev mode, so I'll add more features in future</p>","contentLength":538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-Minute Daily AI News 6/29/2025","url":"https://www.reddit.com/r/artificial/comments/1lnq3vc/oneminute_daily_ai_news_6292025/","date":1751232440,"author":"/u/Excellent-Target-847","guid":176322,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Adventures with Kubuntu, KVM, Windows 11 Pro and My 2011 Macbook Air","url":"https://www.reddit.com/r/linux/comments/1lnpyo7/my_adventures_with_kubuntu_kvm_windows_11_pro_and/","date":1751232062,"author":"/u/ScubadooX","guid":176324,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"First time token access to gmail is not working","url":"https://www.reddit.com/r/golang/comments/1lnpxqm/first_time_token_access_to_gmail_is_not_working/","date":1751232000,"author":"/u/pepiks","guid":175481,"unread":true,"content":"<p>I tried play with Gmail API using Go. So I follow tutorial:</p><p>I generate JSON file with credits, setup app as suggested, even add scope manually on Google, creds file saved in workdir. When I run app it open URL:</p><p>So then it stuck on code:</p><p><code>func getTokenFromWeb(config *oauth2.Config) *oauth2.Token {</code><strong>authURL := config.AuthCodeURL(\"state-token\", oauth2.AccessTypeOffline)</strong><strong>fmt.Printf(\"Go to the following link in your browser then type the \"+</strong><strong>\"authorization code: \\n%v\\n\", authURL)</strong></p><p><code>if _, err := fmt.Scan(&amp;authCode); err != nil {</code><code>log.Fatalf(\"Unable to read authorization code: %v\", err)</code></p><p><code>tok, err := config.Exchange(context.TODO(), authCode)</code><code>log.Fatalf(\"Unable to retrieve token from web: %v\", err)</code></p><p>Logic seems fine, but it looks like wrong setup URI code to follow. On browser I have buttons with access to scope, but when I agree I got side is unreachable. I use code provided byt Google and I don't know idea how move forward from this point. My access is configuret as Desktop App (the same as in tutorial).</p><p>At the end I want add permision to my app as I do for email client once and after that run app and do stuff like saving attachments, add labels etc.</p>","contentLength":1141,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recommended DEs that aren't as common","url":"https://www.reddit.com/r/linux/comments/1lnp3u6/recommended_des_that_arent_as_common/","date":1751229924,"author":"/u/emrldgh","guid":175404,"unread":true,"content":"<p>I'd like to know what everyone's recommendation is for a DE/WM that not everyone may know about or often consider. Anything that isn't KDE, GNOME, or any super common WMs like Hyprland or Sway. These may not be considered very common, but I'd like to hear thoughts on Budgie and Cutefish, I was looking at them and they look neat but what do you guys think? What do you use?</p>","contentLength":374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Review clearly used an LLM, should I report it to AC?","url":"https://www.reddit.com/r/MachineLearning/comments/1lnoqmm/d_review_clearly_used_an_llm_should_i_report_it/","date":1751229015,"author":"/u/AdministrativeRub484","guid":175480,"unread":true,"content":"<p>This review gave me 1.5 in ACL and calls GRPO Generalized Reward Preference Optimization, which is what ChatGPT thinks GRPO is... It also says my work is the first one to use GRPO in my domain while it is not (and we talk about this in the introduction) and says we are missing some specific evaluations, which are present in the appendix and says we did not justify a claim well enough, which is very well known in my domain but when asking ChatGPT about it it says it does not know about it...</p><p>It feels like the reviewer just wanted to give me a bad review and asked an LLM to write a poor review. He clearly did not even check the output because literally everyone knows GRPO stands for Group Relative Policy Optimization...</p><p>Other than reply to the reviewer while pretending I did not know he/she used ChatGPT, what else can I do? My other reviews were both 3, so I really want to get rid of this review if possible...</p>","contentLength":919,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a CPU emulator with its own assembler in java","url":"https://github.com/LPC4/Neptune-32","date":1751228373,"author":"/u/ColdRepresentative91","guid":175402,"unread":true,"content":"<p>Over the past few days I‚Äôve been building a custom 32-bit CPU emulator in java that comes with its own assembler and instruction set. I started on the project for fun, and because I wanted to learn more about CPU architecture and compilers.</p><ul><li>32-bit little-endian architecture with 32 general-purpose registers</li><li>Memory-mapped IO, stack and heap, ROM for syscalls, and RAM/VRAM simulation</li><li>Malloc and Free implemented syscalls (not tested properly)</li><li>128√ó128 RGBA framebuffer + keyboard and console IO devices</li><li>Instruction set includes arithmetic, logic, branches, system calls, and shifts</li><li>Assembler supports labels, immediate values, register addressing, macros, but still expanding</li></ul><p>I‚Äôd love to hear what you think about this project: ideas, critiques, or even some features you‚Äôd like to see added. Would really appreciate any tips, feedback, or things I could do better.</p>","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnohq1/i_built_a_cpu_emulator_with_its_own_assembler_in/"},{"title":"Free App Hidden Gem: Libreoffice - Full Featured Microsoft Office Alternative","url":"https://youtu.be/bhid0z2JUec?feature=shared","date":1751226910,"author":"/u/Putrid_Draft378","guid":175368,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lnnwlf/free_app_hidden_gem_libreoffice_full_featured/"},{"title":"Multus on Multiple Nodes with UDP broadcast","url":"https://www.reddit.com/r/kubernetes/comments/1lnnt1y/multus_on_multiple_nodes_with_udp_broadcast/","date":1751226657,"author":"/u/rickreynoldssf","guid":175360,"unread":true,"content":"<p>Hello. I've been banging my head against my desk trying to setup multus with ipvlan on AKS. I run a multi node cluster. I need to create multiple pods that create a private network <strong>with all pods on the same subnet</strong> and likely , where they will send UDP broadcasts to each other. </p><p>I need to replicate that many times so there's 1-n groups of pods with their private networks. I also need the pods to have the default host network, hence Multus. </p><p>With a single node and macvlan this all works great but with ipvlan and multiple nodes I cannot communicate across the nodes on the private network. </p><p>Are there any examples / tutorials / docs on doing this?</p>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code is skimmed more often than it is written, so it should be clear at a glance","url":"https://jelv.is/blog/Writing-Code-To-Be-Read-at-a-Glance/","date":1751225471,"author":"/u/tikhonjelvis","guid":175365,"unread":true,"content":"<section><p>In software engineering circles, there is a common adage: ‚Äúcode is read more than it is written‚Äù. But this is not the whole picture! <strong>Code is skimmed more than it is read.</strong></p><p>We read code more than we write it because we spend more time maintaining than we do writing from scratch. A line of code, once written, still has a long and storied life ahead of it. You write code once and then return to it multiple times, fixing bugs, adding features, refactoring. To do this you have to change the existing code‚Äîand to change it you have to read and understand it. Not just you; future programmers will also work on the code, pursuing their own goals, operating under different constraints.</p><p>For every part of the code you need to revisit to in depth, there will be dozens of related parts you‚Äôre not touching directly. You need to navigate through the codebase to find relevant code and you need to track surrounding code for context. You don‚Äôt have to understand the related parts of code exactly; just what they are supposed to do. You do this not by reading the code in detail‚Äînobody has the time or working memory to keep a whole codebase in their head!‚Äîbut by scanning through code quickly and getting just the gist.</p><p>This is a multiplicative relationship. Just as you end up reading code multiple times for each time you write it, you end up skimming multiple times for each piece you read.</p><p><strong>Writing code you can understand  is at least as important as writing code that you can read at all.</strong></p></section><div><p>What does writing code that‚Äôs easy to read at a glance actually involve? To me, it comes down to thinking about the  of my code. The structure of the code should give you a quick idea of what it‚Äôs supposed to do.</p><p>An immediate consequence of caring out the shape of your code is that related code  and unrelated code ‚Äîregardless of implementation. How your code looks is a key affordance for guiding the reader‚Äôs attention without forcing them to read the code in detail.</p><p>Verbose identifiers obscure the shape of your code. This doesn‚Äôt mean you should  use verbose names in your code, just that you should be restrained and tasteful. If an identifier comes from far away‚Äîa function from a logically distant module, say‚Äîgiving it a descriptive name can outweigh the downsides.</p><p>As an illustrative‚Äîif extreme‚Äîexample, let‚Äôs compare three versions of the same logic. Here‚Äôs some Java code using <a href=\"https://docs.oracle.com/javase/8/docs/api/java/math/BigDecimal.html\">BigDecimal</a>:</p><div><pre><code></code></pre></div><p>Java does not have operator overloading. If it did, the code might look like this instead:</p><div><pre><code></code></pre></div><p>Both of these implement the same mathematical formula:</p><p>This is about as simple as a polynomial gets, but even that wasn‚Äôt immediately clear from the first version of the code: you‚Äôd have to pay attention to read the sequence of named method calls to understand what was going on. The version with overloaded operators is a real contrast, although it still requires more attention than the math notation. If the first snippet is like reading a paragraph, the second snippet is like reading a short sentence and the math notation is like reading a single word.</p><p>With the math notation, we can  tell we‚Äôre looking at a polynomial just from the shape. The information we care about for polynomials are the terms; the addition and multiplication holding everything together is more like an implementation detail. The math notation reflects this by visually grouping together the information for each term and using an operator (+) as ‚Äúpunctuation‚Äù to pull it together. Addition is part of what makes the polynomial  but, once we know what we‚Äôre looking at, it can fade into the background.</p><p>People are naturally good at tracking context. We can see this with natural language all the time: the same word or phrase can have somewhat different‚Äîor sometimes radically different‚Äîmeanings in different contexts, but this is so natural that, once people are used to it, they barely notice. What was the last time you thought about how red wine is actually purple, or that, for programmers, ‚Äústrings‚Äù and ‚Äúthreads‚Äù have absolutely nothing to do with each other?</p><p>Context sensitive meanings aren‚Äôt free. The way words in natural languages have different meanings (<a href=\"https://en.wikipedia.org/wiki/Polysemy\">polysemy</a>) is a real obstacle for early language acquisition. But part of the reason that polysemy is difficult is that <em>native speakers do not even notice they are relying on it</em>! Once you get comfortable in a language‚Äîwhether a totally different language, or just the jargon and conventions of a new social group‚Äîtracking the meaning of words based on context becomes . The reason natural languages have polysemy is that relying on context takes  mental energy than communicating in verbose, fully-explicit phrases.</p><p>We can take advantage of this natural tendency in programming. If I‚Äôm working on a module that is implementing an HTTP client for the Stripe, I‚Äôm going to be fine using  to mean ‚Äúsend an authenticated HTTP GET request to Stripe‚Äù. But in a broader, less Stripe-specific context, I would want to write something like  instead.</p><p>Some of your code is the ‚Äúmeat‚Äù of your expression, the logic that matters for whatever you are doing. The rest is more like plumbing‚Äîcode that we need to keep everything working but less significant in any  instance. Think of type conversions, control flow, error propagation, configuration management‚Ä¶ Sometimes fixing a bug will hinge on how a specific config value flows into your function but, most of the time, you care far more about .</p><p>Quickly distinguishing plumbing code from logic is key for understanding and navigating code quickly. When you‚Äôre scanning through a codebase, you can ignore plumbing code altogether. I‚Äôve found this is where certain ‚Äúcontroversial‚Äù language features like macros, overloaded operators and control-flow abstractions.</p><p>As an example, using infix operators for plumbing constructs makes the plumbing visually distinct from ‚Äúnormal‚Äù identifiers while also giving your code some additional visual structure by naturally organizing the expression up into groups.</p><p>After a while, plumbing operators start to fade into the background when you‚Äôre scanning through code. Squint a bit, and you start seeing similarities between code that might be doing the same thing in different contexts. Consider the  operators in Haskell: they let us apply functions over values in some functor (like  and ) in a way that‚Äôs immediately reminiscent of normal function application:</p><div><pre><code></code></pre></div><p>This style also lets us see how operators naturally group code together:</p><div><pre><code></code></pre></div><p>Applicative notation might read like line noise to the uninitiated‚Äîand, honestly, it‚Äôs not exactly the best example of clean plumbing code‚Äîbut, once you‚Äôre comfortable in Haskell, it just melts away. (Which is also 100% true of Lisp‚Äôs parentheses! Write enough Lisp and you stop seeing them. It is uncannily like the ‚ÄúI don‚Äôt see code‚Äù scene from .)</p><p>One perspective I‚Äôve found useful is to think about the minimum amount of information an expression  contain. A polynomial in a single variable, for example, only really needs its coefficients. A web route needs the route, the methods it supports, any variables it takes and the variables‚Äô types.</p><p>Anything else is unnecessary from a raw information point of view. We might still  additional code‚Äîfor plumbing, for structure or just as an implementation detail‚Äîbut, for thinking about API design, we want to be able to distinguish the core information the code is conveying from everything else. ‚ÄúEverything else‚Äù may be useful for organizing our code, but it might also be nothing more than unavoidable‚Äîor, depressingly often, completely unforced‚Äîboilerplate.</p><p><strong>Boilerplate makes code harder to read at a glance.</strong></p><p>As a general guide, I try to eliminate more and more of the inessential code as I repeat a particular kind of expression more and more. In extreme cases, a table layout might be the most readable option if you have a whole bunch of structured rows of code repeating.</p></div><section><p>Math notation‚Äîwith its longer history of evolution and development than programming languages‚Äîis a great example of notation that can be read at a glance.</p><p>Compare the following two ways of writing the same expression:</p><p>This is an illustrative, if exaggerated, example.</p><p>Integral notation might be unfamiliar to a beginner, but it is wonderfully efficient. You can tell what the expression represents  thanks to the integral sign and the equation layout. Imagine squinting until you see the general outline of  integral expression‚Äîthat‚Äôs the same principle I talk about as the ‚Äúshape of the code‚Äù.</p><p>It‚Äôs easy to quickly identify parts of the equation to figure out what‚Äôs going on: the limits are distinct from the equation itself and distinct from the variable of integration (ie ). The polynomial itself continues the same theme:  as an operator gives structure to the polynomial, emphasizing its nature as a . In a sense, the  is just plumbing that gets out of our way so that we can identify the content  to this polynomial (namely the coefficients and degree).</p><p>The paragraph, on the other hand, has the advantage of being readable by anyone, even if they aren‚Äôt familiar with notation for integrals. But it has a fatal flaw: you  to read it. Every time. We have to read the text word-by-word to understand that it describes an integral and to see what the limits and function being integrated are. We can‚Äôt even tell that this is an integral of a polynomial without close reading!</p><p>If this paragraph were surrounded by other paragraphs, we wouldn‚Äôt be able to tell it apart from any other prose. I often start reading papers by skipping explanations and looking at figures and equations until I find what I need‚ÄîI wouldn‚Äôt be able to do that without special notation and visual structure.</p></section><div><p>Good code is, ultimately, a human factors problem. We want to understand how people interact with code‚Äîhow they read, write, skim, navigate, modify, reuse and repurpose code‚Äîand then write and organize our code in a style that makes these interactions as easy and natural as possible.</p><p>Writing code to be read at a glance is just part of the story, but it‚Äôs something I‚Äôve found useful and important. Code I‚Äôve written keeping this principle in mind just feels . And that, fuzzy as it sounds, makes a real, practical difference to how much time, energy and focus it takes to work on a codebase. Making code skimmable is nowhere near the most important aspect of making code pleasant and effective, but it does matter, and I have not seen people explicitly talking about it.</p><p>I don‚Äôt have much advice for putting these ideas into action. For me, just realizing that I valued code I could read at a glance and keeping that in mind when I wrote future code was all it took. I quickly developed the right habits and now I don‚Äôt have to think about it.</p></div>","contentLength":10838,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnnbyc/code_is_skimmed_more_often_than_it_is_written_so/"},{"title":"[Media] I built ‚ÄúDecide‚Äù ‚Äì a role and condition-based permission engine for Rust (and also JS/TS)","url":"https://www.reddit.com/r/rust/comments/1lnn6jt/media_i_built_decide_a_role_and_conditionbased/","date":1751225089,"author":"/u/aetheros_","guid":175427,"unread":true,"content":"<p>I recently released <a href=\"https://crates.io/crates/decide-core\">Decide</a>, a fast and lightweight permission engine written in Rust, with built-in support for both Rust and JavaScript/TypeScript.</p><p>It started as a small idea, but turned into something I genuinely found useful, especially because there weren‚Äôt many simple permission engines for Rust.</p><ul><li>Role + condition based permission engine</li><li>Supports conditions like: <code>user_id === resource_owner</code></li><li>Built in Rust (uses Rhai for condition evaluation)</li><li>Comes with a JS/TS wrapper (using napi-rs)</li></ul><p>The code is completely open to view. Visit the repository <a href=\"https://github.com/aether-flux/decide\">here</a>.</p><p>An example usage is given in the code snippet. The part  gets the role definitions from a  file.</p><p>There are a bunch of libraries for auth or RBAC in JS, but almost none in Rust. I thought, why not build a clean one that works for both?</p><p>It‚Äôs fully open-source and MIT licensed.</p><h2>Would love to hear your thoughts</h2><p>It's my first time posting here, and I'd love feedback. Especially around: - Rust conventions or improvements - Performance ideas</p><p>Thanks for reading, I hope this can help someone actually :)</p>","contentLength":1046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One click k8s deploy!","url":"https://www.reddit.com/r/kubernetes/comments/1lnn6i2/one_click_k8s_deploy/","date":1751225086,"author":"/u/wideboi_420","guid":175362,"unread":true,"content":"<p>Hello guys! I have been lurking around for a while, and I wanted to share my little automation project. I was a little bit inspired by Jim's Garage one click deploy script for k3s, but since I am studying k8s here is mine:</p><p>Please feel free to criticize and to give out any advice, this is just for fun, even tho someone might find this useful in the future =)</p>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is void linux in active development, and if so where on that scale?","url":"https://www.reddit.com/r/linux/comments/1lnme1y/is_void_linux_in_active_development_and_if_so/","date":1751223150,"author":"/u/doc1623","guid":175430,"unread":true,"content":"<p>I used it in the past, and loved it, but I remember reading that the lead or main developer left, I think. I see that it's still technically in active development but does that mean they are just barely keeping up or enough resources to make big advances, or somewhere in between. An example to make my point is Redox OS. It's initial release was 10 years ago. It still seems to be in \"active\" development, but it has yet to reach an official 1.0 release. Side note, I hope it does before it is surpassed by other projects with more developmental for me and I assume most at this point.</p><p>I guess, it's a multipart question or just lots of related questions.</p><ol><li><p>Why is it so far down on distrowatch list now?</p></li><li><p>Does it have enough active development resources to not only to keep pace with advancements, but even continue to make some or is too resource to be all but a fringe distro rather than a daily driver without allot of issues popping up that are more natural to developmental, pre-release version?</p></li></ol><p>Please, these are honest questions, that I don't feel I have the answer to. Please keep answers civil, non-defensive/combative. Hoping that people more \"in the know\" and/or have kept up better, might have a better understanding. </p>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Service Binding for K8s in Spring Boot cloud-native applications","url":"https://medium.com/cloudnativepub/service-binding-for-k8s-in-spring-boot-cloud-native-applications-3717d3486886?sk=346dc534327888ca805aad94e5d0f1b5","date":1751222681,"author":"/u/zarinfam","guid":175359,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lnm75f/service_binding_for_k8s_in_spring_boot/"},{"title":"Tools I love: mise(-en-place)","url":"https://blog.vbang.dk/2025/06/29/tools-i-love-mise/","date":1751221999,"author":"/u/micvbang","guid":176274,"unread":true,"content":"<p>Once in a while you get introduced to a tool that instantly changes the way you work. For me, <a href=\"https://github.com/jdx/mise\">mise</a> is one of those tools.</p><p>mise is the logical conclusion to a lot of the meta-tooling that exists around language-specific version and package managers like <a href=\"https://asdf-vm.com/\">asdf</a>, <a href=\"https://github.com/nvm-sh/nvm\">nvm</a>, <a href=\"https://docs.astral.sh/uv/\">uv</a>, <a href=\"https://github.com/pyenv/pyenv\">pyenv</a> etc. It makes it exceptionally easy to install, use, and manage software. It also allows you to manage <a href=\"https://mise.jdx.dev/environments/\">environment variables</a> and <a href=\"https://mise.jdx.dev/tasks/\">declare tasks</a> (run commands).</p><p>The first step in getting an intuitive understanding of what mise can help you with is to use it to install a tool. Pick your favorite and try it out; it supports <a href=\"https://mise.jdx.dev/registry.html\"></a>!</p><div><div><pre><code>jj\ncommand_not_found_handler:5: not found: jj\n\nmise use jj\nmise ~/projects/examples_mise/mise.toml tools: jj@0.30.0\n\njj version\njj 0.30.0\n\n ..\n\njj version\ncommand_not_found_handler:5: not found: jj\n\neaxmples_mise\n\njj version\njj 0.30.0\n</code></pre></div></div><p>As the above shows, with mise we‚Äôre just one command away from installing and trying out a new tool, e.g. .</p><p>In the above we that mise printed <code>mise ~/projects/examples_mise/mise.toml tools: jj@0.30.0</code>. This tells us that mise has created (or updated) the mise configuration . \nWe also see that if we cd out of , the  command is no longer available. If we cd back into , it becomes available again; unless you explicitly install tools globally, mise will only make the tools available which are mentioned in a  file on the path from your current directory to the root of your file system. That of course means that we could potentially meet multiple  files when going back up to the root of the file system. Mise handles this by concatting the configurations and overwriting conflicting configurations, letting the file furthest down the tree win.</p><p>This is a clever design as it allows us to configure different versions of the same tool to be available in different directories. Let‚Äôs have a look at what the  file looks like:</p><p>If we want a specific version of  to be installed in a specific directory, we just update the toml file to say e.g. .</p><p>Let‚Äôs see what it looks like to use mise to manage Python versions for two projects with different requirements:</p><div><div><pre><code>tree\n\n‚îú‚îÄ‚îÄ project_new\n‚îÇ\t‚îî‚îÄ‚îÄ mise.toml\n‚îî‚îÄ‚îÄ project_old\n    ‚îî‚îÄ‚îÄ mise.toml\n\nproject_new/mise.toml\ntools]\npython project_old/mise.toml\ntools]\npython project_new\npython \nPython 3.11.13\n\n ../project_old\npython \nPython 3.8.20\n</code></pre></div></div><p>When we cd into one of the directories listed above, mise automatically makes the version of the tool configured in  available to us. If it isn‚Äôt already installed, mise will install it for us. The implication of this is that you can commit a  to your repository, and anyone that has mise installed will automatically get and use the expected dev tools when they enter the project directory. And when it‚Äôs time to upgrade a dev tool, you can just update the version number in  and everyone will start using the new version!</p><p>The fact that mise makes tools available to you according to the  file in your current working directory has further implications: it‚Äôs not just developer machines that can benefit from using mise; CI/CD pipelines can benefit greatly as well! When you use mise in your pipelines, you avoid the problem of having out of sync versions between developer and build machines. You get to have a single place where you can configure the version of your dev tools everywhere!</p><p>As I mentioned in the beginning, besides managing dev tools, mise also allows you to <a href=\"https://mise.jdx.dev/tasks/toml-tasks.html\">declare and run so-called tasks</a>. Think of a task as an advanced invocation of a bash script. Even if we use tasks as just plain bash scripts (they can do a lot more), it can be a major advantage to declare common operations such as building, testing, linting etc. as mise tasks, since all developers get access to them and will run their commands in exactly the same way every time. If you‚Äôre diligent in your naming, you can even make the experience of building or testing across projects identical.</p><p>The following are examples of some very simple Python-related tasks declared in :</p><div><div><pre><code></code></pre></div></div><p>Adding this to  will make the commands  and  available. Again, if you check this in to your repo, the commands will be available to all developers and pipelines. And reusing these names in your rust project means that you can use the same commands to tell cargo to install your crates or run your tests.</p><p>Once you‚Äôve declared your tasks you should of course also use them in your CI/CD pipeline. Doing this makes you less dependent on the particular yaml syntax and arbitrary requirements of your provider, and makes it easier to move to another one if you need to. It also ensures that there‚Äôs a standard way to build and test your code, helping to further reduce the amount of ‚Äúit works on my machine‚Äù.</p><p>There‚Äôs a lot of depth to what you can use mise to help you automate. It‚Äôs a lovely tool and I hope I‚Äôve spiked your interest enough to give it a try!</p><p>Although this is a very obvious problem, I want to make it explicit: a major concern of all software dependency management is control of your supply chain; how easy is it for somebody to insert malicious code into a binary you will run hugely impacts the integrity of your systems and data. Depending on your industry, it might not be feasible to use mise as it‚Äôs pretty opaque where your dependencies will be downloaded from.</p>","contentLength":5289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnlxds/tools_i_love_miseenplace/"},{"title":"KCSA 2nd attempt","url":"https://www.reddit.com/r/kubernetes/comments/1lnlijw/kcsa_2nd_attempt/","date":1751220964,"author":"/u/Low_Half_6876","guid":175361,"unread":true,"content":"<div><p>Hello I just want to know that in the KCSA 2ndt attempt will the question be same as the first attempt. Did anyone went through the second attempt of kcsa ?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Low_Half_6876\"> /u/Low_Half_6876 </a>","contentLength":192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Klirr: invoice automation tool written on Rust using Typst","url":"https://www.reddit.com/r/rust/comments/1lnkpi7/klirr_invoice_automation_tool_written_on_rust/","date":1751219036,"author":"/u/Sajjon","guid":175461,"unread":true,"content":"<p>Features: * Config once: Set your company, client and project information using interactive Terminal UI (creates RON files). No Rust, Typst or RON skills needed! * Inter-month-idempotent: You build the invoice any number of times, it always results in the same invoice number when run within the same month. The proceeding month the next invoice number will be used. * Calendar aware: Using your machines system time to determine the month, it calculates the number of working days for the target month. Invoice date is set to last day of the target month and due date is set dependent on the payment terms set in your RON files. * Capable: Supports setting number of days you were off, to be extracted from the automatically calculated number of working days. Supports expenses using \"{PRODUCT}, {COST}, {CURRENCY}, {QUANTITY}, {DATE}\" CSV string. * Maintenance free: The invoice number automatically set based on the current month. When you build the invoice the next month, the next number is used * Multi-layout support: Currently only one layout is implemented, but the code base is prepared to very easily support more. * Multi-language support: The labels/headers are dynamically loaded through l18n - supported languages are English and Swedish - it is trivial for anyone to make a PR to add support for more languages.</p><p>Any and all feedback is much appreciated! Especially on ergonomics and features, but codebase well. </p><p>It has 97% test code coverage </p>","contentLength":1457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Lenovo issues with WIFI and Bluetooth after Updating","url":"https://www.reddit.com/r/linux/comments/1lnkpa3/linux_lenovo_issues_with_wifi_and_bluetooth_after/","date":1751219021,"author":"/u/Frequent-Price8935","guid":175310,"unread":true,"content":"<p>Hello guys, in the last months I got many issues after Updating my Linux System with WIFI and Bluetooth and I want to share my experience to help you out if u got the same issue. I personally found not much about this topic that could help me, therefore u got this to help.</p><p>I use  on a new <strong>Lenovo Thinkpad T14 Gen 5 AMD</strong></p><p>At fresh install and not after several months I faced the same Issue that after an update of the system WIFI und Bluetooth disappears. Only after I got in sleep Mode and login again just bluetooth shows up. After a restart nothing shows up again.</p><p>The issue was, that die BIOS got an Update as well. It seems Lenovo + Linux + Qualcomm got several issues after an BIOS Update.</p><p>To solve the issue in my case I got to the Lenovo Support site and downloaded the ISO File of the latest BIOS. This Version was two months older than the Version I got installed automatically. I flashed this to my USB and updated the BIOS to the older version. Now my WIFI and Bluetooth works again.</p><p>If u got the same issue and Google can¬¥t help I hope u see this and maybe is solves your issue as well.</p>","contentLength":1094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Native Subresource Support in Kubectl","url":"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl","date":1751212336,"author":"/u/abhimanyu_saharan","guid":175227,"unread":true,"content":"<p>Managing Kubernetes subresources like  and  has traditionally been clunky for CLI users. Until recently, interacting with these required raw HTTP calls or complex  invocations, making operations difficult to script and error-prone. With the graduation of <a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-cli/2590-kubectl-subresource-support\">KEP-2590</a> to stable in Kubernetes v1.33, users now have native support for subresource manipulation directly within .</p><p>The Kubernetes API defines  such as , , and  to isolate specific update operations. These endpoints are commonly used by controllers, but until now, human users had no clean way to interact with them using kubectl. For example:</p><ul><li>Fetching the current status of a deployment required querying the full object or making raw HTTP requests.</li><li>Updating replica counts through the  subresource was either not possible via  or required complex scripting.</li><li>CustomResourceDefinitions (CRDs) with subresources were even more tedious to handle.</li></ul><p>This KEP introduces a  flag across key kubectl commands, streamlining these workflows.</p><h2>New Capability:  Flag<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#new-capability-subresource-flag\"></a></h2><p>You can now use the  flag with the following commands:</p><ul></ul><p>This change enables consistent and declarative interaction with subresources for both built-in and custom resource types.</p><h3>Example: Get Deployment Status or Scale<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#example-get-deployment-status-or-scale\"></a></h3><article data-module=\"code\"><div><div><pre tabindex=\"0\"><code></code></pre></div></div></article><h3>Example: Patch Subresource<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#example-patch-subresource\"></a></h3><article data-module=\"code\"><div><div><pre tabindex=\"0\"><code></code></pre></div></div></article><p>This is particularly powerful for CRDs that expose scale or status endpoints.</p><h3>Behavior for Unsupported Subresources<a href=\"https://blog.abhimanyu-saharan.com/posts/native-subresource-support-in-kubectl#behavior-for-unsupported-subresources\"></a></h3><p>The CLI includes input validation and proper error handling:</p><article data-module=\"code\"><div><div><pre tabindex=\"0\"><code></code></pre></div></div></article><p>The enhancement builds on the resource builder and visitor pattern already in use within , adding a  chain to target the correct API path. Table printer support was extended to pretty-print responses from  and  subresources, ensuring consistency in output formatting.</p><ul><li> output reuses  defined in the CRD spec.</li><li> output mimics native resources by adding desired/available replicas.</li></ul><ul><li>: Introduced in Kubernetes v1.25 with initial support for , , , and .</li><li>: Promoted in v1.27 with  support and e2e coverage.</li><li>: Graduated in v1.33 after over a year of stable usage with no critical issues.</li></ul><ul><li>The enhancement is entirely client-side.</li><li>Users on older  binaries will simply not see the flag.</li><li>The API behavior remains unchanged, so operations can still be performed using  as before.</li></ul><p>Cluster admins can track usage by inspecting audit logs for subresource API requests, particularly those hitting , , or .</p><p>This enhancement significantly improves the developer experience when managing Kubernetes objects via . By exposing subresources as first-class citizens, it simplifies scripting, debugging, and day-to-day cluster operations. Whether you're scaling CRDs, inspecting status updates, or editing a subset of object fields,  brings much-needed ergonomics to Kubernetes CLI workflows.</p><p>If you're using custom controllers or automation pipelines that rely on subresources, it's time to update your toolchains and start leveraging this feature natively through .</p>","contentLength":2849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1lnhztk/native_subresource_support_in_kubectl/"},{"title":"Ilya Sutskever says future superintelligent data centers are a new form of \"non-human life\". He's working on superalignment: \"We want those data centers to hold warm and positive feelings towards people, towards humanity.\"","url":"https://v.redd.it/46bejt6qyv9f1","date":1751211451,"author":"/u/MetaKnowing","guid":175309,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lnhn34/ilya_sutskever_says_future_superintelligent_data/"},{"title":"How to write Rust in the Linux kernel, part 2","url":"https://lwn.net/SubscriberLink/1025232/fbb2d90d084368e3/","date":1751210673,"author":"/u/kibwen","guid":175364,"unread":true,"content":"<p>\nIn 2023, Fujita Tomonori\n<a href=\"https://lwn.net/ml/all/20231012125349.2702474-5-fujita.tomonori@gmail.com/\">\nwrote a Rust version</a> of the existing driver for the\nAsix AX88796B embedded Ethernet controller. At slightly more than 100 lines,\nit's about as simple as a driver can be, and therefore is a useful touchstone for\nthe differences between writing Rust and C in the kernel. Looking at the Rust\nsyntax, types, and APIs used by the driver and contrasting them with the C\nversion will help illustrate those differences.\n</p><p>\nReaders who are already conversant with Rust may find this article retreads some\nbasics, but it is my hope that it can still serve as a useful reference for\nimplementing simple drivers in Rust. The\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b.c\">\nC version</a> and the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b_rust.rs\">\nRust version</a> of the AX88796B driver are remarkably similar, but there are still some important\ndifferences that could trip up a developer performing a naive rewrite from one to the other.\n</p><p>\nThe least-different thing between the two versions is the legalities. The Rust\ndriver starts with an\n<a href=\"https://docs.kernel.org/process/license-rules.html\">\nSPDX comment</a> asserting that the file is covered by the GPL,\nas many files in the kernel do. Below that is a documentation comment:\n</p><pre>    //! Rust Asix PHYs driver\n    //!\n    //! C version of this driver: [`drivers/net/phy/ax88796b.c`](./ax88796b.c)\n</pre><p>\nAs mentioned in the\n<a href=\"https://lwn.net/Articles/1024202/\">\nprevious article</a>, comments starting with  contain documentation that applies\nto the\nentire file. The next few lines are a  statement, the Rust analogue\nof :\n</p><pre>    use kernel::{\n        c_str,\n        net::phy::{self, reg::C22, DeviceId, Driver},\n        prelude::*,\n        uapi,\n    };\n</pre><p>\nLike C, Rust modules are located starting from a search path and then continuing\ndown a directory tree. Unlike C, a  statement can selectively import\nonly some items defined in a module. For example,  is not a separate module,\nbut rather a specific item inside the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/rust/kernel/net/phy.rs\"></a> module. By\nimporting both  and \nas a whole, the Rust module can refer to  directly, and\nanything else from the PHY module as . These items can always\nbe referred to by their full paths; a  statement just introduces a\nshorter local alias. If a name would be ambiguous, the compiler will complain.\n</p><p>\nAll of these imported items come from the  crate (Rust library),\nwhich contains the bindings between the main kernel and Rust code. In a\nuser-space Rust project, a program would usually also have some imports from\n, Rust's standard library, but that isn't possible in the kernel,\nsince the kernel needs more precise control over allocation and other details\nthat the standard library abstracts away. Kernel C developers can't use\nfunctions from libc in the kernel for much the same reason. The\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/rust/kernel/prelude.rs\"> module</a> contains kernel replacements for many common\nstandard-library functions; the remainder can be found in , the\nsubset of  that doesn't allocate.\n</p><p>\nIn the C version of the driver, the next step is to define some constants\nrepresenting the three different, but related, devices this driver supports: the\nAX88772A, the AX88772C, and the AX88796B. In Rust, items do not have to be\ndeclared before use ‚Äî the entire file is considered at once. Therefore, Fujita\nchose to reorder things slightly to keep the code for each board in its own\nsection; the types for each board ( and so on) are defined\nlater.\nThe next part of the Rust driver is a macro invocation that sets up the\nnecessary symbols for a PHY driver:\n</p><pre>    kernel::module_phy_driver! {\n        drivers: [PhyAX88772A, PhyAX88772C, PhyAX88796B],\n        device_table: [\n            DeviceId::new_with_driver::&lt;PhyAX88772A&gt;(),\n            DeviceId::new_with_driver::&lt;PhyAX88772C&gt;(),\n            DeviceId::new_with_driver::&lt;PhyAX88796B&gt;()\n        ],\n        name: \"rust_asix_phy\",\n        authors: [\"FUJITA Tomonori &lt;fujita.tomonori@gmail.com&gt;\"],\n        description: \"Rust Asix PHYs driver\",\n        license: \"GPL\",\n    }\n</pre><p>\nRust macros come in two general kinds: attribute macros, which are written\n and modify the item that they appear before, and normal\nmacros, which are written . There is also a less common\nvariant of attribute macros written  which applies to the\ndefinition that they appear within. Normal macros can use any\nmatching set of braces to enclose their arguments, but can always be recognized\nby the mandatory exclamation mark between the name and the braces.\nThe convention is to use\nparentheses for macros that return a value and braces for macros that are\ninvoked to define a structure (as is the case here), but that is not actually\nrequired. Invoking the macro with parentheses would have the same result, but it\nwould make it less obvious to other Rust programmers what is happening.\n</p><p>\nThe  argument to the macro contains the names of the three board\ntypes this driver covers. Each driver has to be associated with information such\nas the name of the device and the PHY device ID that it should be active for. In\nthe C version of the driver, this is handled by a separate table:\n</p><pre>    static struct phy_driver asix_driver[] = { ... };\n</pre><p>\nIn the Rust code, this information is stored in the code for each board (see\nbelow), since\nall PHY drivers need to provide it. Overall, the\n macro serves the same role as the\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/include/linux/phy.h#L2126\"></a> macro in C.\n</p><p>\nNext, the Rust driver defines two constants that the code uses later:\n</p><pre>    const BMCR_SPEED100: u16 = uapi::BMCR_SPEED100 as u16;\n    const BMCR_FULLDPLX: u16 = uapi::BMCR_FULLDPLX as u16;\n</pre><p>\nEvery declaration of a value (as opposed to a data structure) in Rust starts\nwith either  or . The former are compile-time\nconstants ‚Äî like a simple  in C. Types are mandatory for\n definitions, but optional for  ones. In either case,\nthe type always appears separated from the name by a colon.\nSo, in this case, both constants are\n values, Rust's unsigned 16-bit integer type. The \npart at the end is a cast, since the original  constants\nbeing referenced are defined in C and assumed to be 32 or 64 bits by default,\ndepending on the platform.\n</p><p>\nThe final piece of code before the actual drivers is a shared function for\nperforming a soft reset on Asix PHYs:\n</p><pre>    // Performs a software PHY reset using the standard\n    // BMCR_RESET bit and poll for the reset bit to be cleared.\n    // Toggle BMCR_RESET bit off to accommodate broken AX8796B\n    // PHY implementation such as used on the Individual\n    // Computers' X-Surf 100 Zorro card.\n    fn asix_soft_reset(dev: &amp;mut phy::Device) -&gt; Result {\n        dev.write(C22::BMCR, 0)?;\n        dev.genphy_soft_reset()\n    }\n</pre><p>\nThere's a few things to notice about this function. First of all, the comment\nabove it is not a documentation comment. This isn't a problem because this\nfunction is also private ‚Äî since it was declared with  instead of\n, it's not visible outside this one module. The C equivalent\nwould be a  function. In Rust, the default is the opposite way around,\nwith functions being private (static) unless declared otherwise.\n</p><p>\nThe argument to the function is an  called .\nReferences (written with an &amp;)\nare in many ways Rust's most prominent feature; they are like\npointers, but with compile-time guarantees that certain classes of bugs (such as\nconcurrent mutable access without synchronization) can't happen. In this case,\n takes a mutable reference (). The\ncompiler guarantees that no other function can have a reference to the\nsame\n<a href=\"https://rust.docs.kernel.org/kernel/net/phy/struct.Device.html\"></a> at the same time. This means that the body of the\nfunction can clear the  pin and trigger a soft reset without\nworrying about concurrent interference.\n</p><p>\nThe last part of the function to understand is the return type,\n<a href=\"https://rust.docs.kernel.org/kernel/error/type.Result.html\"></a>, and the \"try\" operator, . In C, a function that could fail often\nindicates this by returning a special sentinel value, typically a negative number.\nIn Rust, the same thing is true, but the sentinel value is called \ninstead, and is one possible value of the  enumeration. The other\nvalue is , which indicates success. Both  and \ncan carry additional information, but the default in the kernel is for  to carry an\n<a href=\"https://rust.docs.kernel.org/kernel/error/struct.Error.html\">\nerror number</a>, and for  to have no additional information.\n</p><p>\nThe pattern of checking for an error and then immediately propagating it to a\nfunction's caller is so common that Rust introduced the try operator as a\nshortcut. Consider the same function from the C version of the driver:\n</p><pre>    static int asix_soft_reset(struct phy_device *phydev)\n    {\n\t    int ret;\n\n\t    /* Asix PHY won't reset unless reset bit toggles */\n\t    ret = phy_write(phydev, MII_BMCR, 0);\n\t    if (ret &lt; 0)\n\t\t    return ret;\n\n\t    return genphy_soft_reset(phydev);\n    }\n</pre><p>\nIt performs the same two potentially fallible library function calls, but needs\nan extra statement to propagate the potential error. In the Rust version, if the\nfirst call returns an , the try operator automatically returns it.\nFor the second call, note how the line does not end with a semicolon ‚Äî this\nmeans the value of the function call is also the return value of the function as\na whole, and therefore any errors will also be returned to the caller. The\nmissing semicolon is not easy to forget, however, because adding it in will make\nthe compiler complain that the function does not return a .\n</p><p>\nThe actual driver code differs slightly for the three different boards. The\nsimplest is the AX88786B, the implementation of which starts on\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/drivers/net/phy/ax88796b_rust.rs#L124\">\nline 124</a>:\n</p><p>\nThis is an empty structure. An actual instance of this type has no storage\nassociated with it ‚Äî it doesn't take up space in other structures,\n<a href=\"https://doc.rust-lang.org/std/mem/fn.size_of.html\"></a> reports 0, and it has no padding ‚Äî but there can still be\nglobal data for the type as a whole (such as debugging information). In this\ncase, an empty structure is used to implement the  abstraction,\nin order to bundle all of the needed data and functions for a PHY driver\ntogether. When the compiler is asked to produce functions that apply to a\n (which the  macro does), it\nwill use this definition:\n</p><pre>    #[vtable]\n    impl Driver for PhyAX88796B {\n        const NAME: &amp;'static CStr = c_str!(\"Asix Electronics AX88796B\");\n        const PHY_DEVICE_ID: DeviceId =\n            DeviceId::new_with_model_mask(0x003b1841);\n\n        fn soft_reset(dev: &amp;mut phy::Device) -&gt; Result {\n            asix_soft_reset(dev)\n        }\n    }\n</pre><p>\nThe constant and function definitions work in the same way as above. The type of\n uses a static reference (\"\"), which is\na reference\nthat is valid for the entire lifetime of the program. The C equivalent is a\n pointer to the data section of the executable: it is never\nallocated, freed, or modified, and is therefore fine to dereference anywhere in\nthe program.\n</p><p>\nThe new\nRust feature in this part of the driver is the  block, which is used to implement a\n<a href=\"https://doc.rust-lang.org/book/ch10-02-traits.html#traits-defining-shared-behavior\">\ntrait</a>. Often, a program will have multiple different parts that conform to\nthe same interface. For example, all PHY drivers need to provide a name,\nassociated device ID, and some functions implementing driver operations.\nIn Rust, this kind of common interface is represented by a\ntrait, which lets the compiler perform static type dispatch to select the right\nimplementation based on how the trait functions are called.\n</p><p>\nC, of course, does not work like this (although\n<a href=\"https://en.cppreference.com/w/c/language/generic.html\"></a> can sometimes\nbe used to implement type dispatch manually). In the kernel's C code, PHY drivers are\nrepresented by\n<a href=\"https://elixir.bootlin.com/linux/v6.15.2/source/include/linux/phy.h#L876\">\na structure</a> that contains data and function pointers. The\n macro converts a Rust trait into a singular C structure full of\nfunction pointers. Up above, in the call to , the\nreference to the  type lets the compiler find the right\n implementation, and from there produce the correct C structure\nto integrate with the C PHY driver infrastructure.\n</p><p>\nThere are obviously more functions involved in implementing a complete PHY\ndriver. Luckily, these functions are often the same between different devices,\nbecause there is a standard interface for PHY devices. The C PHY driver code\nwill fall back to a generic implementation if a more specific function isn't\npresent in the driver's definition, so the AX88796B code can leave them out.\nThe other two devices supported in this driver\nspecify more custom functions to work around hardware quirks, but those\nfunctions are not much more complicated than what has already been shown.\n</p><p>\nSteps to implement a PHY driver ...\n</p><table><tbody><tr><td valign=\"top\">Write module boilerplate (licensing and authorship information,\n statements, etc.).</td><td valign=\"top\">Write module boilerplate (licensing and authorship information, \nstatements, a call to ).</td></tr><tr><td valign=\"top\">Implement the needed functions for the driver, skipping functions that can\nuse the generic PHY code.\n</td><td valign=\"top\">Implement the needed functions for the driver, skipping functions that can\nuse the generic PHY code.\n</td></tr><tr><td valign=\"top\">Bundle the functions along with a name, optional flags, and PHY device ID\ninto a  and register it with the PHY subsystem.\n</td><td valign=\"top\">Bundle the functions along with a name, optional flags, and PHY device ID\ninto a trait; the  macro converts it into the right form for\nthe PHY subsystem.\n</td></tr></tbody></table><p>\nOf course, many drivers have specific hardware concerns or other complications;\nkernel software is distinguished by its complexity and concern with low-level\ndetails. The next article in this series will look at the design of the interface\nbetween the C and Rust code in the kernel, as well as the process of adding new\nbindings when necessary.\n</p>","contentLength":12956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lnhc47/how_to_write_rust_in_the_linux_kernel_part_2/"},{"title":"A Primer on Memory Management","url":"https://sudomsg.com/posts/a-primer-on-memory-management/","date":1751210640,"author":"/u/marcthe12","guid":175276,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lnhbnn/a_primer_on_memory_management/"},{"title":"Nick Bostrom says AGI won‚Äôt stop at the human level, it will quickly lead to superintelligence. From there, machines will outthink the best scientists and invent everything else -- faster and better than humans. \"It's the last invention we‚Äôll ever need.\"","url":"https://v.redd.it/3vyph6eawv9f1","date":1751210632,"author":"/u/MetaKnowing","guid":175232,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1lnhbjl/nick_bostrom_says_agi_wont_stop_at_the_human/"},{"title":"Rewriting pre-1.0 compiler code for better macro error messages","url":"https://github.com/rust-lang/rust/pull/143070","date":1751210168,"author":"/u/kibwen","guid":175308,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lnh4us/rewriting_pre10_compiler_code_for_better_macro/"},{"title":"Built a tool to sync Obsidian notes across devices without subscriptions or Git commands","url":"https://www.reddit.com/r/linux/comments/1lnh4jy/built_a_tool_to_sync_obsidian_notes_across/","date":1751210147,"author":"/u/believertn","guid":175311,"unread":true,"content":"<p>As someone who spends a lot of time on Linux and likes to take notes in Obsidian, I found syncing notes across multiple devices frustrating. I distro hop often, and making sure my notes are updated everywhere without paying for Obsidian Sync or fiddling with Git commands became a problem I wanted to solve.</p><p>So I built Ogresync, a free and open-source tool that handles syncing your Obsidian vault automatically using GitHub in the background. Instead of opening Obsidian directly, you launch Ogresync, which syncs your vault, opens Obsidian, and then pushes your changes after you finish editing. There‚Äôs no need to remember Git commands or worry about merge conflicts.</p><p>I know there are plugins that do something similar, but they often expect users to be comfortable with Git. I wanted a solution that just works out of the box, even for people who don‚Äôt want to deal with version control.</p><p>I‚Äôd really appreciate feedback from fellow Linux users. How do you sync your notes right now? Does this approach make sense or is there something you‚Äôd want it to do differently?</p>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Software to format a book","url":"https://www.reddit.com/r/linux/comments/1lngyhb/software_to_format_a_book/","date":1751209723,"author":"/u/Popular_Tour1811","guid":175233,"unread":true,"content":"<p>My grandma is writing a book on her familys' history. I, being the computer literate grandchild, am task with formatting and diagramming the book. Unfortunately, the only software I know for that is Microsoft Publisher and Canva. Anyone know a Foss alternative for those?</p><p>(I don't know if formatting is the right word for that, nor diagramming. I mean turning plain text into a nice looking pdf with well positioned images that can be printed into a book)</p>","contentLength":454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what's the convention for how types and their corresponding methods should be grouped within a file?","url":"https://www.reddit.com/r/golang/comments/1lnex8l/whats_the_convention_for_how_types_and_their/","date":1751204349,"author":"/u/Fueled_by_sugar","guid":175403,"unread":true,"content":"<div><p>option a - all the types first, all the methods second:</p><pre><code>type accessToken string type status uint type organizationID string func (a accessToken) String() string { return string(a) } func (o organizationID) String() string { return string(o) } func (s status) Uint() uint { return uint(s) } </code></pre><p>option b - methods go below their corresponding types:</p><pre><code>type accessToken string func (a accessToken) String() string { return string(a) } type status uint func (s status) Uint() uint { return uint(s) } type organizationID string func (o organizationID) String() string { return string(o) } </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/Fueled_by_sugar\"> /u/Fueled_by_sugar </a>","contentLength":615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I built a Python debugger that you can talk to","url":"https://www.reddit.com/r/MachineLearning/comments/1lnem9e/p_i_built_a_python_debugger_that_you_can_talk_to/","date":1751203482,"author":"/u/jsonathan","guid":175277,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Position: Machine Learning Conferences Should Establish a ‚ÄúRefutations and Critiques‚Äù Track","url":"https://arxiv.org/pdf/2506.19882","date":1751202449,"author":"/u/StartledWatermelon","guid":176393,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1lne9e0/d_position_machine_learning_conferences_should/"},{"title":"Tracking Anticheat Updates","url":"https://not-matthias.github.io/posts/anticheat-update-tracking/","date":1751200973,"author":"/u/not-matthias","guid":175230,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lndrvx/tracking_anticheat_updates/"},{"title":"My Linux survived where Windows died","url":"https://www.reddit.com/r/linux/comments/1lndntz/my_linux_survived_where_windows_died/","date":1751200614,"author":"/u/githman","guid":175125,"unread":true,"content":"<p>TLDR: Modern Linux drivers and hardware compatibility are not as finicky as some people say.</p><p>My government keeps trying to break our energy system to goodbye; a recent malfunction of power mains fried my old PC's PSU and motherboard but the drive fortunately survived. I bought a slightly more recent system on the local flea market (i5-7400 instead of the old i7-3770K) for the whole whopping ‚Ç¨70 and plugged the drive into it. The drive had both Windows 10 and Fedora 42 KDE installed.</p><p>The outcome: Fedora picked up the new hardware like nothing happened but Windows is stuck on \"getting devices ready\" forever. Guess it's time to reclaim the Windows partition.</p><p>Great job, Fedora and Linux in general. I had to tell it someone and decided to do it here because where else, right.</p>","contentLength":780,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["reddit"]}