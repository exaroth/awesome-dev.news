{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":59,"items":[{"title":"Help with k3s setup on wsl","url":"https://www.reddit.com/r/kubernetes/comments/1iqqly6/help_with_k3s_setup_on_wsl/","date":1739706589,"author":"/u/watterbottle800","guid":526,"unread":true,"content":"<p>I'm trying to install a mern stack application consisting of 11 microservices some which have init containers that depend response from some of the other containers, I have a k3s cluster installed on wsl2, with single node and the external IP of the node is the eth0 ip of the wsl which is in 192.168 range. My pods are in 10.42.0.0/24 and svc in 10.43.0.0/24. All the pods are in default subnet, one of the pods is exposed on port 15672, behind a nodeport svc (say my-svc) with nodeport 30760. One of the init container completed only after a 200 response to curl http:my-svc:15762, but the connectivity is failing with \"failed to connect to &lt;svc cluster ip&gt; port 15672 : couldn't connect to server\" after sometime. </p><p>This specific initcontainer doesn't have nslookup utility doesn't have nslookup or curl utility hence I tried both curl and nslookup from a test pod in the same namespace. Curl failed while nslookup resolved to correct service name and ip), I'm assuming the traffic is going till the svc but not beyond that. I tried with other pods for example call nginx test pod at port 80 from another test pod it failed as well. </p><p>The same setup works fine in k3s cluster in my ec2 and my personal pc, this is my work pc. It would be really helpful if someone could advice on how to troubleshoot this. Thanks</p>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Nvidia on Linux still bad?","url":"https://www.reddit.com/r/linux/comments/1iqpsy0/is_nvidia_on_linux_still_bad/","date":1739703116,"author":"/u/Szer1410","guid":531,"unread":true,"content":"<p>I am planning to buy a laptop. I want to have a peak Linux experience, so I have been looking for laptops with dedicated AMD GPUs. While searching, I noticed a few things:</p><ol><li><p>There are not many laptops with dedicated AMD GPUs. Most available options come with integrated GPUs like the 780M.</p></li><li><p>For the price of a laptop with a 780M, I can get a laptop with an RTX 3050 or better.</p></li><li><p>System76 sells Linux laptops with Nvidia GPUs on their website.</p></li></ol><p>Additionally, I want to install Manjaro on my laptop. Are there any Linux distributions with better Nvidia support?</p>","contentLength":549,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes In-Place Pod Vertical Scaling","url":"https://scaleops.com/blog/kubernetes-in-place-pod-vertical-scaling/","date":1739703019,"author":"/u/Wownever","guid":527,"unread":true,"content":"<p>Kubernetes continues to evolve, offering features that enhance efficiency and adaptability for developers and operators. Among these are <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/\" target=\"_blank\" rel=\"noreferrer noopener\">Resize CPU and Memory Resources assigned to Containers</a>, introduced in Kubernetes version 1.27. This feature allows for adjusting the CPU and memory resources of running pods without  them, helping to minimize downtime and optimize resource usage. This blog post explores how this feature works, its practical applications, limitations, and cloud provider support. Understanding this functionality is vital for effectively managing containerized workloads and maintaining system reliability.</p><h2>What Is In-Place Pod Vertical Scaling?</h2><p>Traditionally, modifying the resource allocation for a Kubernetes pod required a restart, potentially disrupting applications and causing downtime. In-place scaling changes this by enabling real-time CPU and memory adjustments while the pod continues running. This is particularly useful for workloads with a very low tolerance for pod evictions.</p><p>What’s behind the feature gate?</p><p>The new  spec element allows you to specify how a pod reacts to a patch command that changes its resource requests, enabling changing resource requests without rescheduling the pod.</p><p>The result of the change attempt is communicated as part of the pods’ status in a field called&nbsp;  (for more information on the new fields, check out the Kubernetes API <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#container-v1-core\" target=\"_blank\" rel=\"noreferrer noopener\">documentation</a>.)<p>Additionally, this feature introduces the </p> in the spec element for containers, allowing fine-grained control over resizing behavior and allowing the developer to choose if CPU change or Memory change should lead to rescheduling the pod.</p><div><ul><li>Dynamic Scaling: Modify CPU and memory allocations while pods run.</li><li>No Restarts: Avoid downtime caused by pod restarts.</li><li>Granular Control: Enable precise resource tuning for better efficiency.</li></ul></div><p>The <code>InPlacePodVerticalScaling</code> feature integrates seamlessly into Kubernetes to provide a more dynamic approach to resource allocation. Here’s a detailed breakdown of how it operates:</p><div><ol><li> Activating the <code>InPlacePodVerticalScaling</code> feature gate in your cluster configuration is required to enable this functionality. This allows the kubelet on each node to detect and process resource updates dynamically.</li><li><strong>Dynamic Resource Updates via Kube API:</strong> With the feature enabled, the kubelet directly applies resource changes to running pods without requiring restarts. Supported container runtimes (e.g., <a href=\"https://github.com/containerd/containerd/releases/tag/v1.6.9\" target=\"_blank\" rel=\"noreferrer noopener\">containerd v1.6.9</a> or later) ensure these updates are applied efficiently. If constraints like insufficient free memory or CPU prevent the changes, the pod follows the regular flow: it is recreated and rescheduled.</li><li> The  field dictates how CPU and memory adjustments are handled. For instance, you can set  for live updates without restarts or  to force a restart when a specific resource is modified.</li></ol></div><h2>Limitations and Considerations</h2><p>While In-Place Pod Vertical Scaling offers significant benefits, it has limitations:</p><p>1. Cloud Provider Support</p><div><ul><li>AWS: Not supported by Amazon Elastic Kubernetes Service (EKS) as there is no way to activate the needed feature gate.</li><li>GCP: Google Kubernetes Engine (GKE) supports this feature as an alpha capability, starting with Kubernetes version 1.27. It must be enabled during cluster creation and requires disabling auto-repair and auto-upgrade. See the <a href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/alpha-clusters\" target=\"_blank\" rel=\"noreferrer noopener\">GKE alpha clusters documentation</a>.</li></ul></div><p>Several Kubernetes policies and mechanisms govern resource scaling. These include:</p><div><ul><li>Resource quotas limit the total CPU and memory usage for a namespace. If an <code>InPlacePodVerticalScaling</code> operation exceeds these limits, the scaling request will fail. For example:</li></ul></div><pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: example-namespace\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"32Gi\"\n</code></pre><div><ul><li>Limit ranges enforce minimum and maximum resource constraints for individual pods or containers within a namespace. The pod will be denied the resource adjustment if a scaling operation exceeds these bounds. Example configuration:</li></ul></div><pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limits\n  namespace: example-namespace\nspec:\n  limits:\n  - type: Container\n    max:\n      cpu: \"2\"\n      memory: \"4Gi\"\n    min:\n      cpu: \"100m\"\n      memory: \"128Mi\"\n</code></pre><div><ul><li>Admission controllers, such as <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\" target=\"_blank\" rel=\"noreferrer noopener\">Pod Security Admission</a> or custom webhook controllers, can deny scaling operations if they conflict with security or operational policies. For example, a controller may restrict pods from exceeding certain CPU limits.</li></ul></div><div><ul><li>Not all applications can dynamically consume additional resources or adjust to reduced allocations. Examples include:<div><ul><li>Thread Pool Bound applications, like Gunicorn or Unicorn, rely on predefined worker counts.</li><li>Memory-Bound Applications: Applications like Java with fixed Xmx parameters.</li></ul></div></li></ul></div><div><ul><li>In cases where the <a href=\"https://scaleops.com/blog/kubernetes-hpa/\">HPA</a> is based on the resource being patched, this can cause an erratic horizontal scaling behavior. For example:<div><ul><li>HPA scaling behavior is based on CPU average utilization</li></ul></div></li></ul></div><div><ol><li>A pod is changing from 1 core to 2 cores; this can cause a scale-down in pods and affect the bottom-line performance of the application.</li><li>A pod changes from 2 cores to 1; this can cause a scale-up in pods, creating a waste of resources or potential downstream pressure due to the additional and unexpected pods created.</li></ol></div><div><ul><li> Dynamically allocate resources during training and inference phases.</li><li> Combine Horizontal Pod Autoscaler (HPA) with In-Place Pod Vertical Scaling for efficient surge handling.</li><li> Reduce waste by allocating the right amount of resources to each pod in real-time.</li><li> Some applications require significantly higher CPU and memory resources during startup compared to their runtime needs. Google’s example, <a href=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-kubernetes-dynamic-resource-scaling-and-cpu-boost\" target=\"_blank\" rel=\"noreferrer noopener\">Startup CPU Boost</a>, demonstrates how dynamic resource scaling can address such scenarios effectively.</li></ul></div><h3>1. Enable the Feature Gate</h3><p>Add the following configuration to enable the <code>InPlacePodVerticalScaling</code> feature:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\napiServer:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\ncontrollerManager:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\nscheduler:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\n</code></pre><p>For GKE, create a cluster with alpha features enabled:</p><pre><code>gcloud container clusters create poc \\\n    --enable-kubernetes-alpha \\\n    --no-enable-autorepair \\\n    --no-enable-autoupgrade\n</code></pre><p>Define a deployment with initial CPU and memory requests and limits:</p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n        resizePolicy:\n        - resourceName: cpu\n          restartPolicy: NotRequired\n        - resourceName: memory\n          restartPolicy: NotRequired\n</code></pre><p>Once deployed, you can check the <code>cpu.weight, cpu.max, memory.max, memory.min</code> from within the container to see the initial values that the container starts with.</p><pre><code>kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max \n</code></pre><p>Adjust resource allocations for a running pod dynamically:</p><pre><code>kubectl patch pod $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -p '{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"resources\":{\"requests\":{\"cpu\":\"750m\"}}}]}}'\n</code></pre><p>Confirm updated resource settings:</p><pre><code>kubectl describe pod -l app=app\n</code></pre><p>Additionally, you can connect to the container and see the change in <code>cpu.weight, cpu.max, memory.max, memory.min</code> from within the container.</p><pre><code>kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max\n</code></pre><p>In-Place Pod Vertical Scaling is a powerful tool for managing dynamic workloads in Kubernetes, reducing downtime, and <a href=\"https://scaleops.com/blog/optimizing-kubernetes-resources/\">optimizing resource usage</a>. While its adoption depends on cloud provider support and application compatibility, this feature offers significant efficiency and cost-saving benefits. As Kubernetes evolves, such features will become essential for effective container orchestration.</p><p>While Google’s <a href=\"https://github.com/google/kube-startup-cpu-boost\" target=\"_blank\" rel=\"noreferrer noopener\">Kube Startup CPU Boost</a> example is just a specific use case scenario, <a href=\"https://try.scaleops.com\">ScaleOps</a> provides an all in one resource management solution to address all needed scenarios related to Kubernetes resource management.</p>","contentLength":9121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iqps4m/kubernetes_inplace_pod_vertical_scaling/"},{"title":"I created a CLI trash command","url":"https://github.com/Maxsafer/trash-tool","date":1739701435,"author":"/u/lavishclassman","guid":532,"unread":true,"content":"<div><p>Its a less than 400 lines CLI trash manager :) made it for personal use and for fun.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/lavishclassman\"> /u/lavishclassman </a>","contentLength":121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iqpezl/i_created_a_cli_trash_command/"},{"title":"Proj Ideas 💡 - Willing to lock in for Go (2025)","url":"https://www.reddit.com/r/golang/comments/1iqp4re/proj_ideas_willing_to_lock_in_for_go_2025/","date":1739700252,"author":"/u/ComfortableAcadia839","guid":533,"unread":true,"content":"<p>I'm a full stack JS/TS developer but just recently tried Go, built an in memory key-value Redis clone.. I've realised the language makes me enjoy coding ---&gt; </p><p>Can y'all recommend some project ideas (intermediate to advanced difficulty)</p><p>I want to build some solid projects ;)</p>","contentLength":272,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NASA has a list of 10 rules for software development","url":"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm","date":1739696879,"author":"/u/namanyayg","guid":521,"unread":true,"content":"<h2>NASA has a list of 10 rules for software development</h2><p>Those rules were written from the point of view of people writing\nembedded software for extremely expensive spacecraft, where tolerating\na lot of programming pain is a good tradeoff for not losing a mission.\nI do not know why someone in that situation does not use the SPARK\nsubset of Ada, which subset was explicitly designed for verification,\nand is simply a better starting point for embedded programming than C.\n</p><p>I am criticising them from the point of view of people writing\nprogramming language processors (compilers, interpreters, editors)\nand application software.\n</p><p>We are supposed to teach critical thinking.  This is an example.\n</p><ul><li>How have Gerard J. Holzmann's and my different contexts affected\nour judgement?\n</li><li>Can you blindly follow his advice without considering \ncontext?\n</li><li>Can you blindly follow  advice without considering\nyour context?\n</li><li>Would these rules necessarily apply to a different/better\nprogramming language?  What if <a href=\"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm#ppar\">function pointers\nwere tamed</a>?  What if the language provided opaque abstract\ndata types as Ada does?\n</li></ul><h3>1. Restrict all code to very simple control flow constructs —\ndo not use  statements,\n or  constructs,\nand direct or indirect .</h3><p>Note that  and \nare how C does exception handling, so this rule bans any use\nof exception handling.\n\n</p><p>It is true that banning recursion and jumps and loops without\nexplicit bounds means that you  your program is\ngoing to terminate.  It is also true that recursive functions\ncan be proven to terminate about as often as loops can, with\nreasonably well-understood methods.  What's more important here is\nthat “sure to terminate” does not imply\n“sure to terminate in my lifetime”:\n</p><pre>    int const N = 1000000000;\n    for (x0 = 0; x0 != N; x0++)\n    for (x1 = 0; x1 != N; x1++)\n    for (x2 = 0; x2 != N; x2++)\n    for (x3 = 0; x3 != N; x3++)\n    for (x4 = 0; x4 != N; x4++)\n    for (x5 = 0; x5 != N; x5++)\n    for (x6 = 0; x6 != N; x6++)\n    for (x7 = 0; x7 != N; x7++)\n    for (x8 = 0; x8 != N; x8++)\n    for (x9 = 0; x9 != N; x9++)\n        -- do something --;\n</pre><p>This does a bounded number of iterations.  The bound is N.\nIn this case, that's 10.  If each iteration of the loop body\ntakes 1 nsec, that's 10 seconds, or about 7.9×10\nyears.  What is the  difference between “will stop\nin 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000\nyears” and “will never stop”?\n\n</p><p>Worse still, taking a problem that is  expressed\nusing recursion and contorting it into something that manipulates an\nexplicit stack, while possible, turns clear maintainable code into\nbuggy spaghetti.  (I've done it, several times.  There's an example\non this web site.  It is  a good idea.)\n\n</p><h3>2. All loops must have a fixed upper-bound.  It must be trivially\npossible for a checking tool to prove statically that a preset\nupper-bound on the number of iterations of a loop cannot be exceeded.\nIf the loop-bound cannot be proven statically, the rule is considered\nviolated.</h3><p>This is an old idea.  As the example above shows, it is not enough\nby itself to be of any practical use.  You have to try to make the\nbounds reasonably , and you have to regard hitting an\nartificial bound as a run-time error.\n\n</p><p>By the way, note that putting depth bounds on recursive procedures\nmakes them every bit as safe as loops with fixed bounds.\n\n</p><h3>3. Do not use dynamic memory allocation after initialization.</h3><p>This is also a very old idea.  Some languages designed for embedded\nwork don't even  dynamic memory allocation.  The big\nthing, of course, is that embedded applications have a fixed amount of\nmemory to work with, are never going to get any more, and should not\ncrash because they couldn't handle another record.\n\n</p><p>Note that the rationale actually supports a much stronger rule:\ndon't even  dynamic memory allocation.  You can of\ncourse manage your own storage pool:\n</p><pre>    typedef struct Foo_Record *foo;\n    struct Foo_Record {\n\tfoo next;\n\t...\n    };\n    #define MAX_FOOS ...\n    static struct Foo_Record foo_zone[MAX_FOOS];\n    foo foo_free_list = 0;\n\n    void init_foo_free_list() {\n\tfor (int i = MAX_FOOS - 1; i &gt;= 0; i--) {\n\t    foo_zone[i].next = foo_free_list;\n\t    foo_free_list = &amp;foo_zone[i];\n\t}\n    }\n\n    foo malloc_foo() {\n\tfoo r = foo_free_list;\n\tif (r == 0) report_error();\n\tfoo_free_list = r-&gt;next;\n\treturn r;\n    }\n\n    void free_foo(foo x) {\n\tx-&gt;next = foo_free_list;\n\tfoo_free_list = x;\n    }\n</pre><p>This  satisfies the rule, but it\nviolates the  of the rule.  Simulating malloc()\nand free() this way is  than using the real\nthing, because the memory in foo_zone is permanently tied up\nfor Foo_Records, even if we don't need any of those at the\nmoment but do desperately need the memory for something else.\n\n</p><p>What you really need to do is to use a memory allocator\nwith known behaviour, and to prove that the amount of memory\nin use at any given time (data bytes + headers) is bounded\nby a known value.\n\n</p><p>Note also that SPlint can verify at compile time that\nthe errors NASA speak of do not occur.\n\n</p><p>One of the reasons given for the ban is that the performance\nof malloc() and free() is unpredictable.  Are these the only\nfunctions we use with unpredictable performance?  Is there\nanything about malloc() and free() which makes them\n unpredictable?  The existence of\nhard-real-time garbage collectors suggests not.\n\n</p><p>The rationale for this rule says that\n</p><blockquote>\nNote that the only way\nto dynamically claim memory in the absence of memory allocation from the\nheap is to use stack memory.  In the absence of recursion (Rule 1), an\nupper bound on the use of stack memory can derived statically, thus\nmaking it possible to prove that an application will always live within\nits pre-allocated memory means.\n</blockquote><p>Unfortunately, the sunny optimism shown here is unjustified.  Given\nthe ISO C standard (any version, C89, C99, or C11) it is \nto determine an upper bound on the use of stack memory.  There is not even\nany standard way to determine how much memory a compiler will use for the\nstack frame of a given function.  (There could have been.  There just isn't.)\nThere isn't even any requirement that two invocations of the same function\nwith the same arguments will use the same amount of memory.\nSuch a bound can only be calculated for a  version of a\nspecific compiler with specific options.  Here's a trivial example:\n</p><pre>void f() {\n    char a[100000];\n}\n</pre><p>How much memory will that take on the stack?  Compiled for debugging,\nit might take a full stack frame (however big that is) plus traceback\ninformation plus a million bytes for a[].  Compiled with optimisation,\nthe compiler might notice that a[] isn't used, and might even compile\ncalls to f() inline so that they generate no code and take no space.\nThat's an extreme example, but not really unfair.  If you want bounds\nyou can rely on, you had better  what your compiler does,\nand recheck every time anything about the compiler changes.\n\n</p><h3>4.  No function should be longer than what can be printed on\na single sheet of paper in a standard reference format with one line per\nstatement and one line per declaration.  Typically, this means no more\nthan about 60 lines of code per function.</h3><p>Since programmers these days typically read their code on-screen,\nnot on paper, it's not clear why the size of a sheet of paper is\nrelevant any longer.\n\n</p><p>The rule is arguably stated about the wrong thing.  The thing that\nneeds to be bounded is not the size of a function, but the size of a\nchunk that a programmer needs to read and comprehend.\n\n</p><p>There are also question marks about how to interpret this if you\nare using a sensible language (like Algol 60, Simula 67, Algol 68,\nPascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,\nF#, Clean, Haskell, or Fortran) that allows nested procedures.\nSuppose you have a folding editor that presents a procedure to\nyou like this:\n</p><pre>function Text_To_Floating(S: string, E: integer): Double;\n   � variables �\n   � procedure Mul(Carry: integer) �\n   � function Evaluate: Double �\n\n   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;\n   for N := 1 to S.length do begin\n       C := S[N];\n       if C = '.' then begin\n          Point := -1\n       end else\n       if C = '_' then begin\n          Base := Round(Evaluate);\n          Max, Min, Power := 0, 1, 0\n       end else\n       if Char ≠ ' ' then begin\n          Q := ord(C) - ord('0');\n          if Q &gt; 9 then Q := ord(C) - ord('A') + 10\n          Power := Point + Point\n          Mul(Q)\n       end\n    end;\n    Power := Power + Exp;\n    Value := Evaluate;\n    if Sign &lt; 0 then Value := -Value;\nend;\n</pre><p>which would be much bigger if the declarations\nwere expanded out instead of being hidden behind �folds�.\nWhich size do we count?  The folded size or the unfolded size?\n</p><p>I was using a folding editor called Apprentice on the Classic Mac\nback in the 1980s.  It was written by Peter McInerny and was lightning\nfast.\n\n</p><h3>5.  The  of the code should average to a minimum of\ntwo assertions per function.</h3><p>Assertions are wonderful documentation and the very best debugging tool\nI know of.  I have never seen any real code that had too many assertions.\n\n</p><p>The example here is one of the ugliest pieces of code I've seen in a while.\n</p><pre>if (!c_assert(p &gt;= 0) == true) {\n    return ERROR;\n}\n</pre><p>It should, of course, just be\n</p><pre>if (!c_assert(p &gt;= 0)) {\n    return ERROR;\n}\n</pre><p>Better still, it should be something like\n</p><pre>#ifdef NDEBUG\n#define check(e, c) (void)0\n#else\n#define check(e, c) if (!(c)) return bugout(c), (e)\n#ifdef NDEBUG_LOG\n#define bugout(c) (void)0\n#else\n#define bugout(c) \\\n    fprintf(stderr, \"%s:%d: assertion '%s' failed.\\n\", \\\n    __FILE__, __LINE__, #s)\n#endif\n#endif\n</pre><p>Ahem.  The more interesting part is the required density.\nI just checked an open source project from a large telecoms\ncompany, and 23 out of 704 files (not functions) contained\nat least one assertion.  I just checked my own Smalltalk\nsystem and one SLOC out of every 43 was an assertion, but\nthe average Smalltalk “function” is only a few\nlines.  If the biggest function allowed is 60 lines, then\nlet's suppose the average function is about 36 lines, so\nthis rule requires 1 assertion per 18 lines.\n</p><p>Assertions are good, but what they are especially good\nfor is expressing the requirements on data that come\nfrom outside the function.  I suggest then that\n</p><ul><li>Every argument whose validity is not guaranteed by\nits typed should have an assertion to check it.\n</li><li>Every datum that is obtained from an external\nsource (file, data base, message) whose validity is\nnot guaranteed by its type should have an assertion\nto check it.\n</li></ul><p>The NASA 10 rules are written for embedded systems, where\nreading stuff from sensors is fairly common.\n\n</p><h3>6.  Data objects must be declared at the smallest possible level of\nscope.</h3><p>This is excellent advice, but why limit it to data objects?\nOh yeah, the rules were written for crippled languages where you\n declare functions in the right place.\n\n</p><p>People using Ada, Pascal (Delphi), JavaScript, or functional\nlanguages should also declare types and functions as locally as\npossible.\n\n</p><h3>7.  The return value of non-void functions must be checked by each\ncalling function, and the validity of parameters must be checked inside\neach function.</h3><p>This again is mainly about C, or any other language that indicates\nfailure by returning special values.  “Standard libraries\nfamously violate this rule”?  No, the  library does.\n\n</p><p>You have to be reasonable about this: it simply isn't practical\nto check  aspect of validity for \nargument.  Take the C function\n</p><pre>void *bsearch(\n    void const *key  /* what we are looking for */,\n    void const *base /* points to an array of things like that */,\n    size_t      n    /* how many elements base has */,\n    size_t      size /* the common size of key and base's elements */\n    int (*      cmp)(void const *, void const *)\n);\n</pre><p>This does a binary search in an array.  We must have key≠0,\nbase≠0, size≠0, cmp≠0, cmp(key,key)=0, and for all\n1&lt;i&lt;n,\n</p><pre>cmp((char*)base+size*(i-1), (char*)base+size*i) &lt;= 0\n</pre><p>Checking the validity in full would mean checking\nthat [key..key+size) is a range of readable addresses,\n[base..base+size*n) is a range of readable addresses,\nand doing n calls to cmp.  But the whole point of binary\nsearch is to do O(log(n)) calls to cmp.\n\n</p><p>The fundamental rules here are\n</p><ul><li>Don't let run-time errors go un-noticed, and\n</li><li>any check is safer than no check.\n</li></ul><h3>8. The use of the preprocessor must be limited to the inclusion of\nheader files and simple macro definitions.  Token pasting, variable\nargument lists (ellipses), and recursive macro calls are not allowed.</h3><p>Recursive macro calls don't really work in C, so no quarrel there.\nVariable argument lists were introduced into macros in\nC99 so that you could write code like\n</p><pre>#define err_printf(level, ...) \\\n    if (debug_level &gt;= level) fprintf(stderr, __VA_ARGS__)\n...\n    err_printf(HIGH, \"About to frob %d\\n\", control_index);\n</pre><p>This is a  thing; conditional tracing like this is a\npowerful debugging aid.  It should be , not banned.\n\n</p><p>The rule goes on to ban macros that expand into things that are\nnot complete syntactic units.  This would, for example, prohibit\nsimulating try-catch blocks with macros.  (Fair enough, an earlier rule\nbanned exception handling anyway.)  Consider this code fragment, from\nan actual program.\n</p><pre>    row_flag = border;     \n    if (row_flag) printf(\"\\\\hline\");\n    for_each_element_child(e0, i, j, e1)\n        printf(row_flag ? \"\\\\\\\\\\n\" : \"\\n\");\n        row_flag = true;  \n        col_flag = false;\n        for_each_element_child(e1, k, l, e2)\n            if (col_flag) printf(\" &amp; \");\n            col_flag = true;\n            walk_paragraph(\"\", e2, \"\");\n        end_each_element_child\n    end_each_element_child\n    if (border) printf(\"\\\\\\\\\\\\hline\");\n    printf(\"\\n\\\\end{tabular}\\n\");\n</pre><p>It's part of a program converting slides written in something like HTML\ninto another notation for formatting.  The \n…  loops walk over a tree.  Using\nthese macros means that the programmer has no need to know and no reason to\ncare how the tree is represented and how the loop actually works.\nYou can easily see that  must have at\nleast one unmatched { and  must have at least one\nunmatched }.  That's the kind of macro that's banned by requiring\ncomplete syntactic units.  Yet the readability and maintainability of\nthe code is  improved by these macros.\n\n</p><p>One thing the rule covers, but does not at the beginning stress, is\n“no  macro processing”.  That is,\nno #if.  The argument against it is, I'm afraid, questionable.  If there\nare 10 conditions, there are 2 combinations to test,\nwhether they are expressed as compile-time conditionals or run-time\nconditionals.\n\n</p><p>In particular, the rule against conditional macro processing\nwould prevent you defining your own <a href=\"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm#check\">assertion macros</a>.\nIt is not obvious that that's a good idea.\n\n</p><h3>9.  The use of pointers should be restricted.  Specifically, no more\nthan one level of dereferencing is allowed.  Pointer dereference\noperations may not be hidden in macro definitions or inside typedef\ndeclarations.  Function pointers are not permitted.</h3><p>Let's look at the last point first.\n\n</p><pre>double integral(double (*f)(double), double lower, double upper, int n) {\n    // Compute the integral of f from lower to upper \n    // using Simpson's rule with n+1 points.\n    double const h = (upper - lower) / n;\n    double       s;\n    double       t;\n    int          i;\n    \n    s = 0.0;\n    for (i = 0; i &lt; n; i++) s += f((lower + h/2.0) + h*i);\n    t = 0.0;\n    for (i = 1; i &lt; n; i++) t += f(lower + h*i);\n    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);\n}\n</pre><p>This kind of code has been important in numerical calculations since\nthe very earliest days.  Pascal could do it.  Algol 60 could do it.\nIn the 1950s, Fortran could do it.  And NASA would ban it, because in\nC,  is a function pointer.\n\n</p><p>Now it's important to write functions like this once and only once.\nFor example, the code has at least one error.  The comment says n+1\npoints, but the function is actually evaluated at 2n+1 points.  If we\nneed to bound the number of calls to f in order to meet a deadline,\nhaving that number off by a factor of two will not help.\n</p><p>It's nice to have just one place to fix.\nPerhaps I should not have copied that code from a well-known source (:-).\nCertainly I should not have more than one copy!\n\n</p><p>What can we do if we're not allowed to use function pointers?\nSuppose there are four functions foo, bar, ugh, and zoo that we need\nto integrate.  Now we can write\n</p><pre>enum Fun {FOO, BAR, UGH, ZOO};\n\ndouble call(enum Fun which, double what) {\n    switch (which) {\n        case FOO: return foo(what);\n        case BAR: return bar(what);\n        case UGH: return ugh(what);\n        case ZOO: return zoo(what);\n    }\n}\n\ndouble integral(enum Fun which, double lower, double upper, int n) {\n    // Compute the integral of a function from lower to upper \n    // using Simpson's rule with n+1 points.\n    double const h = (upper - lower) / n;\n    double       s;\n    double       t;\n    int          i;\n    \n    s = 0.0;\n    for (i = 0; i &lt; n; i++) s += call(which, (lower + h/2.0) + h*i);\n    t = 0.0;\n    for (i = 1; i &lt; n; i++) t += call(which, lower + h*i);\n    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);\n}\n</pre><p>Has obeying NASA's rule made the code more reliable?  No, it has made\nthe code  to understand,  maintainable, and\n that it wasn't before.  Here's a call\nillustrating the mistake:\n</p><pre>x = integral(4, 0.0, 1.0, 10);</pre><p>I have checked this with two C compilers and a static checker at their\nhighest settings, and they are completely silent about this.\n\n</p><p>So there are legitimate uses for function pointers, and simulating\nthem makes programs , not better.\n\n</p><p>Now  in Fortran,\nAlgol 60, or Pascal.  Those languages had procedure \nbut not procedure . You could pass a subprogram name as\na parameter, and such a parameter could be passed on, but you could not\nstore them in variables.  You could have a  of C which\nallowed function pointer parameters, but made all function pointer\nvariables read-only.  That would give you a statically checkable subset\nof C that allowed integral().\n\n</p><p>The other use of function pointers is simulating object-orientation.\nImagine for example\n</p><pre>struct Channel {\n    void (*send)(struct Channel *, Message const *);\n    bool (*recv)(struct Channel *, Message *);\n    ...\n};\ninline void send(struct Channel *c, Message const *m) {\n    c-&gt;send(c, m);\n}\ninline bool recv(struct Channel *c, Message *m) {\n    return c-&gt;recv(c, m);\n}\n</pre><p>This lets us use a common interface for sending and receiving\nmessages on different kinds of channels.  This approach has been\nused extensively in operating systems (at least as far back as\nthe Burroughs MCP in the 1960s) to decouple the code that uses\na device from the actual device driver.     I would expect any\nprogram that controls more than one hardware device to do something\nlike this.  It's one of our key tools for controlling complexity.\n</p><p>Again, we can simulate this, but it makes adding a new kind of\nchannel harder than it should be, and the code is \nwhen we do it, not better.\n\n</p><p>The rule against more than one level of dereferencing is also\nan assault on good programming.  One of the key ideas that was\ndeveloped in the 1960s is the idea of ;\nthe idea that it should be possible for one module to define a\ndata type and operations on it and another module to use instances\nof that data type and its operations <em>without having to know\nanything about what the data type is</em>.\n</p><p>One of the things I detest about Java is that it spits in the\nface of the people who worked out that idea.  Yes, Java (now) has\ngeneric type parameters, and that's good, but you cannot use a\n type without knowing what that type is.\n\n</p><p>Suppose I have a module that offers operations\n</p><ul></ul><p>And suppose that I have two interfaces in mind.  One of them\nuses integers as tokens.\n</p><pre>// stasher.h, version 1.\ntypedef int token;\nextern token stash(item);\nextern item  recall(token);\nextern void  delete(token);\n</pre><p>Another uses pointers as tokens.\n</p><pre>// stasher.h, version 2.\ntypedef struct Hidden *token;\nextern  token stash(item);\nextern  item  recall(token);\nextern  void  delete(token);\n</pre><pre>void snoo(token *ans, item x, item y) {\n    if (better(x, y)) {\n\t*ans = stash(x);\n    } else {\n\t*ans = stash(y);\n    }\n}\n</pre><p>By the NASA rule, the function snoo() would not be accepted or rejected on\nits own merits.  With stasher.h, version 1, it would be accepted.\nWith stasher.h, version 2, it would be rejected.\n\n</p><p>One reason to prefer version 2 to version 1 is that version 2 gets\nmore use out of type checking.  There are ever so many ways to get an\nint in C.  Ask yourself if it ever makes sense to do\n</p><pre>token t1 = stash(x);\ntoken t2 = stash(y);\ndelete(t1*t2);\n</pre><p>I really do not like the idea of banning abstract data types.\n\n</p><h3>10.  All code must be compiled, from the first day of development,\nwith all compiler warnings enabled at the compiler’s\nmost pedantic setting.  All code must compile with these setting without\nany warnings.  All code must be checked daily with at least one, but\npreferably more than one, state-of-the-art static source code analyzer\nand should pass the analyses with zero warnings.</h3><p>This one is good advice.  Rule 9 is really about making your code\nworse in order to get more benefit from limited static checkers.  (Since\nC has no standard way to construct new functions at run time, the set of\nfunctions that a particular function pointer  point to can\nbe determined by a fixed-point data flow analysis, at least for most\nprograms.)  So is rule 1.  \n\n\n\n</p>","contentLength":21484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqode3/nasa_has_a_list_of_10_rules_for_software/"},{"title":"Resigning as Asahi Linux project lead","url":"https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/","date":1739696473,"author":"/u/namanyayg","guid":530,"unread":true,"content":"<p>Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them “jailbreaks” these days) to allow users to run their own unofficial apps on the Nintendo Wii.</p><p>I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other people’s work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didn’t attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.</p><p>When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.</p><p>I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.</p><p>Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which I’ve already spoken at length about and I won’t repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience. (<em>Clarification: This has nothing to do with Rust at this point, it’s well before R4L was even merged. Upstreaming to Linux is a terrible experience in C too.</em>)</p><p>But then also came the entitled users. This time, it wasn’t about stealing games, it was about features. “When is Thunderbolt coming?” “Asahi is useless to me until I can use monitors over USB-C” “The battery life sucks compared to macOS” (nobody ever complained when compared to x86 laptops…) “I can’t even check my CPU temperature” (yes, I seriously got that one). (<em>Edit: This wasn’t just a few instances; I’ve seen variations on the first three posted hundreds of times by now, including takes like “Thunderbolt/DP Alt are never going to happen”. A few times is fine, but the same thing repeated over and over again every day while we’re trying to make these things happen will get to anyone.</em>)</p><p>And, of course, “When is M3/M4 support coming?”</p><p>For a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were “alpha” and “unstable” and “not suitable for a daily driver” (despite <a href=\"https://stats.asahilinux.org\">thousands of users</a>, myself included, daily driving it and even using it for servers).</p><p>No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.</p><p>I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.</p><p>Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I won’t go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).</p><p>I did make some progress in 2024, but this left me in a very vulnerable position. I hadn’t gotten nearly as much Asahi work done as I’d liked, and the users weren’t getting any quieter about demanding more features and machine support.</p><p>We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).</p><p>I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. “Full” DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasn’t the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did <a href=\"https://lore.kernel.org/lkml/fda8b831-1ffc-4087-8e7b-d97779b3ecc5@marcan.st/T/#u\">try</a> to bring the subject up on the mailing lists, but it didn’t get much response).</p><p>The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I won’t repeat them in detail here. Suffice it to say, I consider Linus’ handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.</p><p>As you know, this is deeply personal to me, as we’ve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.</p><p>Some might be wondering why we can’t just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But there’s more.</p><p>In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (that’s how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a <a href=\"https://pagure.io/fedora-asahi/mesa-asahi-flatpak\">partial workaround</a> for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.</p><p>I’m not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated <a href=\"https://lore.kernel.org/lkml/20250208204416.GL1130956@mit.edu/\">“We\nare the ‘thin blue line’”</a>, and nobody cared, which just further confirmed to me that I don’t want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to <a href=\"https://lore.kernel.org/lkml/20240828211117.9422-1-wedsonaf@gmail.com/\">quit</a>.</p><p>But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than <a href=\"https://bunny.net/\">bunny.net</a> giving us free CDN credits for the hosting).</p><p>I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say I’m the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they won’t touch the Linux kernel with a 10-foot pole.</p><p>When Apple released the M1, Linus Torvalds <a href=\"https://thenextweb.com/news/linus-torvalds-wants-apples-new-m1-powered-macs-to-run-linux\">wished it could run Linux</a>, but didn’t have much hope it would ever happen. We made it happen, and Linux 5.19 was <a href=\"https://lore.kernel.org/lkml/CAHk-=wgrz5BBk=rCz7W28Fj_o02s0Xi0OEQ3H1uQgOdFvHgx0w@mail.gmail.com/T/#u\">released from an M2 MacBook Air running Asahi Linux</a>. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him <a href=\"https://gist.github.com/marcan/fe70ee6648f3d5ae94eb8332265b8d95\">an invitation</a> to discuss the challenges of kernel contributions and maintenance and see how we could help. He never replied.</p><p>Back in 2011, Con Kolivas <a href=\"https://web.archive.org/web/20110707151924/http://apcmag.com/why_i_quit_kernel_developer_con_kolivas.htm\">left the Linux kernel community</a>. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if you’re not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.</p><p>When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but it’s not fun any more. I have an M3 Pro in a box and I haven’t even turned it on yet. I dread doing the bring-up work. It doesn’t feel worth the trouble.</p><p>I miss having free time where I can relax and not worry about the features we haven’t shipped yet. I miss <a href=\"https://youtube.com/@TsuiokuCircuit\">making music</a>. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we haven’t upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.</p><p>I’m resigning as lead of the Asahi Linux project, effective immediately. The project will <a href=\"https://asahilinux.org/2025/02/passing-the-torch/\">continue on without me</a>, and I’m working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the <a href=\"https://opencollective.com/asahilinux\">Asahi Linux OpenCollective</a> (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).</p><p>I want to thank the entire Asahi Linux team, without whom I would’ve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.</p><p>If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: <a href=\"mailto:marcan@marcan.st\">marcan@marcan.st</a>.</p><p>: A lot of the discussion around this post and the interactions that led to it brings up the term “brigading”. Please read <a href=\"https://hachyderm.io/@chandlerc/114001000657957325\">this excellent Fedi post</a> for a discussion of what is and isn’t brigading.</p>","contentLength":15365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqoa4n/resigning_as_asahi_linux_project_lead/"},{"title":"Which approach to rust is more idiomatic (Helix vs Zed)?","url":"https://www.reddit.com/r/rust/comments/1iqnats/which_approach_to_rust_is_more_idiomatic_helix_vs/","date":1739692206,"author":"/u/No_Penalty2781","guid":529,"unread":true,"content":"<p>Hi! I am curious what is the current \"meta\" (by \"meta\" I mean the current rust's community  and  way of doing things) of rust programming. I am studying source code of 2 editors I am using: <a href=\"https://github.com/helix-editor/helix/\">Helix</a> and <a href=\"https://github.com/zed-industries/zed/\">Zed</a>. And I can see that while they are doing a lot of similar things (like using LSP and parsing it outputs for example) the code is kinda different.</p><p>It starts from the file structure: in Helix there are not that many folders to look at (like you have <a href=\"https://github.com/helix-editor/helix/tree/master/helix-core\">helix-core</a> which contains features like \"diagnostic\", \"diff\", \"history\", etc but in Zed every single one of them is a different crate , which approach is more \"idiomatic\"? To divide every feature as a separate crate or to use more \"packed\" crates like \"core\".</p><p>Then the code itself is kinda different, for example I am currently looking at LSP implementation in both of them and in Helix's case I can follow along and understand the code much more easily (here is the <a href=\"https://github.com/helix-editor/helix/blob/master/helix-lsp/src/lib.rs\">file</a> I am referring to. But in Zed's case it is kinda hard to understand the code because of \"type level programming\" stuff like <a href=\"https://github.com/zed-industries/zed/blob/main/crates/lsp/src/lsp.rs#L397\">this one</a> for example. It also doesn't help that files have a lot of SLOC in them (over 1500 in normal in Zed's repository, is it also how you do rust?) Maybe I am just used to lean functions from other languages (I mainly did TypeScript and Elixir in my career).</p><p>Other thing I see is that Helix has more comments about \"why the thing is doing that in the first place\" which I find very helpful (on the other hand in seems that Zed's is abusing a lot of \"type level\" programming to have a self-documented code but it is harder to reason about at least for me) which approach here you prefer?</p>","contentLength":1635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] A Survey of Logical Reasoning Capabilities in Large Language Models: Frameworks, Methods, and Evaluation","url":"https://www.reddit.com/r/MachineLearning/comments/1iqmjal/r_a_survey_of_logical_reasoning_capabilities_in/","date":1739688936,"author":"/u/Successful-Western27","guid":522,"unread":true,"content":"<p>This new survey provides a comprehensive analysis of logical reasoning capabilities in LLMs, examining different reasoning types, evaluation methods, and current limitations.</p><p>Key technical aspects: - Categorizes logical reasoning into deductive, inductive, and abductive frameworks - Evaluates performance across multiple benchmarks and testing methodologies - Analyzes the relationship between model size and reasoning capability - Reviews techniques for improving logical reasoning, including prompt engineering and chain-of-thought methods</p><p>Main findings: - LLMs show strong performance on basic logical tasks but struggle with complex multi-step reasoning - Model size alone doesn't determine reasoning ability - training methods and problem-solving strategies play crucial roles - Current evaluation methods may not effectively distinguish between true reasoning and pattern matching - Performance degrades significantly when problems require combining multiple reasoning types</p><p>I think the most important contribution here is the systematic breakdown of where current models succeed and fail at logical reasoning. This helps identify specific areas where we need to focus research efforts, rather than treating reasoning as a monolithic capability.</p><p>I think this work highlights the need for better benchmarks - many current tests don't effectively measure true reasoning ability. The field needs more robust evaluation methods that can differentiate between memorization and actual logical inference.</p><p>TLDR: Comprehensive survey of logical reasoning in LLMs showing strong basic capabilities but significant limitations in complex reasoning. Highlights need for better evaluation methods and targeted improvements in specific reasoning types.</p>","contentLength":1740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fluvio: A Rust-powered streaming platform using WebAssembly for programmable data processing","url":"https://www.reddit.com/r/rust/comments/1iqgg02/fluvio_a_rustpowered_streaming_platform_using/","date":1739667635,"author":"/u/drc1728","guid":528,"unread":true,"content":"<div><p>I am in the process of writing an essay on composable streaming first architecture for data intensive applications. I am thinking of it as a follow up on this article.</p><p>Quick question for the Rust community:</p><ul><li>What information would help the Rust community know and experience Fluvio?</li><li>What would you like to see covered in the essay?</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/drc1728\"> /u/drc1728 </a>","contentLength":357,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safe elimination of unnecessary bound checks.","url":"https://www.reddit.com/r/rust/comments/1iqev5s/safe_elimination_of_unnecessary_bound_checks/","date":1739663006,"author":"/u/tjientavara","guid":499,"unread":true,"content":"<p>Hi, I am working on a Unicode database that is pretty fast, it is a 2 step associated lookup.</p><p>Here is the code for getting the east-asian-width value of a Unicode code-point. Pay specific attention to the function. This function is a  function and the byte tables that it references are  as well. This will allow you to eventually run the unicode algorithms at both compile and run-time.</p><p>Since the tables are fixed at compile time, I can proof that all values from the table will result in values that will never break any bounds, so technically the bound checks are unnecessary.</p><p>There are two bound checks in the assembly output for this function.</p><ul><li>The check before accessing the EAST_ASIAN_WIDTH_COLUMN table (I use an assert! to do this, otherwise there will be double bound check).</li><li>And the check on the conversion to the enum.</li></ul><p>The two bound checks are the two compare + conditional-jump instructions in this code.</p><p>I could increase the size of the column table to remove one of the bound checks, but I want to keep the table small if possible.</p><p>Is there a way to safely (I don't want to use the unsafe code) proof to the compiler that those two checks are unnecessary?</p><p>P.S. technically there is a bound check before the index table a CMOV instruction, but it doubles as a way to also decompress the index table (last entry is repeated), so I feel this is not really a bound check.</p><p>I was able to concat the two tables, and use a byte offset. So now there is no way to get an out of bound access, and the bound checks are no longer emitted by the compiler.</p><p>I also added a manual check for out of bound on the enum and return zero instead, this becomes a CMOV and it eliminated all the panic code from the function.</p>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rke2 and DNS","url":"https://www.reddit.com/r/kubernetes/comments/1iqdela/rke2_and_dns/","date":1739658902,"author":"/u/Affectionate_Horse86","guid":515,"unread":true,"content":"<p>I'm going crazy trying to get coredns to talk to my DNS server for names in my domain (I'm using a pihole server that is updated by terraform for VM addresses and by external-dns for k8s services)</p><p>I'm using lablabs ansible role, but a pure rke2 answer is fine, I can figure out the rest. I have</p><pre><code> dest: /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml content: | apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-coredns namespace: kube-system spec: valuesContent: |- nodelocal: enabled: true ipvs: true zoneFiles: - filename: my-domain.com.conf domain: my-domain.com contents: | my-domain.com:53 { errors cache 30 forward . 10.0.200.1 # my Pihole DNS server } extraConfig: import: parameters: /etc/coredns/my-domain.com.conf when: rke2_type == \"server\" </code></pre><p>and this should have the effect of instructing coredns to use my DNS server for everyting in 'my-domain.com', but although this part lands in the appropriate config map, it doesn't seem to do any good.</p><p>I can replace coredns completely with kubelet flags, but then I lose the resolution of cluster addresses and I don;t get too far in bringing the cluster up.</p>","contentLength":1146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating my OS","url":"https://www.reddit.com/r/linux/comments/1iqaxku/creating_my_os/","date":1739652372,"author":"/u/zainali28","guid":517,"unread":true,"content":"<p>Now, I know it sounds absurd, but I just want to understand the general workflow of how do you design a linux, or a unix-based OS.</p><p>I have a fair knowledge of computer architecture and can understand low level language of the computer.</p><p>I am just an enthusiast who wants to just make a functional os, with just a terminal that is able to execute things.</p><p>Any advice is greatly appreciated!</p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon AWS \"whoAMI\" Attack Exploits AMI Name Confusion to Take Over Cloud Instances","url":"https://www.reddit.com/r/programming/comments/1iqav3c/amazon_aws_whoami_attack_exploits_ami_name/","date":1739652192,"author":"/u/Dark-Marc","guid":513,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incoming Rust intern need advice?","url":"https://www.reddit.com/r/rust/comments/1iq9oph/incoming_rust_intern_need_advice/","date":1739649133,"author":"/u/Helpful_Ad_9930","guid":471,"unread":true,"content":"<p>Hey everyone, I'm a 19-year-old college student who just landed a SWE internship at NVIDIA! My manager has me learning Rust and exploring one of its libraries, and I’m also reading up on operating systems and computer networking. I'm almost done with the OS book and plan to start the networking one next week.</p><p>I do have a bit of experience with embedded systems I completed two internships during my freshman year. However, so far I’m really enjoying Rust. I am quite a rookie compared to you experienced folks haha! But so far I love how Rust's compiler enforces safety, how Cargo makes dependency management a breeze compared to CMake, and the whole concept of ownership and borrowing is just super cool.</p><p>At the moment, I’m nearly finished with the Rust book. I am on the concurrency chapter. Guess I am just wondering what next? I really want this return offer and I just want to blow this opportunity out the park. I go too a state school and my manager told me he has high expectations for me after my interviews. I just do not want to let him down you know also plus kind of getting impostor syndrome a bit seeing all the other interns coming from schools such as MIT, Harvard, Standford, etc. Sorry for the vent I guess I just want to prove my worth? and show my manager they made the right choice?</p><ul><li>What fun, Rust projects have helped you learn a lot?</li><li>Are there any books you’d recommend that could help me out for the summer?</li></ul><p><strong>Books I want to read before I start summer:</strong></p><ul><li>Operating Systems (Three easy pieces)</li><li>Beej's Guide to Network Programming</li><li>C++ Concurrency in Action</li></ul>","contentLength":1578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Networking in K8s","url":"https://www.reddit.com/r/kubernetes/comments/1iq9mqp/networking_in_k8s/","date":1739648990,"author":"/u/I-Ad-7","guid":270,"unread":true,"content":"<p>Background: Never used k8s before 4 months ago. I would say I’m pretty good at picking up new stuff and already have lots of knowledge and hands on experience (mostly from doing stuff on my own and reading lots of Oreilly books) for someone like me (age 23). Have a CS background. Doing an internship. </p><p>I was put into a position where I had to use K8s for everyday work and don’t get me wrong I’m ecstatic about being an intern but already having the opportunity to work with deployments etc. </p><p>What I did was read The kubernetes book by Nigel Poulton and got myself 3 cheap PCs and bootstrapped myself a K3s cluster and installed Longorn as the storage and Nginx as the ingress controller.</p><p>Right now I can pretty much do most stuff and have some cool projects running on my cluster.</p><p>I’m also learning new stuff every day. </p><p>But where I find myself lacking is Networking. Not just in Kubernetes but also generally. </p><p>There are two examples of me getting frustrated because of my lacking networking knowledge:</p><ul><li><p>I wanted to let a GitHub actions step access my cluster through the tailscale K8s operator which runs on my cluster but failed</p></li><li><p>Was wondering why I can’t see the real IPs of people that are accessing my api which is on a pod on my cluster and got intimidated by stuff like Layer 2 Networking and why you need a load balancer for that etc.</p></li></ul><p>Do I really have to be as competent as a network engineer to be a good dev ops engineer / data engineer / cloud engineer or anything in ops?</p><p>I don’t mind it but I’m struggling to learn Networking and it’s not that I don’t have the basics but I don’t have the advanced knowledge needed yet, so how do I actually get there?</p>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Is my company missing out by avoiding deep learning?","url":"https://www.reddit.com/r/MachineLearning/comments/1iq9gtk/d_is_my_company_missing_out_by_avoiding_deep/","date":1739648562,"author":"/u/DatAndre","guid":257,"unread":true,"content":"<p>Disclaimer: obviously it does not make sense to use a neural network if a linear regression is enough. </p><p>I work at a company that strictly adheres to mathematical, explainable models. Their stance is that methods like Neural Networks or even Gradient Boosting Machines are too \"black-box\" and thus unreliable for decision-making. While I understand the importance of interpretability (especially in mission critical scenarios) I can't help but feel that this approach is overly restrictive. </p><p>I see a lot of research and industry adoption of these methods, which makes me wonder: are they really just black boxes, or is this an outdated view? Surely, with so many people working in this field, there must be ways to gain insights into these models and make them more trustworthy. </p><p>Am I also missing out on them, since I do not have work experience with such models?</p><p>EDIT: Context is formula one! However, races are a thing and support tools another. I too would avoid such models in anything strictly related to a race, unless completely necessary. I just feels that there's a bias that is context-independent here. </p>","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How are you monitoring your cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1iq94yg/how_are_you_monitoring_your_cluster/","date":1739647696,"author":"/u/psavva","guid":269,"unread":true,"content":"<p>I have a 3 node bare metal cluster and installed Kube Prometheus Stack helm chart.</p><p>I'm having a very hard time getting the service monitors working correctly. I have any 30% of the 150 or so service monitors failing.</p><p>CPU and networking are always displaying 'No Data'</p><p>I fixed the bind addresses for etdc, scheduler, Kube proxy, controller manager from 127.0.0.1 to bind to 0.0.0.0</p><p>That fixes the alerts on a fresh install of the stack. </p><p>1) CPU Metrics 2) Network Metrics 3) Resource Dashboards are all not working properly (Namespace and pods are always empty,) 4) Service Monitors failing.</p><p>I'm using the latest version of the stack on bare metal cluster 1.31, running calico as a CNI.</p><p>Any advice would be appreciated.</p><p>If anyone has a fully working example of the helm chart values that fully work, that would be awesome.</p>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zed for golang","url":"https://www.reddit.com/r/golang/comments/1iq8jsm/zed_for_golang/","date":1739646156,"author":"/u/MrBricole","guid":518,"unread":true,"content":"<p>I am considering using zed for writting go. Is it working out of the box with full syntax high light for noob like me such fmt.Println() ? I mean, I need to have it displaying functions under an import library.</p><p>Should I give it a try or is it only for advanced users ? </p>","contentLength":268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pushing autovectorization to the limit: utf-8 validator","url":"https://www.reddit.com/r/rust/comments/1iq7yn2/pushing_autovectorization_to_the_limit_utf8/","date":1739644600,"author":"/u/Laiho3","guid":483,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Laiho3\"> /u/Laiho3 </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Questions around LoadBalancer","url":"https://www.reddit.com/r/kubernetes/comments/1iq7y2v/questions_around_loadbalancer/","date":1739644561,"author":"/u/HahaHarmonica","guid":525,"unread":true,"content":"<p>New to k8s. I’ve deployed rke2 and i’ve got several questions. </p><p>Main Question) So i’m trying to install rancher UI on it. When you go to install with helm it asks for a “hostname” and the hostname should be the name of your load balancer…i enabled the load balancer of rke2 but I have no clue how to operate with it…how do I change the configuration to point to rancher? The instructions aren’t very clear on the rke2 site on how to use it other than setting the enable-loadbalancer flag. </p><p>2) During my debugging, i ran the command “kubectl get pods -A -o wide. I have a server node and an agent node. In the column of IP it showed the two IPs of the sever and agent. What was odd was that it showed pods running that were running on the agent node that shouldn’t have been running since I stopped the agent service on the agent node and I ran the kill all script. So how in the world can the containers supposedly running on the agent node…actually be running.</p><p>3) I had some problems with ports not opened initially. Forgot to apply the reload command to make sure the ports were open. I then ran systemctl restart rke2-server on the sever and then systemctl restart rke2-agent on the agent and it was still broken. I finally after 30 min of thinking that wasn’t the problem completely resetting the services by running the killall scripts on both of them before it works…so why in the world won’t k8s actually respect systemctl and restart properly without literally shutting everything down. </p>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing encode: Encoders/serializers made easy.","url":"https://www.reddit.com/r/rust/comments/1iq6pz7/introducing_encode_encodersserializers_made_easy/","date":1739641353,"author":"/u/Compux72","guid":282,"unread":true,"content":"<p> is a toolbox for building encoders and serializers in Rust. It is heavily inspired by the <a href=\"https://docs.rs/winnow/latest/winnow/\"></a> and <a href=\"https://docs.rs/nom/latest/nom/\"></a> crates, which are used for building parsers. It is meant to be a companion to these crates, providing a similar level of flexibility and ease of use for reversing the parsing process.</p><p>The main idea behind  is to provide a set of combinators for building serializers. These combinators can be used to build complex encoders from simple building blocks. This makes it easy to build encoders for different types of data, without having to write a lot of boilerplate code.</p><p>Another key feature of  is its support for  environments. This makes it suitable for use in embedded systems, where the standard library (and particularly the [] module) is not available.</p><p>See the <a href=\"https://github.com/Altair-Bueno/encode/tree/master/examples\"></a> folder for some examples of how to use . Also, check the <a href=\"https://docs.rs/encode/0.1.0/encode/combinators/index.html\"></a> module for a list of all the combinators provided by the crate.</p><ul><li>Ready to use combinators for minimizing boilerplate.</li></ul><ul><li>: Enables the  feature.</li><li>: Enables the use of the standard library.</li><li>: Enables the use of the  crate.</li><li>: Implements [] for [].</li></ul><h3>Why the  trait instead of ?</h3><blockquote><p>A buffer stores bytes in memory such that write operations are . The underlying storage may or may not be in contiguous memory. A BufMut value is a cursor into the buffer. Writing to BufMut advances the cursor position.</p></blockquote><p>The bytes crate was never designed with falible writes nor  targets in mind. This means that targets with little memory are forced to crash when memory is low, instead of gracefully handling errors.</p><h3>Why the  trait instead of ?</h3><ul><li>Because there is no alternative, at least that i know of, that supports  properly</li><li>Because it's easier to work with than  and </li><li>Because using  with binary data often leads to a lot of boilerplate</li></ul>","contentLength":1715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lil guy is trying his best","url":"https://www.reddit.com/r/artificial/comments/1iq6dyy/lil_guy_is_trying_his_best/","date":1739640466,"author":"/u/MetaKnowing","guid":292,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transition from C++ to Rust","url":"https://www.reddit.com/r/rust/comments/1iq67vq/transition_from_c_to_rust/","date":1739640014,"author":"/u/Dvorakovsky","guid":281,"unread":true,"content":"<p>Guys, are here any people who were learning/coding in C++ and switched to Rust. How do you feel? I mean I could easily implement linked lists: singly, doubly in c++, but when I saw how it is implemented in Rust I'd say I got lost completely. I'm only learning rust... So yeah, I really like ownership model even tho it puts some difficulties into learning, but I think it's a benefit rather than a downside. Even tho compared to C++ syntax is a bit messy for me</p>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No, your GenAI model isn't going to replace me","url":"https://marioarias.hashnode.dev/no-your-genai-model-isnt-going-to-replace-me","date":1739639200,"author":"/u/dh44t","guid":279,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq5wq0/no_your_genai_model_isnt_going_to_replace_me/"},{"title":"Type safe Go money library beta2!","url":"https://www.reddit.com/r/golang/comments/1iq5stk/type_safe_go_money_library_beta2/","date":1739638928,"author":"/u/HawkSecure4957","guid":288,"unread":true,"content":"<p>Hello, after I released beta1, I received many constructive feedback! mainly lacking of locale support.</p><p>This update brings locale formatting support and an improved interface for better usability. With Fulus, you can perform monetary operations safely and type-soundly. Plus, you can format money for any locale supported by CLDR. You can even define custom money types tailored specifically to your application's needs! </p><p>I still need to battle test it against production projects, I have none at the moment. I am aiming next for performance benchmarking and more improvement, and parsing from string!</p><p>I am open for more feedback. Thank you! </p>","contentLength":639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TIL There is a minor-planet called Linux","url":"https://www.reddit.com/r/linux/comments/1iq5p1p/til_there_is_a_minorplanet_called_linux/","date":1739638670,"author":"/u/forvirringssirkel","guid":267,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Golang Mastery Exercises","url":"https://www.reddit.com/r/golang/comments/1iq5k7w/golang_mastery_exercises/","date":1739638307,"author":"/u/Temporary-Buy-7562","guid":289,"unread":true,"content":"<p>I made a repository which has a prompt for you to write many exercises, if you complete this, and then drill the exercises, I would be sure you would reach mastery with the core of the language.</p><p>I initially wanted to make some exercises for drilling syntax since I use copilot and lsps a lot, but ended up with quite a damn comprehensive list of things you would want to do with the language, and I find this more useful than working on leetcode to really adopt the language.</p>","contentLength":474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Have any LLM papers predicted a token in the middle rather than the next token?","url":"https://www.reddit.com/r/MachineLearning/comments/1iq4f0r/d_have_any_llm_papers_predicted_a_token_in_the/","date":1739635189,"author":"/u/TheWittyScreenName","guid":516,"unread":true,"content":"<p>I’m working on a project (unrelated to NLP) where we use essentially the same architecture and training as GPT-3, but we’re more interested in finding a series of tokens to connect a starting and ending “word” than the next “word”. Since we’re drawing a lot from LLMs in our setup, I’m wondering if there’s been any research into how models perform when the loss function isn’t based on the next token, but instead predicting a masked token somewhere in the input sequence. </p><p>Eventually we would like to expand this (maybe through fine tuning) to predict a longer series of missing tokens than just one but this seems like a good place to start. </p><p>I couldn’t find much about alternate unsupervised training schemes in the literature but it seems like someone must have tried this already. Any suggestions, or reasons that this is a bad idea?</p>","contentLength":859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alexandre Mutel a.k.a. xoofx is leaving Unity","url":"https://mastodon.social/@xoofx/113997304444307991","date":1739631212,"author":"/u/namanyayg","guid":275,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq2zwv/alexandre_mutel_aka_xoofx_is_leaving_unity/"},{"title":"Don't \"optimize\" conditional moves in shaders with mix()+step()","url":"https://iquilezles.org/articles/gpuconditionals/","date":1739631147,"author":"/u/namanyayg","guid":274,"unread":true,"content":"\nIn this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with conditionals when selecting between two results in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation, and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up a little:\nSo, say I have this code, which I actually published the other day:<div> snap45(  v )\n{\n     s = (v);\n     x = (v.x);\n     x&gt;?(s.x,):\n           x&gt;?s*():\n                      (,s.y);\n}</div>\nThe exact details of what it does don't matter for this discussion. All we care about is the two ternary operations deciding what's the final value this function should return. Indeed, depending on the value of the variable , the function will return one of three results, which are simple to compute. I could also have implemented this function with regular  statements, and all that I'm going to say in this article stays true.<p>\nNow, here's the problem - when seeing code like this, somebody somewhere will step up and invariably propose the following \"optimization\", which replaces what they believe (erroneously) are \"conditional branches\" in the code, by arithmetic operations. They will suggest something like this:</p><div> snap45(  v )\n{\n     s = (v);\n     x = (v.x);\n\n     w0 = (,x);\n     w1 = (,x)*(-w0);\n     w2 = -w0-w1;\n\n     res0 = (s.x,);\n     res1 = (s.x,s.y)*();\n     res2 = (,s.y);\n\n     w0*res0 + w1*res1 + w2*res2;\n}</div>\nThere are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. \"select\"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. I'll repeat it again - there is no branching, the instruction pointer isn't manipulated, there's no prediction involved, no pipe to flush, no instruction cache to invalidation, no nothing.<p>\nFor the record, of course GPUs can do real branching, and those are fine and fast and totally worth it when big chunks of code and computation are to be skipped given a condition. As with all things computing, always check the generated machine code to know what is happening exactly and when. But one thing you can safely assume without having to check any generated code - when moving simple values or computations like in my original example, you are guaranteed to not branch. This has been true for decades at this point, with GPUs. And while I'm not an expert in CPUs, I am pretty sure this is true for them as well.</p><p>\nThe second wrong thing with the supposedly optimized version is that it actually runs much slower than the original version. You can measure it in a variety of hardware. I can only assume that's because the </p> function is probably implemented with some sort of conditional move or subtract + bit propagation + AND.<div> step(  x,  y )\n{\n     x &lt; y ?  : ;\n}</div>\nEither way, using the step() \"optimization\" are either using the ternary operation anyways, which produces the  or  which they will use to mask in and out the different potential outputs with a series of arithmetic multiplications and additions. Which is wasteful, the values could have been conditionally moved directly, which is what the original shader code did.<p>\nBut don't take my word for it, let's look at the generated machine code for the original code I published:</p><div><div>\nGLSL<div> x&gt;?(s.x,):\n       x&gt;?s*():\n                  (,s.y);</div></div><div>\nAMD Compiler<div>     s0,      v3, , v1\n     v4, , v0\n     s1,   vcc, (v2), s0\n v3, 0, v3, vcc\n v0, v0, v4, vcc\n vcc, (v2), s1\n v1, v1, v3, vcc\n v0, 0, v0, vcc</div></div><div>\nMicrosoft Compiler<div>   r0.xy, l(, ), v0.xy\n   r0.zw, v0.xy, l(, )\n r0.xy, -r0.xyxx, r0.zwzz\n r0.xy, r0.xyxx\n  r1.xyzw, r0.xyxy, l4()\n   r2.xy, l(,), v0.xx  r0.z, l()\n r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy\n o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw</div></div></div>\nHere we can confirm that the GPU is not branching, as I explained. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).<p>\nThe Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison (</p> - \"lt\"=less than) and the masking or conditional move ( - mov=move, c=conditionally).<p>\nThere are no jump/branch instructions in these listings.</p><p>\nSomething not related to the discussion but interesting, is that some of the </p> GLSL calls I had in my shader before the ternary operator we are discussing, didn't become GPU instructions but rather instruction modifiers, which is the reason you see them in the listing. This means you can think of abs() calls as being free.\nSo, if you ever see somebody proposing this<div> a = ( b, c, ( y, x ) );</div>\nas an optimization to\nthen please correct them for me.","contentLength":5296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq2z4j/dont_optimize_conditional_moves_in_shaders_with/"},{"title":"Altman: OpenAI not for sale, especially to competitor who is not able to beat us","url":"https://www.axios.com/2025/02/11/openai-altman-musk-offer","date":1739629077,"author":"/u/namanyayg","guid":472,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq29zz/altman_openai_not_for_sale_especially_to/"},{"title":"GitHub - yaitoo/xun: Xun is an HTTP web framework built on Go's built-in html/template and net/http package’s router (1.22).","url":"https://github.com/yaitoo/xun","date":1739628954,"author":"/u/imlangzi","guid":290,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iq28it/github_yaitooxun_xun_is_an_http_web_framework/"},{"title":"What is Event Sourcing?","url":"https://newsletter.scalablethread.com/p/what-is-event-sourcing","date":1739628306,"author":"/u/scalablethread","guid":278,"unread":true,"content":"<p>Traditional data storage typically focuses on the current state of an entity. For example, in an e-commerce system, you might store the current state of a customer's order: items, quantities, shipping address, etc. Event sourcing takes a different approach. Instead of storing the current state directly, it stores the events that led to that state. Each event represents a fact that happened in the past. Think of it as a detailed log of transactions on your bank statement. These events are immutable and stored in an append-only event store. The core idea is that an application's state can be derived by replaying events in the order they occurred, just like you can get your current bank balance by replaying all the transactions from the beginning. This makes Event Sourcing particularly useful for applications that require a high degree of audibility and traceability.</p><p>Every change to the application state is captured as an event object in an Event Sourcing system. These events are then stored in an event store, a database optimized for handling event data. Here's a step-by-step breakdown of how Event Sourcing works:</p><ol></ol><p>Reconstructing the state from events involves reading all the events related to an entity from the event store and applying them in sequence to reconstruct the current state. It's like simulating all the changes that have occurred to construct the current state. For example, consider an e-commerce application where an order goes through various states like \"Created,\" \"Paid,\" and \"Shipped.\" To determine the current state of an order, you would:</p><ol><li><p>Retrieve all events related to the order from the event store.</p></li><li><p>Initialize an empty order object.</p></li><li><p>Apply each event to the order object in the order in which they were stored.</p></li></ol><p>By the end of this process, the order object will reflect the current state of the order.</p><p>As the number of events grows, replaying the entire event stream to reconstruct the state can become slow and inefficient. This is where snapshots come in. A snapshot is a saved state of an entity at a specific point in time. Instead of replaying all events from the beginning, the application can load the latest snapshot and then replay only the events that occurred after the snapshot was taken. </p><ul></ul><ul></ul><p><em>If you enjoyed this article, please hit the ❤️ like button.</em></p><p><em>If you think someone else will benefit from this, then please 🔁 share this post.</em></p>","contentLength":2380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq20v8/what_is_event_sourcing/"},{"title":"Career transition in to Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/","date":1739626865,"author":"/u/Similar-Secretary-86","guid":272,"unread":true,"content":"<p>\"I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated </p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Built a cli tool for generating .gitignore files","url":"https://www.reddit.com/r/golang/comments/1iq1ivv/built_a_cli_tool_for_generating_gitignore_files/","date":1739626739,"author":"/u/SoaringSignificant","guid":286,"unread":true,"content":"<p>I built this mostly as an excuse to play around with Charmbracelet’s libraries like Bubble Tea and make a nice TUI, but it also solves the annoying problem of constantly looking up .gitignore templates. It’s a simple CLI tool that lets you grab templates straight from GitHub, TopTal, or even your own custom repository, all from the terminal. You can search through templates using a TUI interface, combine multiple ones like mixing Go and CLion, and even save your own locally so you don’t have to redo them every time. If you’re always setting up new projects and find yourself dealing with .gitignore files over and over, this just makes life a bit easier, hopefully. If that sounds useful, check it out <a href=\"https://github.com/jasonuc/gignr\">here</a> and give it a try. And if you’ve got ideas to make the TUI better or want to add something cool, feel free to open a PR. Always happy to get feedback or contributions!</p>","contentLength":890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ED25519 Digital Signatures In Go","url":"https://www.reddit.com/r/golang/comments/1iq1i84/ed25519_digital_signatures_in_go/","date":1739626679,"author":"/u/mejaz-01","guid":482,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/mejaz-01\"> /u/mejaz-01 </a>","contentLength":31,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Richard Stallman on RISC-V and Free Hardware","url":"https://odysee.com/@SemiTO-V:2/richardstallmanriscv:7?r=BYVDNyJt5757WttAfFdvNmR9TvBSJHCv","date":1739625602,"author":"/u/ShockleyTransistor","guid":262,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iq16im/richard_stallman_on_riscv_and_free_hardware/"},{"title":"Chinese Vice Minister says China and the US must work together to control rogue AI: \"If not... I am afraid that the probability of the machine winning will be high.\"","url":"https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says","date":1739622429,"author":"/u/MetaKnowing","guid":293,"unread":true,"content":"<div datatype=\"p\" data-qa=\"Component-Component\">A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in <a target=\"_self\" href=\"https://www.scmp.com/topics/artificial-intelligence?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> (AI).</div><p datatype=\"p\" data-qa=\"Component-Component\">But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.</p><p datatype=\"p\" data-qa=\"Component-Component\">“Realistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,” Fu said.</p><p datatype=\"p\" data-qa=\"Component-Component\">“As long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.”</p><div datatype=\"p\" data-qa=\"Component-Component\">The panel discussion is part of a two-day global <a target=\"_self\" href=\"https://www.scmp.com/news/world/europe/article/3297992/trumps-ai-ambition-and-chinas-deepseek-overshadow-major-ai-summit-paris?module=Europe&amp;pgtype=section?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> that started in Paris on Monday.</div><p datatype=\"p\" data-qa=\"Component-Component\">Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden’s administration and the United Nations.</p>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq0b4t/chinese_vice_minister_says_china_and_the_us_must/"},{"title":"Karol Herbst steps down as Nouveau maintainer due to “thin blue line comment”","url":"https://www.reddit.com/r/linux/comments/1iq09g6/karol_herbst_steps_down_as_nouveau_maintainer_due/","date":1739622250,"author":"/u/mdedetrich","guid":266,"unread":true,"content":"<p>\"I was pondering with myself for a while if I should just make it official that I'm not really involved in the kernel community anymore, neither as a reviewer, nor as a maintainer.</p><p>Most of the time I simply excused myself with \"if something urgent comes up, I can chime in and help out\". Lyude and Danilo are doing a wonderful job and I've put all my trust into them.</p><p>However, there is one thing I can't stand and it's hurting me the most. I'm convinced, no, my core believe is, that inclusivity and respect, working with others as equals, no power plays involved, is how we should work together within the Free and Open Source community.</p><p>I can understand maintainers needing to learn, being concerned on technical points. Everybody deserves the time to understand and learn. It is my true belief that most people are capable of change eventually. I truly believe this community can change from within, however this doesn't mean it's going to be a smooth process.</p><p>The moment I made up my mind about this was reading the following words written by a maintainer within the kernel community:</p><p>\"we are the thin blue line\"</p><p>This isn't okay. This isn't creating an inclusive environment. This isn't okay with the current political situation especially in the US. A maintainer speaking those words can't be kept. No matter how important or critical or relevant they are. They need to be removed until they learn. Learn what those words mean for a lot of marginalized people. Learn about what horrors it evokes in their minds.</p><p>I can't in good faith remain to be part of a project and its community where those words are tolerated. Those words are not technical, they are a political statement. Even if unintentionally, such words carry power, they carry meanings one needs to be aware of. They do cause an immense amount of harm.</p><p>I wish the best of luck for everybody to continue to try to work from within. You got my full support and I won't hold it against anybody trying to improve the community, it's a thankless job, it's a lot of work. People will continue to burn out.</p><p>I got burned out enough by myself caring about the bits I maintained, but eventually I had to realize my limits. The obligation I felt was eating me from inside. It stopped being fun at some point and I reached a point where I simply couldn't continue the work I was so motivated doing as I've did in the early days.</p><p>Please respect my wishes and put this statement as is into the tree. Leaving anything out destroys its entire meaning.</p>","contentLength":2492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building the MagicMirror in Rust with iced GUI Library 🦀","url":"https://www.reddit.com/r/rust/comments/1ipzubj/building_the_magicmirror_in_rust_with_iced_gui/","date":1739620595,"author":"/u/amindiro","guid":283,"unread":true,"content":"<p>I recently embarked on a journey to build a custom MagicMirror using the Rust programming language, and I’d like to share my experiences. I wrost a blog post titled <a href=\"https://aminediro.com/posts/mirrors/#mirrors\">\"software you can love: miroir Ô mon beau miroir\"</a> this project was my attempt to create a stable, resource-efficient application for the Raspberry Pi 3A.</p><p>Here's what I loved about using Rust and the iced GUI library:</p><ul><li><p><strong>Elm Architecture + Rust is a match made in heaven:</strong> iced was perfect for my needs with its Model, View, and Update paradigms. It helped keep my state management concise and leverage Rust type system</p></li><li><p> Opting for this lightweight rendering library reduced the size of the binary significantly, ending with a 9MB binary.</p></li><li><p> Although troublesome at first, I used ‘cross’ to cross compile Rust for armv7.</p></li></ul><p>If anyone is keen, I’m thinking of open-sourcing this project and sharing it with the community. Insights on enhancing the project's functionality or any feedback would be much appreciated!</p><p>Feel free to reach out if you're interested in the technical nitty-gritty or my experience with Rust GUI libraries in general.</p>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Daily ArXiv filtering powered by LLM judge","url":"https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/","date":1739618056,"author":"/u/MadEyeXZ","guid":258,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My new blog post comparing networking in EKS vs. GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/","date":1739617569,"author":"/u/jumiker","guid":271,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Richard Stallman in Polytechnic University of Turin, Italy","url":"https://www.reddit.com/r/linux/comments/1ipz4wy/richard_stallman_in_polytechnic_university_of/","date":1739617538,"author":"/u/ShockleyTransistor","guid":265,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go Nullable with Generics v2.0.0 - now supports omitzero","url":"https://github.com/LukaGiorgadze/gonull","date":1739617221,"author":"/u/Money-Relative-1184","guid":487,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ipz22f/go_nullable_with_generics_v200_now_supports/"},{"title":"async-arp: library for probing hosts and sending advanced ARP (Address Resolution Protocol) requests.","url":"https://www.reddit.com/r/rust/comments/1ipywbp/asyncarp_library_for_probing_hosts_and_sending/","date":1739616505,"author":"/u/arcycar","guid":280,"unread":true,"content":"<p>After a few months of exploring and working with Rust, I am happy to share my first small Rust crate, <a href=\"https://crates.io/crates/async-arp\"></a> and I’d love to hear your thoughts! 🚀</p><p>This library provides an  way to send and receive , making it useful for network discovery, debugging, and custom networking applications.</p><ul><li>🏎  Built on Tokio for non-blocking network operations</li><li>🔍  Easily detect active devices in a subnet</li><li>⚙️  Craft and send ARP packets dynamically</li></ul><p>You can find usage examples and API documentation here: 📖 <a href=\"https://docs.rs/async-arp/latest/async_arp/\"></a></p><p>Since this is my first crate, I’d really appreciate any feedback on:</p><ul><li>📌  – Is the interface intuitive and ergonomic?</li><li>🚀  – Does it fit well into async Rust workflows?</li><li>🔍  – Any improvements or best practices I may have missed?</li><li>🦀  – Suggestions to make it more \"Rustacean\"?</li></ul><p>If you have further ideas, issues, or want to contribute, check it out on GitHub:</p><p>Thanks for checking it out—let me know what you think! 🦀</p>","contentLength":921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Dive into VPA Recommender","url":"https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/","date":1739615164,"author":"/u/erik_zilinsky","guid":273,"unread":true,"content":"<p>I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.</p><p>Based on my findings, I wrote a <a href=\"https://erikzilinsky.com/posts/vpa1.html\">blog post</a> about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.</p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what do you use golang for?","url":"https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/","date":1739615068,"author":"/u/Notalabel_4566","guid":291,"unread":true,"content":"<p>Is there any other major use than web development?</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux in any distribution is unobtainable for most people because the first two installation steps are basically impossible.","url":"https://www.reddit.com/r/linux/comments/1ipyc1o/linux_in_any_distribution_is_unobtainable_for/","date":1739613947,"author":"/u/trollfinnes","guid":264,"unread":true,"content":"<p>Recently, just before Christmas, I decided to check out Linux again (tried it ~20 years ago) because Windows 11 was about to cause an aneurysm.</p><p>I was expecting to spend the \"weekend\" getting everything to work; find hardware drivers, installing various open source software and generally just 'hack together something that works'.</p><p>To my surprise everything worked flawlessly first time booting up. I had WiFi, sound, usb, webcam, memory card reader, correct screen resolution. I even got battery status and management! It even came with a nice litte 'app center' making installation of a bunch of software as simple as a click!</p><p>And I remember thinking any Windows user could  install Linux and would get comfortable using it in an afternoon.</p><p>I'm pretty 'comfortable' in anything PC and have changed boot orders and created bootable things since the early 90's and considered that part of the installation the easiest part.</p><p>However, most people have never heard about any of them, and that makes the two steps seem 'impossible'.</p><p>I recently convinced a friend of mine, who also couldn't stand Window11, to install Linux instead as it would easily cover all his PC needs. </p><p>And while he is definitely in the upper half of people in terms of 'tech savvyness', both those \"two easy first steps\" made it virtually impossible for him to install it. </p><p>He easily managed downloading the .iso, but turning that iso into a bootable USB-stick turned out to be too difficult. But after guiding him over the phone he was able to create it.</p><p>But he wasn't able to get into bios despite all my attempts explaining what button to push and when</p><p>Next day he came over with his laptop. And just out of reflex I just started smashing the F2 key (or whatever it was) repeatingly and got right into bios where I enabled USB boot and put it at the top at the sequence.</p><p>After that he managed to install Linux just fine without my supervision.</p><p>But it made me realise that the two first steps in installing Linux, that are second nature to me and probably everyone involved with Linux from people just using it to people working on huge distributions, makes them virtually impossible for most people to install it.</p><p>I don't know enough about programming to know of this is possible:</p><p>Instead of an .iso file for download some sort of .exe file can be downloaded that is able to create a bootable USB-stick and change the boot order?</p><p>That would 'open up' Linux to  more people, probably orders of magnitude..</p>","contentLength":2460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lessons from David Lynch: A Software Developer's Perspective","url":"https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/","date":1739612430,"author":"/u/aijan1","guid":277,"unread":true,"content":"<p>David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He’s perhaps best known for the groundbreaking TV series <a href=\"https://en.wikipedia.org/wiki/Twin_Peaks\">Twin Peaks</a>, which inspired countless shows, including The X-Files, The Sopranos, and Lost.</p><p>Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down – even those who truly deserved it.</p><p>Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that <a href=\"https://en.wikipedia.org/wiki/Mulholland_Drive_(film)\">Mulholland Drive</a> remained compulsively watchable while refusing to yield to interpretation.</p><p>While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I’d like to share my perspective on his life lessons from a software developer’s viewpoint.</p><blockquote><p>Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.</p></blockquote><p>We’ve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one –because they’re so rare– write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it’s a film, a painting, or software.</p><blockquote><p>The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.</p></blockquote><p>Software development is part art, part engineering. We don’t build the same software over and over again – virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it’s very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.</p><p>It’s a good habit to listen to what users have to say, but they often can only describe their problems – they rarely come up with good ideas to solve them. And that’s OK. It’s our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.</p><blockquote><p>My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.</p></blockquote><p>Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  – that magical state of mind where we lose track of time and produce code effortlessly. That’s why many developers hate meetings – they are toxic to our productivity.</p><blockquote><p>I believe you need technical knowledge. And also, it’s really, really great to learn by doing. So, you should make a film.</p></blockquote><p>Software development is one of those rare fields where a college degree isn’t required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.</p><p>The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It’s crucial to never stop learning, experimenting, and iterating on our craft.</p><blockquote><p>Happy accidents are real gifts, and they can open the door to a future that didn’t even exist.</p></blockquote><p>Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.</p><p>Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.</p><blockquote><p>I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.</p></blockquote><p>Be kind to your teammates, don’t embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety –that is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by <a href=\"https://rework.withgoogle.com/en/guides/understanding-team-effectiveness\">Google’s research</a> on the subject.</p><p>It’s OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.</p><blockquote><p>Most of Hollywood is about making money - and I love money, but I don’t make the films thinking about money.</p></blockquote><p>Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.</p><p>What makes these projects remarkable is that they didn’t emerge from corporate boardrooms – they were built by communities of passionate developers, collaborating across the world.</p><p>Money is just a means to an end. Unfortunately, many get this confused.</p><p>David, thank you for making the world a better place!</p>","contentLength":5845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipy01t/lessons_from_david_lynch_a_software_developers/"},{"title":"\"Dongly Things\" by Douglas Adams (of Hitchhikers Guide) - Adams wrote this article in the early days of Mac computers, about manufacturers making things difficult with a million different proprietary cables/ports etc.","url":"https://www.douglasadams.com/dna/980707-03-a.html","date":1739605656,"author":"/u/CaesarSalvage","guid":486,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ipwjn0/dongly_things_by_douglas_adams_of_hitchhikers/"},{"title":"Container Networking - Kubernetes with Calico","url":"https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/","date":1739604355,"author":"/u/tkr_2020","guid":268,"unread":true,"content":"<ul><li>: VLAN 10</li><li>: VLAN 20</li></ul><p>When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:</p><p>The inner IP header reflects:</p><p>The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.</p><p>Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?</p>","contentLength":502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] What's the most promising successor to the Transformer?","url":"https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/","date":1739600221,"author":"/u/jsonathan","guid":259,"unread":true,"content":"<p>All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also <a href=\"https://arxiv.org/pdf/2405.04517\">xLSTM</a> and <a href=\"https://arxiv.org/pdf/2405.13956\">Aaren</a>.</p><p>What do y'all think is the most promising alternative architecture to the transformer?</p>","contentLength":283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kafka Delay Queue: When Messages Need a Nap Before They Work","url":"https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need","date":1739596108,"author":"/u/Sushant098123","guid":276,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipu9n3/kafka_delay_queue_when_messages_need_a_nap_before/"},{"title":"Webassembly and go 2025","url":"https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/","date":1739595637,"author":"/u/KosekiBoto","guid":287,"unread":true,"content":"<div><p>so I found <a href=\"https://www.youtube.com/watch?v=HShIpUgCPp4\">this video </a>and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you</p></div>   submitted by   <a href=\"https://www.reddit.com/user/KosekiBoto\"> /u/KosekiBoto </a>","contentLength":402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bringing Nest.js to Rust: Meet Toni.rs, the Framework You’ve Been Waiting For! 🚀","url":"https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/","date":1739587338,"author":"/u/Mysterious-Rust","guid":284,"unread":true,"content":"<p>As a Rust developer coming from TypeScript, I’ve been missing a Nest.js-like framework — its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn’t have a direct counterpart (yet!), I decided to build one myself! 🛠️</p><p>Introducing… <a href=\"https://crates.io/crates/toni\">Toni.rs</a> — a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it’s live in beta! 🎉</p><p>Here’s what makes this project interesting:</p><p>Scalable maintainability 🧩:</p><p>A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code — each module lives in its own context, clean and focused.</p><p>Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?</p><p>Automatic Dependency Injection 🤖:</p><p>Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.</p><p>Leave your thoughts below — suggestions, questions, or even just enthusiasm! 🚀 </p>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what was the Linux expirance like in the 90's and 00's?","url":"https://www.reddit.com/r/linux/comments/1ipql9k/what_was_the_linux_expirance_like_in_the_90s_and/","date":1739583310,"author":"/u/mrcrabs6464","guid":263,"unread":true,"content":"<div><p>I started using Linux about 2 years ago really right at the beginning of the proton revolution. And I know that Gaming in specif was the biggest walls for mass adaption of Linux throughout the 2010's and late 2000's but Ive heard things about how most software ran through WINE until Direct x and other API's became more common. but gaming aside what was the expirance and community like at the time?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mrcrabs6464\"> /u/mrcrabs6464 </a>","contentLength":434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tabiew 0.8.4 Released","url":"https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/","date":1739578902,"author":"/u/shshemi","guid":285,"unread":true,"content":"<p>Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...</p><ul><li>📊 Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and Sqlite</li><li>🗂️ Multi-table functionality</li></ul><ul><li>UI is updated to be more modern and responsive</li><li>Horizontally scrollable tables</li><li>Visible data frame can be referenced with name \"_\"</li><li>Compatibility with older versions of glibc</li><li>Two new themes (Tokyo Night and Catppuccin)</li></ul>","contentLength":450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.","url":"https://v.redd.it/sglstazd96je1","date":1739568243,"author":"/u/eternviking","guid":294,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iple9t/an_art_exhibit_in_japan_where_a_chained_robot_dog/"}],"tags":["reddit"]}