{"id":"5k2tWE6P7gju4SyDmCHtk8gaF8HxhNjkJsE36uK8","title":"cs updates on arXiv.org","displayTitle":"Arxiv","url":"https://rss.arxiv.org/atom/cs","feedLink":"https://rss.arxiv.org/atom/cs","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":711,"items":[{"title":"Multifidelity Simulation-based Inference for Computationally Expensive Simulators","url":"https://arxiv.org/abs/2502.08416","date":1739768400,"author":"","guid":895,"unread":true,"content":"<article>arXiv:2502.08416v2 Announce Type: replace-cross \nAbstract: Across many domains of science, stochastic models are an essential tool to understand the mechanisms underlying empirically observed data. Models can be of different levels of detail and accuracy, with models of high-fidelity (i.e., high accuracy) to the phenomena under study being often preferable. However, inferring parameters of high-fidelity models via simulation-based inference is challenging, especially when the simulator is computationally expensive. We introduce MF-NPE, a multifidelity approach to neural posterior estimation that leverages inexpensive low-fidelity simulations to infer parameters of high-fidelity simulators within a limited simulation budget. MF-NPE performs neural posterior estimation with limited high-fidelity resources by virtue of transfer learning, with the ability to prioritize individual observations using active learning. On one statistical task with analytical ground-truth and two real-world tasks, MF-NPE shows comparable performance to current approaches while requiring up to two orders of magnitude fewer high-fidelity simulations. Overall, MF-NPE opens new opportunities to perform efficient Bayesian inference on computationally expensive simulators.</article>","contentLength":1261,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Devil is in the Prompts: De-Identification Traces Enhance Memorization Risks in Synthetic Chest X-Ray Generation","url":"https://arxiv.org/abs/2502.07516","date":1739768400,"author":"","guid":896,"unread":true,"content":"<article>arXiv:2502.07516v2 Announce Type: replace-cross \nAbstract: Generative models, particularly text-to-image (T2I) diffusion models, play a crucial role in medical image analysis. However, these models are prone to training data memorization, posing significant risks to patient privacy. Synthetic chest X-ray generation is one of the most common applications in medical image analysis with the MIMIC-CXR dataset serving as the primary data repository for this task. This study presents the first systematic attempt to identify prompts and text tokens in MIMIC-CXR that contribute the most to training data memorization. Our analysis reveals two unexpected findings: (1) prompts containing traces of de-identification procedures (markers introduced to hide Protected Health Information) are the most memorized, and (2) among all tokens, de-identification markers contribute the most towards memorization. This highlights a broader issue with the standard anonymization practices and T2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time memorization mitigation strategies are ineffective and fail to sufficiently reduce the model's reliance on memorized text tokens. On this front, we propose actionable strategies for different stakeholders to enhance privacy and improve the reliability of generative models in medical imaging. Finally, our results provide a foundation for future work on developing and benchmarking memorization mitigation techniques for synthetic chest X-ray generation using the MIMIC-CXR dataset. The anonymized code is available at https://anonymous.4open.science/r/diffusion_memorization-8011/</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Supervised contrastive learning for cell stage classification of animal embryos","url":"https://arxiv.org/abs/2502.07360","date":1739768400,"author":"","guid":897,"unread":true,"content":"<article>arXiv:2502.07360v2 Announce Type: replace-cross \nAbstract: Video microscopy, when combined with machine learning, offers a promising approach for studying the early development of in vitro produced (IVP) embryos. However, manually annotating developmental events, and more specifically cell divisions, is time-consuming for a biologist and cannot scale up for practical applications. We aim to automatically classify the cell stages of embryos from 2D time-lapse microscopy videos with a deep learning approach. We focus on the analysis of bovine embryonic development using video microscopy, as we are primarily interested in the application of cattle breeding, and we have created a Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1) low-quality images and bovine dark cells that make the identification of cell stages difficult, (2) class ambiguity at the boundaries of developmental stages, and (3) imbalanced data distribution. To address these challenges, we introduce CLEmbryo, a novel method that leverages supervised contrastive learning combined with focal loss for training, and the lightweight 3D neural network CSN-50 as an encoder. We also show that our method generalizes well. CLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset and the publicly available NYU Mouse Embryos dataset.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Effcient classical error correction for parity encoded spin systems","url":"https://arxiv.org/abs/2502.07170","date":1739768400,"author":"","guid":898,"unread":true,"content":"<article>arXiv:2502.07170v2 Announce Type: replace-cross \nAbstract: Fast solvers for combinatorial optimization problems (COPs) have attracted engineering interest in various industrial and social applications. Quantum annealing (QA) has emerged as a promising candidate and significant efforts have been dedicated to its development. Since COP is encoded in the Ising interaction between logical spins, its realization requires a spin system with all-to-all connectivity, which poses technical difficulties in the physical implementation of large-scale QA devices. W. Lechner, P. Hauke, and P. Zoller proposed parity-encoding (PE) architecture, consisting of a larger system of physical spins with only local connectivities between them, to avoid this diffculty in the near future QA device development. They suggested that this architecture not only reduces implementation diffculties and improves scalability, but also has intrinsic fault tolerance because logical spins are redundantly and nonlocally encoded into the physical spins. Nevertheless, it remains unclear how these advantageous features can be exploited. This paper addresses how to correct errors in a spin readout of PE architecture. Our work is based on the close connection between PE architecture and classical low-density parity-check (LDPC) codes. We have shown that independent and identically distributed errors in a spin readout can be corrected by a very simple decoding algorithm that can be regarded as a bit flipping (BF) algorithm for the LDPC codes. The BF algorithm was shown to have comparable performance to the belief propagation (BP) decoding algorithm. Furthermore, it is suggested that the introduction of post-readout BF decoding reduces the total computational cost and improves the performance of the global optimal solution search using the PE architecture. We believe that our results indicate that the PE architecture is a promising platform for near-term QA devices.</article>","contentLength":1953,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pre-Equalization Aided Grant-Free Massive Access in Massive MIMO System","url":"https://arxiv.org/abs/2502.06239","date":1739768400,"author":"","guid":899,"unread":true,"content":"<article>arXiv:2502.06239v2 Announce Type: replace-cross \nAbstract: The spatial diversity and multiplexing advantages of massive multi-input-multi-output (mMIMO) can significantly improve the capacity of massive non-orthogonal multiple access (NOMA) in machine type communications. However, state-of-the-art grant-free massive NOMA schemes for mMIMO systems require accurate estimation of random access channels to perform activity detection and the following coherent data demodulation, which suffers from excessive pilot overhead and access latency. To address this, we propose a pre-equalization aided grant-free massive access scheme for mMIMO systems, where an iterative detection scheme is conceived. Specifically, the base station (BS) firstly activates one of its antennas (i.e., beacon antenna) to broadcast a beacon signal, which facilitates the user equipment (UEs) to perform downlink channel estimation and pre-equalize the uplink random access signal with respect to the channels associated with the beacon antenna. During the uplink transmission stage, the BS detects UEs' activity and data by using the proposed iterative detection algorithm, which consists of three modules: coarse data detection (DD), data-aided channel estimation (CE), and fine DD. In the proposed algorithm, the joint activity and DD is firstly performed based on the signals received by the beacon antenna. Subsequently, the DD is further refined by iteratively performing data-aided CE module and fine DD module using signals received by all BS antennas. Our simulation results demonstrate that the proposed scheme outperforms state-of-the-art mMIMO-based grant-free massive NOMA schemes with the same access latency. Simulation codes are provided to reproduce the results in this article: https://github.com/owenwang517/tvt-2025.</article>","contentLength":1811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Less is More for Synthetic Speech Detection in the Wild","url":"https://arxiv.org/abs/2502.05674","date":1739768400,"author":"","guid":900,"unread":true,"content":"<article>arXiv:2502.05674v3 Announce Type: replace-cross \nAbstract: Driven by advances in self-supervised learning for speech, state-of-the-art synthetic speech detectors have achieved low error rates on popular benchmarks such as ASVspoof. However, prior benchmarks do not address the wide range of real-world variability in speech. Are reported error rates realistic in real-world conditions? To assess detector failure modes and robustness under controlled distribution shifts, we introduce ShiftySpeech, a benchmark with more than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12 vocoders, and 3 languages. We found that all distribution shifts degraded model performance, and contrary to prior findings, training on more vocoders, speakers, or with data augmentation did not guarantee better generalization. In fact, we found that training on less diverse data resulted in better generalization, and that a detector fit using samples from a single carefully selected vocoder and a small number of speakers, without data augmentations, achieved state-of-the-art results on the challenging In-the-Wild benchmark.</article>","contentLength":1122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Regularized Newton Method for Nonconvex Optimization with Global and Local Complexity Guarantees","url":"https://arxiv.org/abs/2502.04799","date":1739768400,"author":"","guid":901,"unread":true,"content":"<article>arXiv:2502.04799v2 Announce Type: replace-cross \nAbstract: We consider the problem of finding an $\\epsilon$-stationary point of a nonconvex function with a Lipschitz continuous Hessian and propose a quadratic regularized Newton method incorporating a new class of regularizers constructed from the current and previous gradients. The method leverages a recently developed linear conjugate gradient approach with a negative curvature monitor to solve the regularized Newton equation. Notably, our algorithm is adaptive, requiring no prior knowledge of the Lipschitz constant of the Hessian, and achieves a global complexity of $O(\\epsilon^{-\\frac{3}{2}}) + \\tilde O(1)$ in terms of the second-order oracle calls, and $\\tilde O(\\epsilon^{-\\frac{7}{4}})$ for Hessian-vector products, respectively. Moreover, when the iterates converge to a point where the Hessian is positive definite, the method exhibits quadratic local convergence. Preliminary numerical results illustrate the competitiveness of our algorithm.</article>","contentLength":1010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedMimic: Physician-Inspired Multimodal Fusion for Early Diagnosis of Fever of Unknown Origin","url":"https://arxiv.org/abs/2502.04794","date":1739768400,"author":"","guid":902,"unread":true,"content":"<article>arXiv:2502.04794v2 Announce Type: replace-cross \nAbstract: Fever of unknown origin FUO remains a diagnostic challenge. MedMimic is introduced as a multimodal framework inspired by real-world diagnostic processes. It uses pretrained models such as DINOv2, Vision Transformer, and ResNet-18 to convert high-dimensional 18F-FDG PET/CT imaging into low-dimensional, semantically meaningful features. A learnable self-attention-based fusion network then integrates these imaging features with clinical data for classification. Using 416 FUO patient cases from Sichuan University West China Hospital from 2017 to 2023, the multimodal fusion classification network MFCN achieved macro-AUROC scores ranging from 0.8654 to 0.9291 across seven tasks, outperforming conventional machine learning and single-modality deep learning methods. Ablation studies and five-fold cross-validation further validated its effectiveness. By combining the strengths of pretrained large models and deep learning, MedMimic offers a promising solution for disease classification.</article>","contentLength":1050,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation","url":"https://arxiv.org/abs/2502.03930","date":1739768400,"author":"","guid":903,"unread":true,"content":"<article>arXiv:2502.03930v2 Announce Type: replace-cross \nAbstract: Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.</article>","contentLength":1246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gaussian Processes Regression for Uncertainty Quantification: An Introductory Tutorial","url":"https://arxiv.org/abs/2502.03090","date":1739768400,"author":"","guid":904,"unread":true,"content":"<article>arXiv:2502.03090v2 Announce Type: replace-cross \nAbstract: Gaussian Process Regression (GPR) is a powerful nonparametric regression method that is widely used in Uncertainty Quantification (UQ) for constructing surrogate models. This tutorial serves as an introductory guide for beginners, aiming to offer a structured and accessible overview of GPR's applications in UQ. We begin with an introduction to UQ and outline its key tasks, including uncertainty propagation, risk estimation, optimization under uncertainty, parameter estimation, and sensitivity analysis. We then introduce Gaussian Processes (GPs) as a surrogate modeling technique, detailing their formulation, choice of covariance kernels, hyperparameter estimation, and active learning strategies for efficient data acquisition. The tutorial further explores how GPR can be applied to different UQ tasks, including Bayesian quadrature for uncertainty propagation, active learning-based risk estimation, Bayesian optimization for optimization under uncertainty, and surrogate-based sensitivity analysis. Throughout, we emphasize how to leverage the unique formulation of GP for these UQ tasks, rather than simply using it as a standard surrogate model.</article>","contentLength":1216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Quality Control Of MRI Images Using Synthetic Motion Data","url":"https://arxiv.org/abs/2502.00160","date":1739768400,"author":"","guid":905,"unread":true,"content":"<article>arXiv:2502.00160v2 Announce Type: replace-cross \nAbstract: MRI quality control (QC) is challenging due to unbalanced and limited datasets, as well as subjective scoring, which hinder the development of reliable automated QC systems. To address these issues, we introduce an approach that pretrains a model on synthetically generated motion artifacts before applying transfer learning for QC classification. This method not only improves the accuracy in identifying poor-quality scans but also reduces training time and resource requirements compared to training from scratch. By leveraging synthetic data, we provide a more robust and resource-efficient solution for QC automation in MRI, paving the way for broader adoption in diverse research settings.</article>","contentLength":754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A topological theory for qLDPC: non-Clifford gates and magic state fountain on homological product codes with constant rate and beyond the $N^{1/3}$ distance barrier","url":"https://arxiv.org/abs/2501.19375","date":1739768400,"author":"","guid":906,"unread":true,"content":"<article>arXiv:2501.19375v2 Announce Type: replace-cross \nAbstract: We develop a unified theory for fault-tolerant quantum computation in quantum low-density parity-check (qLDPC) and topological codes. We show that there exist hidden simplicial complex structures encoding the topological data for all qLDPC and CSS codes obtained from product construction by generalizing the Freedman-Hastings code-to-manifold mapping. This is achieved by building manifolds corresponding to high-dimensional topological expanders from the Tanner graphs of the skeleton classical or quantum codes, which further form a product manifold and an associated thickened product code defined on its triangulation with only a constant qubit overhead. This suggests that qLDPC or more generally CSS codes obtained from product constructions are topological, and hence can admit cohomology operations such as cup products, physically corresponding to higher symmetries in the underlying topological quantum field theory. When applying this mapping to a 3D hypergraph product code obtained from the product of 3 copies of good classical expander codes, we obtain the first non-Clifford logical CCZ gates via constant depth circuits on a code with constant stabilizer weight $w=O(1)$, constant rate $K=\\Theta(N)$, and polynomial distance $D=\\Omega(N^{1/3})$. When applied to 3D homological product codes consisting of the product of a pair of good quantum and classical LDPC codes, we can further improve the distance to $D=\\Omega(\\sqrt{N})$ exceeding the $N^{1/3}$ distance barrier implied by the Bravyi-K\\\"onig bound for conventional topological codes. Our work suggests that it is feasible to apply native logical non-Clifford gates on qLDPC codes or directly inject high-fidelity magic states as resources (`magic state fountain') without the distillation process. For the homological product construction, the fountain can inject $\\Theta(\\sqrt{N})$ magic states in parallel in a single round.</article>","contentLength":1961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collaborative Channel Access and Transmission for NR Sidelink and Wi-Fi Coexistence over Unlicensed Spectrum","url":"https://arxiv.org/abs/2501.17878","date":1739768400,"author":"","guid":907,"unread":true,"content":"<article>arXiv:2501.17878v2 Announce Type: replace-cross \nAbstract: With the rapid development of various internet of things (IoT) applications, including industrial IoT (IIoT) and visual IoT (VIoT), the demand for direct device-to-device communication to support high data rates continues to grow. To address this demand, 5G-Advanced has introduced sidelink communication over the unlicensed spectrum (SL-U) to increase data rates. However, the primary challenge of SL-U in the unlicensed spectrum is ensuring fair coexistence with other incumbent systems, such as Wi-Fi. In this paper, we address the challenge by designing channel access mechanisms and power control strategies to mitigate interference and ensure fair coexistence. First, we propose a novel collaborative channel access (CCHA) mechanism that integrates channel access with resource allocation through collaborative interactions between base stations (BS) and SL-U users. This mechanism ensures fair coexistence with incumbent systems while improving resource utilization. Second, to further enhance the performance of the coexistence system, we develop a cooperative subgoal-based hierarchical deep reinforcement learning (C-GHDRL) algorithm framework. The framework enables SL-U users to make globally optimal decisions by leveraging cooperative operations between the BS and SL-U users, effectively overcoming the limitations of traditional optimization methods in solving joint optimization problems with nonlinear constraints. Finally, we mathematically model the joint channel access and power control problem and balance the trade-off between fairness and transmission rate in the coexistence system by defining a suitable reward function in the C-GHDRL algorithm. Simulation results demonstrate that the proposed scheme significantly enhances the performance of the coexistence system while ensuring fair coexistence between SL-U and Wi-Fi users.</article>","contentLength":1914,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artifact-free Sound Quality in DNN-based Closed-loop Systems for Audio Processing","url":"https://arxiv.org/abs/2501.04116","date":1739768400,"author":"","guid":908,"unread":true,"content":"<article>arXiv:2501.04116v2 Announce Type: replace-cross \nAbstract: Recent advances in deep neural networks (DNNs) have significantly improved various audio processing applications, including speech enhancement, synthesis, and hearing aid algorithms. DNN-based closed-loop systems have gained popularity in these applications due to their robust performance and ability to adapt to diverse conditions. Despite their effectiveness, current DNN-based closed-loop systems often suffer from sound quality degradation caused by artifacts introduced by suboptimal sampling methods. To address this challenge, we introduce dCoNNear, a novel DNN architecture designed for seamless integration into closed-loop frameworks. This architecture specifically aims to prevent the generation of spurious artifacts. We demonstrate the effectiveness of dCoNNear through a proof-of-principle example within a closed-loop framework that employs biophysically realistic models of auditory processing for both normal and hearing-impaired profiles to design personalized hearing aid algorithms. Our results show that dCoNNear not only accurately simulates all processing stages of existing non-DNN biophysical models but also eliminates audible artifacts, thereby enhancing the sound quality of the resulting hearing aid algorithms. This study presents a novel, artifact-free closed-loop framework that improves the sound quality of audio processing systems, offering a promising solution for high-fidelity applications in audio and hearing technologies.</article>","contentLength":1522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching","url":"https://arxiv.org/abs/2501.00135","date":1739768400,"author":"","guid":909,"unread":true,"content":"<article>arXiv:2501.00135v4 Announce Type: replace-cross \nAbstract: Quantum computing is an exciting non-Von Neumann paradigm, offering provable speedups over classical computing for specific problems. However, the practical limits of classical simulatability for quantum circuits remain unclear, especially with current noisy quantum devices. In this work, we explore the potential of leveraging Large Language Models (LLMs) to simulate the output of a quantum Turing machine using Grover's quantum circuits, known to provide quadratic speedups over classical counterparts. To this end, we developed GroverGPT, a specialized model based on LLaMA's 8-billion-parameter architecture, trained on over 15 trillion tokens. Unlike brute-force state-vector simulations, which demand substantial computational resources, GroverGPT employs pattern recognition to approximate quantum search algorithms without explicitly representing quantum states. Analyzing 97K quantum search instances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\\% accuracy), achieving nearly 100\\% accuracy on 6- and 10-qubit datasets when trained on 4-qubit or larger datasets. It also demonstrated strong generalization, surpassing 95\\% accuracy for systems with over 20 qubits when trained on 3- to 6-qubit data. Analysis indicates GroverGPT captures quantum features of Grover's search rather than classical patterns, supported by novel prompting strategies to enhance performance. Although accuracy declines with increasing system size, these findings offer insights into the practical boundaries of classical simulatability. This work suggests task-specific LLMs can surpass general-purpose models like GPT-4o in quantum algorithm learning and serve as powerful tools for advancing quantum research.</article>","contentLength":1771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Consolidated Volatility Prediction with Back Propagation Neural Network and Genetic Algorithm","url":"https://arxiv.org/abs/2412.07223","date":1739768400,"author":"","guid":910,"unread":true,"content":"<article>arXiv:2412.07223v4 Announce Type: replace-cross \nAbstract: This paper provides a unique approach with AI algorithms to predict emerging stock markets volatility. Traditionally, stock volatility is derived from historical volatility,Monte Carlo simulation and implied volatility as well. In this paper, the writer designs a consolidated model with back-propagation neural network and genetic algorithm to predict future volatility of emerging stock markets and found that the results are quite accurate with low errors.</article>","contentLength":518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption","url":"https://arxiv.org/abs/2412.01858","date":1739768400,"author":"","guid":911,"unread":true,"content":"<article>arXiv:2412.01858v4 Announce Type: replace-cross \nAbstract: The integration of fully homomorphic encryption (FHE) in federated learning (FL) has led to significant advances in data privacy. However, during the aggregation phase, it often results in performance degradation of the aggregated model, hindering the development of robust representational generalization. In this work, we propose a novel multimodal quantum federated learning framework that utilizes quantum computing to counteract the performance drop resulting from FHE. For the first time in FL, our framework combines a multimodal quantum mixture of experts (MQMoE) model with FHE, incorporating multimodal datasets for enriched representation and task-specific learning. Our MQMoE framework enhances performance on multimodal datasets and combined genomics and brain MRI scans, especially for underrepresented categories. Our results also demonstrate that the quantum-enhanced approach mitigates the performance degradation associated with FHE and improves classification accuracy across diverse datasets, validating the potential of quantum interventions in enhancing privacy in FL.</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Comprehensive Framework for Automated Segmentation of Perivascular Spaces in Brain MRI with the nnU-Net","url":"https://arxiv.org/abs/2411.19564","date":1739768400,"author":"","guid":912,"unread":true,"content":"<article>arXiv:2411.19564v2 Announce Type: replace-cross \nAbstract: Background: Enlargement of perivascular spaces (PVS) is common in neurodegenerative disorders including cerebral small vessel disease, Alzheimer's disease, and Parkinson's disease. PVS enlargement may indicate impaired clearance pathways and there is a need for reliable PVS detection methods which are currently lacking. Aim: To optimise a widely used deep learning model, the no-new-UNet (nnU-Net), for PVS segmentation. Methods: In 30 healthy participants (mean$\\pm$SD age: 50$\\pm$18.9 years; 13 females), T1-weighted MRI images were acquired using three different protocols on three MRI scanners (3T Siemens Tim Trio, 3T Philips Achieva, and 7T Siemens Magnetom). PVS were manually segmented across ten axial slices in each participant. Segmentations were completed using a sparse annotation strategy. In total, 11 models were compared using various strategies for image handling, preprocessing and semi-supervised learning with pseudo-labels. Model performance was evaluated using 5-fold cross validation (5FCV). The main performance metric was the Dice Similarity Coefficient (DSC). Results: The voxel-spacing agnostic model (mean$\\pm$SD DSC=64.3$\\pm$3.3%) outperformed models which resampled images to a common resolution (DSC=40.5-55%). Model performance improved substantially following iterative label cleaning (DSC=85.7$\\pm$1.2%). Semi-supervised learning with pseudo-labels (n=12,740) from 18 additional datasets improved the agreement between raw and predicted PVS cluster counts (Lin's concordance correlation coefficient=0.89, 95%CI=0.82-0.94). We extended the model to enable PVS segmentation in the midbrain (DSC=64.3$\\pm$6.5%) and hippocampus (DSC=67.8$\\pm$5%). Conclusions: Our deep learning models provide a robust and holistic framework for the automated quantification of PVS in brain MRI.</article>","contentLength":1870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Schr\\\"odinger Bridge Problem for Jump Diffusions","url":"https://arxiv.org/abs/2411.13765","date":1739768400,"author":"","guid":913,"unread":true,"content":"<article>arXiv:2411.13765v2 Announce Type: replace-cross \nAbstract: The Schr\\\"odinger bridge problem (SBP) seeks to find the measure $\\hat{\\mathbf{P}}$ on a certain path space which interpolates between state-space distributions $\\rho_0$ at time $0$ and $\\rho_T$ at time $T$ while minimizing the KL divergence (relative entropy) to a reference path measure $\\mathbf{R}$. In this work, we tackle the SBP in the case when $\\mathbf{R}$ is the path measure of a jump diffusion. Under mild assumptions, with both the operator theory approach and the stochastic calculus techniques, we establish an $h$-transform theory for jump diffusions and devise an approximation method to achieve the jump-diffusion SBP solution $\\hat{\\mathbf{P}}$ as the strong-convergence limit of a sequence of harmonic $h$-transforms. To the best of our knowledge, these results are novel in the study of SBP. Moreover, the $h$-transform framework and the approximation method developed in this work are robust and applicable to a relatively general class of jump diffusions. In addition, we examine the SBP of particular types of jump diffusions under additional regularity conditions and extend the existing results on the SBP from the diffusion case to the jump-diffusion setting.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature Learning in Chest CT Segmentation","url":"https://arxiv.org/abs/2411.13198","date":1739768400,"author":"","guid":914,"unread":true,"content":"<article>arXiv:2411.13198v2 Announce Type: replace-cross \nAbstract: In the field of medical image segmentation, challenges such as indistinct lesion features, ambiguous boundaries,and multi-scale characteristics have long revailed. This paper proposes an improved method named Intensity-Spatial Dual Masked AutoEncoder (ISD-MAE). Based on the tissue-contrast semi-masked autoencoder, a Masked AutoEncoder (MAE) branch is introduced to perform intensity masking and spatial masking operations on chest CT images for multi-scale feature learning and segmentation tasks. The model utilizes a dual-branch structure and contrastive learning to enhance the ability to learn tissue features and boundary details. Experiments are conducted on multiple 2D and 3D datasets. The results show that ISD-MAE significantly outperforms other methods in 2D pneumonia and mediastinal tumor segmentation tasks. For example, the Dice score reaches 90.10% on the COVID19 LESION dataset, and the performance is relatively stable. However, there is still room for improvement on 3D datasets. In response to this, improvement directions are proposed, including optimizing the loss function, using enhanced 3D convolution blocks, and processing datasets from multiple perspectives.Our code is available at:https://github.com/prowontheus/ISD-MAE.</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Bregman firmly nonexpansive proximal operator for baryconvex optimization","url":"https://arxiv.org/abs/2411.00928","date":1739768400,"author":"","guid":915,"unread":true,"content":"<article>arXiv:2411.00928v2 Announce Type: replace-cross \nAbstract: We present a generalization of the proximal operator defined through a convex combination of convex objectives, where the coefficients are updated in a minimax fashion. We prove that this new operator is Bregman firmly nonexpansive with respect to a Bregman divergence that combines Euclidean and information geometries. Finally, we derive the associated continuous flows.</article>","contentLength":431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MassSpecGym: A benchmark for the discovery and identification of molecules","url":"https://arxiv.org/abs/2410.23326","date":1739768400,"author":"","guid":916,"unread":true,"content":"<article>arXiv:2410.23326v3 Announce Type: replace-cross \nAbstract: The discovery and identification of molecules in biological and environmental samples is crucial for advancing biomedical and chemical sciences. Tandem mass spectrometry (MS/MS) is the leading technique for high-throughput elucidation of molecular structures. However, decoding a molecular structure from its mass spectrum is exceptionally challenging, even when performed by human experts. As a result, the vast majority of acquired MS/MS spectra remain uninterpreted, thereby limiting our understanding of the underlying (bio)chemical processes. Despite decades of progress in machine learning applications for predicting molecular structures from MS/MS spectra, the development of new methods is severely hindered by the lack of standard datasets and evaluation protocols. To address this problem, we propose MassSpecGym -- the first comprehensive benchmark for the discovery and identification of molecules from MS/MS data. Our benchmark comprises the largest publicly available collection of high-quality labeled MS/MS spectra and defines three MS/MS annotation challenges: de novo molecular structure generation, molecule retrieval, and spectrum simulation. It includes new evaluation metrics and a generalization-demanding data split, therefore standardizing the MS/MS annotation tasks and rendering the problem accessible to the broad machine learning community. MassSpecGym is publicly available at https://github.com/pluskal-lab/MassSpecGym.</article>","contentLength":1510,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Statistical Complexity of Estimating Vendi Scores from Empirical Data","url":"https://arxiv.org/abs/2410.21719","date":1739768400,"author":"","guid":917,"unread":true,"content":"<article>arXiv:2410.21719v2 Announce Type: replace-cross \nAbstract: Evaluating the diversity of generative models without access to reference data poses methodological challenges. The reference-free Vendi score offers a solution by quantifying the diversity of generated data using matrix-based entropy measures. The Vendi score is usually computed via the eigendecomposition of an $n \\times n$ kernel matrix for $n$ generated samples. However, the heavy computational cost of eigendecomposition for large $n$ often limits the sample size used in practice to a few tens of thousands. In this paper, we investigate the statistical convergence of the Vendi score. We numerically demonstrate that for kernel functions with an infinite feature map dimension, the score estimated from a limited sample size may exhibit a non-negligible bias relative to the population Vendi score, i.e., the asymptotic limit as the sample size approaches infinity. To address this, we introduce a truncation of the Vendi statistic, called the $t$-truncated Vendi statistic, which is guaranteed to converge to its asymptotic limit given $n=O(t)$ samples. We show that the existing Nystr\\\"om method and the FKEA approximation method for approximating the Vendi score both converge to the population truncated Vendi score. We perform several numerical experiments to illustrate the concentration of the Nystr\\\"om and FKEA-computed Vendi scores around the truncated Vendi and discuss how the truncated Vendi score correlates with the diversity of image and text data.</article>","contentLength":1532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computing MHD equilibria of stellarators with a flexible coordinate frame","url":"https://arxiv.org/abs/2410.17595","date":1739768400,"author":"","guid":918,"unread":true,"content":"<article>arXiv:2410.17595v3 Announce Type: replace-cross \nAbstract: For the representation of axi-symmetric plasma configurations, it is natural to use cyl. coordinates (R,Z,$\\phi$), where $\\phi$ is an independent coordinate. The same cyl. coordinates have also been widely used for representing 3D MHD equilibria of non-axisymmetric configurations (stellarators), with cross-sections, defined in RZ-planes, that vary over $\\phi$. Stellarator equilibria have been found, however, for which cyl. coordinates are not at all a natural choice, for instance certain stellarators obtained using the near-axis expansion (NAE), defined by a magn. axis curve and its Frenet frame.\n  In this contribution, we propose an alternative approach for representing the boundary in a fixed-boundary 3D MHD equil. solver, moving away from cyl. coordinates. Instead, we use planar cross-sections whose orientation is determined by a general coordinate frame (G-Frame). This frame is similar to the conventional Frenet frame, but more flexible. As an additional part of the boundary representation, it becomes an input to the equil. solve, along with the geometry of the cross-sections. We see two advantages: 1) the capability to easily represent configurations where the magn. axis is highly non-planar or even knotted 2) a reduction in the degrees of freedom needed for the boundary surface, and thus the equil. solver, enabling progress in optimization of these configurations.\n  We discuss the properties of the G-Frame, starting from the conventional Frenet frame. Then we show two exemplary ways of constructing it, first from a NAE solution and also from a given boundary surface. We present the details of the implementation of the new frame in the 3D MHD equil. solver GVEC. Furthermore, we demonstrate for a highly shaped QI-optimized stellarator that far fewer degrees of freedom are necessary to find a high quality equil. solution, compared to the solution computed in cyl. coordinates.</article>","contentLength":1970,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The shape of the brain's connections is predictive of cognitive performance: an explainable machine learning study","url":"https://arxiv.org/abs/2410.15108","date":1739768400,"author":"","guid":919,"unread":true,"content":"<article>arXiv:2410.15108v2 Announce Type: replace-cross \nAbstract: The shape of the brain's white matter connections is relatively unexplored in diffusion MRI tractography analysis. While it is known that tract shape varies in populations and across the human lifespan, it is unknown if the variability in dMRI tractography-derived shape may relate to the brain's functional variability across individuals. This work explores the potential of leveraging tractography fiber cluster shape measures to predict subject-specific cognitive performance. We implement machine learning models to predict individual cognitive performance scores. We study a large-scale database from the HCP-YA study. We apply an atlas-based fiber cluster parcellation to the dMRI tractography of each individual. We compute 15 shape, microstructure, and connectivity features for each fiber cluster. Using these features as input, we train a total of 210 models to predict 7 different NIH Toolbox cognitive performance assessments. We apply an explainable AI technique, SHAP, to assess the importance of each fiber cluster for prediction. Our results demonstrate that shape measures are predictive of individual cognitive performance. The studied shape measures, such as irregularity, diameter, total surface area, volume, and branch volume, are as effective for prediction as microstructure and connectivity measures. The overall best-performing feature is a shape feature, irregularity, which describes how different a cluster's shape is from an idealized cylinder. Further interpretation using SHAP values suggest that fiber clusters with features highly predictive of cognitive ability are widespread throughout the brain, including fiber clusters from the superficial association, deep association, cerebellar, striatal, and projection pathways. This study demonstrates the strong potential of shape descriptors to enhance the study of the brain's white matter and its relationship to cognitive function.</article>","contentLength":1975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero-shot Model-based Reinforcement Learning using Large Language Models","url":"https://arxiv.org/abs/2410.11711","date":1739768400,"author":"","guid":920,"unread":true,"content":"<article>arXiv:2410.11711v2 Announce Type: replace-cross \nAbstract: The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs' deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.</article>","contentLength":1137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Channel Distinguishability in Local Neighborhoods of the Model Space in Quantum Neural Networks","url":"https://arxiv.org/abs/2410.09470","date":1739768400,"author":"","guid":921,"unread":true,"content":"<article>arXiv:2410.09470v2 Announce Type: replace-cross \nAbstract: With the increasing interest in Quantum Machine Learning, Quantum Neural Networks (QNNs) have emerged and gained significant attention. These models have, however, been shown to be notoriously difficult to train, which we hypothesize is partially due to the architectures, called ansatzes, that are hardly studied at this point. Therefore, in this paper, we take a step back and analyze ansatzes. We initially consider their expressivity, i.e., the space of operations they are able to express, and show that the closeness to being a 2-design, the primarily used measure, fails at capturing this property. Hence, we look for alternative ways to characterize ansatzes by considering the local neighborhood of the model space, in particular, analyzing model distinguishability upon small perturbation of parameters. We derive an upper bound on their distinguishability, showcasing that QNNs with few parameters are hardly discriminable upon update. Our numerical experiments support our bounds and further indicate that there is a significant degree of variability, which stresses the need for warm-starting or clever initialization. Altogether, our work provides an ansatz-centric perspective on training dynamics and difficulties in QNNs, ultimately suggesting that iterative training of small quantum models may not be effective, which contrasts their initial motivation.</article>","contentLength":1431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CR-CTC: Consistency regularization on CTC for improved speech recognition","url":"https://arxiv.org/abs/2410.05101","date":1739768400,"author":"","guid":922,"unread":true,"content":"<article>arXiv:2410.05101v4 Announce Type: replace-cross \nAbstract: Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input speech mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC. It significantly improves the CTC performance, achieving state-of-the-art results comparable to those attained by transducer or systems combining CTC and attention-based encoder-decoder (CTC/AED). We release our code at https://github.com/k2-fsa/icefall.</article>","contentLength":1337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Performance and Robustness of Signal-Dependent vs. Signal-Independent Binaural Signal Matching with Wearable Microphone Arrays","url":"https://arxiv.org/abs/2409.11731","date":1739768400,"author":"","guid":923,"unread":true,"content":"<article>arXiv:2409.11731v3 Announce Type: replace-cross \nAbstract: The increasing popularity of spatial audio in applications such as teleconferencing, entertainment, and virtual reality has led to the recent developments of binaural reproduction methods. However, only a few of these methods are well-suited for wearable and mobile arrays, which typically consist of a small number of microphones. One such method is binaural signal matching (BSM), which has been shown to produce high-quality binaural signals for wearable arrays. However, BSM may be suboptimal in cases of high direct-to-reverberant ratio (DRR) as it is based on the diffuse sound field assumption. To overcome this limitation, previous studies incorporated sound-field models other than diffuse. However, performance may be sensitive to signal estimation errors. This paper aims to provide a systematic and comprehensive analysis of signal-dependent vs. signal-independent BSM, so that the benefits and limitations of the methods become clearer. Two signal-dependent BSM-based methods designed for high DRR scenarios that incorporate a sound field model composed of direct and reverberant components are investigated mathematically, using simulations, and finally validated by a listening test, and compared to the signal-independent BSM. The results show that signal-dependent BSM can significantly improve performance, in particular in the direction of the source, while presenting only a negligible degradation in other directions. Furthermore, when source direction estimation is inaccurate, performance of of the signal-dependent BSM degrade to equal that of the signal-independent BSM, presenting a desired robustness quality.</article>","contentLength":1695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Application of Langevin Dynamics to Advance the Quantum Natural Gradient Optimization Algorithm","url":"https://arxiv.org/abs/2409.01978","date":1739768400,"author":"","guid":924,"unread":true,"content":"<article>arXiv:2409.01978v3 Announce Type: replace-cross \nAbstract: A Quantum Natural Gradient (QNG) algorithm for optimization of variational quantum circuits has been proposed recently. In this study, we employ the Langevin equation with a QNG stochastic force to demonstrate that its discrete-time solution gives a generalized form of the above-specified algorithm, which we call Momentum-QNG. Similar to other optimization algorithms with the momentum term, such as the Stochastic Gradient Descent with momentum, RMSProp with momentum and Adam, Momentum-QNG is more effective to escape local minima and plateaus in the variational parameter space and, therefore, achieves a better convergence behavior compared to the basic QNG. In this paper we benchmark Momentum-QNG together with basic QNG, Adam and Momentum optimizers and find the optimal values of its hyperparameters. Our open-source code is available at https://github.com/borbysh/Momentum-QNG</article>","contentLength":946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Orientable and negative orientable sequences","url":"https://arxiv.org/abs/2409.00672","date":1739768400,"author":"","guid":925,"unread":true,"content":"<article>arXiv:2409.00672v5 Announce Type: replace-cross \nAbstract: Analogously to de Bruijn sequences, orientable sequences have application in automatic position-location applications and, until recently, studies of these sequences focused on the binary case. In recent work by Alhakim et al., a range of methods of construction were described for orientable sequences over arbitrary finite alphabets; some of these methods involve using negative orientable sequences as a building block. In this paper we describe three techniques for generating such negative orientable sequences, as well as upper bounds on their period. We then go on to show how these negative orientable sequences can be used to generate orientable sequences with period close to the maximum possible for every non-binary alphabet size and for every tuple length. In doing so we use two closely related approaches described by Alhakim et al.</article>","contentLength":906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hedge Fund Portfolio Construction Using PolyModel Theory and iTransformer","url":"https://arxiv.org/abs/2408.03320","date":1739768400,"author":"","guid":926,"unread":true,"content":"<article>arXiv:2408.03320v3 Announce Type: replace-cross \nAbstract: When constructing portfolios, a key problem is that a lot of financial time series data are sparse, making it challenging to apply machine learning methods. Polymodel theory can solve this issue and demonstrate superiority in portfolio construction from various aspects. To implement the PolyModel theory for constructing a hedge fund portfolio, we begin by identifying an asset pool, utilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory also involves choosing a wide-ranging set of risk factors, which includes various financial indices, currencies, and commodity prices. This comprehensive selection mirrors the complexities of the real-world environment. Leveraging on the PolyModel theory, we create quantitative measures such as Long-term Alpha, Long-term Ratio, and SVaR. We also use more classical measures like the Sharpe ratio or Morningstar's MRAR. To enhance the performance of the constructed portfolio, we also employ the latest deep learning techniques (iTransformer) to capture the upward trend, while efficiently controlling the downside, using all the features. The iTransformer model is specifically designed to address the challenges in high-dimensional time series forecasting and could largely improve our strategies. More precisely, our strategies achieve better Sharpe ratio and annualized return. The above process enables us to create multiple portfolio strategies aiming for high returns and low risks when compared to various benchmarks.</article>","contentLength":1550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MEMS and ECM Sensor Technologies for Cardiorespiratory Sound Monitoring - A Comprehensive Review","url":"https://arxiv.org/abs/2406.12432","date":1739768400,"author":"","guid":927,"unread":true,"content":"<article>arXiv:2406.12432v2 Announce Type: replace-cross \nAbstract: This paper presents a comprehensive review of cardiorespiratory auscultation sensing devices (i.e., stethoscopes), which is useful for understanding the theoretical aspects and practical design notes. In this paper, we first introduce the acoustic properties of the heart and lungs, as well as a brief history of stethoscope evolution. Then, we discuss the basic concept of electret condenser microphones (ECMs) and a stethoscope based on them. Then, we discuss the microelectromechanical systems (MEMSs) technology, particularly focusing on piezoelectric transducer sensors. This paper comprehensively reviews sensing technologies for cardiorespiratory auscultation, emphasizing MEMS-based wearable designs in the past decade. To our knowledge, this is the first paper to summarize ECM and MEMS applications for heart and lung sound analysis.</article>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Joint semi-supervised and contrastive learning enables domain generalization and multi-domain segmentation","url":"https://arxiv.org/abs/2405.05336","date":1739768400,"author":"","guid":928,"unread":true,"content":"<article>arXiv:2405.05336v2 Announce Type: replace-cross \nAbstract: Despite their effectiveness, current deep learning models face challenges with images coming from different domains with varying appearance and content. We introduce SegCLR, a versatile framework designed to segment images across different domains, employing supervised and contrastive learning simultaneously to effectively learn from both labeled and unlabeled data. We demonstrate the superior performance of SegCLR through a comprehensive evaluation involving three diverse clinical datasets of 3D retinal Optical Coherence Tomography (OCT) images, for the slice-wise segmentation of fluids with various network configurations and verification across 10 different network initializations. In an unsupervised domain adaptation context, SegCLR achieves results on par with a supervised upper-bound model trained on the intended target domain. Notably, we discover that the segmentation performance of SegCLR framework is marginally impacted by the abundance of unlabeled data from the target domain, thereby we also propose an effective domain generalization extension of SegCLR, known also as zero-shot domain adaptation, which eliminates the need for any target domain information. This shows that our proposed addition of contrastive loss in standard supervised training for segmentation leads to superior models, inherently more generalizable to both in- and out-of-domain test data. We additionally propose a pragmatic solution for SegCLR deployment in realistic scenarios with multiple domains containing labeled data. Accordingly, our framework pushes the boundaries of deep-learning based segmentation in multi-domain applications, regardless of data availability - labeled, unlabeled, or nonexistent.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Layered Uploading for Quantum Convolutional Neural Networks","url":"https://arxiv.org/abs/2404.09750","date":1739768400,"author":"","guid":929,"unread":true,"content":"<article>arXiv:2404.09750v2 Announce Type: replace-cross \nAbstract: Continuing our analysis of quantum machine learning applied to our use-case of malware detection, we investigate the potential of quantum convolutional neural networks. More precisely, we propose a new architecture where data is uploaded all along the quantum circuit. This allows us to use more features from the data, hence giving to the algorithm more information, without having to increase the number of qubits that we use for the quantum circuit. This approach is motivated by the fact that we do not always have great amounts of data, and that quantum computers are currently restricted in their number of logical qubits.</article>","contentLength":687,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computing the spectrum and pseudospectrum of infinite-volume operators from local patches","url":"https://arxiv.org/abs/2403.19055","date":1739768400,"author":"","guid":930,"unread":true,"content":"<article>arXiv:2403.19055v2 Announce Type: replace-cross \nAbstract: We show how the spectrum of normal discrete short-range infinite-volume operators can be approximated with two-sided error control using only data from finite-sized local patches. As a corollary, we prove the computability of the spectrum of such infinite-volume operators with the additional property of finite local complexity and provide an explicit algorithm. Such operators appear in many applications, e.g. as discretizations of differential operators on unbounded domains or as so-called tight-binding Hamiltonians in solid state physics. For a large class of such operators, our result allows for the first time to establish computationally also the absence of spectrum, i.e. the existence and the size of spectral gaps. We extend our results to the $\\varepsilon$-pseudospectrum of non-normal operators, proving that also the pseudospectrum of such operators is computable.</article>","contentLength":940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium","url":"https://arxiv.org/abs/2402.02303","date":1739768400,"author":"","guid":931,"unread":true,"content":"<article>arXiv:2402.02303v3 Announce Type: replace-cross \nAbstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evidence of Scaling Regimes in the Hopfield Dynamics of Whole Brain Model","url":"https://arxiv.org/abs/2401.07538","date":1739768400,"author":"","guid":932,"unread":true,"content":"<article>arXiv:2401.07538v3 Announce Type: replace-cross \nAbstract: It is shown that a Hopfield recurrent neural network exhibits a scaling regime, whose specific exponents depend on the number of parcels used and the decay length of the coupling strength. This scaling regime recovers the picture introduced by Deco et al., according to which the process of information transfer within the human brain shows spatially correlated patterns qualitatively similar to those displayed by turbulent flows, although with a more singular exponent, 1/2 instead of 2/3. Both models employ a coupling strength which decays exponentially with the Euclidean distance between the nodes, informed by experimentally derived brain topology. Nevertheless, their mathematical nature is very different, Hopf oscillators versus a Hopfield neural network, respectively. Hence, their convergence for the same data parameters, suggests an intriguing robustness of the scaling picture.Furthermore, the present analysis shows that the Hopfield model brain remains functional by removing links above about five decay lengths, corresponding to about one sixth of the size of the global brain. This suggests that, in terms of connectivity decay length, the Hopfield brain functions in a sort of intermediate ``turbulent liquid''-like state, whose essential connections are the intermediate ones between the connectivity decay length and the global brain size. The evident sensitivity of the scaling exponent to the value of the decay length, as well as to the number of brain parcels employed, leads us to take with great caution any quantitative assessment regarding the specific nature of the scaling regime.</article>","contentLength":1672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Auto.gov: Learning-based Governance for Decentralized Finance (DeFi)","url":"https://arxiv.org/abs/2302.09551","date":1739768400,"author":"","guid":933,"unread":true,"content":"<article>arXiv:2302.09551v3 Announce Type: replace-cross \nAbstract: Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional DeFi governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce \"Auto.gov\", a learning-based governance framework that employs a deep Q-network (DQN) reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto.gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto.gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric-protocol profitability. Overall, the comprehensive evaluations confirm that Auto.gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.</article>","contentLength":1552,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pitfalls of Evidence-Based AI Policy","url":"https://arxiv.org/abs/2502.09618","date":1739768400,"author":"","guid":934,"unread":true,"content":"<article>arXiv:2502.09618v2 Announce Type: replace \nAbstract: Nations across the world are working to govern AI. However, from a technical perspective, there is uncertainty and disagreement on the best way to do this. Meanwhile, recent debates over AI regulation have led to calls for \"evidence-based AI policy\" which emphasize holding regulatory action to a high evidentiary standard. Evidence is of irreplaceable value to policymaking. However, holding regulatory action to too high an evidentiary standard can lead to systematic neglect of certain risks. In historical policy debates (e.g., over tobacco ca. 1965 and fossil fuels ca. 1985) \"evidence-based policy\" rhetoric is also a well-precedented strategy to downplay the urgency of action, delay regulation, and protect industry interests. Here, we argue that if the goal is evidence-based AI policy, the first regulatory objective must be to actively facilitate the process of identifying, studying, and deliberating about AI risks. We discuss a set of 15 regulatory goals to facilitate this and show that Brazil, Canada, China, the EU, South Korea, the UK, and the USA all have substantial opportunities to adopt further evidence-seeking policies.</article>","contentLength":1197,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Score-of-Mixture Training: Training One-Step Generative Models Made Simple via Score Estimation of Mixture Distributions","url":"https://arxiv.org/abs/2502.09609","date":1739768400,"author":"","guid":935,"unread":true,"content":"<article>arXiv:2502.09609v2 Announce Type: replace \nAbstract: We propose Score-of-Mixture Training (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the $\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call Score-of-Mixture Distillation (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even outperform existing methods.</article>","contentLength":775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering","url":"https://arxiv.org/abs/2502.09573","date":1739768400,"author":"","guid":936,"unread":true,"content":"<article>arXiv:2502.09573v2 Announce Type: replace \nAbstract: In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification systems across various domains in industry.</article>","contentLength":891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based Security","url":"https://arxiv.org/abs/2502.09535","date":1739768400,"author":"","guid":937,"unread":true,"content":"<article>arXiv:2502.09535v2 Announce Type: replace \nAbstract: Mobile sensor data has been proposed for security-critical applications such as device pairing, proximity detection, and continuous authentication. However, the foundational assumption that these signals provide sufficient entropy remains under-explored. In this work, we systematically analyse the entropy of mobile sensor data across four diverse datasets spanning multiple application contexts. Our findings reveal pervasive biases, with single-sensor mean min-entropy values ranging from 3.408-4.483 bits (S.D.=1.018-1.574) despite Shannon entropy being several multiples higher. We further demonstrate that correlations between sensor modalities reduce the worst-case entropy of using multiple sensors by up to approx. 75% compared to average-case Shannon entropy. This brings joint min-entropy well below 10 bits in many cases and, in the best case, yielding only approx. 24 bits of min-entropy when combining 20 sensor modalities. These results call into question the widely held assumption that adding more sensors inherently yields higher security. We ultimately caution against relying on raw sensor data as a primary source of randomness.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling","url":"https://arxiv.org/abs/2502.09509","date":1739768400,"author":"","guid":938,"unread":true,"content":"<article>arXiv:2502.09509v2 Announce Type: replace \nAbstract: Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.</article>","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic Forgetting","url":"https://arxiv.org/abs/2502.09500","date":1739768400,"author":"","guid":939,"unread":true,"content":"<article>arXiv:2502.09500v2 Announce Type: replace \nAbstract: Catastrophic forgetting -- the phenomenon of a neural network learning a task t1 and losing the ability to perform it after being trained on some other task t2 -- is a long-standing problem for neural networks [McCloskey and Cohen, 1989]. We present a method, Eidetic Learning, that provably solves catastrophic forgetting. A network trained with Eidetic Learning -- here, an EideticNet -- requires no rehearsal or replay. We consider successive discrete tasks and show how at inference time an EideticNet automatically routes new instances without auxiliary task information. An EideticNet bears a family resemblance to the sparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network capacity is partitioned across tasks and the network itself performs data-conditional routing. An EideticNet is easy to implement and train, is efficient, and has time and space complexity linear in the number of parameters. The guarantee of our method holds for normalization layers of modern neural networks during both pre-training and fine-tuning. We show with a variety of network architectures and sets of tasks that EideticNets are immune to forgetting. While the practical benefits of EideticNets are substantial, we believe they can be benefit practitioners and theorists alike. The code for training EideticNets is available at https://github.com/amazon-science/eideticnet-training.</article>","contentLength":1448,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Predict Global Atrial Fibrillation Dynamics from Sparse Measurements","url":"https://arxiv.org/abs/2502.09473","date":1739768400,"author":"","guid":940,"unread":true,"content":"<article>arXiv:2502.09473v2 Announce Type: replace \nAbstract: Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all treatment with limited success in persistent AF. This may be due to our inability to map the dynamics of AF with the limited resolution and coverage provided by sequential contact mapping catheters, preventing effective patient phenotyping for personalised, targeted ablation. Here we introduce FibMap, a graph recurrent neural network model that reconstructs global AF dynamics from sparse measurements. Trained and validated on 51 non-contact whole atria recordings, FibMap reconstructs whole atria dynamics from 10% surface coverage, achieving a 210% lower mean absolute error and an order of magnitude higher performance in tracking phase singularities compared to baseline methods. Clinical utility of FibMap is demonstrated on real-world contact mapping recordings, achieving reconstruction fidelity comparable to non-contact mapping. FibMap's state-spaces and patient-specific parameters offer insights for electrophenotyping AF. Integrating FibMap into clinical practice could enable personalised AF care and improve outcomes.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of harmonic average method for interface problems with discontinuous solutions and fluxes","url":"https://arxiv.org/abs/2502.09413","date":1739768400,"author":"","guid":941,"unread":true,"content":"<article>arXiv:2502.09413v2 Announce Type: replace \nAbstract: Harmonic average method has been widely utilized to deal with heterogeneous coefficients in solving differential equations. One remarkable advantage of the harmonic averaging method is that no derivative of the coefficient is needed. Furthermore, the coefficient matrix of the finite difference equations is an M-matrix which guarantees the stability of the algorithm. It has been numerically observed but not theoretically proved that the method produces second order pointwise accuracy when the solution and flux are continuous even if the coefficient has finite discontinuities for which the method is inconsistent ($O(1)$ in the local truncation errors). It has been believed that there are some fortunate error cancellations. The harmonic average method does not converge when the solution or the flux has finite discontinuities. In this paper, not only we rigorously prove the second order convergence of the harmonic averaging method for one-dimensional interface problem when the coefficient has a finite discontinuities and the solution and the flux are continuous, but also proposed an {\\em improved harmonic average method} that is also second order accurate (in the $L^{\\infty}$ norm), which allows discontinuous solutions and fluxes along with the discontinuous coefficients. The key in the convergence proof is the construction of the Green's function. The proof shows how the error cancellations occur in a subtle way. Numerical experiments in both 1D and 2D confirmed the theoretical proof of the improved harmonic average method.</article>","contentLength":1599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code Style Sheets: CSS for Code","url":"https://arxiv.org/abs/2502.09386","date":1739768400,"author":"","guid":942,"unread":true,"content":"<article>arXiv:2502.09386v2 Announce Type: replace \nAbstract: Program text is rendered using impoverished typographic styles. Beyond choice of fonts and syntax-highlighting colors, code editors and related tools utilize very few text decorations. These limited styles are, furthermore, applied in monolithic fashion, regardless of the programs and tasks at hand.\n  We present the notion of code style sheets for styling the textual representation of programs. Motivated by analogy to cascading style sheets (CSS) for styling HTML documents, code style sheets provide mechanisms for defining rules to select and style abstract syntax trees (ASTs). Technically, code style sheets generalize essential notions from CSS over untyped HTML trees to a programming-language setting with algebraic data types (such as ASTs). Practically, code style sheets allow ASTs to be styled granularly, based on semantic information -- such as the structure of abstract syntax, static type information, and corresponding run-time values -- as well as design choices on the part of authors and readers of a program. In this paper, we design and implement a code style sheets system for a subset of Haskell, using it to illustrate several use cases involving code presentation and visualization tasks. These examples demonstrate that code style sheets provide a uniform framework for rendering programs in multivarious ways.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LoRA Training Provably Converges to a Low-Rank Global Minimum or It Fails Loudly (But it Probably Won't Fail)","url":"https://arxiv.org/abs/2502.09376","date":1739768400,"author":"","guid":943,"unread":true,"content":"<article>arXiv:2502.09376v2 Announce Type: replace \nAbstract: Low-rank adaptation (LoRA) has become a standard approach for fine-tuning large foundation models. However, our theoretical understanding of LoRA remains limited as prior analyses of LoRA's training dynamics either rely on linearization arguments or consider highly simplified setups. In this work, we analyze the LoRA loss landscape without such restrictive assumptions. We define two regimes: a ``special regime'', which includes idealized setups where linearization arguments hold, and a ``generic regime'' representing more realistic setups where linearization arguments do not hold. In the generic regime, we show that LoRA training converges to a global minimizer with low rank and small magnitude, or a qualitatively distinct solution with high rank and large magnitude. Finally, we argue that the zero-initialization and weight decay in LoRA training induce an implicit bias toward the low-rank, small-magnitude region of the parameter space -- where global minima lie -- thus shedding light on why LoRA training usually succeeds in finding global minima.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delay Performance Analysis with Short Packets in Intelligent Machine Network","url":"https://arxiv.org/abs/2502.09313","date":1739768400,"author":"","guid":944,"unread":true,"content":"<article>arXiv:2502.09313v2 Announce Type: replace \nAbstract: With the rapid development of delay-sensitive services happened in industrial manufacturing, Internet of Vehicles, and smart logistics, more stringent delay requirements are put forward for the intelligent machine (IM) network. Short packet transmissions are widely adopted to reduce delay in IM networks. However, the delay performance of an IM network has not been sufficiently analyzed. This paper applies queuing theory and stochastic geometry to construct network model and transmission model for downlink communication, respectively, proposes and derives the following three metrics, e.g., the transmission success probability (with delay as the threshold), expected delay, and delay jitter. To accurately characterize the transmission delay with short packets, the finite blocklength capacity is used to measure the channel transmission rate. Simulation results show that the increase of packet length and IM density significantly deteriorates the three metrics. Short packets are needed to improve the three metrics, especially in high IM density scenarios. The outcomes of this paper provide an important theoretical basis for the optimization design and performance improvement of IM networks.</article>","contentLength":1256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Safety for Everyone","url":"https://arxiv.org/abs/2502.09288","date":1739768400,"author":"","guid":945,"unread":true,"content":"<article>arXiv:2502.09288v2 Announce Type: replace \nAbstract: Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI systems, suggesting that work on AI safety necessarily entails serious consideration of potential existential threats. However, this framing has three potential drawbacks: it may exclude researchers and practitioners who are committed to AI safety but approach the field from different angles; it could lead the public to mistakenly view AI safety as focused solely on existential scenarios rather than addressing a wide spectrum of safety challenges; and it risks creating resistance to safety measures among those who disagree with predictions of existential AI risks. Through a systematic literature review of primarily peer-reviewed research, we find a vast array of concrete safety work that addresses immediate and practical concerns with current AI systems. This includes crucial areas like adversarial robustness and interpretability, highlighting how AI safety research naturally extends existing technological and systems safety concerns and practices. Our findings suggest the need for an epistemically inclusive and pluralistic conception of AI safety that can accommodate the full range of safety considerations, motivations, and perspectives that currently shape the field.</article>","contentLength":1390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection","url":"https://arxiv.org/abs/2502.09271","date":1739768400,"author":"","guid":946,"unread":true,"content":"<article>arXiv:2502.09271v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in modeling data with graph structures, yet recent research reveals their susceptibility to adversarial attacks. Traditional attack methodologies, which rely on manipulating the original graph or adding links to artificially created nodes, often prove impractical in real-world settings. This paper introduces a novel adversarial scenario involving the injection of an isolated subgraph to deceive both the link recommender and the node classifier within a GNN system. Specifically, the link recommender is mislead to propose links between targeted victim nodes and the subgraph, encouraging users to unintentionally establish connections and that would degrade the node classification accuracy, thereby facilitating a successful attack. To address this, we present the LiSA framework, which employs a dual surrogate model and bi-level optimization to simultaneously meet two adversarial objectives. Extensive experiments on real-world datasets demonstrate the effectiveness of our method.</article>","contentLength":1112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation","url":"https://arxiv.org/abs/2502.09268","date":1739768400,"author":"","guid":947,"unread":true,"content":"<article>arXiv:2502.09268v2 Announce Type: replace \nAbstract: With the rapid development of embodied artificial intelligence, significant progress has been made in vision-language-action (VLA) models for general robot decision-making. However, the majority of existing VLAs fail to account for the inevitable external perturbations encountered during deployment. These perturbations introduce unforeseen state information to the VLA, resulting in inaccurate actions and consequently, a significant decline in generalization performance. The classic internal model control (IMC) principle demonstrates that a closed-loop system with an internal model that includes external input signals can accurately track the reference input and effectively offset the disturbance. We propose a novel closed-loop VLA method GEVRM that integrates the IMC principle to enhance the robustness of robot visual manipulation. The text-guided video generation model in GEVRM can generate highly expressive future visual planning goals. Simultaneously, we evaluate perturbations by simulating responses, which are called internal embeddings and optimized through prototype contrastive learning. This allows the model to implicitly infer and distinguish perturbations from the external environment. The proposed GEVRM achieves state-of-the-art performance on both standard and perturbed CALVIN benchmarks and shows significant improvements in realistic robot tasks.</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Commitment Schemes from OWFs with Applications to Quantum Oblivious Transfer","url":"https://arxiv.org/abs/2502.09201","date":1739768400,"author":"","guid":948,"unread":true,"content":"<article>arXiv:2502.09201v2 Announce Type: replace \nAbstract: Commitment schemes are essential to many cryptographic protocols and schemes with applications that include privacy-preserving computation on data, privacy-preserving authentication, and, in particular, oblivious transfer protocols. For quantum oblivious transfer (qOT) protocols, unconditionally binding commitment schemes that do not rely on hardness assumptions from structured mathematical problems are required. These additional constraints severely limit the choice of commitment schemes to random oracle-based constructions or Naor's bit commitment scheme. As these protocols commit to individual bits, the use of such commitment schemes comes at a high bandwidth and computational cost. In this work, we investigate improvements to the efficiency of commitment schemes used in qOT protocols and propose an extension of Naor's commitment scheme requiring the existence of one-way functions (OWF) to reduce communication complexity for 2-bit strings. Additionally, we provide an interactive string commitment scheme with preprocessing to enable a fast and efficient computation of commitments.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An hp Multigrid Approach for Tensor-Product Space-Time Finite Element Discretizations of the Stokes Equations","url":"https://arxiv.org/abs/2502.09159","date":1739768400,"author":"","guid":949,"unread":true,"content":"<article>arXiv:2502.09159v2 Announce Type: replace \nAbstract: We present a monolithic $hp$ space-time multigrid method for tensor-product space-time finite element discretizations of the Stokes equations. Geometric and polynomial coarsening of the space-time mesh is performed, and the entire algorithm is expressed through rigorous mathematical mappings. For the discretization, we use inf-sup stable pairs $\\mathbb Q_{r+1}/\\mathbb P_{r}^{\\text{disc}}$ of elements in space and a discontinuous Galerkin (DG$(k)$) discretization in time with piecewise polynomials of order $k$. The key novelty of this work is the application of $hp$ multigrid techniques in space and time, facilitated and accelerated by the matrix-free capabilities of the deal.II library. While multigrid methods are well-established for stationary problems, their application in space-time formulations encounter unique challenges, particularly in constructing suitable smoothers. To overcome these challenges, we employ a space-time cell-wise Vanka smoother. Extensive tests on high-performance computing platforms demonstrate the efficiency of our $hp$ multigrid approach on problem sizes exceeding a trillion degrees of freedom (dofs), sustaining throughputs of hundreds of millions of dofs per second.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Social Construction of Visualizations: Practitioner Challenges and Experiences of Visualizing Race and Gender Demographic Data","url":"https://arxiv.org/abs/2502.09048","date":1739768400,"author":"","guid":950,"unread":true,"content":"<article>arXiv:2502.09048v2 Announce Type: replace \nAbstract: Data visualizations are increasingly seen as socially constructed, with several recent studies positing that perceptions and interpretations of visualization artifacts are shaped through complex sets of interactions between members of a community. However, most of these works have focused on audiences and researchers, and little is known about if and how practitioners account for the socially constructed framing of data visualization. In this paper, we study and analyze how visualization practitioners understand the influence of their beliefs, values, and biases in their design processes and the challenges they experience. In 17 semi-structured interviews with designers working with race and gender demographic data, we find that a complex mix of factors interact to inform how practitioners approach their design process, including their personal experiences, values, and their understandings of power, neutrality, and politics. Based on our findings, we suggest a series of implications for research and practice in this space.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning","url":"https://arxiv.org/abs/2502.09022","date":1739768400,"author":"","guid":951,"unread":true,"content":"<article>arXiv:2502.09022v2 Announce Type: replace \nAbstract: Transformer-based language models have achieved significant success; however, their internal mechanisms remain largely opaque due to the complexity of non-linear interactions and high-dimensional operations. While previous studies have demonstrated that these models implicitly embed reasoning trees, humans typically employ various distinct logical reasoning mechanisms to complete the same task. It is still unclear which multi-step reasoning mechanisms are used by language models to solve such tasks. In this paper, we aim to address this question by investigating the mechanistic interpretability of language models, particularly in the context of multi-step reasoning tasks. Specifically, we employ circuit analysis and self-influence functions to evaluate the changing importance of each token throughout the reasoning process, allowing us to map the reasoning paths adopted by the model. We apply this methodology to the GPT-2 model on a prediction task (IOI) and demonstrate that the underlying circuits reveal a human-interpretable reasoning process used by the model.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural Force Field: Learning Generalized Physical Representation from a Few Examples","url":"https://arxiv.org/abs/2502.08987","date":1739768400,"author":"","guid":952,"unread":true,"content":"<article>arXiv:2502.08987v2 Announce Type: replace \nAbstract: Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF) a modeling framework built on Neural Ordinary Differential Equation (NODE) that learns interpretable force field representations which can be efficiently integrated through an Ordinary Differential Equation ( ODE) solver to predict object trajectories. Unlike existing approaches that rely on high-dimensional latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in an interpretable manner. Experiments on two challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.</article>","contentLength":1497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage","url":"https://arxiv.org/abs/2502.08966","date":1739768400,"author":"","guid":953,"unread":true,"content":"<article>arXiv:2502.08966v2 Announce Type: replace \nAbstract: Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions. Existing defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. RTBAS adapts Information Flow Control to the unique challenges presented by TBAS. We present two novel dependency screeners, using LM-as-a-judge and attention-based saliency, to overcome these challenges. Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.</article>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond the Singular: The Essential Role of Multiple Generations in Effective Benchmark Evaluation and Analysis","url":"https://arxiv.org/abs/2502.08943","date":1739768400,"author":"","guid":954,"unread":true,"content":"<article>arXiv:2502.08943v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated significant utilities in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantic prompts, enabling error detection and quality control in benchmark construction.</article>","contentLength":1300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation","url":"https://arxiv.org/abs/2502.08941","date":1739768400,"author":"","guid":955,"unread":true,"content":"<article>arXiv:2502.08941v2 Announce Type: replace \nAbstract: This paper analyzes multi-step temporal difference (TD)-learning algorithms within the ``deadly triad'' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that $n$-step TD-learning algorithms converge to a solution as the sampling horizon $n$ increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when $n$ is sufficiently large. Based on these findings, in the second part, two $n$-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free reinforcement learning counterparts of the model-based deterministic algorithms.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Siren Song: Manipulating Pose Estimation in XR Headsets Using Acoustic Attacks","url":"https://arxiv.org/abs/2502.08865","date":1739768400,"author":"","guid":956,"unread":true,"content":"<article>arXiv:2502.08865v2 Announce Type: replace \nAbstract: Extended Reality (XR) experiences involve interactions between users, the real world, and virtual content. A key step to enable these experiences is the XR headset sensing and estimating the user's pose in order to accurately place and render virtual content in the real world. XR headsets use multiple sensors (e.g., cameras, inertial measurement unit) to perform pose estimation and improve its robustness, but this provides an attack surface for adversaries to interfere with the pose estimation process. In this paper, we create and study the effects of acoustic attacks that create false signals in the inertial measurement unit (IMU) on XR headsets, leading to adverse downstream effects on XR applications. We generate resonant acoustic signals on a HoloLens 2 and measure the resulting perturbations in the IMU readings, and also demonstrate both fine-grained and coarse attacks on the popular ORB-SLAM3 and an open-source XR system (ILLIXR). With the knowledge gleaned from attacking these open-source frameworks, we demonstrate four end-to-end proof-of-concept attacks on a HoloLens 2: manipulating user input, clickjacking, zone invasion, and denial of user interaction. Our experiments show that current commercial XR headsets are susceptible to acoustic attacks, raising concerns for their security.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges","url":"https://arxiv.org/abs/2502.08859","date":1739768400,"author":"","guid":957,"unread":true,"content":"<article>arXiv:2502.08859v2 Announce Type: replace \nAbstract: As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity -- each typically requiring teams of skilled solvers hours to days to complete -- with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Dataset Size for Recommender Systems: Evaluating Algorithms' Performance via Downsampling","url":"https://arxiv.org/abs/2502.08845","date":1739768400,"author":"","guid":958,"unread":true,"content":"<article>arXiv:2502.08845v2 Announce Type: replace \nAbstract: This thesis investigates dataset downsampling as a strategy to optimize energy efficiency in recommender systems while maintaining competitive performance. With increasing dataset sizes posing computational and environmental challenges, this study explores the trade-offs between energy efficiency and recommendation quality in Green Recommender Systems, which aim to reduce environmental impact. By applying two downsampling approaches to seven datasets, 12 algorithms, and two levels of core pruning, the research demonstrates significant reductions in runtime and carbon emissions. For example, a 30% downsampling portion can reduce runtime by 52% compared to the full dataset, leading to a carbon emission reduction of up to 51.02 KgCO2e during the training of a single algorithm on a single dataset. The analysis reveals that algorithm performance under different downsampling portions depends on factors like dataset characteristics, algorithm complexity, and the specific downsampling configuration (scenario dependent). Some algorithms, which showed lower nDCG@10 scores compared to higher-performing ones, exhibited lower sensitivity to the amount of training data, offering greater potential for efficiency in lower downsampling portions. On average, these algorithms retained 81% of full-size performance using only 50% of the training set. In certain downsampling configurations, where more users were progressively included while keeping the test set size fixed, they even showed higher nDCG@10 scores than when using the full dataset. These findings highlight the feasibility of balancing sustainability and effectiveness, providing insights for designing energy-efficient recommender systems and promoting sustainable AI practices.</article>","contentLength":1799,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Acoustic Wave Manipulation Through Sparse Robotic Actuation","url":"https://arxiv.org/abs/2502.08784","date":1739768400,"author":"","guid":959,"unread":true,"content":"<article>arXiv:2502.08784v2 Announce Type: replace \nAbstract: Recent advancements in robotics, control, and machine learning have facilitated progress in the challenging area of object manipulation. These advancements include, among others, the use of deep neural networks to represent dynamics that are partially observed by robot sensors, as well as effective control using sparse control signals. In this work, we explore a more general problem: the manipulation of acoustic waves, which are partially observed by a robot capable of influencing the waves through spatially sparse actuators. This problem holds great potential for the design of new artificial materials, ultrasonic cutting tools, energy harvesting, and other applications. We develop an efficient data-driven method for robot learning that is applicable to either focusing scattered acoustic energy in a designated region or suppressing it, depending on the desired task. The proposed method is better in terms of a solution quality and computational complexity as compared to a state-of-the-art learning based method for manipulation of dynamical systems governed by partial differential equations. Furthermore our proposed method is competitive with a classical semi-analytical method in acoustics research on the demonstrated tasks. We have made the project code publicly available, along with a web page featuring video demonstrations: https://gladisor.github.io/waves/.</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deployment-friendly Lane-changing Intention Prediction Powered by Brain-inspired Spiking Neural Networks","url":"https://arxiv.org/abs/2502.08659","date":1739768400,"author":"","guid":960,"unread":true,"content":"<article>arXiv:2502.08659v2 Announce Type: replace \nAbstract: Accurate and real-time prediction of surrounding vehicles' lane-changing intentions is a critical challenge in deploying safe and efficient autonomous driving systems in open-world scenarios. Existing high-performing methods remain hard to deploy due to their high computational cost, long training times, and excessive memory requirements. Here, we propose an efficient lane-changing intention prediction approach based on brain-inspired Spiking Neural Networks (SNN). By leveraging the event-driven nature of SNN, the proposed approach enables us to encode the vehicle's states in a more efficient manner. Comparison experiments conducted on HighD and NGSIM datasets demonstrate that our method significantly improves training efficiency and reduces deployment costs while maintaining comparable prediction accuracy. Particularly, compared to the baseline, our approach reduces training time by 75% and memory usage by 99.9%. These results validate the efficiency and reliability of our method in lane-changing predictions, highlighting its potential for safe and efficient autonomous driving systems while offering significant advantages in deployment, including reduced training time, lower memory usage, and faster inference.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and learning in neural networks","url":"https://arxiv.org/abs/2502.08644","date":1739768400,"author":"","guid":961,"unread":true,"content":"<article>arXiv:2502.08644v3 Announce Type: replace \nAbstract: The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic that artificial intelligence algorithms have struggled to mimic. Inspired by oscillatory rhythms of the mechanical structures of neural cells, we developed a learning paradigm that is based on oscillations in link strengths and associates learning with the coordination of these oscillations. We find that this paradigm yields rapid adaptation and learning in artificial neural networks. Link oscillations can rapidly change coordination, endowing the network with the ability to sense subtle context changes in an unsupervised manner. In other words, the network generates the missing contextual tokens required to perform as a generalist AI architecture capable of predicting dynamics in multiple contexts. Oscillations also allow the network to extrapolate dynamics to never-seen-before contexts. These capabilities make our learning paradigm a powerful starting point for novel models of learning and cognition. Furthermore, learning through link coordination is agnostic to the specifics of the neural network architecture, hence our study opens the door for introducing rapid adaptation and learning capabilities into leading AI models.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LucidAtlas$: Learning Uncertainty-Aware, Covariate-Disentangled, Individualized Atlas Representations","url":"https://arxiv.org/abs/2502.08445","date":1739768400,"author":"","guid":962,"unread":true,"content":"<article>arXiv:2502.08445v2 Announce Type: replace \nAbstract: The goal of this work is to develop principled techniques to extract information from high dimensional data sets with complex dependencies in areas such as medicine that can provide insight into individual as well as population level variation. We develop $\\texttt{LucidAtlas}$, an approach that can represent spatially varying information, and can capture the influence of covariates as well as population uncertainty. As a versatile atlas representation, $\\texttt{LucidAtlas}$ offers robust capabilities for covariate interpretation, individualized prediction, population trend analysis, and uncertainty estimation, with the flexibility to incorporate prior knowledge. Additionally, we discuss the trustworthiness and potential risks of neural additive models for analyzing dependent covariates and then introduce a marginalization approach to explain the dependence of an individual predictor on the models' response (the atlas). To validate our method, we demonstrate its generalizability on two medical datasets. Our findings underscore the critical role of by-construction interpretable models in advancing scientific discovery. Our code will be publicly available upon acceptance.</article>","contentLength":1240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","url":"https://arxiv.org/abs/2502.08346","date":1739768400,"author":"","guid":963,"unread":true,"content":"<article>arXiv:2502.08346v2 Announce Type: replace \nAbstract: Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.</article>","contentLength":1129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion Rate Modeling","url":"https://arxiv.org/abs/2502.08277","date":1739768400,"author":"","guid":964,"unread":true,"content":"<article>arXiv:2502.08277v2 Announce Type: replace \nAbstract: Post-click conversion rate (CVR) estimation is a vital task in many recommender systems of revenue businesses, e.g., e-commerce and advertising. In a perspective of sample, a typical CVR positive sample usually goes through a funnel of exposure to click to conversion. For lack of post-event labels for un-clicked samples, CVR learning task commonly only utilizes clicked samples, rather than all exposed samples as for click-through rate (CTR) learning task. However, during online inference, CVR and CTR are estimated on the same assumed exposure space, which leads to a inconsistency of sample space between training and inference, i.e., sample selection bias (SSB). To alleviate SSB, previous wisdom proposes to design novel auxiliary tasks to enable the CVR learning on un-click training samples, such as CTCVR and counterfactual CVR, etc. Although alleviating SSB to some extent, none of them pay attention to the discrimination between ambiguous negative samples (un-clicked) and factual negative samples (clicked but un-converted) during modelling, which makes CVR model lacks robustness. To full this gap, we propose a novel ChorusCVR model to realize debiased CVR learning in entire-space.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents","url":"https://arxiv.org/abs/2502.08226","date":1739768400,"author":"","guid":965,"unread":true,"content":"<article>arXiv:2502.08226v2 Announce Type: replace \nAbstract: Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension.</article>","contentLength":1608,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In-Context Learning of Linear Dynamical Systems with Transformers: Error Bounds and Depth-Separation","url":"https://arxiv.org/abs/2502.08136","date":1739768400,"author":"","guid":966,"unread":true,"content":"<article>arXiv:2502.08136v2 Announce Type: replace \nAbstract: This paper investigates approximation-theoretic aspects of the in-context learning capability of the transformers in representing a family of noisy linear dynamical systems. Our first theoretical result establishes an upper bound on the approximation error of multi-layer transformers with respect to an $L^2$-testing loss uniformly defined across tasks. This result demonstrates that transformers with logarithmic depth can achieve error bounds comparable with those of the least-squares estimator. In contrast, our second result establishes a non-diminishing lower bound on the approximation error for a class of single-layer linear transformers, which suggests a depth-separation phenomenon for transformers in the in-context learning of dynamical systems. Moreover, this second result uncovers a critical distinction in the approximation power of single-layer linear transformers when learning from IID versus non-IID data.</article>","contentLength":980,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping","url":"https://arxiv.org/abs/2502.08054","date":1739768400,"author":"","guid":967,"unread":true,"content":"<article>arXiv:2502.08054v2 Announce Type: replace \nAbstract: This paper addresses the challenge of occluded robot grasping, i.e. grasping in situations where the desired grasp poses are kinematically infeasible due to environmental constraints such as surface collisions. Traditional robot manipulation approaches struggle with the complexity of non-prehensile or bimanual strategies commonly used by humans in these circumstances. State-of-the-art reinforcement learning (RL) methods are unsuitable due to the inherent complexity of the task. In contrast, learning from demonstration requires collecting a significant number of expert demonstrations, which is often infeasible. Instead, inspired by human bimanual manipulation strategies, where two hands coordinate to stabilise and reorient objects, we focus on a bimanual robotic setup to tackle this challenge. In particular, we introduce Constraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), a learning-based approach which leverages two coordinated policies: a constraint policy trained using self-supervised datasets to generate stabilising poses and a grasping policy trained using RL that reorients and grasps the target object. A key contribution lies in value function-guided policy coordination. Specifically, during RL training for the grasping policy, the constraint policy's output is refined through gradients from a jointly trained value function, improving bimanual coordination and task performance. Lastly, COMBO-Grasp employs teacher-student policy distillation to effectively deploy point cloud-based policies in real-world environments. Empirical evaluations demonstrate that COMBO-Grasp significantly improves task success rates compared to competitive baseline approaches, with successful generalisation to unseen objects in both simulated and real-world environments.</article>","contentLength":1857,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models","url":"https://arxiv.org/abs/2502.08008","date":1739768400,"author":"","guid":968,"unread":true,"content":"<article>arXiv:2502.08008v2 Announce Type: replace \nAbstract: Federated learning (FL) enhances privacy by keeping user data on local devices. However, emerging attacks have demonstrated that the updates shared by users during training can reveal significant information about their data. This has greatly thwart the adoption of FL methods for training robust AI models in sensitive applications. Differential Privacy (DP) is considered the gold standard for safeguarding user data. However, DP guarantees are highly conservative, providing worst-case privacy guarantees. This can result in overestimating privacy needs, which may compromise the model's accuracy. Additionally, interpretations of these privacy guarantees have proven to be challenging in different contexts. This is further exacerbated when other factors, such as the number of training iterations, data distribution, and specific application requirements, can add further complexity to this problem. In this work, we proposed a framework that integrates a human entity as a privacy practitioner to determine an optimal trade-off between the model's privacy and utility. Our framework is the first to address the variable memory requirement of existing DP methods in FL settings, where resource-limited devices (e.g., cell phones) can participate. To support such settings, we adopt a recent DP method with fixed memory usage to ensure scalable private FL. We evaluated our proposed framework by fine-tuning a BERT-based LLM model using the GLUE dataset (a common approach in literature), leveraging the new accountant, and employing diverse data partitioning strategies to mimic real-world conditions. As a result, we achieved stable memory usage, with an average accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon = 6$, when compared to the state-of-the-art DP accountant which does not support fixed memory usage.</article>","contentLength":1893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New tools for comparing classical and neural ODE models for tumor growth","url":"https://arxiv.org/abs/2502.07964","date":1739768400,"author":"","guid":969,"unread":true,"content":"<article>arXiv:2502.07964v2 Announce Type: replace \nAbstract: A new computational tool TumorGrowth$.$jl for modeling tumor growth is introduced. The tool allows the comparison of standard textbook models, such as General Bertalanffy and Gompertz, with some newer models, including, for the first time, neural ODE models. As an application, we revisit a human meta-study of non-small cell lung cancer and bladder cancer lesions, in patients undergoing two different treatment options, to determine if previously reported performance differences are statistically significant, and if newer, more complex models perform any better. In a population of examples with at least four time-volume measurements available for calibration, and an average of about 6.3, our main conclusion is that the General Bertalanffy model has superior performance, on average. However, where more measurements are available, we argue that more complex models, capable of capturing rebound and relapse behavior, may be better choices.</article>","contentLength":1000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implicit Language Models are RNNs: Balancing Parallelization and Expressivity","url":"https://arxiv.org/abs/2502.07827","date":1739768400,"author":"","guid":970,"unread":true,"content":"<article>arXiv:2502.07827v2 Announce Type: replace \nAbstract: State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.</article>","contentLength":1291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Simulation-Based Framework for Leveraging Shared Autonomous Vehicles to Enhance Disaster Evacuations in Rural Regions with a Focus on Vulnerable Populations","url":"https://arxiv.org/abs/2502.07787","date":1739768400,"author":"","guid":971,"unread":true,"content":"<article>arXiv:2502.07787v2 Announce Type: replace \nAbstract: Rapid advancements in autonomous vehicles (AVs) are poised to revolutionize transportation and communities, including disaster evacuations, particularly through the deployment of Shared Autonomous Vehicles (SAVs). Despite the potential, the use of SAVs in rural disaster evacuations remains an underexplored area. To address this gap, this study proposes a simulation-based framework that integrates both mathematical programming and SUMO traffic simulation to deploy SAVs in pre- and post-disaster evacuations in rural areas. The framework prioritizes the needs of vulnerable groups, including individuals with disabilities, limited English proficiency, and elderly residents. Sumter County, Florida, serves as the case study due to its unique characteristics: a high concentration of vulnerable individuals and limited access to public transportation, making it one of the most transportation-insecure counties in the state. These conditions present significant challenges for evacuation planning in the region. To explore potential solutions, we conducted mass evacuation simulations by incorporating SAVs across seven scenarios. These scenarios represented varying SAV penetration levels, ranging from 20% to 100% of the vulnerable population, and were compared to a baseline scenario using only passenger cars. Additionally, we examined both pre-disaster and post-disaster conditions, accounting for infrastructure failures and road closures. According to the simulation results, higher SAV integration significantly improves traffic distribution and reduces congestion. Scenarios featuring more SAVs exhibited lower congestion peaks and more stable traffic flow. Conversely, mixed traffic environments demonstrate reduced average speeds attributable to interactions between SAVs and passenger cars, while exclusive use of SAVs results in higher speeds and more stable travel patterns.</article>","contentLength":1943,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EIQP: Execution-time-certified and Infeasibility-detecting QP Solver","url":"https://arxiv.org/abs/2502.07738","date":1739768400,"author":"","guid":972,"unread":true,"content":"<article>arXiv:2502.07738v2 Announce Type: replace \nAbstract: Solving real-time quadratic programming (QP) is a ubiquitous task in control engineering, such as in model predictive control and control barrier function-based QP. In such real-time scenarios, certifying that the employed QP algorithm can either return a solution within a predefined level of optimality or detect QP infeasibility before the predefined sampling time is a pressing requirement. This article considers convex QP (including linear programming) and adopts its homogeneous formulation to achieve infeasibility detection. Exploiting this homogeneous formulation, this article proposes a novel infeasible interior-point method (IPM) algorithm with the best theoretical $O(\\sqrt{n})$ iteration complexity that feasible IPM algorithms enjoy. The iteration complexity is proved to be \\textit{exact} (rather than an upper bound), \\textit{simple to calculate}, and \\textit{data independent}, with the value $\\left\\lceil\\frac{\\log(\\frac{n+1}{\\epsilon})}{-\\log(1-\\frac{0.414213}{\\sqrt{n+1}})}\\right\\rceil$ (where $n$ and $\\epsilon$ denote the number of constraints and the predefined optimality level, respectively), making it appealing to certify the execution time of online time-varying convex QPs. The proposed algorithm is simple to implement without requiring a line search procedure (uses the full Newton step), and its C-code implementation (offering MATLAB, Julia, and Python interfaces) and numerical examples are publicly available at https://github.com/liangwu2019/EIQP.</article>","contentLength":1539,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Magic 1-For-1: Generating One Minute Video Clips within One Minute","url":"https://arxiv.org/abs/2502.07701","date":1739768400,"author":"","guid":973,"unread":true,"content":"<article>arXiv:2502.07701v2 Announce Type: replace \nAbstract: In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.</article>","contentLength":1569,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving","url":"https://arxiv.org/abs/2502.07640","date":1739768400,"author":"","guid":974,"unread":true,"content":"<article>arXiv:2502.07640v2 Announce Type: replace \nAbstract: We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. Despite using only supervised fine-tuning, our final prover significantly outperforms the previous best open-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning. On the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32), surpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.</article>","contentLength":1454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bag of Tricks for Inference-time Computation of LLM Reasoning","url":"https://arxiv.org/abs/2502.07191","date":1739768400,"author":"","guid":975,"unread":true,"content":"<article>arXiv:2502.07191v3 Announce Type: replace \nAbstract: With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at https://github.com/usail-hkust/benchmark_inference_time_computation_LL</article>","contentLength":1892,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero-Knowledge Proof Frameworks: A Systematic Survey","url":"https://arxiv.org/abs/2502.07063","date":1739768400,"author":"","guid":976,"unread":true,"content":"<article>arXiv:2502.07063v2 Announce Type: replace \nAbstract: Zero-Knowledge Proofs (ZKPs) are a cryptographic primitive that allows a prover to demonstrate knowledge of a secret value to a verifier without revealing anything about the secret itself. ZKPs have shown to be an extremely powerful tool, as evidenced in both industry and academic settings. In recent years, the utilization of user data in practical applications has necessitated the rapid development of privacy-preserving techniques, including ZKPs. This has led to the creation of several robust open-source ZKP frameworks. However, there remains a significant gap in understanding the capabilities and real-world applications of these frameworks. Furthermore, identifying the most suitable frameworks for the developers' specific applications and settings is a challenge, given the variety of options available. The primary goal of our work is to lower the barrier to entry for understanding and building applications with open-source ZKP frameworks.\n  In this work, we survey and evaluate 25 general-purpose, prominent ZKP frameworks. Recognizing that ZKPs have various constructions and underlying arithmetic schemes, our survey aims to provide a comprehensive overview of the ZKP landscape. These systems are assessed based on their usability and performance in SHA-256 and matrix multiplication experiments. Acknowledging that setting up a functional development environment can be challenging for these frameworks, we offer a fully open-source collection of Docker containers. These containers include a working development environment and are accompanied by documented code from our experiments. We conclude our work with a thorough analysis of the practical applications of ZKPs, recommendations for ZKP settings in different application scenarios, and a discussion on the future development of ZKP frameworks.</article>","contentLength":1875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion","url":"https://arxiv.org/abs/2502.06860","date":1739768400,"author":"","guid":977,"unread":true,"content":"<article>arXiv:2502.06860v2 Announce Type: replace \nAbstract: The ability to automatically complete a partial sketch that depicts a complex scene, e.g., \"a woman chatting with a man in the park\", is very useful. However, existing sketch generation methods create sketches from scratch; they do not complete a partial sketch in the style of the original. To address this challenge, we introduce AutoSketch, a styleaware vector sketch completion method that accommodates diverse sketch styles. Our key observation is that the style descriptions of a sketch in natural language preserve the style during automatic sketch completion. Thus, we use a pretrained vision-language model (VLM) to describe the styles of the partial sketches in natural language and replicate these styles using newly generated strokes. We initially optimize the strokes to match an input prompt augmented by style descriptions extracted from the VLM. Such descriptions allow the method to establish a diffusion prior in close alignment with that of the partial sketch. Next, we utilize the VLM to generate an executable style adjustment code that adjusts the strokes to conform to the desired style. We compare our method with existing methods across various sketch styles and prompts, performed extensive ablation studies and qualitative and quantitative evaluations, and demonstrate that AutoSketch can support various sketch scenarios.</article>","contentLength":1402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","url":"https://arxiv.org/abs/2502.06648","date":1739768400,"author":"","guid":978,"unread":true,"content":"<article>arXiv:2502.06648v2 Announce Type: replace \nAbstract: In this paper, we introduce a dataset of multilingual news articles covering the 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from 1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and published between July 1, 2021, and August 14, 2021. These articles are written in nine languages from different language families and in different scripts. To create the dataset, the raw news articles were first retrieved via a service that collects and analyzes news articles. Then, the articles were grouped using an online clustering algorithm, with each group containing articles reporting on the same sub-event. Finally, the groups were manually annotated and evaluated. The development of this dataset aims to provide a resource for evaluating the performance of multilingual news clustering algorithms, for which limited datasets are available. It can also be used to analyze the dynamics and events of the 2021 Tokyo Olympics from different perspectives. The dataset is available in CSV format and can be accessed from the CLARIN.SI repository.</article>","contentLength":1139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions","url":"https://arxiv.org/abs/2502.06309","date":1739768400,"author":"","guid":979,"unread":true,"content":"<article>arXiv:2502.06309v2 Announce Type: replace \nAbstract: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. Among all the physical properties of resistive elements, the response to the pulses directly affects the training dynamics. This paper first provides a theoretical foundation for gradient-based training on AIMC hardware and studies the impact of response functions. We demonstrate that noisy update and asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty term on the objective. To overcome the issue, Tiki-Taka, a residual learning algorithm, converges exactly to a critical point by optimizing a main array and a residual array bilevelly. The conclusion is supported by simulations validating our theoretical insights.</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Occupancy-SLAM: An Efficient and Robust Algorithm for Simultaneously Optimizing Robot Poses and Occupancy Map","url":"https://arxiv.org/abs/2502.06292","date":1739768400,"author":"","guid":980,"unread":true,"content":"<article>arXiv:2502.06292v2 Announce Type: replace \nAbstract: Joint optimization of poses and features has been extensively studied and demonstrated to yield more accurate results in feature-based SLAM problems. However, research on jointly optimizing poses and non-feature-based maps remains limited. Occupancy maps are widely used non-feature-based environment representations because they effectively classify spaces into obstacles, free areas, and unknown regions, providing robots with spatial information for various tasks. In this paper, we propose Occupancy-SLAM, a novel optimization-based SLAM method that enables the joint optimization of robot trajectory and the occupancy map through a parameterized map representation. The key novelty lies in optimizing both robot poses and occupancy values at different cell vertices simultaneously, a significant departure from existing methods where the robot poses need to be optimized first before the map can be estimated. Evaluations using simulations and practical 2D laser datasets demonstrate that the proposed approach can robustly obtain more accurate robot trajectories and occupancy maps than state-of-the-art techniques with comparable computational time. Preliminary results in the 3D case further confirm the potential of the proposed method in practical 3D applications, achieving more accurate results than existing methods.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Low Tensor-Rank Adaptation of Kolmogorov--Arnold Networks","url":"https://arxiv.org/abs/2502.06153","date":1739768400,"author":"","guid":981,"unread":true,"content":"<article>arXiv:2502.06153v2 Announce Type: replace \nAbstract: Kolmogorov--Arnold networks (KANs) have demonstrated their potential as an alternative to multi-layer perceptions (MLPs) in various domains, especially for science-related tasks. However, transfer learning of KANs remains a relatively unexplored area. In this paper, inspired by Tucker decomposition of tensors and evidence on the low tensor-rank structure in KAN parameter updates, we develop low tensor-rank adaptation (LoTRA) for fine-tuning KANs. We study the expressiveness of LoTRA based on Tucker decomposition approximations. Furthermore, we provide a theoretical analysis to select the learning rates for each LoTRA component to enable efficient training. Our analysis also shows that using identical learning rates across all components leads to inefficient training, highlighting the need for an adaptive learning rate strategy. Beyond theoretical insights, we explore the application of LoTRA for efficiently solving various partial differential equations (PDEs) by fine-tuning KANs. Additionally, we propose Slim KANs that incorporate the inherent low-tensor-rank properties of KAN parameter tensors to reduce model size while maintaining superior performance. Experimental results validate the efficacy of the proposed learning rate selection strategy and demonstrate the effectiveness of LoTRA for transfer learning of KANs in solving PDEs. Further evaluations on Slim KANs for function representation and image classification tasks highlight the expressiveness of LoTRA and the potential for parameter reduction through low tensor-rank decomposition.</article>","contentLength":1619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sink-free orientations: a local sampler with applications","url":"https://arxiv.org/abs/2502.05877","date":1739768400,"author":"","guid":982,"unread":true,"content":"<article>arXiv:2502.05877v2 Announce Type: replace \nAbstract: For sink-free orientations in graphs of minimum degree at least $3$, we show that there is a deterministic approximate counting algorithm that runs in time $O((n^{73}/\\varepsilon^{72})\\log(n/\\varepsilon))$, a near-linear time sampling algorithm, and a randomised approximate counting algorithm that runs in time $O((n/\\varepsilon)^2\\log(n/\\varepsilon))$, where $n$ denotes the number of vertices of the input graph and $0&lt;\\varepsilon&lt;1$ is the desired accuracy. All three algorithms are based on a local implementation of the sink popping method (Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling framework (Guo, Jerrum, and Liu, 2019).</article>","contentLength":709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Federated Learning with Reservoir State Analysis for Time Series Anomaly Detection","url":"https://arxiv.org/abs/2502.05679","date":1739768400,"author":"","guid":983,"unread":true,"content":"<article>arXiv:2502.05679v2 Announce Type: replace \nAbstract: With a growing data privacy concern, federated learning has emerged as a promising framework to train machine learning models without sharing locally distributed data. In federated learning, local model training by multiple clients and model integration by a server are repeated only through model parameter sharing. Most existing federated learning methods assume training deep learning models, which are often computationally demanding. To deal with this issue, we propose federated learning methods with reservoir state analysis to seek computational efficiency and data privacy protection simultaneously. Specifically, our method relies on Mahalanobis Distance of Reservoir States (MD-RS) method targeting time series anomaly detection, which learns a distribution of reservoir states for normal inputs and detects anomalies based on a deviation from the learned distribution. Iterative updating of statistical parameters in the MD-RS enables incremental federated learning (IncFed MD-RS). We evaluate the performance of IncFed MD-RS using benchmark datasets for time series anomaly detection. The results show that IncFed MD-RS outperforms other federated learning methods with deep learning and reservoir computing models particularly when clients' data are relatively short and heterogeneous. We demonstrate that IncFed MD-RS is robust against reduced sample data compared to other methods. We also show that the computational cost of IncFed MD-RS can be reduced by subsampling from the reservoir states without performance degradation. The proposed method is beneficial especially in anomaly detection applications where computational efficiency, algorithm simplicity, and low communication cost are required.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion","url":"https://arxiv.org/abs/2502.05606","date":1739768400,"author":"","guid":984,"unread":true,"content":"<article>arXiv:2502.05606v2 Announce Type: replace \nAbstract: Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two-Player Zero-Sum Differential Games with One-Sided Information","url":"https://arxiv.org/abs/2502.05314","date":1739768400,"author":"","guid":985,"unread":true,"content":"<article>arXiv:2502.05314v2 Announce Type: replace \nAbstract: Unlike Poker where the action space $\\mathcal{A}$ is discrete, differential games in the physical world often have continuous action spaces not amenable to discrete abstraction, rendering no-regret algorithms with $\\mathcal{O}(|\\mathcal{A}|)$ complexity not scalable. To address this challenge within the scope of two-player zero-sum (2p0s) games with one-sided information, we show that (1) a computational complexity independent of $|\\mathcal{A}|$ can be achieved by exploiting the convexification property of incomplete-information games and the Isaacs' condition that commonly holds for dynamical systems, and that (2) the computation of the two equilibrium strategies can be decoupled under one-sidedness of information. Leveraging these insights, we develop an algorithm that successfully approximates the optimal strategy in a homing game. Code available in https://github.com/ghimiremukesh/cams/tree/workshop</article>","contentLength":969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning","url":"https://arxiv.org/abs/2502.04890","date":1739768400,"author":"","guid":986,"unread":true,"content":"<article>arXiv:2502.04890v2 Announce Type: replace \nAbstract: Federated Learning (FL) is notorious for its vulnerability to Byzantine attacks. Most current Byzantine defenses share a common inductive bias: among all the gradients, the densely distributed ones are more likely to be honest. However, such a bias is a poison to Byzantine robustness due to a newly discovered phenomenon in this paper - gradient skew. We discover that a group of densely distributed honest gradients skew away from the optimal gradient (the average of honest gradients) due to heterogeneous data. This gradient skew phenomenon allows Byzantine gradients to hide within the densely distributed skewed gradients. As a result, Byzantine defenses are confused into believing that Byzantine gradients are honest. Motivated by this observation, we propose a novel skew-aware attack called STRIKE: first, we search for the skewed gradients; then, we construct Byzantine gradients within the skewed gradients. Experiments on three benchmark datasets validate the effectiveness of our attack</article>","contentLength":1053,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tight Bounds for Noisy Computation of High-Influence Functions, Connectivity, and Threshold","url":"https://arxiv.org/abs/2502.04632","date":1739768400,"author":"","guid":987,"unread":true,"content":"<article>arXiv:2502.04632v2 Announce Type: replace \nAbstract: In the noisy query model, the (binary) return value of every query (possibly repeated) is independently flipped with some fixed probability $p \\in (0, 1/2)$. In this paper, we obtain tight bounds on the noisy query complexity of several fundamental problems.\n  Our first contribution is to show that any Boolean function with total influence $\\Omega(n)$ has noisy query complexity $\\Theta(n\\log n)$. Previous works often focus on specific problems, and it is of great interest to have a characterization of noisy query complexity for general functions. Our result is the first noisy query complexity lower bound of this generality, beyond what was known for random Boolean functions [Reischuk and Schmeltz, FOCS 1991].\n  Our second contribution is to prove that Graph Connectivity has noisy query complexity $\\Theta(n^2 \\log n)$. In this problem, the goal is to determine whether an undirected graph is connected using noisy edge queries. While the upper bound can be achieved by a simple algorithm, no non-trivial lower bounds were known prior to this work.\n  Last but not least, we determine the exact number of noisy queries (up to lower order terms) needed to solve the $k$-Threshold problem and the Counting problem. The $k$-Threshold problem asks to decide whether there are at least $k$ ones among $n$ bits, given noisy query access to the bits. We prove that $(1\\pm o(1)) \\frac{n\\log (\\min\\{k,n-k+1\\}/\\delta)}{(1-2p)\\log \\frac{1-p}p}$ queries are both sufficient and necessary to achieve error probability $\\delta = o(1)$. Previously, such a result was only known when $\\min\\{k,n-k+1\\}=o(n)$ [Wang, Ghaddar, Zhu and Wang, arXiv 2024]. We also show a similar $(1\\pm o(1)) \\frac{n\\log (\\min\\{k+1,n-k+1\\}/\\delta)}{(1-2p)\\log \\frac{1-p}p}$ bound for the Counting problem, where one needs to count the number of ones among $n$ bits given noisy query access and $k$ denotes the answer.</article>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt-based Depth Pruning of Large Language Models","url":"https://arxiv.org/abs/2502.04348","date":1739768400,"author":"","guid":988,"unread":true,"content":"<article>arXiv:2502.04348v2 Announce Type: replace \nAbstract: Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a transformer block may be highly task-dependent -- a block that is crucial for a task can be removed without degrading the accuracy on another task. Based on this observation, we develop a dynamic depth pruning algorithm, coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which blocks to omit from the model based on the input prompt. PuDDing operates by training a lightweight router to predict the best omission set among a set of options, where this option set has also been constructed in a data-driven manner. Empirical results on commonsense reasoning benchmarks demonstrate that PuDDing effectively accelerates the inference language models, and achieves better on-task performance than static depth pruning baselines.</article>","contentLength":1044,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs","url":"https://arxiv.org/abs/2502.03824","date":1739768400,"author":"","guid":989,"unread":true,"content":"<article>arXiv:2502.03824v3 Announce Type: replace \nAbstract: LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@$K$. The code is available at \\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.</article>","contentLength":1340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations","url":"https://arxiv.org/abs/2502.03752","date":1739768400,"author":"","guid":990,"unread":true,"content":"<article>arXiv:2502.03752v2 Announce Type: replace \nAbstract: Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, resulting in unstable skill learning and degraded performance. To overcome this, we propose Prioritized Refinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates exploration near noisy data to generate online trajectories and combines them with offline data. Through prioritization, PRISM extracts high-quality data to learn task-relevant skills effectively. By addressing the impact of noise, our method ensures stable skill learning and achieves superior performance in long-horizon tasks, even with noisy and sub-optimal data.</article>","contentLength":954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons","url":"https://arxiv.org/abs/2502.03391","date":1739768400,"author":"","guid":991,"unread":true,"content":"<article>arXiv:2502.03391v2 Announce Type: replace \nAbstract: *Minimal sufficient reasons* represent a prevalent form of explanation - the smallest subset of input features which, when held constant at their corresponding values, ensure that the prediction remains unchanged. Previous *post-hoc* methods attempt to obtain such explanations but face two main limitations: (1) Obtaining these subsets poses a computational challenge, leading most scalable methods to converge towards suboptimal, less meaningful subsets; (2) These methods heavily rely on sampling out-of-distribution input assignments, potentially resulting in counterintuitive behaviors. To tackle these limitations, we propose in this work a self-supervised training approach, which we term *sufficient subset training* (SST). Using SST, we train models to generate concise sufficient reasons for their predictions as an integral part of their output. Our results indicate that our framework produces succinct and faithful subsets substantially more efficiently than competing post-hoc methods, while maintaining comparable predictive performance.</article>","contentLength":1105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symmetry-Aware Bayesian Flow Networks for Crystal Generation","url":"https://arxiv.org/abs/2502.03146","date":1739768400,"author":"","guid":992,"unread":true,"content":"<article>arXiv:2502.03146v2 Announce Type: replace \nAbstract: The discovery of new crystalline materials is essential to scientific and technological progress. However, traditional trial-and-error approaches are inefficient due to the vast search space. Recent advancements in machine learning have enabled generative models to predict new stable materials by incorporating structural symmetries and to condition the generation on desired properties. In this work, we introduce SymmBFN, a novel symmetry-aware Bayesian Flow Network (BFN) for crystalline material generation that accurately reproduces the distribution of space groups found in experimentally observed crystals. SymmBFN substantially improves efficiency, generating stable structures at least 50 times faster than the next-best method. Furthermore, we demonstrate its capability for property-conditioned generation, enabling the design of materials with tailored properties. Our findings establish BFNs as an effective tool for accelerating the discovery of crystalline materials.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation Learning with Visual Observations","url":"https://arxiv.org/abs/2502.02867","date":1739768400,"author":"","guid":993,"unread":true,"content":"<article>arXiv:2502.02867v2 Announce Type: replace \nAbstract: Imitation learning (IL) enables agents to mimic expert behavior without reward signals but faces challenges in cross-domain scenarios with high-dimensional, noisy, and incomplete visual observations. To address this, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL), a novel IL method that extracts domain-invariant features from individual frames and adapts them into sequences to isolate and replicate expert behaviors. We also introduce a frame-wise time labeling technique to segment expert behaviors by timesteps and assign rewards aligned with temporal contexts, enhancing task performance. Experiments across diverse visual environments demonstrate the effectiveness of DIFF-IL in addressing complex visual tasks.</article>","contentLength":814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning","url":"https://arxiv.org/abs/2502.02844","date":1739768400,"author":"","guid":994,"unread":true,"content":"<article>arXiv:2502.02844v2 Announce Type: replace \nAbstract: Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL.</article>","contentLength":753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks","url":"https://arxiv.org/abs/2502.02834","date":1739768400,"author":"","guid":995,"unread":true,"content":"<article>arXiv:2502.02834v2 Announce Type: replace \nAbstract: Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments.</article>","contentLength":844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Relative Homology Theory of Representation in Neural Networks","url":"https://arxiv.org/abs/2502.01360","date":1739768400,"author":"","guid":996,"unread":true,"content":"<article>arXiv:2502.01360v2 Announce Type: replace \nAbstract: Previous research has proven that the set of maps implemented by neural networks with a ReLU activation function is identical to the set of piecewise linear continuous maps. Furthermore, such networks induce a hyperplane arrangement splitting the input domain into convex polyhedra $G_J$ over which the network $\\Phi$ operates in an affine manner.\n  In this work, we leverage these properties to define the equivalence class of inputs $\\sim_\\Phi$, which can be split into two sets related to the local rank of $\\Phi_J$ and the intersections $\\cap \\text{Im}\\Phi_{J_i}$. We refer to the latter as the overlap decomposition $O_\\Phi$ and prove that if the intersections between each polyhedron and the input manifold are convex, the homology groups of neural representations are isomorphic to relative homology groups $H_k(\\Phi(M)) \\simeq H_k(M,O_\\Phi)$. This lets us compute Betti numbers without the choice of an external metric. We develop methods to numerically compute the overlap decomposition through linear programming and a union-find algorithm.\n  Using this framework, we perform several experiments on toy datasets showing that, compared to standard persistent homology, our relative homology-based computation of Betti numbers tracks purely topological rather than geometric features. Finally, we study the evolution of the overlap decomposition during training on various classification problems while varying network width and depth and discuss some shortcomings of our method.</article>","contentLength":1540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"`Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs","url":"https://arxiv.org/abs/2502.00735","date":1739768400,"author":"","guid":997,"unread":true,"content":"<article>arXiv:2502.00735v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the state-of-the-art multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. Further, to better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flanking Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"S2CFormer: Reorienting Learned Image Compression from Spatial Interaction to Channel Aggregation","url":"https://arxiv.org/abs/2502.00700","date":1739768400,"author":"","guid":998,"unread":true,"content":"<article>arXiv:2502.00700v2 Announce Type: replace \nAbstract: Transformers have achieved significant success in learned image compression (LIC), with Swin Transformers emerging as the mainstream choice for nonlinear transforms. A common belief is that their sophisticated spatial operations contribute most to their efficacy. However, the crucial role of the feed-forward network (FFN) based Channel Aggregation module within the transformer architecture has been largely overlooked, and the over-design of spatial operations leads to a suboptimal trade-off between decoding latency and R-D performance. In this paper, we reevaluate the key factors behind the competence of transformers in LIC. By replacing spatial operations with identity mapping, we are surprised to find that channel operations alone can approach the R-D performance of the leading methods. This solid lower bound of performance emphasizes that the presence of channel aggregation is more essential for the LIC model to achieve competitive performance, while the previously complex spatial interactions are partly redundant. Based on this insight, we initiate the \"S2CFormer\" paradigm, a general architecture that reorients the focus of LIC from Spatial Interaction to Channel Aggregation. We present two instantiations of the S2CFormer: S2C-Conv, and S2C-Attention. Each one incorporates a simple operator for spatial interaction and serves as nonlinear transform blocks for our LIC models. Both models demonstrate state-of-the-art (SOTA) R-D performance and significantly faster decoding speed. These results also motivate further exploration of advanced FFN structures to enhance the R-D performance while maintaining model efficiency. With these foundations, we introduce S2C-Hybrid, an enhanced LIC model that combines the strengths of different S2CFormer instantiations. This model outperforms all the existing methods on several datasets, setting a new benchmark for efficient and high-performance LIC.</article>","contentLength":1971,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improved Community Detection using Stochastic Block Models","url":"https://arxiv.org/abs/2502.00686","date":1739768400,"author":"","guid":999,"unread":true,"content":"<article>arXiv:2502.00686v3 Announce Type: replace \nAbstract: Identifying edge-dense communities that are also well-connected is an important aspect of understanding community structure. Prior work has shown that community detection methods can produce poorly connected communities, and some can even produce internally disconnected communities. In this study we evaluate the connectivity of communities obtained using Stochastic Block Models. We find that SBMs produce internally disconnected communities from real-world networks. We present a simple technique, Well-Connected Clusters (WCC), which repeatedly removes small edge cuts until the communities meet a user-specified threshold for well-connectivity. Our study using a large collection of synthetic networks based on clustered real-world networks shows that using WCC as a post-processing tool with SBM community detection typically improves clustering accuracy. WCC is fast enough to use on networks with millions of nodes and is freely available in open source form.</article>","contentLength":1020,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction, and Column Exploration","url":"https://arxiv.org/abs/2502.00675","date":1739768400,"author":"","guid":1000,"unread":true,"content":"<article>arXiv:2502.00675v2 Announce Type: replace \nAbstract: Text-to-SQL systems have unlocked easier access to critical data insights by enabling natural language queries over structured databases. However, deploying such systems in enterprise environments remains challenging due to factors such as large, complex schemas (&gt; 3000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake) and sophisticated query requirements (e.g., transformation, analytics). Current state-of-the-art performance on the Spider 2.0 dataset -- a benchmark built to mimic such complex environments -- remains limited at 20%. Key limitations include inadequate instruction-following, poor long-context comprehension, weak self-refinement, and insufficient dialect-specific knowledge. To address these gaps, we propose ReFoRCE (Self-Refinement Agent with Format Restriction and Column Exploration) which introduces (1) table compression to mitigate long-context limitations (2) format restriction to ensure accurate answer format, and (3) iterative column exploration for enhanced schema understanding. Additionally, it employs self-refinement pipeline consisting of (1) parallelized workflows with voting mechanisms and (2) a Common Table Expression (CTE) based refinement approach to handle unresolved cases. ReFoRCE achieves state-of-the-art results scoring 31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks.</article>","contentLength":1411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Scalable Solver for 2p0s Differential Games with One-Sided Payoff Information and Continuous Actions, States, and Time","url":"https://arxiv.org/abs/2502.00560","date":1739768400,"author":"","guid":1001,"unread":true,"content":"<article>arXiv:2502.00560v2 Announce Type: replace \nAbstract: Existing solvers for imperfect-information extensive-form games (IIEFGs) often struggle with scalability in terms of action and state space sizes and the number of time steps. However, many real-world games involve continuous action and state spaces and occur in continuous time, making them differential in nature. This paper addresses the scalability challenges for a representative class of two-player zero-sum (2p0s) differential games where the informed player knows the game type (payoff) while the uninformed one only has a prior belief over the set of possible types. Such games encompass a wide range of attack-defense scenarios, where the defender adapts based on their belief about the attacker's target. We make the following contributions: (1) We show that under the Isaacs' condition, the complexity of computing the Nash equilibrium for these games is not related to the action space size; and (2) we propose a multigrid approach to effectively reduce the cost of these games when many time steps are involved. Code for this work is available at https://github.com/ghimiremukesh/cams/tree/conf_sub.</article>","contentLength":1166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited Communication","url":"https://arxiv.org/abs/2502.00558","date":1739768400,"author":"","guid":1002,"unread":true,"content":"<article>arXiv:2502.00558v2 Announce Type: replace \nAbstract: We consider the problem setting in which multiple autonomous agents must cooperatively navigate and perform tasks in an unknown, communication-constrained environment. Traditional multi-agent reinforcement learning (MARL) approaches assume synchronous communications and perform poorly in such environments. We propose AsynCoMARL, an asynchronous MARL approach that uses graph transformers to learn communication protocols from dynamic graphs. AsynCoMARL can accommodate infrequent and asynchronous communications between agents, with edges of the graph only forming when agents communicate with each other. We show that AsynCoMARL achieves similar success and collision rates as leading baselines, despite 26\\% fewer messages being passed between agents.</article>","contentLength":808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Line Balancing in the Modern Garment Industry","url":"https://arxiv.org/abs/2502.00455","date":1739768400,"author":"","guid":1003,"unread":true,"content":"<article>arXiv:2502.00455v2 Announce Type: replace \nAbstract: This article presents applied research on line balancing within the modern garment industry, focusing on the significant impact of intelligent hanger systems and hanger lines on the stitching process, by Lean Methodology for garment modernization. It explores the application of line balancing in the modern garment industry, focusing on the significant impact of intelligent hanger systems and hanger lines on the stitching process. It aligns with Lean Methodology principles for garment modernization. Without the implementation of line balancing technology, the garment manufacturing process using hanger systems cannot improve output rates. The case study demonstrates that implementing intelligent line balancing in a straightforward practical setup facilitates lean practices combined with a digitalization system and automaton. This approach illustrates how to enhance output and reduce accumulated work in progress.</article>","contentLength":976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Middleman Bias in Advertising: Aligning Relevance of Keyphrase Recommendations with Search","url":"https://arxiv.org/abs/2502.00131","date":1739768400,"author":"","guid":1004,"unread":true,"content":"<article>arXiv:2502.00131v2 Announce Type: replace \nAbstract: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). Keyphrases must be pertinent to items; otherwise, it can result in seller dissatisfaction and poor targeting -- towards that end relevance filters are employed. In this work, we describe the shortcomings of training relevance filter models on biased click/sales signals. We re-conceptualize advertiser keyphrase relevance as interaction between two dynamical systems -- Advertising which produces the keyphrases and Search which acts as a middleman to reach buyers. We discuss the bias of search relevance systems (middleman bias) and the need to align advertiser keyphrases with search relevance signals. We also compare the performance of cross encoders and bi-encoders in modeling this alignment and the scalability of such a solution for sellers at eBay.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Large Language Models to Enhance Machine Learning Interpretability and Predictive Performance: A Case Study on Emergency Department Returns for Mental Health Patients","url":"https://arxiv.org/abs/2502.00025","date":1739768400,"author":"","guid":1005,"unread":true,"content":"<article>arXiv:2502.00025v3 Announce Type: replace \nAbstract: Importance: Emergency department (ED) returns for mental health conditions pose a major healthcare burden, with 24-27% of patients returning within 30 days. Traditional machine learning models for predicting these returns often lack interpretability for clinical use.\n  Objective: To assess whether integrating large language models (LLMs) with machine learning improves predictive accuracy and clinical interpretability of ED mental health return risk models.\n  Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an academic medical center in the Deep South from January 2018 to December 2022.\n  Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30-day ED return prediction accuracy and (2) model interpretability using a novel LLM-enhanced framework integrating SHAP (SHapley Additive exPlanations) values with clinical knowledge.\n  Results: For chief complaint classification, LLaMA 3 (8B) with 10-shot learning outperformed traditional models (accuracy: 0.882, F1-score: 0.86). In SDoH classification, LLM-based models achieved 0.95 accuracy and 0.96 F1-score, with Alcohol, Tobacco, and Substance Abuse performing best (F1: 0.96-0.89), while Exercise and Home Environment showed lower performance (F1: 0.70-0.67). The LLM-based interpretability framework achieved 99% accuracy in translating model predictions into clinically relevant explanations. LLM-extracted features improved XGBoost AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61.\n  Conclusions and Relevance: Integrating LLMs with machine learning models yielded modest but consistent accuracy gains while significantly enhancing interpretability through automated, clinically relevant explanations. This approach provides a framework for translating predictive analytics into actionable clinical insights.</article>","contentLength":1904,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reward-Guided Speculative Decoding for Efficient LLM Reasoning","url":"https://arxiv.org/abs/2501.19324","date":1739768400,"author":"","guid":1006,"unread":true,"content":"<article>arXiv:2501.19324v2 Announce Type: replace \nAbstract: We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios. The code is available at https://github.com/BaohaoLiao/RSD.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Communication Framework for Compositional Generation","url":"https://arxiv.org/abs/2501.19182","date":1739768400,"author":"","guid":1007,"unread":true,"content":"<article>arXiv:2501.19182v2 Announce Type: replace \nAbstract: Compositionality and compositional generalization--the ability to understand novel combinations of known concepts--are central characteristics of human language and are hypothesized to be essential for human cognition. In machine learning, the emergence of this property has been studied in a communication game setting, where independent agents (a sender and a receiver) converge to a shared encoding policy from a set of states to a space of discrete messages, where the receiver can correctly reconstruct the states observed by the sender using only the sender's messages. The use of communication games in generation tasks is still largely unexplored, with recent methods for compositional generation focusing mainly on the use of supervised guidance (either through class labels or text). In this work, we take the first steps to fill this gap, and we present a self-supervised generative communication game-based framework for creating compositional encodings in learned representations from pre-trained encoder-decoder models. In an Iterated Learning (IL) protocol involving a sender and a receiver, we apply alternating pressures for compression and diversity of encoded discrete messages, so that the protocol converges to an efficient but unambiguous encoding. Approximate message entropy regularization is used to favor compositional encodings. Our framework is based on rigorous justifications and proofs of defining and balancing the concepts of Efficiency, Unambiguity and Non-Holisticity in encoding. We test our method on the compositional image dataset Shapes3D, demonstrating robust performance in both reconstruction and compositionality metrics, surpassing other tested discrete message frameworks.</article>","contentLength":1771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Neural Function Approximation: The XNet Outperforming KAN","url":"https://arxiv.org/abs/2501.18959","date":1739768400,"author":"","guid":1008,"unread":true,"content":"<article>arXiv:2501.18959v2 Announce Type: replace \nAbstract: XNet is a single-layer neural network architecture that leverages Cauchy integral-based activation functions for high-order function approximation. Through theoretical analysis, we show that the Cauchy activation functions used in XNet can achieve arbitrary-order polynomial convergence, fundamentally outperforming traditional MLPs and Kolmogorov-Arnold Networks (KANs) that rely on increased depth or B-spline activations. Our extensive experiments on function approximation, PDE solving, and reinforcement learning demonstrate XNet's superior performance - reducing approximation error by up to 50000 times and accelerating training by up to 10 times compared to existing approaches. These results establish XNet as a highly efficient architecture for both scientific computing and AI applications.</article>","contentLength":854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building Bridges, Not Walls -- Advancing Interpretability by Unifying Feature, Data, and Model Component Attribution","url":"https://arxiv.org/abs/2501.18887","date":1739768400,"author":"","guid":1009,"unread":true,"content":"<article>arXiv:2501.18887v2 Announce Type: replace \nAbstract: The increasing complexity of AI systems has made understanding their behavior a critical challenge. Numerous methods have been developed to attribute model behavior to three key aspects: input features, training data, and internal model components. However, these attribution methods are studied and applied rather independently, resulting in a fragmented landscape of approaches and terminology. This position paper argues that feature, data, and component attribution methods share fundamental similarities, and bridging them can benefit interpretability research. We conduct a detailed analysis of successful methods of these three attribution aspects and present a unified view to demonstrate that these seemingly distinct methods employ similar approaches, such as perturbations, gradients, and linear approximations, differing primarily in their perspectives rather than core techniques. Our unified perspective enhances understanding of existing attribution methods, identifies shared concepts and challenges, makes this field more accessible to newcomers, and highlights new directions not only for attribution and interpretability but also for broader AI research, including model editing, steering, and regulation.</article>","contentLength":1277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Echoes of Discord: Forecasting Hater Reactions to Counterspeech","url":"https://arxiv.org/abs/2501.16235","date":1739768400,"author":"","guid":1010,"unread":true,"content":"<article>arXiv:2501.16235v2 Announce Type: replace \nAbstract: Hate speech (HS) erodes the inclusiveness of online users and propagates negativity and division. Counterspeech has been recognized as a way to mitigate the harmful consequences. While some research has investigated the impact of user-generated counterspeech on social media platforms, few have examined and modeled haters' reactions toward counterspeech, despite the immediate alteration of haters' attitudes being an important aspect of counterspeech. This study fills the gap by analyzing the impact of counterspeech from the hater's perspective, focusing on whether the counterspeech leads the hater to reenter the conversation and if the reentry is hateful. We compile the Reddit Echoes of Hate dataset (ReEco), which consists of triple-turn conversations featuring haters' reactions, to assess the impact of counterspeech. To predict haters' behaviors, we employ two strategies: a two-stage reaction predictor and a three-way classifier. The linguistic analysis sheds insights on the language of counterspeech to hate eliciting different haters' reactions. Experimental results demonstrate that the 3-way classification model outperforms the two-stage reaction predictor, which first predicts reentry and then determines the reentry type. We conclude the study with an assessment showing the most common errors identified by the best-performing model.</article>","contentLength":1410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection","url":"https://arxiv.org/abs/2501.15451","date":1739768400,"author":"","guid":1011,"unread":true,"content":"<article>arXiv:2501.15451v2 Announce Type: replace \nAbstract: The proliferation of hate speech has caused significant harm to society. The intensity and directionality of hate are closely tied to the target and argument it is associated with. However, research on hate speech detection in Chinese has lagged behind, and existing datasets lack span-level fine-grained annotations. Furthermore, the lack of research on Chinese hateful slang poses a significant challenge. In this paper, we provide a solution for fine-grained detection of Chinese hate speech. First, we construct a dataset containing Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first span-level Chinese hate speech dataset. Secondly, we evaluate the span-level hate speech detection performance of existing models using STATE ToxiCN. Finally, we conduct the first study on Chinese hateful slang and evaluate the ability of LLMs to detect such expressions. Our work contributes valuable resources and insights to advance span-level hate speech detection in Chinese.</article>","contentLength":1046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models","url":"https://arxiv.org/abs/2501.15140","date":1739768400,"author":"","guid":1012,"unread":true,"content":"<article>arXiv:2501.15140v2 Announce Type: replace \nAbstract: Multi-modal large language models (MLLMs) have shown remarkable abilities in various visual understanding tasks. However, MLLMs still struggle with fine-grained visual recognition (FGVR), which aims to identify subordinate-level categories from images. This can negatively impact more advanced capabilities of MLLMs, such as object-centric visual question answering and reasoning. In our study, we revisit three quintessential capabilities of MLLMs for FGVR, including object information extraction, category knowledge reserve, object-category alignment, and position of the root cause as a misalignment problem. To address this issue, we present Finedefics, an MLLM that enhances the model's FGVR capability by incorporating informative attribute descriptions of objects into the training phase. We employ contrastive learning on object-attribute pairs and attribute-category pairs simultaneously and use examples from similar but incorrect categories as hard negatives, naturally bringing representations of visual objects and category names closer. Extensive evaluations across multiple popular FGVR datasets demonstrate that Finedefics outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy. The code is available at https://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Center Cooling System Optimization Using Offline Reinforcement Learning","url":"https://arxiv.org/abs/2501.15085","date":1739768400,"author":"","guid":1013,"unread":true,"content":"<article>arXiv:2501.15085v2 Announce Type: replace \nAbstract: The recent advances in information technology and artificial intelligence have fueled a rapid expansion of the data center (DC) industry worldwide, accompanied by an immense appetite for electricity to power the DCs. In a typical DC, around 30~40% of the energy is spent on the cooling system rather than on computer servers, posing a pressing need for developing new energy-saving optimization technologies for DC cooling systems. However, optimizing such real-world industrial systems faces numerous challenges, including but not limited to a lack of reliable simulation environments, limited historical data, and stringent safety and control robustness requirements. In this work, we present a novel physics-informed offline reinforcement learning (RL) framework for energy efficiency optimization of DC cooling systems. The proposed framework models the complex dynamical patterns and physical dependencies inside a server room using a purposely designed graph neural network architecture that is compliant with the fundamental time-reversal symmetry. Because of its well-behaved and generalizable state-action representations, the model enables sample-efficient and robust latent space offline policy learning using limited real-world operational data. Our framework has been successfully deployed and verified in a large-scale production DC for closed-loop control of its air-cooling units (ACUs). We conducted a total of 2000 hours of short and long-term experiments in the production DC environment. The results show that our method achieves 14~21% energy savings in the DC cooling system, without any violation of the safety or operational constraints. Our results have demonstrated the significant potential of offline RL in solving a broad range of data-limited, safety-critical real-world industrial control problems.</article>","contentLength":1882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation","url":"https://arxiv.org/abs/2501.14679","date":1739768400,"author":"","guid":1014,"unread":true,"content":"<article>arXiv:2501.14679v4 Announce Type: replace \nAbstract: Attention-based methods have demonstrated exceptional performance in modelling long-range dependencies on spherical cortical surfaces, surpassing traditional Geometric Deep Learning (GDL) models. However, their extensive inference time and high memory demands pose challenges for application to large datasets with limited computing resources. Inspired by the state space model in computer vision, we introduce the attention-free Vision Mamba (Vim) to spherical surfaces, presenting a domain-agnostic architecture for analyzing data on spherical manifolds. Our method achieves surface patching by representing spherical data as a sequence of triangular patches derived from a subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on multiple neurodevelopmental phenotype regression tasks using cortical surface metrics from neonatal brains. Experimental results demonstrate that SiM outperforms both attention- and GDL-based methods, delivering 4.8 times faster inference and achieving 91.7% lower memory consumption compared to the Surface Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity analysis further underscores the potential of SiM to identify subtle cognitive developmental patterns. The code is available at https://github.com/Rongzhao-He/surface-vision-mamba.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating and Improving Graph to Text Generation with Large Language Models","url":"https://arxiv.org/abs/2501.14497","date":1739768400,"author":"","guid":1015,"unread":true,"content":"<article>arXiv:2501.14497v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we conduct a comprehensive evaluation of prompting current open-source LLMs on graph-to-text generation tasks. Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found that the improvements from tuning-free approaches were incremental, as LLMs struggle with planning on complex graphs, particularly those with a larger number of triplets. To further improve LLMs in planning with graph sequences and grounding in truth, we introduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks: reordering and attribution. Through extensive automatic and human evaluations, we demonstrate significant improvements in the quality of generated text from both few-shot learning and fine-tuning perspectives using the PlanGTG dataset. Our study paves the way for new research directions in graph-to-text generation. PlanGTG datasets can be found in https://github.com/probe2/kg_text.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Inverse Linear Optimization: Improved Regret Bound, Robustness to Suboptimality, and Toward Tight Regret Analysis","url":"https://arxiv.org/abs/2501.14349","date":1739768400,"author":"","guid":1016,"unread":true,"content":"<article>arXiv:2501.14349v5 Announce Type: replace \nAbstract: We study an online learning problem where, over $T$ rounds, a learner observes both time-varying sets of feasible actions and an agent's optimal actions, selected by solving linear optimization over the feasible actions. The learner sequentially makes predictions of the agent's underlying linear objective function, and their quality is measured by the regret, the cumulative gap between optimal objective values and those achieved by following the learner's predictions. A seminal work by B\\\"armann et al. (ICML 2017) showed that online learning methods can be applied to this problem to achieve regret bounds of $O(\\sqrt{T})$. Recently, Besbes et al. (COLT 2021, Oper. Res. 2023) significantly improved the result by achieving an $O(n^4\\ln T)$ regret bound, where $n$ is the dimension of the ambient space of objective vectors. Their method, based on the ellipsoid method, runs in polynomial time but is inefficient for large $n$ and $T$. In this paper, we obtain an $O(n\\ln T)$ regret bound, improving upon the previous bound of $O(n^4\\ln T)$ by a factor of $n^3$. Our method is simple and efficient: we apply the online Newton step (ONS) to appropriate exp-concave loss functions. Moreover, for the case where the agent's actions are possibly suboptimal, we establish an $O(n\\ln T+\\sqrt{\\Delta_Tn\\ln T})$ regret bound, where $\\Delta_T$ is the cumulative suboptimality of the agent's actions. This bound is achieved by using MetaGrad, which runs ONS with $\\Theta(\\ln T)$ different learning rates in parallel. We also provide a simple instance that implies an $\\Omega(n)$ lower bound, showing that our $O(n\\ln T)$ bound is tight up to an $O(\\ln T)$ factor. This gives rise to a natural question: can the $O(\\ln T)$ factor in the upper bound be removed? For the special case of $n=2$, we show that an $O(1)$ regret bound is possible, while we delineate challenges in extending this result to higher dimensions.</article>","contentLength":1965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS","url":"https://arxiv.org/abs/2501.13238","date":1739768400,"author":"","guid":1017,"unread":true,"content":"<article>arXiv:2501.13238v2 Announce Type: replace \nAbstract: This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary search (BS) locking, designed to cover a broad frequency range from 533 MHz to 4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a linear to a logarithmic function, completing in B+1 cycles, where B represents the digital-to-analog (DAC) resolution controlling the voltage-controlled delay line (VCDL). At the start of the BS process, large step sizes can cause significant bias overshoots, potentially leading to clock failure conditions (i.e., clocks fail to propagate through the VCDL). To address this issue, a toggle detector is introduced to monitor clock activity and adjust the binary search controller. Upon detecting a stalled clock, the controller reverts the DAC code to the previous working code and resumes the BS with a reduced step size. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a locking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at 4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with a static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter of 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit (FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\\cdot ns^2$.</article>","contentLength":1353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small Language Models for Biomedical Question Answering","url":"https://arxiv.org/abs/2501.12746","date":1739768400,"author":"","guid":1018,"unread":true,"content":"<article>arXiv:2501.12746v4 Announce Type: replace \nAbstract: When addressing professional questions in the biomedical domain, humans typically acquire multiple pieces of information as evidence and engage in multifaceted analysis to provide high-quality answers. Current LLM-based question answering methods lack a detailed definition and learning process for evidence analysis, leading to the risk of error propagation and hallucinations while using evidence. Although increasing the parameter size of LLMs can alleviate these issues, it also presents challenges in training and deployment with limited resources. In this study, we propose EvidenceMap, which aims to enable a tiny pre-trained language model to explicitly learn multiple aspects of biomedical evidence, including supportive evaluation, logical correlation and content summarization, thereby latently guiding a small generative model (around 3B parameters) to provide textual responses. Experimental results demonstrate that our method, learning evidence analysis by fine-tuning a model with only 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in reference-based quality and accuracy, respectively.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Growth strategies for arbitrary DAG neural architectures","url":"https://arxiv.org/abs/2501.12690","date":1739768400,"author":"","guid":1019,"unread":true,"content":"<article>arXiv:2501.12690v2 Announce Type: replace \nAbstract: Deep learning has shown impressive results obtained at the cost of training huge neural networks. However, the larger the architecture, the higher the computational, financial, and environmental costs during training and inference. We aim at reducing both training and inference durations. We focus on Neural Architecture Growth, which can increase the size of a small model when needed, directly during training using information from the backpropagation. We expand existing work and freely grow neural networks in the form of any Directed Acyclic Graph by reducing expressivity bottlenecks in the architecture. We explore strategies to reduce excessive computations and steer network growth toward more parameter-efficient architectures.</article>","contentLength":792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL","url":"https://arxiv.org/abs/2501.12372","date":1739768400,"author":"","guid":1020,"unread":true,"content":"<article>arXiv:2501.12372v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.\n  In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong performances on various benchmark datasets without finetuning and expensive self-consistency based techniques.</article>","contentLength":1494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Generalized Chernoff-Stein Lemma, Applications and Examples","url":"https://arxiv.org/abs/2501.12066","date":1739768400,"author":"","guid":1021,"unread":true,"content":"<article>arXiv:2501.12066v2 Announce Type: replace \nAbstract: In this manuscript we define the notion of \"$\\delta$-typicality\" for both entropy and relative entropy, as well as a notion of $\\epsilon$-goodness and provide an extension to Stein's lemma for continuous quantities as well as correlated setups. We apply the derived results on the Gaussian hypothesis testing problem where the observations are possibly correlated.</article>","contentLength":417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM","url":"https://arxiv.org/abs/2501.11893","date":1739768400,"author":"","guid":1022,"unread":true,"content":"<article>arXiv:2501.11893v2 Announce Type: replace \nAbstract: Traditional Visual Simultaneous Localization and Mapping (vSLAM) systems focus solely on static scene structures, overlooking dynamic elements in the environment. Although effective for accurate visual odometry in complex scenarios, these methods discard crucial information about moving objects. By incorporating this information into a Dynamic SLAM framework, the motion of dynamic entities can be estimated, enhancing navigation whilst ensuring accurate localization. However, the fundamental formulation of Dynamic SLAM remains an open challenge, with no consensus on the optimal approach for accurate motion estimation within a SLAM pipeline. Therefore, we developed DynoSAM, an open-source framework for Dynamic SLAM that enables the efficient implementation, testing, and comparison of various Dynamic SLAM optimization formulations. DynoSAM integrates static and dynamic measurements into a unified optimization problem solved using factor graphs, simultaneously estimating camera poses, static scene, object motion or poses, and object structures. We evaluate DynoSAM across diverse simulated and real-world datasets, achieving state-of-the-art motion estimation in indoor and outdoor environments, with substantial improvements over existing systems. Additionally, we demonstrate DynoSAM utility in downstream applications, including 3D reconstruction of dynamic scenes and trajectory prediction, thereby showcasing potential for advancing dynamic object-aware SLAM systems. DynoSAM is open-sourced at https://github.com/ACFR-RPG/DynOSAM.</article>","contentLength":1601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ParkView: Visualizing Monotone Interleavings","url":"https://arxiv.org/abs/2501.10728","date":1739768400,"author":"","guid":1023,"unread":true,"content":"<article>arXiv:2501.10728v2 Announce Type: replace \nAbstract: Merge trees are a powerful tool from topological data analysis that is frequently used to analyze scalar fields. The similarity between two merge trees can be captured by an interleaving: a pair of maps between the trees that jointly preserve ancestor relations in the trees. Interleavings can have a complex structure; visualizing them requires a sense of (drawing) order which is not inherent in this purely topological concept. However, in practice it is often desirable to introduce additional geometric constraints, which leads to variants such as labeled or monotone interleavings. Monotone interleavings respect a given order on the leaves of the merge trees and hence have the potential to be visualized in a clear and comprehensive manner.\n  In this paper, we introduce ParkView: a schematic, scalable encoding for monotone interleavings. ParkView captures both maps of the interleaving using an optimal decomposition of both trees into paths and corresponding branches. We prove several structural properties of monotone interleavings, which support a sparse visual encoding using active paths and hedges that can be linked using a maximum of 6 colors for merge trees of arbitrary size. We show how to compute an optimal path-branch decomposition in linear time and illustrate ParkView on a number of real-world datasets.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation using Tree-based Search","url":"https://arxiv.org/abs/2501.10053","date":1739768400,"author":"","guid":1024,"unread":true,"content":"<article>arXiv:2501.10053v2 Announce Type: replace \nAbstract: Leveraging the autonomous decision-making capabilities of large language models (LLMs) has demonstrated superior performance in reasoning tasks. However, despite the success of iterative or recursive retrieval-augmented generation (RAG) techniques, these methods are often constrained to a single solution space when confronted with complex problems. In this paper, we propose a novel thinking pattern in RAG that integrates system analysis with efficient reasoning actions, significantly activating intrinsic reasoning capabilities and expanding the solution space of specific tasks via Monte Carlo Tree Search (MCTS), which we refer to as AirRAG. Specifically, our approach designs five fundamental reasoning actions, which are expanded to a broad tree-based reasoning space using MCTS. The approach also incorporates self-consistency verification to explore potential reasoning paths and inference scaling law. Additionally, computationally optimal strategies are employed to allocate more inference resources to key actions, thereby enhancing overall performance. Experimental results demonstrate the effectiveness of AirRAG, showing significant performance gains on complex question-answering datasets. Furthermore, AirRAG is flexible and lightweight, making it easy to integrate with other advanced technologies.</article>","contentLength":1371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities","url":"https://arxiv.org/abs/2501.08648","date":1739768400,"author":"","guid":1025,"unread":true,"content":"<article>arXiv:2501.08648v2 Announce Type: replace \nAbstract: While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.</article>","contentLength":1267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification","url":"https://arxiv.org/abs/2501.02844","date":1739768400,"author":"","guid":1026,"unread":true,"content":"<article>arXiv:2501.02844v3 Announce Type: replace \nAbstract: Text classification is a fundamental task in data mining, pivotal to various applications such as tabular understanding and recommendation. Although neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and new target labels frequently appear based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding ability. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to classify texts. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. Rather than treating each input independently, GORAG constructs and maintains a weighted graph by extracting side information across all target texts. In this graph, text keywords and labels are represented as nodes, with edges indicating the correlations between them. To model these correlations, GORAG employs an edge weighting mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and precise contextual information.</article>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations","url":"https://arxiv.org/abs/2501.02074","date":1739768400,"author":"","guid":1027,"unread":true,"content":"<article>arXiv:2501.02074v2 Announce Type: replace \nAbstract: Societal stereotypes are at the center of a myriad of responsible AI interventions targeted at reducing the generation and propagation of potentially harmful outcomes. While these efforts are much needed, they tend to be fragmented and often address different parts of the issue without taking in a unified or holistic approach about social stereotypes and how they impact various parts of the machine learning pipeline. As a result, it fails to capitalize on the underlying mechanisms that are common across different types of stereotypes, and to anchor on particular aspects that are relevant in certain cases. In this paper, we draw on social psychological research, and build on NLP data and methods, to propose a unified framework to operationalize stereotypes in generative AI evaluations. Our framework identifies key components of stereotypes that are crucial in AI evaluation, including the target group, associated attribute, relationship characteristics, perceiving group, and relevant context. We also provide considerations and recommendations for its responsible use.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Refinement Strategies for LLM-based Product Attribute Value Extraction","url":"https://arxiv.org/abs/2501.01237","date":1739768400,"author":"","guid":1028,"unread":true,"content":"<article>arXiv:2501.01237v2 Announce Type: replace \nAbstract: Structured product data, in the form of attribute-value pairs, is essential for e-commerce platforms to support features such as faceted product search and attribute-based product comparison. However, vendors often provide unstructured product descriptions, making attribute value extraction necessary to ensure data consistency and usability. Large language models (LLMs) have demonstrated their potential for product attribute value extraction in few-shot scenarios. Recent research has shown that self-refinement techniques can improve the performance of LLMs on tasks such as code generation and text-to-SQL translation. For other tasks, the application of these techniques has resulted in increased costs due to processing additional tokens, without achieving any improvement in performance. This paper investigates applying two self-refinement techniques (error-based prompt rewriting and self-correction) to the product attribute value extraction task. The self-refinement techniques are evaluated across zero-shot, few-shot in-context learning, and fine-tuning scenarios using GPT-4o. The experiments show that both self-refinement techniques fail to significantly improve the extraction performance while substantially increasing processing costs. For scenarios with development data, fine-tuning yields the highest performance, while the ramp-up costs of fine-tuning are balanced out as the amount of product descriptions increases.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reduced Order Models and Conditional Expectation -- Analysing Parametric Low-Order Approximations","url":"https://arxiv.org/abs/2412.19836","date":1739768400,"author":"","guid":1029,"unread":true,"content":"<article>arXiv:2412.19836v2 Announce Type: replace \nAbstract: Systems may depend on parameters which one may control, or which serve to optimise the system, or are imposed externally, or they could be uncertain. This last case is taken as the ``Leitmotiv'' for the following. A reduced order model is produced from the full order model by some kind of projection onto a relatively low-dimensional manifold or subspace. The parameter dependent reduction process produces a function of the parameters into the manifold. One now wants to examine the relation between the full and the reduced state for all possible parameter values of interest. Similarly, in the field of machine learning, also a function of the parameter set into the image space of the machine learning model is learned on a training set of samples, typically minimising the mean-square error. This set may be seen as a sample from some probability distribution, and thus the training is an approximate computation of the expectation, giving an approximation to the conditional expectation, a special case of an Bayesian updating where the Bayesian loss function is the mean-square error. This offers the possibility of having a combined look at these methods, and also of introducing more general loss functions.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations","url":"https://arxiv.org/abs/2412.18940","date":1739768400,"author":"","guid":1030,"unread":true,"content":"<article>arXiv:2412.18940v2 Announce Type: replace \nAbstract: Songwriting is often driven by multimodal inspirations, such as imagery, narratives, or existing music, yet songwriters remain unsupported by current music AI systems in incorporating these multimodal inputs into their creative processes. We introduce Amuse, a songwriting assistant that transforms multimodal (image, text, or audio) inputs into chord progressions that can be seamlessly incorporated into songwriters' creative processes. A key feature of Amuse is its novel method for generating coherent chords that are relevant to music keywords in the absence of datasets with paired examples of multimodal inputs and chords. Specifically, we propose a method that leverages multimodal large language models (LLMs) to convert multimodal inputs into noisy chord suggestions and uses a unimodal chord model to filter the suggestions. A user study with songwriters shows that Amuse effectively supports transforming multimodal ideas into coherent musical suggestions, enhancing users' agency and creativity throughout the songwriting process.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty-Aware Critic Augmentation for Hierarchical Multi-Agent EV Charging Control","url":"https://arxiv.org/abs/2412.18047","date":1739768400,"author":"","guid":1031,"unread":true,"content":"<article>arXiv:2412.18047v2 Announce Type: replace \nAbstract: The advanced bidirectional EV charging and discharging technology, aimed at supporting grid stability and emergency operations, has driven a growing interest in workplace applications. It not only reduces electricity expenses but also enhances the resilience in handling practical matters, such as peak power limitation, fluctuating energy prices, and unpredictable EV departures. Considering these factors systematically can benefit energy efficiency in office buildings and for EV users simultaneously. To employ AI to address these issues, we propose HUCA, a novel real-time charging control for regulating energy demands for both the building and EVs. HUCA employs hierarchical actor-critic networks to dynamically reduce electricity costs in buildings, accounting for the needs of EV charging in the dynamic pricing scenario. To tackle the uncertain EV departures, we introduce a new critic augmentation to account for departure uncertainties in evaluating the charging decisions, while maintaining the robustness of the charging control. Experiments on real-world electricity datasets under both simulated certain and uncertain departure scenarios demonstrate that HUCA outperforms baselines in terms of total electricity costs while maintaining competitive performance in fulfilling EV charging requirements. A case study also manifests that HUCA effectively balances energy supply between the building and EVs based on real-time information, showcasing its potential as a key AI-driven solution for vehicle charging control.</article>","contentLength":1585,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ArchComplete: Autoregressive 3D Architectural Design Generation with Hierarchical Diffusion-Based Upsampling","url":"https://arxiv.org/abs/2412.17957","date":1739768400,"author":"","guid":1032,"unread":true,"content":"<article>arXiv:2412.17957v2 Announce Type: replace \nAbstract: Recent advances in 3D generative models have shown promising results but often fall short in capturing the complexity of architectural geometries and topologies and fine geometric details at high resolutions. To tackle this, we present ArchComplete, a two-stage voxel-based 3D generative pipeline consisting of a vector-quantised model, whose composition is modelled with an autoregressive transformer for generating coarse shapes, followed by a hierarchical upsampling strategy for further enrichment with fine structures and details. Key to our pipeline is (i) learning a contextually rich codebook of local patch embeddings, optimised alongside a 2.5D perceptual loss that captures global spatial correspondence of projections onto three axis-aligned orthogonal planes, and (ii) redefining upsampling as a set of conditional diffusion models learning from a hierarchy of randomly cropped coarse-to-fine local volumetric patches. Trained on our introduced dataset of 3D house models with fully modelled exterior and interior, ArchComplete autoregressively generates models at the resolution of $64^{3}$ and progressively refines them up to $512^{3}$, with voxel sizes as small as $ \\approx 9\\text{cm}$. ArchComplete solves a variety of tasks, including genetic interpolation and variation, unconditional synthesis, shape and plan-drawing completion, as well as geometric detailisation, while achieving state-of-the-art performance in quality, diversity, and computational efficiency.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UAV Communications: Impact of Obstacles on Channel Characteristics","url":"https://arxiv.org/abs/2412.17934","date":1739768400,"author":"","guid":1033,"unread":true,"content":"<article>arXiv:2412.17934v3 Announce Type: replace \nAbstract: In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as effective platforms for carrying Wi-Fi Access Points (APs) and cellular Base Stations (BSs), enabling low-cost, agile, and flexible wireless networks with high Quality of Service (QoS). The next generation of wireless communications will rely on increasingly higher frequencies, which are easily obstructed by obstacles. One of the most critical concepts yet to be fully addressed is positioning the UAV at optimal coordinates while accounting for obstacles. To ensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS, and establish reliable wireless links with maximum coverage, obstacles must be integrated into the proposed placement algorithms. This paper introduces a simulation-based measurement approach for characterizing an air-to-ground (AG) channel in a simple scenario. By considering obstacles, we present a novel perspective on channel characterization. The results, in terms of throughput, packet delivery, packet loss, and delay, are compared using the proposed positioning approach.</article>","contentLength":1146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero Shot Time Series Forecasting Using Kolmogorov Arnold Networks","url":"https://arxiv.org/abs/2412.17853","date":1739768400,"author":"","guid":1034,"unread":true,"content":"<article>arXiv:2412.17853v2 Announce Type: replace \nAbstract: Accurate energy price forecasting is crucial for participants in day-ahead energy markets, as it significantly influences their decision-making processes. While machine learning-based approaches have shown promise in enhancing these forecasts, they often remain confined to the specific markets on which they are trained, thereby limiting their adaptability to new or unseen markets. In this paper, we introduce a cross-domain adaptation model designed to forecast energy prices by learning market-invariant representations across different markets during the training phase. We propose a doubly residual N-BEATS network with Kolmogorov Arnold networks at its core for time series forecasting. These networks, grounded in the Kolmogorov-Arnold representation theorem, offer a powerful way to approximate multivariate continuous functions. The cross domain adaptation model was generated with an adversarial framework. The model's effectiveness was tested in predicting day-ahead electricity prices in a zero shot fashion. In comparison with baseline models, our proposed framework shows promising results. By leveraging the Kolmogorov-Arnold networks, our model can potentially enhance its ability to capture complex patterns in energy price data, thus improving forecast accuracy across diverse market conditions. This addition not only enriches the model's representational capacity but also contributes to a more robust and flexible forecasting tool adaptable to various energy markets.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Examining Imbalance Effects on Performance and Demographic Fairness of Clinical Language Models","url":"https://arxiv.org/abs/2412.17803","date":1739768400,"author":"","guid":1035,"unread":true,"content":"<article>arXiv:2412.17803v2 Announce Type: replace \nAbstract: Data imbalance is a fundamental challenge in applying language models to biomedical applications, particularly in ICD code prediction tasks where label and demographic distributions are uneven. While state-of-the-art language models have been increasingly adopted in biomedical tasks, few studies have systematically examined how data imbalance affects model performance and fairness across demographic groups. This study fills the gap by statistically probing the relationship between data imbalance and model performance in ICD code prediction. We analyze imbalances in a standard benchmark data across gender, age, ethnicity, and social determinants of health by state-of-the-art biomedical language models. By deploying diverse performance metrics and statistical analyses, we explore the influence of data imbalance on performance variations and demographic fairness. Our study shows that data imbalance significantly impacts model performance and fairness, but feature similarity to the majority class may be a more critical factor. We believe this study provides valuable insights for developing more equitable and robust language models in healthcare applications.</article>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level Feature Distribution for Medical Image Segmentation","url":"https://arxiv.org/abs/2412.17241","date":1739768400,"author":"","guid":1036,"unread":true,"content":"<article>arXiv:2412.17241v2 Announce Type: replace \nAbstract: Medical image segmentation plays a crucial role in assisting healthcare professionals with accurate diagnoses and enabling automated diagnostic processes. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies, while transformer-based architectures, despite their effectiveness, come with increased computational complexity. Recent efforts have focused on combining CNNs and transformers to balance performance and efficiency, but existing approaches still face challenges in achieving high segmentation accuracy while maintaining low computational costs. Furthermore, many methods underutilize the CNN encoder's capability to capture local spatial information, concentrating primarily on mitigating long-range dependency issues. To address these limitations, we propose QTSeg, a novel architecture for medical image segmentation that effectively integrates local and global information. QTSeg features a dual-mix attention decoder designed to enhance segmentation performance through: (1) a cross-attention mechanism for improved feature alignment, (2) a spatial attention module to capture long-range dependencies, and (3) a channel attention block to learn inter-channel relationships. Additionally, we introduce a multi-level feature distribution module, which adaptively balances feature propagation between the encoder and decoder, further boosting performance. Extensive experiments on five publicly available datasets covering diverse segmentation tasks, including lesion, polyp, breast cancer, cell, and retinal vessel segmentation, demonstrate that QTSeg outperforms state-of-the-art methods across multiple evaluation metrics while maintaining lower computational costs. Our implementation can be found at: https://github.com/tpnam0901/QTSeg (v1.0.0)</article>","contentLength":1862,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learn2Mix: Training Neural Networks Using Adaptive Data Integration","url":"https://arxiv.org/abs/2412.16482","date":1739768400,"author":"","guid":1037,"unread":true,"content":"<article>arXiv:2412.16482v2 Announce Type: replace \nAbstract: Accelerating model convergence within resource-constrained environments is critical to ensure fast and efficient neural network training. This work presents learn2mix, a novel training strategy that adaptively adjusts class proportions within batches, focusing on classes with higher error rates. Unlike classical training methods that use static class proportions, learn2mix continually adapts class proportions during training, leading to faster convergence. Empirical evaluations conducted on benchmark datasets show that neural networks trained with learn2mix converge faster than those trained with existing approaches, achieving improved results for classification, regression, and reconstruction tasks under limited training resources and with imbalanced classes. Our empirical findings are supported by theoretical analysis.</article>","contentLength":885,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous Driving Systems","url":"https://arxiv.org/abs/2412.16265","date":1739768400,"author":"","guid":1038,"unread":true,"content":"<article>arXiv:2412.16265v3 Announce Type: replace \nAbstract: Existing Autonomous Driving Systems (ADS) independently make driving decisions, but they face two significant limitations. First, in complex scenarios, ADS may misinterpret the environment and make inappropriate driving decisions. Second, these systems are unable to incorporate human driving preferences in their decision-making processes. This paper proposes Autoware$.$Flex, a novel ADS system that incorporates human input into the driving process, allowing users to guide the ADS in making more appropriate decisions and ensuring their preferences are satisfied. Achieving this needs to address two key challenges: (1) translating human instructions, expressed in natural language, into a format the ADS can understand, and (2) ensuring these instructions are executed safely and consistently within the ADS' s decision-making framework. For the first challenge, we employ a Large Language Model (LLM) assisted by an ADS-specialized knowledge base to enhance domain-specific translation. For the second challenge, we design a validation mechanism to ensure that human instructions result in safe and consistent driving behavior. Experiments conducted on both simulators and a real-world autonomous vehicle demonstrate that Autoware$.$Flex effectively interprets human instructions and executes them safely.</article>","contentLength":1364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continual Learning with Strategic Selection and Forgetting for Network Intrusion Detection","url":"https://arxiv.org/abs/2412.16264","date":1739768400,"author":"","guid":1039,"unread":true,"content":"<article>arXiv:2412.16264v3 Announce Type: replace \nAbstract: Intrusion Detection Systems (IDS) are crucial for safeguarding digital infrastructure. In dynamic network environments, both threat landscapes and normal operational behaviors are constantly changing, resulting in concept drift. While continuous learning mitigates the adverse effects of concept drift, insufficient attention to drift patterns and excessive preservation of outdated knowledge can still hinder the IDS's adaptability. In this paper, we propose SSF (Strategic Selection and Forgetting), a novel continual learning method for IDS, providing continuous model updates with a constantly refreshed memory buffer. Our approach features a strategic sample selection algorithm to select representative new samples and a strategic forgetting mechanism to drop outdated samples. The proposed strategic sample selection algorithm prioritizes new samples that cause the `drifted' pattern, enabling the model to better understand the evolving landscape. Additionally, we introduce strategic forgetting upon detecting significant drift by discarding outdated samples to free up memory, allowing the incorporation of more recent data. SSF captures evolving patterns effectively and ensures the model is aligned with the change of data patterns, significantly enhancing the IDS's adaptability to concept drift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15 datasets demonstrates its superior adaptability to concept drift for network intrusion detection. The code is released at https://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.</article>","contentLength":1619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WigglyEyes: Inferring Eye Movements from Keypress Data","url":"https://arxiv.org/abs/2412.15669","date":1739768400,"author":"","guid":1040,"unread":true,"content":"<article>arXiv:2412.15669v2 Announce Type: replace \nAbstract: We present a model for inferring where users look during interaction based on keypress data only. Given a key log, it outputs a scanpath that tells, moment-by-moment, how the user had moved eyes while entering those keys. The model can be used as a proxy for human data in cases where collecting real eye tracking data is expensive or impossible. Our technical insight is three-fold: first, we present an inference architecture that considers the individual characteristics of the user, inferred as a low-dimensional parameter vector; second, we present a novel loss function for synchronizing inferred eye movements with the keypresses; third, we train the model using a hybrid approach with both human data and synthetically generated data. The approach can be applied in interactive systems where predictive models of user behavior are available. We report results from evaluation in the challenging case of touchscreen typing, where the model accurately inferred real eye movements.</article>","contentLength":1039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation","url":"https://arxiv.org/abs/2412.14050","date":1739768400,"author":"","guid":1041,"unread":true,"content":"<article>arXiv:2412.14050v3 Announce Type: replace \nAbstract: Recent generative large language models (LLMs) show remarkable performance in non-English languages, but when prompted in those languages they tend to express higher harmful social biases and toxicity levels. Prior work has shown that finetuning on specialized datasets can mitigate this behavior, and doing so in English can transfer to other languages. In this work, we investigate the impact of different finetuning methods on the model's bias and toxicity, but also on its ability to produce fluent and diverse text. We reduce biases by finetuning on curated non-harmful text, but find only direct preference optimization to be effective for mitigating toxicity. The mitigation caused by applying these methods in English also transfers to non-English languages. We find evidence that the extent to which transfer takes place can be predicted by the amount of data in a given language present in the model's pretraining data. However, this transfer of bias and toxicity mitigation often comes at the expense of decreased language generation ability in non-English languages, highlighting the importance of developing language-specific bias and toxicity mitigation methods.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation","url":"https://arxiv.org/abs/2412.13877","date":1739768400,"author":"","guid":1042,"unread":true,"content":"<article>arXiv:2412.13877v2 Announce Type: replace \nAbstract: In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot Manipulation), a dataset containing 107k demonstration trajectories across 479 diverse tasks involving 96 object classes. RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related information, including multi-view observations, proprioceptive robot state information, and linguistic task descriptions. To ensure data consistency and reliability for imitation learning, RoboMIND is built on a unified data collection platform and a standardized protocol, covering four distinct robotic embodiments: the Franka Emika Panda, the UR5e, the AgileX dual-arm robot, and a humanoid robot with dual dexterous hands. Our dataset also includes 5k real-world failure demonstrations, each accompanied by detailed causes, enabling failure reflection and correction during policy learning. Additionally, we created a digital twin environment in the Isaac Sim simulator, replicating the real-world tasks and assets, which facilitates the low-cost collection of additional training data and enables efficient evaluation. To demonstrate the quality and diversity of our dataset, we conducted extensive experiments using various imitation learning methods for single-task settings and state-of-the-art Vision-Language-Action (VLA) models for multi-task scenarios. By leveraging RoboMIND, the VLA models achieved high manipulation success rates and demonstrated strong generalization capabilities. To the best of our knowledge, RoboMIND is the largest multi-embodiment teleoperation dataset collected on a unified platform, providing large-scale and high-quality robotic training data. Our project is at https://x-humanoid-robomind.github.io/.</article>","contentLength":1807,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RareAgents: Advancing Rare Disease Care through LLM-Empowered Multi-disciplinary Team","url":"https://arxiv.org/abs/2412.12475","date":1739768400,"author":"","guid":1043,"unread":true,"content":"<article>arXiv:2412.12475v2 Announce Type: replace \nAbstract: Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the vast number of diseases. The involvement of multiple organs and systems, and the shortage of specialized doctors with relevant experience make diagnosing and treating rare diseases more challenging than common diseases. Recently, agents powered by large language models (LLMs) have demonstrated notable applications across various domains. In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical examinations. However, current agent frameworks are not well-adapted to real-world clinical scenarios, especially those involving the complex demands of rare diseases. To bridge this gap, we introduce RareAgents, the first LLM-driven multi-disciplinary team framework designed specifically for the complex clinical context of rare diseases. RareAgents integrates advanced Multidisciplinary Team (MDT) coordination, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents outperforms state-of-the-art domain-specific models, GPT-4o, and current agent frameworks in differential diagnosis and medication recommendation for rare diseases. Furthermore, we contribute a novel rare disease dataset, MIMIC-IV-Ext-Rare, to support further advancements in this field.</article>","contentLength":1471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"THESAURUS: Contrastive Graph Clustering by Swapping Fused Gromov-Wasserstein Couplings","url":"https://arxiv.org/abs/2412.11550","date":1739768400,"author":"","guid":1044,"unread":true,"content":"<article>arXiv:2412.11550v3 Announce Type: replace \nAbstract: Graph node clustering is a fundamental unsupervised task. Existing methods typically train an encoder through selfsupervised learning and then apply K-means to the encoder output. Some methods use this clustering result directly as the final assignment, while others initialize centroids based on this initial clustering and then finetune both the encoder and these learnable centroids. However, due to their reliance on K-means, these methods inherit its drawbacks when the cluster separability of encoder output is low, facing challenges from the Uniform Effect and Cluster Assimilation. We summarize three reasons for the low cluster separability in existing methods: (1) lack of contextual information prevents discrimination between similar nodes from different clusters; (2) training tasks are not sufficiently aligned with the downstream clustering task; (3) the cluster information in the graph structure is not appropriately exploited. To address these issues, we propose conTrastive grapH clustEring by SwApping fUsed gRomov-wasserstein coUplingS (THESAURUS). Our method introduces semantic prototypes to provide contextual information, and employs a cross-view assignment prediction pretext task that aligns well with the downstream clustering task. Additionally, it utilizes Gromov-Wasserstein Optimal Transport (GW-OT) along with the proposed prototype graph to thoroughly exploit cluster information in the graph structure. To adapt to diverse real-world data, THESAURUS updates the prototype graph and the prototype marginal distribution in OT by using momentum. Extensive experiments demonstrate that THESAURUS achieves higher cluster separability than the prior art, effectively mitigating the Uniform Effect and Cluster Assimilation issues</article>","contentLength":1810,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Image Forgery Localization with State Space Models","url":"https://arxiv.org/abs/2412.11214","date":1739768400,"author":"","guid":1045,"unread":true,"content":"<article>arXiv:2412.11214v2 Announce Type: replace \nAbstract: Pixel dependency modeling from tampered images is pivotal for image forgery localization. Current approaches predominantly rely on Convolutional Neural Networks (CNNs) or Transformer-based models, which often either lack sufficient receptive fields or entail significant computational overheads. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, we propose LoMa, a novel image forgery localization method that leverages the selective SSMs. Specifically, LoMa initially employs atrous selective scan to traverse the spatial domain and convert the tampered image into ordered patch sequences, and subsequently applies multi-directional state space modeling. In addition, an auxiliary convolutional branch is introduced to enhance local feature extraction. Extensive experimental results validate the superiority of LoMa over CNN-based and Transformer-based state-of-the-arts. To our best knowledge, this is the first image forgery localization model constructed based on the SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based forgery localization models. Code is available at https://github.com/multimediaFor/LoMa.</article>","contentLength":1432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SEW: Self-calibration Enhanced Whole Slide Pathology Image Analysis","url":"https://arxiv.org/abs/2412.10853","date":1739768400,"author":"","guid":1046,"unread":true,"content":"<article>arXiv:2412.10853v2 Announce Type: replace \nAbstract: Pathology images are considered the ``gold standard\" for cancer diagnosis and treatment, with gigapixel images providing extensive tissue and cellular information. Existing methods fail to simultaneously extract global structural and local detail features for comprehensive pathology image analysis efficiently. To address these limitations, we propose a self-calibration enhanced framework for whole slide pathology image analysis, comprising three components: a global branch, a focus predictor, and a detailed branch. The global branch initially classifies using the pathological thumbnail, while the focus predictor identifies relevant regions for classification based on the last layer features of the global branch. The detailed extraction branch then assesses whether the magnified regions correspond to the lesion area. Finally, a feature consistency constraint between the global and detail branches ensures that the global branch focuses on the appropriate region and extracts sufficient discriminative features for final identification. These focused discriminative features prove invaluable for uncovering novel prognostic tumor markers from the perspective of feature cluster uniqueness and tissue spatial distribution. Extensive experiment results demonstrate that the proposed framework can rapidly deliver accurate and explainable results for pathological grading and prognosis tasks.</article>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SuperMerge: An Approach For Gradient-Based Model Merging","url":"https://arxiv.org/abs/2412.10416","date":1739768400,"author":"","guid":1047,"unread":true,"content":"<article>arXiv:2412.10416v2 Announce Type: replace \nAbstract: Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic, monolithic, and possess the superpower to simultaneously support thousands of tasks. However, high-throughput applications often prefer smaller task-specific models because of their lower latency and cost. One challenge of using task-specific models is the incremental need for solving newer tasks after the model is already deployed for existing tasks. A straightforward solution requires fine-tuning the model again for both existing and new tasks, which is computationally expensive and time-consuming. To address this issue, we propose a model merging based approach called SUPERMERGE. SUPERMERGE is a gradient-based method to systematically merge several fine-tuned models trained on existing and new tasks. SUPERMERGE is designed to be lightweight and fast, and the merged model achieves similar performance to fully fine-tuned models on all tasks. Furthermore, we proposed a hierarchical model merging strategy to reduce the peak space requirement without sacrificing the performance of the merged model. We experimentally demonstrate that SUPERMERGE outperforms existing model merging methods on common natural language processing and computer vision tasks.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\textrm{A}^{\\textrm{2}}$RNet: Adversarial Attack Resilient Network for Robust Infrared and Visible Image Fusion","url":"https://arxiv.org/abs/2412.09954","date":1739768400,"author":"","guid":1048,"unread":true,"content":"<article>arXiv:2412.09954v3 Announce Type: replace \nAbstract: Infrared and visible image fusion (IVIF) is a crucial technique for enhancing visual performance by integrating unique information from different modalities into one fused image. Exiting methods pay more attention to conducting fusion with undisturbed data, while overlooking the impact of deliberate interference on the effectiveness of fusion results. To investigate the robustness of fusion models, in this paper, we propose a novel adversarial attack resilient network, called $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial paradigm with an anti-attack loss function to implement adversarial attacks and training. It is constructed based on the intrinsic nature of IVIF and provide a robust foundation for future research advancements. We adopt a Unet as the pipeline with a transformer-based defensive refinement module (DRM) under this paradigm, which guarantees fused image quality in a robust coarse-to-fine manner. Compared to previous works, our method mitigates the adverse effects of adversarial perturbations, consistently maintaining high-fidelity fusion results. Furthermore, the performance of downstream tasks can also be well maintained under adversarial attacks. Code is available at https://github.com/lok-18/A2RNet.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning","url":"https://arxiv.org/abs/2412.09078","date":1739768400,"author":"","guid":1049,"unread":true,"content":"<article>arXiv:2412.09078v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable abilities across various language tasks, but solving complex reasoning problems remains a significant challenge. While existing methods, such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this limitation, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT employs sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction, along with consensus-guided decision-making strategies to optimize both correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency.Code will be available at https://github.com/iamhankai/Forest-of-Thought.</article>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PhishIntel: Toward Practical Deployment of Reference-Based Phishing Detection","url":"https://arxiv.org/abs/2412.09057","date":1739768400,"author":"","guid":1050,"unread":true,"content":"<article>arXiv:2412.09057v2 Announce Type: replace \nAbstract: Phishing is a critical cyber threat, exploiting deceptive tactics to compromise victims and cause significant financial losses. While reference-based phishing detectors (RBPDs) have achieved notable advancements in detection accuracy, their real-world deployment is hindered by challenges such as high latency and inefficiency in URL analysis. To address these limitations, we present PhishIntel, an end-to-end phishing detection system for real-world deployment. PhishIntel intelligently determines whether a URL can be processed immediately or not, segmenting the detection process into two distinct tasks: a fast task that checks against local blacklists and result cache, and a slow task that conducts online blacklist verification, URL crawling, and webpage analysis using an RBPD. This fast-slow task system architecture ensures low response latency while retaining the robust detection capabilities of RBPDs for zero-day phishing threats. Furthermore, we develop two downstream applications based on PhishIntel: a phishing intelligence platform and a phishing email detection plugin for Microsoft Outlook, demonstrating its practical efficacy and utility.</article>","contentLength":1215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What You See Is Not Always What You Get: An Empirical Study of Code Comprehension by Large Language Models","url":"https://arxiv.org/abs/2412.08098","date":1739768400,"author":"","guid":1051,"unread":true,"content":"<article>arXiv:2412.08098v2 Announce Type: replace \nAbstract: Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering tasks, including code generation and comprehension. While LLMs have shown significant potential in assisting with coding, it is perceived that LLMs are vulnerable to adversarial attacks. In this paper, we investigate the vulnerability of LLMs to imperceptible attacks, where hidden character manipulation in source code misleads LLMs' behaviour while remaining undetectable to human reviewers. We devise these attacks into four distinct categories and analyse their impacts on code analysis and comprehension tasks. These four types of imperceptible coding character attacks include coding reordering, invisible coding characters, code deletions, and code homoglyphs. To comprehensively benchmark the robustness of current LLMs solutions against the attacks, we present a systematic experimental evaluation on multiple state-of-the-art LLMs. Our experimental design introduces two key performance metrics, namely model confidence using log probabilities of response, and the response correctness. A set of controlled experiments are conducted using a large-scale perturbed and unperturbed code snippets as the primary prompt input. Our findings confirm the susceptibility of LLMs to imperceptible coding character attacks, while different LLMs present different negative correlations between perturbation magnitude and performance. These results highlight the urgent need for robust LLMs capable of manoeuvring behaviours under imperceptible adversarial conditions. We anticipate this work provides valuable insights for enhancing the security and trustworthiness of LLMs in software engineering applications.</article>","contentLength":1776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parameter optimization for restarted mixed precision iterative sparse solver","url":"https://arxiv.org/abs/2412.08059","date":1739768400,"author":"","guid":1052,"unread":true,"content":"<article>arXiv:2412.08059v3 Announce Type: replace \nAbstract: We consider the problem of optimizing the parameter of a two-stage algorithm for approximate solution of a system of linear algebraic equations with a sparse $n\\times n$-matrix, i.e., with one in which the number of nonzero elements is $m\\!=\\!O(n)$. The two-stage algorithm uses conjugate gradient method at its stages. At the 1st stage, an approximate solution with accuracy $\\varepsilon_1$ is found for zero initial vector. All numerical values used at this stage are represented as single-precision numbers. The obtained solution is used as initial approximation for an approximate solution with a given accuracy $\\varepsilon_2$ that we obtain at the 2nd stage, where double-precision numbers are used. Based on the values of some matrix parameters, computed in a time not exceeding $O(m)$, we need to determine the value $\\varepsilon_1$ which minimizes the total computation time at two stages.\n  Using single-precision numbers for computations at the 1st stage is advantageous, since the execution time of one iteration will be approximately half that of one iteration at the 2nd stage. At the same time, using machine numbers with half the mantissa length accelerates the growth of the rounding error per iteration of the conjugate gradient method at the 1st stage, which entails an increase in the number of iterations performed at 2nd stage.\n  As parameters that allow us to determine $\\varepsilon_1$ for the input matrix, we use $n$, $m$, an estimate of the diameter of the graph associated with the matrix, an estimate of the spread of the matrix' eigenvalues, and estimates of its maximum eigenvalue. The optimal or close to the optimal value of $\\varepsilon_1$ can be determined for matrix with such a vector of parameters using the nearest neighbor regression or some other type of regression.</article>","contentLength":1859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Automated Cross-domain Exploratory Data Analysis through Large Language Models","url":"https://arxiv.org/abs/2412.07214","date":1739768400,"author":"","guid":1053,"unread":true,"content":"<article>arXiv:2412.07214v3 Announce Type: replace \nAbstract: Exploratory data analysis (EDA), coupled with SQL, is essential for data analysts involved in data exploration and analysis. However, data analysts often encounter two primary challenges: (1) the need to craft SQL queries skillfully, and (2) the requirement to generate suitable visualization types that enhance the interpretation of query results. Due to its significance, substantial research efforts have been made to explore different approaches to address these challenges, including leveraging large language models (LLMs). However, existing methods fail to meet real-world data exploration requirements primarily due to (1) complex database schema; (2) unclear user intent; (3) limited cross-domain generalization capability; and (4) insufficient end-to-end text-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain exploratory data analysis system. First, we propose hierarchical data context (i.e., HDC), which leverages LLMs to summarize the contexts related to the database schema, which is crucial for open-world EDA systems to generalize across data domains. Second, the EDA system is divided into four components (i.e., stages): HDC generation, question clarification and decomposition, text-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Finally, we implemented an end-to-end EDA system with a user-friendly GUI interface in the production environment at PingCAP. We have also open-sourced all APIs of TiInsight to facilitate research within the EDA community. Through extensive evaluations by a real-world user study, we demonstrate that TiInsight offers remarkable performance compared to human experts. Specifically, TiSQL achieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It also demonstrates state-of-the-art performance on the Bird dataset.</article>","contentLength":1910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stabilizing and Solving Inverse Problems using Data and Machine Learning","url":"https://arxiv.org/abs/2412.04409","date":1739768400,"author":"","guid":1054,"unread":true,"content":"<article>arXiv:2412.04409v2 Announce Type: replace \nAbstract: We consider an inverse problem involving the reconstruction of the solution to a nonlinear partial differential equation (PDE) with unknown boundary conditions. Instead of direct boundary data, we are provided with a large dataset of boundary observations for typical solutions (collective data) and a bulk measurement of a specific realization. To leverage this collective data, we first compress the boundary data using proper orthogonal decomposition (POD) in a linear expansion. Next, we identify a possible nonlinear low-dimensional structure in the expansion coefficients using an autoencoder, which provides a parametrization of the dataset in a lower-dimensional latent space. We then train an operator network to map the expansion coefficients representing the boundary data to the finite element solution of the PDE. Finally, we connect the autoencoder's decoder to the operator network which enables us to solve the inverse problem by optimizing a data-fitting term over the latent space. We analyze the underlying stabilized finite element method in the linear setting and establish an optimal error estimate in the $H^1$-norm. The nonlinear problem is then studied numerically, demonstrating the effectiveness of our approach.</article>","contentLength":1292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Preference Optimization for Reasoning with Pseudo Feedback","url":"https://arxiv.org/abs/2411.16345","date":1739768400,"author":"","guid":1055,"unread":true,"content":"<article>arXiv:2411.16345v2 Announce Type: replace \nAbstract: Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited. In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated test cases. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case. We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.</article>","contentLength":1399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Next Tokens via Second-to-Last Predictions with Generate and Refine","url":"https://arxiv.org/abs/2411.15661","date":1739768400,"author":"","guid":1056,"unread":true,"content":"<article>arXiv:2411.15661v2 Announce Type: replace \nAbstract: Autoregressive language models like GPT aim to predict next tokens, while autoencoding models such as BERT are trained on tasks such as predicting masked tokens. We train a decoder-only architecture for predicting the second to last token for a sequence of tokens. Our approach yields higher computational training efficiency than BERT-style models by employing a structured deterministic approach to masking tokens. We use our model to improve the next token predictions of a standard GPT by combining both predictions in a ``generate-then-refine'' approach. We demonstrate on different variants of GPT-2 and different datasets that (not unexpectedly) second to last token predictions are much more accurate, i.e., more than 15\\% higher accuracy than standard next token predictions. The ``generate-then-refine'' approach also demonstrates notable improvements in next-token predictions, yielding smaller yet consistent and significant gains.</article>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tulu 3: Pushing Frontiers in Open Language Model Post-Training","url":"https://arxiv.org/abs/2411.15124","date":1739768400,"author":"","guid":1057,"unread":true,"content":"<article>arXiv:2411.15124v4 Announce Type: replace \nAbstract: Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.\n  In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.</article>","contentLength":1707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","url":"https://arxiv.org/abs/2411.14695","date":1739768400,"author":"","guid":1058,"unread":true,"content":"<article>arXiv:2411.14695v2 Announce Type: replace \nAbstract: Regular unsupervised domain adaptive person re-identification (ReID) focuses on adapting a model from a source domain to a fixed target domain. However, an adapted ReID model can hardly retain previously-acquired knowledge and generalize to unseen data. In this paper, we propose a Dual-level Joint Adaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a model to new domains without forgetting source domain and each adapted target domain. We explore the possibility of using prototype and instance-level consistency to mitigate the forgetting during the adaptation. Specifically, we store a small number of representative image samples and corresponding cluster prototypes in a memory buffer, which is updated at each adaptation step. With the buffered images and prototypes, we regularize the image-to-image similarity and image-to-prototype similarity to rehearse old knowledge. After the multi-step adaptation, the model is tested on all seen domains and several unseen domains to validate the generalization ability of our method. Extensive experiments demonstrate that our proposed method significantly improves the anti-forgetting, generalization and backward-compatible ability of an unsupervised person ReID model.</article>","contentLength":1299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers","url":"https://arxiv.org/abs/2411.12118","date":1739768400,"author":"","guid":1059,"unread":true,"content":"<article>arXiv:2411.12118v2 Announce Type: replace \nAbstract: In this paper, I introduce the retrieval problem, a simple yet common reasoning task that can be solved only by transformers with a minimum number of layers, which grows logarithmically with the input size. I empirically show that large language models can solve the task under different prompting formulations without any fine-tuning. To understand how transformers solve the retrieval problem, I train several transformers on a minimal formulation. Successful learning occurs only under the presence of an implicit curriculum. I uncover the learned mechanisms by studying the attention maps in the trained transformers. I also study the training process, uncovering that attention heads always emerge in a specific sequence guided by the implicit curriculum.</article>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with Semantic-Aware Views for Efficient Visual Representation","url":"https://arxiv.org/abs/2411.09858","date":1739768400,"author":"","guid":1060,"unread":true,"content":"<article>arXiv:2411.09858v2 Announce Type: replace \nAbstract: This paper proposes a scalable and straightforward pre-training paradigm for efficient visual conceptual representation called occluded image contrastive learning (OCL). Our OCL approach is simple: we randomly mask patches to generate different views within an image and contrast them among a mini-batch of images. The core idea behind OCL consists of two designs. First, masked tokens have the potential to significantly diminish the conceptual redundancy inherent in images, and create distinct views with substantial fine-grained differences on the semantic concept level instead of the instance level. Second, contrastive learning is adept at extracting high-level semantic conceptual features during the pre-training, circumventing the high-frequency interference and additional costs associated with image reconstruction. Importantly, OCL learns highly semantic conceptual representations efficiently without relying on hand-crafted data augmentations or additional auxiliary modules. Empirically, OCL demonstrates high scalability with Vision Transformers, as the ViT-L/16 can complete pre-training in 133 hours using only 4 A100 GPUs, achieving 85.8\\% accuracy in downstream fine-tuning tasks. Code is available at https://anonymous.4open.science/r/OLRS/.</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation","url":"https://arxiv.org/abs/2411.08300","date":1739768400,"author":"","guid":1061,"unread":true,"content":"<article>arXiv:2411.08300v4 Announce Type: replace \nAbstract: Achieving low remote memory access latency remains the primary challenge in realizing memory disaggregation over Ethernet within the datacenters. We present EDM that attempts to overcome this challenge using two key ideas. First, while existing network protocols for remote memory access over the Ethernet, such as TCP/IP and RDMA, are implemented on top of the MAC layer, EDM takes a radical approach by implementing the entire network protocol stack for remote memory access within the Physical layer (PHY) of the Ethernet. This overcomes fundamental latency and bandwidth overheads imposed by the MAC layer, especially for small memory messages. Second, EDM implements a centralized, fast, in-network scheduler for memory traffic within the PHY of the Ethernet switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm, the scheduler dynamically reserves bandwidth between compute and memory nodes by creating virtual circuits in the PHY, thus eliminating queuing delay and layer 2 packet processing delay at the switch for memory traffic, while maintaining high bandwidth utilization. Our FPGA testbed demonstrates that EDM's network fabric incurs a latency of only $\\sim$300 ns for remote memory access in an unloaded network, which is an order of magnitude lower than state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to emerging PCIe-based solutions such as CXL. Larger-scale network simulations indicate that even at high network loads, EDM's average latency remains within 1.3$\\times$ its unloaded latency.</article>","contentLength":1608,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks","url":"https://arxiv.org/abs/2411.07902","date":1739768400,"author":"","guid":1062,"unread":true,"content":"<article>arXiv:2411.07902v2 Announce Type: replace \nAbstract: Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by generating an ensemble of predictive distributions. However, inference via ensembling is resource-intensive, requiring additional entropy sources to generate stochasticity which increases resource consumption. We introduce Bayes2IMC, an in-memory computing (IMC) architecture designed for binary Bayesian neural networks that leverage nanoscale device stochasticity to generate desired distributions. Our novel approach utilizes Phase-Change Memory (PCM) to harness inherent noise characteristics, enabling the creation of a binary neural network. This design eliminates the necessity for a pre-neuron Analog-to-Digital Converter (ADC), significantly improving power and area efficiency. We also develop a hardware-software co-optimized correction method applied solely on the logits in the final layer to reduce device-induced accuracy variations across deployments on hardware. Additionally, we devise a simple compensation technique that ensures no drop in classification accuracy despite conductance drift of PCM. We validate the effectiveness of our approach on the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy metrics comparable to ideal software implementations as well as results reported in the literature using other technologies. Finally, we present a complete core architecture and compare its projected power, performance, and area efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6 \\times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6 \\times$ improvement in power efficiency (in GOPS/W). In addition, the projected hardware performance of Bayes2IMC surpasses that of most of the BNN architectures based on memristive devices reported in the literature, and achieves up to $20\\%$ higher power efficiency compared to the state-of-the-art.</article>","contentLength":1950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Maritime Search and Rescue Missions with Aerial Images: A Survey","url":"https://arxiv.org/abs/2411.07649","date":1739768400,"author":"","guid":1063,"unread":true,"content":"<article>arXiv:2411.07649v2 Announce Type: replace \nAbstract: The speed of response by search and rescue teams at sea is of vital importance, as survival may depend on it. Recent technological advancements have led to the development of more efficient systems for locating individuals involved in a maritime incident, such as the use of Unmanned Aerial Vehicles (UAVs) equipped with cameras and other integrated sensors. Over the past decade, several researchers have contributed to the development of automatic systems capable of detecting people using aerial images, particularly by leveraging the advantages of deep learning. In this article, we provide a comprehensive review of the existing literature on this topic. We analyze the methods proposed to date, including both traditional techniques and more advanced approaches based on machine learning and neural networks. Additionally, we take into account the use of synthetic data to cover a wider range of scenarios without the need to deploy a team to collect data, which is one of the major obstacles for these systems. Overall, this paper situates the reader in the field of detecting people at sea using aerial images by quickly identifying the most suitable methodology for each scenario, as well as providing an in-depth discussion and direction for future trends.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains","url":"https://arxiv.org/abs/2411.06426","date":1739768400,"author":"","guid":1064,"unread":true,"content":"<article>arXiv:2411.06426v2 Announce Type: replace \nAbstract: As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Variational Inference on the Boolean Hypercube with the Quantum Entropy","url":"https://arxiv.org/abs/2411.03759","date":1739768400,"author":"","guid":1065,"unread":true,"content":"<article>arXiv:2411.03759v2 Announce Type: replace \nAbstract: In this paper, we derive variational inference upper-bounds on the log-partition function of pairwise Markov random fields on the Boolean hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We then propose an efficient algorithm to compute these bounds based on primal-dual optimization. An improvement of these bounds through the use of ''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and we present a greedy algorithm to select among these relaxations. We carry extensive numerical experiments and compare with state-of-the-art methods for this inference problem.</article>","contentLength":668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit Quality Estimation","url":"https://arxiv.org/abs/2411.00843","date":1739768400,"author":"","guid":1066,"unread":true,"content":"<article>arXiv:2411.00843v2 Announce Type: replace \nAbstract: Logic synthesis is a crucial phase in the circuit design process, responsible for transforming hardware description language (HDL) designs into optimized netlists. However, traditional logic synthesis methods are computationally intensive, restricting their iterative use in refining chip designs. Recent advancements in large language models (LLMs), particularly those fine-tuned on programming languages, present a promising alternative. This work proposes augmenting LLMs with predictor networks trained to estimate circuit quality directly from HDL code. To enhance performance, the model is regularized using embeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT) graphs, thereby incorporating lower-level circuit insights. The proposed method demonstrates superior performance compared to existing graph-based RTL-level estimation techniques on the established benchmark OpenABCD, while providing instant feedback on HDL code quality.</article>","contentLength":1014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Consumer Segmentation and Participation Drivers in Community-Supported Agriculture: A Choice Experiment and PLS-SEM Approach","url":"https://arxiv.org/abs/2411.00010","date":1739768400,"author":"","guid":1067,"unread":true,"content":"<article>arXiv:2411.00010v2 Announce Type: replace \nAbstract: As the global food system faces increasing challenges from sustainability, climate change, and food security issues, alternative food networks like Community-Supported Agriculture (CSA) play an essential role in fostering stronger connections between consumers and producers. However, understanding consumer engagement with CSA is fragmented, particularly in Japan where CSA participation is still emerging. This study aims to identify potential CSA participants in Japan and validate existing theories on CSA participation through a quantitative analysis of 2,484 Japanese consumers. Using choice experiments, Latent Class Analysis, and Partial Least Squares Structural Equation Modeling, we identified five distinct consumer segments. The \"Sustainable Food Seekers\" group showed the highest positive utility for CSA, driven primarily by \"Food Education and Learning Opportunities\" and \"Contribution to Environmental and Social Issues.\" These factors were consistently significant across all segments, suggesting that many Japanese consumers value CSA for its educational and environmental benefits. In contrast, factors related to \"Variety of Ingredients\" were less influential in determining participation intentions. The findings suggest that promoting CSA in Japan may be most effective by emphasizing its role in environmental and social impact, rather than focusing solely on product attributes like organic certification, which is readily available in supermarkets. This reflects a key distinction between CSA adoption in Japan and in other cultural contexts, where access to organic produce is a primary driver. For \"Sustainable Food Seekers,\" CSA offers a way to contribute to broader societal goals rather than just securing organic products.</article>","contentLength":1806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point Clouds","url":"https://arxiv.org/abs/2410.22099","date":1739768400,"author":"","guid":1068,"unread":true,"content":"<article>arXiv:2410.22099v4 Announce Type: replace \nAbstract: Brain imaging studies have demonstrated that diffusion MRI tractography geometric shape descriptors can inform the study of the brain's white matter pathways and their relationship to brain function. In this work, we investigate the possibility of utilizing a deep learning model to compute shape measures of the brain's white matter connections. We introduce a novel framework, TractShapeNet, that leverages a point cloud representation of tractography to compute five shape measures: length, span, volume, total surface area, and irregularity. We assess the performance of the method on a large dataset including 1065 healthy young adults. Experiments for shape measure computation demonstrate that our proposed TractShapeNet outperforms other point cloud-based neural network models in both the Pearson correlation coefficient and normalized error metrics. We compare the inference runtime results with the conventional shape computation tool DSI-Studio. Our results demonstrate that a deep learning approach enables faster and more efficient shape measure computation. We also conduct experiments on two downstream language cognition prediction tasks, showing that shape measures from TractShapeNet perform similarly to those computed by DSI-Studio. Our code will be available at: https://github.com/SlicerDMRI/TractShapeNet.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flaming-hot Initiation with Regular Execution Sampling for Large Language Models","url":"https://arxiv.org/abs/2410.21236","date":1739768400,"author":"","guid":1069,"unread":true,"content":"<article>arXiv:2410.21236v2 Announce Type: replace \nAbstract: Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This becomes especially critical in reasoning-related tasks with sandbox checkers, such as math or code, where the goal is to generate correct solutions to specific problems with higher probability. In this work, we introduce Flaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet highly effective method to efficiently find good responses. Our empirical findings show that FIRE sampling enhances inference-time generation quality and also benefits training in the alignment stage. Furthermore, we explore how FIRE sampling improves performance by promoting diversity and analyze the impact of employing FIRE at different positions within a response.</article>","contentLength":958,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strada-LLM: Graph LLM for traffic prediction","url":"https://arxiv.org/abs/2410.20856","date":1739768400,"author":"","guid":1070,"unread":true,"content":"<article>arXiv:2410.20856v2 Announce Type: replace \nAbstract: Traffic prediction is a vital component of intelligent transportation systems. By reasoning about traffic patterns in both the spatial and temporal dimensions, accurate and interpretable predictions can be provided. A considerable challenge in traffic prediction lies in handling the diverse data distributions caused by vastly different traffic conditions occurring at different locations. LLMs have been a dominant solution due to their remarkable capacity to adapt to new datasets with very few labeled data samples, i.e., few-shot adaptability. However, existing forecasting techniques mainly focus on extracting local graph information and forming a text-like prompt, leaving LLM- based traffic prediction an open problem. This work presents a probabilistic LLM for traffic forecasting with three highlights. We propose a graph-aware LLM for traffic prediction that considers proximal traffic information. Specifically, by considering the traffic of neighboring nodes as covariates, our model outperforms the corresponding time-series LLM. Furthermore, we adopt a lightweight approach for efficient domain adaptation when facing new data distributions in few-shot fashion. The comparative experiment demonstrates the proposed method outperforms the state-of-the-art LLM-based methods and the traditional GNN- based supervised approaches. Furthermore, Strada-LLM can be easily adapted to different LLM backbones without a noticeable performance drop.</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update","url":"https://arxiv.org/abs/2410.20299","date":1739768400,"author":"","guid":1071,"unread":true,"content":"<article>arXiv:2410.20299v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated impressive capabilities in language tasks, but they require high computing power and rely on static knowledge. To overcome these limitations, Retrieval-Augmented Generation (RAG) incorporates up-to-date external information into LLMs without extensive fine-tuning. Meanwhile, small language models (SLMs) deployed on edge devices offer efficiency and low latency but often struggle with complex reasoning tasks. Unfortunately, current RAG approaches are predominantly based on centralized databases and have not been adapted to address the distinct constraints associated with deploying SLMs in edge environments. To bridge this gap, we propose Edge-Assisted and Collaborative RAG (EACO-RAG), a lightweight framework that leverages distributed edge nodes for adaptive knowledge updates and retrieval. EACO-RAG also employs a hierarchical collaborative gating mechanism to dynamically select among local, edge-assisted, and cloud-based strategies, with a carefully designed algorithm based on Safe Online Bayesian Optimization to maximize the potential performance enhancements. Experimental results demonstrate that EACO-RAG matches the accuracy of cloud-based knowledge graph RAG systems while reducing total costs by up to 84.6% under relaxed delay constraints and by 65.3% under stricter delay requirements. This work represents our initial effort toward achieving a distributed and scalable tiered LLM deployments, with EACO-RAG serving as a promising first step in unlocking the full potential of hybrid edge-cloud intelligence.</article>","contentLength":1633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FRTree Planner: Robot Navigation in Cluttered and Unknown Environments with Tree of Free Regions","url":"https://arxiv.org/abs/2410.20230","date":1739768400,"author":"","guid":1072,"unread":true,"content":"<article>arXiv:2410.20230v2 Announce Type: replace \nAbstract: In this work, we present FRTree planner, a novel robot navigation framework that leverages a tree structure of free regions, specifically designed for navigation in cluttered and unknown environments with narrow passages. The framework continuously incorporates real-time perceptive information to identify distinct navigation options and dynamically expands the tree toward explorable and traversable directions. This dynamically constructed tree incrementally encodes the geometric and topological information of the collision-free space, enabling efficient selection of the intermediate goals, navigating around dead-end situations, and avoidance of dynamic obstacles without a prior map. Crucially, our method performs a comprehensive analysis of the geometric relationship between free regions and the robot during online replanning. In particular, the planner assesses the accessibility of candidate passages based on the robot's geometries, facilitating the effective selection of the most viable intermediate goals through accessible narrow passages while minimizing unnecessary detours. By combining the free region information with a bi-level trajectory optimization tailored for robots with specific geometries, our approach generates robust and adaptable obstacle avoidance strategies in confined spaces. Through extensive simulations and real-world experiments, FRTree demonstrates its superiority over benchmark methods in generating safe, efficient motion plans through highly cluttered and unknown terrains with narrow gaps.</article>","contentLength":1593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A distributional simplicity bias in the learning dynamics of transformers","url":"https://arxiv.org/abs/2410.19637","date":1739768400,"author":"","guid":1073,"unread":true,"content":"<article>arXiv:2410.19637v2 Announce Type: replace \nAbstract: The remarkable capability of over-parameterised neural networks to generalise effectively has been explained by invoking a ``simplicity bias'': neural networks prevent overfitting by initially learning simple classifiers before progressing to more complex, non-linear functions. While simplicity biases have been described theoretically and experimentally in feed-forward networks for supervised learning, the extent to which they also explain the remarkable success of transformers trained with self-supervised techniques remains unclear. In our study, we demonstrate that transformers, trained on natural language data, also display a simplicity bias. Specifically, they sequentially learn many-body interactions among input tokens, reaching a saturation point in the prediction error for low-degree interactions while continuing to learn high-degree interactions. To conduct this analysis, we develop a procedure to generate \\textit{clones} of a given natural language data set, which rigorously capture the interactions between tokens up to a specified order. This approach opens up the possibilities of studying how interactions of different orders in the data affect learning, in natural language processing and beyond.</article>","contentLength":1278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Overcoming Non-Submodularity: Towards Constant Approximation for Network Immunization","url":"https://arxiv.org/abs/2410.19205","date":1739768400,"author":"","guid":1074,"unread":true,"content":"<article>arXiv:2410.19205v4 Announce Type: replace \nAbstract: Given a network with an ongoing epidemic, the network immunization problem seeks to identify a fixed number of nodes to immunize in order to maximize the number of infections prevented. A fundamental computational challenge in network immunization is that the objective function is generally neither submodular nor supermodular. Consequently, no efficient algorithm is known to consistently achieve a constant-factor approximation. Traditionally, this problem is partially addressed using proxy objectives that offer better approximation properties, but these indirect optimizations often introduce losses in effectiveness due to gaps between the proxy and natural objectives.\n  In this paper, we overcome these fundamental barriers by leveraging the underlying stochastic structure of the diffusion process. Similar to the traditional influence objective, the immunization objective is an expectation expressed as a sum over deterministic instances. However, unlike the former, some of these terms are not submodular. Our key step is to prove that this sum has a bounded deviation from submodularity, enabling the classic greedy algorithm to achieve a constant-factor approximation for any sparse cascading network. We demonstrate that this approximation holds across various immunization settings and spread models.</article>","contentLength":1370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Models, Humans, Predictive Models: Who Is Worse at High-Stakes Decision Making?","url":"https://arxiv.org/abs/2410.15471","date":1739768400,"author":"","guid":1075,"unread":true,"content":"<article>arXiv:2410.15471v2 Announce Type: replace \nAbstract: Despite strong advisory against it, large generative models (LMs) are already being used for decision making tasks that were previously done by predictive models or humans. We put popular LMs to the test in a high-stakes decision making task: recidivism prediction. Studying three closed-access and open-source LMs, we analyze the LMs not exclusively in terms of accuracy, but also in terms of agreement with (imperfect, noisy, and sometimes biased) human predictions or existing predictive models. We conduct experiments that assess how providing different types of information, including distractor information such as photos, can influence LM decisions. We also stress test techniques designed to either increase accuracy or mitigate bias in LMs, and find that some to have unintended consequences on LM decisions. Our results provide additional quantitative evidence to the wisdom that current LMs are not the right tools for these types of tasks.</article>","contentLength":1004,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Context-Aware or Context-Insensitive? Assessing LLMs' Performance in Document-Level Translation","url":"https://arxiv.org/abs/2410.14391","date":1739768400,"author":"","guid":1076,"unread":true,"content":"<article>arXiv:2410.14391v2 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly strong contenders in machine translation. In this work, we focus on document-level translation, where some words cannot be translated without context from outside the sentence. Specifically, we investigate the ability of prominent LLMs to utilize the document context during translation through a perturbation analysis (analyzing models' robustness to perturbed and randomized document context) and an attribution analysis (examining the contribution of relevant context to the translation). We conduct an extensive evaluation across nine LLMs from diverse model families and training paradigms, including translation-specialized LLMs, alongside two encoder-decoder transformer baselines. We find that LLMs' improved document-translation performance compared to encoder-decoder models is not reflected in pronoun translation performance. Our analysis highlight the need for context-aware finetuning of LLMs with a focus on relevant parts of the context to improve their reliability for document-level translation.</article>","contentLength":1112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sliding Puzzles Gym: A Scalable Benchmark for State Representation in Visual Reinforcement Learning","url":"https://arxiv.org/abs/2410.14038","date":1739768400,"author":"","guid":1077,"unread":true,"content":"<article>arXiv:2410.14038v3 Announce Type: replace \nAbstract: Learning effective visual representations enables agents to extract meaningful information from raw sensory inputs, which is essential for generalizing across different tasks. However, evaluating representation learning separately from policy learning remains a challenge with most reinforcement learning (RL) benchmarks. To address this gap, we introduce the Sliding Puzzles Gym (SPGym), a novel benchmark that reimagines the classic 8-tile puzzle with a visual observation space of images sourced from arbitrarily large datasets. SPGym provides precise control over representation complexity through visual diversity, allowing researchers to systematically scale the representation learning challenge while maintaining consistent environment dynamics. Despite the apparent simplicity of the task, our experiments with both model-free and model-based RL algorithms reveal fundamental limitations in current methods. As we increase visual diversity by expanding the pool of possible images, all tested algorithms show significant performance degradation, with even state-of-the-art methods struggling to generalize across different visual inputs while maintaining consistent puzzle-solving capabilities. These results highlight critical gaps in visual representation learning for RL and provide clear directions for improving robustness and generalization in decision-making systems.</article>","contentLength":1436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Kuramoto Oscillatory Neurons","url":"https://arxiv.org/abs/2410.13821","date":1739768400,"author":"","guid":1078,"unread":true,"content":"<article>arXiv:2410.13821v2 Announce Type: replace \nAbstract: It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations. Code: https://github.com/autonomousvision/akorn Project page: https://github.com/takerum/akorn_project_page</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling","url":"https://arxiv.org/abs/2410.13610","date":1739768400,"author":"","guid":1079,"unread":true,"content":"<article>arXiv:2410.13610v2 Announce Type: replace \nAbstract: Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual's health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.</article>","contentLength":1503,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs","url":"https://arxiv.org/abs/2410.13502","date":1739768400,"author":"","guid":1080,"unread":true,"content":"<article>arXiv:2410.13502v3 Announce Type: replace \nAbstract: Large language models (LLMs) can solve arithmetic word problems with high accuracy, but little is known about how well they generalize to more complex problems. This is difficult to study, as (i) much of the available evaluation data has already been seen by the most capable models during training, and (ii) existing benchmarks do not capture how problem proofs may be arbitrarily complex in various ways. In this paper, we present a data-generation framework for evaluating LLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP. MathGAP generates problem statements and chain-of-thought reasoning traces according to specifications about their arithmetic proof structure, enabling systematic studies on easy-to-hard generalization with respect to complexity of proof trees. Using MathGAP, we find that LLMs show a significant decrease in performance as proofs get deeper and wider. This effect is more pronounced in complex, nonlinear proof structures, which are challenging even for the most capable models. The models are also sensitive to simple changes in sentence ordering. However, they remain capable of solving some complex problems, suggesting that reasoning generalization is noisy.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Federated Temporal Graph Clustering","url":"https://arxiv.org/abs/2410.12343","date":1739768400,"author":"","guid":1081,"unread":true,"content":"<article>arXiv:2410.12343v2 Announce Type: replace \nAbstract: Temporal graph clustering is a complex task that involves discovering meaningful structures in dynamic graphs where relationships and entities change over time. Existing methods typically require centralized data collection, which poses significant privacy and communication challenges. In this work, we introduce a novel Federated Temporal Graph Clustering (FTGC) framework that enables decentralized training of graph neural networks (GNNs) across multiple clients, ensuring data privacy throughout the process. Our approach incorporates a temporal aggregation mechanism to effectively capture the evolution of graph structures over time and a federated optimization strategy to collaboratively learn high-quality clustering representations. By preserving data privacy and reducing communication overhead, our framework achieves competitive performance on temporal graph datasets, making it a promising solution for privacy-sensitive, real-world applications involving dynamic data.</article>","contentLength":1037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CATCH: Channel-Aware multivariate Time Series Anomaly Detection via Frequency Patching","url":"https://arxiv.org/abs/2410.12261","date":1739768400,"author":"","guid":1082,"unread":true,"content":"<article>arXiv:2410.12261v2 Announce Type: replace \nAbstract: Anomaly detection in multivariate time series is challenging as heterogeneous subsequence anomalies may occur. Reconstruction-based methods, which focus on learning normal patterns in the frequency domain to detect diverse abnormal subsequences, achieve promising results, while still falling short on capturing fine-grained frequency characteristics and channel correlations. To contend with the limitations, we introduce CATCH, a framework based on frequency patching. We propose to patchify the frequency domain into frequency bands, which enhances its ability to capture fine-grained frequency characteristics. To perceive appropriate channel correlations, we propose a Channel Fusion Module (CFM), which features a patch-wise mask generator and a masked-attention mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM is encouraged to iteratively discover appropriate patch-wise channel correlations, and to cluster relevant channels while isolating adverse effects from irrelevant channels. Extensive experiments on 10 real-world datasets and 12 synthetic datasets demonstrate that CATCH achieves state-of-the-art performance. We make our code and datasets available at https://github.com/decisionintelligence/CATCH.</article>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DR-MPC: Deep Residual Model Predictive Control for Real-world Social Navigation","url":"https://arxiv.org/abs/2410.10646","date":1739768400,"author":"","guid":1083,"unread":true,"content":"<article>arXiv:2410.10646v2 Announce Type: replace \nAbstract: How can a robot safely navigate around people with complex motion patterns? Deep Reinforcement Learning (DRL) in simulation holds some promise, but much prior work relies on simulators that fail to capture the nuances of real human motion. Thus, we propose Deep Residual Model Predictive Control (DR-MPC) to enable robots to quickly and safely perform DRL from real-world crowd navigation data. By blending MPC with model-free DRL, DR-MPC overcomes the DRL challenges of large data requirements and unsafe initial behavior. DR-MPC is initialized with MPC-based path tracking, and gradually learns to interact more effectively with humans. To further accelerate learning, a safety component estimates out-of-distribution states to guide the robot away from likely collisions. In simulation, we show that DR-MPC substantially outperforms prior work, including traditional DRL and residual DRL models. Hardware experiments show our approach successfully enables a robot to navigate a variety of crowded situations with few errors using less than 4 hours of training data.</article>","contentLength":1121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model-Based Privacy-Preserving Knowledge Transfer for Large Language Models","url":"https://arxiv.org/abs/2410.10481","date":1739768400,"author":"","guid":1084,"unread":true,"content":"<article>arXiv:2410.10481v2 Announce Type: replace \nAbstract: As large language models (LLMs) become more prevalent, effectively utilizing domain-specific knowledge while ensuring privacy has become critical. Existing methods often struggle to balance utility and privacy. For instance, retrieval-augmented generation (RAG) enables LLMs to access domain-specific knowledge but compromises the privacy of sensitive data. On the other hand, differentially private data synthesis techniques offer strong privacy guarantees but often result in poor utility. To address this challenge, we propose Llamdex, a novel framework that enhances LLMs using only models trained on domain-specific data, integrated into LLMs through carefully designed connection modules. Our approach significantly enhances the accuracy of domain-specific tasks, achieving up to a 26% accuracy improvement compared to state-of-the-art data synthesis methods under the same differential privacy constraints. Experimental results show that Llamdex not only improves the accuracy of LLM responses but also maintains comparable inference efficiency to the original LLM, highlighting its potential for real applications.</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Approach to Routing and Cascading for LLMs","url":"https://arxiv.org/abs/2410.10347","date":1739768400,"author":"","guid":1085,"unread":true,"content":"<article>arXiv:2410.10347v2 Announce Type: replace \nAbstract: The availability of a wide range of large language models (LLMs) embedded in various agentic systems has significantly increased the potential of model selection strategies to improve the cost-performance tradeoff. Existing strategies involve either routing, where a single model is chosen per query, or cascading, which sequentially runs increasingly larger models until a satisfactory answer is found. However, current approaches face three key limitations: they (1) lack formal proofs of optimality, (2) fail to identify the conditions under which these strategies are most effective to improve the cost-performance tradeoff, and (3) are unable to combine both paradigms for further improvements. To address these issues, we first derive a novel optimal strategy for cascading and prove the optimality of an existing routing strategy. Further, we propose cascade routing, a unified framework that integrates routing and cascading into a theoretically optimal strategy. Through our analysis, we identify good quality estimators as the critical factor for the success of model selection paradigms. Finally, in our experiments, we show that cascade routing consistently outperforms the individual approaches by a large margin and we analyze quality estimators to determine when routing and/or cascading are useful paradigms for model selection.</article>","contentLength":1397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning","url":"https://arxiv.org/abs/2410.09536","date":1739768400,"author":"","guid":1086,"unread":true,"content":"<article>arXiv:2410.09536v3 Announce Type: replace \nAbstract: This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TinyEmo: Scaling down Emotional Reasoning via Metric Projection","url":"https://arxiv.org/abs/2410.07062","date":1739768400,"author":"","guid":1087,"unread":true,"content":"<article>arXiv:2410.07062v4 Announce Type: replace \nAbstract: This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems. We release code, models, and dataset at https://github.com/ggcr/TinyEmo</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning","url":"https://arxiv.org/abs/2410.05894","date":1739768400,"author":"","guid":1088,"unread":true,"content":"<article>arXiv:2410.05894v2 Announce Type: replace \nAbstract: In the realm of computational physics, an enduring topic is the numerical solutions to partial differential equations (PDEs). Recently, the attention of researchers has shifted towards Neural Operator methods, renowned for their capability to approximate ``operators'' -- mappings from functions to functions. Despite the universal approximation theorem within neural operators, ensuring error bounds often requires employing numerous Fourier layers. However, what about lightweight models? In response to this question, we introduce DimOL (Dimension-aware Operator Learning), drawing insights from dimensional analysis. To implement DimOL, we propose the ProdLayer, which can be seamlessly integrated into FNO-based and Transformer-based PDE solvers, enhancing their ability to handle sum-of-products structures inherent in many physical systems. Empirically, DimOL models achieve up to 48% performance gain within the PDE datasets. Furthermore, by analyzing Fourier components' weights, we can symbolically discern the physical significance of each term. This sheds light on the opaque nature of neural networks, unveiling underlying physical principles.</article>","contentLength":1209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?","url":"https://arxiv.org/abs/2410.05584","date":1739768400,"author":"","guid":1089,"unread":true,"content":"<article>arXiv:2410.05584v5 Announce Type: replace \nAbstract: Reward Models (RMs) are crucial for aligning language models with human preferences. Currently, the evaluation of RMs depends on measuring accuracy against a validation set of manually annotated preference data. Although this method is straightforward and widely adopted, the relationship between RM accuracy and downstream policy performance remains under-explored. In this work, we conduct experiments in a synthetic setting to investigate how differences in RM measured by accuracy translate into gaps in optimized policy performance. Our findings reveal that while there is a weak positive correlation between accuracy and downstream performance, policies optimized towards RMs with similar accuracy can exhibit quite different performance. Moreover, we discover that the way of measuring accuracy significantly impacts its ability to predict the final policy performance. Through the lens of the Regressional Goodhart effect, we recognize that accuracy, when used for measuring RM quality, can fail to fully capture the potential RM overoptimization. This underscores the inadequacy of relying solely on accuracy to reflect their impact on policy optimization.</article>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models","url":"https://arxiv.org/abs/2410.04634","date":1739768400,"author":"","guid":1090,"unread":true,"content":"<article>arXiv:2410.04634v2 Announce Type: replace \nAbstract: Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose Concept2Concept, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. A demo is available at https://tinyurl.com/Concept2ConceptDemo.</article>","contentLength":1073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A scalable, gradient-stable approach to multi-step, nonlinear system identification using first-order methods","url":"https://arxiv.org/abs/2410.03544","date":1739768400,"author":"","guid":1091,"unread":true,"content":"<article>arXiv:2410.03544v2 Announce Type: replace \nAbstract: This paper presents three main contributions to the field of multi-step system identification. First, drawing inspiration from Neural Network (NN) training, it introduces a tool for solving identification problems by leveraging first-order optimization and Automatic Differentiation (AD). The proposed method exploits gradients with respect to the parameters to be identified and leverages Linear Parameter-Varying (LPV) sensitivity equations to model gradient evolution. Second, it demonstrates that the computational complexity of the proposed method is linear in both the multi-step horizon length and the parameter size, ensuring scalability for large identification problems. Third, it formally addresses the \"exploding gradient\" issue: via a stability analysis of the LPV equations, it derives conditions for a reliable and efficient optimization and identification process for dynamical systems. Simulation results indicate that the proposed method is both effective and efficient, making it a promising tool for future research and applications in nonlinear system identification and non-convex optimization.</article>","contentLength":1169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spatial-aware decision-making with ring attractors in reinforcement learning systems","url":"https://arxiv.org/abs/2410.03119","date":1739768400,"author":"","guid":1092,"unread":true,"content":"<article>arXiv:2410.03119v2 Announce Type: replace \nAbstract: This paper explores the integration of ring attractors, a mathematical model inspired by neural circuit dynamics, into the Reinforcement Learning (RL) action selection process. Serving as specialized brain-inspired structures that encode spatial information and uncertainty, ring attractors offer a biologically plausible mechanism to improve learning speed and accuracy in RL. They do so by explicitly encoding the action space, facilitating the organization of neural activity, and enabling the distribution of spatial representations across the neural network in the context of Deep Reinforcement Learning (DRL). For example, preserving the continuity between rotation angles in robotic control or adjacency between tactical moves in game-like environments. The application of ring attractors in the action selection process involves mapping actions to specific locations on the ring and decoding the selected action based on neural activity. We investigate the application of ring attractors by both building an exogenous model and integrating them as part of DRL agents. Our approach significantly improves state-of-the-art performance on the Atari 100k benchmark, achieving a 53\\% increase in performance across selected state-of-the-art baselines. Codebase available at https://anonymous.4open.science/r/RA_RL-8026.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DomainLynx: Leveraging Large Language Models for Enhanced Domain Squatting Detection","url":"https://arxiv.org/abs/2410.02095","date":1739768400,"author":"","guid":1093,"unread":true,"content":"<article>arXiv:2410.02095v3 Announce Type: replace \nAbstract: Domain squatting poses a significant threat to Internet security, with attackers employing increasingly sophisticated techniques. This study introduces DomainLynx, an innovative compound AI system leveraging Large Language Models (LLMs) for enhanced domain squatting detection. Unlike existing methods focusing on predefined patterns for top-ranked domains, DomainLynx excels in identifying novel squatting techniques and protecting less prominent brands. The system's architecture integrates advanced data processing, intelligent domain pairing, and LLM-powered threat assessment. Crucially, DomainLynx incorporates specialized components that mitigate LLM hallucinations, ensuring reliable and context-aware detection. This approach enables efficient analysis of vast security data from diverse sources, including Certificate Transparency logs, Passive DNS records, and zone files. Evaluated on a curated dataset of 1,649 squatting domains, DomainLynx achieved 94.7\\% accuracy using Llama-3-70B. In a month-long real-world test, it detected 34,359 squatting domains from 2.09 million new domains, outperforming baseline methods by 2.5 times. This research advances Internet security by providing a versatile, accurate, and adaptable tool for combating evolving domain squatting threats. DomainLynx's approach paves the way for more robust, AI-driven cybersecurity solutions, enhancing protection for a broader range of online entities and contributing to a safer digital ecosystem.</article>","contentLength":1536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conformal Prediction Sets Can Cause Disparate Impact","url":"https://arxiv.org/abs/2410.01888","date":1739768400,"author":"","guid":1094,"unread":true,"content":"<article>arXiv:2410.01888v2 Announce Type: replace \nAbstract: Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact.</article>","contentLength":1063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lines of Thought in Large Language Models","url":"https://arxiv.org/abs/2410.01545","date":1739768400,"author":"","guid":1095,"unread":true,"content":"<article>arXiv:2410.01545v3 Announce Type: replace \nAbstract: Large Language Models achieve next-token prediction by transporting a vectorized piece of text (prompt) across an accompanying embedding space under the action of successive transformer layers. The resulting high-dimensional trajectories realize different contextualization, or 'thinking', steps, and fully determine the output probability distribution. We aim to characterize the statistical properties of ensembles of these 'lines of thought.' We observe that independent trajectories cluster along a low-dimensional, non-Euclidean manifold, and that their path can be well approximated by a stochastic equation with few parameters extracted from data. We find it remarkable that the vast complexity of such large models can be reduced to a much simpler form, and we reflect on implications.</article>","contentLength":846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Stochastic Dynamics from Snapshots through Regularized Unbalanced Optimal Transport","url":"https://arxiv.org/abs/2410.00844","date":1739768400,"author":"","guid":1096,"unread":true,"content":"<article>arXiv:2410.00844v2 Announce Type: replace \nAbstract: Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data. Theoretically, we explore the connections between the RUOT and Schr\\\"odinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: https://github.com/zhenyiizhang/DeepRUOT.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models","url":"https://arxiv.org/abs/2409.19471","date":1739768400,"author":"","guid":1097,"unread":true,"content":"<article>arXiv:2409.19471v2 Announce Type: replace \nAbstract: Despite significant advancements in large language models (LLMs) that enhance robot agents' understanding and execution of natural language (NL) commands, ensuring the agents adhere to user-specified constraints remains challenging, particularly for complex commands and long-horizon tasks. To address this challenge, we present three key insights, equivalence voting, constrained decoding, and domain-specific fine-tuning, which significantly enhance LLM planners' capability in handling complex tasks. Equivalence voting ensures consistency by generating and sampling multiple Linear Temporal Logic (LTL) formulas from NL commands, grouping equivalent LTL formulas, and selecting the majority group of formulas as the final LTL formula. Constrained decoding then uses the generated LTL formula to enforce the autoregressive inference of plans, ensuring the generated plans conform to the LTL. Domain-specific fine-tuning customizes LLMs to produce safe and efficient plans within specific task domains. Our approach, Safe Efficient LLM Planner (SELP), combines these insights to create LLM planners to generate plans adhering to user commands with high confidence. We demonstrate the effectiveness and generalizability of SELP across different robot agents and tasks, including drone navigation and robot manipulation. For drone navigation tasks, SELP outperforms state-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks conforming to NL commands) and by 19.8% in plan efficiency. For robot manipulation tasks, SELP achieves 20.4% improvement in safety rate. Our datasets for evaluating NL-to-LTL and robot task planning will be released in github.com/lt-asset/selp.</article>","contentLength":1737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Strategy Representation for Imitation Learning in Multi-Agent Games","url":"https://arxiv.org/abs/2409.19363","date":1739768400,"author":"","guid":1098,"unread":true,"content":"<article>arXiv:2409.19363v2 Announce Type: replace \nAbstract: The offline datasets for imitation learning (IL) in multi-agent games typically contain player trajectories exhibiting diverse strategies, which necessitate measures to prevent learning algorithms from acquiring undesirable behaviors. Learning representations for these trajectories is an effective approach to depicting the strategies employed by each demonstrator. However, existing learning strategies often require player identification or rely on strong assumptions, which are not appropriate for multi-agent games. Therefore, in this paper, we introduce the Strategy Representation for Imitation Learning (STRIL) framework, which (1) effectively learns strategy representations in multi-agent games, (2) estimates proposed indicators based on these representations, and (3) filters out sub-optimal data using the indicators. STRIL is a plug-in method that can be integrated into existing IL algorithms. We demonstrate the effectiveness of STRIL across competitive multi-agent scenarios, including Two-player Pong, Limit Texas Hold'em, and Connect Four. Our approach successfully acquires strategy representations and indicators, thereby identifying dominant trajectories and significantly enhancing existing IL performance across these environments.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent","url":"https://arxiv.org/abs/2409.18997","date":1739768400,"author":"","guid":1099,"unread":true,"content":"<article>arXiv:2409.18997v2 Announce Type: replace \nAbstract: Propaganda plays a critical role in shaping public opinion and fueling disinformation. While existing research primarily focuses on identifying propaganda techniques, it lacks the ability to capture the broader motives and the impacts of such content. To address these challenges, we introduce propainsight, a conceptual framework grounded in foundational social science research, which systematically dissects propaganda into techniques, arousal appeals, and underlying intent. propainsight offers a more granular understanding of how propaganda operates across different contexts. Additionally, we present propagaze, a novel dataset that combines human-annotated data with high-quality synthetic data generated through a meticulously designed pipeline. Our experiments show that off-the-shelf LLMs struggle with propaganda analysis, but training with propagaze significantly improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span IoU in technique identification and 66.2% higher BertScore in appeal analysis compared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited human-annotated data in data-sparse and cross-domain scenarios, showing its potential for comprehensive and generalizable propaganda analysis.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base","url":"https://arxiv.org/abs/2409.18472","date":1739768400,"author":"","guid":1100,"unread":true,"content":"<article>arXiv:2409.18472v3 Announce Type: replace \nAbstract: URIEL is a knowledge base offering geographical, phylogenetic, and typological vector representations for 7970 languages. It includes distance measures between these vectors for 4005 languages, which are accessible via the lang2vec tool. Despite being frequently cited, URIEL is limited in terms of linguistic inclusion and overall usability. To tackle these challenges, we introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses these limitations. In addition to expanding typological feature coverage for 2898 languages, URIEL+ improves the user experience with robust, customizable distance calculations to better suit the needs of users. These upgrades also offer competitive performance on downstream tasks and provide distances that better align with linguistic distance studies.</article>","contentLength":857,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale","url":"https://arxiv.org/abs/2409.17115","date":1739768400,"author":"","guid":1101,"unread":true,"content":"<article>arXiv:2409.17115v2 Announce Type: replace \nAbstract: Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb, FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training. We are open-sourcing ProX with &gt;500B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX</article>","contentLength":1951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling","url":"https://arxiv.org/abs/2409.15156","date":1739768400,"author":"","guid":1102,"unread":true,"content":"<article>arXiv:2409.15156v2 Announce Type: replace \nAbstract: The remarkable success of large language pretraining and the discovery of scaling laws signify a paradigm shift in machine learning. Notably, the primary objective has evolved from minimizing generalization error to reducing approximation error, and the most effective strategy has transitioned from regularization (in a broad sense) to scaling up models. This raises a critical question:\n  Do the established principles that proved successful in the generalization-centric era remain valid in this new era of scaling?\n  This paper examines several influential regularization-based principles that may no longer hold true in the scaling-centric, large language model (LLM) era. These principles include explicit L2 regularization and implicit regularization through small batch sizes and large learning rates. Additionally, we identify a new phenomenon termed ``scaling law crossover,'' where two scaling curves intersect at a certain scale, implying that methods effective at smaller scales may not generalize to larger ones. Together, these observations highlight two fundamental questions within this new paradigm:\n  $\\bullet$ Guiding Principles for Scaling: If regularization is no longer the primary guiding principle for model design, what new principles are emerging to guide scaling?\n  $\\bullet$ Model Comparison at Scale: How to reliably and effectively compare models at the scale where only a single experiment is feasible?</article>","contentLength":1487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LATTE: Improving Latex Recognition for Tables and Formulae with Iterative Refinement","url":"https://arxiv.org/abs/2409.14201","date":1739768400,"author":"","guid":1103,"unread":true,"content":"<article>arXiv:2409.14201v2 Announce Type: replace \nAbstract: Portable Document Format (PDF) files are dominantly used for storing and disseminating scientific research, legal documents, and tax information. LaTeX is a popular application for creating PDF documents. Despite its advantages, LaTeX is not WYSWYG -- what you see is what you get, i.e., the LaTeX source and rendered PDF images look drastically different, especially for formulae and tables. This gap makes it hard to modify or export LaTeX sources for formulae and tables from PDF images, and existing work is still limited. First, prior work generates LaTeX sources in a single iteration and struggles with complex LaTeX formulae. Second, existing work mainly recognizes and extracts LaTeX sources for formulae; and is incapable or ineffective for tables. This paper proposes LATTE, the first iterative refinement framework for LaTeX recognition. Specifically, we propose delta-view as feedback, which compares and pinpoints the differences between a pair of rendered images of the extracted LaTeX source and the expected correct image. Such delta-view feedback enables our fault localization model to localize the faulty parts of the incorrect recognition more accurately and enables our LaTeX refinement model to repair the incorrect extraction more accurately. LATTE improves the LaTeX source extraction accuracy of both LaTeX formulae and tables, outperforming existing techniques as well as GPT-4V by at least 7.03% of exact match, with a success refinement rate of 46.08% (formula) and 25.51% (table).</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Representations and Interventions in Time Series Foundation Models","url":"https://arxiv.org/abs/2409.12915","date":1739768400,"author":"","guid":1104,"unread":true,"content":"<article>arXiv:2409.12915v3 Announce Type: replace \nAbstract: Time series foundation models (TSFMs) promise to be powerful tools for a wide range of applications. However, their internal representations and learned concepts are still not well understood. In this study, we investigate the structure and redundancy of representations across various TSFMs, examining the self-similarity of model layers within and across different model sizes. This analysis reveals block-like redundancy in the representations, which can be utilized for informed pruning to improve inference speed and efficiency. Additionally, we explore the concepts learned by these models - such as periodicity and trends - and how these can be manipulated through latent space steering to influence model behavior. Our experiments show that steering interventions can introduce new features, e.g., adding periodicity or trends to signals that initially lacked them. These findings underscore the value of representational analysis for optimizing models and demonstrate how conceptual steering offers new possibilities for more controlled and efficient time series analysis with TSFMs.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference","url":"https://arxiv.org/abs/2409.12467","date":1739768400,"author":"","guid":1105,"unread":true,"content":"<article>arXiv:2409.12467v2 Announce Type: replace \nAbstract: Surgical phase recognition is critical for assisting surgeons in understanding surgical videos. Existing studies focused more on online surgical phase recognition, by leveraging preceding frames to predict the current frame. Despite great progress, they formulated the task as a series of frame-wise classification, which resulted in a lack of global context of the entire procedure and incoherent predictions. Moreover, besides online analysis, accurate offline surgical phase recognition is also in significant clinical need for retrospective analysis, and existing online algorithms do not fully analyze the entire video, thereby limiting accuracy in offline analysis. To overcome these challenges and enhance both online and offline inference capabilities, we propose a universal Surgical Phase Localization Network, named SurgPLAN++, with the principle of temporal detection. To ensure a global understanding of the surgical procedure, we devise a phase localization strategy for SurgPLAN++ to predict phase segments across the entire video through phase proposals. For online analysis, to generate high-quality phase proposals, SurgPLAN++ incorporates a data augmentation strategy to extend the streaming video into a pseudo-complete video through mirroring, center-duplication, and down-sampling. For offline analysis, SurgPLAN++ capitalizes on its global phase prediction framework to continuously refine preceding predictions during each online inference step, thereby significantly improving the accuracy of phase recognition. We perform extensive experiments to validate the effectiveness, and our SurgPLAN++ achieves remarkable performance in both online and offline modes, which outperforms state-of-the-art methods. The source code is available at https://github.com/franciszchen/SurgPLAN-Plus.</article>","contentLength":1861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Overcoming Ambient Drift and Negative-Bias Temperature Instability in Foundry Carbon Nanotube Transistors","url":"https://arxiv.org/abs/2409.11297","date":1739768400,"author":"","guid":1106,"unread":true,"content":"<article>arXiv:2409.11297v3 Announce Type: replace \nAbstract: Back-end-of-line (BEOL) logic integration is emerging as a complementary scaling path to supplement front-end-of-line (FEOL) Silicon. Among various options for BEOL logic, Carbon Nanotube Field-Effect Transistors (CNFETs) have been integrated within commercial silicon foundries, and complex CNFET circuits (e.g., RISC-V core, SRAM arrays) have been demonstrated. However, there lacks comprehensive studies that analyze the ambient drift (i.e., air-stability) and reliability of CNFETs. Here, for the first time, we thoroughly characterize and demonstrate how to overcome ambient drift and negative bias temperature instability (NBTI) in CNFETs using the following techniques: (1) Silicon Nitride encapsulation to limit ambient atmosphere induced threshold voltage shift (~8x reduction of median VT shift over 90 days) and (2) AC/pulsed operation to significantly improve CNFET NBTI vs. DC operation across a wide frequency range (e.g., 20% duty cycle AC operation at 10 MHz could extend CNFET NBTI time-to-failure by &gt;10000x vs. DC for a target VT shift tolerance &lt; 100 mV with gate stress bias VGS,stress = -1.2 V at 125 C).</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Steinmetz Neural Networks for Complex-Valued Data","url":"https://arxiv.org/abs/2409.10075","date":1739768400,"author":"","guid":1107,"unread":true,"content":"<article>arXiv:2409.10075v3 Announce Type: replace \nAbstract: We introduce a new approach to processing complex-valued data using DNNs consisting of parallel real-valued subnetworks with coupled outputs. Our proposed class of architectures, referred to as Steinmetz Neural Networks, incorporates multi-view learning to construct more interpretable representations in the latent space. Moreover, we present the Analytic Neural Network, which incorporates a consistency penalty that encourages analytic signal representations in the latent space of the Steinmetz neural network. This penalty enforces a deterministic and orthogonal relationship between the real and imaginary components. Using an information-theoretic construction, we demonstrate that the generalization gap upper bound posited by the analytic neural network is lower than that of the general class of Steinmetz neural networks. Our numerical experiments depict the improved performance and robustness to additive noise, afforded by our proposed networks on benchmark datasets and synthetic examples.</article>","contentLength":1057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AACessTalk: Fostering Communication between Minimally Verbal Autistic Children and Parents with Contextual Guidance and Card Recommendation","url":"https://arxiv.org/abs/2409.09641","date":1739768400,"author":"","guid":1108,"unread":true,"content":"<article>arXiv:2409.09641v4 Announce Type: replace \nAbstract: As minimally verbal autistic (MVA) children communicate with parents through few words and nonverbal cues, parents often struggle to encourage their children to express subtle emotions and needs and to grasp their nuanced signals. We present AACessTalk, a tablet-based, AI-mediated communication system that facilitates meaningful exchanges between an MVA child and a parent. AACessTalk provides real-time guides to the parent to engage the child in conversation and, in turn, recommends contextual vocabulary cards to the child. Through a two-week deployment study with 11 MVA child-parent dyads, we examine how AACessTalk fosters everyday conversation practice and mutual engagement. Our findings show high engagement from all dyads, leading to increased frequency of conversation and turn-taking. AACessTalk also encouraged parents to explore their own interaction strategies and empowered the children to have more agency in communication. We discuss the implications of designing technologies for balanced communication dynamics in parent-MVA child interaction.</article>","contentLength":1119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent","url":"https://arxiv.org/abs/2409.08770","date":1739768400,"author":"","guid":1109,"unread":true,"content":"<article>arXiv:2409.08770v4 Announce Type: replace \nAbstract: The performance of mini-batch stochastic gradient descent (SGD) strongly depends on setting the batch size and learning rate to minimize the empirical loss in training the deep neural network. In this paper, we present theoretical analyses of mini-batch SGD with four schedulers: (i) constant batch size and decaying learning rate scheduler, (ii) increasing batch size and decaying learning rate scheduler, (iii) increasing batch size and increasing learning rate scheduler, and (iv) increasing batch size and warm-up decaying learning rate scheduler. We show that mini-batch SGD using scheduler (i) does not always minimize the expectation of the full gradient norm of the empirical loss, whereas it does using any of schedulers (ii), (iii), and (iv). Furthermore, schedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides numerical results of supporting analyses showing that using scheduler (iii) or (iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or (ii).</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contrastive Federated Learning with Tabular Data Silos","url":"https://arxiv.org/abs/2409.06123","date":1739768400,"author":"","guid":1110,"unread":true,"content":"<article>arXiv:2409.06123v2 Announce Type: replace \nAbstract: Learning from vertical partitioned data silos is challenging due to the segmented nature of data, sample misalignment, and strict privacy concerns. Federated learning has been proposed as a solution. However, sample misalignment across silos often hinders optimal model performance and suggests data sharing within the model, which breaks privacy. Our proposed solution is Contrastive Federated Learning with Tabular Data Silos (CFL), which offers a solution for data silos with sample misalignment without the need for sharing original or representative data to maintain privacy. CFL begins with local acquisition of contrastive representations of the data within each silo and aggregates knowledge from other silos through the federated learning algorithm. Our experiments demonstrate that CFL solves the limitations of existing algorithms for data silos and outperforms existing tabular contrastive learning. CFL provides performance improvements without loosening privacy.</article>","contentLength":1029,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HelmetPoser: A Helmet-Mounted IMU Dataset for Data-Driven Estimation of Human Head Motion in Diverse Conditions","url":"https://arxiv.org/abs/2409.05006","date":1739768400,"author":"","guid":1111,"unread":true,"content":"<article>arXiv:2409.05006v2 Announce Type: replace \nAbstract: Helmet-mounted wearable positioning systems are crucial for enhancing safety and facilitating coordination in industrial, construction, and emergency rescue environments. These systems, including LiDAR-Inertial Odometry (LIO) and Visual-Inertial Odometry (VIO), often face challenges in localization due to adverse environmental conditions such as dust, smoke, and limited visual features. To address these limitations, we propose a novel head-mounted Inertial Measurement Unit (IMU) dataset with ground truth, aimed at advancing data-driven IMU pose estimation. Our dataset captures human head motion patterns using a helmet-mounted system, with data from ten participants performing various activities. We explore the application of neural networks, specifically Long Short-Term Memory (LSTM) and Transformer networks, to correct IMU biases and improve localization accuracy. Additionally, we evaluate the performance of these methods across different IMU data window dimensions, motion patterns, and sensor types. We release a publicly available dataset, demonstrate the feasibility of advanced neural network approaches for helmet-based localization, and provide evaluation metrics to establish a baseline for future studies in this field. Data and code can be found at https://lqiutong.github.io/HelmetPoser.github.io/.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection","url":"https://arxiv.org/abs/2409.04796","date":1739768400,"author":"","guid":1112,"unread":true,"content":"<article>arXiv:2409.04796v2 Announce Type: replace \nAbstract: Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods. Code is released at https://github.com/AuroraZengfh/Local-Prompt.</article>","contentLength":1670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Willingness to Read AI-Generated News Is Not Driven by Their Perceived Quality","url":"https://arxiv.org/abs/2409.03500","date":1739768400,"author":"","guid":1113,"unread":true,"content":"<article>arXiv:2409.03500v3 Announce Type: replace \nAbstract: The advancement of artificial intelligence has led to its application in many areas, including news media, which makes it crucial to understand public reception of AI-generated news. This preregistered study investigates (i) the perceived quality of AI-assisted and AI-generated versus human-generated news articles, (ii) whether disclosure of AI's involvement in generating these news articles influences engagement with them, and (iii) whether such awareness affects the willingness to read AI-generated articles in the future. We conducted a survey experiment with 599 Swiss participants, who evaluated the credibility, readability, and expertise of news articles either written by journalists (control group), rewritten by AI (AI-assisted group), or entirely written by AI (AI-generated group). Our results indicate that all articles were perceived to be of equal quality. When participants in the treatment groups were subsequently made aware of AI's role, they expressed a higher willingness to continue reading the articles than participants in the control group. However, they were not more willing to read AI-generated news in the future. These results suggest that aversion to AI usage in news media is not primarily rooted in a perceived lack of quality, and that by disclosing using AI, journalists could induce more short-term engagement.</article>","contentLength":1404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review","url":"https://arxiv.org/abs/2409.02375","date":1739768400,"author":"","guid":1114,"unread":true,"content":"<article>arXiv:2409.02375v4 Announce Type: replace \nAbstract: The recent advances in large language models (LLMs) have significantly expanded their applications across various fields such as language generation, summarization, and complex question answering. However, their application to privacy compliance and technical privacy reviews remains under-explored, raising critical concerns about their ability to adhere to global privacy standards and protect sensitive user data. This paper seeks to address this gap by providing a comprehensive case study evaluating LLMs' performance in privacy-related tasks such as privacy information extraction (PIE), legal and regulatory key point detection (KPD), and question answering (QA) with respect to privacy policies and data protection regulations. We introduce a Privacy Technical Review (PTR) framework, highlighting its role in mitigating privacy risks during the software development life-cycle. Through an empirical assessment, we investigate the capacity of several prominent LLMs, including BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks and technical privacy reviews. Our experiments benchmark the models across multiple dimensions, focusing on their precision, recall, and F1-scores in extracting privacy-sensitive information and detecting key regulatory compliance points. While LLMs show promise in automating privacy reviews and identifying regulatory discrepancies, significant gaps persist in their ability to fully comply with evolving legal standards. We provide actionable recommendations for enhancing LLMs' capabilities in privacy compliance, emphasizing the need for robust model improvements and better integration with legal and regulatory requirements. This study underscores the growing importance of developing privacy-aware LLMs that can both support businesses in compliance efforts and safeguard user privacy rights.</article>","contentLength":1918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Large Language Models Possess Sensitive to Sentiment?","url":"https://arxiv.org/abs/2409.02370","date":1739768400,"author":"","guid":1115,"unread":true,"content":"<article>arXiv:2409.02370v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) have recently displayed their extraordinary capabilities in language understanding. However, how to comprehensively assess the sentiment capabilities of LLMs continues to be a challenge. This paper investigates the ability of LLMs to detect and react to sentiment in text modal. As the integration of LLMs into diverse applications is on the rise, it becomes highly critical to comprehend their sensitivity to emotional tone, as it can influence the user experience and the efficacy of sentiment-driven tasks. We conduct a series of experiments to evaluate the performance of several prominent LLMs in identifying and responding appropriately to sentiments like positive, negative, and neutral emotions. The models' outputs are analyzed across various sentiment benchmarks, and their responses are compared with human evaluations. Our discoveries indicate that although LLMs show a basic sensitivity to sentiment, there are substantial variations in their accuracy and consistency, emphasizing the requirement for further enhancements in their training processes to better capture subtle emotional cues. Take an example in our findings, in some cases, the models might wrongly classify a strongly positive sentiment as neutral, or fail to recognize sarcasm or irony in the text. Such misclassifications highlight the complexity of sentiment analysis and the areas where the models need to be refined. Another aspect is that different LLMs might perform differently on the same set of data, depending on their architecture and training datasets. This variance calls for a more in-depth study of the factors that contribute to the performance differences and how they can be optimized.</article>","contentLength":1765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey","url":"https://arxiv.org/abs/2409.01980","date":1739768400,"author":"","guid":1116,"unread":true,"content":"<article>arXiv:2409.01980v3 Announce Type: replace \nAbstract: Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers.</article>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models","url":"https://arxiv.org/abs/2409.01584","date":1739768400,"author":"","guid":1117,"unread":true,"content":"<article>arXiv:2409.01584v2 Announce Type: replace \nAbstract: As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow. However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English. In addition, multilingual QA benchmarks that create datasets using machine translation have cultural differences and biases, remaining issues for use as evaluation tasks. To address these challenges, this study created an extended dataset in multiple languages without relying on machine translation. This dataset that takes into account nuances and country-specific phrases was then used to evaluate the generation explanation abilities of LVLMs. Furthermore, this study examined whether Instruction-Tuning in resource-rich English improves performance in other languages. Our findings indicate that LVLMs perform worse in languages other than English compared to English. In addition, it was observed that LVLMs struggle to effectively manage the knowledge learned from English data. Our dataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified, Practical, and Understandable Summary of Non-transactional Consistency Levels in Distributed Replication","url":"https://arxiv.org/abs/2409.01576","date":1739768400,"author":"","guid":1118,"unread":true,"content":"<article>arXiv:2409.01576v3 Announce Type: replace \nAbstract: We present a summary of practical non-transactional consistency levels in the context of distributed data replication. Unlike prior work, we build upon a simple Shared Object Pool (SOP) model and define consistency levels in a unified framework centered around the concept of ordering. This model naturally reflects modern cloud object storage services and is thus easy to understand. We show that a consistency level can be intuitively defined by specifying two types of constraints on the validity of orderings allowed by the level: convergence, which bounds the lineage shape of the ordering, and relationship, which bounds the relative positions between operations. We give examples of representative protocols and systems, and discuss their availability upper bound.</article>","contentLength":824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"High-order finite element methods for three-dimensional multicomponent convection-diffusion","url":"https://arxiv.org/abs/2408.17390","date":1739768400,"author":"","guid":1119,"unread":true,"content":"<article>arXiv:2408.17390v2 Announce Type: replace \nAbstract: We derive and analyze a broad class of finite element methods for numerically simulating the stationary, low Reynolds number flow of concentrated mixtures of several distinct chemical species in a common thermodynamic phase. The underlying partial differential equations that we discretize are the Stokes$\\unicode{x2013}$Onsager$\\unicode{x2013}$Stefan$\\unicode{x2013}$Maxwell (SOSM) equations, which model bulk momentum transport and multicomponent diffusion within ideal and non-ideal mixtures. Unlike previous approaches, the methods are straightforward to implement in two and three spatial dimensions, and allow for high-order finite element spaces to be employed. The key idea in deriving the discretization is to suitably reformulate the SOSM equations in terms of the species mass fluxes and chemical potentials, and discretize these unknown fields using stable $H(\\textrm{div}) \\unicode{x2013} L^2$ finite element pairs. We prove that the methods are convergent and yield a symmetric linear system for a Picard linearization of the SOSM equations, which staggers the updates for concentrations and chemical potentials. We also discuss how the proposed approach can be extended to the Newton linearization of the SOSM equations, which requires the simultaneous solution of mole fractions, chemical potentials, and other variables. Our theoretical results are supported by numerical experiments and we present an example of a physical application involving the microfluidic non-ideal mixing of hydrocarbons.</article>","contentLength":1566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Error-controlled non-additive interaction discovery in machine learning models","url":"https://arxiv.org/abs/2408.17016","date":1739768400,"author":"","guid":1120,"unread":true,"content":"<article>arXiv:2408.17016v2 Announce Type: replace \nAbstract: Machine learning (ML) models are powerful tools for detecting complex patterns within data, yet their \"black box\" nature limits their interpretability, hindering their use in critical domains like healthcare and finance. To address this challenge, interpretable ML methods have been developed to explain how features influence model predictions. However, these methods often focus on univariate feature importance, overlooking the complex interactions between features that ML models are capable of capturing. Recognizing this limitation, recent efforts have aimed to extend these methods to discover feature interactions, but existing approaches struggle with robustness and error control, especially under data perturbations. In this study, we introduce Diamond, a novel method for trustworthy feature interaction discovery. Diamond uniquely integrates the model-X knockoffs framework to control the false discovery rate (FDR), ensuring that the proportion of falsely discovered interactions remains low. A key innovation in Diamond is its non-additivity distillation procedure, which refines existing interaction importance measures to distill non-additive interaction effects, ensuring that FDR control is maintained. This approach addresses the limitations of off-the-shelf interaction measures, which, when used naively, can lead to inaccurate discoveries. Diamond's applicability spans a wide range of ML models, including deep neural networks, transformer models, tree-based models, and factorization-based models. Our empirical evaluations on both simulated and real datasets across various biomedical studies demonstrate Diamond's utility in enabling more reliable data-driven scientific discoveries. This method represents a significant step forward in the deployment of ML models for scientific innovation and hypothesis generation.</article>","contentLength":1897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network","url":"https://arxiv.org/abs/2408.10919","date":1739768400,"author":"","guid":1121,"unread":true,"content":"<article>arXiv:2408.10919v4 Announce Type: replace \nAbstract: In recent years, Wi-Fi sensing has garnered significant attention due to its numerous benefits, such as privacy protection, low cost, and penetration ability. Extensive research has been conducted in this field, focusing on areas such as gesture recognition, people identification, and fall detection. However, many data-driven methods encounter challenges related to domain shift, where the model fails to perform well in environments different from the training data. One major factor contributing to this issue is the limited availability of Wi-Fi sensing datasets, which makes models learn excessive irrelevant information and over-fit to the training set. Unfortunately, collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a challenging task. To address this problem, we propose CrossFi, a siamese network-based approach that excels in both in-domain scenario and cross-domain scenario, including few-shot, zero-shot scenarios, and even works in few-shot new-class scenario where testing set contains new categories. The core component of CrossFi is a sample-similarity calculation network called CSi-Net, which improves the structure of the siamese network by using an attention mechanism to capture similarity information, instead of simply calculating the distance or cosine similarity. Based on it, we develop an extra Weight-Net that can generate a template for each class, so that our CrossFi can work in different scenarios. Experimental results demonstrate that our CrossFi achieves state-of-the-art performance across various scenarios. In gesture recognition task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72% in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario, and 84.75% in one-shot new-class scenario. The code for our model is publicly available at https://github.com/RS2002/CrossFi.</article>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improved Community Detection using Stochastic Block Models","url":"https://arxiv.org/abs/2408.10464","date":1739768400,"author":"","guid":1122,"unread":true,"content":"<article>arXiv:2408.10464v2 Announce Type: replace \nAbstract: Community detection approaches resolve complex networks into smaller groups (communities) that are expected to be relatively edge-dense and well-connected. The stochastic block model (SBM) is one of several approaches used to uncover community structure in graphs. In this study, we demonstrate that SBM software applied to various real-world and synthetic networks produces poorly-connected to disconnected clusters. We present simple modifications to improve the connectivity of SBM clusters, and show that the modifications improve accuracy using simulated networks.</article>","contentLength":622,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HaSPeR: An Image Repository for Hand Shadow Puppet Recognition","url":"https://arxiv.org/abs/2408.10360","date":1739768400,"author":"","guid":1123,"unread":true,"content":"<article>arXiv:2408.10360v5 Announce Type: replace \nAbstract: Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of theatrical art and storytelling where hand shadows are projected onto flat surfaces to create illusions of living creatures. The skilled performers create these silhouettes by hand positioning, finger movements, and dexterous gestures to resemble shadows of animals and objects. Due to the lack of practitioners and a seismic shift in people's entertainment standards, this art form is on the verge of extinction. To facilitate its preservation and proliferate it to a wider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset consisting of 15,000 images of hand shadow puppets across 15 classes extracted from both professional and amateur hand shadow puppeteer clips. We provide a detailed statistical analysis of the dataset and employ a range of pretrained image classification models to establish baselines. Our findings show a substantial performance superiority of skip-connected convolutional models over attention-based transformer architectures. We also find that lightweight models, such as MobileNetV2, suited for mobile applications and embedded devices, perform comparatively well. We surmise that such low-latency architectures can be useful in developing ombromanie teaching tools, and we create a prototype application to explore this surmission. Keeping the best-performing model ResNet34 under the limelight, we conduct comprehensive feature-spatial, explainability, and error analyses to gain insights into its decision-making process. To the best of our knowledge, this is the first documented dataset and research endeavor to preserve this dying art for future generations, with computer vision approaches. Our code and data will be publicly available.</article>","contentLength":1825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation","url":"https://arxiv.org/abs/2408.08192","date":1739768400,"author":"","guid":1124,"unread":true,"content":"<article>arXiv:2408.08192v2 Announce Type: replace \nAbstract: Mean field games (MFGs) model interactions in large-population multi-agent systems through population distributions. Traditional learning methods for MFGs are based on fixed-point iteration (FPI), where policy updates and induced population distributions are computed separately and sequentially. However, FPI-type methods may suffer from inefficiency and instability due to potential oscillations caused by this forward-backward procedure. In this work, we propose a novel perspective that treats the policy and population as a unified parameter controlling the game dynamics. By applying stochastic parameter approximation to this unified parameter, we develop SemiSGD, a simple stochastic gradient descent (SGD)-type method, where an agent updates its policy and population estimates simultaneously and fully asynchronously. Building on this perspective, we further apply linear function approximation (LFA) to the unified parameter, resulting in the first population-aware LFA (PA-LFA) for learning MFGs on continuous state-action spaces. A comprehensive finite-time convergence analysis is provided for SemiSGD with PA-LFA, including its convergence to the equilibrium for linear MFGs -- a class of MFGs with a linear structure concerning the population -- under the standard contractivity condition, and to a neighborhood of the equilibrium under a more practical condition. We also characterize the approximation error for non-linear MFGs. We validate our theoretical findings with six experiments on three MFGs.</article>","contentLength":1572,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Expressway Ramp Merge Safety and Efficiency via Spatiotemporal Cooperative Control","url":"https://arxiv.org/abs/2408.08121","date":1739768400,"author":"","guid":1125,"unread":true,"content":"<article>arXiv:2408.08121v3 Announce Type: replace \nAbstract: In the context of autonomous driving on expressways, the issue of ensuring safe and efficient ramp merging remains a significant challenge. Existing systems often struggle to accurately assess the status and intentions of other vehicles, leading to a persistent occurrence of accidents despite efforts to maintain safe distances. This study proposes a novel spatiotemporal cooperative control approach integrating vehicle-road coordination to address this critical issue. A comprehensive methodology is developed, beginning with the calculation of safe distances under varying spatiotemporal conditions. This involves considering multiple factors, including vehicle speed differentials, positioning errors, and clock synchronization errors. Subsequently, an advanced vehicle conflict risk evaluation model is constructed. By incorporating collision acceleration and emergency acceleration as key parameters, this model offers a more accurate and detailed evaluation of potential risks during the ramp merging process. Based on the calculated safe distances and conflict risk evaluations, a mainline priority coordinated control method is formulated. This method enables the pre-planning of vehicle trajectories, effectively reducing conflicts among vehicles. Through rigorous simulations using diverse traffic volume and speed scenarios, the efficacy of the proposed strategy is validated. The results demonstrate remarkable improvements, with the average delay time reduced by an impressive 97.96% and fuel consumption decreased by 6.01%. These outcomes indicate that the proposed approach not only enhances the speed of vehicle merging but also significantly reduces latency and fuel consumption, thereby enhancing the overall performance of ramp merging operations.</article>","contentLength":1821,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Video Coding Meets Multimodal Large Language Models: A Unified Paradigm for Video Coding","url":"https://arxiv.org/abs/2408.08093","date":1739768400,"author":"","guid":1126,"unread":true,"content":"<article>arXiv:2408.08093v3 Announce Type: replace \nAbstract: Existing codecs are designed to eliminate intrinsic redundancies to create a compact representation for compression. However, strong external priors from Multimodal Large Language Models (MLLMs) have not been explicitly explored in video compression. Herein, we introduce a unified paradigm for Cross-Modality Video Coding (CMVC), which is a pioneering approach to explore multimodality representation and video generative models in video coding. Specifically, on the encoder side, we disentangle a video into spatial content and motion components, which are subsequently transformed into distinct modalities to achieve very compact representation by leveraging MLLMs. During decoding, previously encoded components and video generation models are leveraged to create multiple encoding-decoding modes that optimize video reconstruction quality for specific decoding requirements, including Text-Text-to-Video (TT2V) mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V) mode to achieve superb perceptual consistency. In addition, we propose an efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA) tuning to guarantee perceptual quality, which allows the generated motion cues to behave smoothly. Experiments on benchmarks indicate that TT2V achieves effective semantic reconstruction, while IT2V exhibits competitive perceptual consistency. These results highlight potential directions for future research in video coding.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CTE-MLO: Continuous-time and Efficient Multi-LiDAR Odometry with Localizability-aware Point Cloud Sampling","url":"https://arxiv.org/abs/2408.04901","date":1739768400,"author":"","guid":1127,"unread":true,"content":"<article>arXiv:2408.04901v2 Announce Type: replace \nAbstract: In recent years, LiDAR-based localization and mapping methods have achieved significant progress thanks to their reliable and real-time localization capability. Considering single LiDAR odometry often faces hardware failures and degeneracy in practical scenarios, Multi-LiDAR Odometry (MLO), as an emerging technology, is studied to enhance the performance of LiDAR-based localization and mapping systems. However, MLO can suffer from high computational complexity introduced by dense point clouds that are fused from multiple LiDARs, and the continuous-time measurement characteristic is constantly neglected by existing LiDAR odometry. This motivates us to develop a Continuous-Time and Efficient MLO, namely CTE-MLO, which can achieve accurate and real-time estimation using multi-LiDAR measurements through a continuous-time perspective. In this paper, the Gaussian process estimation is naturally combined with the Kalman filter, which enables each LiDAR point in a point stream to query the corresponding continuous-time trajectory using its time instants. A decentralized multi-LiDAR synchronization scheme is also devised to combine points from separate LiDARs into a single point cloud without the primary LiDAR assignment. Moreover, with the aim of improving the real-time performance of MLO without sacrificing robustness, a point cloud sampling strategy is designed with the consideration of localizability. To this end, CTE-MLO integrates synchronization, localizability-aware sampling, continuous-time estimation, and voxel map management within a Kalman filter framework, which can achieve high accuracy and robust continuous-time estimation within only a few linear iterations. The effectiveness of the proposed method is demonstrated through various scenarios, including public datasets and real-world applications. The code is available at https://github.com/shenhm516/CTE-MLO to benefit the community.</article>","contentLength":1973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering","url":"https://arxiv.org/abs/2407.20908","date":1739768400,"author":"","guid":1128,"unread":true,"content":"<article>arXiv:2407.20908v2 Announce Type: replace \nAbstract: Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.</article>","contentLength":1410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Global Solver based on the Sperner-Lemma and Mazurkewicz-Knaster-Kuratowski-Lemma based proof of the Brouwer Fixed-Point Theorem","url":"https://arxiv.org/abs/2407.18816","date":1739768400,"author":"","guid":1129,"unread":true,"content":"<article>arXiv:2407.18816v5 Announce Type: replace \nAbstract: In this paper a fixed-point solver for mappings from a Simplex into itself that is gradient-free, global and requires $d$ function evaluations for halvening the error is presented, where $d$ is the dimension. It is based on topological arguments and uses the constructive proof of the Mazurkewicz-Knaster-Kuratowski lemma as used as part of the proof for Brouwers Fixed-Point theorem.</article>","contentLength":437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transformer-based Graph Neural Networks for Battery Range Prediction in AIoT Battery-Swap Services","url":"https://arxiv.org/abs/2407.16115","date":1739768400,"author":"","guid":1130,"unread":true,"content":"<article>arXiv:2407.16115v2 Announce Type: replace \nAbstract: The concept of the sharing economy has gained broad recognition, and within this context, Sharing E-Bike Battery (SEB) have emerged as a focal point of societal interest. Despite the popularity, a notable discrepancy remains between user expectations regarding the remaining battery range of SEBs and the reality, leading to a pronounced inclination among users to find an available SEB during emergency situations. In response to this challenge, the integration of Artificial Intelligence of Things (AIoT) and battery-swap services has surfaced as a viable solution. In this paper, we propose a novel structural Transformer-based model, referred to as the SEB-Transformer, designed specifically for predicting the battery range of SEBs. The scenario is conceptualized as a dynamic heterogeneous graph that encapsulates the interactions between users and bicycles, providing a comprehensive framework for analysis. Furthermore, we incorporate the graph structure into the SEB-Transformer to facilitate the estimation of the remaining e-bike battery range, in conjunction with mean structural similarity, enhancing the prediction accuracy. By employing the predictions made by our model, we are able to dynamically adjust the optimal cycling routes for users in real-time, while also considering the strategic locations of charging stations, thereby optimizing the user experience. Empirically our results on real-world datasets demonstrate the superiority of our model against nine competitive baselines. These innovations, powered by AIoT, not only bridge the gap between user expectations and the physical limitations of battery range but also significantly improve the operational efficiency and sustainability of SEB services. Through these advancements, the shared electric bicycle ecosystem is evolving, making strides towards a more reliable, user-friendly, and sustainable mode of transportation.</article>","contentLength":1957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships","url":"https://arxiv.org/abs/2407.12543","date":1739768400,"author":"","guid":1131,"unread":true,"content":"<article>arXiv:2407.12543v2 Announce Type: replace \nAbstract: While interpretability methods identify a model's learned concepts, they overlook the relationships between concepts that make up its abstractions and inform its ability to generalize to new data. To assess whether models' have learned human-aligned abstractions, we introduce abstraction alignment, a methodology to compare model behavior against formal human knowledge. Abstraction alignment externalizes domain-specific human knowledge as an abstraction graph, a set of pertinent concepts spanning levels of abstraction. Using the abstraction graph as a ground truth, abstraction alignment measures the alignment of a model's behavior by determining how much of its uncertainty is accounted for by the human abstractions. By aggregating abstraction alignment across entire datasets, users can test alignment hypotheses, such as which human concepts the model has learned and where misalignments recur. In evaluations with experts, abstraction alignment differentiates seemingly similar errors, improves the verbosity of existing model-quality metrics, and uncovers improvements to current human abstractions.</article>","contentLength":1164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detect, Investigate, Judge and Determine: A Knowledge-guided Framework for Few-shot Fake News Detection","url":"https://arxiv.org/abs/2407.08952","date":1739768400,"author":"","guid":1132,"unread":true,"content":"<article>arXiv:2407.08952v2 Announce Type: replace \nAbstract: Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Knowledge-guided Fake News Detection (DKFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DKFND first identifies the knowledge concepts of each news article through a Detection Module. Subsequently, DKFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to evaluate the relevance and confidence of them. Finally, a Determination Module further derives two respective predictions and obtain the final result. Extensive experiments on two public datasets show the efficacy of our proposed method, particularly in low-resource settings.</article>","contentLength":1387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fair and Truthful Allocations Under Leveled Valuations","url":"https://arxiv.org/abs/2407.05891","date":1739768400,"author":"","guid":1133,"unread":true,"content":"<article>arXiv:2407.05891v2 Announce Type: replace \nAbstract: We study the problem of fairly allocating indivisible goods among agents which are equipped with {\\em leveled} valuation functions. Such preferences, that have been studied before in economics and fair division literature, capture a simple and intuitive economic behavior; larger bundles are always preferred to smaller ones. We provide a fine-grained analysis for various subclasses of leveled valuations focusing on two extensively studied notions of fairness, (approximate) MMS and EFX. In particular, we present a general positive result, showing the existence of $2/3$-MMS allocations under valuations that are both leveled and submodular. We also show how some of our ideas can be used beyond the class of leveled valuations; for the case of two submodular (not necessarily leveled) agents we show that there always exists a $2/3$-MMS allocation, complementing a recent impossibility result. Then, we switch to the case of subadditive and fractionally subadditive leveled agents, where we are able to show tight (lower and upper) bounds of $1/2$ on the approximation factor of MMS. Moreover, we show the existence of exact EFX allocations under general leveled valuations via a simple protocol that in addition satisfies several natural economic properties. Finally, we take a mechanism design approach and we propose protocols that are both truthful and approximately fair under leveled valuations.</article>","contentLength":1458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Associative Recurrent Memory Transformer","url":"https://arxiv.org/abs/2407.04841","date":1739768400,"author":"","guid":1134,"unread":true,"content":"<article>arXiv:2407.04841v2 Announce Type: replace \nAbstract: This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.</article>","contentLength":779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hardware-Friendly Shuffling Countermeasure Against Side-Channel Attacks for Kyber","url":"https://arxiv.org/abs/2407.02452","date":1739768400,"author":"","guid":1135,"unread":true,"content":"<article>arXiv:2407.02452v3 Announce Type: replace \nAbstract: CRYSTALS-Kyber has been standardized as the only key-encapsulation mechanism (KEM) scheme by NIST to withstand attacks by large-scale quantum computers. However, the side-channel attacks (SCAs) on its implementation are still needed to be well considered for the upcoming migration. In this brief, we propose a secure and efficient hardware implementation for Kyber by incorporating a novel compact shuffling architecture. First of all, we modify the Fisher-Yates shuffle to make it more hardware-friendly. We then design an optimized shuffling architecture for the well-known open-source Kyber hardware implementation to enhance the security of all known and potential side-channel leakage points. Finally, we implement the modified Kyber design on FPGA and evaluate its security and performance. The security is verified by conducting correlation power analysis (CPA) and test vector leakage assessment (TVLA) on the hardware. Meanwhile, FPGA place-and-route results show that the proposed design reports only 8.7% degradation on the hardware efficiency compared with the original unprotected version, much better than existing hardware hiding schemes.</article>","contentLength":1207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking Predictive Coding Networks -- Made Simple","url":"https://arxiv.org/abs/2407.01163","date":1739768400,"author":"","guid":1136,"unread":true,"content":"<article>arXiv:2407.01163v2 Announce Type: replace \nAbstract: In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we propose a library, called PCX, that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library and a comprehensive set of benchmarks would address all these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii)~reach new state-of-the-art results in all of the tasks and datasets provided; (iii)~clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we release code, tests, and benchmarks. Link to the library: https://github.com/liukidar/pcx</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents","url":"https://arxiv.org/abs/2406.11132","date":1739768400,"author":"","guid":1137,"unread":true,"content":"<article>arXiv:2406.11132v2 Announce Type: replace \nAbstract: In the past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and their capacity is further expanded into the so-called LLM agents when connected with external tools. In all domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering (APE) has become an important question for many researchers and users of LLMs. However, previous works in APE rely on a final checker to evaluate the performance of the given prompt -- a requirement that is hard to meet in the case of LLM agents, where intermediate feedback is easier to obtain, and the final evaluation could be expensive, inaccurate, or even missing. In this paper, we propose a novel method, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to optimize the step-by-step instructions in the prompts given to LLM agents, based on the chat history obtained from interactions and reflections with LLM agents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the prompt without the need for a final solution checker. We evaluate our approach on PDDL generation, TravelPlanner, and Meeting Planning to show that our method could generally improve performance for different reasoning tasks.</article>","contentLength":1437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents","url":"https://arxiv.org/abs/2406.10291","date":1739768400,"author":"","guid":1138,"unread":true,"content":"<article>arXiv:2406.10291v2 Announce Type: replace \nAbstract: Large language models (LLMs) excel across many natural language processing tasks but face challenges in domain-specific, analytical tasks such as conducting research surveys. This study introduces ResearchArena, a benchmark designed to evaluate LLMs' capabilities in conducting academic surveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papers' relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps. Notably, mind-map construction is treated as a bonus task, reflecting its supplementary role in survey-writing. To support these evaluations, we construct an offline environment of 12M full-text academic papers and 7.9K survey papers. To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, underscoring significant opportunities for advancing LLMs in autonomous research.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RASPNet: A Benchmark Dataset for Radar Adaptive Signal Processing Applications","url":"https://arxiv.org/abs/2406.09638","date":1739768400,"author":"","guid":1139,"unread":true,"content":"<article>arXiv:2406.09638v2 Announce Type: replace \nAbstract: We present a large-scale dataset for radar adaptive signal processing (RASP) applications to support the development of data-driven models within the adaptive radar community. The dataset, RASPNet, exceeds 16 TB in size and comprises 100 realistic scenarios compiled over a variety of topographies and land types from across the contiguous United States. For each scenario, RASPNet consists of 10,000 clutter realizations from an airborne radar setting, which can be used to benchmark radar and complex-valued learning algorithms. RASPNet intends to fill a prominent gap in the availability of a large-scale, realistic dataset that standardizes the evaluation of adaptive radar processing techniques and complex-valued neural networks. We outline its construction, organization, and several applications, including a transfer learning example to demonstrate how RASPNet can be used for realistic adaptive radar processing scenarios.</article>","contentLength":985,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentially Private Prototypes for Imbalanced Transfer Learning","url":"https://arxiv.org/abs/2406.08039","date":1739768400,"author":"","guid":1140,"unread":true,"content":"<article>arXiv:2406.08039v3 Announce Type: replace \nAbstract: Machine learning (ML) models have been shown to leak private information from their training datasets. Differential Privacy (DP), typically implemented through the differential private stochastic gradient descent algorithm (DP-SGD), has become the standard solution to bound leakage from the models. Despite recent improvements, DP-SGD-based approaches for private learning still usually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes, and when the private training datasets are imbalanced. To overcome these limitations, we propose Differentially Private Prototype Learning (DPPL) as a new paradigm for private transfer learning. DPPL leverages publicly pre-trained encoders to extract features from private data and generates DP prototypes that represent each private class in the embedding space and can be publicly released for inference. Since our DP prototypes can be obtained from only a few private training data points and without iterative noise addition, they offer high-utility predictions and strong privacy guarantees even under the notion of \\textit{pure DP}. We additionally show that privacy-utility trade-offs can be further improved when leveraging the public data beyond pre-training of the encoder: in particular, we can privately sample our DP prototypes from the publicly available data points used to train the encoder. Our experimental evaluation with four state-of-the-art encoders, four vision datasets, and under different data and imbalancedness regimes demonstrate DPPL's high performance under strong privacy guarantees in challenging private learning setups</article>","contentLength":1665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Critical Look At Tokenwise Reward-Guided Text Generation","url":"https://arxiv.org/abs/2406.07780","date":1739768400,"author":"","guid":1141,"unread":true,"content":"<article>arXiv:2406.07780v2 Announce Type: replace \nAbstract: Large language models (LLMs) can be improved by aligning with human preferences through fine-tuning -- the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users. Due to their ability to bypass LLM fine-tuning, prediction-time tokenwise reward-guided text generation (RGTG) methods have recently been proposed. They use a reward model trained on full sequences to score partial sequences during decoding in a bid to steer the generation towards sequences with high rewards. However, these methods have so far been only heuristically motivated and poorly analyzed. In this work, we show that reward models trained on full sequences are not compatible with scoring partial sequences. To alleviate this issue, we propose to train a Bradley-Terry reward model on partial sequences explicitly, and autoregressively sample from the implied tokenwise policy during decoding time. We study the properties of this reward model and the resulting policy: we show that this policy is proportional to the ratio of two distinct RLHF policies. Our simple approach outperforms previous RGTG methods and performs similarly to strong offline baselines without large-scale LLM finetuning.</article>","contentLength":1298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delving into LLM-assisted writing in biomedical publications through excess vocabulary","url":"https://arxiv.org/abs/2406.07016","date":1739768400,"author":"","guid":1142,"unread":true,"content":"<article>arXiv:2406.07016v3 Announce Type: replace \nAbstract: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.</article>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Latent Diffusion Model-Enabled Low-Latency Semantic Communication in the Presence of Semantic Ambiguities and Wireless Channel Noises","url":"https://arxiv.org/abs/2406.06644","date":1739768400,"author":"","guid":1143,"unread":true,"content":"<article>arXiv:2406.06644v4 Announce Type: replace \nAbstract: Deep learning (DL)-based Semantic Communications (SemCom) is becoming critical to maximize overall efficiency of communication networks. Nevertheless, SemCom is sensitive to wireless channel uncertainties, source outliers, and suffer from poor generalization bottlenecks. To address the mentioned challenges, this paper develops a latent diffusion model-enabled SemCom system with three key contributions, i.e., i) to handle potential outliers in the source data, semantic errors obtained by projected gradient descent based on the vulnerabilities of DL models, are utilized to update the parameters and obtain an outlier-robust encoder, ii) a lightweight single-layer latent space transformation adapter completes one-shot learning at the transmitter and is placed before the decoder at the receiver, enabling adaptation for out-of-distribution data and enhancing human-perceptual quality, and iii) an end-to-end consistency distillation (EECD) strategy is used to distill the diffusion models trained in latent space, enabling deterministic single or few-step low-latency denoising in various noisy channels while maintaining high semantic quality. Extensive numerical experiments across different datasets demonstrate the superiority of the proposed SemCom system, consistently proving its robustness to outliers, the capability to transmit data with unknown distributions, and the ability to perform real-time channel denoising tasks while preserving high human perceptual quality, outperforming the existing denoising approaches in semantic metrics such as multi-scale structural similarity index measure (MS-SSIM) and learned perceptual image path similarity (LPIPS).</article>","contentLength":1726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Verbalized Machine Learning: Revisiting Machine Learning with Language Models","url":"https://arxiv.org/abs/2406.04344","date":1739768400,"author":"","guid":1144,"unread":true,"content":"<article>arXiv:2406.04344v3 Announce Type: replace \nAbstract: Motivated by the progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning (ML) models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical ML problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why an update is performed. We empirically verify the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shield Synthesis for LTL Modulo Theories","url":"https://arxiv.org/abs/2406.04184","date":1739768400,"author":"","guid":1145,"unread":true,"content":"<article>arXiv:2406.04184v2 Announce Type: replace \nAbstract: In recent years, Machine Learning (ML) models have achieved remarkable success in various domains. However, these models also tend to demonstrate unsafe behaviors, precluding their deployment in safety-critical systems. To cope with this issue, ample research focuses on developing methods that guarantee the safe behaviour of a given ML model. A prominent example is shielding which incorporates an external component (a ``shield'') that blocks unwanted behavior. Despite significant progress, shielding suffers from a main setback: it is currently geared towards properties encoded solely in propositional logics (e.g., LTL) and is unsuitable for richer logics. This, in turn, limits the widespread applicability of shielding in many real-world systems. In this work, we address this gap, and extend shielding to LTL modulo theories, by building upon recent advances in reactive synthesis modulo theories. This allowed us to develop a novel approach for generating shields conforming to complex safety specifications in these more expressive, logics. We evaluated our shields and demonstrate their ability to handle rich data with temporal dynamics. To the best of our knowledge, this is the first approach for synthesizing shields for such expressivity.</article>","contentLength":1309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning","url":"https://arxiv.org/abs/2406.03519","date":1739768400,"author":"","guid":1146,"unread":true,"content":"<article>arXiv:2406.03519v4 Announce Type: replace \nAbstract: High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients aggregation weights proportional to their privacy parameters will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here.</article>","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection","url":"https://arxiv.org/abs/2406.00625","date":1739768400,"author":"","guid":1147,"unread":true,"content":"<article>arXiv:2406.00625v4 Announce Type: replace \nAbstract: Visual anomaly detection is vital in real-world applications, such as industrial defect detection and medical diagnosis. However, most existing methods focus on local structural anomalies and fail to detect higher-level functional anomalies under logical conditions. Although recent studies have explored logical anomaly detection, they can only address simple anomalies like missing or addition and show poor generalizability due to being heavily data-driven. To fill this gap, we propose SAM-LAD, a zero-shot, plug-and-play framework for logical anomaly detection in any scene. First, we obtain a query image's feature map using a pre-trained backbone. Simultaneously, we retrieve the reference images and their corresponding feature maps via the nearest neighbor search of the query image. Then, we introduce the Segment Anything Model (SAM) to obtain object masks of the query and reference images. Each object mask is multiplied with the entire image's feature map to obtain object feature maps. Next, an Object Matching Model (OMM) is proposed to match objects in the query and reference images. To facilitate object matching, we further propose a Dynamic Channel Graph Attention (DCGA) module, treating each object as a keypoint and converting its feature maps into feature vectors. Finally, based on the object matching relations, an Anomaly Measurement Model (AMM) is proposed to detect objects with logical anomalies. Structural anomalies in the objects can also be detected. We validate our proposed SAM-LAD using various benchmarks, including industrial datasets (MVTec Loco AD, MVTec AD), and the logical dataset (DigitAnatomy). Extensive experimental results demonstrate that SAM-LAD outperforms existing SoTA methods, particularly in detecting logical anomalies.</article>","contentLength":1830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentially Private Clustered Federated Learning","url":"https://arxiv.org/abs/2405.19272","date":1739768400,"author":"","guid":1148,"unread":true,"content":"<article>arXiv:2405.19272v4 Announce Type: replace \nAbstract: Federated learning (FL), which is a decentralized machine learning (ML) approach, often incorporates differential privacy (DP) to provide rigorous data privacy guarantees. Previous works attempted to address high structured data heterogeneity in vanilla FL settings through clustering clients (a.k.a clustered FL), but these methods remain sensitive and prone to errors, further exacerbated by the DP noise. This vulnerability makes the previous methods inappropriate for differentially private FL (DPFL) settings with structured data heterogeneity. To address this gap, we propose an algorithm for differentially private clustered FL, which is robust to the DP noise in the system and identifies the underlying clients' clusters correctly. To this end, we propose to cluster clients based on both their model updates and training loss values. Furthermore, for clustering clients' model updates at the end of the first round, our proposed approach addresses the server's uncertainties by employing large batch sizes as well as Gaussian Mixture Models (GMM) to reduce the impact of DP and stochastic noise and avoid potential clustering errors. This idea is efficient especially in privacy-sensitive scenarios with more DP noise. We provide theoretical analysis to justify our approach and evaluate it across diverse data distributions and privacy budgets. Our experimental results show its effectiveness in addressing large structured data heterogeneity in DPFL.</article>","contentLength":1515,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LeDex: Training LLMs to Better Self-Debug and Explain Code","url":"https://arxiv.org/abs/2405.18649","date":1739768400,"author":"","guid":1149,"unread":true,"content":"<article>arXiv:2405.18649v2 Announce Type: replace \nAbstract: In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose LeDex, a training framework that significantly improves the self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.</article>","contentLength":1651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model Cascading for Code: A Cascaded Black-Box Multi-Model Framework for Cost-Efficient Code Completion with Self-Testing","url":"https://arxiv.org/abs/2405.15842","date":1739768400,"author":"","guid":1150,"unread":true,"content":"<article>arXiv:2405.15842v2 Announce Type: replace \nAbstract: The rapid advancement of large language models (LLMs) has significantly improved code completion tasks, yet the trade-off between accuracy and computational cost remains a critical challenge. While using larger models and incorporating inference-time self-testing algorithms can significantly improve output accuracy, they incur substantial computational expenses at the same time. Furthermore, servers in real-world scenarios usually have a dynamic preference on the cost-accuracy tradeoff, depending on the budget, bandwidth, the concurrent user volume, and users' sensitivity to wrong answers. In this work, we introduce a novel framework combining model cascading and inference-time self-feedback algorithms to find multiple near-optimal self-testing options on the cost-accuracy tradeoff in LLM-based code generation. Our approach leverages self-generated tests to both enhance accuracy and evaluate model cascading decisions. As a blackbox inference-time method, it requires no access to internal model parameters. We further propose a threshold-based algorithm to determine when to deploy larger models and a heuristic to optimize the number of solutions, test cases, and test lines generated per model, based on budget constraints. Experimental results show that our cascading approach reduces costs by an average of 26%, and up to 70% in the best case, across various model families and datasets, while maintaining or improving accuracy in natural language generation tasks compared to both random and optimal single-model self-testing schemes. To our knowledge, this is the first work to provide a series of choices for optimizing the cost-accuracy trade-off in LLM code generation with self-testing.</article>","contentLength":1763,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code","url":"https://arxiv.org/abs/2405.15568","date":1739768400,"author":"","guid":1151,"unread":true,"content":"<article>arXiv:2405.15568v3 Announce Type: replace \nAbstract: Open-ended and AI-generating algorithms aim to continuously generate and solve increasingly complex tasks indefinitely, offering a promising path toward more general intelligence. To accomplish this grand vision, learning must occur within a vast array of potential tasks. Existing approaches to automatically generating environments are constrained within manually predefined, often narrow distributions of environment, limiting their ability to create any learning environment. To address this limitation, we introduce a novel framework, OMNI-EPIC, that augments previous work in Open-endedness via Models of human Notions of Interestingness (OMNI) with Environments Programmed in Code (EPIC). OMNI-EPIC leverages foundation models to autonomously generate code specifying the next learnable (i.e., not too easy or difficult for the agent's current skill set) and interesting (e.g., worthwhile and novel) tasks. OMNI-EPIC generates both environments (e.g., an obstacle course) and reward functions (e.g., progress through the obstacle course quickly without touching red objects), enabling it, in principle, to create any simulatable learning task. We showcase the explosive creativity of OMNI-EPIC, which continuously innovates to suggest new, interesting learning challenges. We also highlight how OMNI-EPIC can adapt to reinforcement learning agents' learning progress, generating tasks that are of suitable difficulty. Overall, OMNI-EPIC can endlessly create learnable and interesting environments, further propelling the development of self-improving AI systems and AI-Generating Algorithms. Project website with videos: https://dub.sh/omniepic</article>","contentLength":1704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linear Mode Connectivity in Differentiable Tree Ensembles","url":"https://arxiv.org/abs/2405.14596","date":1739768400,"author":"","guid":1152,"unread":true,"content":"<article>arXiv:2405.14596v2 Announce Type: replace \nAbstract: Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Space-aware Socioeconomic Indicator Inference with Heterogeneous Graphs","url":"https://arxiv.org/abs/2405.14135","date":1739768400,"author":"","guid":1153,"unread":true,"content":"<article>arXiv:2405.14135v2 Announce Type: replace \nAbstract: Regional socioeconomic indicators are critical across various domains, yet their acquisition can be costly. Inferring global socioeconomic indicators from a limited number of regional samples is essential for enhancing management and sustainability in urban areas and human settlements. Current inference methods typically rely on spatial interpolation based on the assumption of spatial continuity, which does not adequately address the complex variations present within regional spaces. In this paper, we present GeoHG, the first space-aware socioeconomic indicator inference method that utilizes a heterogeneous graph-based structure to represent geospace for non-continuous inference. Extensive experiments demonstrate the effectiveness of GeoHG in comparison to existing methods, achieving an $R^2$ score exceeding 0.8 under extreme data scarcity with a masked ratio of 95\\%.</article>","contentLength":933,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"City-Scale Multi-Camera Vehicle Tracking System with Improved Self-Supervised Camera Link Model","url":"https://arxiv.org/abs/2405.11345","date":1739768400,"author":"","guid":1154,"unread":true,"content":"<article>arXiv:2405.11345v3 Announce Type: replace \nAbstract: Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms the basis for numerous future city-wide systems (e.g. traffic management, crash detection, etc.). However, the challenge of matching vehicle trajectories across different cameras based solely on feature extraction poses significant difficulties. This article introduces an innovative multi-camera vehicle tracking system that utilizes a self-supervised camera link model. In contrast to related works that rely on manual spatial-temporal annotations, our model automatically extracts crucial multi-camera relationships for vehicle matching. The camera link is established through a pre-matching process that evaluates feature similarities, pair numbers, and time variance for high-quality tracks. This process calculates the probability of spatial linkage for all camera combinations, selecting the highest scoring pairs to create camera links. Our approach significantly improves deployment times by eliminating the need for human annotation, offering substantial improvements in efficiency and cost-effectiveness when it comes to real-world application. This pairing process supports cross camera matching by setting spatial-temporal constraints, reducing the searching space for potential vehicle matches. According to our experimental results, the proposed method achieves a new state-of-the-art among automatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1 Score.</article>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solving the enigma: Enhancing faithfulness and comprehensibility in explanations of deep networks","url":"https://arxiv.org/abs/2405.10008","date":1739768400,"author":"","guid":1155,"unread":true,"content":"<article>arXiv:2405.10008v2 Announce Type: replace \nAbstract: The accelerated progress of artificial intelligence (AI) has popularized deep learning models across various domains, yet their inherent opacity poses challenges, particularly in critical fields like healthcare, medicine, and the geosciences. Explainable AI (XAI) has emerged to shed light on these 'black box' models, aiding in deciphering their decision-making processes. However, different XAI methods often produce significantly different explanations, leading to high inter-method variability that increases uncertainty and undermines trust in deep networks' predictions. In this study, we address this challenge by introducing a novel framework designed to enhance the explainability of deep networks through a dual focus on maximizing both accuracy and comprehensibility in the explanations. Our framework integrates outputs from multiple established XAI methods and leverages a non-linear neural network model, termed the 'explanation optimizer,' to construct a unified, optimal explanation. The optimizer evaluates explanations using two key metrics: faithfulness (accuracy in reflecting the network's decisions) and complexity (comprehensibility). By balancing these, it provides accurate and accessible explanations, addressing a key XAI limitation. Experiments on multi-class and binary classification in 2D object and 3D neuroscience imaging confirm its efficacy. Our optimizer achieved faithfulness scores 155% and 63% higher than the best XAI methods in 3D and 2D tasks, respectively, while also reducing complexity for better understanding. These results demonstrate that optimal explanations based on specific quality criteria are achievable, offering a solution to the issue of inter-method variability in the current XAI literature and supporting more trustworthy deep network predictions</article>","contentLength":1860,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Personalized Content Synthesis with Diffusion Models","url":"https://arxiv.org/abs/2405.05538","date":1739768400,"author":"","guid":1156,"unread":true,"content":"<article>arXiv:2405.05538v2 Announce Type: replace \nAbstract: Recent advancements in generative models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS). With a small set of user-provided examples, PCS aims to customize the subject of interest to specific user-defined prompts. Over the past two years, more than 150 methods have been proposed. However, existing surveys mainly focus on text-to-image generation, with few providing up-to-date summaries on PCS. This paper offers a comprehensive survey of PCS, with a particular focus on the diffusion models. Specifically, we introduce the generic frameworks of PCS research, which can be broadly classified into optimization-based and learning-based approaches. We further categorize and analyze these methodologies, discussing their strengths, limitations, and key techniques. Additionally, we delve into specialized tasks within the field, such as personalized object generation, face synthesis, and style personalization, highlighting their unique challenges and innovations. Despite encouraging progress, we also present an analysis of the challenges such as overfitting and the trade-off between subject fidelity and text alignment. Through this detailed overview and analysis, we propose future directions to advance the development of PCS.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Reliable Empirical Machine Unlearning Evaluation: A Cryptographic Game Perspective","url":"https://arxiv.org/abs/2404.11577","date":1739768400,"author":"","guid":1157,"unread":true,"content":"<article>arXiv:2404.11577v3 Announce Type: replace \nAbstract: Machine unlearning updates machine learning models to remove information from specific training samples, complying with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics lacking theoretical understanding and reliability. Specifically, by modeling the proposed evaluation process as a \\emph{cryptographic game} between unlearning algorithms and MIA adversaries, the naturally-induced evaluation metric measures the data removal efficacy of unlearning algorithms and enjoys provable guarantees that existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient approximation of the induced evaluation metric and demonstrate its effectiveness through both theoretical analysis and empirical experiments. Overall, this work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models","url":"https://arxiv.org/abs/2404.04237","date":1739768400,"author":"","guid":1158,"unread":true,"content":"<article>arXiv:2404.04237v2 Announce Type: replace \nAbstract: The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to address complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two aspects that are central to human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.</article>","contentLength":1128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minimizing the Number of Tardy Jobs and Maximal Tardiness on a Single Machine is NP-hard","url":"https://arxiv.org/abs/2404.02784","date":1739768400,"author":"","guid":1159,"unread":true,"content":"<article>arXiv:2404.02784v2 Announce Type: replace \nAbstract: This paper resolves a long-standing open question in bicriteria scheduling regarding the complexity of a single machine scheduling problem which combines the number of tardy jobs and the maximal tardiness criteria. We use the lexicographic approach with the maximal tardiness being the primary criterion. Accordingly, the objective is to find, among all solutions minimizing the maximal tardiness, the one which has the minimum number of tardy jobs. The complexity of this problem has been open for over thirty years, and has been known since then to be one of the most challenging open questions in multicriteria scheduling. We resolve this question by proving that the problem is strongly NP-hard. We also prove that the problem is at least weakly NP-hard when we switch roles between the two criteria (i.e., when the number of tardy jobs is the primary criterion). Finally, we provide hardness results for two other approaches (constraint and a priori approaches) to deal with these two criteria.</article>","contentLength":1052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AgentStudio: A Toolkit for Building General Virtual Agents","url":"https://arxiv.org/abs/2403.17918","date":1739768400,"author":"","guid":1160,"unread":true,"content":"<article>arXiv:2403.17918v3 Announce Type: replace \nAbstract: General virtual agents need to handle multimodal observations, master complex action spaces, and self-improve in dynamic, open-domain environments. However, existing environments are often domain-specific and require complex setups, which limits agent development and evaluation in real-world settings. As a result, current evaluations lack in-depth analyses that decompose fundamental agent capabilities. We introduce AgentStudio, a trinity of environments, tools, and benchmarks to address these issues. AgentStudio provides a lightweight, interactive environment with highly generic observation and action spaces, e.g., video observations and GUI/API actions. It integrates tools for creating online benchmark tasks, annotating GUI elements, and labeling actions in videos. Based on our environment and tools, we curate an online task suite that benchmarks both GUI interactions and function calling with efficient auto-evaluation. We also reorganize existing datasets and collect new ones using our tools to establish three datasets: GroundUI, IDMBench, and CriticBench. These datasets evaluate fundamental agent abilities, including GUI grounding, learning from videos, and success detection, pointing to the desiderata for robust, general, and open-ended virtual agents.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty-Aware Explanations Through Probabilistic Self-Explainable Neural Networks","url":"https://arxiv.org/abs/2403.13740","date":1739768400,"author":"","guid":1161,"unread":true,"content":"<article>arXiv:2403.13740v2 Announce Type: replace \nAbstract: The lack of transparency of Deep Neural Networks continues to be a limitation that severely undermines their reliability and usage in high-stakes applications. Promising approaches to overcome such limitations are Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions rely on the similarity between the input at hand and a set of prototypical representations of the output classes, offering therefore a deep, yet transparent-by-design, architecture. In this paper, we introduce a probabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for the prototypes with probability distributions over their values. This provides not only a more flexible framework for an end-to-end learning of prototypes, but can also capture the explanatory uncertainty of the model, which is a missing feature in previous approaches. In addition, since the prototypes determine both the explanation and the prediction, Prob-PSENNs allow us to detect when the model is making uninformed or uncertain predictions, and to obtain valid explanations for them. Our experiments demonstrate that Prob-PSENNs provide more meaningful and robust explanations than their non-probabilistic counterparts, while remaining competitive in terms of predictive performance, thus enhancing the explainability and reliability of the models.</article>","contentLength":1404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Deep Learning finally better than Decision Trees on Tabular Data?","url":"https://arxiv.org/abs/2402.03970","date":1739768400,"author":"","guid":1162,"unread":true,"content":"<article>arXiv:2402.03970v2 Announce Type: replace \nAbstract: Tabular data is a ubiquitous data modality due to its versatility and ease of use in many real-world applications. The predominant heuristics for handling classification tasks on tabular data rely on classical machine learning techniques, as the superiority of deep learning models has not yet been demonstrated. This raises the question of whether new deep learning paradigms can surpass classical approaches. Recent studies on tabular data offer a unique perspective on the limitations of neural networks in this domain and highlight the superiority of gradient boosted decision trees (GBDTs) in terms of scalability and robustness across various datasets. However, novel foundation models have not been thoroughly assessed regarding quality or fairly compared to existing methods for tabular classification. Our study categorizes ten state-of-the-art neural models based on their underlying learning paradigm, demonstrating specifically that meta-learned foundation models outperform GBDTs in small data regimes. Although dataset-specific neural networks generally outperform LLM-based tabular classifiers, they are surpassed by an AutoML library which exhibits the best performance but at the cost of higher computational demands.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simultaneous Computation and Communication over MAC","url":"https://arxiv.org/abs/2401.16751","date":1739768400,"author":"","guid":1163,"unread":true,"content":"<article>arXiv:2401.16751v4 Announce Type: replace \nAbstract: We study communication over a Gaussian multiple-access channel (MAC) with two types of transmitters: Digital transmitters hold a message from a discrete set that needs to be communicated to the receiver with vanishing error probability. Analog transmitters hold sequences of analog values. Some functions of these distributed values (but not the values themselves) need to be conveyed to the receiver, subject to a fidelity criterion such as mean squared error (MSE) or a certain maximum error with given confidence. For the case in which the computed function for the analog transmitters is a sum of values in [-1,1], we derive inner and outer bounds for the tradeoff of digital and analog rates of communication under peak and average power constraints for digital transmitters and a peak power constraint for analog transmitters. We then extend the achievability result to a class of functions that includes all linear and some non-linear functions. This extended scheme works over fading channels as long as full channel state information is available at the transmitter. The practicality of our proposed communication scheme is shown in channel simulations that use a version of the scheme based on low density parity check (LDPC) coding. We evaluate the system performance for different block lengths and Gaussian as well as non-Gaussian noise distributions.</article>","contentLength":1417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Game-Theoretic Cybersecurity: the Good, the Bad and the Ugly","url":"https://arxiv.org/abs/2401.13815","date":1739768400,"author":"","guid":1164,"unread":true,"content":"<article>arXiv:2401.13815v2 Announce Type: replace \nAbstract: Given the scale of consequences attributable to cyber attacks, the field of cybersecurity has long outgrown ad-hoc decision-making. A popular choice to provide disciplined decision-making in cybersecurity is Game Theory, which seeks to mathematically understand strategic interaction. In practice though, game-theoretic approaches are scarcely utilized (to our knowledge), highlighting the need to understand the deficit between the existing state-of-the-art and the needs of cybersecurity practitioners. Therefore, we develop a framework to characterize the function and assumptions of existing works as applied to cybersecurity and leverage it to characterize 80 unique technical papers. Then, we leverage this information to analyze the capabilities of the proposed models in comparison to the application-specific needs they are meant to serve, as well as the practicality of implementing the proposed solution. Our main finding is that Game Theory largely fails to incorporate notions of uncertainty critical to the application being considered. To remedy this, we provide guidance in terms of how to incorporate uncertainty in a model, what forms of uncertainty are critical to consider in each application area, and how to model the information that is available in each application area.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Test Case Repair Using Language Models","url":"https://arxiv.org/abs/2401.06765","date":1739768400,"author":"","guid":1165,"unread":true,"content":"<article>arXiv:2401.06765v4 Announce Type: replace \nAbstract: Ensuring the quality of software systems through testing is essential, yet maintaining test cases poses significant challenges and costs. The need for frequent updates to align with the evolving system under test often entails high complexity and cost for maintaining these test cases. Further, unrepaired broken test cases can degrade test suite quality and disrupt the software development process, wasting developers' time. To address this challenge, we present TaRGET (Test Repair GEneraTor), a novel approach leveraging pre-trained code language models for automated test case repair. TaRGET treats test repair as a language translation task, employing a two-step process to fine-tune a language model based on essential context data characterizing the test breakage. To evaluate our approach, we introduce TaRBench, a comprehensive benchmark we developed covering 45,373 broken test repairs across 59 open-source projects. Our results demonstrate TaRGET's effectiveness, achieving a 66.1% exact match accuracy. Furthermore, our study examines the effectiveness of TaRGET across different test repair scenarios. We provide a practical guide to predict situations where the generated test repairs might be less reliable. We also explore whether project-specific data is always necessary for fine-tuning and if our approach can be effective on new projects.</article>","contentLength":1413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning a Diffusion Model Policy from Rewards via Q-Score Matching","url":"https://arxiv.org/abs/2312.11752","date":1739768400,"author":"","guid":1166,"unread":true,"content":"<article>arXiv:2312.11752v5 Announce Type: replace \nAbstract: Diffusion models have become a popular choice for representing actor policies in behavior cloning and offline reinforcement learning. This is due to their natural ability to optimize an expressive class of distributions over a continuous space. However, previous works fail to exploit the score-based structure of diffusion models, and instead utilize a simple behavior cloning term to train the actor, limiting their ability in the actor-critic setting. In this paper, we present a theoretical framework linking the structure of diffusion model policies to a learned Q-function, by linking the structure between the score of the policy to the action gradient of the Q-function. We focus on off-policy reinforcement learning and propose a new policy update method from this theory, which we denote Q-score matching. Notably, this algorithm only needs to differentiate through the denoising model rather than the entire diffusion model evaluation, and converged policies through Q-score matching are implicitly multi-modal and explorative in continuous domains. We conduct experiments in simulated environments to demonstrate the viability of our proposed method and compare to popular baselines. Source code is available from the project website: https://michaelpsenka.io/qsm.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning county from pixels: Corn yield prediction with attention-weighted multiple instance learning","url":"https://arxiv.org/abs/2312.01001","date":1739768400,"author":"","guid":1167,"unread":true,"content":"<article>arXiv:2312.01001v3 Announce Type: replace \nAbstract: Remote sensing technology has become a promising tool in yield prediction. Most prior work employs satellite imagery for county-level corn yield prediction by spatially aggregating all pixels within a county into a single value, potentially overlooking the detailed information and valuable insights offered by more granular data. To this end, this research examines each county at the pixel level and applies multiple instance learning to leverage detailed information within a county. In addition, our method addresses the \"mixed pixel\" issue caused by the inconsistent resolution between feature datasets and crop mask, which may introduce noise into the model and therefore hinder accurate yield prediction. Specifically, the attention mechanism is employed to automatically assign weights to different pixels, which can mitigate the influence of mixed pixels. The experimental results show that the developed model outperforms four other machine learning models over the past five years in the U.S. corn belt and demonstrates its best performance in 2022, achieving a coefficient of determination (R2) value of 0.84 and a root mean square error (RMSE) of 0.83. This paper demonstrates the advantages of our approach from both spatial and temporal perspectives. Furthermore, through an in-depth study of the relationship between mixed pixels and attention, it is verified that our approach can capture critical feature information while filtering out noise from mixed pixels.</article>","contentLength":1532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering","url":"https://arxiv.org/abs/2311.17331","date":1739768400,"author":"","guid":1168,"unread":true,"content":"<article>arXiv:2311.17331v4 Announce Type: replace \nAbstract: Recently, to comprehensively improve Vision Language Models (VLMs) for Visual Question Answering (VQA), several methods have been proposed to further reinforce the inference capabilities of VLMs to independently tackle VQA tasks rather than some methods that only utilize VLMs as aids to Large Language Models (LLMs). However, these methods ignore the rich common-sense knowledge inside the given VQA image sampled from the real world. Thus, they cannot fully use the powerful VLM for the given VQA question to achieve optimal performance. Attempt to overcome this limitation and inspired by the human top-down reasoning process, i.e., systematically exploring relevant issues to derive a comprehensive answer, this work introduces a novel, explainable multi-agent collaboration framework by leveraging the expansive knowledge of Large Language Models (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our framework comprises three agents, i.e., Responder, Seeker, and Integrator, to collaboratively answer the given VQA question by seeking its relevant issues and generating the final answer in such a top-down reasoning process. The VLM-based Responder agent generates the answer candidates for the question and responds to other relevant issues. The Seeker agent, primarily based on LLM, identifies relevant issues related to the question to inform the Responder agent and constructs a Multi-View Knowledge Base (MVKB) for the given visual scene by leveraging the build-in world knowledge of LLM. The Integrator agent combines knowledge from the Seeker agent and the Responder agent to produce the final VQA answer. Extensive and comprehensive evaluations on diverse VQA datasets with a variety of VLMs demonstrate the superior performance and interpretability of our framework over the baseline method in the zero-shot setting without extra training cost.</article>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry","url":"https://arxiv.org/abs/2311.16466","date":1739768400,"author":"","guid":1169,"unread":true,"content":"<article>arXiv:2311.16466v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. We analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We find that LLM usage is associated with an increased likelihood of obtaining relief from financial firms. To investigate this relationship, we employ an instrumental variable approach to mitigate endogeneity concerns around LLM adoption. Although instrumental variables suggest a potential causal link, they cannot fully capture all unobserved heterogeneity. To further establish this causal relationship, we conducted controlled experiments, which support that LLMs can enhance the clarity and persuasiveness of consumer narratives, thereby increasing the likelihood of obtaining relief. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.</article>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ephemeral Rollups are All you Need","url":"https://arxiv.org/abs/2311.02650","date":1739768400,"author":"","guid":1170,"unread":true,"content":"<article>arXiv:2311.02650v4 Announce Type: replace \nAbstract: In the realm of open and composable gaming, we envision platforms where users actively expand, create, engage, and immerse themselves in a rich world of entertainment. One promising avenue for achieving this vision is through fully on-chain (FOC) games, where both game state and logic reside on the blockchain, maximizing composability. However, we must grapple with inherent limitations and trade-offs, particularly in terms of costs and scalability. This paper proposes a framework that leverages the Solana Virtual Machine (SVM) to scale FOC games without state fragmentation or compromised trust assumptions. The framework introduces a systematic approach for discovering, utilizing, and publishing modular pieces of logic as components deeply rooted in the Entity-Component-System (ECS) pattern. To enhance scalability and resource optimization, we introduce the concept of Ephemeral Rollups (ERs) that overcome the tradeoffs of L2 horizontal scaling. These dedicated runtimes can be customized to provide higher operational speed, configurable ticking mechanisms, provable sessions and gasless transactions without composability-scalability tradeoffs.</article>","contentLength":1211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos","url":"https://arxiv.org/abs/2311.02076","date":1739768400,"author":"","guid":1171,"unread":true,"content":"<article>arXiv:2311.02076v2 Announce Type: replace \nAbstract: In gradient descent dynamics of neural networks, the top eigenvalue of the loss Hessian (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, (iii) the crucial role of initialization and parameterization, and (iv) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Finally, we demonstrate that various predictions from this simplified model generalize to real-world scenarios and discuss its limitations.</article>","contentLength":1245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Experiencing Urban Air Mobility: How Passengers evaluate a simulated flight with an Air Taxi","url":"https://arxiv.org/abs/2311.01079","date":1739768400,"author":"","guid":1172,"unread":true,"content":"<article>arXiv:2311.01079v2 Announce Type: replace \nAbstract: For the successful development and implementation of novel concepts and technology, the acceptance of potential users is crucial. Therefore, within the project HorizonUAM of the German Aerospace Center (DLR), we investigated passengers' acceptance of air taxis. One challenge is that not many people have real experiences with urban air mobility (UAM) at the moment and thus requirements formulated by potential users refer to rather abstract concepts. To allow participants to gain realistic impressions of UAM concepts, a Mixed Reality (MR) Air Taxi Simulator was set up. In a study, 30 participants experienced an inner-city business shuttle flight. We assessed the influence of another person on board on wellbeing and information needs in nominal (experiment 1) and non-nominal situations (experiment 2). For the latter, participants experienced a re-routing of the flight due to landing side unavailability. During and after the flights, participants answered questionnaires and extensive interviews were conducted. The study produced empirical data on relevant factors regarding interaction, information needs and comfort within an air taxi. The findings show that passengers want to be informed about intentions of the vehicle. The presence of a flight attendant on board is not necessary but can increase wellbeing especially during non-nominal situations.</article>","contentLength":1418,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A polynomial bound on the number of minimal separators and potential maximal cliques in $P_6$-free graphs of bounded clique number","url":"https://arxiv.org/abs/2310.11573","date":1739768400,"author":"","guid":1173,"unread":true,"content":"<article>arXiv:2310.11573v2 Announce Type: replace \nAbstract: In this note we show a polynomial bound on the number of minimal separators and potential maximal cliques in $P_6$-free graphs of bounded clique number.</article>","contentLength":205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Local Reasoning about Probabilistic Behaviour for Classical-Quantum Programs","url":"https://arxiv.org/abs/2308.04741","date":1739768400,"author":"","guid":1174,"unread":true,"content":"<article>arXiv:2308.04741v3 Announce Type: replace \nAbstract: Verifying the functional correctness of programs with both classical and quantum constructs is a challenging task. The presence of probabilistic behaviour entailed by quantum measurements and unbounded while loops complicate the verification task greatly. We propose a new quantum Hoare logic for local reasoning about probabilistic behaviour by introducing distribution formulas to specify probabilistic properties. We show that the proof rules in the logic are sound with respect to a denotational semantics. To demonstrate the effectiveness of the logic, we formally verify the correctness of non-trivial quantum algorithms including the HHL and Shor's algorithms. Moreover, we embed our logic into the proof assistant Coq. The resulting logical framework, called CoqQLR, can facilitate semi-automated reasoning about classical--quantum programs.</article>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compositional Shape Analysis with Shared Abduction and Biabductive Loop Acceleration (Extended Version)","url":"https://arxiv.org/abs/2307.06346","date":1739768400,"author":"","guid":1175,"unread":true,"content":"<article>arXiv:2307.06346v5 Announce Type: replace \nAbstract: Biabduction-based shape analysis is a compositional verification and analysis technique that can prove memory safety in the presence of complex, linked data structures. Despite its usefulness, several open problems persist for this kind of analysis; two of which we address in this paper. On the one hand, the original analysis is path-sensitive but cannot combine safety requirements for related branches. This causes the analysis to require additional soundness checks and decreases the analysis' precision. We extend the underlying symbolic execution and propose a framework for shared abduction where a common pre-condition is maintained for related computation branches. On the other hand, prior implementations lift loop acceleration methods from forward analysis to biabduction analysis by applying them separately on the pre- and post-condition, which can lead to imprecise or even unsound acceleration results that do not form a loop invariant. In contrast, we propose biabductive loop acceleration, which explicitly constructs and checks candidate loop invariants. For this, we also introduce a novel heuristic called shape extrapolation. This heuristic takes advantage of locality in the handling of list-like data structures (which are the most common data structures found in low-level code) and jointly accelerates pre- and post-conditions by extrapolating the related shapes. In addition to making the analysis more precise, our techniques also make biabductive analysis more efficient since they are sound in just one analysis phase. In contrast, prior techniques always require two phases (as the first phase can produce contracts that are unsound and must hence be verified). We experimentally confirm that our techniques improve on prior techniques; both in terms of precision and runtime of the analysis.</article>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Applying Process Mining on Scientific Workflows: a Case Study on High Performance Computing Data","url":"https://arxiv.org/abs/2307.02833","date":1739768400,"author":"","guid":1176,"unread":true,"content":"<article>arXiv:2307.02833v2 Announce Type: replace \nAbstract: Computer-based scientific experiments are becoming increasingly data-intensive, necessitating the use of High-Performance Computing (HPC) clusters to handle large scientific workflows. These workflows result in complex data and control flows within the system, making analysis challenging. This paper focuses on the extraction of case IDs from SLURM-based HPC cluster logs, a crucial step for applying mainstream process mining techniques. The core contribution is the development of methods to correlate jobs in the system, whether their interdependencies are explicitly specified or not. We present our log extraction and correlation techniques, supported by experiments that validate our approach, enabling comprehensive documentation of workflows and identification of performance bottlenecks.</article>","contentLength":850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption","url":"https://arxiv.org/abs/2307.00677","date":1739768400,"author":"","guid":1177,"unread":true,"content":"<article>arXiv:2307.00677v5 Announce Type: replace \nAbstract: Density-based clustering is the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as they are separated by low-density regions. However, a high-density region that is not separated by low-density ones might also have different structures belonging to multiple clusters. As far as we know, all previous density-based clustering algorithms fail to detect such structures. In this paper, we provide a novel density-based clustering scheme to address this problem. It is the rst clustering algorithm that can detect meticulous structures in a high-density region that is not separated by low-density ones and thus extends the range of applications of clustering. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, called Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption, dubbed SDC-HSDD-NDSA. Experiments on synthetic and real datasets are implemented to verify the effectiveness, robustness, and granularity independence of the algorithm, and the scheme is compared to unsupervised schemes in the Python package Scikit-learn. Results demonstrate that our algorithm outperforms previous ones in many situations, especially significantly when clusters have regular internal structures. For example, averaging over the eight noiseless synthetic datasets with structures employing ARI and NMI criteria, previous algorithms obtain scores below 0.6 and 0.7, while the presented algorithm obtains scores higher than 0.9 and 0.95, respectively.</article>","contentLength":1674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does my medical AI look at pictures of birds? Exploring the efficacy of transfer learning across domain boundaries","url":"https://arxiv.org/abs/2306.17555","date":1739768400,"author":"","guid":1178,"unread":true,"content":"<article>arXiv:2306.17555v2 Announce Type: replace \nAbstract: It is an open secret that ImageNet is treated as the panacea of pretraining. Particularly in medical machine learning, models not trained from scratch are often finetuned based on ImageNet-pretrained models. We posit that pretraining on data from the domain of the downstream task should almost always be preferred instead. We leverage RadNet-12M, a dataset containing more than 12 million computed tomography (CT) image slices, to explore the efficacy of self-supervised pretraining on medical and natural images. Our experiments cover intra- and cross-domain transfer scenarios, varying data scales, finetuning vs. linear evaluation, and feature space analysis. We observe that intra-domain transfer compares favorably to cross-domain transfer, achieving comparable or improved performance (0.44% - 2.07% performance increase using RadNet pretraining, depending on the experiment) and demonstrate the existence of a domain boundary-related generalization gap and domain-specific learned features.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detecting Phishing Sites Using ChatGPT","url":"https://arxiv.org/abs/2306.05816","date":1739768400,"author":"","guid":1179,"unread":true,"content":"<article>arXiv:2306.05816v3 Announce Type: replace \nAbstract: The emergence of Large Language Models (LLMs), including ChatGPT, is having a significant impact on a wide range of fields. While LLMs have been extensively researched for tasks such as code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, has been largely unexplored. To combat the rising tide of cyber attacks due to the misuse of LLMs, it is important to automate detection by leveraging the advanced capabilities of LLMs.\n  In this paper, we propose a novel system called ChatPhishDetector that utilizes LLMs to detect phishing sites. Our system involves leveraging a web crawler to gather information from websites, generating prompts for LLMs based on the crawled data, and then retrieving the detection results from the responses generated by the LLMs. The system enables us to detect multilingual phishing sites with high accuracy by identifying impersonated brands and social engineering techniques in the context of the entire website, without the need to train machine learning models. To evaluate the performance of our system, we conducted experiments on our own dataset and compared it with baseline systems and several LLMs. The experimental results using GPT-4V demonstrated outstanding performance, with a precision of 98.7% and a recall of 99.6%, outperforming the detection results of other LLMs and existing systems. These findings highlight the potential of LLMs for protecting users from online fraudulent activities and have important implications for enhancing cybersecurity measures.</article>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification with Snoring Usecase","url":"https://arxiv.org/abs/2305.06110","date":1739768400,"author":"","guid":1180,"unread":true,"content":"<article>arXiv:2305.06110v4 Announce Type: replace \nAbstract: This paper proposes an atomic behaviour intervention strategy using Pavlok device. Pavlok utilises beeps, vibration and shocks as a mode of aversion technique to help individuals with behaviour modification. While the device can be useful in certain periodic daily life situations, like alarms and exercise notifications, the device relies on manual operations that limit its usage. To automate behaviour modification, we propose a framework that first detects targeted behaviours through a lightweight deep learning model and subsequently nudges the user through Pavlok. Our proposed solution is implemented and verified in the context of snoring, which captures audio from the environment following a prediction of whether the audio content is a snore or not using a 1D convolutional neural network. Based on the prediction, we use Pavlok to nudge users for preventive measures, such as a change in sleeping posture. We believe that this simple solution can help people change their atomic habits, which may lead to long-term health benefits. Our proposed real-time, lightweight model (99.8% fewer parameters over SOTA; 1,278,049 --&gt; 1337) achieves SOTA performance (test accuracy of 0.99) on a public benchmark. The code and model are publicly available at https://github.com/hasan-rakibul/pavlok-nudge-snore.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cure is in the Cause: A Filesystem for Container Debloating","url":"https://arxiv.org/abs/2305.04641","date":1739768400,"author":"","guid":1181,"unread":true,"content":"<article>arXiv:2305.04641v3 Announce Type: replace \nAbstract: Containers have become a standard for deploying applications due to their convenience, but they often suffer from significant software bloat-unused files that inflate image sizes, increase provisioning times, and waste resources. These inefficiencies are particularly problematic in serverless and edge computing scenarios, where resources are constrained, and performance is critical. Existing debloating tools are limited in scope and effectiveness, failing to address the widespread issue of container bloat at scale. In this paper, we conduct a large-scale evaluation of container bloat, analyzing the top 20 most downloaded containers on DockerHub. We evaluate two state-of-the-art debloating tools, identify their limitations, and propose a novel solution, BAFFS, which addresses bloat at the filesystem level by introducing a flexible debloating layer that preserves the layered structure of container filesystems. The debloating layer can be organized in different ways to meet diverse requirements. Our evaluation demonstrates that over 50% of the top-downloaded containers have more than 60% bloat, and BAFFS reduces container sizes significantly while maintaining functionality. For serverless functions, BAFFS reduces cold start latency by up to 68%. Additionally, when combined with lazy-loading snapshotters, BAFFS enhances provisioning efficiency, reducing conversion times by up to 93% and provisioning times by up to 19%.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coevolution of Camouflage","url":"https://arxiv.org/abs/2304.11793","date":1739768400,"author":"","guid":1182,"unread":true,"content":"<article>arXiv:2304.11793v3 Announce Type: replace \nAbstract: Camouflage in nature seems to arise from competition between predator and prey. To survive, predators must find prey, and prey must avoid being found. This work simulates an abstract model of that adversarial relationship. It looks at crypsis through evolving prey camouflage patterns (as color textures) in competition with evolving predator vision. During their \"lifetime\" predators learn to better locate camouflaged prey. The environment for this 2D simulation is provided by a set of photographs, typically of natural scenes. This model is based on two evolving populations, one of prey and another of predators. Mutual conflict between these populations can produce both effective prey camouflage and predators skilled at \"breaking\" camouflage. The result is an open source artificial life model to help study camouflage in nature, and the perceptual phenomenon of camouflage more generally.</article>","contentLength":950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models","url":"https://arxiv.org/abs/2304.00228","date":1739768400,"author":"","guid":1183,"unread":true,"content":"<article>arXiv:2304.00228v3 Announce Type: replace \nAbstract: Search engines increasingly leverage large language models (LLMs) to generate direct answers, and AI chatbots now access the Internet for fresh data. As information curators for billions of users, LLMs must assess the accuracy and reliability of different sources. This paper audits nine widely used LLMs from three leading providers -- OpenAI, Google, and Meta -- to evaluate their ability to discern credible and high-quality information sources from low-credibility ones. We find that while LLMs can rate most tested news outlets, larger models more frequently refuse to provide ratings due to insufficient information, whereas smaller models are more prone to making errors in their ratings. For sources where ratings are provided, LLMs exhibit a high level of agreement among themselves (average Spearman's $\\rho = 0.79$), but their ratings align only moderately with human expert evaluations (average $\\rho = 0.50$). Analyzing news sources with different political leanings in the US, we observe a liberal bias in credibility ratings yielded by all LLMs in default configurations. Additionally, assigning partisan roles to LLMs consistently induces strong politically congruent bias in their ratings. These findings have important implications for the use of LLMs in curating news and political information.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Connected Trading Cycles","url":"https://arxiv.org/abs/2303.09759","date":1739768400,"author":"","guid":1184,"unread":true,"content":"<article>arXiv:2303.09759v2 Announce Type: replace \nAbstract: Incentivizing the existing participants to invite new participants to join an auction, matching or cooperative game have been extensively studied recently. One common challenge to design such incentive in these games is that the invitees and inviters are competitors. To have such an incentive, we normally have to sacrifice some of the traditional properties. Especially, in a housing market (one kind of one-sided matching), we cannot maintain the traditional stability and optimality. The previous studies proposed some new matching mechanisms to have the invitation incentive (part of the incentive compatibility), but did not have any guarantee on stability and optimality.\n  In this paper, we propose new notions of stability and optimality which are achievable with incentive compatibility. We weaken stability and optimality on a special structure (complete components) on networks. We first prove that the weakened notions are the best we can achieve with incentive compatibility. Then, we propose three mechanisms (Swap With Neighbors, Leave and Share, and Connected Trading Cycles) to satisfy the desirable properties. Connected Trading Cycles is the first mechanism to satisfy the best stability and optimality compatible with incentive compatibility.</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Task Aware Dreamer for Task Generalization in Reinforcement Learning","url":"https://arxiv.org/abs/2303.05092","date":1739768400,"author":"","guid":1185,"unread":true,"content":"<article>arXiv:2303.05092v4 Announce Type: replace \nAbstract: A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. The ability to generalize across tasks is important as it determines an agent's adaptability to real-world scenarios where reward mechanisms might vary. In this work, we first show that training a general world model can utilize similar structures in these tasks and help train more generalizable agents. Extending world models into the task generalization setting, we introduce a novel method named Task Aware Dreamer (TAD), which integrates reward-informed features to identify consistent latent characteristics across tasks. Within TAD, we compute the variational lower bound of sample data log-likelihood, which introduces a new term designed to differentiate tasks using their states, as the optimization objective of our reward-informed world models. To demonstrate the advantages of the reward-informed policy in TAD, we introduce a new metric called Task Distribution Relevance (TDR) which quantitatively measures the relevance of different tasks. For tasks exhibiting a high TDR, i.e., the tasks differ significantly, we illustrate that Markovian policies struggle to distinguish them, thus it is necessary to utilize reward-informed policies in TAD. Extensive experiments in both image-based and state-based tasks show that TAD can significantly improve the performance of handling different tasks simultaneously, especially for those with high TDR, and display a strong generalization ability to unseen tasks.</article>","contentLength":1680,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intermittently Observable Markov Decision Processes","url":"https://arxiv.org/abs/2302.11761","date":1739768400,"author":"","guid":1186,"unread":true,"content":"<article>arXiv:2302.11761v2 Announce Type: replace \nAbstract: This paper investigates MDPs with intermittent state information. We consider a scenario where the controller perceives the state information of the process via an unreliable communication channel. The transmissions of state information over the whole time horizon are modeled as a Bernoulli lossy process. Hence, the problem is finding an optimal policy for selecting actions in the presence of state information losses. We first formulate the problem as a belief MDP to establish structural results. The effect of state information losses on the expected total discounted reward is studied systematically. Then, we reformulate the problem as a tree MDP whose state space is organized in a tree structure. Two finite-state approximations to the tree MDP are developed to find near-optimal policies efficiently. Finally, we put forth a nested value iteration algorithm for the finite-state approximations, which is proved to be faster than standard value iteration. Numerical results demonstrate the effectiveness of our methods.</article>","contentLength":1082,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do PAC-Learners Learn the Marginal Distribution?","url":"https://arxiv.org/abs/2302.06285","date":1739768400,"author":"","guid":1187,"unread":true,"content":"<article>arXiv:2302.06285v2 Announce Type: replace \nAbstract: The Fundamental Theorem of PAC Learning asserts that learnability of a concept class $H$ is equivalent to the $\\textit{uniform convergence}$ of empirical error in $H$ to its mean, or equivalently, to the problem of $\\textit{density estimation}$, learnability of the underlying marginal distribution with respect to events in $H$. This seminal equivalence relies strongly on PAC learning's `distribution-free' assumption, that the adversary may choose any marginal distribution over data. Unfortunately, the distribution-free model is known to be overly adversarial in practice, failing to predict the success of modern machine learning algorithms, but without the Fundamental Theorem our theoretical understanding of learning under distributional constraints remains highly limited.\n  In this work, we revisit the connection between PAC learning, uniform convergence, and density estimation beyond the distribution-free setting when the adversary is restricted to choosing a marginal distribution from a known family $\\mathscr{P}$. We prove that while the traditional Fundamental Theorem indeed fails, a finer-grained connection between the three fundamental notions continues to hold:\n  1. PAC-Learning is strictly sandwiched between two refined models of density estimation, both equivalent to standard density estimation in the distribution-free case, differing only in whether the learner $\\textit{knows}$ the set of well-estimated events in $H$.\n  2. Under reasonable assumptions on $H$ and $\\mathscr{P}$, density estimation is equivalent to \\emph{uniform estimation}, a relaxation of uniform convergence allowing non-empirical estimators.\n  Together, our results give a clearer picture of how the Fundamental Theorem extends beyond the distribution-free setting and shed new light on the classically challenging problem of learning under arbitrary distributional assumptions.</article>","contentLength":1934,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated Learning","url":"https://arxiv.org/abs/2302.01706","date":1739768400,"author":"","guid":1188,"unread":true,"content":"<article>arXiv:2302.01706v2 Announce Type: replace \nAbstract: This paper presents the application of Vertical Federated Learning (VFL) to generate synthetic tabular data using Generative Adversarial Networks (GANs). VFL is a collaborative approach to train machine learning models among distinct tabular data holders, such as financial institutions, who possess disjoint features for the same group of customers. In this paper we introduce the VT-GAN framework, Vertical federated Tabular GAN, and demonstrate that VFL can be successfully used to implement GANs for distributed tabular data in privacy-preserving manner, with performance close to centralized GANs that assume shared data. We make design choices with respect to the distribution of GAN generator and discriminator models and introduce a training-with-shuffling technique so that no party can reconstruct training data from the GAN conditional vector. The paper presents (1) an implementation of VT-GAN, (2) a detailed quality evaluation of the VT-GAN-generated synthetic data, (3) an overall scalability examination of VT-GAN framework, (4) a security analysis on VT-GAN's robustness against Membership Inference Attack with different settings of Differential Privacy, for a range of datasets with diverse distribution characteristics. Our results demonstrate that VT-GAN can consistently generate high-fidelity synthetic tabular data of comparable quality to that generated by a centralized GAN algorithm. The difference in machine learning utility can be as low as 2.7%, even under extremely imbalanced data distributions across clients or with different numbers of clients.</article>","contentLength":1633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Decouple Complex Systems","url":"https://arxiv.org/abs/2302.01581","date":1739768400,"author":"","guid":1189,"unread":true,"content":"<article>arXiv:2302.01581v2 Announce Type: replace \nAbstract: A complex system with cluttered observations may be a coupled mixture of multiple simple sub-systems corresponding to latent entities. Such sub-systems may hold distinct dynamics in the continuous-time domain; therein, complicated interactions between sub-systems also evolve over time. This setting is fairly common in the real world but has been less considered. In this paper, we propose a sequential learning approach under this setting by decoupling a complex system for handling irregularly sampled and cluttered sequential observations. Such decoupling brings about not only subsystems describing the dynamics of each latent entity but also a meta-system capturing the interaction between entities over time. Specifically, we argue that the meta-system evolving within a simplex is governed by projected differential equations (ProjDEs). We further analyze and provide neural-friendly projection operators in the context of Bregman divergence. Experimental results on synthetic and real-world datasets show the advantages of our approach when facing complex and cluttered sequential data compared to the state-of-the-art.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Typed Lambda-Calculus for Establishing Trust in Probabilistic Programs","url":"https://arxiv.org/abs/2302.00958","date":1739768400,"author":"","guid":1190,"unread":true,"content":"<article>arXiv:2302.00958v3 Announce Type: replace \nAbstract: The extensive deployment of probabilistic algorithms has radically changed our perspective on several well-established computational notions. Correctness is probably the most basic one. While a typical probabilistic program cannot be said to compute the correct result, we often have quite strong expectations about the frequency with which it should return certain outputs. In these cases, trust as a generalisation of correctness fares better. One way to understand it is to say that a probabilistic computational process is trustworthy if the frequency of its outputs is compliant with a probability distribution which models its expected behaviour. We present a formal computational framework that formalises this idea. In order to do so, we define a typed lambda-calculus that features operators for conducting experiments at runtime on probabilistic programs and for evaluating whether they compute outputs as determined by a target probability distribution. After proving some fundamental computational properties of the calculus, such as progress and termination, we define a static notion of confidence that allows to prove that our notion of trust behaves correctly with respect to the basic tenets of probability theory.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Training Neural Networks on Data Sources with Unknown Reliability","url":"https://arxiv.org/abs/2212.02895","date":1739768400,"author":"","guid":1191,"unread":true,"content":"<article>arXiv:2212.02895v4 Announce Type: replace \nAbstract: When data is generated by multiple sources, conventional training methods update models assuming equal reliability for each source and do not consider their individual data quality. However, in many applications, sources have varied levels of reliability that can have negative effects on the performance of a neural network. A key issue is that often the quality of the data for individual sources is not known during training. Previous methods for training models in the presence of noisy data do not make use of the additional information that the source label can provide. Focusing on supervised learning, we aim to train neural networks on each data source for a number of steps proportional to the source's estimated reliability by using a dynamic re-weighting strategy motivated by likelihood tempering. This way, we allow training on all sources during the warm-up and reduce learning on less reliable sources during the final training stages, when it has been shown that models overfit to noise. We show through diverse experiments that this can significantly improve model performance when trained on mixtures of reliable and unreliable data sources, and maintain performance when models are trained on reliable sources only.</article>","contentLength":1288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Inclusion for Boundedly-Ambiguous Vector Addition Systems is Decidable","url":"https://arxiv.org/abs/2202.08033","date":1739768400,"author":"","guid":1192,"unread":true,"content":"<article>arXiv:2202.08033v5 Announce Type: replace \nAbstract: We consider the problems of language inclusion and language equivalence for Vector Addition Systems with States (VASS) with the acceptance condition defined by the set of accepting states (and more generally by some upward-closed conditions). In general, the problem of language equivalence is undecidable even for one-dimensional VASS, thus to get decidability we investigate restricted subclasses. On the one hand, we show that the problem of language inclusion of a VASS in k-ambiguous VASS (for any natural k) is decidable and even in Ackermann. On the other hand, we prove that the language equivalence problem is already Ackermann-hard for deterministic VASS. These two results imply Ackermann-completeness for language inclusion and equivalence in several possible restrictions. Some of our techniques can be also applied in much broader generality in infinite-state systems, namely for some subclass of well-structured transition systems.</article>","contentLength":999,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proof complexity of positive branching programs","url":"https://arxiv.org/abs/2102.06673","date":1739768400,"author":"","guid":1193,"unread":true,"content":"<article>arXiv:2102.06673v4 Announce Type: replace \nAbstract: We investigate the proof complexity of systems based on positive branching programs, i.e. non-deterministic branching programs (NBPs) where, for any 0-transition between two nodes, there is also a 1-transition. Positive NBPs compute monotone Boolean functions, just like negation-free circuits or formulas, but constitute a positive version of (non-uniform) NL, rather than P or NC1, respectively.\n  The proof complexity of NBPs was investigated in previous work by Buss, Das and Knop, using extension variables to represent the dag-structure, over a language of (non-deterministic) decision trees, yielding the system eLNDT. Our system eLNDT+ is obtained by restricting their systems to a positive syntax, similarly to how the 'monotone sequent calculus' MLK is obtained from the usual sequent calculus LK by restricting to negation-free formulas.\n  Our main result is that eLNDT+ polynomially simulates eLNDT over positive sequents. Our proof method is inspired by a similar result for MLK by Atserias, Galesi and Pudl\\'ak, that was recently improved to a bona fide polynomial simulation via works of Je\\v{r}\\'abek and Buss, Kabanets, Kolokolova and Kouck\\'y. Along the way we formalise several properties of counting functions within eLNDT+ by polynomial-size proofs and, as a case study, give explicit polynomial-size poofs of the propositional pigeonhole principle.</article>","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zone Theorem for Arrangements in three dimensions","url":"https://arxiv.org/abs/2006.01428","date":1739768400,"author":"","guid":1194,"unread":true,"content":"<article>arXiv:2006.01428v2 Announce Type: replace \nAbstract: In this note, a simple description of zone theorem in three dimensions is given.Arrangements in three dimensions are useful for constructing higher-order Voronoi diagrams in plane. An elementary and very intuitive treatment of this result is also given.</article>","contentLength":306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Euler Factors of Elliptic Curves","url":"https://arxiv.org/abs/2502.10357","date":1739768400,"author":"","guid":1195,"unread":true,"content":"<article>arXiv:2502.10357v1 Announce Type: cross \nAbstract: We apply transformer models and feedforward neural networks to predict Frobenius traces $a_p$ from elliptic curves given other traces $a_q$. We train further models to predict $a_p \\bmod 2$ from $a_q \\bmod 2$, and cross-analysis such as $a_p \\bmod 2$ from $a_q$. Our experiments reveal that these models achieve high accuracy, even in the absence of explicit number-theoretic tools like functional equations of $L$-functions. We also present partial interpretability findings.</article>","contentLength":527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Studying number theory with deep learning: a case study with the M\\\"obius and squarefree indicator functions","url":"https://arxiv.org/abs/2502.10335","date":1739768400,"author":"","guid":1196,"unread":true,"content":"<article>arXiv:2502.10335v1 Announce Type: cross \nAbstract: Building on work of Charton, we train small transformer models to calculate the M\\\"obius function $\\mu(n)$ and the squarefree indicator function $\\mu^2(n)$. The models attain nontrivial predictive power. We then iteratively train additional models to understand how the model functions, ultimately finding a theoretical explanation.</article>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalised Parallel Tempering: Flexible Replica Exchange via Flows and Diffusions","url":"https://arxiv.org/abs/2502.10328","date":1739768400,"author":"","guid":1197,"unread":true,"content":"<article>arXiv:2502.10328v1 Announce Type: cross \nAbstract: Parallel Tempering (PT) is a classical MCMC algorithm designed for leveraging parallel computation to sample efficiently from high-dimensional, multimodal or otherwise complex distributions via annealing. One limitation of the standard formulation of PT is the growth of computational resources required to generate high-quality samples, as measured by effective sample size or round trip rate, for increasingly challenging distributions. To address this issue, we propose the framework: Generalised Parallel Tempering (GePT) which allows for the incorporation of recent advances in modern generative modelling, such as normalising flows and diffusion models, within Parallel Tempering, while maintaining the same theoretical guarantees as MCMC-based methods. For instance, we show that this allows us to utilise diffusion models in a parallelised manner, bypassing the usual computational cost of a large number of steps to generate quality samples. Further, we empirically demonstrate that GePT can improve sample quality and reduce the growth of computational resources required to handle complex distributions over the classical algorithm.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting","url":"https://arxiv.org/abs/2502.10235","date":1739768400,"author":"","guid":1198,"unread":true,"content":"<article>arXiv:2502.10235v1 Announce Type: cross \nAbstract: Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Network fault costs based on minimum leaf spanning trees","url":"https://arxiv.org/abs/2502.10213","date":1739768400,"author":"","guid":1199,"unread":true,"content":"<article>arXiv:2502.10213v1 Announce Type: cross \nAbstract: We study the fault-tolerance of networks from both the structural and computational point of view using the minimum leaf number of the corresponding graph $G$, i.e. the minimum number of leaves of the spanning trees of $G$, and its vertex-deleted subgraphs. We investigate networks that are leaf-guaranteed, i.e. which satisfy a certain stability condition with respect to minimum leaf numbers and vertex-deletion. Next to this, our main notion is the so-called fault cost, which is based on the number of vertices that have different degrees in minimum leaf spanning trees of the network and its vertex-deleted subgraphs. We characterise networks with vanishing fault cost via leaf-guaranteed graphs and describe, for any given network $N$, leaf-guaranteed networks containing $N$. We determine for all non-negative integers $k \\le 8$ except $1$ the smallest network with fault cost $k$. We also give a detailed treatment of the fault cost $1$ case, prove that there are infinitely many $3$-regular networks with fault cost $3$, and show that for any non-negative integer $k$ there exists a network with fault cost exactly $k$.</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model","url":"https://arxiv.org/abs/2502.10173","date":1739768400,"author":"","guid":1200,"unread":true,"content":"<article>arXiv:2502.10173v1 Announce Type: cross \nAbstract: Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.</article>","contentLength":1736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing anomaly detection with topology-aware autoencoders","url":"https://arxiv.org/abs/2502.10163","date":1739768400,"author":"","guid":1201,"unread":true,"content":"<article>arXiv:2502.10163v1 Announce Type: cross \nAbstract: Anomaly detection in high-energy physics is essential for identifying new physics beyond the Standard Model. Autoencoders provide a signal-agnostic approach but are limited by the topology of their latent space. This work explores topology-aware autoencoders, embedding phase-space distributions onto compact manifolds that reflect energy-momentum conservation. We construct autoencoders with spherical ($S^n$), product ($S^2 \\otimes S^2$), and projective ($\\mathbb{RP}^2$) latent spaces and compare their anomaly detection performance against conventional Euclidean embeddings. Our results show that autoencoders with topological priors significantly improve anomaly separation by preserving the global structure of the data manifold and reducing spurious reconstruction errors. Applying our approach to simulated hadronic top-quark decays, we show that latent spaces with appropriate topological constraints enhance sensitivity and robustness in detecting anomalous events. This study establishes topology-aware autoencoders as a powerful tool for unsupervised searches for new physics in particle-collision data.</article>","contentLength":1166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses","url":"https://arxiv.org/abs/2502.10161","date":1739768400,"author":"","guid":1202,"unread":true,"content":"<article>arXiv:2502.10161v1 Announce Type: cross \nAbstract: Reasoning about fairness through correlation-based notions is rife with pitfalls. The 1973 University of California, Berkeley graduate school admissions case from Bickel et. al. (1975) is a classic example of one such pitfall, namely Simpson's paradox. The discrepancy in admission rates among males and female applicants, in the aggregate data over all departments, vanishes when admission rates per department are examined. We reason about the Berkeley graduate school admissions case through a causal lens. In the process, we introduce a statistical test for causal hypothesis testing based on Pearl's instrumental-variable inequalities (Pearl 1995). We compare different causal notions of fairness that are based on graphical, counterfactual and interventional queries on the causal model, and develop statistical tests for these notions that use only observational data. We study the logical relations between notions, and show that while notions may not be equivalent, their corresponding statistical tests coincide for the case at hand. We believe that a thorough case-based causal analysis helps develop a more principled understanding of both causal hypothesis testing and fairness.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Combinatorial Reinforcement Learning with Preference Feedback","url":"https://arxiv.org/abs/2502.10158","date":1739768400,"author":"","guid":1203,"unread":true,"content":"<article>arXiv:2502.10158v1 Announce Type: cross \nAbstract: In this paper, we consider combinatorial reinforcement learning with preference feedback, where a learning agent sequentially offers an action--an assortment of multiple items to--a user, whose preference feedback follows a multinomial logistic (MNL) model. This framework allows us to model real-world scenarios, particularly those involving long-term user engagement, such as in recommender systems and online advertising. However, this framework faces two main challenges: (1) the unknown value of each item, unlike traditional MNL bandits that only address single-step preference feedback, and (2) the difficulty of ensuring optimism while maintaining tractable assortment selection in the combinatorial action space with unknown values. In this paper, we assume a contextual MNL preference model, where the mean utilities are linear, and the value of each item is approximated by a general function. We propose an algorithm, MNL-VQL, that addresses these challenges, making it both computationally and statistically efficient. As a special case, for linear MDPs (with the MNL preference feedback), we establish the first regret lower bound in this framework and show that MNL-VQL achieves nearly minimax-optimal regret. To the best of our knowledge, this is the first work to provide statistical guarantees in combinatorial RL with preference feedback.</article>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Characterization of Logarithmic Fekete Critical Configurations of at Most Six Points in All Dimensions","url":"https://arxiv.org/abs/2502.10152","date":1739768400,"author":"","guid":1204,"unread":true,"content":"<article>arXiv:2502.10152v1 Announce Type: cross \nAbstract: We consider the logarithmic Fekete problem, which consists of placing a fixed number of points on the unit sphere in $\\mathbb{R}^d$, in such a way that the product of all pairs of mutual Euclidean distances is maximized or, equivalently, so that their logarithmic energy is minimized. Using tools from Computational Algebraic Geometry, we find and classify all critical configurations for this problem when considering at most six points in every dimension $d$. Our results discover some previously unknown optimal configurations and give the first reported case of a spurious local minimum for the Fekete problem.</article>","contentLength":665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pangraphs as models of higher-order interactions","url":"https://arxiv.org/abs/2502.10141","date":1739768400,"author":"","guid":1205,"unread":true,"content":"<article>arXiv:2502.10141v1 Announce Type: cross \nAbstract: Graphs depict pairwise relationships between objects within a system. Higher-order interactions (HOIs), which involve more than two objects simultaneously, are common in nature. Such interactions can change the stability of a complex system. Hypergraphs can represent an HOI as an arbitrary subset of vertices. However, they fail to capture the specific roles of the vertices involved, which can be highly asymmetric, particularly in the case of interaction modifications.\n  We introduce pangraphs, a robust and quantitative generalisation of graphs that accurately captures arbitrarily complex higher-order interactions. We demonstrate that several higher-order representations proposed in the literature are specific instances of pangraphs. Additionally, we introduce an incidence multilayer digraph representation of a pangraph, referred to as Levi digraph. We adapt degree and Katz centrality measures to the pangraph framework and show that a consistent generalisation of recursive graph measures cannot be simplified to a Levi digraph of a pangraph.\n  We construct a pangraph for a real-world coffee agroecosystem and compare Katz centrality between its dihypergraph and pangraph representations, both analytically and numerically. The choice of representation significantly affects centrality values and alters vertex ranks. Additionally, we emphasise the use of real-valued incidence matrices to quantify interaction strengths and the roles of vertices within the system.</article>","contentLength":1530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Consecutive and quasi-consecutive patterns: $\\mathrm{des}$-Wilf classifications and generating functions","url":"https://arxiv.org/abs/2502.10128","date":1739768400,"author":"","guid":1206,"unread":true,"content":"<article>arXiv:2502.10128v1 Announce Type: cross \nAbstract: Motivated by a correlation between the distribution of descents over permutations that avoid a consecutive pattern and those avoiding the respective quasi-consecutive pattern, as established in this paper, we obtain a complete $\\des$-Wilf classification for quasi-consecutive patterns of length up to 4. For equivalence classes containing more than one pattern, we construct various descent-preserving bijections to establish the equivalences, which lead to the provision of proper versions of two incomplete bijective arguments previously published in the literature. Additionally, for two singleton classes, we derive explicit bivariate generating functions using the generalized run theorem.</article>","contentLength":745,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strain-Induced Optical and Molecular Transformations in PET Films for Organic Electronic Applications","url":"https://arxiv.org/abs/2502.10113","date":1739768400,"author":"","guid":1207,"unread":true,"content":"<article>arXiv:2502.10113v1 Announce Type: cross \nAbstract: Poly(ethylene terephthalate) (PET) films are widely used in flexible electronics and optoelectronics, where their mechanical durability and optical performance under strain are essential for device reliability. This study investigates the impact of applied mechanical strain on the optical and molecular properties of PET at room temperature,using UV-Vis absorption and Raman spectroscopy. The work explores how varying strain levels, from 0% (unstretched) to 30%, affect the transparency, vibrational modes, and molecular reorganization within PET films. UV-Vis absorbance measurements reveal that strain induces significant changes in the light transmission properties of PET, particularly in the visible range, and increases absorption in the UVA and visible region by up to 100%. Raman spectra indicate that strain levels higher than 5% lead to irreversible shifts of vibrational lines, accompanied by an increase of their full width at half maximum (FWHM), suggesting molecular reorientation and crystallinity changes. The phonon mode coupled with C-O stretching [O-CH2] shows the strongest response to applied mechanical stress. This study provides a comprehensive understanding of strain-induced optical and structural alterations in PET, with implications for improving the mechanical and optical performance of PET-based devices in strainsensitive applications, such as organic solar cells (OSCs), organic light-emitting diodes (OLEDs), and flexible sensors.</article>","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An adaptive importance sampling algorithm for risk-averse optimization","url":"https://arxiv.org/abs/2502.10084","date":1739768400,"author":"","guid":1208,"unread":true,"content":"<article>arXiv:2502.10084v1 Announce Type: cross \nAbstract: Adaptive sampling algorithms are modern and efficient methods that dynamically adjust the sample size throughout the optimization process. However, they may encounter difficulties in risk-averse settings, particularly due to the challenge of accurately sampling from the tails of the underlying distribution of random inputs. This often leads to a much faster growth of the sample size compared to risk-neutral problems. In this work, we propose a novel adaptive sampling algorithm that adapts both the sample size and the sampling distribution at each iteration. The biasing distributions are constructed on the fly, leveraging a reduced-order model of the objective function to be minimized, and are designed to oversample a so-called risk region. As a result, a reduction of the variance of the gradients is achieved, which permits to use fewer samples per iteration compared to a standard algorithm, while still preserving the asymptotic convergence rate. Our focus is on the minimization of the Conditional Value-at-Risk (CVaR), and we establish the convergence of the proposed computational framework. Numerical experiments confirm the substantial computational savings achieved by our approach.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improved Online Confidence Bounds for Multinomial Logistic Bandits","url":"https://arxiv.org/abs/2502.10020","date":1739768400,"author":"","guid":1209,"unread":true,"content":"<article>arXiv:2502.10020v1 Announce Type: cross \nAbstract: In this paper, we propose an improved online confidence bound for multinomial logistic (MNL) models and apply this result to MNL bandits, achieving variance-dependent optimal regret. Recently, Lee &amp; Oh (2024) established an online confidence bound for MNL models and achieved nearly minimax-optimal regret in MNL bandits. However, their results still depend on the norm-boundedness of the unknown parameter $B$ and the maximum size of possible outcomes $K$. To address this, we first derive an online confidence bound of $O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over the previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee &amp; Oh, 2024). This is mainly achieved by establishing tighter self-concordant properties of the MNL loss and introducing a novel intermediary term to bound the estimation error. Using this new online confidence bound, we propose a constant-time algorithm, OFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log T \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $ for sufficiently large $T$, where $\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$ is the dimension of the contexts, and $T$ is the total number of rounds. Furthermore, we introduce an Maximum Likelihood Estimation (MLE)-based algorithm that achieves an anytime, OFU-MN$^2$L, poly($(B)$)-free regret of $O \\Big( d \\log (BT) \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strength and partition rank under limits and field extensions","url":"https://arxiv.org/abs/2502.10007","date":1739768400,"author":"","guid":1210,"unread":true,"content":"<article>arXiv:2502.10007v1 Announce Type: cross \nAbstract: The strength of a multivariate homogeneous polynomial is the minimal number of terms in an expression as a sum of products of lower-degree homogeneous polynomials. Partition rank is the analogue for multilinear forms. Both ranks can drop under field extensions, and both can jump in a limit. We show that, for fixed degree and under mild conditions on the characteristic of the ground field, the strength is at most a polynomial in the border strength. We also establish an analogous result for partition rank. Our results control both the jump under limits and the drop under field extensions.</article>","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Estimation of the Learning Coefficient Using Empirical Loss","url":"https://arxiv.org/abs/2502.09998","date":1739768400,"author":"","guid":1211,"unread":true,"content":"<article>arXiv:2502.09998v1 Announce Type: cross \nAbstract: The learning coefficient plays a crucial role in analyzing the performance of information criteria, such as the Widely Applicable Information Criterion (WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC), which Sumio Watanabe developed to assess model generalization ability. In regular statistical models, the learning coefficient is given by d/2, where d is the dimension of the parameter space. More generally, it is defined as the absolute value of the pole order of a zeta function derived from the Kullback-Leibler divergence and the prior distribution. However, except for specific cases such as reduced-rank regression, the learning coefficient cannot be derived in a closed form. Watanabe proposed a numerical method to estimate the learning coefficient, which Imai further refined to enhance its convergence properties. These methods utilize the asymptotic behavior of WBIC and have been shown to be statistically consistent as the sample size grows. In this paper, we propose a novel numerical estimation method that fundamentally differs from previous approaches and leverages a new quantity, \"Empirical Loss,\" which was introduced by Watanabe. Through numerical experiments, we demonstrate that our proposed method exhibits both lower bias and lower variance compared to those of Watanabe and Imai. Additionally, we provide a theoretical analysis that elucidates why our method outperforms existing techniques and present empirical evidence that supports our findings.</article>","contentLength":1551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Volume Minimization in Conformal Regression","url":"https://arxiv.org/abs/2502.09985","date":1739768400,"author":"","guid":1212,"unread":true,"content":"<article>arXiv:2502.09985v1 Announce Type: cross \nAbstract: We study the question of volume optimality in split conformal regression, a topic still poorly understood in comparison to coverage control. Using the fact that the calibration step can be seen as an empirical volume minimization problem, we first derive a finite-sample upper-bound on the excess volume loss of the interval returned by the classical split method. This important quantity measures the difference in length between the interval obtained with the split method and the shortest oracle prediction interval. Then, we introduce EffOrt, a methodology that modifies the learning step so that the base prediction function is selected in order to minimize the length of the returned intervals. In particular, our theoretical analysis of the excess volume loss of the prediction sets produced by EffOrt reveals the links between the learning and calibration steps, and notably the impact of the choice of the function class of the base predictor. We also introduce Ad-EffOrt, an extension of the previous method, which produces intervals whose size adapts to the value of the covariate. Finally, we evaluate the empirical performance and the robustness of our methodologies.</article>","contentLength":1231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Universal Machine Learning Interatomic Potentials are Ready for Solid Ion Conductors","url":"https://arxiv.org/abs/2502.09970","date":1739768400,"author":"","guid":1213,"unread":true,"content":"<article>arXiv:2502.09970v1 Announce Type: cross \nAbstract: With the rapid development of energy storage technology, high-performance solid-state electrolytes (SSEs) have become critical for next-generation lithium-ion batteries. These materials require high ionic conductivity, excellent electrochemical stability, and good mechanical properties to meet the demands of electric vehicles and portable electronics. However, traditional methods like density functional theory (DFT) and empirical force fields face challenges such as high computational costs, poor scalability, and limited accuracy across material systems. Universal machine learning interatomic potentials (uMLIPs) offer a promising solution with their efficiency and near-DFT-level accuracy.This study systematically evaluates six advanced uMLIP models (MatterSim, MACE, SevenNet, CHGNet, M3GNet, and ORBFF) in terms of energy, forces, thermodynamic properties, elastic moduli, and lithium-ion diffusion behavior. The results show that MatterSim outperforms others in nearly all metrics, particularly in complex material systems, demonstrating superior accuracy and physical consistency. Other models exhibit significant deviations due to issues like energy inconsistency or insufficient training data coverage.Further analysis reveals that MatterSim achieves excellent agreement with reference values in lithium-ion diffusivity calculations, especially at room temperature. Studies on Li3YCl6 and Li6PS5Cl uncover how crystal structure, anion disorder levels, and Na/Li arrangements influence ionic conductivity. Appropriate S/Cl disorder levels and optimized Na/Li arrangements enhance diffusion pathway connectivity, improving overall ionic transport performance.</article>","contentLength":1723,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Learning for Phase Estimation in Satellite-to-Earth Quantum Communication","url":"https://arxiv.org/abs/2502.09920","date":1739768400,"author":"","guid":1214,"unread":true,"content":"<article>arXiv:2502.09920v1 Announce Type: cross \nAbstract: A global continuous-variable quantum key distribution (CV-QKD) network can be established using a series of satellite-to-Earth channels. Increased performance in such a network is provided by performing coherent measurement of the optical quantum signals using a real local oscillator, calibrated locally by encoding known information on transmitted reference pulses and using signal phase error estimation algorithms. The speed and accuracy of the signal phase error estimation algorithm are vital to practical CV-QKD implementation. Our work provides a framework to analyze long short-term memory neural network (NN) architecture parameterization, with respect to the quantum Cram\\'er-Rao uncertainty bound of the signal phase error estimation, with a focus on reducing the model complexity. More specifically, we demonstrate that signal phase error estimation can be achieved using a low-complexity NN architecture, without significantly sacrificing accuracy. Our results significantly improve the real-time performance of practical CV-QKD systems deployed over satellite-to-Earth channels, thereby contributing to the ongoing development of the Quantum Internet.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic-Computed Tomography Angiography for Cerebral Vessel Templates and Segmentation","url":"https://arxiv.org/abs/2502.09893","date":1739768400,"author":"","guid":1215,"unread":true,"content":"<article>arXiv:2502.09893v1 Announce Type: cross \nAbstract: Background: Computed Tomography Angiography (CTA) is crucial for cerebrovascular disease diagnosis. Dynamic CTA is a type of imaging that captures temporal information about the We aim to develop and evaluate two segmentation techniques to segment vessels directly on CTA images: (1) creating and registering population-averaged vessel atlases and (2) using deep learning (DL). Methods: We retrieved 4D-CT of the head from our institutional research database, with bone and soft tissue subtracted from post-contrast images. An Advanced Normalization Tools pipeline was used to create angiographic atlases from 25 patients. Then, atlas-driven ROIs were identified by a CT attenuation threshold to generate segmentation of the arteries and veins using non-linear registration. To create DL vessel segmentations, arterial and venous structures were segmented using the MRA vessel segmentation tool, iCafe, in 29 patients. These were then used to train a DL model, with bone-in CT images as input. Multiple phase images in the 4D-CT were used to increase the training and validation dataset. Both segmentation approaches were evaluated on a test 4D-CT dataset of 11 patients which were also processed by iCafe and validated by a neuroradiologist. Specifically, branch-wise segmentation accuracy was quantified with 20 labels for arteries and one for veins. DL outperformed the atlas-based segmentation models for arteries (average modified dice coefficient (amDC) 0.856 vs. 0.324) and veins (amDC 0.743 vs. 0.495) overall. For ICAs, vertebral and basilar arteries, DL and atlas -based segmentation had an amDC of 0.913 and 0.402, respectively. The amDC for MCA-M1, PCA-P1, and ACA-A1 segments were 0.932 and 0.474, respectively. Conclusion: Angiographic CT templates are developed for the first time in literature. Using 4D-CTA enables the use of tools like iCafe, lessening the burden of manual annotation.</article>","contentLength":1954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpretable Early Warnings using Machine Learning in an Online Game-experiment","url":"https://arxiv.org/abs/2502.09880","date":1739768400,"author":"","guid":1216,"unread":true,"content":"<article>arXiv:2502.09880v1 Announce Type: cross \nAbstract: Stemming from physics and later applied to other fields such as ecology, the theory of critical transitions suggests that some regime shifts are preceded by statistical early warning signals. Reddit's r/place experiment, a large-scale social game, provides a unique opportunity to test these signals consistently across thousands of subsystems undergoing critical transitions. In r/place, millions of users collaboratively created compositions, or pixel-art drawings, in which transitions occur when one composition rapidly replaces another. We develop a machine-learning-based early warning system that combines the predictive power of multiple system-specific time series via gradient-boosted decision trees with memory-retaining features. Our method significantly outperforms standard early warning indicators. Trained on the 2022 r/place data, our algorithm detects half of the transitions occurring within 20 minutes at a false positive rate of just 3.7%. Its performance remains robust when tested on the 2023 r/place event, demonstrating generalizability across different contexts. Using SHapley Additive exPlanations (SHAP) for interpreting the predictions, we investigate the underlying drivers of warnings, which could be relevant to other complex systems, especially online social systems. We reveal an interplay of patterns preceding transitions, such as critical slowing down or speeding up, a lack of innovation or coordination, turbulent histories, and a lack of image complexity. These findings show the potential of machine learning indicators in socio-ecological systems for predicting regime shifts and understanding their dynamics.</article>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gradient GA: Gradient Genetic Algorithm for Drug Molecular Design","url":"https://arxiv.org/abs/2502.09860","date":1739768400,"author":"","guid":1217,"unread":true,"content":"<article>arXiv:2502.09860v1 Announce Type: cross \nAbstract: Molecular discovery has brought great benefits to the chemical industry. Various molecule design techniques are developed to identify molecules with desirable properties. Traditional optimization methods, such as genetic algorithms, continue to achieve state-of-the-art results across multiple molecular design benchmarks. However, these techniques rely solely on random walk exploration, which hinders both the quality of the final solution and the convergence speed. To address this limitation, we propose a novel approach called Gradient Genetic Algorithm (Gradient GA), which incorporates gradient information from the objective function into genetic algorithms. Instead of random exploration, each proposed sample iteratively progresses toward an optimal solution by following the gradient direction. We achieve this by designing a differentiable objective function parameterized by a neural network and utilizing the Discrete Langevin Proposal to enable gradient guidance in discrete molecular spaces. Experimental results demonstrate that our method significantly improves both convergence speed and solution quality, outperforming cutting-edge techniques. For example, it achieves up to a 25% improvement in the top-10 score over the vanilla genetic algorithm. The code is publicly available at https://github.com/debadyuti23/GradientGA.</article>","contentLength":1396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Algorithmic contiguity from low-degree conjecture and applications in correlated random graphs","url":"https://arxiv.org/abs/2502.09832","date":1739768400,"author":"","guid":1218,"unread":true,"content":"<article>arXiv:2502.09832v1 Announce Type: cross \nAbstract: In this paper, assuming a natural strengthening of the low-degree conjecture, we provide evidence of computational hardness for two problems: (1) the (partial) matching recovery problem in the sparse correlated Erd\\H{o}s-R\\'enyi graphs $\\mathcal G(n,q;\\rho)$ when the edge-density $q=n^{-1+o(1)}$ and the correlation $\\rho&lt;\\sqrt{\\alpha}$ lies below the Otter's threshold, solving a remaining problem in \\cite{DDL23+}; (2) the detection problem between the correlated sparse stochastic block model $\\mathcal S(n,\\tfrac{\\lambda}{n};k,\\epsilon;s)$ and a pair of independent stochastic block models $\\mathcal S(n,\\tfrac{\\lambda s}{n};k,\\epsilon)$ when $\\epsilon^2 \\lambda s&lt;1$ lies below the Kesten-Stigum (KS) threshold and $s&lt;\\sqrt{\\alpha}$ lies below the Otter's threshold, solving a remaining problem in \\cite{CDGL24+}.\n  One of the main ingredient in our proof is to derive certain forms of \\emph{algorithmic contiguity} between two probability measures based on bounds on their low-degree advantage. To be more precise, consider the high-dimensional hypothesis testing problem between two probability measures $\\mathbb{P}$ and $\\mathbb{Q}$ based on the sample $\\mathsf Y$. We show that if the low-degree advantage $\\mathsf{Adv}_{\\leq D} \\big( \\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{Q}} \\big)=O(1)$, then (assuming the low-degree conjecture) there is no efficient algorithm $\\mathcal A$ such that $\\mathbb{Q}(\\mathcal A(\\mathsf Y)=0)=1-o(1)$ and $\\mathbb{P}(\\mathcal A(\\mathsf Y)=1)=\\Omega(1)$. This framework provides a useful tool for performing reductions between different inference tasks.</article>","contentLength":1652,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\Lambda$CDM and early dark energy in latent space: a data-driven parametrization of the CMB temperature power spectrum","url":"https://arxiv.org/abs/2502.09810","date":1739768400,"author":"","guid":1219,"unread":true,"content":"<article>arXiv:2502.09810v1 Announce Type: cross \nAbstract: Finding the best parametrization for cosmological models in the absence of first-principle theories is an open question. We propose a data-driven parametrization of cosmological models given by the disentangled 'latent' representation of a variational autoencoder (VAE) trained to compress cosmic microwave background (CMB) temperature power spectra. We consider a broad range of $\\Lambda$CDM and beyond-$\\Lambda$CDM cosmologies with an additional early dark energy (EDE) component. We show that these spectra can be compressed into 5 ($\\Lambda$CDM) or 8 (EDE) independent latent parameters, as expected when using temperature power spectra alone, and which reconstruct spectra at an accuracy well within the Planck errors. These latent parameters have a physical interpretation in terms of well-known features of the CMB temperature spectrum: these include the position, height and even-odd modulation of the acoustic peaks, as well as the gravitational lensing effect. The VAE also discovers one latent parameter which entirely isolates the EDE effects from those related to $\\Lambda$CDM parameters, thus revealing a previously unknown degree of freedom in the CMB temperature power spectrum. We further showcase how to place constraints on the latent parameters using Planck data as typically done for cosmological parameters, obtaining latent values consistent with previous $\\Lambda$CDM and EDE cosmological constraints. Our work demonstrates the potential of a data-driven reformulation of current beyond-$\\Lambda$CDM phenomenological models into the independent degrees of freedom to which the data observables are sensitive.</article>","contentLength":1683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prioritized Ranking Experimental Design Using Recommender Systems in Two-Sided Platforms","url":"https://arxiv.org/abs/2502.09806","date":1739768400,"author":"","guid":1220,"unread":true,"content":"<article>arXiv:2502.09806v1 Announce Type: cross \nAbstract: Interdependencies between units in online two-sided marketplaces complicate estimating causal effects in experimental settings. We propose a novel experimental design to mitigate the interference bias in estimating the total average treatment effect (TATE) of item-side interventions in online two-sided marketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the recommender system as an instrument for experimentation. TSPR strategically prioritizes items based on their treatment status in the listings displayed to users. We designed TSPR to provide users with a coherent platform experience by ensuring access to all items and a consistent realization of their treatment by all users. We evaluate our experimental design through simulations using a search impression dataset from an online travel agency. Our methodology closely estimates the true simulated TATE, while a baseline item-side estimator significantly overestimates TATE.</article>","contentLength":1001,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Patient-Specific Surgical Planning for Bicuspid Aortic Valve Repair: Fully Automated Segmentation of the Aortic Valve in 4D CT","url":"https://arxiv.org/abs/2502.09805","date":1739768400,"author":"","guid":1221,"unread":true,"content":"<article>arXiv:2502.09805v1 Announce Type: cross \nAbstract: The bicuspid aortic valve (BAV) is the most prevalent congenital heart defect and may require surgery for complications such as stenosis, regurgitation, and aortopathy. BAV repair surgery is effective but challenging due to the heterogeneity of BAV morphology. Multiple imaging modalities can be employed to assist the quantitative assessment of BAVs for surgical planning. Contrast-enhanced 4D computed tomography (CT) produces volumetric temporal sequences with excellent contrast and spatial resolution. Segmentation of the aortic cusps and root in these images is an essential step in creating patient specific models for visualization and quantification. While deep learning-based methods are capable of fully automated segmentation, no BAV-specific model exists. Among valve segmentation studies, there has been limited quantitative assessment of the clinical usability of the segmentation results. In this work, we developed a fully auto- mated multi-label BAV segmentation pipeline based on nnU-Net. The predicted segmentations were used to carry out surgically relevant morphological measurements including geometric cusp height, commissural angle and annulus diameter, and the results were compared against manual segmentation. Automated segmentation achieved average Dice scores of over 0.7 and symmetric mean distance below 0.7 mm for all three aortic cusps and the root wall. Clinically relevant benchmarks showed good consistency between manual and predicted segmentations. Overall, fully automated BAV segmentation of 3D frames in 4D CT can produce clinically usable measurements for surgical risk stratification, but the temporal consistency of segmentations needs to be improved.</article>","contentLength":1747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8, ResNet50, and Inception-ResNet-v2 Deep Learning Models","url":"https://arxiv.org/abs/2502.09804","date":1739768400,"author":"","guid":1222,"unread":true,"content":"<article>arXiv:2502.09804v1 Announce Type: cross \nAbstract: Thousands of individuals succumb annually to leukemia alone. As artificial intelligence-driven technologies continue to evolve and advance, the question of their applicability and reliability remains unresolved. This study aims to utilize image processing and deep learning methodologies to achieve state-of-the-art results for the detection of Acute Lymphoblastic Leukemia (ALL) using data that best represents real-world scenarios. ALL is one of several types of blood cancer, and it is an aggressive form of leukemia. In this investigation, we examine the most recent advancements in ALL detection, as well as the latest iteration of the YOLO series and its performance. We address the question of whether white blood cells are malignant or benign. Additionally, the proposed models can identify different ALL stages, including early stages. Furthermore, these models can detect hematogones despite their frequent misclassification as ALL. By utilizing advanced deep learning models, namely, YOLOv8, YOLOv11, ResNet50 and Inception-ResNet-v2, the study achieves accuracy rates as high as 99.7%, demonstrating the effectiveness of these algorithms across multiple datasets and various real-world situations.</article>","contentLength":1260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reconstruction of frequency-localized functions from pointwise samples via least squares and deep learning","url":"https://arxiv.org/abs/2502.09794","date":1739768400,"author":"","guid":1223,"unread":true,"content":"<article>arXiv:2502.09794v1 Announce Type: cross \nAbstract: Recovering frequency-localized functions from pointwise data is a fundamental task in signal processing. We examine this problem from an approximation-theoretic perspective, focusing on least squares and deep learning-based methods. First, we establish a novel recovery theorem for least squares approximations using the Slepian basis from uniform random samples in low dimensions, explicitly tracking the dependence of the bandwidth on the sampling complexity. Building on these results, we then present a recovery guarantee for approximating bandlimited functions via deep learning from pointwise data. This result, framed as a practical existence theorem, provides conditions on the network architecture, training procedure, and data acquisition sufficient for accurate approximation. To complement our theoretical findings, we perform numerical comparisons between least squares and deep learning for approximating one- and two-dimensional functions. We conclude with a discussion of the theoretical limitations and the practical gaps between theory and implementation.</article>","contentLength":1124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Atom identification in bilayer moire materials with Gomb-Net","url":"https://arxiv.org/abs/2502.09791","date":1739768400,"author":"","guid":1224,"unread":true,"content":"<article>arXiv:2502.09791v1 Announce Type: cross \nAbstract: Moire patterns in van der Waals bilayer materials complicate the analysis of atomic-resolution images, hindering the atomic-scale insight typically attainable with scanning transmission electron microscopy. Here, we report a method to detect the positions and identity of atoms in each of the individual layers that compose bilayer heterostructures. We developed a deep learning model, Gomb-Net, which can distinguish atomic species in each individual layer, effectively deconvoluting the moire pattern to enable layer-specific mapping of strain and dopant distributions, unlike other methods which struggle with moire-induced complexity. Using this approach, we explored Se atom substitutional sites in a twisted fractional Janus WS2-WS2(1-x)Se2x heterostructure and found that layer specific implantation sites are unaffected by the moire pattern's local energetic or electronic modulation. This advancement enables atom-identification within material regimes where it was not possible before, opening new insights into previously inaccessible material physics.</article>","contentLength":1114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ExoMiner++ on TESS with Transfer Learning from Kepler: Transit Classification and Vetting Catalog for 2-min Data","url":"https://arxiv.org/abs/2502.09790","date":1739768400,"author":"","guid":1225,"unread":true,"content":"<article>arXiv:2502.09790v1 Announce Type: cross \nAbstract: We present ExoMiner++, an enhanced deep learning model that builds on the success of ExoMiner to improve transit signal classification in 2-minute TESS data. ExoMiner++ incorporates additional diagnostic inputs, including periodogram, flux trend, difference image, unfolded flux, and spacecraft attitude control data, all of which are crucial for effectively distinguishing transit signals from more challenging sources of false positives. To further enhance performance, we leverage transfer learning from high-quality labeled data from the Kepler space telescope, mitigating the impact of TESS's noisier and more ambiguous labels. ExoMiner++ achieves high accuracy across various classification and ranking metrics, significantly narrowing the search space for follow-up investigations to confirm new planets. To serve the exoplanet community, we introduce new TESS catalogs containing ExoMiner++ classifications and confidence scores for each transit signal. Among the 147,568 unlabeled TCEs, ExoMiner++ identifies 7,330 as planet candidates, with the remainder classified as false positives. These 7,330 planet candidates correspond to 1,868 existing TESS Objects of Interest (TOIs), 69 Community TESS Objects of Interest (CTOIs), and 50 newly introduced CTOIs. 1,797 out of the 2,506 TOIs previously labeled as planet candidates in ExoFOP are classified as planet candidates by ExoMiner++. This reduction in plausible candidates combined with the excellent ranking quality of ExoMiner++ allows the follow-up efforts to be focused on the most likely candidates, increasing the overall planet yield.</article>","contentLength":1653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Muscle and Fat Segmentation in Computed Tomography for Comprehensive Body Composition Analysis","url":"https://arxiv.org/abs/2502.09779","date":1739768400,"author":"","guid":1226,"unread":true,"content":"<article>arXiv:2502.09779v1 Announce Type: cross \nAbstract: Body composition assessment using CT images can potentially be used for a number of clinical applications, including the prognostication of cardiovascular outcomes, evaluation of metabolic health, monitoring of disease progression, assessment of nutritional status, prediction of treatment response in oncology, and risk stratification for surgical and critical care outcomes. While multiple groups have developed in-house segmentation tools for this analysis, there are very limited publicly available tools that could be consistently used across different applications. To mitigate this gap, we present a publicly accessible, end-to-end segmentation and feature calculation model specifically for CT body composition analysis. Our model performs segmentation of skeletal muscle, subcutaneous adipose tissue (SAT), and visceral adipose tissue (VAT) across the chest, abdomen, and pelvis area in axial CT images. It also provides various body composition metrics, including muscle density, visceral-to-subcutaneous fat (VAT/SAT) ratio, muscle area/volume, and skeletal muscle index (SMI), supporting both 2D and 3D assessments. The model is shared for public use. To evaluate the model, the segmentation was applied to both internal and external datasets, with body composition metrics analyzed across different age, sex, and race groups. The model achieved high dice coefficients on both internal and external datasets, exceeding 89% for skeletal muscle, SAT, and VAT segmentation. The model outperforms the benchmark by 2.40% on skeletal muscle and 10.26% on SAT compared to the manual annotations given by the publicly available dataset. Body composition metrics show mean relative absolute errors (MRAEs) under 10% for all measures. Furthermore, the model provided muscular fat segmentation with a Dice coefficient of 56.27%, which can be utilized for additional analyses as needed.</article>","contentLength":1937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CellFlow: Simulating Cellular Morphology Changes via Flow Matching","url":"https://arxiv.org/abs/2502.09775","date":1739768400,"author":"","guid":1227,"unread":true,"content":"<article>arXiv:2502.09775v1 Announce Type: cross \nAbstract: Building a virtual cell capable of accurately simulating cellular behaviors in silico has long been a dream in computational biology. We introduce CellFlow, an image-generative model that simulates cellular morphology changes induced by chemical and genetic perturbations using flow matching. Unlike prior methods, CellFlow models distribution-wise transformations from unperturbed to perturbed cell states, effectively distinguishing actual perturbation effects from experimental artifacts such as batch effects -- a major challenge in biological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined perturbation (JUMP) datasets, CellFlow generates biologically meaningful cell images that faithfully capture perturbation-specific morphological changes, achieving a 35% improvement in FID scores and a 12% increase in mode-of-action prediction accuracy over existing methods. Additionally, CellFlow enables continuous interpolation between cellular states, providing a potential tool for studying perturbation dynamics. These capabilities mark a significant step toward realizing virtual cell modeling for biomedical research.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implementation and Analysis of Regev's Quantum Factorization Algorithm","url":"https://arxiv.org/abs/2502.09772","date":1739768400,"author":"","guid":1228,"unread":true,"content":"<article>arXiv:2502.09772v1 Announce Type: cross \nAbstract: Quantum computing represents a significant advancement in computational capabilities. Of particular concern is its impact on asymmetric cryptography through, notably, Shor's algorithm and the more recently developed Regev's algorithm for factoring composite numbers. We present our implementation of the latter. Our analysis encompasses both quantum simulation results and classical component examples, with particular emphasis on comparative cases between Regev's and Shor's algorithms. Our experimental results reveal that Regev's algorithm indeed outperforms Shor's algorithm for certain composite numbers in practice. However, we observed significant performance variations across different input values. Despite Regev's algorithm's theoretical asymptotic efficiency advantage, our implementation exhibited execution times longer than Shor's algorithm for small integer factorization in both quantum and classical components. These findings offer insights into the practical challenges and performance characteristics of implementing Regev's algorithm in realistic quantum computing scenarios.</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-consistent bounds method for dissipative PDEs","url":"https://arxiv.org/abs/2502.09760","date":1739768400,"author":"","guid":1229,"unread":true,"content":"<article>arXiv:2502.09760v1 Announce Type: cross \nAbstract: We discuss the method of self-consistent bounds for dissipative PDEs with periodic boundary conditions. We prove convergence theorems for a class of dissipative PDEs, which constitute a theoretical basis of a general framework for construction of an algorithm that computes bounds for the solutions of the underlying PDE and its dependence on initial conditions.\n  We also show, that the classical examples of parabolic PDEs including Kuramoto-Sivashinsky equation and the Navier-Stokes on the torus fit into this framework.</article>","contentLength":575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast Inexact Bilevel Optimization for Analytical Deep Image Priors","url":"https://arxiv.org/abs/2502.09758","date":1739768400,"author":"","guid":1230,"unread":true,"content":"<article>arXiv:2502.09758v1 Announce Type: cross \nAbstract: The analytical deep image prior (ADP) introduced by Dittmer et al. (2020) establishes a link between deep image priors and classical regularization theory via bilevel optimization. While this is an elegant construction, it involves expensive computations if the lower-level problem is to be solved accurately. To overcome this issue, we propose to use adaptive inexact bilevel optimization to solve ADP problems. We discuss an extension of a recent inexact bilevel method called the method of adaptive inexact descent of Salehi et al.(2024) to an infinite-dimensional setting required by the ADP framework. In our numerical experiments we demonstrate that the computational speed-up achieved by adaptive inexact bilevel optimization allows one to use ADP on larger-scale problems than in the previous literature, e.g. in deblurring of 2D color images.</article>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contracting Strategies for Electrolyzers to Secure Grid Connection: The Dutch Case","url":"https://arxiv.org/abs/2502.09748","date":1739768400,"author":"","guid":1231,"unread":true,"content":"<article>arXiv:2502.09748v1 Announce Type: cross \nAbstract: In response to increasing grid congestion in the Netherlands, non-firm connection and transport agreements (CTAs) and capacity restriction contracts (CRCs) have been introduced, allowing consumer curtailment in exchange for grid tariff discounts or per-MW compensations. This study examines the interaction between an electrolyzer project, facing sizing and contracting decisions, and a network operator, responsible for contract activations and determining grid connection capacity, under the new Dutch regulations. The interaction is modeled using two bilevel optimization problems with alternating leader-follower roles. Results highlight a trade-off between CRC income and non-firm CTA tariff discounts, showing that voluntary congestion management by the network operator increases electrolyzer profitability at CRC prices below 10 euro per MW but reduces it at higher prices. Furthermore, the network operator benefits more from reacting to the electrolyzer owner's CTA decisions than from leading the interaction at CRC prices above 10 euro per MW. Ignoring the other party's optimization problem overestimates profits for both the network operator and the electrolyzer owner, emphasizing the importance of coordinated decision-making.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Iterative quantum optimisation with a warm-started quantum state","url":"https://arxiv.org/abs/2502.09704","date":1739768400,"author":"","guid":1232,"unread":true,"content":"<article>arXiv:2502.09704v1 Announce Type: cross \nAbstract: We provide a method to prepare a warm-started quantum state from measurements with an iterative framework to enhance the quantum approximate optimisation algorithm (QAOA). The numerical simulations show the method can effectively address the \"stuck issue\" of the standard QAOA using a single-string warm-started initial state described in [Cain et al., 2023]. When applied to the $3$-regular MaxCut problem, our approach achieves an improved approximation ratio, with a lower bound that iteratively converges toward the best classical algorithms for $p=1$ standard QAOA. Additionally, in the context of the discrete global minimal variance portfolio (DGMVP) model, simulations reveal a more favourable scaling of identifying the global minimal compared to the QAOA standalone, the single-string warm-started QAOA and a classical constrained sampling approach.</article>","contentLength":910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lifespan tree of brain anatomy: diagnostic values for motor and cognitive neurodegenerative diseases","url":"https://arxiv.org/abs/2502.09682","date":1739768400,"author":"","guid":1233,"unread":true,"content":"<article>arXiv:2502.09682v1 Announce Type: cross \nAbstract: The differential diagnosis of neurodegenerative diseases, characterized by overlapping symptoms, may be challenging. Brain imaging coupled with artificial intelligence has been previously proposed for diagnostic support, but most of these methods have been trained to discriminate only isolated diseases from controls. Here, we develop a novel machine learning framework, named lifespan tree of brain anatomy, dedicated to the differential diagnosis between multiple diseases simultaneously. It integrates the modeling of volume changes for 124 brain structures during the lifespan with non-linear dimensionality reduction and synthetic sampling techniques to create easily interpretable representations of brain anatomy over the course of disease progression. As clinically relevant proof- of-concept applications, we constructed a cognitive lifespan tree of brain anatomy for the differential diagnosis of six causes of neurodegenerative dementia and a motor lifespan tree of brain anatomy for the differential diagnosis of four causes of parkinsonism using 37594 MRI as a training dataset. This original approach enhanced significantly the efficiency of differential diagnosis in the external validation cohort of 1754 cases, outperforming existing state-of-the art machine learning techniques. Lifespan tree holds promise as a valuable tool for differential diagnostic in relevant clinical conditions, especially for diseases still lacking effective biological markers.</article>","contentLength":1524,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalizable Cervical Cancer Screening via Large-scale Pretraining and Test-Time Adaptation","url":"https://arxiv.org/abs/2502.09662","date":1739768400,"author":"","guid":1234,"unread":true,"content":"<article>arXiv:2502.09662v1 Announce Type: cross \nAbstract: Cervical cancer is a leading malignancy in female reproductive system. While AI-assisted cytology offers a cost-effective and non-invasive screening solution, current systems struggle with generalizability in complex clinical scenarios. To address this issue, we introduced Smart-CCS, a generalizable Cervical Cancer Screening paradigm based on pretraining and adaptation to create robust and generalizable screening systems. To develop and validate Smart-CCS, we first curated a large-scale, multi-center dataset named CCS-127K, which comprises a total of 127,471 cervical cytology whole-slide images collected from 48 medical centers. By leveraging large-scale self-supervised pretraining, our CCS models are equipped with strong generalization capability, potentially generalizing across diverse scenarios. Then, we incorporated test-time adaptation to specifically optimize the trained CCS model for complex clinical settings, which adapts and refines predictions, improving real-world applicability. We conducted large-scale system evaluation among various cohorts. In retrospective cohorts, Smart-CCS achieved an overall area under the curve (AUC) value of 0.965 and sensitivity of 0.913 for cancer screening on 11 internal test datasets. In external testing, system performance maintained high at 0.950 AUC across 6 independent test datasets. In prospective cohorts, our Smart-CCS achieved AUCs of 0.947, 0.924, and 0.986 in three prospective centers, respectively. Moreover, the system demonstrated superior sensitivity in diagnosing cervical cancer, confirming the accuracy of our cancer screening results by using histology findings for validation. Interpretability analysis with cell and slide predictions further indicated that the system's decision-making aligns with clinical practice. Smart-CCS represents a significant advancement in cancer screening across diverse clinical contexts.</article>","contentLength":1951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Omics Fusion with Soft Labeling for Enhanced Prediction of Distant Metastasis in Nasopharyngeal Carcinoma Patients after Radiotherapy","url":"https://arxiv.org/abs/2502.09656","date":1739768400,"author":"","guid":1235,"unread":true,"content":"<article>arXiv:2502.09656v1 Announce Type: cross \nAbstract: Omics fusion has emerged as a crucial preprocessing approach in the field of medical image processing, providing significant assistance to several studies. One of the challenges encountered in the integration of omics data is the presence of unpredictability arising from disparities in data sources and medical imaging equipment. In order to overcome this challenge and facilitate the integration of their joint application to specific medical objectives, this study aims to develop a fusion methodology that mitigates the disparities inherent in omics data. The utilization of the multi-kernel late-fusion method has gained significant popularity as an effective strategy for addressing this particular challenge. An efficient representation of the data may be achieved by utilizing a suitable single-kernel function to map the inherent features and afterward merging them in a space with a high number of dimensions. This approach effectively addresses the differences noted before. The inflexibility of label fitting poses a constraint on the use of multi-kernel late-fusion methods in complex nasopharyngeal carcinoma (NPC) datasets, hence affecting the efficacy of general classifiers in dealing with high-dimensional characteristics. This innovative methodology aims to increase the disparity between the two cohorts, hence providing a more flexible structure for the allocation of labels. The examination of the NPC-ContraParotid dataset demonstrates the model's robustness and efficacy, indicating its potential as a valuable tool for predicting distant metastases in patients with nasopharyngeal carcinoma (NPC).</article>","contentLength":1673,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Heterogeneous Mixture of Experts for Remote Sensing Image Super-Resolution","url":"https://arxiv.org/abs/2502.09654","date":1739768400,"author":"","guid":1236,"unread":true,"content":"<article>arXiv:2502.09654v1 Announce Type: cross \nAbstract: Remote sensing image super-resolution (SR) aims to reconstruct high-resolution remote sensing images from low-resolution inputs, thereby addressing limitations imposed by sensors and imaging conditions. However, the inherent characteristics of remote sensing images, including diverse ground object types and complex details, pose significant challenges to achieving high-quality reconstruction. Existing methods typically employ a uniform structure to process various types of ground objects without distinction, making it difficult to adapt to the complex characteristics of remote sensing images. To address this issue, we introduce a Mixture of Experts (MoE) model and design a set of heterogeneous experts. These experts are organized into multiple expert groups, where experts within each group are homogeneous while being heterogeneous across groups. This design ensures that specialized activation parameters can be employed to handle the diverse and intricate details of ground objects effectively. To better accommodate the heterogeneous experts, we propose a multi-level feature aggregation strategy to guide the routing process. Additionally, we develop a dual-routing mechanism to adaptively select the optimal expert for each pixel. Experiments conducted on the UCMerced and AID datasets demonstrate that our proposed method achieves superior SR reconstruction accuracy compared to state-of-the-art methods. The code will be available at https://github.com/Mr-Bamboo/MFG-HMoE.</article>","contentLength":1541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SASVi - Segment Any Surgical Video","url":"https://arxiv.org/abs/2502.09653","date":1739768400,"author":"","guid":1237,"unread":true,"content":"<article>arXiv:2502.09653v1 Announce Type: cross \nAbstract: Purpose: Foundation models, trained on multitudes of public datasets, often require additional fine-tuning or re-prompting mechanisms to be applied to visually distinct target domains such as surgical videos. Further, without domain knowledge, they cannot model the specific semantics of the target domain. Hence, when applied to surgical video segmentation, they fail to generalise to sections where previously tracked objects leave the scene or new objects enter. Methods: We propose SASVi, a novel re-prompting mechanism based on a frame-wise Mask R-CNN Overseer model, which is trained on a minimal amount of scarcely available annotations for the target domain. This model automatically re-prompts the foundation model SAM2 when the scene constellation changes, allowing for temporally smooth and complete segmentation of full surgical videos. Results: Re-prompting based on our Overseer model significantly improves the temporal consistency of surgical video segmentation compared to similar prompting techniques and especially frame-wise segmentation, which neglects temporal information, by at least 1.5%. Our proposed approach allows us to successfully deploy SAM2 to surgical videos, which we quantitatively and qualitatively demonstrate for three different cholecystectomy and cataract surgery datasets. Conclusion: SASVi can serve as a new baseline for smooth and temporally consistent segmentation of surgical videos with scarcely available annotation data. Our method allows us to leverage scarce annotations and obtain complete annotations for full videos of the large-scale counterpart datasets. We make those annotations publicly available, providing extensive annotation data for the future development of surgical data science models.</article>","contentLength":1804,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Volumetric Temporal Texture Synthesis for Smoke Stylization using Neural Cellular Automata","url":"https://arxiv.org/abs/2502.09631","date":1739768400,"author":"","guid":1238,"unread":true,"content":"<article>arXiv:2502.09631v1 Announce Type: cross \nAbstract: Artistic stylization of 3D volumetric smoke data is still a challenge in computer graphics due to the difficulty of ensuring spatiotemporal consistency given a reference style image, and that within reasonable time and computational resources. In this work, we introduce Volumetric Neural Cellular Automata (VNCA), a novel model for efficient volumetric style transfer that synthesizes, in real-time, multi-view consistent stylizing features on the target smoke with temporally coherent transitions between stylized simulation frames. VNCA synthesizes a 3D texture volume with color and density stylization and dynamically aligns this volume with the intricate motion patterns of the smoke simulation under the Eulerian framework. Our approach replaces the explicit fluid advection modeling and the inter-frame smoothing terms with the self-emerging motion of the underlying cellular automaton, thus reducing the training time by over an order of magnitude. Beyond smoke simulations, we demonstrate the versatility of our approach by showcasing its applicability to mesh stylization.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Bias, Fairness, and Bias Mitigation for a Wearable-based Freezing of Gait Detection in Parkinson's Disease","url":"https://arxiv.org/abs/2502.09626","date":1739768400,"author":"","guid":1239,"unread":true,"content":"<article>arXiv:2502.09626v1 Announce Type: cross \nAbstract: Freezing of gait (FOG) is a debilitating feature of Parkinson's disease (PD), which is a cause of injurious falls among PD patients. Recent advances in wearable-based human activity recognition (HAR) technology have enabled the detection of FOG subtypes across benchmark datasets. Since FOG manifestation is heterogeneous, developing models that quantify FOG consistently across patients with varying demographics, FOG types, and PD conditions is important. Bias and fairness in FOG models remain understudied in HAR, with research focused mainly on FOG detection using single benchmark datasets. We evaluated the bias and fairness of HAR models for wearable-based FOG detection across demographics and PD conditions using multiple datasets and the effectiveness of transfer learning as a potential bias mitigation approach. Our evaluation using demographic parity ratio (DPR) and equalized odds ratio (EOR) showed model bias (DPR &amp; EOR &lt; 0.8) for all stratified demographic variables, including age, sex, and disease duration. Our experiments demonstrated that transfer learning from multi-site datasets and generic human activity representations significantly improved fairness (average change in DPR +0.027, +0.039, respectively) and performance (average change in F1-score +0.026, +0.018, respectively) across attributes, supporting the hypothesis that generic human activity representations learn fairer representations applicable to health analytics.</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transformer Based Time-Series Forecasting for Stock","url":"https://arxiv.org/abs/2502.09625","date":1739768400,"author":"","guid":1240,"unread":true,"content":"<article>arXiv:2502.09625v1 Announce Type: cross \nAbstract: To the naked eye, stock prices are considered chaotic, dynamic, and unpredictable. Indeed, it is one of the most difficult forecasting tasks that hundreds of millions of retail traders and professional traders around the world try to do every second even before the market opens. With recent advances in the development of machine learning and the amount of data the market generated over years, applying machine learning techniques such as deep learning neural networks is unavoidable. In this work, we modeled the task as a multivariate forecasting problem, instead of a naive autoregression problem. The multivariate analysis is done using the attention mechanism via applying a mutated version of the Transformer, \"Stockformer\", which we created.</article>","contentLength":801,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"iFormer: Integrating ConvNet and Transformer for Mobile Application","url":"https://arxiv.org/abs/2501.15369","date":1739768400,"author":"","guid":1241,"unread":true,"content":"<article>arXiv:2501.15369v1 Announce Type: cross \nAbstract: We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming a standard convolutional network, \\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks. Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding","url":"https://arxiv.org/abs/2502.10392","date":1739768400,"author":"","guid":1242,"unread":true,"content":"<article>arXiv:2502.10392v1 Announce Type: new \nAbstract: In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code is available at \\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.</article>","contentLength":1671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MM-RLHF: The Next Step Forward in Multimodal LLM Alignment","url":"https://arxiv.org/abs/2502.10391","date":1739768400,"author":"","guid":1243,"unread":true,"content":"<article>arXiv:2502.10391v1 Announce Type: new \nAbstract: Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across $\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a $\\mathbf{19.5}$% increase in conversational abilities and a $\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.</article>","contentLength":1955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"(How) Can Transformers Predict Pseudo-Random Numbers?","url":"https://arxiv.org/abs/2502.10390","date":1739768400,"author":"","guid":1244,"unread":true,"content":"<article>arXiv:2502.10390v1 Announce Type: new \nAbstract: Transformers excel at discovering patterns in sequential data, yet their fundamental limitations and learning mechanisms remain crucial topics of investigation. In this paper, we study the ability of Transformers to learn pseudo-random number sequences from linear congruential generators (LCGs), defined by the recurrence relation $x_{t+1} = a x_t + c \\;\\mathrm{mod}\\; m$. Our analysis reveals that with sufficient architectural capacity and training data variety, Transformers can perform in-context prediction of LCG sequences with unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding layers and attention patterns, we uncover how Transformers develop algorithmic structures to learn these sequences in two scenarios of increasing complexity. First, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but fixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our analysis reveals that models learn to factorize the modulus and utilize digit-wise number representations to make sequential predictions. In the second, more challenging scenario of unseen moduli, we show that Transformers can generalize to unseen moduli up to $m_{\\text{test}} = 2^{16}$. In this case, the model employs a two-step strategy: first estimating the unknown modulus from the context, then utilizing prime factorizations to generate predictions. For this task, we observe a sharp transition in the accuracy at a critical depth $=3$. We also find that the number of in-context sequence elements needed to reach high accuracy scales sublinearly with the modulus.</article>","contentLength":1646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Region-Adaptive Sampling for Diffusion Transformers","url":"https://arxiv.org/abs/2502.10389","date":1739768400,"author":"","guid":1245,"unread":true,"content":"<article>arXiv:2502.10389v1 Announce Type: new \nAbstract: Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.</article>","contentLength":1687,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction","url":"https://arxiv.org/abs/2502.10388","date":1739768400,"author":"","guid":1246,"unread":true,"content":"<article>arXiv:2502.10388v1 Announce Type: new \nAbstract: Recent progress in large language models (LLMs) has enabled the automated processing of lengthy documents even without supervised training on a task-specific dataset. Yet, their zero-shot performance in complex tasks as opposed to straightforward information extraction tasks remains suboptimal. One feasible approach for tasks with lengthy, complex input is to first summarize the document and then apply supervised fine-tuning to the summary. However, the summarization process inevitably results in some loss of information. In this study we present a method for processing the summaries of long documents aimed to capture different important aspects of the original document. We hypothesize that LLM summaries generated with different aspect-oriented prompts contain different \\textit{information signals}, and we propose methods to measure these differences. We introduce approaches to effectively integrate signals from these different summaries for supervised training of transformer models. We validate our hypotheses on a high-impact task -- 30-day readmission prediction from a psychiatric discharge -- using real-world data from four hospitals, and show that our proposed method increases the prediction performance for the complex task of predicting patient outcome.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simplifying DINO via Coding Rate Regularization","url":"https://arxiv.org/abs/2502.10385","date":1739768400,"author":"","guid":1247,"unread":true,"content":"<article>arXiv:2502.10385v1 Announce Type: new \nAbstract: DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-the-art performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable -- many hyperparameters need to be carefully tuned to ensure that the representations do not collapse -- which poses considerable difficulty to improving them or adapting them to new domains. In this work, we posit that we can remove most such-motivated idiosyncrasies in the pre-training pipelines, and only need to add an explicit coding rate term in the loss function to avoid collapse of the representations. As a result, we obtain highly simplified variants of the DINO and DINOv2 which we call SimDINO and SimDINOv2, respectively. Remarkably, these simplified models are more robust to different design choices, such as network architecture and hyperparameters, and they learn even higher-quality representations, measured by performance on downstream tasks, offering a Pareto improvement over the corresponding DINO and DINOv2 models. This work highlights the potential of using simplifying design principles to improve the empirical practice of deep learning.</article>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Representation and Interpretation in Artificial and Natural Computing","url":"https://arxiv.org/abs/2502.10383","date":1739768400,"author":"","guid":1248,"unread":true,"content":"<article>arXiv:2502.10383v1 Announce Type: new \nAbstract: Artificial computing machinery transforms representations through an objective process, to be interpreted subjectively by humans, so the machine and the interpreter are different entities, but in the putative natural computing both processes are performed by the same agent. The method or process that transforms a representation is called here \\emph{the mode of computing}. The mode used by digital computers is the algorithmic one, but there are others, such as quantum computers and diverse forms of non-conventional computing, and there is an open-ended set of representational formats and modes that could be used in artificial and natural computing. A mode based on a notion of computing different from Turing's may perform feats beyond what the Turing Machine does but the modes would not be of the same kind and could not be compared. For a mode of computing to be more powerful than the algorithmic one, it ought to compute functions lacking an effective algorithm, and Church Thesis would not hold. Here, a thought experiment including a computational demon using a hypothetical mode for such an effect is presented. If there is natural computing, there is a mode of natural computing whose properties may be causal to the phenomenological experience. Discovering it would come with solving the hard problem of consciousness; but if it turns out that such a mode does not exist, there is no such thing as natural computing, and the mind is not a computational process.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Balancing the Scales: A Theoretical and Algorithmic Framework for Learning from Imbalanced Data","url":"https://arxiv.org/abs/2502.10381","date":1739768400,"author":"","guid":1249,"unread":true,"content":"<article>arXiv:2502.10381v1 Announce Type: new \nAbstract: Class imbalance remains a major challenge in machine learning, especially in multi-class problems with long-tailed distributions. Existing methods, such as data resampling, cost-sensitive techniques, and logistic loss modifications, though popular and often effective, lack solid theoretical foundations. As an example, we demonstrate that cost-sensitive methods are not Bayes consistent. This paper introduces a novel theoretical framework for analyzing generalization in imbalanced classification. We propose a new class-imbalanced margin loss function for both binary and multi-class settings, prove its strong $H$-consistency, and derive corresponding learning guarantees based on empirical loss and a new notion of class-sensitive Rademacher complexity. Leveraging these theoretical results, we devise novel and general learning algorithms, IMMAX (Imbalanced Margin Maximization), which incorporate confidence margins and are applicable to various hypothesis sets. While our focus is theoretical, we also present extensive empirical results demonstrating the effectiveness of our algorithms compared to existing baselines.</article>","contentLength":1176,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unknown Word Detection for English as a Second Language (ESL) Learners Using Gaze and Pre-trained Language Models","url":"https://arxiv.org/abs/2502.10378","date":1739768400,"author":"","guid":1250,"unread":true,"content":"<article>arXiv:2502.10378v1 Announce Type: new \nAbstract: English as a Second Language (ESL) learners often encounter unknown words that hinder their text comprehension. Automatically detecting these words as users read can enable computing systems to provide just-in-time definitions, synonyms, or contextual explanations, thereby helping users learn vocabulary in a natural and seamless manner. This paper presents EyeLingo, a transformer-based machine learning method that predicts the probability of unknown words based on text content and eye gaze trajectory in real time with high accuracy. A 20-participant user study revealed that our method can achieve an accuracy of 97.6%, and an F1-score of 71.1%. We implemented a real-time reading assistance prototype to show the effectiveness of EyeLingo. The user study shows improvement in willingness to use and usefulness compared to baseline methods.</article>","contentLength":895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences","url":"https://arxiv.org/abs/2502.10377","date":1739768400,"author":"","guid":1251,"unread":true,"content":"<article>arXiv:2502.10377v1 Announce Type: new \nAbstract: We introduce ReStyle3D, a novel framework for scene-level appearance transfer from a single style image to a real-world scene represented by multiple views. The method combines explicit semantic correspondences with multi-view consistency to achieve precise and coherent stylization. Unlike conventional stylization methods that apply a reference style globally, ReStyle3D uses open-vocabulary segmentation to establish dense, instance-level correspondences between the style and real-world images. This ensures that each object is stylized with semantically matched textures. It first transfers the style to a single view using a training-free semantic-attention mechanism in a diffusion model. It then lifts the stylization to additional views via a learned warp-and-refine network guided by monocular depth and pixel-wise correspondences. Experiments show that ReStyle3D consistently outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence. User studies further validate its ability to produce photo-realistic, semantically faithful results. Our code, pretrained models, and dataset will be publicly released, to support new applications in interior design, virtual staging, and 3D-consistent stylization.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robustness tests for biomedical foundation models should tailor to specification","url":"https://arxiv.org/abs/2502.10374","date":1739768400,"author":"","guid":1252,"unread":true,"content":"<article>arXiv:2502.10374v1 Announce Type: new \nAbstract: Existing regulatory frameworks for biomedical AI include robustness as a key component but lack detailed implementational guidance. The recent rise of biomedical foundation models creates new hurdles in testing and certification given their broad capabilities and susceptibility to complex distribution shifts. To balance test feasibility and effectiveness, we suggest a priority-based, task-oriented approach to tailor robustness evaluation objectives to a predefined specification. We urge concrete policies to adopt a granular categorization of robustness concepts in the specification. Our approach promotes the standardization of risk assessment and monitoring, which guides technical developments and mitigation efforts.</article>","contentLength":775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models","url":"https://arxiv.org/abs/2502.10373","date":1739768400,"author":"","guid":1253,"unread":true,"content":"<article>arXiv:2502.10373v1 Announce Type: new \nAbstract: Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages/dialects, helping to mitigate bias and improve the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d for future studies.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized State Estimation and Opacity Verification Based on Partially Ordered Observation Sequences","url":"https://arxiv.org/abs/2502.10367","date":1739768400,"author":"","guid":1254,"unread":true,"content":"<article>arXiv:2502.10367v1 Announce Type: new \nAbstract: In this paper, we investigate state estimation and opacity verification problems within a decentralized observation architecture. Specifically, we consider a discrete event system whose behavior is recorded by a set of observation sites. These sites transmit the partially ordered sequences of observations that they record to a coordinator whenever a \\textit{synchronization} occurs. To properly analyze the system behavior from the coordinator's viewpoint, we first introduce the notion of an \\textit{All Sequence Structure} (ASS), which concisely captures the state evolution of each system state upon different information provided by the observation sites. Based on the ASS, we then construct corresponding current-state and initial-state estimators for offline state estimation at the coordinator. When used to verify state-isolation properties under this decentralized architecture, the use of ASS demonstrates a significant reduction in complexity compared with existing approaches in the literature. In particular, we discuss how to verify initial-state opacity at the coordinator, as well as a novel opacity notion, namely current-state-at-synchronization opacity.</article>","contentLength":1223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AffinityFlow: Guided Flows for Antibody Affinity Maturation","url":"https://arxiv.org/abs/2502.10365","date":1739768400,"author":"","guid":1255,"unread":true,"content":"<article>arXiv:2502.10365v1 Announce Type: new \nAbstract: Antibodies are widely used as therapeutics, but their development requires costly affinity maturation, involving iterative mutations to enhance binding affinity.This paper explores a sequence-only scenario for affinity maturation, using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold within flow matching to generate diverse protein structures, enabling a sequence-conditioned generative model of structure. Building on this, we propose an alternating optimization framework that (1) fixes the sequence to guide structure generation toward high binding affinity using a structure-based affinity predictor, then (2) applies inverse folding to create sequence mutations, refined by a sequence-based affinity predictor for post selection. To address this, we develop a co-teaching module that incorporates valuable information from noisy biophysical energies into predictor refinement. The sequence-based predictor selects consensus samples to teach the structure-based predictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art performance in affinity maturation experiments. We plan to open-source our code after acceptance.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds","url":"https://arxiv.org/abs/2502.10363","date":1739768400,"author":"","guid":1256,"unread":true,"content":"<article>arXiv:2502.10363v1 Announce Type: new \nAbstract: Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing approaches designed for quadrupedal robots often fail to generalize to humanoid robots due to differences in foot geometry and unstable morphology, while learning-based approaches for humanoid locomotion still face great challenges on complex terrains due to sparse foothold reward signals and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the learning process between dense locomotion rewards and sparse foothold rewards. To encourage sufficient trail-and-error exploration, BeamDojo incorporates a two-stage RL approach: the first stage relaxes the terrain dynamics by training the humanoid on flat terrain while providing it with task terrain perceptive observations, and the second stage fine-tunes the policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based elevation map to enable real-world deployment. Extensive simulation and real-world experiments demonstrate that BeamDojo achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.</article>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages","url":"https://arxiv.org/abs/2502.10362","date":1739768400,"author":"","guid":1257,"unread":true,"content":"<article>arXiv:2502.10362v1 Announce Type: new \nAbstract: CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.</article>","contentLength":1182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Multilingual LLM Pretraining with Model-Based Data Selection","url":"https://arxiv.org/abs/2502.10361","date":1739768400,"author":"","guid":1258,"unread":true,"content":"<article>arXiv:2502.10361v1 Announce Type: new \nAbstract: Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.</article>","contentLength":1309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proper Learnability and the Role of Unlabeled Data","url":"https://arxiv.org/abs/2502.10359","date":1739768400,"author":"","guid":1259,"unread":true,"content":"<article>arXiv:2502.10359v1 Announce Type: new \nAbstract: Proper learning refers to the setting in which learners must emit predictors in the underlying hypothesis class $H$, and often leads to learners with simple algorithmic forms (e.g. empirical risk minimization (ERM), structural risk minimization (SRM)). The limitation of proper learning, however, is that there exist problems which can only be learned improperly, e.g. in multiclass classification. Thus, we ask: Under what assumptions on the hypothesis class or the information provided to the learner is a problem properly learnable? We first demonstrate that when the unlabeled data distribution is given, there always exists an optimal proper learner governed by distributional regularization, a randomized generalization of regularization. We refer to this setting as the distribution-fixed PAC model, and continue to evaluate the learner on its worst-case performance over all distributions. Our result holds for all metric loss functions and any finite learning problem (with no dependence on its size). Further, we demonstrate that sample complexities in the distribution-fixed PAC model can shrink by only a logarithmic factor from the classic PAC model, strongly refuting the role of unlabeled data in PAC learning (from a worst-case perspective).\n  We complement this with impossibility results which obstruct any characterization of proper learnability in the realizable PAC model. First, we observe that there are problems whose proper learnability is logically undecidable, i.e., independent of the ZFC axioms. We then show that proper learnability is not a monotone property of the underlying hypothesis class, and that it is not a local property (in a precise sense). Our impossibility results all hold even for the fundamental setting of multiclass classification, and go through a reduction of EMX learning (Ben-David et al., 2019) to proper classification which may be of independent interest.</article>","contentLength":1961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dimension-free Score Matching and Time Bootstrapping for Diffusion Models","url":"https://arxiv.org/abs/2502.10354","date":1739768400,"author":"","guid":1260,"unread":true,"content":"<article>arXiv:2502.10354v1 Announce Type: new \nAbstract: Diffusion models generate samples by estimating the score function of the target distribution at various noise levels. The model is trained using samples drawn from the target distribution, progressively adding noise. In this work, we establish the first (nearly) dimension-free sample complexity bounds for learning these score functions, achieving a double exponential improvement in dimension over prior results. A key aspect of our analysis is the use of a single function approximator to jointly estimate scores across noise levels, a critical feature of diffusion models in practice which enables generalization across timesteps. Our analysis introduces a novel martingale-based error decomposition and sharp variance bounds, enabling efficient learning from dependent data generated by Markov processes, which may be of independent interest. Building on these insights, we propose Bootstrapped Score Matching (BSM), a variance reduction technique that utilizes previously learned scores to improve accuracy at higher noise levels. These results provide crucial insights into the efficiency and effectiveness of diffusion models for generative modeling.</article>","contentLength":1208,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Assortment Optimization for Patient-Provider Matching","url":"https://arxiv.org/abs/2502.10353","date":1739768400,"author":"","guid":1261,"unread":true,"content":"<article>arXiv:2502.10353v1 Announce Type: new \nAbstract: Rising provider turnover forces healthcare administrators to frequently rematch patients to available providers, which can be cumbersome and labor-intensive. To reduce the burden of rematching, we study algorithms for matching patients and providers through assortment optimization. We develop a patient-provider matching model in which we simultaneously offer each patient a menu of providers, and patients subsequently respond and select providers. By offering assortments upfront, administrators can balance logistical ease and patient autonomy. We study policies for assortment optimization and characterize their performance under different problem settings. We demonstrate that the selection of assortment policy is highly dependent on problem specifics and, in particular, on a patient's willingness to match and the ratio between patients and providers. On real-world data, we show that our best policy can improve match quality by 13% over a greedy solution by tailoring assortment sizes based on patient characteristics. We conclude with recommendations for running a real-world patient-provider matching system inspired by our results.</article>","contentLength":1195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic Verification for Ambiguous Query Disambiguation","url":"https://arxiv.org/abs/2502.10352","date":1739768400,"author":"","guid":1262,"unread":true,"content":"<article>arXiv:2502.10352v1 Announce Type: new \nAbstract: In this work, we tackle the challenge of disambiguating queries in retrieval-augmented generation (RAG) to diverse yet answerable interpretations. State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse interpretations are generated by an LLM, later used as search queries to retrieve supporting passages. Such a process may introduce noise in either interpretations or retrieval, particularly in enterprise settings, where LLMs -- trained on static data -- may struggle with domain-specific disambiguations. Thus, a post-hoc verification phase is introduced to prune noises. Our distinction is to unify diversification with verification by incorporating feedback from retriever and generator early on. This joint approach improves both efficiency and robustness by reducing reliance on multiple retrieval and inference steps, which are susceptible to cascading errors. We validate the efficiency and effectiveness of our method, Verified-Diversification with Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve diverse yet verifiable interpretations. Empirical results show that VERDICT improves grounding-aware F1 score by an average of 23% over the strongest baseline across different backbone LLMs.</article>","contentLength":1295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Incremental Approximate Shortest Paths in Directed Graphs","url":"https://arxiv.org/abs/2502.10348","date":1739768400,"author":"","guid":1263,"unread":true,"content":"<article>arXiv:2502.10348v1 Announce Type: new \nAbstract: In this paper, we show new data structures maintaining approximate shortest paths in sparse directed graphs with polynomially bounded non-negative edge weights under edge insertions.\n  We give more efficient incremental $(1+\\epsilon)$-approximate APSP data structures that work against an adaptive adversary: a deterministic one with $\\tilde{O}(m^{3/2}n^{3/4})$ total update time and a randomized one with $\\tilde{O}(m^{4/3}n^{5/6})$ total update time. For sparse graphs, these both improve polynomially upon the best-known bound against an adaptive adversary. To achieve that, building on the ideas of [Chechik-Zhang, SODA'21] and [Kyng-Meierhans-Probst Gutenberg, SODA'22], we show a near-optimal $(1+\\epsilon)$-approximate incremental SSSP data structure for a special case when all edge updates are adjacent to the source, that might be of independent interest.\n  We also describe a very simple and near-optimal \\emph{offline} incremental $(1+\\epsilon)$-approximate SSSP data structure. While online near-linear partially dynamic SSSP data structures have been elusive so far (except for dense instances), our result excludes using certain types of impossibility arguments to rule them out. Additionally, our offline solution leads to near-optimal and deterministic all-pairs bounded-leg shortest paths data structure for sparse graphs.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Organize the Web: Constructing Domains Enhances Pre-Training Data Curation","url":"https://arxiv.org/abs/2502.10341","date":1739768400,"author":"","guid":1264,"unread":true,"content":"<article>arXiv:2502.10341v1 Announce Type: new \nAbstract: Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.</article>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STAR: Spectral Truncation and Rescale for Model Merging","url":"https://arxiv.org/abs/2502.10339","date":1739768400,"author":"","guid":1265,"unread":true,"content":"<article>arXiv:2502.10339v1 Announce Type: new \nAbstract: Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd $\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is publicly available at https://github.com/IBM/STAR.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering","url":"https://arxiv.org/abs/2502.10338","date":1739768400,"author":"","guid":1266,"unread":true,"content":"<article>arXiv:2502.10338v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel in natural language tasks but still face challenges in Question Answering (QA) tasks requiring complex, multi-step reasoning. We outline the types of reasoning required in some of these tasks, and reframe them in terms of meta-level reasoning (akin to high-level strategic reasoning or planning) and object-level reasoning (embodied in lower-level tasks such as mathematical reasoning). Franklin, a novel dataset with requirements of meta- and object-level reasoning, is introduced and used along with three other datasets to evaluate four LLMs at question answering tasks requiring multiple steps of reasoning. Results from human annotation studies suggest LLMs demonstrate meta-level reasoning with high frequency, but struggle with object-level reasoning tasks in some of the datasets used. Additionally, evidence suggests that LLMs find the object-level reasoning required for the questions in the Franklin dataset challenging, yet they do exhibit strong performance with respect to the meta-level reasoning requirements.</article>","contentLength":1109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ocular Disease Classification Using CNN with Deep Convolutional Generative Adversarial Network","url":"https://arxiv.org/abs/2502.10334","date":1739768400,"author":"","guid":1267,"unread":true,"content":"<article>arXiv:2502.10334v1 Announce Type: new \nAbstract: The Convolutional Neural Network (CNN) has shown impressive performance in image classification because of its strong learning capabilities. However, it demands a substantial and balanced dataset for effective training. Otherwise, networks frequently exhibit over fitting and struggle to generalize to new examples. Publicly available dataset of fundus images of ocular disease is insufficient to train any classification model to achieve satisfactory accuracy. So, we propose Generative Adversarial Network(GAN) based data generation technique to synthesize dataset for training CNN based classification model and later use original disease containing ocular images to test the model. During testing the model classification accuracy with the original ocular image, the model achieves an accuracy rate of 78.6% for myopia, 88.6% for glaucoma, and 84.6% for cataract, with an overall classification accuracy of 84.6%.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InfoPos: A ML-Assisted Solution Design Support Framework for Industrial Cyber-Physical Systems","url":"https://arxiv.org/abs/2502.10331","date":1739768400,"author":"","guid":1268,"unread":true,"content":"<article>arXiv:2502.10331v1 Announce Type: new \nAbstract: The variety of building blocks and algorithms incorporated in data-centric and ML-assisted solutions is high, contributing to two challenges: selection of most effective set and order of building blocks, as well as achieving such a selection with minimum cost. Considering that ML-assisted solution design is influenced by the extent of available data, as well as available knowledge of the target system, it is advantageous to be able to select matching building blocks. We introduce the first iteration of our InfoPos framework, allowing the placement of use-cases considering the available positions (levels), i.e., from poor to rich, of knowledge and data dimensions. With that input, designers and developers can reveal the most effective corresponding choice(s), streamlining the solution design process. The results from our demonstrator, an anomaly identification use-case for industrial Cyber-Physical Systems, reflects achieved effects upon the use of different building blocks throughout knowledge and data positions. The achieved ML model performance is considered as the indicator. Our data processing code and the composed data sets are publicly available.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiOpt: Self-supervised Diffusion for Constrained Optimization","url":"https://arxiv.org/abs/2502.10330","date":1739768400,"author":"","guid":1269,"unread":true,"content":"<article>arXiv:2502.10330v1 Announce Type: new \nAbstract: Recent advances in diffusion models show promising potential for learning-based optimization by leveraging their multimodal sampling capability to escape local optima. However, existing diffusion-based optimization approaches, often reliant on supervised training, lacks a mechanism to ensure strict constraint satisfaction which is often required in real-world applications. One resulting observation is the distributional misalignment, i.e. the generated solution distribution often exhibits small overlap with the feasible domain. In this paper, we propose DiOpt, a novel diffusion paradigm that systematically learns near-optimal feasible solution distributions through iterative self-training. Our framework introduces several key innovations: a target distribution specifically designed to maximize overlap with the constrained solution manifold; a bootstrapped self-training mechanism that adaptively weights candidate solutions based on the severity of constraint violations and optimality gaps; and a dynamic memory buffer that accelerates convergence by retaining high-quality solutions over training iterations. To our knowledge, DiOpt represents the first successful integration of self-supervised diffusion with hard constraint satisfaction. Evaluations on diverse tasks, including power grid control, motion retargeting, wireless allocation demonstrate its superiority in terms of both optimality and constraint satisfaction.</article>","contentLength":1488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking Effect","url":"https://arxiv.org/abs/2502.10329","date":1739768400,"author":"","guid":1270,"unread":true,"content":"<article>arXiv:2502.10329v1 Announce Type: new \nAbstract: The rapid advancements in AI voice cloning, fueled by machine learning, have significantly impacted text-to-speech (TTS) and voice conversion (VC) fields. While these developments have led to notable progress, they have also raised concerns about the misuse of AI VC technology, causing economic losses and negative public perceptions. To address this challenge, this study focuses on creating active defense mechanisms against AI VC systems.\n  We propose a novel active defense method, VocalCrypt, which embeds pseudo-timbre (jamming information) based on SFS into audio segments that are imperceptible to the human ear, thereby forming systematic fragments to prevent voice cloning. This approach protects the voice without compromising its quality. In comparison to existing methods, such as adversarial noise incorporation, VocalCrypt significantly enhances robustness and real-time performance, achieving a 500\\% increase in generation speed while maintaining interference effectiveness.\n  Unlike audio watermarking techniques, which focus on post-detection, our method offers preemptive defense, reducing implementation costs and enhancing feasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets show that our AI-cloned speech defense system performs excellently in automatic speaker verification (ASV) tests while preserving the integrity of the protected audio.</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Process Reward Models for LLM Agents: Practical Framework and Directions","url":"https://arxiv.org/abs/2502.10325","date":1739768400,"author":"","guid":1271,"unread":true,"content":"<article>arXiv:2502.10325v1 Announce Type: new \nAbstract: We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: https://github.com/sanjibanc/agent_prm.</article>","contentLength":954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis and Prediction of Coverage and Channel Rank for UAV Networks in Rural Scenarios with Foliage","url":"https://arxiv.org/abs/2502.10324","date":1739768400,"author":"","guid":1272,"unread":true,"content":"<article>arXiv:2502.10324v1 Announce Type: new \nAbstract: Unmanned aerial vehicles (UAVs) are expected to play a key role in 6G-enabled vehicular-to-everything (V2X) communications requiring high data rates, low latency, and reliable connectivity for mission-critical applications. Multi-input multi-output (MIMO) technology is essential for meeting these demands. However, UAV link performance is significantly affected by environmental factors such as signal attenuation, multipath propagation, and blockage from obstacles, particularly dense foliage in rural areas. In this paper, we investigate RF coverage and channel rank over UAV channels in foliage-dominated rural environments using ray tracing (RT) simulations. We conduct RT-based channel rank and RF coverage analysis over Lake Wheeler Field Labs at NC State University to examine the impact on UAV links. Custom-modeled trees are integrated into the RT simulations using NVIDIA Sionna, Blender, and Open Street Map (OSM) database to capture realistic blockage effects. Results indicate that tree-induced blockage impacts RF coverage and channel rank at lower UAV altitudes. We also propose a Kriging interpolation-based 3D channel rank interpolation scheme, leveraging the observed spatial correlation of channel rank in the given environments. The accuracy of the proposed scheme is evaluated using the mean absolute error (MAE) metric and compared against baseline interpolation methods. Finally, we compare the RT-based received signal strength (RSS) and channel rank results with real-world measurements from the NSF AERPAW testbed demonstrating reasonable consistency between simulation results and the measurements.</article>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Fraud Proof","url":"https://arxiv.org/abs/2502.10321","date":1739768400,"author":"","guid":1273,"unread":true,"content":"<article>arXiv:2502.10321v1 Announce Type: new \nAbstract: In this paper, we present a novel fraud-proof mechanism that achieves fast finality and, when combined with optimistic execution, enables real-time transaction processing. State-of-the-art optimistic rollups typically adopt a 7-day challenge window, during which any honest party can raise a challenge in case of fraud. We propose a new assert/challenge construction called \"Dynamic Fraud Proofs\" that achieves sub-second finality in ideal scenarios, while dynamically delaying settlement in the event of fraud detection and challenge resolution. The system relies on 1) a dynamic challenge period and 2) a configurable number of randomly selected verifier nodes who must interactively approve a state commitment without raising a challenge. If these conditions are not met, the state is not finalized, and the challenge period and approval criteria are dynamically adjusted. We provide a detailed analysis of the system's design, explaining how it maintains the assumption of a single honest node and addresses censorship attacks by inverting the traditional challenge process. Additionally, we formalize the system's probabilistic security model and discuss how bonding, incentives, and slashing mechanisms can encourage honest behavior, thereby increasing the likelihood of fast settlement in ideal scenarios.</article>","contentLength":1361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigations of multi-socket high core count RISC-V for HPC workloads","url":"https://arxiv.org/abs/2502.10320","date":1739768400,"author":"","guid":1274,"unread":true,"content":"<article>arXiv:2502.10320v1 Announce Type: new \nAbstract: Whilst RISC-V has become popular in fields such as embedded computing, it is yet to find mainstream success in High Performance Computing (HPC). However, the 64-core RISC-V Sophon SG2042 is a potential game changer as it provides a commodity available CPU with much higher core count than existing technologies. In this work we benchmark the SG2042 CPU hosted in an experimental, dual-socket, system to explore the performance properties of the CPU when running a common HPC benchmark suite across sockets. Earlier benchmarks found that, on the Milk-V Pioneer workstation, whilst the SG2042 performs well for compute bound codes, it struggles when pressure is placed on the memory subsystem. The performance results reported here confirm that, even on a different system, these memory performance limitations are still present and hence inherent in the CPU. However, a multi-socket configuration does enable the CPU to scale to a larger number of threads which, in the main, delivers an improvement in performance and-so this is a realistic system configuration for the HPC community.</article>","contentLength":1133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interval Selection with Binary Predictions","url":"https://arxiv.org/abs/2502.10314","date":1739768400,"author":"","guid":1275,"unread":true,"content":"<article>arXiv:2502.10314v1 Announce Type: new \nAbstract: Following a line of work that takes advantage of vast machine-learned data to enhance online algorithms with (possibly erroneous) information about future inputs, we consider predictions in the context of deterministic algorithms for the problem of selecting a maximum weight independent set of intervals arriving on the real line. We look at two weight functions, unit (constant) weights, and weights proportional to the interval's length. In the classical online model of irrevocable decisions, no algorithm can achieve constant competitiveness (Bachmann et al. [BHS13] for unit, Lipton and Tomkins [LT94] for proportional). In this setting, we show that a simple algorithm that is faithful to the predictions is optimal, and achieves an objective value of at least $OPT -\\eta$, with $\\eta$ being the total error in the predictions, both for unit, and proportional weights.\n  When revocable acceptances (a form of preemption) are allowed, the optimal deterministic algorithm for unit weights is $2k$-competitive [BK23], where $k$ is the number of different interval lengths. We give an algorithm with performance $OPT - \\eta$ (and therefore $1$-consistent), that is also $(2k +1)$-robust. For proportional weights, Garay et al. [GGKMY97] give an optimal $(2\\phi + 1)$-competitive algorithm, where $\\phi$ is the golden ratio. We present an algorithm with parameter $\\lambda &gt; 1$ that is $\\frac{3\\lambda}{\\lambda -1}$-consistent, and $\\frac{4\\lambda^2 +2\\lambda}{\\lambda -1}$-robust. Although these bounds are not tight, we show that for $\\lambda &gt; 3.42$ we achieve consistency better than the optimal online guarantee in [GGKMY97], while maintaining bounded robustness.\n  We conclude with some experimental results on real-world data that complement our theoretical findings, and show the benefit of prediction algorithms for online interval selection, even in the presence of high error.</article>","contentLength":1938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ExplainReduce: Summarising local explanations via proxies","url":"https://arxiv.org/abs/2502.10311","date":1739768400,"author":"","guid":1276,"unread":true,"content":"<article>arXiv:2502.10311v1 Announce Type: new \nAbstract: Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small \"proxy set\" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.</article>","contentLength":783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Object Detection and Tracking","url":"https://arxiv.org/abs/2502.10310","date":1739768400,"author":"","guid":1277,"unread":true,"content":"<article>arXiv:2502.10310v1 Announce Type: new \nAbstract: Efficient and accurate object detection is an important topic in the development of computer vision systems. With the advent of deep learning techniques, the accuracy of object detection has increased significantly. The project aims to integrate a modern technique for object detection with the aim of achieving high accuracy with real-time performance. The reliance on other computer vision algorithms in many object identification systems, which results in poor and ineffective performance, is a significant obstacle. In this research, we solve the end-to-end object detection problem entirely using deep learning techniques. The network is trained using the most difficult publicly available dataset, which is used for an annual item detection challenge. Applications that need object detection can benefit the system's quick and precise finding.</article>","contentLength":898,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Powered Preference Elicitation in Combinatorial Assignment","url":"https://arxiv.org/abs/2502.10308","date":1739768400,"author":"","guid":1278,"unread":true,"content":"<article>arXiv:2502.10308v1 Announce Type: new \nAbstract: We study the potential of large language models (LLMs) as proxies for humans to simplify preference elicitation (PE) in combinatorial assignment. While traditional PE methods rely on iterative queries to capture preferences, LLMs offer a one-shot alternative with reduced human effort. We propose a framework for LLM proxies that can work in tandem with SOTA ML-powered preference elicitation schemes. Our framework handles the novel challenges introduced by LLMs, such as response variability and increased computational costs. We experimentally evaluate the efficiency of LLM proxies against human queries in the well-studied course allocation domain, and we investigate the model capabilities required for success. We find that our approach improves allocative efficiency by up to 20%, and these results are robust across different LLMs and to differences in quality and accuracy of reporting.</article>","contentLength":945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer learning using Foundation Models","url":"https://arxiv.org/abs/2502.10307","date":1739768400,"author":"","guid":1279,"unread":true,"content":"<article>arXiv:2502.10307v1 Announce Type: new \nAbstract: Traditional solar forecasting models are based on several years of site-specific historical irradiance data, often spanning five or more years, which are unavailable for newer photovoltaic farms. As renewable energy is highly intermittent, building accurate solar irradiance forecasting systems is essential for efficient grid management and enabling the ongoing proliferation of solar energy, which is crucial to achieve the United Nations' net zero goals. In this work, we propose SPIRIT, a novel approach leveraging foundation models for solar irradiance forecasting, making it applicable to newer solar installations. Our approach outperforms state-of-the-art models in zero-shot transfer learning by about 70%, enabling effective performance at new locations without relying on any historical data. Further improvements in performance are achieved through fine-tuning, as more location-specific data becomes available. These findings are supported by statistical significance, further validating our approach. SPIRIT represents a pivotal step towards rapid, scalable, and adaptable solar forecasting solutions, advancing the integration of renewable energy into global power systems.</article>","contentLength":1237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When 1+1 does not equal 2: Synergy in games","url":"https://arxiv.org/abs/2502.10304","date":1739768400,"author":"","guid":1280,"unread":true,"content":"<article>arXiv:2502.10304v1 Announce Type: new \nAbstract: Although synergy is an important concept that is strongly ingrained in games, it has not been widely discussed by the games community. This is due to the vagueness of the concept and the fact that there is no clear agreement on what it means. To solve this, we present a strict definition of what is synergy. Then we propose a methodology to use this definition to analyze synergy in games. Applying this definition to various games (Chess, League of Legends, and Magic: The Gathering), we illustrate how it can be used to solve many of the practical problems related to synergy.</article>","contentLength":628,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations","url":"https://arxiv.org/abs/2502.10303","date":1739768400,"author":"","guid":1281,"unread":true,"content":"<article>arXiv:2502.10303v1 Announce Type: new \nAbstract: Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and deep Q-network approaches, to create advanced AI models such as AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning and reinforcement learning to master the game of Go, surpassing professional human players. AlphaGo Zero refines this approach by eliminating reliance on human gameplay data, instead utilizing self-play for enhanced learning efficiency. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across various games, including complex Atari games. This paper reviews the significance of reinforcement learning applications in Atari and strategy-based games, analyzing these three models, their key innovations, training processes, challenges encountered, and improvements made. Additionally, we discuss advancements in the field of gaming, including MiniZero and multi-agent models, highlighting future directions and emerging AI models from Google DeepMind.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Open-Source AI-Powered Optimization in Scalene: Advancing Python Performance Profiling with DeepSeek-R1 and LLaMA 3.2","url":"https://arxiv.org/abs/2502.10299","date":1739768400,"author":"","guid":1282,"unread":true,"content":"<article>arXiv:2502.10299v1 Announce Type: new \nAbstract: Python's flexibility and ease of use come at the cost of performance inefficiencies, requiring developers to rely on profilers to optimize execution. SCALENE, a high-performance CPU, GPU, and memory profiler, provides fine-grained insights into Python applications while running significantly faster than traditional profilers. Originally, SCALENE integrated OpenAI's API to generate AI-powered optimization suggestions, but its reliance on a proprietary API limited accessibility. This study explores the feasibility of using opensource large language models (LLMs), such as DeepSeek-R1 and Llama 3.2, to generate optimization recommendations within SCALENE. Our evaluation reveals that DeepSeek-R1 provides effective code optimizations comparable to proprietary models. We integrate DeepSeek-R1 into SCALENE to automatically analyze performance bottlenecks and suggest improvements, enhancing SCALENE's utility while maintaining its open-source nature. This study demonstrates that open-source LLMs can be viable alternatives for AI-driven code optimization, paving the way for more accessible and cost-effective performance analysis tools.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders","url":"https://arxiv.org/abs/2502.10297","date":1739768400,"author":"","guid":1283,"unread":true,"content":"<article>arXiv:2502.10297v1 Announce Type: new \nAbstract: Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. While diagonal matrices used in architectures like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited expressivity. To address this, recent architectures such as (Gated) DeltaNet and RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous token-channel mixing, which overcomes some expressivity limitations with only a slight decrease in training efficiency. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency and a stable recurrence. Through extensive experiments, we demonstrate that DeltaProduct achieves superior state-tracking and language modeling capabilities while exhibiting significantly improved length extrapolation compared to DeltaNet. Additionally, we also strengthen the theoretical foundation of DeltaNet's expressivity by proving that it can solve dihedral group word problems in just two layers.</article>","contentLength":1635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fenchel-Young Variational Learning","url":"https://arxiv.org/abs/2502.10295","date":1739768400,"author":"","guid":1284,"unread":true,"content":"<article>arXiv:2502.10295v1 Announce Type: new \nAbstract: From a variational perspective, many statistical learning criteria involve seeking a distribution that balances empirical risk and regularization. In this paper, we broaden this perspective by introducing a new general class of variational methods based on Fenchel-Young (FY) losses, treated as divergences that generalize (and encompass) the familiar Kullback-Leibler divergence at the core of classical variational learning. Our proposed formulation -- FY variational learning -- includes as key ingredients new notions of FY free energy, FY evidence, FY evidence lower bound, and FY posterior. We derive alternating minimization and gradient backpropagation algorithms to compute (or lower bound) the FY evidence, which enables learning a wider class of models than previous variational formulations. This leads to generalized FY variants of classical algorithms, such as an FY expectation-maximization (FYEM) algorithm, and latent-variable models, such as an FY variational autoencoder (FYVAE). Our new methods are shown to be empirically competitive, often outperforming their classical counterparts, and most importantly, to have qualitatively novel features. For example, FYEM has an adaptively sparse E-step, while the FYVAE can support models with sparse observations and sparse posteriors.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for Scribble-Supervised Segmentation of Medical Images","url":"https://arxiv.org/abs/2502.10294","date":1739768400,"author":"","guid":1285,"unread":true,"content":"<article>arXiv:2502.10294v1 Announce Type: new \nAbstract: The deployment of advanced deep learning models for medical image segmentation is often constrained by the requirement for extensively annotated datasets. Weakly-supervised learning, which allows less precise labels, has become a promising solution to this challenge. Building on this approach, we propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image segmentation. This framework is built on the U-Net architecture, with the encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks. These blocks enhance the model's ability to learn local and global features efficiently. Additionally, our approach integrates a query-based Transformer decoder to refine features and an edge enhancement module to compensate for the limited boundary information in the scribble label. We evaluate the proposed QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation metrics include the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+ achieves 89.1\\% DSC and 1.316mm HD95 on ACDC, 88.4\\% DSC and 2.226mm HD95 on MS-CMRSeg, 71.4\\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\\% DSC and 50.122mm HD95 on BUSI. These results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency while remaining competitive with fully-supervised learning approaches. This makes it ideal for medical image analysis, where high-quality annotations are often scarce and require significant effort and expense. The code is available at: https://github.com/anpc849/QMaxViT-Unet</article>","contentLength":1765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Roadmap to Address Burnout in the Cybersecurity Profession: Outcomes from a Multifaceted Workshop","url":"https://arxiv.org/abs/2502.10293","date":1739768400,"author":"","guid":1286,"unread":true,"content":"<article>arXiv:2502.10293v1 Announce Type: new \nAbstract: This paper addresses the critical issue of burnout among cybersecurity professionals, a growing concern that threatens the effectiveness of digital defense systems. As the industry faces a significant attrition crisis, with nearly 46% of cybersecurity leaders contemplating departure from their roles, it is imperative to explore the causes and consequences of burnout through a socio-technical lens. These challenges were discussed by experts from academia and industry in a multi-disciplinary workshop at the 26th International Conference on Human-Computer Interaction to address broad antecedents of burnout, manifestation and its consequences among cybersecurity professionals, as well as programs to mitigate impacts from burnout. Central to the analysis is an empirical study of former National Security Agency (NSA) tactical cyber operators. This paper presents key insights in the following areas based on discussions in the workshop: lessons for public and private sectors from the NSA study, a comparative review of addressing burnout in the healthcare profession. It also outlines a roadmap for future collaborative research, thereby informing interdisciplinary studies.</article>","contentLength":1230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Loss Bounds for Online Learning Separated Function Classes: A Gaussian Process Perspective","url":"https://arxiv.org/abs/2502.10292","date":1739768400,"author":"","guid":1287,"unread":true,"content":"<article>arXiv:2502.10292v1 Announce Type: new \nAbstract: In order to develop practical and efficient algorithms while circumventing overly pessimistic computational lower bounds, recent work has been interested in developing oracle-efficient algorithms in a variety of learning settings. Two such settings of particular interest are online and differentially private learning. While seemingly different, these two fields are fundamentally connected by the requirement that successful algorithms in each case satisfy stability guarantees; in particular, recent work has demonstrated that algorithms for online learning whose performance adapts to beneficial problem instances, attaining the so-called small-loss bounds, require a form of stability similar to that of differential privacy. In this work, we identify the crucial role that separation plays in allowing oracle-efficient algorithms to achieve this strong stability. Our notion, which we term $\\rho$-separation, generalizes and unifies several previous approaches to enforcing this strong stability, including the existence of small-separator sets and the recent notion of $\\gamma$-approximability. We present an oracle-efficient algorithm that is capable of achieving small-loss bounds with improved rates in greater generality than previous work, as well as a variant for differentially private learning that attains optimal rates, again under our separation condition. In so doing, we prove a new stability result for minimizers of a Gaussian process that strengthens and generalizes previous work.</article>","contentLength":1553,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Immersive virtual games: winners for deep cognitive assessment","url":"https://arxiv.org/abs/2502.10290","date":1739768400,"author":"","guid":1288,"unread":true,"content":"<article>arXiv:2502.10290v1 Announce Type: new \nAbstract: Studies of human cognition often rely on brief, highly controlled tasks that emphasize group-level effects but poorly capture the rich variability within and between individuals. Here, we present PixelDOPA, a suite of minigames designed to overcome these limitations by embedding classic cognitive task paradigms in an immersive 3D virtual environment with continuous behavior logging. Four minigames explore overlapping constructs such as processing speed, rule shifting, inhibitory control and working memory, comparing against established NIH Toolbox tasks. Across a clinical sample of 60 participants collected outside a controlled laboratory setting, we found significant, large correlations (r = 0.50-0.93) between the PixelDOPA tasks and NIH Toolbox counterparts, despite differences in stimuli and task structures. Process-informed metrics (e.g., gaze-based response times derived from continuous logging) substantially improved both task convergence and data quality. Test-retest analyses revealed high reliability (ICC = 0.50-0.92) for all minigames. Beyond endpoint metrics, movement and gaze trajectories revealed stable, idiosyncratic profiles of gameplay strategy, with unsupervised clustering distinguishing subjects by their navigational and viewing behaviors. These trajectory-based features showed lower within-person variability than between-person variability, facilitating player identification across repeated sessions. Game-based tasks can therefore retain the psychometric rigor of standard cognitive assessments while providing new insights into dynamic behaviors. By leveraging a highly engaging, fully customizable game engine, we show that comprehensive behavioral tracking boosts the power to detect individual differences--offering a path toward cognitive measures that are both robust and ecologically valid, even in less-than-ideal settings for data collection.</article>","contentLength":1942,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigation of the Estimation Accuracy of 5 Different Numerical ODE Solvers on 3 Case Studies","url":"https://arxiv.org/abs/2502.10289","date":1739768400,"author":"","guid":1289,"unread":true,"content":"<article>arXiv:2502.10289v1 Announce Type: new \nAbstract: Numerical ordinary differential equation (ODE) solvers are indispensable tools in various engineering domains, enabling the simulation and analysis of dynamic systems. In this work, we utilize 5 different numerical ODE solvers namely: Euler's method, Heun's method, Midpoint Method, Runge-kutta 4th order and ODE45 method in order to discover the answer of three wellknown case studies and compare their results by calculation of relative errors. To check for the validity of the estimations, the experimental data of previous literature have been compared with the data in this paper which shows a good accordance. We observe that for each of the case studies based on the behavior of the model, the estimation accuracy of the solvers is different. For the logistic population change as the first case study, the results of all solvers are so close to each other that only their solution cost can be considered for their superiority. For temperature change of a building as the second case study we see that in some especial areas the accuracy of the solvers is different and in general Midpoint ODE solver shows better results. As the last case study, market equilibrium price shows that none of the numerical ODE solvers can estimate its behavior which is due to its sudden changing nature.</article>","contentLength":1342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adversarial Mixup Unlearning","url":"https://arxiv.org/abs/2502.10288","date":1739768400,"author":"","guid":1290,"unread":true,"content":"<article>arXiv:2502.10288v1 Announce Type: new \nAbstract: Machine unlearning is a critical area of research aimed at safeguarding data privacy by enabling the removal of sensitive information from machine learning models. One unique challenge in this field is catastrophic unlearning, where erasing specific data from a well-trained model unintentionally removes essential knowledge, causing the model to deviate significantly from a retrained one. To address this, we introduce a novel approach that regularizes the unlearning process by utilizing synthesized mixup samples, which simulate the data susceptible to catastrophic effects. At the core of our approach is a generator-unlearner framework, MixUnlearn, where a generator adversarially produces challenging mixup examples, and the unlearner effectively forgets target information based on these synthesized data. Specifically, we first introduce a novel contrastive objective to train the generator in an adversarial direction: generating examples that prompt the unlearner to reveal information that should be forgotten, while losing essential knowledge. Then the unlearner, guided by two other contrastive loss terms, processes the synthesized and real data jointly to ensure accurate unlearning without losing critical knowledge, overcoming catastrophic effects. Extensive evaluations across benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches, offering a robust solution to machine unlearning. This work not only deepens understanding of unlearning mechanisms but also lays the foundation for effective machine unlearning with mixup augmentation.</article>","contentLength":1649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hybrid Cross-Stage Coordination Pre-ranking Model for Online Recommendation Systems","url":"https://arxiv.org/abs/2502.10284","date":1739768400,"author":"","guid":1291,"unread":true,"content":"<article>arXiv:2502.10284v1 Announce Type: new \nAbstract: Large-scale recommendation systems often adopt cascading architecture consisting of retrieval, pre-ranking, ranking, and re-ranking stages. With strict latency requirements, pre-ranking utilizes lightweight models to perform a preliminary selection from massive retrieved candidates. However, recent works focus solely on improving consistency with ranking, relying exclusively on downstream stages. Since downstream input is derived from the pre-ranking output, they will exacerbate the sample selection bias (SSB) issue and Matthew effect, leading to sub-optimal results. To address the limitation, we propose a novel Hybrid Cross-Stage Coordination Pre-ranking model (HCCP) to integrate information from upstream (retrieval) and downstream (ranking, re-ranking) stages. Specifically, cross-stage coordination refers to the pre-ranking's adaptability to the entire stream and the role of serving as a more effective bridge between upstream and downstream. HCCP consists of Hybrid Sample Construction and Hybrid Objective Optimization. Hybrid sample construction captures multi-level unexposed data from the entire stream and rearranges them to become the optimal guiding \"ground truth\" for pre-ranking learning. Hybrid objective optimization contains the joint optimization of consistency and long-tail precision through our proposed Margin InfoNCE loss. It is specifically designed to learn from such hybrid unexposed samples, improving the overall performance and mitigating the SSB issue. The appendix describes a proof of the efficacy of the proposed loss in selecting potential positives. Extensive offline and online experiments indicate that HCCP outperforms SOTA methods by improving cross-stage coordination. It contributes up to 14.9% UCVR and 1.3% UCTR in the JD E-commerce recommendation system. Concerning code privacy, we provide a pseudocode for reference.</article>","contentLength":1922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anomaly Detection with LWE Encrypted Control","url":"https://arxiv.org/abs/2502.10283","date":1739768400,"author":"","guid":1292,"unread":true,"content":"<article>arXiv:2502.10283v1 Announce Type: new \nAbstract: Detecting attacks using encrypted signals is challenging since encryption hides its information content. We present a novel mechanism for anomaly detection over Learning with Errors (LWE) encrypted signals without using decryption, secure channels, nor complex communication schemes. Instead, the detector exploits the homomorphic property of LWE encryption to perform hypothesis tests on transformations of the encrypted samples. The specific transformations are determined by solutions to a hard lattice-based minimization problem. While the test's sensitivity deteriorates with suboptimal solutions, similar to the exponential deterioration of the (related) test that breaks the cryptosystem, we show that the deterioration is polynomial for our test. This rate gap can be exploited to pick parameters that lead to somewhat weaker encryption but large gains in detection capability. Finally, we conclude the paper by presenting a numerical example that simulates anomaly detection, demonstrating the effectiveness of our method in identifying attacks.</article>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TrustZero - open, verifiable and scalable zero-trust","url":"https://arxiv.org/abs/2502.10281","date":1739768400,"author":"","guid":1293,"unread":true,"content":"<article>arXiv:2502.10281v1 Announce Type: new \nAbstract: We present a passport-level trust token for Europe. In an era of escalating cyber threats fueled by global competition in economic, military, and technological domains, traditional security models are proving inadequate. The rise of advanced attacks exploiting zero-day vulnerabilities, supply chain infiltration, and system interdependencies underscores the need for a paradigm shift in cybersecurity. Zero Trust Architecture (ZTA) emerges as a transformative framework that replaces implicit trust with continuous verification of identity and granular access control. This thesis introduces TrustZero, a scalable layer of zero-trust security built around a universal \"trust token\" - a non-revocable self-sovereign identity with cryptographic signatures to enable robust, mathematically grounded trust attestations. By integrating ZTA principles with cryptography, TrustZero establishes a secure web-of-trust framework adaptable to legacy systems and inter-organisational communication.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probabilistic Super-Resolution for High-Fidelity Physical System Simulations with Uncertainty Quantification","url":"https://arxiv.org/abs/2502.10280","date":1739768400,"author":"","guid":1294,"unread":true,"content":"<article>arXiv:2502.10280v1 Announce Type: new \nAbstract: Super-resolution (SR) is a promising tool for generating high-fidelity simulations of physical systems from low-resolution data, enabling fast and accurate predictions in engineering applications. However, existing deep-learning based SR methods, require large labeled datasets and lack reliable uncertainty quantification (UQ), limiting their applicability in real-world scenarios. To overcome these challenges, we propose a probabilistic SR framework that leverages the Statistical Finite Element Method and energy-based generative modeling. Our method enables efficient high-resolution predictions with inherent UQ, while eliminating the need for extensive labeled datasets. The method is validated on a 2D Poisson example and compared with bicubic interpolation upscaling. Results demonstrate a computational speed-up over high-resolution numerical solvers while providing reliable uncertainty estimates.</article>","contentLength":957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Emit As You Go: Enumerating Edges of a Spanning Tree","url":"https://arxiv.org/abs/2502.10279","date":1739768400,"author":"","guid":1295,"unread":true,"content":"<article>arXiv:2502.10279v1 Announce Type: new \nAbstract: Classically, planning tasks are studied as a two-step process: plan creation and plan execution. In situations where plan creation is slow (for example, due to expensive information access or complex constraints), a natural speed-up tactic is interleaving planning and execution. We implement such an approach with an enumeration algorithm that, after little preprocessing time, outputs parts of a plan one by one with little delay in-between consecutive outputs. As concrete planning task, we consider efficient connectivity in a network formalized as the minimum spanning tree problem in all four standard variants: (un)weighted (un)directed graphs. Solution parts to be emitted one by one for this concrete task are the individual edges that form the final tree.\n  We show with algorithmic upper bounds and matching unconditional adversary lower bounds that efficient enumeration is possible for three of four problem variants; specifically for undirected unweighted graphs (delay in the order of the average degree), as well as graphs with either weights (delay in the order of the maximum degree and the average runtime per emitted edge of a total-time algorithm) or directions (delay in the order of the maximum degree). For graphs with both weighted and directed edges, we show that no meaningful enumeration is possible.\n  Finally, with experiments on random undirected unweighted graphs, we show that the theoretical advantage of little preprocessing and delay carries over to practice.</article>","contentLength":1544,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Intelligence to Assess Dental Findings from Panoramic Radiographs -- A Multinational Study","url":"https://arxiv.org/abs/2502.10277","date":1739768400,"author":"","guid":1296,"unread":true,"content":"<article>arXiv:2502.10277v1 Announce Type: new \nAbstract: Dental panoramic radiographs (DPRs) are widely used in clinical practice for comprehensive oral assessment but present challenges due to overlapping structures and time constraints in interpretation.\n  This study aimed to establish a solid baseline for the AI-automated assessment of findings in DPRs by developing, evaluating an AI system, and comparing its performance with that of human readers across multinational data sets.\n  We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and Taiwan), focusing on 8 types of dental findings. The AI system combined object detection and semantic segmentation techniques for per-tooth finding identification. Performance metrics included sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). AI generalizability was tested across data sets, and performance was compared with human dental practitioners.\n  The AI system demonstrated comparable or superior performance to human readers, particularly +67.9% (95% CI: 54.0%-81.9%; p &lt; .001) sensitivity for identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008) sensitivity for identifying missing teeth. The AI achieved a macro-averaged AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with the reference were comparable to inter-human agreements in 7 of 8 findings except for caries (p = .024). The AI system demonstrated robust generalization across diverse imaging and demographic settings and processed images 79 times faster (95% CI: 75-82) than human readers.\n  The AI system effectively assessed findings in DPRs, achieving performance on par with or better than human experts while significantly reducing interpretation time. These results highlight the potential for integrating AI into clinical workflows to improve diagnostic efficiency and accuracy, and patient management.</article>","contentLength":1930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probing Perceptual Constancy in Large Vision Language Models","url":"https://arxiv.org/abs/2502.10273","date":1739768400,"author":"","guid":1297,"unread":true,"content":"<article>arXiv:2502.10273v1 Announce Type: new \nAbstract: Perceptual constancy is the ability to maintain stable perceptions of objects despite changes in sensory input, such as variations in distance, angle, or lighting. This ability is crucial for recognizing visual information in a dynamic world, making it essential for Vision-Language Models (VLMs). However, whether VLMs are currently and theoretically capable of mastering this ability remains underexplored. In this study, we evaluated 33 VLMs using 253 experiments across three domains: color, size, and shape constancy. The experiments included single-image and video adaptations of classic cognitive tasks, along with novel tasks in in-the-wild conditions, to evaluate the models' recognition of object properties under varying conditions. We found significant variability in VLM performance, with models performance in shape constancy clearly dissociated from that of color and size constancy.</article>","contentLength":947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimized Strategies for Peak Shaving and BESS Efficiency Enhancement through Cycle-Based Control and Cluster-Level Power Allocation","url":"https://arxiv.org/abs/2502.10268","date":1739768400,"author":"","guid":1298,"unread":true,"content":"<article>arXiv:2502.10268v1 Announce Type: new \nAbstract: Battery Energy Storage Systems (BESS) are essential for peak shaving, balancing power supply and demand while enhancing grid efficiency. This study proposes a cycle-based control strategy for charging and discharging, which optimizes capture rate (CR), release rate (RR), and capacity utilization rate (CUR), improving BESS performance. Compared to traditional day-ahead methods, the cycle-based approach enhances operational accuracy and reduces capacity waste, achieving a CUR increase from 75.1% to 79.9%. An innovative cluster-level power allocation method, leveraging an improved Particle Swarm Optimization (PSO) algorithm, is introduced. This strategy reduces daily energy loss by 174.21 kWh (3.7%) and increases BESS efficiency by 0.4%. Transient and steady-state energy loss components are analyzed, revealing that transient loss proportion decreases significantly as power depth increases, from 27.2% at 1 MW to 1.3% at 10 MW. Simulations based on a detailed Simulink/Simscape model validate these methods, demonstrating enhanced peak shaving effectiveness and prolonged BESS lifespan by reducing equivalent cycles. The study provides a robust framework for optimizing BESS performance and efficiency in real-world applications.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Large Language Models the future crowd workers of Linguistics?","url":"https://arxiv.org/abs/2502.10266","date":1739768400,"author":"","guid":1299,"unread":true,"content":"<article>arXiv:2502.10266v1 Announce Type: new \nAbstract: Data elicitation from human participants is one of the core data collection strategies used in empirical linguistic research. The amount of participants in such studies may vary considerably, ranging from a handful to crowdsourcing dimensions. Even if they provide resourceful extensive data, both of these settings come alongside many disadvantages, such as low control of participants' attention during task completion, precarious working conditions in crowdsourcing environments, and time-consuming experimental designs. For these reasons, this research aims to answer the question of whether Large Language Models (LLMs) may overcome those obstacles if included in empirical linguistic pipelines. Two reproduction case studies are conducted to gain clarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced elicitation tasks, originally designed for human participants, are reproduced in the proposed framework with the help of OpenAI's GPT-4o-mini model. Its performance with our zero-shot prompting baseline shows the effectiveness and high versatility of LLMs, that tend to outperform human informants in linguistic tasks. The findings of the second replication further highlight the need to explore additional prompting techniques, such as Chain-of-Thought (CoT) prompting, which, in a second follow-up experiment, demonstrates higher alignment to human performance on both critical and filler items. Given the limited scale of this study, it is worthwhile to further explore the performance of LLMs in empirical Linguistics and in other future applications in the humanities.</article>","contentLength":1655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers","url":"https://arxiv.org/abs/2502.10263","date":1739768400,"author":"","guid":1300,"unread":true,"content":"<article>arXiv:2502.10263v1 Announce Type: new \nAbstract: Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.</article>","contentLength":1546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MITO: Enabling Non-Line-of-Sight Perception using Millimeter-waves through Real-World Datasets and Simulation Tools","url":"https://arxiv.org/abs/2502.10259","date":1739768400,"author":"","guid":1301,"unread":true,"content":"<article>arXiv:2502.10259v1 Announce Type: new \nAbstract: We present MITO, the first dataset of multi-spectral millimeter-wave (mmWave) images of everyday objects. Unlike visible light, mmWave signals can image through everyday occlusions (e.g., cardboard boxes, fabric, plastic). However, due to the dearth of publicly-available mmWave images and the interdisciplinary challenges in collecting and processing mmWave signals, it remains difficult today for computer vision researchers to develop mmWave-based non-line-of-sight perception algorithms and models.\n  To overcome these challenges, we introduce a real-world dataset and open-source simulation tool for mmWave imaging. The dataset is acquired using a UR5 robotic arm with two mmWave radars operating at different frequencies and an RGB-D camera. Through a signal processing pipeline, we capture and create over 580 real-world 3D mmWave images from over 76 different objects in the YCB dataset, a standard dataset for robotics manipulation. We provide real-world mmWave images in line-of-sight and non-line-of-sight, as well as RGB-D images and ground truth segmentation masks. We also develop an open-source simulation tool that can be used to generate synthetic mmWave images for any 3D triangle mesh, which achieves a median F-Score of 94% when compared to real-world mmWave images.\n  We show the usefulness of this dataset and simulation tool in multiple CV tasks in non-line-of-sight. First, we perform object segmentation for mmWave images using the segment anything model (SAM), and achieve a median precision and recall of 92.6% and 64%. Second, we train a classifier that can recognize objects in non-line-of-sight. It is trained on synthetic images and can classify real-world images with 85% accuracy.\n  We believe MITO will be a valuable resource for computer vision researchers in developing non-line-of-sight perception, similar to how early camera-based datasets shaped the field.</article>","contentLength":1945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PromptArtisan: Multi-instruction Image Editing in Single Pass with Complete Attention Control","url":"https://arxiv.org/abs/2502.10258","date":1739768400,"author":"","guid":1302,"unread":true,"content":"<article>arXiv:2502.10258v1 Announce Type: new \nAbstract: We present PromptArtisan, a groundbreaking approach to multi-instruction image editing that achieves remarkable results in a single pass, eliminating the need for time-consuming iterative refinement. Our method empowers users to provide multiple editing instructions, each associated with a specific mask within the image. This flexibility allows for complex edits involving mask intersections or overlaps, enabling the realization of intricate and nuanced image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix model in conjunction with a novel Complete Attention Control Mechanism (CACM). This mechanism ensures precise adherence to user instructions, granting fine-grained control over the editing process. Furthermore, our approach is zero-shot, requiring no additional training, and boasts improved processing complexity compared to traditional iterative methods. By seamlessly integrating multi-instruction capabilities, single-pass efficiency, and complete attention control, PromptArtisan unlocks new possibilities for creative and efficient image editing workflows, catering to both novice and expert users alike.</article>","contentLength":1193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seamless acceleration of Fortran intrinsics via AMD AI engines","url":"https://arxiv.org/abs/2502.10254","date":1739768400,"author":"","guid":1303,"unread":true,"content":"<article>arXiv:2502.10254v1 Announce Type: new \nAbstract: A major challenge that the HPC community faces is how to continue delivering the performance demanded by scientific programmers, whilst meeting an increased emphasis on sustainable operations. Specialised architectures, such as FPGAs and AMD's AI Engines (AIEs), have been demonstrated to provide significant energy efficiency advantages, however a major challenge is that to most effectively program these architectures requires significant expertise and investment of time which is a major blocker.\n  Fortran in the lingua franca of scientific computing, and in this paper we explore automatically accelerating Fortran intrinsics via the AIEs in AMD's Ryzen AI CPU. Leveraging the open source Flang compiler and MLIR ecosystem, we describe an approach that lowers the MLIR linear algebra dialect to AMD's AIE dialects, and demonstrate that for suitable workloads the AIEs can provide significant performance advantages over the CPU without any code modifications required by the programmer.</article>","contentLength":1041,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models","url":"https://arxiv.org/abs/2502.10250","date":1739768400,"author":"","guid":1304,"unread":true,"content":"<article>arXiv:2502.10250v1 Announce Type: new \nAbstract: Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved image-text web documents. Our approach transforms 45K web documents from the OBELICS dataset into 100K image conversation samples. We utilize GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert these captions into diverse free-form and multiple-choice question-answer pairs. Integrating this dataset for fine-tuning considerably enhances VLM performance across multiple benchmarks. Unlike methods that focus solely on fine-grained visual content, our approach leverages accompanying web context, yielding superior results. We also discover that a `leaky modality mix,' where conversation samples contain questions answerable from both the image and its contextual caption, outperforms non-leaky combinations of captions and Q\\&amp;A pairs. VisCon-100k dataset shows strong performance with two popular VLM approaches: text-only large language model (LLM) aligned with a vision encoder using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM (IDEFICS2-8b) using interleaved image-text data. In addition to releasing the VisCon-100K dataset, we provide a contextual captioner trained on this dataset, facilitating scalable fine-tuning data generation for future research and open-source applications. Using the same pipeline, but substituting our trained contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.</article>","contentLength":1659,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding the relationships between the perceptions of burnout and instability in Software Engineering","url":"https://arxiv.org/abs/2502.10249","date":1739768400,"author":"","guid":1305,"unread":true,"content":"<article>arXiv:2502.10249v1 Announce Type: new \nAbstract: Changes are inherent in software development, often increasing developers' perception of instability. Understanding the relationship between human factors and Software Engineering processes is crucial to mitigating and preventing issues. One such factor is burnout, a recognized disease that impacts productivity, turnover, and, most importantly, developers' well-being. Investigating the link between instability and burnout can help organizations implement strategies to improve developers' work conditions and performance.\n  This study aims to identify and describe the relationship between perceived instability and burnout among software developers. A cross-sectional survey was conducted with 411 respondents, using convenience sampling and self-selection. In addition to analyzing variable relationships, confirmatory factor analysis was applied.\n  Key findings include: (1) A significant positive relationship between burnout (exhaustion and cynicism) and team, technological, and task instability; (2) A weak negative relationship between efficacy and technological/team instability, with no correlation to task instability; (3) Exhaustion was the most frequently reported burnout symptom, while task instability was the most perceived type of instability.\n  These results are valuable for both industry and academia, providing insights to reduce burnout and instability among software engineers. Future research can further explore the impact of instability, offering new perspectives on monitoring and mitigating its effects in software development.</article>","contentLength":1609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model","url":"https://arxiv.org/abs/2502.10248","date":1739768400,"author":"","guid":1306,"unread":true,"content":"<article>arXiv:2502.10248v1 Announce Type: new \nAbstract: We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.</article>","contentLength":1494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safety Blind Spot in Remote Driving: Considerations for Risk Assessment of Connection Loss Fallback Strategies","url":"https://arxiv.org/abs/2502.10243","date":1739768400,"author":"","guid":1307,"unread":true,"content":"<article>arXiv:2502.10243v1 Announce Type: new \nAbstract: As part of the overall goal of driverless road vehicles, remote driving is a major emerging field of research of its own. Current remote driving concepts for public road traffic often establish a fallback strategy of immediate braking to a standstill in the event of a connection loss. This may seem like the most logical option when human control of the vehicle is lost. However, our simulation results from hundreds of scenarios based on naturalistic traffic scenes indicate high collision rates for any immediate substantial deceleration to a standstill in urban settings. We show that such a fallback strategy can result in a SOTIF relevant hazard, making it questionable whether such a design decision can be considered acceptable. Therefore, from a safety perspective, we would call this problem a safety blind spot, as safety analyses in this regard seem to be very rare.\n  In this article, we first present a simulation on a naturalistic dataset that shows a high probability of collision in the described case. Second, we discuss the severity of the resulting potential rear-end collisions and provide an even more severe example by including a large commercial vehicle in the potential collision.</article>","contentLength":1255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices","url":"https://arxiv.org/abs/2502.10239","date":1739768400,"author":"","guid":1308,"unread":true,"content":"<article>arXiv:2502.10239v1 Announce Type: new \nAbstract: Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose Federated Split-Perturbation Zero-order Optimization (FedSPZO) that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.</article>","contentLength":894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control","url":"https://arxiv.org/abs/2502.10236","date":1739768400,"author":"","guid":1309,"unread":true,"content":"<article>arXiv:2502.10236v1 Announce Type: new \nAbstract: Diffusion Probabilistic Models (DPMs) are powerful generative models that have achieved unparalleled success in a number of generative tasks. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. For topologically structured data, we devise a frequency-based noising operator to purposefully manipulate, and set, these inductive biases. We first show that appropriate manipulations of the noising forward process can lead DPMs to focus on particular aspects of the distribution to learn. We show that different datasets necessitate different inductive biases, and that appropriate frequency-based noise control induces increased generative performance compared to standard diffusion. Finally, we demonstrate the possibility of ignoring information at particular frequencies while learning. We show this in an image corruption and recovery task, where we train a DPM to recover the original target distribution after severe noise corruption.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via Hierarchical and Parallel Decoding","url":"https://arxiv.org/abs/2502.10233","date":1739768400,"author":"","guid":1310,"unread":true,"content":"<article>arXiv:2502.10233v1 Announce Type: new \nAbstract: The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge in warehouse logistics, where pickers must navigate a mixed-shelves environment to retrieve SKUs efficiently. Traditional heuristics and optimization-based approaches struggle with scalability, while recent machine learning methods often rely on sequential decision-making, leading to high solution latency and suboptimal agent coordination. In this work, we propose a novel hierarchical and parallel decoding approach for solving the min-max variant of the MSPRP via multi-agent reinforcement learning. While our approach generates a joint distribution over agent actions, allowing for fast decoding and effective picker coordination, our method introduces a sequential action selection to avoid conflicts in the multi-dimensional action space. Experiments show state-of-the-art performance in both solution quality and inference speed, particularly for large-scale and out-of-distribution instances. Our code is publicly available at http://github.com/LTluttmann/marl4msprp.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProReco: A Process Discovery Recommender System","url":"https://arxiv.org/abs/2502.10230","date":1739768400,"author":"","guid":1311,"unread":true,"content":"<article>arXiv:2502.10230v1 Announce Type: new \nAbstract: Process discovery aims to automatically derive process models from historical execution data (event logs). While various process discovery algorithms have been proposed in the last 25 years, there is no consensus on a dominating discovery algorithm. Selecting the most suitable discovery algorithm remains a challenge due to competing quality measures and diverse user requirements. Manually selecting the most suitable process discovery algorithm from a range of options for a given event log is a time-consuming and error-prone task. This paper introduces ProReco, a Process discovery Recommender system designed to recommend the most appropriate algorithm based on user preferences and event log characteristics. ProReco incorporates state-of-the-art discovery algorithms, extends the feature pools from previous work, and utilizes eXplainable AI (XAI) techniques to provide explanations for its recommendations.</article>","contentLength":964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Multiagent Path Search Algorithm for Large-Scale Coalition Structure Generation","url":"https://arxiv.org/abs/2502.10226","date":1739768400,"author":"","guid":1312,"unread":true,"content":"<article>arXiv:2502.10226v1 Announce Type: new \nAbstract: Coalition structure generation (CSG), i.e. the problem of optimally partitioning a set of agents into coalitions to maximize social welfare, is a fundamental computational problem in multiagent systems. This problem is important for many applications where small run times are necessary, including transportation and disaster response. In this paper, we develop SALDAE, a multiagent path finding algorithm for CSG that operates on a graph of coalition structures. Our algorithm utilizes a variety of heuristics and strategies to perform the search and guide it. It is an anytime algorithm that can handle large problems with hundreds and thousands of agents. We show empirically on nine standard value distributions, including disaster response and electric vehicle allocation benchmarks, that our algorithm enables a rapid finding of high-quality solutions and compares favorably with other state-of-the-art methods.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comparison of Deep Recurrent Neural Networks and Bayesian Neural Networks for Detecting Electric Motor Damage Through Sound Signal Analysis","url":"https://arxiv.org/abs/2502.10224","date":1739768400,"author":"","guid":1313,"unread":true,"content":"<article>arXiv:2502.10224v1 Announce Type: new \nAbstract: Fault detection in electric motors is a critical challenge in various industries, where failures can result in significant operational disruptions. This study investigates the use of Recurrent Neural Networks (RNNs) and Bayesian Neural Networks (BNNs) for diagnosing motor damage using acoustic signal analysis. A novel approach is proposed, leveraging frequency domain representation of sound signals for enhanced diagnostic accuracy. The architectures of both RNNs and BNNs are designed and evaluated on real-world acoustic data collected from household appliances using smartphones. Experimental results demonstrate that BNNs provide superior fault detection performance, particularly for imbalanced datasets, offering more robust and interpretable predictions compared to traditional methods. The findings suggest that BNNs, with their ability to incorporate uncertainty, are well-suited for industrial diagnostic applications. Further analysis and benchmarks are suggested to explore resource efficiency and classification capabilities of these architectures.</article>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal and Coordinated Voltage Control: Case Study on a 132 kV Norwegian Grid Subsystem","url":"https://arxiv.org/abs/2502.10220","date":1739768400,"author":"","guid":1314,"unread":true,"content":"<article>arXiv:2502.10220v1 Announce Type: new \nAbstract: This work presents a framework for dynamic performance assessment of the higher layers in the hierarchical voltage regulation scheme, with case studies applied to specific areas of the Norwegian grid. Unlike the primary (PVR) level, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single device at a time, handling instead several reactive power resources available within a control zone including generator units, static VAr compensators and others. Proper SVR-TVR coordination for realistic transmission systems is a challenging topic at the core of many ongoing discussions in voltage control literature. Special focus is placed on practical considerations from the system operator perspective, since this research is also aimed at simplifying daily control centre routines. Dynamic simulation results concern a 21-bus equivalent of a 132 kV network model that accurately represents a Norwegian grid subsystem. Case studies address daily grid operation with real-life load demand and wind power generation profiles, showing that the proposed strategy is effective not only to minimize total active power losses as much as possible within system-wide limitations, but also to maintain adequate voltage profiles and reactive power flows. Findings pertaining to this work showcase the benefits of applying hierarchical voltage regulation layers as an asset to day-to-day control center management of a realistic transmission network.</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrated Multi-Simulation Environments for Aerial Robotics Research","url":"https://arxiv.org/abs/2502.10218","date":1739768400,"author":"","guid":1315,"unread":true,"content":"<article>arXiv:2502.10218v1 Announce Type: new \nAbstract: Simulation frameworks play a pivotal role in the safe development of robotic applications. However, often different components of an envisioned robotic system are best simulated in different environments/simulators. This poses a significant challenge in simulating the entire project into a single integrated robotic framework. Specifically, for partially-open or closed-source simulators, often two core limitations arise. i) Actors in the scene other than the designated robots cannot be controlled during runtime via interfaces such as ROS and ii) retrieving real-time state information (such as pose, velocity etc.) of objects in the scene is prevented. In this work, we address these limitations and describe our solution for the use case of integrating aerial drones simulated by the powerful simulator Sphinx (provided by Parrot Drone) into the Gazebo simulator. We achieve this by means of a mirrored instance of a drone that is included into existing Gazebo-based environments. A promising application of our integrated simulation environment is the task of target tracking that is common in aerial multi-robot scenarios. Therefore, to demonstrate the effectiveness our our integrated simulation, we also implement a model predictive controller (MPC) that outperforms the default PID-based controller framework provided with the Parrot's popular Anafi drone in various dynamic tracking scenarios thus enhancing the utility of the overall system. We test our solution by including the Anafi drone in an existing Gazebo-based simulation and evaluate the performance of the MPC through rigorous testing in simulated and real-world tracking experiments against a customized PID controller baseline. Source code is published on https://github.com/robot-perception-group/anafi_sim.</article>","contentLength":1833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Forget the Data and Fine-Tuning! Just Fold the Network to Compress","url":"https://arxiv.org/abs/2502.10216","date":1739768400,"author":"","guid":1316,"unread":true,"content":"<article>arXiv:2502.10216v1 Announce Type: new \nAbstract: We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.</article>","contentLength":916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Large Language Models Reason Causally Like Us? Even Better?","url":"https://arxiv.org/abs/2502.10215","date":1739768400,"author":"","guid":1317,"unread":true,"content":"<article>arXiv:2502.10215v1 Announce Type: new \nAbstract: Causal reasoning is a core component of intelligence. Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns. We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. We find that LLMs reason causally along a spectrum from human-like to normative inference, with alignment shifting based on model, context, and task. Overall, GPT-4o and Claude showed the most normative behavior, including \"explaining away\", whereas Gemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected independence of causes - Claude the least - they exhibited strong associative reasoning and predictive inference when assessing the likelihood of the effect given its causes. These findings underscore the need to assess AI biases as they increasingly assist human decision-making.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mapping bathymetry of inland water bodies on the North Slope of Alaska with Landsat using Random Forest","url":"https://arxiv.org/abs/2502.10214","date":1739768400,"author":"","guid":1318,"unread":true,"content":"<article>arXiv:2502.10214v1 Announce Type: new \nAbstract: The North Slope of Alaska is dominated by small waterbodies that provide critical ecosystem services for local population and wildlife. Detailed information on the depth of the waterbodies is scarce due to the challenges with collecting such information. In this work we have trained a machine learning (Random Forest Regressor) model to predict depth from multispectral Landsat data in waterbodies across the North Slope of Alaska. The greatest challenge is the scarcity of in situ data, which is expensive and difficult to obtain, to train the model. We overcame this challenge by using modeled depth predictions from a prior study as synthetic training data to provide a more diverse training data pool for the Random Forest. The final Random Forest model was more robust than models trained directly on the in situ data and when applied to 208 Landsat 8 scenes from 2016 to 2018 yielded a map with an overall $r^{2}$ value of 0.76 on validation. The final map has been made available through the Oak Ridge National Laboratory Distribute Active Archive Center (ORNL-DAAC). This map represents a first of its kind regional assessment of waterbody depth with per pixel estimates of depth for the entire North Slope of Alaska.</article>","contentLength":1275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Control-flow anomaly detection by process mining-based feature extraction and dimensionality reduction","url":"https://arxiv.org/abs/2502.10211","date":1739768400,"author":"","guid":1319,"unread":true,"content":"<article>arXiv:2502.10211v1 Announce Type: new \nAbstract: The business processes of organizations may deviate from normal control flow due to disruptive anomalies, including unknown, skipped, and wrongly-ordered activities. To identify these control-flow anomalies, process mining can check control-flow correctness against a reference process model through conformance checking, an explainable set of algorithms that allows linking any deviations with model elements. However, the effectiveness of conformance checking-based techniques is negatively affected by noisy event data and low-quality process models. To address these shortcomings and support the development of competitive and explainable conformance checking-based techniques for control-flow anomaly detection, we propose a novel process mining-based feature extraction approach with alignment-based conformance checking. This variant aligns the deviating control flow with a reference process model; the resulting alignment can be inspected to extract additional statistics such as the number of times a given activity caused mismatches. We integrate this approach into a flexible and explainable framework for developing techniques for control-flow anomaly detection. The framework combines process mining-based feature extraction and dimensionality reduction to handle high-dimensional feature sets, achieve detection effectiveness, and support explainability. The results show that the framework techniques implementing our approach outperform the baseline conformance checking-based techniques while maintaining the explainable nature of conformance checking. We also provide an explanation of why existing conformance checking-based techniques may be ineffective.</article>","contentLength":1724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mutual Coupling in Holographic MIMO: Physical Modeling and Information-Theoretic Analysis","url":"https://arxiv.org/abs/2502.10209","date":1739768400,"author":"","guid":1320,"unread":true,"content":"<article>arXiv:2502.10209v1 Announce Type: new \nAbstract: This paper presents a comprehensive framework for holographic multiantenna communication, a paradigm that integrates both wide apertures and closely spaced antennas relative to the wavelength. The presented framework is physically grounded, enabling information-theoretic analyses that inherently incorporate correlation and mutual coupling among the antennas. This establishes the combined effects of correlation and coupling on the information-theoretic performance limits across SNR levels. Additionally, it reveals that, by suitably selecting the individual antenna patterns, mutual coupling can be harnessed to either reinforce or counter spatial correlations as appropriate for specific SNRs, thereby improving the performance.</article>","contentLength":782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SGS-GNN: A Supervised Graph Sparsification method for Graph Neural Networks","url":"https://arxiv.org/abs/2502.10208","date":1739768400,"author":"","guid":1321,"unread":true,"content":"<article>arXiv:2502.10208v1 Announce Type: new \nAbstract: We propose SGS-GNN, a novel supervised graph sparsifier that learns the sampling probability distribution of edges and samples sparse subgraphs of a user-specified size to reduce the computational costs required by GNNs for inference tasks on large graphs. SGS-GNN employs regularizers in the loss function to enhance homophily in sparse subgraphs, boosting the accuracy of GNNs on heterophilic graphs, where a significant number of the neighbors of a node have dissimilar labels. SGS-GNN also supports conditional updates of the probability distribution learning module based on a prior, which helps narrow the search space for sparse graphs. SGS-GNN requires fewer epochs to obtain high accuracies since it learns the search space of subgraphs more effectively than methods using fixed distributions such as random sampling. Extensive experiments using 33 homophilic and heterophilic graphs demonstrate the following: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN improves the F1-scores by a geometric mean of 4% relative to the original graph; on heterophilic graphs, the prediction accuracy is better up to 30%. (ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores of 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and (iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires about half the number of epochs to converge.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RIPOST: Two-Phase Private Decomposition for Multidimensional Data","url":"https://arxiv.org/abs/2502.10207","date":1739768400,"author":"","guid":1322,"unread":true,"content":"<article>arXiv:2502.10207v1 Announce Type: new \nAbstract: Differential privacy (DP) is considered as the gold standard for data privacy. While the problem of answering simple queries and functions under DP guarantees has been thoroughly addressed in recent years, the problem of releasing multidimensional data under DP remains challenging. In this paper, we focus on this problem, in particular on how to construct privacy-preserving views using a domain decomposition approach. The main idea is to recursively split the domain into sub-domains until a convergence condition is met. The resulting sub-domains are perturbed and then published in order to be used to answer arbitrary queries. Existing methods that have addressed this problem using domain decomposition face two main challenges: (i) efficient privacy budget management over a variable and undefined decomposition depth $h$; and (ii) defining an optimal data-dependent splitting strategy that minimizes the error in the sub-domains while ensuring the smallest possible decomposition. To address these challenges, we present RIPOST, a multidimensional data decomposition algorithm that bypasses the constraint of predefined depth $h$ and applies a data-aware splitting strategy to optimize the quality of the decomposition results.The core of RIPOST is a two-phase strategy that separates non-empty sub-domains at an early stage from empty sub-domains by exploiting the properties of multidimensional datasets, and then decomposes the resulting sub-domains with minimal inaccuracies using the mean function. Moreover, RIPOST introduces a privacy budget distribution that allows decomposition without requiring prior computation of the depth $h$. Through extensive experiments, we demonstrated that \\texttt{RIPOST} outperforms state-of-the-art methods in terms of data utility and accuracy on a variety of datasets and test cases</article>","contentLength":1883,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Looking around you: external information enhances representations for event sequences","url":"https://arxiv.org/abs/2502.10205","date":1739768400,"author":"","guid":1323,"unread":true,"content":"<article>arXiv:2502.10205v1 Announce Type: new \nAbstract: Representation learning produces models in different domains, such as store purchases, client transactions, and general people's behaviour. However, such models for sequential data usually process a single sequence, ignoring context from other relevant ones, even in domains with rapidly changing external environments like finance or misguiding the prediction for a user with no recent events.\n  We are the first to propose a method that aggregates information from multiple user representations augmenting a specific user one for a scenario of multiple co-occurring event sequences. Our study considers diverse aggregation approaches, ranging from simple pooling techniques to trainable attention-based approaches, especially Kernel attention aggregation, that can highlight more complex information flow from other users. The proposed method operates atop an existing encoder and supports its efficient fine-tuning. Across considered datasets of financial transactions and downstream tasks, Kernel attention improves ROC AUC scores, both with and without fine-tuning, while mean pooling yields a smaller but still significant gain.</article>","contentLength":1183,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-in-the-Loop Sensing and Communication Joint Design for Edge Intelligence","url":"https://arxiv.org/abs/2502.10203","date":1739768400,"author":"","guid":1324,"unread":true,"content":"<article>arXiv:2502.10203v1 Announce Type: new \nAbstract: Recent breakthroughs in artificial intelligence (AI), wireless communications, and sensing technologies have accelerated the evolution of edge intelligence. However, conventional systems still grapple with issues such as low communication efficiency, redundant data acquisition, and poor model generalization. To overcome these challenges, we propose an innovative framework that enhances edge intelligence through AI-in-the-loop joint sensing and communication (JSAC). This framework features an AI-driven closed-loop control architecture that jointly optimizes system resources, thereby delivering superior system-level performance. A key contribution of our work is establishing an explicit relationship between validation loss and the system's tunable parameters. This insight enables dynamic reduction of the generalization error through AI-driven closed-loop control. Specifically, for sensing control, we introduce an adaptive data collection strategy based on gradient importance sampling, allowing edge devices to autonomously decide when to terminate data acquisition and how to allocate sample weights based on real-time model feedback. For communication control, drawing inspiration from stochastic gradient Langevin dynamics (SGLD), our joint optimization of transmission power and batch size converts channel and data noise into gradient perturbations that help mitigate overfitting. Experimental evaluations demonstrate that our framework reduces communication energy consumption by up to 77 percent and sensing costs measured by the number of collected samples by up to 52 percent while significantly improving model generalization -- with up to 58 percent reductions of the final validation loss. It validates that the proposed scheme can harvest the mutual benefit of AI and JSAC systems by incorporating the model itself into the control loop of the system.</article>","contentLength":1925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Post-Training Quantization Benefit from an Additional QLoRA Integration?","url":"https://arxiv.org/abs/2502.10202","date":1739768400,"author":"","guid":1325,"unread":true,"content":"<article>arXiv:2502.10202v1 Announce Type: new \nAbstract: Large language models (LLMs) have transformed natural language processing but pose significant challenges for real-world deployment. These models necessitate considerable computing resources, which can be costly and frequently unavailable. Model compression techniques such as quantization are often leveraged to alleviate resource demand, but they may have a negative impact on the generation quality. In this study, we explore the integration of 4-bit Post-training Quantization (PTQ) with QLoRA to address these issues. We demonstrate through extensive experiments that this integration outperforms standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs, validated across proprietary and public datasets with different quantization algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration, offering a viable solution for deploying powerful LLMs in resource-constrained environments without compromising on performance.</article>","contentLength":1011,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prediction hubs are context-informed frequent tokens in LLMs","url":"https://arxiv.org/abs/2502.10201","date":1739768400,"author":"","guid":1326,"unread":true,"content":"<article>arXiv:2502.10201v1 Announce Type: new \nAbstract: Hubness, the tendency for few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first show, theoretically, that the only representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appeareance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. On the other hand, when other distance computations involving LLM representations are performed, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. In summary, our work highlights, on the one hand, how hubness, while omnipresent in high-dimensional spaces, is not always a negative property that needs to be mitigated, and, on the other hand, it shows that various widely-used LLMs have developed a guessing strategy that consists in constantly assigning a high probability to frequent tokens.</article>","contentLength":1579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Reinforcement Learning for Actors","url":"https://arxiv.org/abs/2502.10200","date":1739768400,"author":"","guid":1327,"unread":true,"content":"<article>arXiv:2502.10200v1 Announce Type: new \nAbstract: Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly controls system dynamics, instead of the actor (action-generating neural network) outputs at each moment, bringing about a major qualitative shift in reinforcement learning (RL) from static to dynamic. The actor is initially designed to generate chaotic dynamics through the loop with its environment, enabling the agent to perform flexible and deterministic exploration. Dynamic RL controls global system dynamics using a local index called \"sensitivity,\" which indicates how much the input neighborhood contracts or expands into the corresponding output neighborhood through each neuron's processing. While sensitivity adjustment learning (SAL) prevents excessive convergence of the dynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them -- to converge more to improve reproducibility around better state transitions with positive TD error and to diverge more to enhance exploration around worse transitions with negative TD error. Dynamic RL was applied only to the actor in an Actor-Critic RL architecture while applying it to the critic remains a challenge. It was tested on two dynamic tasks and functioned effectively without external exploration noise or backward computation through time. Moreover, it exhibited excellent adaptability to new environments, although some problems remain. Drawing parallels between 'exploration' and 'thinking,' the author hypothesizes that \"exploration grows into thinking through learning\" and believes this RL could be a key technique for the emergence of thinking, including inspiration that cannot be reconstructed from massive existing text data. Finally, despite being presumptuous, the author presents the argument that this research should not proceed due to its potentially fatal risks, aiming to encourage discussion.</article>","contentLength":1915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the unconventional Hug integrator","url":"https://arxiv.org/abs/2502.10199","date":1739768400,"author":"","guid":1328,"unread":true,"content":"<article>arXiv:2502.10199v1 Announce Type: new \nAbstract: Hug is a recently proposed iterative mapping used to design efficient updates in Markov chain Monte Carlo (MCMC) methods when sampling along manifolds is of interest. In this paper we show that Hug may be interpreted as a consistent discretization of a system of differential equations with a rather complicated structure. The proof of convergence of this discretization includes a number of unusual features we explore fully. We uncover an unexpected and, yet, undocumented property of the solutions of the underlying dynamical system that manifest itself by the existence of Hug trajectories that fail to cover the manifold of interest. This suggests caution when using the Hug update.</article>","contentLength":736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MathConstruct: Challenging LLM Reasoning with Constructive Proofs","url":"https://arxiv.org/abs/2502.10197","date":1739768400,"author":"","guid":1329,"unread":true,"content":"<article>arXiv:2502.10197v1 Announce Type: new \nAbstract: While Large Language Models (LLMs) demonstrate impressive performance in mathematics, existing math benchmarks come with significant limitations. Many focus on problems with fixed ground-truth answers, and are often saturated due to problem simplicity or the viability of guessing or memorization. Crucially, they capture only a narrow subset of relevant math problems. To address this research gap, we introduce \\mc, a new benchmark of 126 challenging problems sourced from various math competitions, which targets constructive proofs, a widely encountered problem type requiring the construction of mathematical objects with specific properties. These proofs are particularly suitable for LLM evaluation, as solution correctness can be easily verified. Our automated verifiers also enable MathConstruct to generate problem variations, used to evaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct problems, highlighting its complexity and importance for LLM evaluation.</article>","contentLength":1041,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring the Camera Bias of Person Re-identification","url":"https://arxiv.org/abs/2502.10195","date":1739768400,"author":"","guid":1330,"unread":true,"content":"<article>arXiv:2502.10195v1 Announce Type: new \nAbstract: We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.</article>","contentLength":1559,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Translating Common Security Assertions Across Processor Designs: A RISC-V Case Study","url":"https://arxiv.org/abs/2502.10194","date":1739768400,"author":"","guid":1331,"unread":true,"content":"<article>arXiv:2502.10194v1 Announce Type: new \nAbstract: RISC-V is gaining popularity for its adaptability and cost-effectiveness in processor design. With the increasing adoption of RISC-V, the importance of implementing robust security verification has grown significantly. In the state of the art, various approaches have been developed to strengthen the security verification process. Among these methods, assertion-based security verification has proven to be a promising approach for ensuring that security features are effectively met. To this end, some approaches manually define security assertions for processor designs; however, these manual methods require significant time, cost, and human expertise. Consequently, recent approaches focus on translating pre-defined security assertions from one design to another. Nonetheless, these methods are not primarily centered on processor security, particularly RISC-V. Furthermore, many of these approaches have not been validated against real-world attacks, such as hardware Trojans. In this work, we introduce a methodology for translating security assertions across processors with different architectures, using RISC-V as a case study. Our approach reduces time and cost compared to developing security assertions manually from the outset. Our methodology was applied to five critical security modules with assertion translation achieving nearly 100% success across all modules. These results validate the efficacy of our approach and highlight its potential for enhancing security verification in modern processor designs. The effectiveness of the translated assertions was rigorously tested against hardware Trojans defined by large language models (LLMs), demonstrating their reliability in detecting security breaches.</article>","contentLength":1774,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Merging public elementary schools to reduce racial/ethnic segregation","url":"https://arxiv.org/abs/2502.10193","date":1739768400,"author":"","guid":1332,"unread":true,"content":"<article>arXiv:2502.10193v1 Announce Type: new \nAbstract: Diverse schools can help address implicit biases and increase empathy, mutual respect, and reflective thought by fostering connections between students from different racial/ethnic, socioeconomic, and other backgrounds. Unfortunately, demographic segregation remains rampant in US public schools, despite over 70 years since the passing of federal legislation formally outlawing segregation by race. However, changing how students are assigned to schools can help foster more integrated learning environments. In this paper, we explore \"school mergers\" as one such under-explored, yet promising, student assignment policy change. School mergers involve merging the school attendance boundaries, or catchment areas, of schools and subsequently changing the grades each school offers. We develop an algorithm to simulate elementary school mergers across 200 large school districts serving 4.5 million elementary school students and find that pairing or tripling schools in this way could reduce racial/ethnic segregation by a median relative 20% -- and as much as nearly 60% in some districts -- while increasing driving times to schools by an average of a few minutes each way. Districts with many interfaces between racially/ethnically-disparate neighborhoods tend to be prime candidates for mergers. We also compare the expected results of school mergers to other typical integration policies, like redistricting, and find that different policies may be more or less suitable in different places. Finally, we make our results available through a public dashboard for policymakers and community members to explore further (https://mergers.schooldiversity.org). Together, our study offers new findings and tools to support integration policy-making across US public school districts.</article>","contentLength":1831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Note on \"Constructing Bent Functions Outside the Maiorana-McFarland Class Using a General Form of Rothaus\"","url":"https://arxiv.org/abs/2502.10192","date":1739768400,"author":"","guid":1333,"unread":true,"content":"<article>arXiv:2502.10192v1 Announce Type: new \nAbstract: In 2017, Zhang et al. proposed a question (not open problem) and two open problems in [IEEE TIT 63 (8): 5336--5349, 2017] about constructing bent functions by using Rothaus' construction. In this note, we prove that the sufficient conditions of Rothaus' construction are also necessary, which answers their question. Besides, we demonstrate that the second open problem, which considers the iterative method of constructing bent functions by using Rothaus' construction, has only a trivial solution. It indicates that all bent functions obtained by using Rothaus' construction iteratively can be generated from the direct sum of an initial bent function and a quadratic bent function. This directly means that Zhang et al.'s construction idea makes no contribution to the construction of bent functions. To compensate the weakness of their work, we propose an iterative construction of bent functions by using a secondary construction in [DCC 88: 2007--2035, 2020].</article>","contentLength":1014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VideoDiff: Human-AI Video Co-Creation with Alternatives","url":"https://arxiv.org/abs/2502.10190","date":1739768400,"author":"","guid":1334,"unread":true,"content":"<article>arXiv:2502.10190v1 Announce Type: new \nAbstract: To make an engaging video, people sequence interesting moments and add visuals such as B-rolls or text. While video editing requires time and effort, AI has recently shown strong potential to make editing easier through suggestions and automation. A key strength of generative models is their ability to quickly generate multiple variations, but when provided with many alternatives, creators struggle to compare them to find the best fit. We propose VideoDiff, an AI video editing tool designed for editing with alternatives. With VideoDiff, creators can generate and review multiple AI recommendations for each editing process: creating a rough cut, inserting B-rolls, and adding text effects. VideoDiff simplifies comparisons by aligning videos and highlighting differences through timelines, transcripts, and video previews. Creators have the flexibility to regenerate and refine AI suggestions as they compare alternatives. Our study participants (N=12) could easily compare and customize alternatives, creating more satisfying results.</article>","contentLength":1090,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning based Constrained Optimal Control: an Interpretable Reward Design","url":"https://arxiv.org/abs/2502.10187","date":1739768400,"author":"","guid":1335,"unread":true,"content":"<article>arXiv:2502.10187v1 Announce Type: new \nAbstract: This paper presents an interpretable reward design framework for reinforcement learning based constrained optimal control problems with state and terminal constraints. The problem is formalized within a standard partially observable Markov decision process framework. The reward function is constructed from four weighted components: a terminal constraint reward, a guidance reward, a penalty for state constraint violations, and a cost reduction incentive reward. A theoretically justified reward design is then presented, which establishes bounds on the weights of the components. This approach ensures that constraints are satisfied and objectives are optimized while mitigating numerical instability. Acknowledging the importance of prior knowledge in reward design, we sequentially solve two subproblems, using each solution to inform the reward design for the subsequent problem. Subsequently, we integrate reinforcement learning with curriculum learning, utilizing policies derived from simpler subproblems to assist in tackling more complex challenges, thereby facilitating convergence. The framework is evaluated against original and randomly weighted reward designs in a multi-agent particle environment. Experimental results demonstrate that the proposed approach significantly enhances satisfaction of terminal and state constraints and optimization of control cost.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Powerful Random Forest Featuring Linear Extensions (RaFFLE)","url":"https://arxiv.org/abs/2502.10185","date":1739768400,"author":"","guid":1336,"unread":true,"content":"<article>arXiv:2502.10185v1 Announce Type: new \nAbstract: Random forests are widely used in regression. However, the decision trees used as base learners are poor approximators of linear relationships. To address this limitation we propose RaFFLE (Random Forest Featuring Linear Extensions), a novel framework that integrates the recently developed PILOT trees (Piecewise Linear Organic Trees) as base learners within a random forest ensemble. PILOT trees combine the computational efficiency of traditional decision trees with the flexibility of linear model trees. To ensure sufficient diversity of the individual trees, we introduce an adjustable regularization parameter and use node-level feature sampling. These modifications improve the accuracy of the forest. We establish theoretical guarantees for the consistency of RaFFLE under weak conditions, and its faster convergence when the data are generated by a linear model. Empirical evaluations on 136 regression datasets demonstrate that RaFFLE outperforms the classical CART and random forest methods, the regularized linear methods Lasso and Ridge, and the state-of-the-art XGBoost algorithm, across both linear and nonlinear datasets. By balancing predictive accuracy and computational efficiency, RaFFLE proves to be a versatile tool for tackling a wide variety of regression problems.</article>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Realistic Evaluation of Deep Partial-Label Learning Algorithms","url":"https://arxiv.org/abs/2502.10184","date":1739768400,"author":"","guid":1337,"unread":true,"content":"<article>arXiv:2502.10184v1 Announce Type: new \nAbstract: Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.</article>","contentLength":1717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Doing More With Less: Towards More Data-Efficient Syndrome-Based Neural Decoders","url":"https://arxiv.org/abs/2502.10183","date":1739768400,"author":"","guid":1338,"unread":true,"content":"<article>arXiv:2502.10183v1 Announce Type: new \nAbstract: While significant research efforts have been directed toward developing more capable neural decoding architectures, comparatively little attention has been paid to the quality of training data. In this study, we address the challenge of constructing effective training datasets to maximize the potential of existing syndrome-based neural decoder architectures. We emphasize the advantages of using fixed datasets over generating training data dynamically and explore the problem of selecting appropriate training targets within this framework. Furthermore,we propose several heuristics for selecting training samples and present experimental evidence demonstrating that, with carefully curated datasets, it is possible to train neural decoders to achieve superior performance while requiring fewer training examples.</article>","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safe platooning control of connected and autonomous vehicles on curved multi-lane roads","url":"https://arxiv.org/abs/2502.10180","date":1739768400,"author":"","guid":1339,"unread":true,"content":"<article>arXiv:2502.10180v1 Announce Type: new \nAbstract: This paper investigates the safe platoon formation tracking and merging control problem of connected and automated vehicles (CAVs) on curved multi-lane roads. The first novelty is the separation of the control designs into two distinct parts: a lateral control law that ensures a geometrical convergence towards the reference path regardless of the translational velocity, and a longitudinal control design for each vehicle to achieve the desired relative arc length and velocity with respect to its neighboring vehicle. The second novelty is exploiting the constructive barrier feedback as an additive term to the nominal tracking control, ensuring both lateral and longitudinal collision avoidance. This constructive barrier feedback acts as a dissipative term, slowing down the relative velocity toward obstacles without affecting the nominal controller's performance. Consequently, our proposed control method enables safe platoon formation of vehicles on curved multi-lane roads, with theoretical guarantees for safety invariance and stability analysis. Simulation and experimental results on connected vehicles are provided to further validate the effectiveness of the proposed method.</article>","contentLength":1240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Markov to Laplace: How Mamba In-Context Learns Markov Chains","url":"https://arxiv.org/abs/2502.10178","date":1739768400,"author":"","guid":1340,"unread":true,"content":"<article>arXiv:2502.10178v1 Announce Type: new \nAbstract: While transformer-based language models have driven the AI revolution thus far, their computational complexity has spurred growing interest in viable alternatives, such as structured state space sequence models (SSMs) and Selective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown remarkable inference speed ups over transformers while achieving comparable or superior performance on complex language modeling tasks. However, despite these architectural innovations and empirical successes, the fundamental learning capabilities of Mamba remain poorly understood. In this paper, we address this gap by studying in-context learning (ICL) on Markov chains and uncovering a surprising phenomenon: unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator, which is both Bayes and minimax optimal, for all Markovian orders. To explain this, we theoretically characterize the representation capacity of Mamba and reveal the fundamental role of convolution in enabling it to represent the optimal Laplacian smoothing. These theoretical insights align strongly with empirical results and, to the best of our knowledge, represent the first formal connection between Mamba and optimal statistical estimators. Finally, we outline promising research directions inspired by these findings.</article>","contentLength":1395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning","url":"https://arxiv.org/abs/2502.10177","date":1739768400,"author":"","guid":1341,"unread":true,"content":"<article>arXiv:2502.10177v1 Announce Type: new \nAbstract: A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.</article>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Technical Risks of (Lethal) Autonomous Weapons Systems","url":"https://arxiv.org/abs/2502.10174","date":1739768400,"author":"","guid":1342,"unread":true,"content":"<article>arXiv:2502.10174v1 Announce Type: new \nAbstract: The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS in short, promise unprecedented operational capabilities, but they also introduce profound risks that challenge the principles of control, accountability, and stability in international security. This report outlines the key technological risks associated with (L)AWS deployment, emphasizing their unpredictability, lack of transparency, and operational unreliability, which can lead to severe unintended consequences.\n  Key Takeaways:\n  1. Proposed advantages of (L)AWS can only be achieved through objectification and classification, but a range of systematic risks limit the reliability and predictability of classifying algorithms.\n  2. These systematic risks include the black-box nature of AI decision-making, susceptibility to reward hacking, goal misgeneralization and potential for emergent behaviors that escape human control.\n  3. (L)AWS could act in ways that are not just unexpected but also uncontrollable, undermining mission objectives and potentially escalating conflicts.\n  4. Even rigorously tested systems may behave unpredictably and harmfully in real-world conditions, jeopardizing both strategic stability and humanitarian principles.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling and Simulating Emerging Memory Technologies: A Tutorial","url":"https://arxiv.org/abs/2502.10167","date":1739768400,"author":"","guid":1343,"unread":true,"content":"<article>arXiv:2502.10167v1 Announce Type: new \nAbstract: Non-volatile Memory (NVM) technologies present a promising alternative to traditional volatile memories such as SRAM and DRAM. Due to the limited availability of real NVM devices, simulators play a crucial role in architectural exploration and hardware-software co-design. This tutorial presents a simulation toolchain through four detailed case studies, showcasing its applicability to various domains of system design, including hybrid main-memory and cache, compute-in-memory, and wear-leveling design. These case studies provide the reader with practical insights on customizing the toolchain for their specific research needs. The source code is open-sourced.</article>","contentLength":713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"It's Like Not Being Able to Read and Write\": Narrowing the Digital Divide for Older Adults and Leveraging the Role of Digital Educators in Ireland","url":"https://arxiv.org/abs/2502.10166","date":1739768400,"author":"","guid":1344,"unread":true,"content":"<article>arXiv:2502.10166v1 Announce Type: new \nAbstract: As digital services increasingly replace traditional analogue systems, ensuring that older adults are not left behind is critical to fostering inclusive access. This study explores how digital educators support older adults in developing essential digital skills, drawing insights from interviews with $34$ educators in Ireland. These educators, both professional and volunteer, offer instruction through a range of formats, including workshops, remote calls, and in-person sessions. Our findings highlight the importance of personalized, step-by-step guidance tailored to older adults' learning needs, as well as fostering confidence through hands-on engagement with technology. Key challenges identified include limited transportation options, poor internet connectivity, outdated devices, and a lack of familial support for learning. To address these barriers, we propose enhanced public funding, expanded access to resources, and sustainable strategies such as providing relevant and practical course materials. Additionally, innovative tools like simulated online platforms for practicing digital transactions can help reduce anxiety and enhance digital literacy among older adults. This study underscores the vital role that digital educators play in bridging the digital divide, creating a more inclusive, human-centered approach to digital learning for older adults.</article>","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Generalization Power of a DNN in Terms of Symbolic Interactions","url":"https://arxiv.org/abs/2502.10162","date":1739768400,"author":"","guid":1345,"unread":true,"content":"<article>arXiv:2502.10162v1 Announce Type: new \nAbstract: This paper aims to analyze the generalization power of deep neural networks (DNNs) from the perspective of interactions. Unlike previous analysis of a DNN's generalization power in a highdimensional feature space, we find that the generalization power of a DNN can be explained as the generalization power of the interactions. We found that the generalizable interactions follow a decay-shaped distribution, while non-generalizable interactions follow a spindle-shaped distribution. Furthermore, our theory can effectively disentangle these two types of interactions from a DNN. We have verified that our theory can well match real interactions in a DNN in experiments.</article>","contentLength":718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SessionRec: Next Session Prediction Paradigm For Generative Sequential Recommendation","url":"https://arxiv.org/abs/2502.10157","date":1739768400,"author":"","guid":1346,"unread":true,"content":"<article>arXiv:2502.10157v1 Announce Type: new \nAbstract: We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for generative sequential recommendation, addressing the fundamental misalignment between conventional next-item prediction paradigm (NIPP) and real-world recommendation scenarios. Unlike NIPP's item-level autoregressive generation that contradicts actual session-based user interactions, our framework introduces a session-aware representation learning through hierarchical sequence aggregation (intra/inter-session), reducing attention computation complexity while enabling implicit modeling of massive negative interactions, and a session-based prediction objective that better captures users' diverse interests through multi-item recommendation in next sessions. Moreover, we found that incorporating a rank loss for items within the session under the next session prediction paradigm can significantly improve the ranking effectiveness of generative sequence recommendation models. We also verified that SessionRec exhibits clear power-law scaling laws similar to those observed in LLMs. Extensive experiments conducted on public datasets and online A/B test in Meituan App demonstrate the effectiveness of SessionRec. The proposed paradigm establishes new foundations for developing industrial-scale generative recommendation systems through its model-agnostic architecture and computational efficiency.</article>","contentLength":1431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MonoForce: Learnable Image-conditioned Physics Engine","url":"https://arxiv.org/abs/2502.10156","date":1739768400,"author":"","guid":1347,"unread":true,"content":"<article>arXiv:2502.10156v1 Announce Type: new \nAbstract: We propose a novel model for the prediction of robot trajectories on rough offroad terrain from the onboard camera images. This model enforces the laws of classical mechanics through a physics-aware neural symbolic layer while preserving the ability to learn from large-scale data as it is end-to-end differentiable. The proposed hybrid model integrates a black-box component that predicts robot-terrain interaction forces with a neural-symbolic layer. This layer includes a differentiable physics engine that computes the robot's trajectory by querying these forces at the points of contact with the terrain. As the proposed architecture comprises substantial geometrical and physics priors, the resulting model can also be seen as a learnable physics engine conditioned on real images that delivers $10^4$ trajectories per second. We argue and empirically demonstrate that this architecture reduces the sim-to-real gap and mitigates out-of-distribution sensitivity. The differentiability, in conjunction with the rapid simulation speed, makes the model well-suited for various applications including model predictive control, trajectory shooting, supervised and reinforcement learning or SLAM. The codes and data are publicly available.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complete Symmetry Breaking for Finite Models","url":"https://arxiv.org/abs/2502.10155","date":1739768400,"author":"","guid":1348,"unread":true,"content":"<article>arXiv:2502.10155v1 Announce Type: new \nAbstract: This paper introduces a SAT-based technique that calculates a compact and complete symmetry-break for finite model finding, with the focus on structures with a single binary operation (magmas). Classes of algebraic structures are typically described as first-order logic formulas and the concrete algebras are models of these formulas. Such models include an enormous number of isomorphic, i.e. symmetric, algebras.\n  A complete symmetry-break is a formula that has as models, exactly one canonical representative from each equivalence class of algebras. Thus, we enable answering questions about properties of the models so that computation and search are restricted to the set of canonical representations.\n  For instance, we can answer the question: How many non-isomorphic semigroups are there of size $n$? Such questions can be answered by counting the satisfying assignments of a SAT formula, which already filters out non-isomorphic models. The introduced technique enables us calculating numbers of algebraic structures not present in the literature and going beyond the possibilities of pure enumeration approaches.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries","url":"https://arxiv.org/abs/2502.10154","date":1739768400,"author":"","guid":1349,"unread":true,"content":"<article>arXiv:2502.10154v1 Announce Type: new \nAbstract: We introduce EMSYNC, a video-based symbolic music generation model that aligns music with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate and align musical chords with scene cuts. Unlike existing models, our approach retains event-based encoding, ensuring fine-grained timing control and expressive musical nuances. We also propose a mapping scheme to bridge the video emotion classifier, which produces discrete emotion categories, with the emotion-conditioned MIDI generator, which operates on continuous-valued valence-arousal inputs. In subjective listening tests, EMSYNC outperforms state-of-the-art models across all subjective metrics, for music theory-aware participants as well as the general listeners.</article>","contentLength":1083,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay","url":"https://arxiv.org/abs/2502.10151","date":1739768400,"author":"","guid":1350,"unread":true,"content":"<article>arXiv:2502.10151v1 Announce Type: new \nAbstract: Centralized search engines are key for the Internet, but lead to undesirable concentration of power. Decentralized alternatives fail to offer equal document retrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come close to the performance of centralized solutions when the semantics of documents are properly captured. This work uses embeddings from Large Language Models to capture semantics and fulfill the promise of Semantic Overlay Networks. Our proposed algorithm, called Semantica, constructs a prefix tree (trie) utilizing document embeddings calculated by a language model. Users connect to each other based on the embeddings of their documents, ensuring that semantically similar users are directly linked. Thereby, this construction makes it more likely for user searches to be answered by the users that they are directly connected to, or by the users they are close to in the network connection graph. The implementation of our algorithm also accommodates the semantic diversity of individual users by spawning \"clone\" user identifiers in the tree. Our experiments use emulation with a real-world workload to show Semantica's ability to identify and connect to similar users quickly. Semantica finds up to ten times more semantically similar users than current state-of-the-art approaches. At the same time, Semantica can retrieve more than two times the number of relevant documents given the same network load. We also make our code publicly available to facilitate further research in the area.</article>","contentLength":1581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IRS-assisted Edge Computing for Vehicular Networks: A Generative Diffusion Model-based Stackelberg Game Approach","url":"https://arxiv.org/abs/2502.10149","date":1739768400,"author":"","guid":1351,"unread":true,"content":"<article>arXiv:2502.10149v1 Announce Type: new \nAbstract: Recent advancements in intelligent reflecting surfaces (IRS) and mobile edge computing (MEC) offer new opportunities to enhance the performance of vehicular networks. However, meeting the computation-intensive and latency-sensitive demands of vehicles remains challenging due to the energy constraints and dynamic environments. To address this issue, we study an IRS-assisted MEC architecture for vehicular networks. We formulate a multi-objective optimization problem aimed at minimizing the total task completion delay and total energy consumption by jointly optimizing task offloading, IRS phase shift vector, and computation resource allocation. Given the mixed-integer nonlinear programming (MINLP) and NP-hard nature of the problem, we propose a generative diffusion model (GDM)-based Stackelberg game (GDMSG) approach. Specifically, the problem is reformulated within a Stackelberg game framework, where generative GDM is integrated to capture complex dynamics to efficiently derive optimal solutions. Simulation results indicate that the proposed GDMSG achieves outstanding performance compared to the benchmark approaches.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cooperative Multi-Agent Planning with Adaptive Skill Synthesis","url":"https://arxiv.org/abs/2502.10148","date":1739768400,"author":"","guid":1352,"unread":true,"content":"<article>arXiv:2502.10148v1 Announce Type: new \nAbstract: Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpretable Concept-based Deep Learning Framework for Multimodal Human Behavior Modeling","url":"https://arxiv.org/abs/2502.10145","date":1739768400,"author":"","guid":1353,"unread":true,"content":"<article>arXiv:2502.10145v1 Announce Type: new \nAbstract: In the contemporary era of intelligent connectivity, Affective Computing (AC), which enables systems to recognize, interpret, and respond to human behavior states, has become an integrated part of many AI systems. As one of the most critical components of responsible AI and trustworthiness in all human-centered systems, explainability has been a major concern in AC. Particularly, the recently released EU General Data Protection Regulation requires any high-risk AI systems to be sufficiently interpretable, including biometric-based systems and emotion recognition systems widely used in the affective computing field. Existing explainable methods often compromise between interpretability and performance. Most of them focus only on highlighting key network parameters without offering meaningful, domain-specific explanations to the stakeholders. Additionally, they also face challenges in effectively co-learning and explaining insights from multimodal data sources. To address these limitations, we propose a novel and generalizable framework, namely the Attention-Guided Concept Model (AGCM), which provides learnable conceptual explanations by identifying what concepts that lead to the predictions and where they are observed. AGCM is extendable to any spatial and temporal signals through multimodal concept alignment and co-learning, empowering stakeholders with deeper insights into the model's decision-making process. We validate the efficiency of AGCM on well-established Facial Expression Recognition benchmark datasets while also demonstrating its generalizability on more complex real-world human behavior understanding applications.</article>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages","url":"https://arxiv.org/abs/2502.10140","date":1739768400,"author":"","guid":1354,"unread":true,"content":"<article>arXiv:2502.10140v1 Announce Type: new \nAbstract: Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Provably Efficient RL under Episode-Wise Safety in Linear CMDPs","url":"https://arxiv.org/abs/2502.10138","date":1739768400,"author":"","guid":1355,"unread":true,"content":"<article>arXiv:2502.10138v1 Announce Type: new \nAbstract: We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ regret with an episode-wise zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs.</article>","contentLength":930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation","url":"https://arxiv.org/abs/2502.10127","date":1739768400,"author":"","guid":1356,"unread":true,"content":"<article>arXiv:2502.10127v1 Announce Type: new \nAbstract: High-Definition (HD) maps play a crucial role in autonomous vehicle navigation, complementing onboard perception sensors for improved accuracy and safety. Traditional HD map generation relies on dedicated mapping vehicles, which are costly and fail to capture real-time infrastructure changes. This paper presents HDMapLaneNet, a novel framework leveraging V2X communication and Scene Graph Generation to collaboratively construct a localized geometric layer of HD maps. The approach extracts lane centerlines from front-facing camera images, represents them as graphs, and transmits the data for global aggregation to the cloud via V2X. Preliminary results on the nuScenes dataset demonstrate superior association prediction performance compared to a state-of-the-art method.</article>","contentLength":825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Relational Tabular Data without Shared Features","url":"https://arxiv.org/abs/2502.10125","date":1739768400,"author":"","guid":1357,"unread":true,"content":"<article>arXiv:2502.10125v1 Announce Type: new \nAbstract: Learning relational tabular data has gained significant attention recently, but most studies focus on single tables, overlooking the potential of cross-table learning. Cross-table learning, especially in scenarios where tables lack shared features and pre-aligned data, offers vast opportunities but also introduces substantial challenges. The alignment space is immense, and determining accurate alignments between tables is highly complex. We propose Latent Entity Alignment Learning (Leal), a novel framework enabling effective cross-table training without requiring shared features or pre-aligned data. Leal operates on the principle that properly aligned data yield lower loss than misaligned data, a concept embodied in its soft alignment mechanism. This mechanism is coupled with a differentiable cluster sampler module, ensuring efficient scaling to large relational tables. Furthermore, we provide a theoretical proof of the cluster sampler's approximation capacity. Extensive experiments on five real-world and five synthetic datasets show that Leal achieves up to a 26.8% improvement in predictive performance compared to state-of-the-art methods, demonstrating its effectiveness and scalability.</article>","contentLength":1256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling the Impact of Visual Stimuli on Redirection Noticeability with Gaze Behavior in Virtual Reality","url":"https://arxiv.org/abs/2502.10124","date":1739768400,"author":"","guid":1358,"unread":true,"content":"<article>arXiv:2502.10124v1 Announce Type: new \nAbstract: While users could embody virtual avatars that mirror their physical movements in Virtual Reality, these avatars' motions can be redirected to enable novel interactions. Excessive redirection, however, could break the user's sense of embodiment due to perceptual conflicts between vision and proprioception. While prior work focused on avatar-related factors influencing the noticeability of redirection, we investigate how the visual stimuli in the surrounding virtual environment affect user behavior and, in turn, the noticeability of redirection. Given the wide variety of different types of visual stimuli and their tendency to elicit varying individual reactions, we propose to use users' gaze behavior as an indicator of their response to the stimuli and model the noticeability of redirection. We conducted two user studies to collect users' gaze behavior and noticeability, investigating the relationship between them and identifying the most effective gaze behavior features for predicting noticeability. Based on the data, we developed a regression model that takes users' gaze behavior as input and outputs the noticeability of redirection. We then conducted an evaluation study to test our model on unseen visual stimuli, achieving an accuracy of 0.012 MSE. We further implemented an adaptive redirection technique and conducted a proof-of-concept study to evaluate its effectiveness with complex visual stimuli in two applications. The results indicated that participants experienced less physical demanding and a stronger sense of body ownership when using our adaptive technique, demonstrating the potential of our model to support real-world use cases.</article>","contentLength":1717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modern Hopfield Networks with Continuous-Time Memories","url":"https://arxiv.org/abs/2502.10122","date":1739768400,"author":"","guid":1359,"unread":true,"content":"<article>arXiv:2502.10122v1 Announce Type: new \nAbstract: Recent research has established a connection between modern Hopfield networks (HNs) and transformer attention heads, with guarantees of exponential storage capacity. However, these models still face challenges scaling storage efficiently. Inspired by psychological theories of continuous neural resource allocation in working memory, we propose an approach that compresses large discrete Hopfield memories into smaller, continuous-time memories. Leveraging continuous attention, our new energy function modifies the update rule of HNs, replacing the traditional softmax-based probability mass function with a probability density, over the continuous memory. This formulation aligns with modern perspectives on human executive function, offering a principled link between attractor dynamics in working memory and resource-efficient memory allocation. Our framework maintains competitive performance with HNs while leveraging a compressed memory, reducing computational costs across synthetic and video datasets.</article>","contentLength":1059,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compress image to patches for Vision Transformer","url":"https://arxiv.org/abs/2502.10120","date":1739768400,"author":"","guid":1360,"unread":true,"content":"<article>arXiv:2502.10120v1 Announce Type: new \nAbstract: The Vision Transformer (ViT) has made significant strides in the field of computer vision. However, as the depth of the model and the resolution of the input images increase, the computational cost associated with training and running ViT models has surged dramatically.This paper proposes a hybrid model based on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a module called CI2P, which utilizes the CompressAI encoder to compress images and subsequently generates a sequence of patches through a series of convolutions. CI2P can replace the Patch Embedding component in the ViT model, enabling seamless integration into existing ViT models.Compared to ViT-B/16, CI2P-ViT has the number of patches input to the self-attention layer reduced to a quarter of the original.This design not only significantly reduces the computational cost of the ViT model but also effectively enhances the model's accuracy by introducing the inductive bias properties of CNN.The ViT model's precision is markedly enhanced.When trained from the ground up on the Animals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing a 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's computational operations, measured in floating-point operations per second (FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in training velocity on identical hardware configurations.</article>","contentLength":1468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SeWA: Selective Weight Average via Probabilistic Masking","url":"https://arxiv.org/abs/2502.10119","date":1739768400,"author":"","guid":1361,"unread":true,"content":"<article>arXiv:2502.10119v1 Announce Type: new \nAbstract: Weight averaging has become a standard technique for enhancing model performance. However, methods such as Stochastic Weight Averaging (SWA) and Latest Weight Averaging (LAWA) often require manually designed procedures to sample from the training trajectory, and the results depend heavily on hyperparameter tuning. To minimize human effort, this paper proposes a simple yet efficient algorithm called Selective Weight Averaging (SeWA), which adaptively selects checkpoints during the final stages of training for averaging. Based on SeWA, we show that only a few points are needed to achieve better generalization and faster convergence. Theoretically, solving the discrete subset selection problem is inherently challenging. To address this, we transform it into a continuous probabilistic optimization framework and employ the Gumbel-Softmax estimator to learn the non-differentiable mask for each checkpoint. Further, we theoretically derive the SeWA's stability-based generalization bounds, which are sharper than that of SGD under both convex and non-convex assumptions. Finally, solid extended experiments in various domains, including behavior cloning, image classification, and text classification, further validate the effectiveness of our approach.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Image Embedding Sampling Method for Diverse Captioning","url":"https://arxiv.org/abs/2502.10118","date":1739768400,"author":"","guid":1362,"unread":true,"content":"<article>arXiv:2502.10118v1 Announce Type: new \nAbstract: Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerometry-based Energy Expenditure Estimation During Activities of Daily Living: A Comparison Among Different Accelerometer Compositions","url":"https://arxiv.org/abs/2502.10112","date":1739768400,"author":"","guid":1363,"unread":true,"content":"<article>arXiv:2502.10112v1 Announce Type: new \nAbstract: Physical activity energy expenditure (PAEE) can be measured from breath-by-breath respiratory data, which can serve as a reference. Alternatively, PAEE can be predicted from the body movements, which can be measured and estimated with accelerometers. The body center of mass (COM) acceleration reflects the movements of the whole body and thus serves as a good predictor for PAEE. However, the wrist has also become a popular location due to recent advancements in wrist-worn devices. Therefore, in this work, using the respiratory data measured by COSMED K5 as the reference, we evaluated and compared the performances of COM-based settings and wrist-based settings. The COM-based settings include two different accelerometer compositions, using only the pelvis accelerometer (pelvis-acc) and the pelvis accelerometer with two accelerometers from two thighs (3-acc). The wrist-based settings include using only the left wrist accelerometer (l-wrist-acc) and only the right wrist accelerometer (r-wrist-acc). We implemented two existing PAEE estimation methods on our collected dataset, where 9 participants performed activities of daily living while wearing 5 accelerometers (i.e., pelvis, two thighs, and two wrists). These two methods include a linear regression (LR) model and a CNN-LSTM model. Both models yielded the best results with the COM-based 3-acc setting (LR: $R^2$ = 0.41, CNN-LSTM: $R^2$ = 0.53). No significant difference was found between the 3-acc and pelvis-acc settings (p-value = 0.278). For both models, neither the l-wrist-acc nor the r-wrist-acc settings demonstrated predictive power on PAEE with $R^2$ values close to 0, significantly outperformed by the two COM-based settings (p-values $&lt;$ 0.05). No significant difference was found between the two wrists (p-value = 0.329).</article>","contentLength":1852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"COMBINEX: A Unified Counterfactual Explainer for Graph Neural Networks via Node Feature and Structural Perturbations","url":"https://arxiv.org/abs/2502.10111","date":1739768400,"author":"","guid":1364,"unread":true,"content":"<article>arXiv:2502.10111v1 Announce Type: new \nAbstract: Counterfactual explanations have emerged as a powerful tool to unveil the opaque decision-making processes of graph neural networks (GNNs). However, existing techniques primarily focus on edge modifications, often overlooking the crucial role of node feature perturbations in shaping model predictions. To address this limitation, we propose COMBINEX, a novel GNN explainer that generates counterfactual explanations for both node and graph classification tasks. Unlike prior methods, which treat structural and feature-based changes independently, COMBINEX optimally balances modifications to edges and node features by jointly optimizing these perturbations. This unified approach ensures minimal yet effective changes required to flip a model's prediction, resulting in realistic and interpretable counterfactuals. Additionally, COMBINEX seamlessly handles both continuous and discrete node features, enhancing its versatility across diverse datasets and GNN architectures. Extensive experiments on real-world datasets and various GNN architectures demonstrate the effectiveness and robustness of our approach over existing baselines.</article>","contentLength":1186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ScamFerret: Detecting Scam Websites Autonomously with Large Language Models","url":"https://arxiv.org/abs/2502.10110","date":1739768400,"author":"","guid":1365,"unread":true,"content":"<article>arXiv:2502.10110v1 Announce Type: new \nAbstract: With the rise of sophisticated scam websites that exploit human psychological vulnerabilities, distinguishing between legitimate and scam websites has become increasingly challenging. This paper presents ScamFerret, an innovative agent system employing a large language model (LLM) to autonomously collect and analyze data from a given URL to determine whether it is a scam. Unlike traditional machine learning models that require large datasets and feature engineering, ScamFerret leverages LLMs' natural language understanding to accurately identify scam websites of various types and languages without requiring additional training or fine-tuning. Our evaluation demonstrated that ScamFerret achieves 0.972 accuracy in classifying four scam types in English and 0.993 accuracy in classifying online shopping websites across three different languages, particularly when using GPT-4. Furthermore, we confirmed that ScamFerret collects and analyzes external information such as web content, DNS records, and user reviews as necessary, providing a basis for identifying scam websites from multiple perspectives. These results suggest that LLMs have significant potential in enhancing cybersecurity measures against sophisticated scam websites.</article>","contentLength":1291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NeuroXVocal: Detection and Explanation of Alzheimer's Disease through Non-invasive Analysis of Picture-prompted Speech","url":"https://arxiv.org/abs/2502.10108","date":1739768400,"author":"","guid":1366,"unread":true,"content":"<article>arXiv:2502.10108v1 Announce Type: new \nAbstract: The early diagnosis of Alzheimer's Disease (AD) through non invasive methods remains a significant healthcare challenge. We present NeuroXVocal, a novel dual-component system that not only classifies but also explains potential AD cases through speech analysis. The classification component (Neuro) processes three distinct data streams: acoustic features capturing speech patterns and voice characteristics, textual features extracted from speech transcriptions, and precomputed embeddings representing linguistic patterns. These streams are fused through a custom transformer-based architecture that enables robust cross-modal interactions. The explainability component (XVocal) implements a Retrieval-Augmented Generation (RAG) approach, leveraging Large Language Models combined with a domain-specific knowledge base of AD research literature. This architecture enables XVocal to retrieve relevant clinical studies and research findings to generate evidence-based context-sensitive explanations of the acoustic and linguistic markers identified in patient speech. Using the IS2021 ADReSSo Challenge benchmark dataset, our system achieved state-of-the-art performance with 95.77% accuracy in AD classification, significantly outperforming previous approaches. The explainability component was qualitatively evaluated using a structured questionnaire completed by medical professionals, validating its clinical relevance. NeuroXVocal's unique combination of high-accuracy classification and interpretable, literature-grounded explanations demonstrates its potential as a practical tool for supporting clinical AD diagnosis.</article>","contentLength":1674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Adaptive Low-Rank Sparse Subspace Clustering","url":"https://arxiv.org/abs/2502.10106","date":1739768400,"author":"","guid":1367,"unread":true,"content":"<article>arXiv:2502.10106v1 Announce Type: new \nAbstract: Low-rank sparse subspace clustering (LRSSC) algorithms built on self-expressive model effectively capture both the global and local structure of the data. However, existing solutions, primarily based on proximal operators associated with Sp/Lp , p e {0, 1/2, 2/3, 1}, norms are not data-adaptive. In this work, we propose an LRSSC algorithm incorporating a data-adaptive surrogate for the S0/L0 quasi-norm. We provide a numerical solution for the corresponding proximal operator in cases where an analytical expression is unavailable. The proposed LRSSC algorithm is formulated within the proximal mapping framework, and we present theoretical proof of its global convergence toward a stationary point. We evaluate the performance of the proposed method on three well known datasets, comparing it against LRSSC algorithms constrained by Sp/Lp, p e {0, 1/2, 2/3, 1}, norms.</article>","contentLength":921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Membership and Conjugacy in Inverse Semigroups","url":"https://arxiv.org/abs/2502.10103","date":1739768400,"author":"","guid":1368,"unread":true,"content":"<article>arXiv:2502.10103v1 Announce Type: new \nAbstract: The membership problem for an algebraic structure asks whether a given element is contained in some substructure, which is usually given by generators. In this work we study the membership problem, as well as the conjugacy problem, for finite inverse semigroups. The closely related membership problem for finite semigroups has been shown to be PSPACE-complete in the transformation model by Kozen (1977) and NL-complete in the Cayley table model by Jones, Lien, and Laaser (1976). More recently, both the membership and the conjugacy problem for finite inverse semigroups were shown to be PSPACE-complete in the partial bijection model by Jack (2023).\n  Here we present a more detailed analysis of the complexity of the membership and conjugacy problems parametrized by varieties of finite inverse semigroups. We establish dichotomy theorems for the partial bijection model and for the Cayley table model. In the partial bijection model these problems are in NC (resp. NP for conjugacy) for strict inverse semigroups and PSPACE-complete otherwise. In the Cayley table model we obtain general LOGSPACE-algorithms as well as NPOLYLOGTIME upper bounds for Clifford semigroups and LOGSPACE-completeness otherwise.\n  Furthermore, by applying our findings, we show the following: the intersection non-emptiness problem for inverse automata is PSPACE-complete even for automata with only two states; the subpower membership problem is in NC for every strict inverse semi-group and PSPACE-complete otherwise; the minimum generating set and the equation satisfiability problems are in NP for varieties of finite strict inverse semigroups and PSPACE-complete otherwise.</article>","contentLength":1709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Statistical data analysis for Tourism in Poland in R Programming Environment","url":"https://arxiv.org/abs/2502.10100","date":1739768400,"author":"","guid":1369,"unread":true,"content":"<article>arXiv:2502.10100v1 Announce Type: new \nAbstract: This study utilises the R programming language for statistical data analysis to understand Tourism dynamics in Poland. It focuses on methods for data visualisation, multivariate statistics, and hypothesis testing. To investigate the expenditure behavior of tourist, spending patterns, correlations, and associations among variables were analysed in the dataset. The results revealed a significant relationship between accommodation type and the purpose of trip, showing that the purpose of a trip impacts the selection of accommodation. A strong correlation was observed between organizer expenditure and private expenditure, indicating that individual spending are more when the spending on organizing the trip are higher. However, no significant difference was observed in total expenditure across different accommodation types and purpose of the trip revealing that travelers tend to spend similar amounts regardless of their reason for travel or choice of accommodation. Although significant relationships were observed among certain variables, ANOVA could not be applied because the dataset was not able to hold on the normality assumption. In future, the dataset can be explored further to find more meaningful insights. The developed code is available on GitHub: https://github.com/SaadAhmedJamal/DataAnalysis RProgEnv.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Causal Information Prioritization for Efficient Reinforcement Learning","url":"https://arxiv.org/abs/2502.10097","date":1739768400,"author":"","guid":1370,"unread":true,"content":"<article>arXiv:2502.10097v1 Announce Type: new \nAbstract: Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. To fully assess the effectiveness of CIP, we conduct extensive experiments across 39 tasks in 5 diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.</article>","contentLength":1565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Representation Learning on Out of Distribution in Tabular Data","url":"https://arxiv.org/abs/2502.10095","date":1739768400,"author":"","guid":1371,"unread":true,"content":"<article>arXiv:2502.10095v1 Announce Type: new \nAbstract: The open-world assumption in model development suggests that a model might lack sufficient information to adequately handle data that is entirely distinct or out of distribution (OOD). While deep learning methods have shown promising results in handling OOD data through generalization techniques, they often require specialized hardware that may not be accessible to all users. We present TCL, a lightweight yet effective solution that operates efficiently on standard CPU hardware. Our approach adapts contrastive learning principles specifically for tabular data structures, incorporating full matrix augmentation and simplified loss calculation. Through comprehensive experiments across 10 diverse datasets, we demonstrate that TCL outperforms existing models, including FT-Transformer and ResNet, particularly in classification tasks, while maintaining competitive performance in regression problems. TCL achieves these results with significantly reduced computational requirements, making it accessible to users with limited hardware capabilities. This study also provides practical guidance for detecting and evaluating OOD data through straightforward experiments and visualizations. Our findings show that TCL offers a promising balance between performance and efficiency in handling OOD prediction tasks, which is particularly beneficial for general machine learning practitioners working with computational constraints.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A novel approach to data generation in generative model","url":"https://arxiv.org/abs/2502.10092","date":1739768400,"author":"","guid":1372,"unread":true,"content":"<article>arXiv:2502.10092v1 Announce Type: new \nAbstract: Variational Autoencoders (VAEs) and other generative models are widely employed in artificial intelligence to synthesize new data. However, current approaches rely on Euclidean geometric assumptions and statistical approximations that fail to capture the structured and emergent nature of data generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory, a novel geometric framework that redefines data generation by integrating dimensional expansion accompanied by qualitative transformation. By modifying the latent space geometry to interact with emergent high-dimensional structures, CFP theory addresses key challenges such as identifiability issues and unintended artifacts like hallucinations in Large Language Models (LLMs). CFP theory is based on two key conceptual hypotheses that redefine how generative models structure relationships between data and algorithms. Through the lens of CFP theory, we critically examine existing metric-learning approaches. CFP theory advances this perspective by introducing time-reversed metric embeddings and structural convergence mechanisms, leading to a novel geometric approach that better accounts for data generation as a structured epistemic process. Beyond its computational implications, CFP theory provides philosophical insights into the ontological underpinnings of data generation. By offering a systematic framework for high-dimensional learning dynamics, CFP theory contributes to establishing a theoretical foundation for understanding the data-relationship structures in AI. Finally, future research in CFP theory will be led to its implications for fully realizing qualitative transformations, introducing the potential of Hilbert space in generative modeling.</article>","contentLength":1790,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ELAA-ISAC: Environmental Mapping Utilizing the LoS State of Communication Channel","url":"https://arxiv.org/abs/2502.10091","date":1739768400,"author":"","guid":1373,"unread":true,"content":"<article>arXiv:2502.10091v1 Announce Type: new \nAbstract: In this paper, a novel environmental mapping method is proposed to outline the indoor layout utilizing the line-of-sight (LoS) state information of extremely large aperture array (ELAA) channels. It leverages the spatial resolution provided by ELAA and the mobile terminal (MT)'s mobility to infer the presence and location of obstacles in the environment. The LoS state estimation is formulated as a binary hypothesis testing problem, and the optimal decision rule is derived based on the likelihood ratio test. Subsequently, the theoretical error probability of LoS estimation is derived, showing close alignment with simulation results. Then, an environmental mapping method is proposed, which progressively outlines the layout by combining LoS state information from multiple MT locations. It is demonstrated that the proposed method can accurately outline the environment layout, with the mapping accuracy improving as the number of service-antennas and MT locations increases. This paper also investigates the impact of channel estimation error and non-LoS (NLoS) components on the quality of environmental mapping. The proposed method exhibits particularly promising performance in LoS dominated wireless environments characterized by high Rician K-factor. Specifically, it achieves an average intersection over union (IoU) exceeding 80% when utilizing 256 service antennas and 18 MT locations.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models","url":"https://arxiv.org/abs/2502.10090","date":1739768400,"author":"","guid":1374,"unread":true,"content":"<article>arXiv:2502.10090v1 Announce Type: new \nAbstract: Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS ACAM for Energy-Efficient Inference","url":"https://arxiv.org/abs/2502.10089","date":1739768400,"author":"","guid":1375,"unread":true,"content":"<article>arXiv:2502.10089v1 Announce Type: new \nAbstract: In recent years, the development of smart edge computing systems to process information locally is on the rise. Many near-sensor machine learning (ML) approaches have been implemented to introduce accurate and energy efficient template matching operations in resource-constrained edge sensing systems, such as wearables. To introduce novel solutions that can be viable for extreme edge cases, hybrid solutions combining conventional and emerging technologies have started to be proposed. Deep Neural Networks (DNN) optimised for edge application alongside new approaches of computing (both device and architecture -wise) could be a strong candidate in implementing edge ML solutions that aim at competitive accuracy classification while using a fraction of the power of conventional ML solutions. In this work, we are proposing a hybrid software-hardware edge classifier aimed at the extreme edge near-sensor systems. The classifier consists of two parts: (i) an optimised digital tinyML network, working as a front-end feature extractor, and (ii) a back-end RRAM-CMOS analogue content addressable memory (ACAM), working as a final stage template matching system. The combined hybrid system exhibits a competitive trade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$ and $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with 78.06$\\mu$J for the original teacher model, representing a 792-fold reduction, making it a viable solution for extreme edge applications.</article>","contentLength":1556,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Patient Acceptance of Robotic Ultrasound through Conversational Virtual Agent and Immersive Visualizations","url":"https://arxiv.org/abs/2502.10088","date":1739768400,"author":"","guid":1376,"unread":true,"content":"<article>arXiv:2502.10088v1 Announce Type: new \nAbstract: Robotic ultrasound systems can enhance medical diagnostics, but patient acceptance is a challenge. We propose a system combining an AI-powered conversational virtual agent with three mixed reality visualizations to improve trust and comfort. The virtual agent, powered by a large language model, engages in natural conversations and guides the ultrasound robot, enhancing interaction reliability. The visualizations include augmented reality, augmented virtuality, and fully immersive virtual reality, each designed to create patient-friendly experiences. A user study demonstrated significant improvements in trust and acceptance, offering valuable insights for designing mixed reality and virtual agents in autonomous medical procedures.</article>","contentLength":788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coordinated control of multiple autonomous surface vehicles: challenges and advances - a systematic review","url":"https://arxiv.org/abs/2502.10080","date":1739768400,"author":"","guid":1377,"unread":true,"content":"<article>arXiv:2502.10080v1 Announce Type: new \nAbstract: The increasing use and implementation of Autonomous Surface Vessels (ASVs) for various activities in maritime environments is expected to drive a rise in developments and research on their control. Particularly, the coordination of multiple ASVs presents novel challenges and opportunities, requiring interdisciplinary research efforts at the intersection of robotics, control theory, communication systems, and marine sciences. The wide variety of missions or objectives for which these vessels can be collectively used allows for the application and combination of different control techniques. This includes the exploration of machine learning to consider aspects previously deemed infeasible. This review provides a comprehensive exploration of coordinated ASV control while addressing critical gaps left by previous reviews. Unlike previous works, we adopt a systematic approach to ensure integrity and minimize bias in article selection. We delve into the complex world of sub-actuated ASVs with a focus on customized control strategies and the integration of machine learning techniques for increased autonomy. By synthesizing recent advances and identifying emerging trends, we offer insights that drive this field forward, providing both a comprehensive overview of state-of-the-art techniques and guidance for future research efforts.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Empowerment Gain through Causal Structure Learning in Model-Based RL","url":"https://arxiv.org/abs/2502.10077","date":1739768400,"author":"","guid":1378,"unread":true,"content":"<article>arXiv:2502.10077v1 Announce Type: new \nAbstract: In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environments, enabling efficient decision. Empowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future states and actions. We posit that empowerment coupled with causal understanding can improve controllability, while enhanced empowerment gain can further facilitate causal reasoning in MBRL. To improve learning efficiency and controllability, we propose a novel framework, Empowerment through Causal Learning (ECL), where an agent with the awareness of causal dynamics models achieves empowerment-driven exploration and optimizes its causal structure for task learning. Specifically, ECL operates by first training a causal dynamics model of the environment based on collected data. We then maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update causal dynamics model to be more controllable than dense dynamics model without causal structure. In downstream task learning, an intrinsic curiosity reward is included to balance the causality, mitigating overfitting. Importantly, ECL is method-agnostic and is capable of integrating various causal discovery methods. We evaluate ECL combined with 3 causal discovery methods across 6 environments including pixel-based tasks, demonstrating its superior performance compared to other causal MBRL methods, in terms of causal discovery, sample efficiency, and asymptotic performance.</article>","contentLength":1708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Classification of Temporal Graphs using Persistent Homology","url":"https://arxiv.org/abs/2502.10076","date":1739768400,"author":"","guid":1379,"unread":true,"content":"<article>arXiv:2502.10076v1 Announce Type: new \nAbstract: Temporal graphs effectively model dynamic systems by representing interactions as timestamped edges. However, analytical tools for temporal graphs are limited compared to static graphs. We propose a novel method for analyzing temporal graphs using Persistent Homology. Our approach leverages $\\delta$-temporal motifs (recurrent subgraphs) to capture temporal dynamics %without aggregation\n  . By evolving these motifs, we define the \\textit{average filtration} and compute PH on the associated clique complex. This method captures both local and global temporal structures and is stable with respect to reference models. We demonstrate the applicability of our approach to the temporal graph classification task. Experiments verify the effectiveness of our approach, achieving over 92\\% accuracy, with some cases reaching 100\\%. Unlike existing methods that require node classes, our approach is node class free, offering flexibility for a wide range of temporal graph analysis.</article>","contentLength":1027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthemius: Efficient & Modular Block Assembly for Concurrent Execution","url":"https://arxiv.org/abs/2502.10074","date":1739768400,"author":"","guid":1380,"unread":true,"content":"<article>arXiv:2502.10074v1 Announce Type: new \nAbstract: Many blockchains such as Ethereum execute all incoming transactions sequentially significantly limiting the potential throughput. A common approach to scale execution is parallel execution engines that fully utilize modern multi-core architectures. Parallel execution is then either done optimistically, by executing transactions in parallel and detecting conflicts on the fly, or guided, by requiring exhaustive client transaction hints and scheduling transactions accordingly.\n  However, recent studies have shown that the performance of parallel execution engines depends on the nature of the underlying workload. In fact, in some cases, only a 60% speed-up compared to sequential execution could be obtained. This is the case, as transactions that access the same resources must be executed sequentially. For example, if 10% of the transactions in a block access the same resource, the execution cannot meaningfully scale beyond 10 cores. Therefore, a single popular application can bottleneck the execution and limit the potential throughput.\n  In this paper, we introduce Anthemius, a block construction algorithm that optimizes parallel transaction execution throughput. We evaluate Anthemius exhaustively under a range of workloads, and show that Anthemius enables the underlying parallel execution engine to process over twice as many transactions.</article>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LifeSaver: Predictive Load Limit Estimation for Transport Vehicles in Hilly Areas","url":"https://arxiv.org/abs/2502.10072","date":1739768400,"author":"","guid":1381,"unread":true,"content":"<article>arXiv:2502.10072v1 Announce Type: new \nAbstract: The transportation of essential goods in moun- tainous regions faces severe logistical challenges and frequent disruptions. To mitigate these difficulties, transport companies often overload trucks, which, though cost-saving, significantly heightens the risk of accidents and mechanical failures. This paper presents the development of a device that detects over- loaded and insecurely fastened loads on trucks and commercial vehicles. Using advanced load sensors, the device offers real- time monitoring of cargo weight distribution, alerting drivers and authorities to unsafe conditions. The initial prototype utilised two basic load cells and an Arduino microcontroller. The second version was enhanced with four load cells and extended sensors. This version was tested by placing an electric golf cart onto the prototype. Various loads were then added to the cart in different orientations to assess whether the system could accurately detect improper or excessive load conditions.</article>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Topological Neural Networks over the Air","url":"https://arxiv.org/abs/2502.10070","date":1739768400,"author":"","guid":1382,"unread":true,"content":"<article>arXiv:2502.10070v1 Announce Type: new \nAbstract: Topological neural networks (TNNs) are information processing architectures that model representations from data lying over topological spaces (e.g., simplicial or cell complexes) and allow for decentralized implementation through localized communications over different neighborhoods. Existing TNN architectures have not yet been considered in realistic communication scenarios, where channel effects typically introduce disturbances such as fading and noise. This paper aims to propose a novel TNN design, operating on regular cell complexes, that performs over-the-air computation, incorporating the wireless communication model into its architecture. Specifically, during training and inference, the proposed method considers channel impairments such as fading and noise in the topological convolutional filtering operation, which takes place over different signal orders and neighborhoods. Numerical results illustrate the architecture's robustness to channel impairments during testing and the superior performance with respect to existing architectures, which are either communication-agnostic or graph-based.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bound preserving {P}oint-{A}verage-{M}oment {P}olynomi{A}l-interpreted ({PAMPA}) on polygonal meshes","url":"https://arxiv.org/abs/2502.10069","date":1739768400,"author":"","guid":1383,"unread":true,"content":"<article>arXiv:2502.10069v1 Announce Type: new \nAbstract: We present a novel discretisation strategy, strongly inspired from Roe's Active Flux scheme. It can use polygonal meshes and is provably bound preserving for scalar problems and the Euler equations. Several cases demonstrates the quality of the method, and improvements with respect to previous work of the authors. This paper is a summary of \\cite{BPPampa}.</article>","contentLength":407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proportional Clustering, the $\\beta$-Plurality Problem, and Metric Distortion","url":"https://arxiv.org/abs/2502.10068","date":1739768400,"author":"","guid":1384,"unread":true,"content":"<article>arXiv:2502.10068v1 Announce Type: new \nAbstract: We show that the proportional clustering problem using the Droop quota for $k = 1$ is equivalent to the $\\beta$-plurality problem. We also show that the Plurality Veto rule can be used to select ($\\sqrt{5} - 2$)-plurality points using only ordinal information about the metric space and resolve an open question of Kalayci et al. (AAAI 2024) by proving that $(2+\\sqrt{5})$-proportionally fair clusterings can be found using purely ordinal information.</article>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmenting Plane Straight-Line Graphs to Meet Parity Constraints","url":"https://arxiv.org/abs/2502.10066","date":1739768400,"author":"","guid":1385,"unread":true,"content":"<article>arXiv:2502.10066v1 Announce Type: new \nAbstract: Given a plane geometric graph $G$ on $n$ vertices, we want to augment it so that given parity constraints of the vertex degrees are met. In other words, given a subset $R$ of the vertices, we are interested in a plane geometric supergraph $G'$ such that exactly the vertices of $R$ have odd degree in $G'\\setminus G$. We show that the question whether such a supergraph exists can be decided in polynomial time for two interesting cases. First, when the vertices are in convex position, we present a linear-time algorithm. Building on this insight, we solve the case when $G$ is a plane geometric path in $O(n \\log n)$ time. This solves an open problem posed by Catana, Olaverri, Tejel, and Urrutia (Appl. Math. Comput. 2020).</article>","contentLength":775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hands-off Image Editing: Language-guided Editing without any Task-specific Labeling, Masking or even Training","url":"https://arxiv.org/abs/2502.10064","date":1739768400,"author":"","guid":1386,"unread":true,"content":"<article>arXiv:2502.10064v1 Announce Type: new \nAbstract: Instruction-guided image editing consists in taking an image and an instruction and deliverring that image altered according to that instruction. State-of-the-art approaches to this task suffer from the typical scaling up and domain adaptation hindrances related to supervision as they eventually resort to some kind of task-specific labelling, masking or training. We propose a novel approach that does without any such task-specific supervision and offers thus a better potential for improvement. Its assessment demonstrates that it is highly effective, achieving very competitive performance.</article>","contentLength":644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strassen Multisystolic Array Hardware Architectures","url":"https://arxiv.org/abs/2502.10063","date":1739768400,"author":"","guid":1387,"unread":true,"content":"<article>arXiv:2502.10063v1 Announce Type: new \nAbstract: While Strassen's matrix multiplication algorithm reduces the complexity of naive matrix multiplication, general-purpose hardware is not suitable for achieving the algorithm's promised theoretical speedups. This leaves the question of if it could be better exploited in custom hardware architectures designed specifically for executing the algorithm. However, there is limited prior work on this and it is not immediately clear how to derive such architectures or if they can ultimately lead to real improvements. We bridge this gap, presenting and evaluating new systolic array architectures that efficiently translate the theoretical complexity reductions of Strassen's algorithm directly into hardware resource savings. Furthermore, the architectures are multisystolic array designs that can multiply smaller matrices with higher utilization than single-systolic array designs. The proposed designs implemented on FPGA reduce DSP requirements by a factor of $1.14^r$ for $r$ implemented Strassen recursion levels, and otherwise require overall similar soft logic resources when instantiated to support matrix sizes down to 32x32 and 24x24 at 1-2 levels of Strassen recursion, respectively. We evaluate the proposed designs both in isolation and in an end-to-end machine learning accelerator compared to baseline designs and prior works, achieving state-of-the-art performance.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Bi-Level Multi-Robot Task Allocation and Learning under Uncertainty with Temporal Logic Constraints","url":"https://arxiv.org/abs/2502.10062","date":1739768400,"author":"","guid":1388,"unread":true,"content":"<article>arXiv:2502.10062v1 Announce Type: new \nAbstract: This work addresses the problem of multi-robot coordination under unknown robot transition models, ensuring that tasks specified by Time Window Temporal Logic are satisfied with user-defined probability thresholds. We present a bi-level framework that integrates (i) high-level task allocation, where tasks are assigned based on the robots' estimated task completion probabilities and expected rewards, and (ii) low-level distributed policy learning and execution, where robots independently optimize auxiliary rewards while fulfilling their assigned tasks. To handle uncertainty in robot dynamics, our approach leverages real-time task execution data to iteratively refine expected task completion probabilities and rewards, enabling adaptive task allocation without explicit robot transition models. We theoretically validate the proposed algorithm, demonstrating that the task assignments meet the desired probability thresholds with high confidence. Finally, we demonstrate the effectiveness of our framework through comprehensive simulations.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Annotating Compositionality Scores for Irish Noun Compounds is Hard Work","url":"https://arxiv.org/abs/2502.10061","date":1739768400,"author":"","guid":1389,"unread":true,"content":"<article>arXiv:2502.10061v1 Announce Type: new \nAbstract: Noun compounds constitute a challenging construction for NLP applications, given their variability in idiomaticity and interpretation. In this paper, we present an analysis of compound nouns identified in Irish text of varied domains by expert annotators, focusing on compositionality as a key feature, but also domain specificity, as well as familiarity and confidence of the annotator giving the ratings. Our findings and the discussion that ensued contributes towards a greater understanding of how these constructions appear in Irish language, and how they might be treated separately from English noun compounds.</article>","contentLength":666,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery","url":"https://arxiv.org/abs/2502.10060","date":1739768400,"author":"","guid":1390,"unread":true,"content":"<article>arXiv:2502.10060v1 Announce Type: new \nAbstract: Visual data is used in numerous different scientific workflows ranging from remote sensing to ecology. As the amount of observation data increases, the challenge is not just to make accurate predictions but also to understand the underlying mechanisms for those predictions. Good interpretation is important in scientific workflows, as it allows for better decision-making by providing insights into the data. This paper introduces an automatic way of obtaining such interpretable-by-design models, by learning programs that interleave neural networks. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution) an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create Python programs explaining visual data. Additionally, we propose two improvements: a program critic and a program simplifier to improve our method further to synthesize good programs. On three different real-world problems, DiSciPLE learns state-of-the-art programs on novel tasks with no prior literature. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control","url":"https://arxiv.org/abs/2502.10059","date":1739768400,"author":"","guid":1391,"unread":true,"content":"<article>arXiv:2502.10059v1 Announce Type: new \nAbstract: Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale. To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. During training, the reconstructed 3D scene enables scaling camera parameters from relative to absolute values, ensuring compatibility and scale consistency across diverse real-world images. In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene. To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic, coherent video generation in lower noise stages. RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. We will release our absolute-scale annotation, codes, and all checkpoints. Please see dynamic results in https://zgctroy.github.io/RealCam-I2V.</article>","contentLength":1641,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MTLM: an Innovative Language Model Training Paradigm for ASR","url":"https://arxiv.org/abs/2502.10058","date":1739768400,"author":"","guid":1392,"unread":true,"content":"<article>arXiv:2502.10058v1 Announce Type: new \nAbstract: Pre-training Transformer-based language models (LMs) on a large amount of text has proven crucial for improving automatic speech recognition (ASR) performance. Generally, traditional LMs are unidirectional and unable to access the context on the right. This paper proposes a method for training LMs that enable traditional unidirectional LMs to fully utilize left and right contexts. Compared with the unidirectional LMs, our LM facilitates ASR to transcribe hypotheses more consistently and in a more semantically unambiguous way, as it incorporates richer contextual representations. Finally, our experimental results on the LibriSpeech corpus demonstrate that our model outperforms traditional unidirectional LMs, whether n-best rescoring or shallow fusion is used as the decoding algorithm.</article>","contentLength":843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Generalized Modeling Approach to Liquid-driven Ballooning Membranes","url":"https://arxiv.org/abs/2502.10057","date":1739768400,"author":"","guid":1393,"unread":true,"content":"<article>arXiv:2502.10057v1 Announce Type: new \nAbstract: Soft robotics is advancing the use of flexible materials for adaptable robotic systems. Membrane-actuated soft robots address the limitations of traditional soft robots by using pressurized, extensible membranes to achieve stable, large deformations, yet control and state estimation remain challenging due to their complex deformation dynamics. This paper presents a novel modeling approach for liquid-driven ballooning membranes, employing an ellipsoid approximation to model shape and stretch under planar deformation. Relying solely on intrinsic feedback from pressure data and controlled liquid volume, this approach enables accurate membrane state estimation. We demonstrate the effectiveness of the proposed model for ballooning membrane-based actuators by experimental validation, obtaining the indentation depth error of $RMSE_{h_2}=0.80\\;$mm, which is $23\\%$ of the indentation range and $6.67\\%$ of the unindented actuator height range. For the force estimation, the error range is obtained to be $RMSE_{F}=0.15\\;$N which is $10\\%$ of the measured force range.</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Breaking Symmetries from a Set-Covering Perspective","url":"https://arxiv.org/abs/2502.10056","date":1739768400,"author":"","guid":1394,"unread":true,"content":"<article>arXiv:2502.10056v1 Announce Type: new \nAbstract: We formalize symmetry breaking as a set-covering problem. For the case of breaking symmetries on graphs, a permutation covers a graph if applying it to the graph yields a smaller graph in a given order. Canonical graphs are those that cannot be made smaller by any permutation. A complete symmetry break is then a set of permutations that covers all non-canonical graphs. A complete symmetry break with a minimal number of permutations can be obtained by solving an optimal set-covering problem.\n  The challenge is in the sizes of the corresponding set-covering problems and in how these can be tamed.\n  The set-covering perspective on symmetry breaking opens up a range of new opportunities deriving from decades of studies on both precise and approximate techniques for this problem.\n  Application of our approach leads to optimal LexLeader symmetry breaks for graphs of order $n\\leq 10$ as well as to partial symmetry breaks which improve on the state-of-the-art.</article>","contentLength":1015,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Polyp Counting In Full-Procedure Colonoscopy Videos","url":"https://arxiv.org/abs/2502.10054","date":1739768400,"author":"","guid":1395,"unread":true,"content":"<article>arXiv:2502.10054v1 Announce Type: new \nAbstract: Automated colonoscopy reporting holds great potential for enhancing quality control and improving cost-effectiveness of colonoscopy procedures. A major challenge lies in the automated identification, tracking, and re-association (ReID) of polyps tracklets across full-procedure colonoscopy videos. This is essential for precise polyp counting and enables automated computation of key quality metrics, such as Adenoma Detection Rate (ADR) and Polyps Per Colonoscopy (PPC). However, polyp ReID is challenging due to variations in polyp appearance, frequent disappearance from the field of view, and occlusions. In this work, we leverage the REAL-Colon dataset, the first open-access dataset providing full-procedure videos, to define tasks, data splits and metrics for the problem of automatically count polyps in full-procedure videos, establishing an open-access framework. We re-implement previously proposed SimCLR-based methods for learning representations of polyp tracklets, both single-frame and multi-view, and adapt them to the polyp counting task. We then propose an Affinity Propagation-based clustering method to further improve ReID based on these learned representations, ultimately enhancing polyp counting. Our approach achieves state-of-the-art performance, with a polyp fragmentation rate of 6.30 and a false positive rate (FPR) below 5% on the REAL-Colon dataset. We release code at https://github.com/lparolari/towards-polyp-counting.</article>","contentLength":1502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ORI: O Routing Intelligence","url":"https://arxiv.org/abs/2502.10051","date":1739768400,"author":"","guid":1396,"unread":true,"content":"<article>arXiv:2502.10051v1 Announce Type: new \nAbstract: Single large language models (LLMs) often fall short when faced with the ever-growing range of tasks, making a single-model approach insufficient. We address this challenge by proposing ORI (O Routing Intelligence), a dynamic framework that leverages a set of LLMs. By intelligently routing incoming queries to the most suitable model, ORI not only improves task-specific accuracy, but also maintains efficiency. Comprehensive evaluations across diverse benchmarks demonstrate consistent accuracy gains while controlling computational overhead. By intelligently routing queries, ORI outperforms the strongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR, ties the top performance on ARC, and on BBH. These results underscore the benefits of a multi-model strategy and demonstrate how ORI's adaptive architecture can more effectively handle diverse tasks, offering a scalable, high-performance solution for a system of multiple large language models.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on LLM-powered Agents for Recommender Systems","url":"https://arxiv.org/abs/2502.10050","date":1739768400,"author":"","guid":1397,"unread":true,"content":"<article>arXiv:2502.10050v1 Announce Type: new \nAbstract: Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.</article>","contentLength":1599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Janus: Collaborative Vision Transformer Under Dynamic Network Environment","url":"https://arxiv.org/abs/2502.10047","date":1739768400,"author":"","guid":1398,"unread":true,"content":"<article>arXiv:2502.10047v1 Announce Type: new \nAbstract: Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Network architectures and achieved state-of-the-art results in various computer vision tasks. Since ViTs are computationally expensive, the models either have to be pruned to run on resource-limited edge devices only or have to be executed on remote cloud servers after receiving the raw data transmitted over fluctuating networks. The resulting degraded performance or high latency all hinder their widespread applications. In this paper, we present Janus, the first framework for low-latency cloud-device collaborative Vision Transformer inference over dynamic networks. Janus overcomes the intrinsic model limitations of ViTs and realizes collaboratively executing ViT models on both cloud and edge devices, achieving low latency, high accuracy, and low communication overhead. Specifically, Janus judiciously combines token pruning techniques with a carefully designed fine-to-coarse model splitting policy and non-static mixed pruning policy. It attains a balance between accuracy and latency by dynamically selecting the optimal pruning level and split point. Experimental results across various tasks demonstrate that Janus enhances throughput by up to 5.15 times and reduces latency violation ratios by up to 98.7% when compared with baseline approaches under various network environments.</article>","contentLength":1424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in Arbitrary Virtual Environments","url":"https://arxiv.org/abs/2502.10046","date":1739768400,"author":"","guid":1399,"unread":true,"content":"<article>arXiv:2502.10046v1 Announce Type: new \nAbstract: Creating lifelike virtual agents capable of interacting with their environments is a longstanding goal in computer graphics. This paper addresses the challenge of generating natural head rotations, a critical aspect of believable agent behavior for visual information gathering and dynamic responses to environmental cues. Although earlier methods have made significant strides, many rely on data-driven or saliency-based approaches, which often underperform in diverse settings and fail to capture deeper cognitive factors such as risk assessment, information seeking, and contextual prioritization. Consequently, generated behaviors can appear rigid or overlook critical scene elements, thereby diminishing the sense of realism. In this paper, we propose \\textbf{ViRAC}, a \\textbf{Vi}sion-\\textbf{R}easoning \\textbf{A}gent Head Movement \\textbf{C}ontrol framework, which exploits the common-sense knowledge and reasoning capabilities of large-scale models, including Vision-Language Models (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling every cognitive mechanism, ViRAC leverages the biases and patterns internalized by these models from extensive training, thus emulating human-like perceptual processes without hand-tuned heuristics. Experimental results in multiple scenarios reveal that ViRAC produces more natural and context-aware head rotations than recent state-of-the-art techniques. Quantitative evaluations show a closer alignment with real human head-movement data, while user studies confirm improved realism and cognitive plausibility.</article>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unsupervised Entity Alignment Based on Personalized Discriminative Rooted Tree","url":"https://arxiv.org/abs/2502.10044","date":1739768400,"author":"","guid":1400,"unread":true,"content":"<article>arXiv:2502.10044v1 Announce Type: new \nAbstract: Entity Alignment (EA) is to link potential equivalent entities across different knowledge graphs (KGs). Most existing EA methods are supervised as they require the supervision of seed alignments, i.e., manually specified aligned entity pairs. Very recently, several EA studies have made some attempts to get rid of seed alignments. Despite achieving preliminary progress, they still suffer two limitations: (1) The entity embeddings produced by their GNN-like encoders lack personalization since some of the aggregation subpaths are shared between different entities. (2) They cannot fully alleviate the distribution distortion issue between candidate KGs due to the absence of the supervised signal. In this work, we propose a novel unsupervised entity alignment approach called UNEA to address the above two issues. First, we parametrically sample a tree neighborhood rooted at each entity, and accordingly develop a tree attention aggregation mechanism to extract a personalized embedding for each entity. Second, we introduce an auxiliary task of maximizing the mutual information between the input and the output of the KG encoder, to regularize the model and prevent the distribution distortion. Extensive experiments show that our UNEA achieves a new state-of-the-art for the unsupervised EA task, and can even outperform many existing supervised EA baselines.</article>","contentLength":1416,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Law Tradeoff Between Throughput and Sensing Distance in Large ISAC Networks","url":"https://arxiv.org/abs/2502.10042","date":1739768400,"author":"","guid":1401,"unread":true,"content":"<article>arXiv:2502.10042v1 Announce Type: new \nAbstract: In this paper, we investigate the fundamental tradeoff between communication and sensing performance of \\emph{ad hoc} integrated sensing and communication (ISAC) wireless networks. Specifically, we consider that $n$ nodes are randomly located in an extended network with area $n$ and transmit ISAC signals. Under the pure path loss channel gain model and the condition that the transmission power scales according to the communication distance, we fully characterize the optimal scaling law tradeoff between throughput and sensing distance by proposing an achievable scheme and proving its converse. Our results can be interpreted as follows: by reducing the throughput by a factor of a function of $n$, the sensing range order improves according to the same function of $n$, raised to the power of the ratio between the path loss factors in communication and sensing. We prove that the same result also holds true for ISAC networks with random fading, despite the uncertainty on the connectivity and power level created by random fading. In addition, we show that the scaling law tradeoff cannot be improved by allowing the transmission power and communication distance to scale freely. To the best of our knowledge, this is the first work formally formulating and characterizing the communication and sensing performance scaling law tradeoff of \\emph{ad hoc} ISAC networks.</article>","contentLength":1424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation","url":"https://arxiv.org/abs/2502.10040","date":1739768400,"author":"","guid":1402,"unread":true,"content":"<article>arXiv:2502.10040v1 Announce Type: new \nAbstract: Recently, Vision-Language-Action models (VLA) have advanced robot imitation learning, but high data collection costs and limited demonstrations hinder generalization and current imitation learning methods struggle in out-of-distribution scenarios, especially for long-horizon tasks. A key challenge is how to mitigate compounding errors in imitation learning, which lead to cascading failures over extended trajectories. To address these challenges, we propose the Diffusion Trajectory-guided Policy (DTP) framework, which generates 2D trajectories through a diffusion model to guide policy learning for long-horizon tasks. By leveraging task-relevant trajectories, DTP provides trajectory-level guidance to reduce error accumulation. Our two-stage approach first trains a generative vision-language model to create diffusion-based trajectories, then refines the imitation policy using them. Experiments on the CALVIN benchmark show that DTP outperforms state-of-the-art baselines by 25% in success rate, starting from scratch without external pretraining. Moreover, DTP significantly improves real-world robot performance.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning","url":"https://arxiv.org/abs/2502.10038","date":1739768400,"author":"","guid":1403,"unread":true,"content":"<article>arXiv:2502.10038v1 Announce Type: new \nAbstract: POI representation learning plays a crucial role in handling tasks related to user mobility data. Recent studies have shown that enriching POI representations with multimodal information can significantly enhance their task performance. Previously, the textual information incorporated into POI representations typically involved only POI categories or check-in content, leading to relatively weak textual features in existing methods. In contrast, large language models (LLMs) trained on extensive text data have been found to possess rich textual knowledge. However leveraging such knowledge to enhance POI representation learning presents two key challenges: first, how to extract POI-related knowledge from LLMs effectively, and second, how to integrate the extracted information to enhance POI representations. To address these challenges, we propose POI-Enhancer, a portable framework that leverages LLMs to improve POI representations produced by classic POI learning models. We first design three specialized prompts to extract semantic information from LLMs efficiently. Then, the Dual Feature Alignment module enhances the quality of the extracted information, while the Semantic Feature Fusion module preserves its integrity. The Cross Attention Fusion module then fully adaptively integrates such high-quality information into POI representations and Multi-View Contrastive Learning further injects human-understandable semantic information into these representations. Extensive experiments on three real-world datasets demonstrate the effectiveness of our framework, showing significant improvements across all baseline representations.</article>","contentLength":1698,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automation Bias in the AI Act: On the Legal Implications of Attempting to De-Bias Human Oversight of AI","url":"https://arxiv.org/abs/2502.10036","date":1739768400,"author":"","guid":1404,"unread":true,"content":"<article>arXiv:2502.10036v1 Announce Type: new \nAbstract: This paper examines the legal implications of the explicit mentioning of automation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates human oversight for high-risk AI systems and requires providers to enable awareness of AB, i.e., the tendency to over-rely on AI outputs. The paper analyses how this extra-juridical concept is embedded in the AIA, the division of responsibility between AI providers and deployers, and the challenges of legally enforcing this novel awareness requirement. The analysis shows that the AIA's focus on providers does not adequately address design and context as causes of AB, and questions whether the AIA should directly regulate the risk of AB rather than just mandating awareness. As the AIA's approach requires a balance between legal mandates and behavioural science, the paper proposes that harmonised standards should reference the state of research on AB and human-AI interaction. Ultimately, further empirical research will be essential for effective safeguards.</article>","contentLength":1069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Phi-FEM-FNO: a new approach to train a Neural Operator as a fast PDE solver for variable geometries","url":"https://arxiv.org/abs/2502.10033","date":1739768400,"author":"","guid":1405,"unread":true,"content":"<article>arXiv:2502.10033v1 Announce Type: new \nAbstract: In this paper, we propose a way to solve partial differential equations (PDEs) by combining machine learning techniques and the finite element method called Phi-FEM. For that, we use the Fourier Neural Operator (FNO), a learning mapping operator. The purpose of this paper is to provide numerical evidence to show the effectiveness of this technique. We will focus here on the resolution of two equations: the Poisson-Dirichlet equation and the non-linear elasticity equations. The key idea of our method is to address the challenging scenario of varying domains, where each problem is solved on a different geometry. The considered domains are defined by level-set functions due to the use of the Phi-FEM approach. We will first recall the idea of $\\varphi$-FEM and of the Fourier Neural Operator. Then, we will explain how to combine these two methods. We will finally illustrate the efficiency of this combination with some numerical results on three test cases. In addition, in the last test case, we propose a new numerical scheme for hyperelastic materials following the Phi-FEM paradigm.</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation","url":"https://arxiv.org/abs/2502.10028","date":1739768400,"author":"","guid":1406,"unread":true,"content":"<article>arXiv:2502.10028v1 Announce Type: new \nAbstract: Language-conditioned manipulation is a vital but challenging robotic task due to the high-level abstraction of language. To address this, researchers have sought improved goal representations derived from natural language. In this paper, we highlight 3D flow - representing the motion trend of 3D particles within a scene - as an effective bridge between language-based future image generation and fine-grained action prediction. To this end, we develop ManiTrend, a unified framework that models the dynamics of 3D particles, vision observations and manipulation actions with a causal transformer. Within this framework, features for 3D flow prediction serve as additional conditions for future image generation and action prediction, alleviating the complexity of pixel-wise spatiotemporal modeling and providing seamless action guidance. Furthermore, 3D flow can substitute missing or heterogeneous action labels during large-scale pretraining on cross-embodiment demonstrations. Experiments on two comprehensive benchmarks demonstrate that our method achieves state-of-the-art performance with high efficiency. Our code and model checkpoints will be available upon acceptance.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Heterogeneous Resource Allocation with Multi-task Learning for Wireless Networks","url":"https://arxiv.org/abs/2502.10027","date":1739768400,"author":"","guid":1407,"unread":true,"content":"<article>arXiv:2502.10027v1 Announce Type: new \nAbstract: The optimal solution to an optimization problem depends on the problem's objective function, constraints, and size. While deep neural networks (DNNs) have proven effective in solving optimization problems, changes in the problem's size, objectives, or constraints often require adjustments to the DNN architecture to maintain effectiveness, or even retraining a new DNN from scratch. Given the dynamic nature of wireless networks, which involve multiple and diverse objectives that can have conflicting requirements and constraints, we propose a multi-task learning (MTL) framework to enable a single DNN to jointly solve a range of diverse optimization problems. In this framework, optimization problems with varying dimensionality values, objectives, and constraints are treated as distinct tasks. To jointly address these tasks, we propose a conditional computation-based MTL approach with routing. The multi-task DNN consists of two components, the base DNN (bDNN), which is the single DNN used to extract the solutions for all considered optimization problems, and the routing DNN (rDNN), which manages which nodes and layers of the bDNN to be used during the forward propagation of each task. The output of the rDNN is a binary vector which is multiplied with all bDNN's weights during the forward propagation, creating a unique computational path through the bDNN for each task. This setup allows the tasks to either share parameters or use independent ones, with the decision controlled by the rDNN. The proposed framework supports both supervised and unsupervised learning scenarios. Numerical results demonstrate the efficiency of the proposed MTL approach in solving diverse optimization problems. In contrast, benchmark DNNs lacking the rDNN mechanism were unable to achieve similar levels of performance, highlighting the effectiveness of the proposed architecture.</article>","contentLength":1927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is a Feature, Really? Toward a Unified Understanding Across SE Disciplines","url":"https://arxiv.org/abs/2502.10025","date":1739768400,"author":"","guid":1408,"unread":true,"content":"<article>arXiv:2502.10025v1 Announce Type: new \nAbstract: In software engineering, the concept of a ``feature'' is widely used but inconsistently defined across disciplines such as requirements engineering (RE) and software product lines (SPL). This lack of consistency often results in communication gaps, rework, and inefficiencies in projects. To address these challenges, this paper proposes an empirical, data-driven approach to explore how features are described, implemented, and managed across real-world projects, starting with open-source software (OSS). By analyzing feature-related branches in OSS repositories, we identify patterns in contributor behavior, feature implementation, and project management activities. Our findings provide actionable insights to improve project planning, resource allocation, and team coordination. Additionally, we outline a roadmap to unify the understanding of features across software engineering disciplines. This research aims to bridge gaps between academic inquiry and practical strategies, fostering better feature planning and development workflows in diverse project environments.</article>","contentLength":1126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Differential Equation Approach to the Most-Informative Boolean Function Conjecture","url":"https://arxiv.org/abs/2502.10019","date":1739768400,"author":"","guid":1409,"unread":true,"content":"<article>arXiv:2502.10019v1 Announce Type: new \nAbstract: We study the most-informative Boolean function conjecture using a differential equation approach. This leads to a formulation of a functional inequality on finite-dimensional random variables. We also develop a similar inequality in the case of the Hellinger conjecture. Finally, we conjecture a specific finite dimensional inequality that, if proved, will lead to a proof of the Boolean function conjecture in the balanced case. We further show that the above inequality holds modulo four explicit inequalities (all of which seems to hold via numerical simulation) with the first three containing just two variables and a final one involving four variables.</article>","contentLength":707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recovering nonlinear dynamics from non-uniform observations: A physics-based identification approach with practical case studies","url":"https://arxiv.org/abs/2502.10014","date":1739768400,"author":"","guid":1410,"unread":true,"content":"<article>arXiv:2502.10014v1 Announce Type: new \nAbstract: Uniform and smooth data collection is often infeasible in real-world scenarios. In this paper, we propose an identification framework to effectively handle the so-called non-uniform observations, i.e., data scenarios that include missing measurements, multiple runs, or aggregated observations. The goal is to provide a general approach for accurately recovering the overall dynamics of possibly nonlinear systems, allowing the capture of the system behavior over time from non-uniform observations. The proposed approach exploits prior knowledge by integrating domain-specific, interpretable, physical principles with black-box approximators, proving significant flexibility and adaptability in handling different types of non-uniform measurements, and addressing the limitations of traditional linear and black-box methods. The description of this novel framework is supported by a theoretical study on the effect of non-uniform observations on the accuracy of parameter estimation. Specifically, we demonstrate the existence of upper bounds on the parametric error resulting from missing measurements and aggregated observations. Then, the effectiveness of the approach is demonstrated through two case studies. These include a practical application with missing samples, i.e., the identification of a continuous stirred-tank reactor using real data, and a simulated Lotka-Volterra system under aggregated observations. The results highlight the ability of the framework to robustly estimate the system parameters and to accurately reconstruct the model dynamics despite the availability of non-uniform measurements.</article>","contentLength":1668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probabilistic Lexical Manifold Construction in Large Language Models via Hierarchical Vector Field Interpolation","url":"https://arxiv.org/abs/2502.10013","date":1739768400,"author":"","guid":1411,"unread":true,"content":"<article>arXiv:2502.10013v1 Announce Type: new \nAbstract: Hierarchical vector field interpolation introduces a structured probabilistic framework for lexical representation, ensuring that word embeddings transition smoothly across a continuous manifold rather than being constrained to discrete token mappings. The proposed methodology constructs a probabilistic function space where word representations adhere to topological consistency, mitigating representational discontinuities commonly observed in transformer-based embeddings. Empirical evaluations reveal that probabilistic constraints enhance lexical coherence by refining contextual relationships, leading to improvements in semantic stability across multiple linguistic distributions. The application of divergence minimization techniques ensures that interpolated embeddings maintain probabilistic consistency while preserving computational feasibility for large-scale implementations. Experimental findings demonstrate that interpolated lexical manifolds improve representation density alignment, reducing anisotropic distortions in contextual embedding distributions. Comparative analyses with standard transformer-based models highlight that structured interpolation yields more stable representations, particularly in tasks requiring fine-grained semantic differentiation. The statistical evaluation of embedding divergence confirms that probabilistic lexical manifolds reduce representational inconsistencies while maintaining coherence across varying scales of contextual abstraction. An assessment of computational efficiency reveals that while interpolation introduces minor processing overhead, the structured representation learning approach remains scalable for practical deployment.</article>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dream to Drive: Model-Based Vehicle Control Using Analytic World Models","url":"https://arxiv.org/abs/2502.10012","date":1739768400,"author":"","guid":1412,"unread":true,"content":"<article>arXiv:2502.10012v1 Announce Type: new \nAbstract: Differentiable simulators have recently shown great promise for training autonomous vehicle controllers. Being able to backpropagate through them, they can be placed into an end-to-end training loop where their known dynamics turn into useful priors for the policy to learn, removing the typical black box assumption of the environment. So far, these systems have only been used to train policies. However, this is not the end of the story in terms of what they can offer. Here, for the first time, we use them to train world models. Specifically, we present three new task setups that allow us to learn next state predictors, optimal planners, and optimal inverse states. Unlike analytic policy gradients (APG), which requires the gradient of the next simulator state with respect to the current actions, our proposed setups rely on the gradient of the next state with respect to the current state. We call this approach Analytic World Models (AWMs) and showcase its applications, including how to use it for planning in the Waymax simulator. Apart from pushing the limits of what is possible with such simulators, we offer an improved training recipe that increases performance on the large-scale Waymo Open Motion dataset by up to 12% compared to baselines at essentially no additional cost.</article>","contentLength":1343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InterGridNet: An Electric Network Frequency Approach for Audio Source Location Classification Using Convolutional Neural Networks","url":"https://arxiv.org/abs/2502.10011","date":1739768400,"author":"","guid":1413,"unread":true,"content":"<article>arXiv:2502.10011v1 Announce Type: new \nAbstract: A novel framework, called InterGridNet, is introduced, leveraging a shallow RawNet model for geolocation classification of Electric Network Frequency (ENF) signatures in the SP Cup 2016 dataset. During data preparation, recordings are sorted into audio and power groups based on inherent characteristics, further divided into 50 Hz and 60 Hz groups via spectrogram analysis. Residual blocks within the classification model extract frame-level embeddings, aiding decision-making through softmax activation. The topology and the hyperparameters of the shallow RawNet are optimized using a Neural Architecture Search. The overall accuracy of InterGridNet in the test recordings is 92%, indicating its effectiveness against the state-of-the-art methods tested in the SP Cup 2016. These findings underscore InterGridNet's effectiveness in accurately classifying audio recordings from diverse power grids, advancing state-of-the-art geolocation estimation methods.</article>","contentLength":1007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discovering Polynomial and Quadratic Structure in Nonlinear Ordinary Differential Equations","url":"https://arxiv.org/abs/2502.10005","date":1739768400,"author":"","guid":1414,"unread":true,"content":"<article>arXiv:2502.10005v1 Announce Type: new \nAbstract: Dynamical systems with quadratic or polynomial drift exhibit complex dynamics, yet compared to nonlinear systems in general form, are often easier to analyze, simulate, control, and learn. Results going back over a century have shown that the majority of nonpolynomial nonlinear systems can be recast in polynomial form, and their degree can be reduced further to quadratic. This process of polynomialization/quadratization reveals new variables (in most cases, additional variables have to be added to achieve this) in which the system dynamics adhere to that specific form, which leads us to discover new structures of a model. This chapter summarizes the state of the art for the discovery of polynomial and quadratic representations of finite-dimensional dynamical systems. We review known existence results, discuss the two prevalent algorithms for automating the discovery process, and give examples in form of a single-layer neural network and a phenomenological model of cell signaling.</article>","contentLength":1043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SciClaimHunt: A Large Dataset for Evidence-based Scientific Claim Verification","url":"https://arxiv.org/abs/2502.10003","date":1739768400,"author":"","guid":1415,"unread":true,"content":"<article>arXiv:2502.10003v1 Announce Type: new \nAbstract: Verifying scientific claims presents a significantly greater challenge than verifying political or news-related claims. Unlike the relatively broad audience for political claims, the users of scientific claim verification systems can vary widely, ranging from researchers testing specific hypotheses to everyday users seeking information on a medication. Additionally, the evidence for scientific claims is often highly complex, involving technical terminology and intricate domain-specific concepts that require specialized models for accurate verification. Despite considerable interest from the research community, there is a noticeable lack of large-scale scientific claim verification datasets to benchmark and train effective models. To bridge this gap, we introduce two large-scale datasets, SciClaimHunt and SciClaimHunt_Num, derived from scientific research papers. We propose several baseline models tailored for scientific claim verification to assess the effectiveness of these datasets. Additionally, we evaluate models trained on SciClaimHunt and SciClaimHunt_Num against existing scientific claim verification datasets to gauge their quality and reliability. Furthermore, we conduct human evaluations of the claims in proposed datasets and perform error analysis to assess the effectiveness of the proposed baseline models. Our findings indicate that SciClaimHunt and SciClaimHunt_Num serve as highly reliable resources for training models in scientific claim verification.</article>","contentLength":1537,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EmbBERT-Q: Breaking Memory Barriers in Embedded NLP","url":"https://arxiv.org/abs/2502.10001","date":1739768400,"author":"","guid":1416,"unread":true,"content":"<article>arXiv:2502.10001v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computational demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781 kB, representing a 25x reduction in size with respect to SotA models. By combining architectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2 MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA. Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with respect to existing approaches, achieving an unmatched balance between memory and performance. To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at https://github.com/RiccardoBravin/tiny-LLM.</article>","contentLength":1610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scheduling Strategies for Partially-Replicable Task Chains on Two Types of Resources","url":"https://arxiv.org/abs/2502.10000","date":1739768400,"author":"","guid":1417,"unread":true,"content":"<article>arXiv:2502.10000v1 Announce Type: new \nAbstract: The arrival of heterogeneous (or hybrid) multicore architectures on parallel platforms has brought new performance opportunities for applications and efficiency opportunities to systems. They have also increased the challenges related to thread scheduling, as tasks' execution times will vary depending if they are placed in big (performance) cores or little (efficient) ones. In this paper, we focus on the challenges heterogeneous multicore problems bring to partially-replicable task chains, such as the ones that implement digital communication standards in Software-Defined Radio (SDR). Our objective is to maximize the throughput of these task chains while also minimizing their power consumption. We model this problem as a pipelined workflow scheduling problem using pipelined and replicated parallelism on two types of resources whose objectives are to minimize the period and to use as many little cores as necessary. We propose two greedy heuristics (FERTAC and 2CATAC) and one optimal dynamic programming (HeRAD) solution to the problem. We evaluate our solutions and compare the quality of their schedules (in period and resource utilization) and their execution times using synthetic task chains and an implementation of the DVB-S2 communication standard running on StreamPU. Our results demonstrate the benefits and drawbacks of the different proposed solutions. On average, FERTAC and 2CATAC achieve near-optimal solutions, with periods that are less than 10% worse than the optimal (HeRAD) using fewer than 2 extra cores. These three scheduling strategies now enable programmers and users of StreamPU to transparently make use of heterogeneous multicore processors and achieve throughputs that differ from their theoretical maximums by less than 8% on average.</article>","contentLength":1826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decision Information Meets Large Language Models: The Future of Explainable Operations Research","url":"https://arxiv.org/abs/2502.09994","date":1739768400,"author":"","guid":1418,"unread":true,"content":"<article>arXiv:2502.09994v1 Announce Type: new \nAbstract: Operations Research (OR) is vital for decision-making in many industries. While recent OR methods have seen significant improvements in automation and efficiency through integrating Large Language Models (LLMs), they still struggle to produce meaningful explanations. This lack of clarity raises concerns about transparency and trustworthiness in OR applications. To address these challenges, we propose a comprehensive framework, Explainable Operations Research (EOR), emphasizing actionable and understandable explanations accompanying optimization. The core of EOR is the concept of Decision Information, which emerges from what-if analysis and focuses on evaluating the impact of complex constraints (or parameters) changes on decision-making. Specifically, we utilize bipartite graphs to quantify the changes in the OR model and adopt LLMs to improve the explanation capabilities. Additionally, we introduce the first industrial benchmark to rigorously evaluate the effectiveness of explanations and analyses in OR, establishing a new standard for transparency and clarity in the field.</article>","contentLength":1140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigating Label Ambiguity for Facial Expression Recognition in the Wild","url":"https://arxiv.org/abs/2502.09993","date":1739768400,"author":"","guid":1419,"unread":true,"content":"<article>arXiv:2502.09993v1 Announce Type: new \nAbstract: Facial expression recognition (FER) remains a challenging task due to label ambiguity caused by the subjective nature of facial expressions and noisy samples. Additionally, class imbalance, which is common in real-world datasets, further complicates FER. Although many studies have shown impressive improvements, they typically address only one of these issues, leading to suboptimal results. To tackle both challenges simultaneously, we propose a novel framework called Navigating Label Ambiguity (NLA), which is robust under real-world conditions. The motivation behind NLA is that dynamically estimating and emphasizing ambiguous samples at each iteration helps mitigate noise and class imbalance by reducing the model's bias toward majority classes. To achieve this, NLA consists of two main components: Noise-aware Adaptive Weighting (NAW) and consistency regularization. Specifically, NAW adaptively assigns higher importance to ambiguous samples and lower importance to noisy ones, based on the correlation between the intermediate prediction scores for the ground truth and the nearest negative. Moreover, we incorporate a regularization term to ensure consistent latent distributions. Consequently, NLA enables the model to progressively focus on more challenging ambiguous samples, which primarily belong to the minority class, in the later stages of training. Extensive experiments demonstrate that NLA outperforms existing methods in both overall and mean accuracy, confirming its robustness against noise and class imbalance. To the best of our knowledge, this is the first framework to address both problems simultaneously.</article>","contentLength":1686,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Diffusion Models","url":"https://arxiv.org/abs/2502.09992","date":1739768400,"author":"","guid":1420,"unread":true,"content":"<article>arXiv:2502.09992v1 Announce Type: new \nAbstract: Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.</article>","contentLength":1170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability","url":"https://arxiv.org/abs/2502.09990","date":1739768400,"author":"","guid":1421,"unread":true,"content":"<article>arXiv:2502.09990v1 Announce Type: new \nAbstract: Despite the rapid development of safety alignment techniques for LLMs, defending against multi-turn jailbreaks is still a challenging task. In this paper, we conduct a comprehensive comparison, revealing that some existing defense methods can improve the robustness of LLMs against multi-turn jailbreaks but compromise usability, i.e., reducing general capabilities or causing the over-refusal problem. From the perspective of mechanism interpretability of LLMs, we discover that these methods fail to establish a boundary that exactly distinguishes safe and harmful feature representations. Therefore, boundary-safe representations close to harmful representations are inevitably disrupted, leading to a decline in usability. To address this issue, we propose X-Boundary to push harmful representations away from boundary-safe representations and obtain an exact distinction boundary. In this way, harmful representations can be precisely erased without disrupting safe ones. Experimental results show that X-Boundary achieves state-of-the-art defense performance against multi-turn jailbreaks, while reducing the over-refusal rate by about 20% and maintaining nearly complete general capability. Furthermore, we theoretically prove and empirically verify that X-Boundary can accelerate the convergence process during training. Please see our code at: https://github.com/AI45Lab/X-Boundary.</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Logical Formalisation of a Hypothesis in Weighted Abduction: towards User-Feedback Dialogues","url":"https://arxiv.org/abs/2502.09989","date":1739768400,"author":"","guid":1422,"unread":true,"content":"<article>arXiv:2502.09989v1 Announce Type: new \nAbstract: Weighted abduction computes hypotheses that explain input observations. A reasoner of weighted abduction first generates possible hypotheses and then selects the hypothesis that is the most plausible. Since a reasoner employs parameters, called weights, that control its plausibility evaluation function, it can output the most plausible hypothesis according to a specific application using application-specific weights. This versatility makes it applicable from plant operation to cybersecurity or discourse analysis. However, the predetermined application-specific weights are not applicable to all cases of the application. Hence, the hypothesis selected by the reasoner does not necessarily seem the most plausible to the user. In order to resolve this problem, this article proposes two types of user-feedback dialogue protocols, in which the user points out, either positively, negatively or neutrally, properties of the hypotheses presented by the reasoner, and the reasoner regenerates hypotheses that satisfy the user's feedback. As it is required for user-feedback dialogue protocols, we then prove: (i) our protocols necessarily terminate under certain reasonable conditions; (ii) they achieve hypotheses that have the same properties in common as fixed target hypotheses do in common if the user determines the positivity, negativity or neutrality of each pointed-out property based on whether the target hypotheses have that property.</article>","contentLength":1496,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Verified error bounds for the singular values of structured matrices with applications to computer-assisted proofs for differential equations","url":"https://arxiv.org/abs/2502.09984","date":1739768400,"author":"","guid":1423,"unread":true,"content":"<article>arXiv:2502.09984v1 Announce Type: new \nAbstract: This paper introduces two methods for verifying the singular values of the structured matrix denoted by $R^{-H}AR^{-1}$, where $R$ is a nonsingular matrix and $A$ is a general nonsingular square matrix. The first of the two methods uses the computed factors from a singular value decomposition (SVD) to verify all singular values; the second estimates a lower bound of the minimum singular value without performing the SVD. The proposed approach for verifying all singular values efficiently computes tight error bounds. The method for estimating a lower bound of the minimum singular value is particularly effective for sparse matrices. These methods have proven to be efficient in verifying solutions to differential equation problems, that were previously challenging due to the extensive computational time and memory requirements.</article>","contentLength":884,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ICST Tool Competition 2025 - Self-Driving Car Testing Track","url":"https://arxiv.org/abs/2502.09982","date":1739768400,"author":"","guid":1424,"unread":true,"content":"<article>arXiv:2502.09982v1 Announce Type: new \nAbstract: This is the first edition of the tool competition on testing self-driving cars (SDCs) at the International Conference on Software Testing, Verification and Validation (ICST). The aim is to provide a platform for software testers to submit their tools addressing the test selection problem for simulation-based testing of SDCs, which is considered an emerging and vital domain. The competition provides an advanced software platform and representative case studies to ease participants' entry into SDC regression testing, enabling them to develop their initial test generation tools for SDCS. In this first edition, the competition includes five tools from different authors. All tools were evaluated using (regression) metrics for test selection as well as compared with a baseline approache. This paper provides an overview of the competition, detailing its context, framework, participating tools, evaluation methodology, and key findings.</article>","contentLength":990,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal Dependencies in Complex Data","url":"https://arxiv.org/abs/2502.09981","date":1739768400,"author":"","guid":1425,"unread":true,"content":"<article>arXiv:2502.09981v1 Announce Type: new \nAbstract: Causality in time series can be difficult to determine, especially in the presence of non-linear dependencies. The concept of Granger causality helps analyze potential relationships between variables, thereby offering a method to determine whether one time series can predict-Granger cause-future values of another. Although successful, Granger causal methods still struggle with capturing long-range relations between variables. To this end, we leverage the recently successful Extended Long Short-Term Memory (xLSTM) architecture and propose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between the time series components by using a novel dynamic lass penalty on the initial projection. Specifically, we adaptively improve the model and identify sparsity candidates. Our joint optimization procedure then ensures that the Granger causal relations are recovered in a robust fashion. Our experimental evaluations on three datasets demonstrate the overall efficacy of our proposed GC-xLSTM model.</article>","contentLength":1060,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models","url":"https://arxiv.org/abs/2502.09980","date":1739768400,"author":"","guid":1426,"unread":true,"content":"<article>arXiv:2502.09980v1 Announce Type: new \nAbstract: Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoadFed: A Multimodal Federated Learning System for Improving Road Safety","url":"https://arxiv.org/abs/2502.09978","date":1739768400,"author":"","guid":1427,"unread":true,"content":"<article>arXiv:2502.09978v1 Announce Type: new \nAbstract: Internet of Things (IoTs) have been widely applied in Collaborative Intelligent Transportation Systems (C-ITS) for the prevention of road accidents. As one of the primary causes of road accidents in C-ITS, the efficient detection and early alarm of road hazards are of paramount importance. Given the importance, extensive research has explored this topic and obtained favorable results. However, most existing solutions only explore single-modality data, struggle with high computation and communication overhead, or suffer from the curse of high dimensionality in their privacy-preserving methodologies. To overcome these obstacles, in this paper, we introduce RoadFed, an innovative and private multimodal Federated learning-based system tailored for intelligent Road hazard detection and alarm. This framework encompasses an innovative Multimodal Road Hazard Detector, a communication-efficient federated learning approach, and a customized low-error-rate local differential privacy method crafted for high dimensional multimodal data. Experimental results reveal that the proposed RoadFed surpasses most existing systems in the self-gathered real-world and CrisisMMD public datasets. In particular, RoadFed achieves an accuracy of 96.42% with a mere 0.0351 seconds of latency and its communication cost is up to 1,000 times lower than existing systems in this field. It facilitates collaborative training with non-iid high dimensional multimodal real-world data across various data modalities on multiple edges while ensuring privacy preservation for road users.</article>","contentLength":1616,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing","url":"https://arxiv.org/abs/2502.09977","date":1739768400,"author":"","guid":1428,"unread":true,"content":"<article>arXiv:2502.09977v1 Announce Type: new \nAbstract: Effectively incorporating external knowledge into Large Language Models (LLMs) is crucial for enhancing their capabilities and addressing real-world needs. Retrieval-Augmented Generation (RAG) offers an effective method for achieving this by retrieving the most relevant fragments into LLMs. However, the advancements in context window size for LLMs offer an alternative approach, raising the question of whether RAG remains necessary for effectively handling external knowledge. Several existing studies provide inconclusive comparisons between RAG and long-context (LC) LLMs, largely due to limitations in the benchmark designs. In this paper, we present LaRA, a novel benchmark specifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses 2,326 test cases across four practical QA task categories and three types of naturally occurring long texts. Through systematic evaluation of seven open-source and four proprietary LLMs, we find that the optimal choice between RAG and LC depends on a complex interplay of factors, including the model's parameter size, long-text capabilities, context length, task type, and the characteristics of the retrieved chunks. Our findings provide actionable guidelines for practitioners to effectively leverage both RAG and LC approaches in developing and deploying LLM applications. Our code and dataset is provided at: \\href{https://github.com/likuanppd/LaRA}{\\textbf{https://github.com/likuanppd/LaRA}}.</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DPM-Bench: Benchmark for Distributed Process Mining Algorithms on Cyber-Physical Systems","url":"https://arxiv.org/abs/2502.09975","date":1739768400,"author":"","guid":1429,"unread":true,"content":"<article>arXiv:2502.09975v1 Announce Type: new \nAbstract: Process Mining is established in research and industry systems to analyze and optimize processes based on event data from information systems. Within this work, we accomodate process mining techniques to Cyber-Physical Systems. To capture the distributed and heterogeneous characteristics of data, computational resources, and network communication in CPS, the todays process mining algorithms and techniques must be augmented. Specifically, there is a need for new Distributed Process Mining algorithms that enable computations to be performed directly on edge resources, eliminating the need for moving all data to central cloud systems. This paper introduces the DPM-Bench benchmark for comparing such Distributed Process Mining algorithms. DPM-Bench is used to compare algorithms deployed in different computational topologies. The results enable information system engineers to assess whether the existing infrastructure is sufficient to perform distributed process mining, or to identify required improvements in algorithms and hardware. We present and discuss an experimental evaluation with DPM-Bench.</article>","contentLength":1158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Has My System Prompt Been Used? Large Language Model Prompt Membership Inference","url":"https://arxiv.org/abs/2502.09974","date":1739768400,"author":"","guid":1430,"unread":true,"content":"<article>arXiv:2502.09974v1 Announce Type: new \nAbstract: Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of model outputs corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective for prompt membership inference. Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.</article>","contentLength":1033,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InteRecon: Towards Reconstructing Interactivity of Personal Memorable Items in Mixed Reality","url":"https://arxiv.org/abs/2502.09973","date":1739768400,"author":"","guid":1431,"unread":true,"content":"<article>arXiv:2502.09973v1 Announce Type: new \nAbstract: Digital capturing of memorable personal items is a key way to archive personal memories. Although current digitization methods (e.g., photos, videos, 3D scanning) can replicate the physical appearance of an item, they often cannot preserve its real-world interactivity. We present Interactive Digital Item (IDI), a concept of reconstructing both the physical appearance and, more importantly, the interactivity of an item. We first conducted a formative study to understand users' expectations of IDI, identifying key physical interactivity features, including geometry, interfaces, and embedded content of items. Informed by these findings, we developed InteRecon, an AR prototype enabling personal reconstruction functions for IDI creation. An exploratory study was conducted to assess the feasibility of using InteRecon and explore the potential of IDI to enrich personal memory archives. Results show that InteRecon is feasible for IDI creation, and the concept of IDI brings new opportunities for augmenting personal memory archives.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditional Latent Coding with Learnable Synthesized Reference for Deep Image Compression","url":"https://arxiv.org/abs/2502.09971","date":1739768400,"author":"","guid":1432,"unread":true,"content":"<article>arXiv:2502.09971v1 Announce Type: new \nAbstract: In this paper, we study how to synthesize a dynamic reference from an external dictionary to perform conditional coding of the input image in the latent domain and how to learn the conditional latent synthesis and coding modules in an end-to-end manner. Our approach begins by constructing a universal image feature dictionary using a multi-stage approach involving modified spatial pyramid pooling, dimension reduction, and multi-scale feature clustering. For each input image, we learn to synthesize a conditioning latent by selecting and synthesizing relevant features from the dictionary, which significantly enhances the model's capability in capturing and exploring image source correlation. This conditional latent synthesis involves a correlation-based feature matching and alignment strategy, comprising a Conditional Latent Matching (CLM) module and a Conditional Latent Synthesis (CLS) module. The synthesized latent is then used to guide the encoding process, allowing for more efficient compression by exploiting the correlation between the input image and the reference dictionary. According to our theoretical analysis, the proposed conditional latent coding (CLC) method is robust to perturbations in the external dictionary samples and the selected conditioning latent, with an error bound that scales logarithmically with the dictionary size, ensuring stability even with large and diverse dictionaries. Experimental results on benchmark datasets show that our new method improves the coding performance by a large margin (up to 1.2 dB) with a very small overhead of approximately 0.5\\% bits per pixel. Our code is publicly available at https://github.com/ydchen0806/CLC.</article>","contentLength":1738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning","url":"https://arxiv.org/abs/2502.09969","date":1739768400,"author":"","guid":1433,"unread":true,"content":"<article>arXiv:2502.09969v1 Announce Type: new \nAbstract: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VicKAM: Visual Conceptual Knowledge Guided Action Map for Weakly Supervised Group Activity Recognition","url":"https://arxiv.org/abs/2502.09967","date":1739768400,"author":"","guid":1434,"unread":true,"content":"<article>arXiv:2502.09967v1 Announce Type: new \nAbstract: Existing weakly supervised group activity recognition methods rely on object detectors or attention mechanisms to capture key areas automatically. However, they overlook the semantic information associated with captured areas, which may adversely affect the recognition performance. In this paper, we propose a novel framework named Visual Conceptual Knowledge Guided Action Map (VicKAM) which effectively captures the locations of individual actions and integrates them with action semantics for weakly supervised group activity recognition.It generates individual action prototypes from training set as visual conceptual knowledge to bridge action semantics and visual representations. Guided by this knowledge, VicKAM produces action maps that indicate the likelihood of each action occurring at various locations, based on image correlation theorem. It further augments individual action maps using group activity related statistical information, representing individual action distribution under different group activities, to establish connections between action maps and specific group activities. The augmented action map is incorporated with action semantic representations for group activity recognition.Extensive experiments on two public benchmarks, the Volleyball and the NBA datasets, demonstrate the effectiveness of our proposed method, even in cases of limited training data. The code will be released later.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating on Generated: An Approach Towards Self-Evolving Diffusion Models","url":"https://arxiv.org/abs/2502.09963","date":1739768400,"author":"","guid":1435,"unread":true,"content":"<article>arXiv:2502.09963v1 Announce Type: new \nAbstract: Recursive Self-Improvement (RSI) enables intelligence systems to autonomously refine their capabilities. This paper explores the application of RSI in text-to-image diffusion models, addressing the challenge of training collapse caused by synthetic data. We identify two key factors contributing to this collapse: the lack of perceptual alignment and the accumulation of generative hallucinations. To mitigate these issues, we propose three strategies: (1) a prompt construction and filtering pipeline designed to facilitate the generation of perceptual aligned data, (2) a preference sampling method to identify human-preferred samples and filter out generative hallucinations, and (3) a distribution-based weighting scheme to penalize selected samples with hallucinatory errors. Our extensive experiments validate the effectiveness of these approaches.</article>","contentLength":903,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strategyproof Maximum Matching under Dichotomous Agent Preferences","url":"https://arxiv.org/abs/2502.09962","date":1739768400,"author":"","guid":1436,"unread":true,"content":"<article>arXiv:2502.09962v1 Announce Type: new \nAbstract: We consider a two-sided matching problem in which the agents on one side have dichotomous preferences and the other side representing institutions has strict preferences (priorities). It captures several important applications in matching market design in which the agents are only interested in getting matched to an acceptable institution. These include centralized daycare assignment and healthcare rationing. We present a compelling new mechanism that satisfies many prominent and desirable properties including individual rationality, maximum size, fairness, Pareto-efficiency on both sides, strategyproofness on both sides, non-bossiness and having polynomial time running time. As a result, we answer an open problem whether there exists a mechanism that is agent-strategyproof, maximum, fair and non-bossy.</article>","contentLength":863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Global-Local Interface for On-Demand Teleoperation","url":"https://arxiv.org/abs/2502.09960","date":1739768400,"author":"","guid":1437,"unread":true,"content":"<article>arXiv:2502.09960v1 Announce Type: new \nAbstract: Teleoperation is a critical method for human-robot interface, holds significant potential for enabling robotic applications in industrial and unstructured environments. Existing teleoperation methods have distinct strengths and limitations in flexibility, range of workspace and precision. To fuse these advantages, we introduce the Global-Local (G-L) Teleoperation Interface. This interface decouples robotic teleoperation into global behavior, which ensures the robot motion range and intuitiveness, and local behavior, which enhances human operator's dexterity and capability for performing fine tasks. The G-L interface enables efficient teleoperation not only for conventional tasks like pick-and-place, but also for challenging fine manipulation and large-scale movements. Based on the G-L interface, we constructed a single-arm and a dual-arm teleoperation system with different remote control devices, then demonstrated tasks requiring large motion range, precise manipulation or dexterous end-effector control. Extensive experiments validated the user-friendliness, accuracy, and generalizability of the proposed interface.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KGGen: Extracting Knowledge Graphs from Plain Text with Language Models","url":"https://arxiv.org/abs/2502.09956","date":1739768400,"author":"","guid":1438,"unread":true,"content":"<article>arXiv:2502.09956v1 Announce Type: new \nAbstract: Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.</article>","contentLength":1063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diverse Inference and Verification for Advanced Reasoning","url":"https://arxiv.org/abs/2502.09955","date":1739768400,"author":"","guid":1439,"unread":true,"content":"<article>arXiv:2502.09955v1 Announce Type: new \nAbstract: Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Space Folds of ReLU Neural Networks","url":"https://arxiv.org/abs/2502.09954","date":1739768400,"author":"","guid":1440,"unread":true,"content":"<article>arXiv:2502.09954v1 Announce Type: new \nAbstract: Recent findings suggest that the consecutive layers of ReLU neural networks can be understood geometrically as space folding transformations of the input space, revealing patterns of self-similarity. In this paper, we present the first quantitative analysis of this space folding phenomenon in ReLU neural networks. Our approach focuses on examining how straight paths in the Euclidean input space are mapped to their counterparts in the Hamming activation space. In this process, the convexity of straight lines is generally lost, giving rise to non-convex folding behavior. To quantify this effect, we introduce a novel measure based on range metrics, similar to those used in the study of random walks, and provide the proof for the equivalence of convexity notions between the input and activation spaces. Furthermore, we provide empirical analysis on a geometrical analysis benchmark (CantorNet) as well as an image classification benchmark (MNIST). Our work advances the understanding of the activation space in ReLU neural networks by leveraging the phenomena of geometric folding, providing valuable insights on how these models process input information.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using MRNet to Predict Lunar Rock Categories Detected by Chang'e 5 Probe","url":"https://arxiv.org/abs/2502.09952","date":1739768400,"author":"","guid":1441,"unread":true,"content":"<article>arXiv:2502.09952v1 Announce Type: new \nAbstract: China's Chang'e 5 mission has been a remarkable success, with the chang'e 5 lander traveling on the Oceanus Procellarum to collect images of the lunar surface. Over the past half century, people have brought back some lunar rock samples, but its quantity does not meet the need for research. Under current circumstances, people still mainly rely on the analysis of rocks on the lunar surface through the detection of lunar rover. The Oceanus Procellarum, chosen by Chang'e 5 mission, contains various kind of rock species. Therefore, we first applied to the National Astronomical Observatories of the China under the Chinese Academy of Sciences for the Navigation and Terrain Camera (NaTeCam) of the lunar surface image, and established a lunar surface rock image data set CE5ROCK. The data set contains 100 images, which randomly divided into training, validation and test set. Experimental results show that the identification accuracy testing on convolutional neural network (CNN) models like AlexNet or MobileNet is about to 40.0%. In order to make full use of the global information in Moon images, this paper proposes the MRNet (MoonRockNet) network architecture. The encoding structure of the network uses VGG16 for feature extraction, and the decoding part adds dilated convolution and commonly used U-Net structure on the original VGG16 decoding structure, which is more conducive to identify more refined but more sparsely distributed types of lunar rocks. We have conducted extensive experiments on the established CE5ROCK data set, and the experimental results show that MRNet can achieve more accurate rock type identification, and outperform other existing mainstream algorithms in the identification performance.</article>","contentLength":1776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Blind Men and the Elephant: Mapping Interdisciplinarity in Research on Decentralized Autonomous Organizations","url":"https://arxiv.org/abs/2502.09949","date":1739768400,"author":"","guid":1442,"unread":true,"content":"<article>arXiv:2502.09949v1 Announce Type: new \nAbstract: Decentralized Autonomous Organizations (DAOs) are attracting interdisciplinary interest, particularly in business, economics, and computer science. However, much like the parable of the blind men and the elephant, where each observer perceives only a fragment of the whole, DAO research remains fragmented across disciplines, limiting a comprehensive understanding of their potential. This paper assesses the maturity of interdisciplinary research on DAOs by analyzing knowledge flows between Business &amp; Economics and Computer Science through citation network analysis, topic modelling, and outlet analysis. Our findings reveal that while DAOs serve as a vibrant topic of interdisciplinary discourse, current research remains predominantly applied and case-driven, with limited theoretical integration. Strengthening the alignment between organizational and technical insights is crucial for advancing DAO research and fostering a more cohesive interdisciplinary framework.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analyzing Patient Daily Movement Behavior Dynamics Using Two-Stage Encoding Model","url":"https://arxiv.org/abs/2502.09947","date":1739768400,"author":"","guid":1443,"unread":true,"content":"<article>arXiv:2502.09947v1 Announce Type: new \nAbstract: In the analysis of remote healthcare monitoring data, time series representation learning offers substantial value in uncovering deeper patterns of patient behavior, especially given the fine temporal granularity of the data. In this study, we focus on a dataset of home activity records from people living with Dementia. We propose a two-stage self-supervised learning approach. The first stage involves converting time-series activities into text strings, which are then encoded by a fine-tuned language model. In the second stage, these time-series vectors are bi-dimensionalized for applying PageRank method, to analyze latent state transitions to quantitatively assess participants behavioral patterns and identify activity biases. These insights, combined with diagnostic data, aim to support personalized care interventions.</article>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Supervised Learning for Neural Topic Models with Variance-Invariance-Covariance Regularization","url":"https://arxiv.org/abs/2502.09944","date":1739768400,"author":"","guid":1444,"unread":true,"content":"<article>arXiv:2502.09944v1 Announce Type: new \nAbstract: In our study, we propose a self-supervised neural topic model (NTM) that combines the power of NTMs and regularized self-supervised learning methods to improve performance. NTMs use neural networks to learn latent topics hidden behind the words in documents, enabling greater flexibility and the ability to estimate more coherent topics compared to traditional topic models. On the other hand, some self-supervised learning methods use a joint embedding architecture with two identical networks that produce similar representations for two augmented versions of the same input. Regularizations are applied to these representations to prevent collapse, which would otherwise result in the networks outputting constant or redundant representations for all inputs. Our model enhances topic quality by explicitly regularizing latent topic representations of anchor and positive samples. We also introduced an adversarial data augmentation method to replace the heuristic sampling method. We further developed several variation models including those on the basis of an NTM that incorporates contrastive learning with both positive and negative samples. Experimental results on three datasets showed that our models outperformed baselines and state-of-the-art models both quantitatively and qualitatively.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Lightweight and Effective Image Tampering Localization Network with Vision Mamba","url":"https://arxiv.org/abs/2502.09941","date":1739768400,"author":"","guid":1445,"unread":true,"content":"<article>arXiv:2502.09941v1 Announce Type: new \nAbstract: Current image tampering localization methods primarily rely on Convolutional Neural Networks (CNNs) and Transformers. While CNNs suffer from limited local receptive fields, Transformers offer global context modeling at the expense of quadratic computational complexity. Recently, the state space model Mamba has emerged as a competitive alternative, enabling linear-complexity global dependency modeling. Inspired by it, we propose a lightweight and effective FORensic network based on vision MAmba (ForMa) for blind image tampering localization. Firstly, ForMa captures multi-scale global features that achieves efficient global dependency modeling through linear complexity. Then the pixel-wise localization map is generated by a lightweight decoder, which employs a parameter-free pixel shuffle layer for upsampling. Additionally, a noise-assisted decoding strategy is proposed to integrate complementary manipulation traces from tampered images, boosting decoder sensitivity to forgery cues. Experimental results on 10 standard datasets demonstrate that ForMa achieves state-of-the-art generalization ability and robustness, while maintaining the lowest computational complexity. Code is available at https://github.com/multimediaFor/ForMa.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Preliminary Exploration with GPT-4o Voice Mode","url":"https://arxiv.org/abs/2502.09940","date":1739768400,"author":"","guid":1446,"unread":true,"content":"<article>arXiv:2502.09940v1 Announce Type: new \nAbstract: With the rise of multimodal large language models, GPT-4o stands out as a pioneering model, driving us to evaluate its capabilities. This report assesses GPT-4o across various tasks to analyze its audio processing and reasoning abilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and music understanding, performing well in tasks like intent classification, spoken command classification, semantic and grammatical reasoning., multilingual speech recognition, and singing analysis. It also shows greater robustness against hallucinations than other large audio-language models (LALMs). However, it struggles with tasks such as audio duration prediction and instrument classification. Additionally, GPT-4o's safety mechanisms cause it to decline tasks like speaker identification, age classification, MOS prediction, and audio deepfake detection. Notably, the model exhibits a significantly different refusal rate when responding to speaker verification tasks on different datasets. This is likely due to variations in the accompanying instructions or the quality of the input audio, suggesting the sensitivity of its built-in safeguards. Finally, we acknowledge that model performance varies with evaluation protocols. This report only serves as a preliminary exploration of the current state of LALMs.</article>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Temporal Scale and Shift Invariant Automatic Event Recognition using the Mellin Transform","url":"https://arxiv.org/abs/2502.09939","date":1739768400,"author":"","guid":1447,"unread":true,"content":"<article>arXiv:2502.09939v1 Announce Type: new \nAbstract: The Spatio-temporal holographic correlator combines the traditional 2D optical image correlation techniques with inhomogeneously broadened arrays of cold atoms to achieve 3D time-space correlation to realize automatic event recognition at an ultra-high speed. Here we propose a method to realize such event recognition for videos running at different speeds. With this method, we can highly improve recognition accuracy and filter almost all the unwanted events in the video database.</article>","contentLength":533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tradeoffs in Processing Queries and Supporting Updates over an ML-Enhanced R-tree","url":"https://arxiv.org/abs/2502.09937","date":1739768400,"author":"","guid":1448,"unread":true,"content":"<article>arXiv:2502.09937v1 Announce Type: new \nAbstract: Machine Learning (ML) techniques have been successfully applied to design various learned database index structures for both the one- and multi-dimensional spaces. Particularly, a class of traditional multi-dimensional indexes has been augmented with ML models to design ML-enhanced variants of their traditional counterparts. This paper focuses on the R-tree multi-dimensional index structure as it is widely used for indexing multi-dimensional data. The R-tree has been augmented with machine learning models to enhance the R-tree performance. The AI+R-tree is an ML-enhanced R-tree index structure that augments a traditional disk-based R-tree with an ML model to enhance the R-tree's query processing performance, mainly, to avoid navigating the overlapping branches of the R-tree that do not yield query results, e.g., in the presence of high-overlap among the rectangles of the R-tree nodes. We investigate the empirical tradeoffs in processing dynamic query workloads and in supporting updates over the AI+R-tree. Particularly, we investigate the impact of the choice of ML models over the AI+R-tree query processing performance. Moreover, we present a case study of designing a custom loss function for a neural network model tailored to the query processing requirements of the AI+R-tree. Furthermore, we present the design tradeoffs for adopting various strategies for supporting dynamic inserts, updates, and deletes with the vision of realizing a mutable AI+R-tree. Experiments on real datasets demonstrate that the AI+R-tree can enhance the query processing performance of a traditional R-tree for high-overlap range queries by up to 5.4X while achieving up to 99% average query recall.</article>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Precise Parameter Localization for Textual Generation in Diffusion Models","url":"https://arxiv.org/abs/2502.09935","date":1739768400,"author":"","guid":1449,"unread":true,"content":"<article>arXiv:2502.09935v1 Announce Type: new \nAbstract: Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.</article>","contentLength":1476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fused Partial Gromov-Wasserstein for Structured Objects","url":"https://arxiv.org/abs/2502.09934","date":1739768400,"author":"","guid":1450,"unread":true,"content":"<article>arXiv:2502.09934v1 Announce Type: new \nAbstract: Structured data, such as graphs, are vital in machine learning due to their capacity to capture complex relationships and interactions. In recent years, the Fused Gromov-Wasserstein (FGW) distance has attracted growing interest because it enables the comparison of structured data by jointly accounting for feature similarity and geometric structure. However, as a variant of optimal transport (OT), classical FGW assumes an equal mass constraint on the compared data. In this work, we relax this mass constraint and propose the Fused Partial Gromov-Wasserstein (FPGW) framework, which extends FGW to accommodate unbalanced data. Theoretically, we establish the relationship between FPGW and FGW and prove the metric properties of FPGW. Numerically, we introduce Frank-Wolfe solvers for the proposed FPGW framework and provide a convergence analysis. Finally, we evaluate the FPGW distance through graph classification and clustering experiments, demonstrating its robust performance, especially when data is corrupted by outlier noise.</article>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning","url":"https://arxiv.org/abs/2502.09933","date":1739768400,"author":"","guid":1451,"unread":true,"content":"<article>arXiv:2502.09933v1 Announce Type: new \nAbstract: Inductive Reasoning (IR), the ability to summarize rules from examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually $&lt;$10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings.</article>","contentLength":1444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AffectSRNet : Facial Emotion-Aware Super-Resolution Network","url":"https://arxiv.org/abs/2502.09932","date":1739768400,"author":"","guid":1452,"unread":true,"content":"<article>arXiv:2502.09932v1 Announce Type: new \nAbstract: Facial expression recognition (FER) systems in low-resolution settings face significant challenges in accurately identifying expressions due to the loss of fine-grained facial details. This limitation is especially problematic for applications like surveillance and mobile communications, where low image resolution is common and can compromise recognition accuracy. Traditional single-image face super-resolution (FSR) techniques, however, often fail to preserve the emotional intent of expressions, introducing distortions that obscure the original affective content. Given the inherently ill-posed nature of single-image super-resolution, a targeted approach is required to balance image quality enhancement with emotion retention. In this paper, we propose AffectSRNet, a novel emotion-aware super-resolution framework that reconstructs high-quality facial images from low-resolution inputs while maintaining the intensity and fidelity of facial expressions. Our method effectively bridges the gap between image resolution and expression accuracy by employing an expression-preserving loss function, specifically tailored for FER applications. Additionally, we introduce a new metric to assess emotion preservation in super-resolved images, providing a more nuanced evaluation of FER system performance in low-resolution scenarios. Experimental results on standard datasets, including CelebA, FFHQ, and Helen, demonstrate that AffectSRNet outperforms existing FSR approaches in both visual quality and emotion fidelity, highlighting its potential for integration into practical FER applications. This work not only improves image clarity but also ensures that emotion-driven applications retain their core functionality in suboptimal resolution environments, paving the way for broader adoption in FER systems.</article>","contentLength":1863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TransGUNet: Transformer Meets Graph-based Skip Connection for Medical Image Segmentation","url":"https://arxiv.org/abs/2502.09931","date":1739768400,"author":"","guid":1453,"unread":true,"content":"<article>arXiv:2502.09931v1 Announce Type: new \nAbstract: Skip connection engineering is primarily employed to address the semantic gap between the encoder and decoder, while also integrating global dependencies to understand the relationships among complex anatomical structures in medical image segmentation. Although several models have proposed transformer-based approaches to incorporate global dependencies within skip connections, they often face limitations in capturing detailed local features with high computational complexity. In contrast, graph neural networks (GNNs) exploit graph structures to effectively capture local and global features. Leveraging these properties, we introduce an attentional cross-scale graph neural network (ACS-GNN), which enhances the skip connection framework by converting cross-scale feature maps into a graph structure and capturing complex anatomical structures through node attention. Additionally, we observed that deep learning models often produce uninformative feature maps, which degrades the quality of spatial attention maps. To address this problem, we integrated entropy-driven feature selection (EFS) with spatial attention, calculating an entropy score for each channel and filtering out high-entropy feature maps. Our innovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial attentio} to effectively enhance domain generalizability across various modalities by leveraging GNNs alongside a reliable spatial attention map, ensuring more robust features within the skip connection. Through comprehensive experiments and analysis, TransGUNet achieved superior segmentation performance on six seen and eight unseen datasets, demonstrating significantly higher efficiency compared to previous methods.</article>","contentLength":1762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Tree Tensor Networks for Image Recognition","url":"https://arxiv.org/abs/2502.09928","date":1739768400,"author":"","guid":1454,"unread":true,"content":"<article>arXiv:2502.09928v1 Announce Type: new \nAbstract: Originating in quantum physics, tensor networks (TNs) have been widely adopted as exponential machines and parameter decomposers for recognition tasks. Typical TN models, such as Matrix Product States (MPS), have not yet achieved successful application in natural image processing. When employed, they primarily serve to compress parameters within off-the-shelf networks, thus losing their distinctive capability to enhance exponential-order feature interactions. This paper introduces a novel architecture named \\textit{\\textbf{D}eep \\textbf{T}ree \\textbf{T}ensor \\textbf{N}etwork} (DTTN), which captures $2^L$-order multiplicative interactions across features through multilinear operations, while essentially unfolding into a \\emph{tree}-like TN topology with the parameter-sharing property. DTTN is stacked with multiple antisymmetric interacting modules (AIMs), and this design facilitates efficient implementation. Moreover, we theoretically reveal the equivalency among quantum-inspired TN models and polynomial and multilinear networks under certain conditions, and we believe that DTTN can inspire more interpretable studies in this field. We evaluate the proposed model against a series of benchmarks and achieve excellent performance compared to its peers and cutting-edge architectures. Our code will soon be publicly available.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence","url":"https://arxiv.org/abs/2502.09927","date":1739768400,"author":"","guid":1455,"unread":true,"content":"<article>arXiv:2502.09927v1 Announce Type: new \nAbstract: We introduce Granite Vision, a lightweight large language model with vision capabilities, specifically designed to excel in enterprise use cases, particularly in visual document understanding. Our model is trained on a comprehensive instruction-following dataset, including document-related tasks, such as content extraction from tables, charts, diagrams, sketches, and infographics, as well as general image tasks. The architecture of Granite Vision is centered around visual modality alignment with a decoder-only, 2 billion parameter Granite large language model. Additionally, we introduce a dedicated safety classification approach in test-time that leverages a sparse set of attention vectors to identify potential harmful inputs. Despite its lightweight architecture, Granite Vision achieves strong results in standard benchmarks related to visual document understanding, as well as on the LiveXiv benchmark, which is designed to avoid test set contamination by using a constantly updated corpus of recently published Arxiv papers. We are releasing the model under the Apache-2 license, allowing for both research and commercial use, while offering complete visibility into the training data and other relevant details. See https://huggingface.co/ibm-granite/ for model weights.</article>","contentLength":1334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Anomaly Detection via Tensor Chidori Pseudoskeleton Decomposition","url":"https://arxiv.org/abs/2502.09926","date":1739768400,"author":"","guid":1456,"unread":true,"content":"<article>arXiv:2502.09926v1 Announce Type: new \nAbstract: Anomaly detection plays a critical role in modern data-driven applications, from identifying fraudulent transactions and safeguarding network infrastructure to monitoring sensor systems for irregular patterns. Traditional approaches, such as distance, density, or cluster-based methods, face significant challenges when applied to high dimensional tensor data, where complex interdependencies across dimensions amplify noise and computational complexity. To address these limitations, this paper leverages Tensor Chidori pseudoskeleton decomposition within a tensor-robust principal component analysis framework to extract low Tucker rank structure while isolating sparse anomalies, ensuring robustness to anomaly detection. We establish theoretical results regarding convergence, and estimation error, demonstrating the stability and accuracy of the proposed approach. Numerical experiments on real-world spatiotemporal data from New York City taxi trip records validate the superiority of the proposed method in detecting anomalous urban events compared to existing benchmark methods. The results underscore the potential of Tensor Chidori pseudoskeleton decomposition to enhance anomaly detection for large-scale, high-dimensional data.</article>","contentLength":1288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types","url":"https://arxiv.org/abs/2502.09925","date":1739768400,"author":"","guid":1457,"unread":true,"content":"<article>arXiv:2502.09925v1 Announce Type: new \nAbstract: Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated process enhances both task diversity and data quality, reducing manual intervention. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrating the critical importance of task diversity. TaskGalaxy is publicly released at https://github.com/Kwai-YuanQi/TaskGalaxy.</article>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Consistent Model-based Adaptation for Visual Reinforcement Learning","url":"https://arxiv.org/abs/2502.09923","date":1739768400,"author":"","guid":1458,"unread":true,"content":"<article>arXiv:2502.09923v1 Announce Type: new \nAbstract: Visual reinforcement learning agents typically face serious performance declines in real-world applications caused by visual distractions. Existing methods rely on fine-tuning the policy's representations with hand-crafted augmentations. In this work, we propose Self-Consistent Model-based Adaptation (SCMA), a novel method that fosters robust adaptation without modifying the policy. By transferring cluttered observations to clean ones with a denoising model, SCMA can mitigate distractions for various policies as a plug-and-play enhancement. To optimize the denoising model in an unsupervised manner, we derive an unsupervised distribution matching objective with a theoretical analysis of its optimality. We further present a practical algorithm to optimize the objective by estimating the distribution of clean observations with a pre-trained world model. Extensive experiments on multiple visual generalization benchmarks and real robot data demonstrate that SCMA effectively boosts performance across various distractions and exhibits better sample efficiency.</article>","contentLength":1118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"{\\lambda}Scale: Enabling Fast Scaling for Serverless Large Language Model Inference","url":"https://arxiv.org/abs/2502.09922","date":1739768400,"author":"","guid":1459,"unread":true,"content":"<article>arXiv:2502.09922v1 Announce Type: new \nAbstract: Serverless computing has emerged as a compelling solution for cloud-based model inference. However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead. This poses a significant challenge in efficiently scaling model instances to accommodate dynamic, bursty workloads commonly observed in real-world inference services. In this paper, we introduce {\\lambda}Scale, an efficient serverless inference system to achieve fast model scaling. The key idea behind {\\lambda}Scale is to leverage high-speed RDMA networks between GPU nodes for fast model multicast, while enabling distributed inference execution during model transmission -- referred to as \"execute-while-load\". {\\lambda}Scale proposes an efficient model scaling scheme, {\\lambda}Pipe, which supports adaptive model multicast and dynamically constructs execution pipelines across receiving nodes for collaborative, distributed inference. Additionally, {\\lambda}Scale supports efficient model management across GPU and host memory, allowing fast scaling for models across different storage tiers. Evaluation results show that {\\lambda}Scale enables fast model scaling and effectively handles load spikes, achieving up to 5x tail-latency improvement and 31.3% cost reduction compared to state-of-the-art solutions on real-world LLM inference traces.</article>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"INF^2: High-Throughput Generative Inference of Large Language Models using Near-Storage Processing","url":"https://arxiv.org/abs/2502.09921","date":1739768400,"author":"","guid":1460,"unread":true,"content":"<article>arXiv:2502.09921v1 Announce Type: new \nAbstract: The growing memory and computational demands of large language models (LLMs) for generative inference present significant challenges for practical deployment. One promising solution to address these challenges is offloading-based batched inference, which leverages host memory and disk as an extended memory hierarchy for GPUs. While the approach cost-effectively enables LLM inference, its performance is limited by substantial I/O overhead, primarily due to the large key-value (KV) cache sizes, which increase with batch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that boosts generative inference throughput using computational storage devices (CSDs). The core of INF^2 is attention-near storage, which offloads memory-intensive self-attention operations to near-storage accelerators, significantly reducing traffic through the system interconnect. We also propose delayed KV cache writeback to hide storage write latency by delaying newly generated KV cache writes until the cache reaches sufficient size in system memory. Additionally, we introduce cooperative X-cache, a technique designed to further trade off the remaining memory capacity for storage bandwidth. Our methods effectively minimize idle time for computation, improving the overall throughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been implemented on PyTorch and evaluated on a real system. Our experiments show that INF^2 achieves up to 3.46$\\times$ throughput improvement compared to state-of-the-art baselines. We will open-source INF^2 to facilitate broader adoption.</article>","contentLength":1682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting on AI-READI Dataset","url":"https://arxiv.org/abs/2502.09919","date":1739768400,"author":"","guid":1461,"unread":true,"content":"<article>arXiv:2502.09919v1 Announce Type: new \nAbstract: Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively.</article>","contentLength":1952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dual Control for Interactive Autonomous Merging with Model Predictive Diffusion","url":"https://arxiv.org/abs/2502.09918","date":1739768400,"author":"","guid":1462,"unread":true,"content":"<article>arXiv:2502.09918v1 Announce Type: new \nAbstract: Interactive decision-making is essential in applications such as autonomous driving, where the agent must infer the behavior of nearby human drivers while planning in real-time. Traditional predict-then-act frameworks are often insufficient or inefficient because accurate inference of human behavior requires a continuous interaction rather than isolated prediction. To address this, we propose an active learning framework in which we rigorously derive predicted belief distributions. Additionally, we introduce a novel model-based diffusion solver tailored for online receding horizon control problems, demonstrated through a complex, non-convex highway merging scenario. Our approach extends previous high-fidelity dual control simulations to hardware experiments, which may be viewed at https://youtu.be/Q_JdZuopGL4, and verifies behavior inference in human-driven traffic scenarios, moving beyond idealized models. The results show improvements in adaptive planning under uncertainty, advancing the field of interactive decision-making for real-world applications.</article>","contentLength":1119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Deep Learning Approach to Interface Color Quality Assessment in HCI","url":"https://arxiv.org/abs/2502.09914","date":1739768400,"author":"","guid":1463,"unread":true,"content":"<article>arXiv:2502.09914v1 Announce Type: new \nAbstract: In this paper, a quantitative evaluation model for the color quality of human-computer interaction interfaces is proposed by combining deep convolutional neural networks (CNN). By extracting multidimensional features of interface images, including hue, brightness, purity, etc., CNN is used for efficient feature modeling and quantitative analysis, and the relationship between interface design and user perception is studied. The experiment is based on multiple international mainstream website interface datasets, covering e-commerce platforms, social media, education platforms, etc., and verifies the evaluation effect of the model on indicators such as contrast, clarity, color coordination, and visual appeal. The results show that the CNN evaluation is highly consistent with the user rating, with a correlation coefficient of up to 0.96, and it also shows high accuracy in mean square error and absolute error. Compared with traditional experience-based evaluation methods, the proposed model can efficiently and scientifically capture the visual characteristics of the interface and avoid the influence of subjective factors. Future research can explore the introduction of multimodal data (such as text and interactive behavior) into the model to further enhance the evaluation ability of dynamic interfaces and expand it to fields such as smart homes, medical systems, and virtual reality. This paper provides new methods and new ideas for the scientific evaluation and optimization of interface design.</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AutoS$^2$earch: Unlocking the Reasoning Potential of Large Models for Web-based Source Search","url":"https://arxiv.org/abs/2502.09913","date":1739768400,"author":"","guid":1464,"unread":true,"content":"<article>arXiv:2502.09913v1 Announce Type: new \nAbstract: Web-based management systems have been widely used in risk control and industrial safety. However, effectively integrating source search capabilities into these systems, to enable decision-makers to locate and address the hazard (e.g., gas leak detection) remains a challenge. While prior efforts have explored using web crowdsourcing and AI algorithms for source search decision support, these approaches suffer from overheads in recruiting human participants and slow response times in time-sensitive situations. To address this, we introduce AutoS$^2$earch, a novel framework leveraging large models for zero-shot source search in web applications. AutoS$^2$earch operates on a simplified visual environment projected through a web-based display, utilizing a chain-of-thought prompt designed to emulate human reasoning. The multi-modal large language model (MLLMs) dynamically converts visual observations into language descriptions, enabling the LLM to perform linguistic reasoning on four directional choices. Extensive experiments demonstrate that AutoS$^2$earch achieves performance nearly equivalent to human-AI collaborative source search while eliminating dependency on crowdsourced labor. Our work offers valuable insights in using web engineering to design such autonomous systems in other industrial applications.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Breaking the Familiarity Bias: Employing Virtual Reality Environments to Enhance Team Formation and Inclusion","url":"https://arxiv.org/abs/2502.09912","date":1739768400,"author":"","guid":1465,"unread":true,"content":"<article>arXiv:2502.09912v1 Announce Type: new \nAbstract: Team closeness provides the foundations of trust and communication, contributing to teams' success and viability. However, newcomers often struggle to be included in a team since incumbents tend to interact more with other existing members. Previous research suggests that online communication technologies can help team inclusion by mitigating members' perceived differences. In this study, we test how virtual reality (VR) can promote team closeness when forming teams. We conducted a between-subject experiment with teams working in-person and VR, where two members interacted first, and then a third member was added later to conduct a hidden-profile task. Participants evaluated how close they felt with their teammates after the task was completed. Our results show that VR newcomers felt closer to the incumbents than in-person newcomers. However, incumbents' closeness to newcomers did not vary across conditions. We discuss the implications of these findings and offer suggestions for how VR can promote inclusion.</article>","contentLength":1072,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prior-Independent Bidding Strategies for First-Price Auctions","url":"https://arxiv.org/abs/2502.09907","date":1739768400,"author":"","guid":1466,"unread":true,"content":"<article>arXiv:2502.09907v1 Announce Type: new \nAbstract: First-price auctions are one of the most popular mechanisms for selling goods and services, with applications ranging from display advertising to timber sales. Unlike their close cousin, the second-price auction, first-price auctions do not admit a dominant strategy. Instead, each buyer must design a bidding strategy that maps values to bids -- a task that is often challenging due to the lack of prior knowledge about competing bids. To address this challenge, we conduct a principled analysis of prior-independent bidding strategies for first-price auctions using worst-case regret as the performance measure. First, we develop a technique to evaluate the worst-case regret for (almost) any given value distribution and bidding strategy, reducing the complex task of ascertaining the worst-case competing-bid distribution to a simple line search. Next, building on our evaluation technique, we minimize worst-case regret and characterize a minimax-optimal bidding strategy for every value distribution. We achieve it by explicitly constructing a bidding strategy as a solution to an ordinary differential equation, and by proving its optimality for the intricate infinite-dimensional minimax problem underlying worst-case regret minimization. Our construction provides a systematic and computationally-tractable procedure for deriving minimax-optimal bidding strategies. When the value distribution is continuous, it yields a deterministic strategy that maps each value to a single bid. We also show that our minimax strategy significantly outperforms the uniform-bid-shading strategies advanced by prior work. Our result allows us to precisely quantify, through minimax regret, the performance loss due to a lack of knowledge about competing bids. We leverage this to analyze the impact of the value distribution on the performance loss, and find that it decreases as the buyer's values become more dispersed.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding","url":"https://arxiv.org/abs/2502.09906","date":1739768400,"author":"","guid":1467,"unread":true,"content":"<article>arXiv:2502.09906v1 Announce Type: new \nAbstract: Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual insects since they are often trained on the general knowledge of vision-language data. Meanwhile, understanding insects is a fundamental problem in precision agriculture, helping to promote sustainable development in agriculture. Therefore, this paper proposes a novel multimodal conversational model, Insect-LLaVA, to promote visual understanding in insect-domain knowledge. In particular, we first introduce a new large-scale Multimodal Insect Dataset with Visual Insect Instruction Data that enables the capability of learning the multimodal foundation models. Our proposed dataset enables conversational models to comprehend the visual and semantic features of the insects. Second, we propose a new Insect-LLaVA model, a new general Large Language and Vision Assistant in Visual Insect Understanding. Then, to enhance the capability of learning insect features, we develop an Insect Foundation Model by introducing a new micro-feature self-supervised learning with a Patch-wise Relevant Attention mechanism to capture the subtle differences among insect images. We also present Description Consistency loss to improve micro-feature learning via text descriptions. The experimental results evaluated on our new Visual Insect Question Answering benchmarks illustrate the effective performance of our proposed approach in visual insect understanding and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks.</article>","contentLength":1722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards personalised assessment of abdominal aortic aneurysm structural integrity","url":"https://arxiv.org/abs/2502.09905","date":1739768400,"author":"","guid":1468,"unread":true,"content":"<article>arXiv:2502.09905v1 Announce Type: new \nAbstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition involving the permanent dilation of the aorta, often detected incidentally through imaging for some other condition. The standard clinical approach to managing AAA follows a one-size-fits-all model based on aneurysm size and growth rate, leading to underestimation or overestimation of rupture risk in individual patients. The widely studied stress-based rupture risk estimation using computational biomechanics requires wall strength information. However, non-invasive methods for local patient-specific wall strength measurement have not yet been developed. Recently, we introduced an image-based approach for patient-specific, in vivo, non-invasive AAA kinematic analysis using time-resolved 3D computed tomography angiography (4D-CTA) images to measure wall strain throughout the cardiac cycle. In the present study, we integrated wall tension computation and strain measurement to develop a novel measure of local structural integrity of AAA wall - Relative Structural Integrity Index (RSII), independent of material properties and thickness of the wall and conditions of blood pressure measurement. Our methods provide a visual map of AAA wall structural integrity for individual patients using only their medical images and blood pressure data. We applied our methods to twelve patients. Additionally, we compared our measure of structural integrity of aneurysmal and non-aneurysmal aortas. Our results show similar values of the wall structural integrity measure across the patients, indicating the reliability of our methods. In line with experimental observations reported in the literature, our analysis revealed that localized low stiffness areas are primarily found in the most dilated AAA regions. Our results clearly demonstrate that the AAA wall is stiffer than the non-aneurysmal aorta.</article>","contentLength":1916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Ann Arbor Architecture for Agent-Oriented Programming","url":"https://arxiv.org/abs/2502.09903","date":1739768400,"author":"","guid":1469,"unread":true,"content":"<article>arXiv:2502.09903v1 Announce Type: new \nAbstract: In this paper, we reexamine prompt engineering for large language models through the lens of automata theory. We argue that language models function as automata and, like all automata, should be programmed in the languages they accept, a unified collection of all natural and formal languages. Therefore, traditional software engineering practices--conditioned on the clear separation of programming languages and natural languages--must be rethought. We introduce the Ann Arbor Architecture, a conceptual framework for agent-oriented programming of language models, as a higher-level abstraction over raw token generation, and provide a new perspective on in-context learning. Based on this framework, we present the design of our agent platform Postline, and report on our initial experiments in agent training.</article>","contentLength":862,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thompson Sampling for Repeated Newsvendor","url":"https://arxiv.org/abs/2502.09900","date":1739768400,"author":"","guid":1470,"unread":true,"content":"<article>arXiv:2502.09900v1 Announce Type: new \nAbstract: In this paper, we investigate the performance of Thompson Sampling (TS) for online learning with censored feedback, focusing primarily on the classic repeated newsvendor model--a foundational framework in inventory management--and demonstrating how our techniques can be naturally extended to a broader class of problems. We model demand using a Weibull distribution and initialize TS with a Gamma prior to dynamically adjust order quantities. Our analysis establishes optimal (up to logarithmic factors) frequentist regret bounds for TS without imposing restrictive prior assumptions. More importantly, it yields novel and highly interpretable insights on how TS addresses the exploration-exploitation trade-off in the repeated newsvendor setting. Specifically, our results show that when past order quantities are sufficiently large to overcome censoring, TS accurately estimates the unknown demand parameters, leading to near-optimal ordering decisions. Conversely, when past orders are relatively small, TS automatically increases future order quantities to gather additional demand information. Extensive numerical simulations further demonstrate that TS outperforms more conservative and widely-used approaches such as online convex optimization, upper confidence bounds, and myopic Bayesian dynamic programming. This study also lays the foundation for exploring general online learning problems with censored feedback.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transtiff: A Stylus-shaped Interface for Rendering Perceived Stiffness of Virtual Objects via Stylus Stiffness Control","url":"https://arxiv.org/abs/2502.09899","date":1739768400,"author":"","guid":1471,"unread":true,"content":"<article>arXiv:2502.09899v1 Announce Type: new \nAbstract: The replication of object stiffness is essential for enhancing haptic feedback in virtual environments. However, existing research has overlooked how stylus stiffness influences the perception of virtual object stiffness during tool-mediated interactions. To address this, we conducted a psychophysical experiment demonstrating that changing stylus stiffness combined with visual stimuli altered users' perception of virtual object stiffness. Based on these insights, we developed Transtiff, a stylus-shaped interface capable of on-demand stiffness control using a McKibben artificial muscle mechanism. Unlike previous approaches, our method manipulates the perceived stiffness of virtual objects via the stylus by controlling the stiffness of the stylus without altering the properties of the real object being touched, creating the illusion of a hard object feeing soft. Our user study confirmed that Transtiff effectively simulates a range of material properties, such as sponge, plastic, and tennis balls, providing haptic rendering that is closely aligned with the perceived material characteristics. By addressing the challenge of delivering realistic haptic feedback through tool-based interactions, Transtiff represents a significant advancement in the haptic interface design for VR applications.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase retrieval","url":"https://arxiv.org/abs/2502.09898","date":1739768400,"author":"","guid":1472,"unread":true,"content":"<article>arXiv:2502.09898v1 Announce Type: new \nAbstract: The injectivity of ReLU layers in neural networks, the recovery of vectors from clipped or saturated measurements, and (real) phase retrieval in $\\mathbb{R}^n$ allow for a similar problem formulation and characterization using frame theory. In this paper, we revisit all three problems with a unified perspective and derive lower Lipschitz bounds for ReLU layers and clipping which are analogous to the previously known result for phase retrieval and are optimal up to a constant factor.</article>","contentLength":536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Intelligence in Spectroscopy: Advancing Chemistry from Prediction to Generation and Beyond","url":"https://arxiv.org/abs/2502.09897","date":1739768400,"author":"","guid":1473,"unread":true,"content":"<article>arXiv:2502.09897v1 Announce Type: new \nAbstract: The rapid advent of machine learning (ML) and artificial intelligence (AI) has catalyzed major transformations in chemistry, yet the application of these methods to spectroscopic and spectrometric data, referred to as Spectroscopy Machine Learning (SpectraML), remains relatively underexplored. Modern spectroscopic techniques (MS, NMR, IR, Raman, UV-Vis) generate an ever-growing volume of high-dimensional data, creating a pressing need for automated and intelligent analysis beyond traditional expert-based workflows. In this survey, we provide a unified review of SpectraML, systematically examining state-of-the-art approaches for both forward tasks (molecule-to-spectrum prediction) and inverse tasks (spectrum-to-molecule inference). We trace the historical evolution of ML in spectroscopy, from early pattern recognition to the latest foundation models capable of advanced reasoning, and offer a taxonomy of representative neural architectures, including graph-based and transformer-based methods. Addressing key challenges such as data quality, multimodal integration, and computational scalability, we highlight emerging directions such as synthetic data generation, large-scale pretraining, and few- or zero-shot learning. To foster reproducible research, we also release an open-source repository containing recent papers and their corresponding curated datasets (https://github.com/MINE-Lab-ND/SpectrumML_Survey_Papers). Our survey serves as a roadmap for researchers, guiding progress at the intersection of spectroscopy and AI.</article>","contentLength":1591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatIoT: Large Language Model-based Security Assistant for Internet of Things with Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2502.09896","date":1739768400,"author":"","guid":1474,"unread":true,"content":"<article>arXiv:2502.09896v1 Announce Type: new \nAbstract: Internet of Things (IoT) has gained widespread popularity, revolutionizing industries and daily life. However, it has also emerged as a prime target for attacks. Numerous efforts have been made to improve IoT security, and substantial IoT security and threat information, such as datasets and reports, have been developed. However, existing research often falls short in leveraging these insights to assist or guide users in harnessing IoT security practices in a clear and actionable way. In this paper, we propose ChatIoT, a large language model (LLM)-based IoT security assistant designed to disseminate IoT security and threat intelligence. By leveraging the versatile property of retrieval-augmented generation (RAG), ChatIoT successfully integrates the advanced language understanding and reasoning capabilities of LLM with fast-evolving IoT security information. Moreover, we develop an end-to-end data processing toolkit to handle heterogeneous datasets. This toolkit converts datasets of various formats into retrievable documents and optimizes chunking strategies for efficient retrieval. Additionally, we define a set of common use case specifications to guide the LLM in generating answers aligned with users' specific needs and expertise levels. Finally, we implement a prototype of ChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3, LLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can generate more reliable, relevant, and technical in-depth answers for most use cases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above metrics by over 10% on average, particularly in relevance and technicality, compared to using LLMs alone.</article>","contentLength":1762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2502.09891","date":1739768400,"author":"","guid":1475,"unread":true,"content":"<article>arXiv:2502.09891v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) has proven effective in integrating external knowledge into large language models (LLMs) for question-answer (QA) tasks. The state-of-the-art RAG approaches often use the graph data as the external data since they capture the rich semantic information and link relationships between entities. However, existing graph-based RAG approaches cannot accurately identify the relevant information from the graph and also consume large numbers of tokens in the online retrieval process. To address these issues, we introduce a novel graph-based RAG approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by augmenting the question using attributed communities, and also introducing a novel LLM-based hierarchical clustering method. To retrieve the most relevant information from the graph for the question, we build a novel hierarchical index structure for the attributed communities and develop an effective online retrieval method. Experimental results demonstrate that ArchRAG outperforms existing methods in terms of both accuracy and token cost.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symmetry-Preserving Diffusion Models via Target Symmetrization","url":"https://arxiv.org/abs/2502.09890","date":1739768400,"author":"","guid":1476,"unread":true,"content":"<article>arXiv:2502.09890v1 Announce Type: new \nAbstract: Diffusion models are powerful tools for capturing complex distributions, but modeling data with inherent symmetries, such as molecular structures, remains challenging. Equivariant denoisers are commonly used to address this, but they introduce architectural complexity and optimization challenges, including noisy gradients and convergence issues. We propose a novel approach that enforces equivariance through a symmetrized loss function, which applies a time-dependent weighted averaging operation over group actions to the model's prediction target. This ensures equivariance without explicit architectural constraints and reduces gradient variance, leading to more stable and efficient optimization. Our method uses Monte Carlo sampling to estimate the average, incurring minimal computational overhead. We provide theoretical guarantees of equivariance for the minimizer of our loss function and demonstrate its effectiveness on synthetic datasets and the molecular conformation generation task using the GEOM-QM9 dataset. Experiments show improved sample quality compared to existing methods, highlighting the potential of our approach to enhance the scalability and practicality of equivariant diffusion models in generative tasks.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating and Improving Graph-based Explanation Methods for Multi-Agent Coordination","url":"https://arxiv.org/abs/2502.09889","date":1739768400,"author":"","guid":1477,"unread":true,"content":"<article>arXiv:2502.09889v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs), developed by the graph learning community, have been adopted and shown to be highly effective in multi-robot and multi-agent learning. Inspired by this successful cross-pollination, we investigate and characterize the suitability of existing GNN explanation methods for explaining multi-agent coordination. We find that these methods have the potential to identify the most-influential communication channels that impact the team's behavior. Informed by our initial analyses, we propose an attention entropy regularization term that renders GAT-based policies more amenable to existing graph-based explainers. Intuitively, minimizing attention entropy incentivizes agents to limit their attention to the most influential or impactful agents, thereby easing the challenge faced by the explainer. We theoretically ground this intuition by showing that minimizing attention entropy increases the disparity between the explainer-generated subgraph and its complement. Evaluations across three tasks and three team sizes i) provides insights into the effectiveness of existing explainers, and ii) demonstrates that our proposed regularization consistently improves explanation quality without sacrificing task performance.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Efficient Large Recommendation Model: Towards a Resource-Optimal Scaling Law","url":"https://arxiv.org/abs/2502.09888","date":1739768400,"author":"","guid":1478,"unread":true,"content":"<article>arXiv:2502.09888v1 Announce Type: new \nAbstract: The pursuit of scaling up recommendation models confronts intrinsic tensions between expanding model capacity and preserving computational tractability. While prior studies have explored scaling laws for recommendation systems, their resource-intensive paradigms -- often requiring tens of thousands of A100 GPU hours -- remain impractical for most industrial applications. This work addresses a critical gap: achieving sustainable model scaling under strict computational budgets. We propose Climber, a resource-efficient recommendation framework comprising two synergistic components: the ASTRO model architecture for algorithmic innovation and the TURBO acceleration framework for engineering optimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts two core innovations: (1) multi-scale sequence partitioning that reduces attention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks, enabling more efficient scaling with sequence length; (2) dynamic temperature modulation that adaptively adjusts attention scores for multimodal distributions arising from inherent multi-scenario and multi-behavior interactions. Complemented by TURBO (Two-stage Unified Ranking with Batched Output), a co-designed acceleration framework integrating gradient-aware feature compression and memory-efficient Key-Value caching, Climber achieves 5.15x throughput gains without performance degradation. Comprehensive offline experiments on multiple datasets validate that Climber exhibits a more ideal scaling curve. To our knowledge, this is the first publicly documented framework where controlled model scaling drives continuous online metric growth (12.19% overall lift) without prohibitive resource costs. Climber has been successfully deployed on Netease Cloud Music, one of China's largest music streaming platforms, serving tens of millions of users daily.</article>","contentLength":1927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos","url":"https://arxiv.org/abs/2502.09886","date":1739768400,"author":"","guid":1479,"unread":true,"content":"<article>arXiv:2502.09886v1 Announce Type: new \nAbstract: Simulation offers a promising approach for cheaply scaling training data for generalist policies. To scalably generate data from diverse and realistic tasks, existing algorithms either rely on large language models (LLMs) that may hallucinate tasks not interesting for robotics; or digital twins, which require careful real-to-sim alignment and are hard to scale. To address these challenges, we introduce Video2Policy, a novel framework that leverages internet RGB videos to reconstruct tasks based on everyday human behavior. Our approach comprises two phases: (1) task generation in simulation from videos; and (2) reinforcement learning utilizing in-context LLM-generated reward functions iteratively. We demonstrate the efficacy of Video2Policy by reconstructing over 100 videos from the Something-Something-v2 (SSv2) dataset, which depicts diverse and complex human behaviors on 9 different tasks. Our method can successfully train RL policies on such tasks, including complex and challenging tasks such as throwing. Finally, we show that the generated simulation data can be scaled up for training a general policy, and it can be transferred back to the real robot in a Real2Sim2Real way.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comprehensive Review of Neural Differential Equations for Time Series Analysis","url":"https://arxiv.org/abs/2502.09885","date":1739768400,"author":"","guid":1480,"unread":true,"content":"<article>arXiv:2502.09885v1 Announce Type: new \nAbstract: Time series modeling and analysis has become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.</article>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation","url":"https://arxiv.org/abs/2502.09884","date":1739768400,"author":"","guid":1481,"unread":true,"content":"<article>arXiv:2502.09884v1 Announce Type: new \nAbstract: We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first non-asymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\\sqrt{n}$, which significantly improves on the rates of convergence in prior works.</article>","contentLength":1043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stretching Rubber, Not Budgets: Accurate Parking Utilization on a Shoestring","url":"https://arxiv.org/abs/2502.09877","date":1739768400,"author":"","guid":1482,"unread":true,"content":"<article>arXiv:2502.09877v1 Announce Type: new \nAbstract: Effective parking management is essential for ensuring accessibility, safety, and convenience in master-planned communities, particularly in active adult neighborhoods experiencing rapid growth. Accurately assessing parking utilization is a crucial first step in planning for future demand, but data collection methods can be costly and labor-intensive. This paper presents a low-cost yet highly accurate methodology for measuring parking utilization using road tubes connected to portable traffic counters from JAMAR Technologies, Inc. By integrating results from JAMAR's analysis tool with custom Python scripting, the methodology enables precise parking lot counts through parameter optimization and automated error correction. The system's efficiency allows for scalable deployment without significant manual observation, reducing both costs and disruptions to daily operations. Using Tellico Village as a case study, this research demonstrates that community planners can obtain actionable parking insights on a limited budget, empowering them to make informed decisions about capacity expansion, traffic flow improvements, and facility scheduling. The findings underscore the feasibility of leveraging cost-effective technology to optimize infrastructure planning and ensure long-term resident satisfaction as communities grow.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation","url":"https://arxiv.org/abs/2502.09874","date":1739768400,"author":"","guid":1483,"unread":true,"content":"<article>arXiv:2502.09874v1 Announce Type: new \nAbstract: Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised training.In this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear instance segmentation problem. In this framework, we construct a fourier guidance module to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the nuclear.Meanwhile, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module. This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear. We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full supervision.In addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively. As open science, all codes and pre-trained models are available at https://github.com/LQY404/FrGNet.</article>","contentLength":1504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal","url":"https://arxiv.org/abs/2502.09873","date":1739768400,"author":"","guid":1484,"unread":true,"content":"<article>arXiv:2502.09873v1 Announce Type: new \nAbstract: Diffusion models have demonstrated remarkable success in image restoration tasks. However, their multi-step denoising process introduces significant computational overhead, limiting their practical deployment. Furthermore, existing methods struggle to effectively remove severe JPEG artifact, especially in highly compressed images. To address these challenges, we propose CODiff, a compression-aware one-step diffusion model for JPEG artifact removal. The core of CODiff is the compression-aware visual embedder (CaVE), which extracts and leverages JPEG compression priors to guide the diffusion model. We propose a dual learning strategy that combines explicit and implicit learning. Specifically, explicit learning enforces a quality prediction objective to differentiate low-quality images with different compression levels. Implicit learning employs a reconstruction objective that enhances the model's generalization. This dual learning allows for a deeper and more comprehensive understanding of JPEG compression. Experimental results demonstrate that CODiff surpasses recent leading methods in both quantitative and visual quality metrics. The code and models will be released at https://github.com/jp-guo/CODiff.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Calibrate for Reliable Visual Fire Detection","url":"https://arxiv.org/abs/2502.09872","date":1739768400,"author":"","guid":1485,"unread":true,"content":"<article>arXiv:2502.09872v1 Announce Type: new \nAbstract: Fire is characterized by its sudden onset and destructive power, making early fire detection crucial for ensuring human safety and protecting property. With the advancement of deep learning, the application of computer vision in fire detection has significantly improved. However, deep learning models often exhibit a tendency toward overconfidence, and most existing works focus primarily on enhancing classification performance, with limited attention given to uncertainty modeling. To address this issue, we propose transforming the Expected Calibration Error (ECE), a metric for measuring uncertainty, into a differentiable ECE loss function. This loss is then combined with the cross-entropy loss to guide the training process of multi-class fire detection models. Additionally, to achieve a good balance between classification accuracy and reliable decision, we introduce a curriculum learning-based approach that dynamically adjusts the weight of the ECE loss during training. Extensive experiments are conducted on two widely used multi-class fire detection datasets, DFAN and EdgeFireSmoke, validating the effectiveness of our uncertainty modeling method.</article>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies","url":"https://arxiv.org/abs/2502.09870","date":1739768400,"author":"","guid":1486,"unread":true,"content":"<article>arXiv:2502.09870v1 Announce Type: new \nAbstract: Recent attention to anthropomorphism -- the attribution of human-like qualities to non-human objects or entities -- of language technologies like LLMs has sparked renewed discussions about potential negative impacts of anthropomorphism. To productively discuss the impacts of this anthropomorphism and in what contexts it is appropriate, we need a shared vocabulary for the vast variety of ways that language can be anthropomorphic. In this work, we draw on existing literature and analyze empirical cases of user interactions with language technologies to develop a taxonomy of textual expressions that can contribute to anthropomorphism. We highlight challenges and tensions involved in understanding linguistic anthropomorphism, such as how all language is fundamentally human and how efforts to characterize and shift perceptions of humanness in machines can also dehumanize certain humans. We discuss ways that our taxonomy supports more precise and effective discussions of and decisions about anthropomorphism of language technologies.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Explicit and Implicit: How Users Provide Feedback to Shape Personalized Recommendation Content","url":"https://arxiv.org/abs/2502.09869","date":1739768400,"author":"","guid":1487,"unread":true,"content":"<article>arXiv:2502.09869v1 Announce Type: new \nAbstract: As personalized recommendation algorithms become integral to social media platforms, users are increasingly aware of their ability to influence recommendation content. However, limited research has explored how users provide feedback through their behaviors and platform mechanisms to shape the recommendation content. We conducted semi-structured interviews with 34 active users of algorithmic-driven social media platforms (e.g., Xiaohongshu, Douyin). In addition to explicit and implicit feedback, this study introduced intentional implicit feedback, highlighting the actions users intentionally took to refine recommendation content through perceived feedback mechanisms. Additionally, choices of feedback behaviors were found to align with specific purposes. Explicit feedback was primarily used for feed customization, while unintentional implicit feedback was more linked to content consumption. Intentional implicit feedback was employed for multiple purposes, particularly in increasing content diversity and improving recommendation relevance. This work underscores the user intention dimension in the explicit-implicit feedback dichotomy and offers insights for designing personalized recommendation feedback that better responds to users' needs.</article>","contentLength":1306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DesignWeaver: Dimensional Scaffolding for Text-to-Image Product Design","url":"https://arxiv.org/abs/2502.09867","date":1739768400,"author":"","guid":1488,"unread":true,"content":"<article>arXiv:2502.09867v1 Announce Type: new \nAbstract: Generative AI has enabled novice designers to quickly create professional-looking visual representations for product concepts. However, novices have limited domain knowledge that could constrain their ability to write prompts that effectively explore a product design space. To understand how experts explore and communicate about design spaces, we conducted a formative study with 12 experienced product designers and found that experts -- and their less-versed clients -- often use visual references to guide co-design discussions rather than written descriptions. These insights inspired DesignWeaver, an interface that helps novices generate prompts for a text-to-image model by surfacing key product design dimensions from generated images into a palette for quick selection. In a study with 52 novices, DesignWeaver enabled participants to craft longer prompts with more domain-specific vocabularies, resulting in more diverse, innovative product designs. However, the nuanced prompts heightened participants' expectations beyond what current text-to-image models could deliver. We discuss implications for AI-based product design support tools.</article>","contentLength":1200,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Users Who are Blind or Low Vision Play Mobile Games: Perceptions, Challenges, and Strategies","url":"https://arxiv.org/abs/2502.09866","date":1739768400,"author":"","guid":1489,"unread":true,"content":"<article>arXiv:2502.09866v1 Announce Type: new \nAbstract: As blind and low-vision (BLV) players engage more deeply with games, accessibility features have become essential. While some research has explored tools and strategies to enhance game accessibility, the specific experiences of these players with mobile games remain underexamined. This study addresses this gap by investigating how BLV users experience mobile games with varying accessibility levels. Through interviews with 32 experienced BLV mobile players, we explore their perceptions, challenges, and strategies for engaging with mobile games. Our findings reveal that BLV players turn to mobile games to alleviate boredom, achieve a sense of accomplishment, and build social connections, but face barriers depending on the game's accessibility level. We also compare mobile games to other forms of gaming, highlighting the relative advantages of mobile games, such as the inherent accessibility of smartphones. This study contributes to understanding BLV mobile gaming experiences and provides insights for enhancing accessible mobile game design.</article>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"U Can Touch This! Microarchitectural Timing Attacks via Machine Clears","url":"https://arxiv.org/abs/2502.09864","date":1739768400,"author":"","guid":1490,"unread":true,"content":"<article>arXiv:2502.09864v1 Announce Type: new \nAbstract: Microarchitectural timing attacks exploit subtle timing variations caused by hardware behaviors to leak sensitive information. In this paper, we introduce MCHammer, a novel side-channel technique that leverages machine clears induced by self-modifying code detection mechanisms. Unlike most traditional techniques, MCHammer does not require memory access or waiting periods, making it highly efficient. We compare MCHammer to the classical Flush+Reload technique, improving in terms of trace granularity, providing a powerful side-channel attack vector. Using MCHammer, we successfully recover keys from a deployed implementation of a cryptographic tool. Our findings highlight the practical implications of MCHammer and its potential impact on real-world systems.</article>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence of Analogical Reasoning","url":"https://arxiv.org/abs/2502.09863","date":1739768400,"author":"","guid":1491,"unread":true,"content":"<article>arXiv:2502.09863v1 Announce Type: new \nAbstract: The remarkable success of large language models relies on their ability to implicitly learn structured latent representations from the pretraining corpus. As a simpler surrogate for representation learning in language modeling, we study a class of solvable contrastive self-supervised algorithms which we term quadratic word embedding models. These models resemble the word2vec algorithm and perform similarly on downstream tasks. Our main contributions are analytical solutions for both the training dynamics (under certain hyperparameter choices) and the final word embeddings, given in terms of only the corpus statistics. Our solutions reveal that these models learn orthogonal linear subspaces one at a time, each one incrementing the effective rank of the embeddings until model capacity is saturated. Training on WikiText, we find that the top subspaces represent interpretable concepts. Finally, we use our dynamical theory to predict how and when models acquire the ability to complete analogies.</article>","contentLength":1054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Scoresheet for Explainable AI","url":"https://arxiv.org/abs/2502.09861","date":1739768400,"author":"","guid":1492,"unread":true,"content":"<article>arXiv:2502.09861v1 Announce Type: new \nAbstract: Explainability is important for the transparency of autonomous and intelligent systems and for helping to support the development of appropriate levels of trust. There has been considerable work on developing approaches for explaining systems and there are standards that specify requirements for transparency. However, there is a gap: the standards are too high-level and do not adequately specify requirements for explainability. This paper develops a scoresheet that can be used to specify explainability requirements or to assess the explainability aspects provided for particular applications. The scoresheet is developed by considering the requirements of a range of stakeholders and is applicable to Multiagent Systems as well as other AI technologies. We also provide guidance for how to use the scoresheet and illustrate its generality and usefulness by applying it to a range of applications.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Hypothesis Validation with Agentic Sequential Falsifications","url":"https://arxiv.org/abs/2502.09858","date":1739768400,"author":"","guid":1493,"unread":true,"content":"<article>arXiv:2502.09858v1 Announce Type: new \nAbstract: Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning","url":"https://arxiv.org/abs/2502.09854","date":1739768400,"author":"","guid":1494,"unread":true,"content":"<article>arXiv:2502.09854v1 Announce Type: new \nAbstract: In this work, we demonstrate that small language models (SLMs), specifically a 100M parameter GPT-2 model, can achieve competitive performance in multitask prompt generation tasks while requiring only a fraction of the computational resources needed by large language models (LLMs). Through a novel combination of upside-down reinforcement learning and synthetic data distillation from a powerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5% of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite being up to 80 times smaller, making it highly suitable for resource-constrained and real-time applications. This study highlights the potential of SLMs as efficient multitask learners in multimodal settings, providing a promising alternative to LLMs for scalable, low-latency deployments.</article>","contentLength":881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elastic Representation: Mitigating Spurious Correlations for Group Robustness","url":"https://arxiv.org/abs/2502.09850","date":1739768400,"author":"","guid":1495,"unread":true,"content":"<article>arXiv:2502.09850v1 Announce Type: new \nAbstract: Deep learning models can suffer from severe performance degradation when relying on spurious correlations between input features and labels, making the models perform well on training data but have poor prediction accuracy for minority groups. This problem arises especially when training data are limited or imbalanced. While most prior work focuses on learning invariant features (with consistent correlations to y), it overlooks the potential harm of spurious correlations between features. We hereby propose Elastic Representation (ElRep) to learn features by imposing Nuclear- and Frobenius-norm penalties on the representation from the last layer of a neural network. Similar to the elastic net, ElRep enjoys the benefits of learning important features without losing feature diversity. The proposed method is simple yet effective. It can be integrated into many deep learning approaches to mitigate spurious correlations and improve group robustness. Moreover, we theoretically show that ElRep has minimum negative impacts on in-distribution predictions. This is a remarkable advantage over approaches that prioritize minority groups at the cost of overall performance.</article>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems","url":"https://arxiv.org/abs/2502.09849","date":1739768400,"author":"","guid":1496,"unread":true,"content":"<article>arXiv:2502.09849v1 Announce Type: new \nAbstract: Explainable AI (XAI) has become a crucial component of Clinical Decision Support Systems (CDSS) to enhance transparency, trust, and clinical adoption. However, while many XAI methods have been proposed, their effectiveness in real-world medical settings remains underexplored. This paper provides a survey of human-centered evaluations of Explainable AI methods in Clinical Decision Support Systems. By categorizing existing works based on XAI methodologies, evaluation frameworks, and clinical adoption challenges, we offer a structured understanding of the landscape. Our findings reveal key challenges in the integration of XAI into healthcare workflows and propose a structured framework to align the evaluation methods of XAI with the clinical needs of stakeholders.</article>","contentLength":820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Event-Triggered Integrated Communication and Control with Graph Information Bottleneck Optimization","url":"https://arxiv.org/abs/2502.09846","date":1739768400,"author":"","guid":1497,"unread":true,"content":"<article>arXiv:2502.09846v1 Announce Type: new \nAbstract: Integrated communication and control serves as a critical ingredient in Multi-Agent Reinforcement Learning. However, partial observability limitations will impair collaboration effectiveness, and a potential solution is to establish consensus through well-calibrated latent variables obtained from neighboring agents. Nevertheless, the rigid transmission of less informative content can still result in redundant information exchanges. Therefore, we propose a Consensus-Driven Event-Based Graph Information Bottleneck (CDE-GIB) method, which integrates the communication graph and information flow through a GIB regularizer to extract more concise message representations while avoiding the high computational complexity of inner-loop operations. To further minimize the communication volume required for establishing consensus during interactions, we also develop a variable-threshold event-triggering mechanism. By simultaneously considering historical data and current observations, this mechanism capably evaluates the importance of information to determine whether an event should be triggered. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods in terms of both efficiency and adaptability.</article>","contentLength":1298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solving Empirical Bayes via Transformers","url":"https://arxiv.org/abs/2502.09844","date":1739768400,"author":"","guid":1498,"unread":true,"content":"<article>arXiv:2502.09844v1 Announce Type: new \nAbstract: This work applies modern AI tools (transformers) to solving one of the oldest statistical problems: Poisson means under empirical Bayes (Poisson-EB) setting. In Poisson-EB a high-dimensional mean vector $\\theta$ (with iid coordinates sampled from an unknown prior $\\pi$) is estimated on the basis of $X=\\mathrm{Poisson}(\\theta)$. A transformer model is pre-trained on a set of synthetically generated pairs $(X,\\theta)$ and learns to do in-context learning (ICL) by adapting to unknown $\\pi$. Theoretically, we show that a sufficiently wide transformer can achieve vanishing regret with respect to an oracle estimator who knows $\\pi$ as dimension grows to infinity. Practically, we discover that already very small models (100k parameters) are able to outperform the best classical algorithm (non-parametric maximum likelihood, or NPMLE) both in runtime and validation loss, which we compute on out-of-distribution synthetic data as well as real-world datasets (NHL hockey, MLB baseball, BookCorpusOpen). Finally, by using linear probes, we confirm that the transformer's EB estimator appears to internally work differently from either NPMLE or Robbins' estimators.</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MuDoC: An Interactive Multimodal Document-grounded Conversational AI System","url":"https://arxiv.org/abs/2502.09843","date":1739768400,"author":"","guid":1499,"unread":true,"content":"<article>arXiv:2502.09843v1 Announce Type: new \nAbstract: Multimodal AI is an important step towards building effective tools to leverage multiple modalities in human-AI communication. Building a multimodal document-grounded AI system to interact with long documents remains a challenge. Our work aims to fill the research gap of directly leveraging grounded visuals from documents alongside textual content in documents for response generation. We present an interactive conversational AI agent 'MuDoC' based on GPT-4o to generate document-grounded responses with interleaved text and figures. MuDoC's intelligent textbook interface promotes trustworthiness and enables verification of system responses by allowing instant navigation to source text and figures in the documents. We also discuss qualitative observations based on MuDoC responses highlighting its strengths and limitations.</article>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient, Accurate, and Robust Penalty-Projection Algorithm for Parameterized Stochastic Navier-Stokes Flow Problems","url":"https://arxiv.org/abs/2502.09842","date":1739768400,"author":"","guid":1500,"unread":true,"content":"<article>arXiv:2502.09842v1 Announce Type: new \nAbstract: This paper presents and analyzes a fast, robust, efficient, and optimally accurate fully discrete splitting algorithm for the Uncertainty Quantification (UQ) of parameterized Stochastic Navier-Stokes Equations (SNSEs) flow problems those occur in the convection-dominated regimes. The time-stepping algorithm is an implicit backward-Euler linearized method, grad-div and Ensemble Eddy Viscosity (EEV) regularized, and split using discrete Hodge decomposition. Additionally, the scheme's sub-problems are all designed to have different Right-Hand-Side (RHS) vectors but the same system matrix for all realizations at each time-step. The stability of the algorithm is rigorously proven, and it has been shown that appropriately large grad-div stabilization parameters vanish the splitting error. The proposed UQ algorithm is then combined with the Stochastic Collocation Methods (SCMs). Several numerical experiments are given to verify this superior scheme's predicted convergence rates and performance on benchmark problems for high expected Reynolds numbers ($Re$).</article>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation","url":"https://arxiv.org/abs/2502.09838","date":1739768400,"author":"","guid":1501,"unread":true,"content":"<article>arXiv:2502.09838v1 Announce Type: new \nAbstract: We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.</article>","contentLength":925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SoK: State of the time: On Trustworthiness of Digital Clocks","url":"https://arxiv.org/abs/2502.09837","date":1739768400,"author":"","guid":1502,"unread":true,"content":"<article>arXiv:2502.09837v1 Announce Type: new \nAbstract: Despite the critical role of timing infrastructure in enabling essential services, from public key infrastructure and smart grids to autonomous navigation and high-frequency trading, modern timing stacks remain highly vulnerable to malicious attacks. These threats emerge due to several reasons, including inadequate security mechanisms, the timing architectures unique vulnerability to delays, and implementation issues. In this paper, we aim to obtain a holistic understanding of the issues that make the timing stacks vulnerable to adversarial manipulations, what the challenges are in securing them, and what solutions can be borrowed from the research community to address them. To this end, we perform a systematic analysis of the security vulnerabilities of the timing stack. In doing so, we discover new attack surfaces, i.e., physical timing components and on-device timekeeping, which are often overlooked by existing research that predominantly studies the security of time synchronization protocols. We also show that the emerging trusted timing architectures are flawed and risk compromising wider system security, and propose an alternative design using hardware-software co-design.</article>","contentLength":1245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal $k$-Secretary with Logarithmic Memory","url":"https://arxiv.org/abs/2502.09834","date":1739768400,"author":"","guid":1503,"unread":true,"content":"<article>arXiv:2502.09834v1 Announce Type: new \nAbstract: We study memory-bounded algorithms for the $k$-secretary problem. The algorithm of Kleinberg (2005) achieves an optimal competitive ratio of $1 - O(1/\\sqrt{k})$, yet a straightforward implementation requires $\\Omega(k)$ memory.\n  Our main result is a $k$-secretary algorithm that matches the optimal competitive ratio using $O(\\log k)$ words of memory. We prove this result by establishing a general reduction from $k$-secretary to (random-order) quantile estimation, the problem of finding the $k$-th largest element in a stream. We show that a quantile estimation algorithm with an $O(k^{\\alpha})$ expected error (in terms of the rank) gives a $(1 - O(1/k^{1-\\alpha}))$-competitive $k$-secretary algorithm with $O(1)$ extra words.\n  We then introduce a new quantile estimation algorithm that achieves an $O(\\sqrt{k})$ expected error bound using $O(\\log k)$ memory. Of independent interest, we give a different algorithm that uses $O(\\sqrt{k})$ words and finds the $k$-th largest element exactly with high probability, generalizing a result of Munro and Paterson (1980).</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized Entropy-Based Ransomware Detection Using Autonomous Feature Resonance","url":"https://arxiv.org/abs/2502.09833","date":1739768400,"author":"","guid":1504,"unread":true,"content":"<article>arXiv:2502.09833v1 Announce Type: new \nAbstract: The increasing sophistication of cyber threats has necessitated the development of advanced detection mechanisms capable of identifying malicious activities with high precision and efficiency. A novel approach, termed Autonomous Feature Resonance, is introduced to address the limitations of traditional ransomware detection methods through the analysis of entropy-based feature interactions within system processes. The proposed method achieves an overall detection accuracy of 97.3\\%, with false positive and false negative rates of 1.8\\% and 2.1\\%, respectively, outperforming existing techniques such as signature-based detection and behavioral analysis. Its decentralized architecture enables local processing of data, reducing latency and improving scalability, while a self-learning mechanism ensures continuous adaptation to emerging threats. Experimental results demonstrate consistent performance across diverse ransomware families, including LockBit 3.0, BlackCat, and Royal, with low detection latency and efficient resource utilization. The method's reliance on entropy as a distinguishing feature provides robustness against obfuscation techniques, making it suitable for real-time deployment in high-throughput environments. These findings highlight the potential of entropy-based approaches to enhance cybersecurity frameworks, offering a scalable and adaptive solution for modern ransomware detection challenges.</article>","contentLength":1478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Fair Policies for Infectious Diseases Mitigation using Path Integral Control","url":"https://arxiv.org/abs/2502.09831","date":1739768400,"author":"","guid":1505,"unread":true,"content":"<article>arXiv:2502.09831v1 Announce Type: new \nAbstract: Infectious diseases pose major public health challenges to society, highlighting the importance of designing effective policies to reduce economic loss and mortality. In this paper, we propose a framework for sequential decision-making under uncertainty to design fairness-aware disease mitigation policies that incorporate various measures of unfairness. Specifically, our approach learns equitable vaccination and lockdown strategies based on a stochastic multi-group SIR model. To address the challenges of solving the resulting sequential decision-making problem, we adopt the path integral control algorithm as an efficient solution scheme. Through a case study, we demonstrate that our approach effectively improves fairness compared to conventional methods and provides valuable insights for policymakers.</article>","contentLength":861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection","url":"https://arxiv.org/abs/2502.09829","date":1739768400,"author":"","guid":1506,"unread":true,"content":"<article>arXiv:2502.09829v1 Announce Type: new \nAbstract: Evaluating learned robot control policies to determine their physical task-level capabilities costs experimenter time and effort. The growing number of policies and tasks exacerbates this issue. It is impractical to test every policy on every task multiple times; each trial requires a manual environment reset, and each task change involves re-arranging objects or even changing robots. Naively selecting a random subset of tasks and policies to evaluate is a high-cost solution with unreliable, incomplete results. In this work, we formulate robot evaluation as an active testing problem. We propose to model the distribution of robot performance across all tasks and policies as we sequentially execute experiments. Tasks often share similarities that can reveal potential relationships in policy behavior, and we show that natural language is a useful prior in modeling these relationships between tasks. We then leverage this formulation to reduce the experimenter effort by using a cost-aware expected information gain heuristic to efficiently select informative trials. Our framework accommodates both continuous and discrete performance outcomes. We conduct experiments on existing evaluation data from real robots and simulations. By prioritizing informative trials, our framework reduces the cost of calculating evaluation metrics for robot policies across many tasks.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data and Decision Traceability for the Welder's Arc","url":"https://arxiv.org/abs/2502.09827","date":1739768400,"author":"","guid":1507,"unread":true,"content":"<article>arXiv:2502.09827v1 Announce Type: new \nAbstract: Space Protocol is applying the principles derived from MITRE and NIST's Supply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a complex multi party system to achieve introspection, auditing, and replay of data and decisions that ultimately lead to a end decision. The core goal of decision traceability is to ensure transparency, accountability, and integrity within the WA system. This is accomplished by providing a clear, auditable path from the system's inputs all the way to the final decision. This traceability enables the system to track the various algorithms and data flows that have influenced a particular outcome.</article>","contentLength":695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safe Reinforcement Learning-based Control for Hydrogen Diesel Dual-Fuel Engines","url":"https://arxiv.org/abs/2502.09826","date":1739768400,"author":"","guid":1508,"unread":true,"content":"<article>arXiv:2502.09826v1 Announce Type: new \nAbstract: The urgent energy transition requirements towards a sustainable future stretch across various industries and are a significant challenge facing humanity. Hydrogen promises a clean, carbon-free future, with the opportunity to integrate with existing solutions in the transportation sector. However, adding hydrogen to existing technologies such as diesel engines requires additional modeling effort. Reinforcement Learning (RL) enables interactive data-driven learning that eliminates the need for mathematical modeling. The algorithms, however, may not be real-time capable and need large amounts of data to work in practice. This paper presents a novel approach which uses offline model learning with RL to demonstrate safe control of a 4.5 L Hydrogen Diesel Dual-Fuel (H2DF) engine. The controllers are demonstrated to be constraint compliant and can leverage a novel state-augmentation approach for sample-efficient learning. The offline policy is subsequently experimentally validated on the real engine where the control algorithm is executed on a Raspberry Pi controller and requires 6 times less computation time compared to online Model Predictive Control (MPC) optimization.</article>","contentLength":1232,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PUGS: Perceptual Uncertainty for Grasp Selection in Underwater Environments","url":"https://arxiv.org/abs/2502.09824","date":1739768400,"author":"","guid":1509,"unread":true,"content":"<article>arXiv:2502.09824v1 Announce Type: new \nAbstract: When navigating and interacting in challenging environments where sensory information is imperfect and incomplete, robots must make decisions that account for these shortcomings. We propose a novel method for quantifying and representing such perceptual uncertainty in 3D reconstruction through occupancy uncertainty estimation. We develop a framework to incorporate it into grasp selection for autonomous manipulation in underwater environments. Instead of treating each measurement equally when deciding which location to grasp from, we present a framework that propagates uncertainty inherent in the multi-view reconstruction process into the grasp selection. We evaluate our method with both simulated and the real world data, showing that by accounting for uncertainty, the grasp selection becomes robust against partial and noisy measurements. Code will be made available at https://onurbagoren.github.io/PUGS/</article>","contentLength":965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compression Properties for large Toeplitz-like matrices","url":"https://arxiv.org/abs/2502.09823","date":1739768400,"author":"","guid":1510,"unread":true,"content":"<article>arXiv:2502.09823v1 Announce Type: new \nAbstract: Toeplitz matrices are abundant in computational mathematics, and there is a rich literature on the development of fast and superfast algorithms for solving linear systems involving such matrices. Any Toeplitz matrix can be transformed into a matrix with off-diagonal blocks that are of low numerical rank.Surprisingly little is known about the compressibility of these matrices in a theoretically rigorous sense, even though this compressibility is relied upon in practice in a number of superfast Toeplitz solvers. In this paper, we show that the compression properties of these matrices can be thoroughly explained using their displacement structure. We provide explicit bounds on the numerical ranks of important submatrices that arise when applying HSS, HODLR and other approximations with hierarchical low-rank structure to transformed Toeplitz and Toeplitz-like matrices. Our results lead to very efficient displacement-based compression strategies that can be used to formulate adaptive superfast rank-structured solvers.</article>","contentLength":1077,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ATM-Net: Adaptive Termination and Multi-Precision Neural Networks for Energy-Harvested Edge Intelligence","url":"https://arxiv.org/abs/2502.09822","date":1739768400,"author":"","guid":1511,"unread":true,"content":"<article>arXiv:2502.09822v1 Announce Type: new \nAbstract: ATM-Net is a novel neural network architecture tailored for energy-harvested IoT devices, integrating adaptive termination points with multi-precision computing. It dynamically adjusts computational precision (32/8/4-bit) and network depth based on energy availability via early exit points. An energy-aware task scheduler optimizes the energy-accuracy trade-off. Experiments on CIFAR-10, PlantVillage, and TissueMNIST show ATM-Net achieves up to 96.93% accuracy while reducing power consumption by 87.5% with Q4 quantization compared to 32-bit operations. The power-delay product improves from 13.6J to 0.141J for DenseNet-121 and from 10.3J to 0.106J for ResNet-18, demonstrating its suitability for energy-harvesting systems.</article>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Solver-Aided Hierarchical Language for LLM-Driven CAD Design","url":"https://arxiv.org/abs/2502.09819","date":1739768400,"author":"","guid":1512,"unread":true,"content":"<article>arXiv:2502.09819v1 Announce Type: new \nAbstract: Large language models (LLMs) have been enormously successful in solving a wide variety of structured and unstructured generative tasks, but they struggle to generate procedural geometry in Computer Aided Design (CAD). These difficulties arise from an inability to do spatial reasoning and the necessity to guide a model through complex, long range planning to generate complex geometry. We enable generative CAD Design with LLMs through the introduction of a solver-aided, hierarchical domain specific language (DSL) called AIDL, which offloads the spatial reasoning requirements to a geometric constraint solver. Additionally, we show that in the few-shot regime, AIDL outperforms even a language with in-training data (OpenSCAD), both in terms of generating visual results closer to the prompt and creating objects that are easier to post-process and reason about.</article>","contentLength":915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the robustness of multimodal language model towards distractions","url":"https://arxiv.org/abs/2502.09818","date":1739768400,"author":"","guid":1513,"unread":true,"content":"<article>arXiv:2502.09818v1 Announce Type: new \nAbstract: Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt variations remains an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could have noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the ScienceQA dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts to evaluate the reasoning capacity of VLMs amid these distractions. Our findings reveal that most-of-the-art VLMs, including GPT-4, are vulnerable to various types of distractions, experiencing noticeable degradation in reasoning capabilities when confronted with distractions. Notably, models such as InternVL2 demonstrate a higher degree of robustness to these distractions. We also found that models exhibit greater sensitivity to textual distractions than visual ones. Additionally, we explored various mitigation strategies, such as prompt engineering, to counteract the impact of distractions. While these strategies improved solution accuracy, our analysis shows that there remain significant opportunities for improvement.</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vector Linear Secure Aggregation","url":"https://arxiv.org/abs/2502.09817","date":1739768400,"author":"","guid":1514,"unread":true,"content":"<article>arXiv:2502.09817v1 Announce Type: new \nAbstract: The secure summation problem, where $K$ users wish to compute the sum of their inputs at a server while revealing nothing about all $K$ inputs beyond the desired sum, is generalized in two aspects - first, the desired function is an arbitrary linear function (multiple linear combinations) of the $K$ inputs instead of just the sum; second, rather than protecting all $K$ inputs, we wish to guarantee that no information is leaked about an arbitrary linear function of the $K$ inputs. For this vector linear generalization of the secure summation problem, we characterize the optimal randomness cost, i.e., to compute one instance of the desired vector linear function, the minimum number of the random key variables held by the users is equal to the dimension of the vector space that is in the span of the vectors formed by the coefficients of the linear function to protect but not in the span of the vectors formed by the coefficients of the linear function to compute.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence","url":"https://arxiv.org/abs/2502.09815","date":1739768400,"author":"","guid":1515,"unread":true,"content":"<article>arXiv:2502.09815v1 Announce Type: new \nAbstract: Representation learning plays a central role in structuring internal embeddings to capture the statistical properties of language, influencing the coherence and contextual consistency of generated text. Statistical Coherence Alignment is introduced as a method to enforce structured token representations through tensor field convergence, guiding embeddings to reflect statistical dependencies inherent in linguistic data. A mathematical framework is established to quantify coherence alignment, integrating a loss function that optimizes representational consistency across training iterations. Empirical evaluations demonstrate that applying coherence constraints improves perplexity, enhances classification accuracy, and refines rare word embeddings, contributing to a more stable representation space. Comparative analyses with baseline models reveal that the proposed method fosters a more interpretable internal structure, ensuring that embeddings retain contextual dependencies while mitigating representation collapse. The impact on coherence score distributions suggests that the alignment mechanism strengthens semantic integrity across diverse linguistic constructs, leading to a more balanced organization of learned embeddings. Computational assessments indicate that while the method introduces additional memory and training costs, the structured optimization process justifies the trade-offs in applications requiring heightened contextual fidelity. Experimental results validate the effectiveness of coherence alignment in optimizing token representations, providing insights into how statistical dependencies can be leveraged to improve language model training.</article>","contentLength":1729,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for 16 African Languages","url":"https://arxiv.org/abs/2502.09814","date":1739768400,"author":"","guid":1516,"unread":true,"content":"<article>arXiv:2502.09814v1 Announce Type: new \nAbstract: Slot-filling and intent detection are well-established tasks in Conversational AI. However, current large-scale benchmarks for these tasks often exclude evaluations of low-resource languages and rely on translations from English benchmarks, thereby predominantly reflecting Western-centric concepts. In this paper, we introduce Injongo -- a multicultural, open-source benchmark dataset for 16 African languages with utterances generated by native speakers across diverse domains, including banking, travel, home, and dining. Through extensive experiments, we benchmark the fine-tuning multilingual transformer models and the prompting large language models (LLMs), and show the advantage of leveraging African-cultural utterances over Western-centric utterances for improving cross-lingual transfer from the English language. Experimental results reveal that current LLMs struggle with the slot-filling task, with GPT-4o achieving an average performance of 26 F1-score. In contrast, intent detection performance is notably better, with an average accuracy of 70.6%, though it still falls behind the fine-tuning baselines. Compared to the English language, GPT-4o and fine-tuning baselines perform similarly on intent detection, achieving an accuracy of approximately 81%. Our findings suggest that the performance of LLMs is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance.</article>","contentLength":1500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Suture Thread Modeling Using Control Barrier Functions for Autonomous Surgery","url":"https://arxiv.org/abs/2502.09813","date":1739768400,"author":"","guid":1517,"unread":true,"content":"<article>arXiv:2502.09813v1 Announce Type: new \nAbstract: Automating surgical systems enhances precision and safety while reducing human involvement in high-risk environments. A major challenge in automating surgical procedures like suturing is accurately modeling the suture thread, a highly flexible and compliant component. Existing models either lack the accuracy needed for safety critical procedures or are too computationally intensive for real time execution. In this work, we introduce a novel approach for modeling suture thread dynamics using control barrier functions (CBFs), achieving both realism and computational efficiency. Thread like behavior, collision avoidance, stiffness, and damping are all modeled within a unified CBF and control Lyapunov function (CLF) framework. Our approach eliminates the need to calculate complex forces or solve differential equations, significantly reducing computational overhead while maintaining a realistic model suitable for both automation and virtual reality surgical training systems. The framework also allows visual cues to be provided based on the thread's interaction with the environment, enhancing user experience when performing suture or ligation tasks. The proposed model is tested on the MagnetoSuture system, a minimally invasive robotic surgical platform that uses magnetic fields to manipulate suture needles, offering a less invasive solution for surgical procedures.</article>","contentLength":1430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Face Deepfakes - A Comprehensive Review","url":"https://arxiv.org/abs/2502.09812","date":1739768400,"author":"","guid":1518,"unread":true,"content":"<article>arXiv:2502.09812v1 Announce Type: new \nAbstract: In recent years, remarkable advancements in deep- fake generation technology have led to unprecedented leaps in its realism and capabilities. Despite these advances, we observe a notable lack of structured and deep analysis deepfake technology. The principal aim of this survey is to contribute a thorough theoretical analysis of state-of-the-art face deepfake generation and detection methods. Furthermore, we provide a coherent and systematic evaluation of the implications of deepfakes on face biometric recognition approaches. In addition, we outline key applications of face deepfake technology, elucidating both positive and negative applications of the technology, provide a detailed discussion regarding the gaps in existing research, and propose key research directions for further investigation.</article>","contentLength":854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inclusive Avatar Guidelines for People with Disabilities: Supporting Disability Representation in Social Virtual Reality","url":"https://arxiv.org/abs/2502.09811","date":1739768400,"author":"","guid":1519,"unread":true,"content":"<article>arXiv:2502.09811v1 Announce Type: new \nAbstract: Avatar is a critical medium for identity representation in social virtual reality (VR). However, options for disability expression are highly limited on current avatar interfaces. Improperly designed disability features may even perpetuate misconceptions about people with disabilities (PWD). As more PWD use social VR, there is an emerging need for comprehensive design standards that guide developers and designers to create inclusive avatars. Our work aim to advance the avatar design practices by delivering a set of centralized, comprehensive, and validated design guidelines that are easy to adopt, disseminate, and update. Through a systematic literature review and interview with 60 participants with various disabilities, we derived 20 initial design guidelines that cover diverse disability expression methods through five aspects, including avatar appearance, body dynamics, assistive technology design, peripherals around avatars, and customization control. We further evaluated the guidelines via a heuristic evaluation study with 10 VR practitioners, validating the guideline coverage, applicability, and actionability. Our evaluation resulted in a final set of 17 design guidelines with recommendation levels.</article>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration","url":"https://arxiv.org/abs/2502.09809","date":1739768400,"author":"","guid":1520,"unread":true,"content":"<article>arXiv:2502.09809v1 Announce Type: new \nAbstract: The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AgentGuard leverages the LLM orchestrator's innate capabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges - to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated constraints, enables multiple security applications. We empirically demonstrate AgentGuard's feasibility with experiments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening procedures for LLM agents to enhance their trustworthiness in real-world applications.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from Sparse Matrix Decomposition","url":"https://arxiv.org/abs/2502.09808","date":1739768400,"author":"","guid":1521,"unread":true,"content":"<article>arXiv:2502.09808v1 Announce Type: new \nAbstract: Securely computing graph convolutional networks (GCNs) is critical for applying their analytical capabilities to privacy-sensitive data like social/credit networks. Multiplying a sparse yet large adjacency matrix of a graph in GCN--a core operation in training/inference--poses a performance bottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it incurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and leveraging the monotonicity of non-zero entry locations, we propose a co-design harmonizing secure multi-party computation (MPC) with matrix sparsity. Our sparse matrix decomposition transforms an arbitrary sparse matrix into a product of structured matrices. Specialized MPC protocols for oblivious permutation and selection multiplication are then tailored, enabling our secure sparse matrix multiplication ($(SM)^2$) protocol, optimized for secure multiplication of these structured matrices. Together, these techniques take $O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present Virgos, a secure 2-party framework that is communication-efficient and memory-friendly on standard vertically-partitioned graph datasets. Performance of Virgos has been empirically validated across diverse network conditions.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency","url":"https://arxiv.org/abs/2502.09801","date":1739768400,"author":"","guid":1522,"unread":true,"content":"<article>arXiv:2502.09801v1 Announce Type: new \nAbstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into software engineering workflows has shown potential to enhance productivity, particularly in software testing. This paper investigates whether LLM support improves defect detection effectiveness during unit testing. Building on prior studies comparing manual and tool-supported testing, we replicated and extended an experiment where participants wrote unit tests for a Java-based system with seeded defects within a time-boxed session, supported by LLMs. Comparing LLM supported and manual testing, results show that LLM support significantly increases the number of unit tests generated, defect detection rates, and overall testing efficiency. These findings highlight the potential of LLMs to improve testing and defect detection outcomes, providing empirical insights into their practical application in software testing.</article>","contentLength":963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Co-designing Large Language Model Tools for Project-Based Learning with K12 Educators","url":"https://arxiv.org/abs/2502.09799","date":1739768400,"author":"","guid":1523,"unread":true,"content":"<article>arXiv:2502.09799v1 Announce Type: new \nAbstract: The emergence of generative AI, particularly large language models (LLMs), has opened the door for student-centered and active learning methods like project-based learning (PBL). However, PBL poses practical implementation challenges for educators around project design and management, assessment, and balancing student guidance with student autonomy. The following research documents a co-design process with interdisciplinary K-12 teachers to explore and address the current PBL challenges they face. Through teacher-driven interviews, collaborative workshops, and iterative design of wireframes, we gathered evidence for ways LLMs can support teachers in implementing high-quality PBL pedagogy by automating routine tasks and enhancing personalized learning. Teachers in the study advocated for supporting their professional growth and augmenting their current roles without replacing them. They also identified affordances and challenges around classroom integration, including resource requirements and constraints, ethical concerns, and potential immediate and long-term impacts. Drawing on these, we propose design guidelines for future deployment of LLM tools in PBL.</article>","contentLength":1224,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on LLM-based News Recommender Systems","url":"https://arxiv.org/abs/2502.09797","date":1739768400,"author":"","guid":1524,"unread":true,"content":"<article>arXiv:2502.09797v1 Announce Type: new \nAbstract: News recommender systems play a critical role in mitigating the information overload problem. In recent years, due to the successful applications of large language model technologies, researchers have utilized Discriminative Large Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve the performance of news recommender systems. Although several recent surveys review significant challenges for deep learning-based news recommender systems, such as fairness, privacy-preserving, and responsibility, there is a lack of a systematic survey on Large Language Model (LLM)-based news recommender systems. In order to review different core methodologies and explore potential issues systematically, we categorize DLLM-based and GLLM-based news recommender systems under the umbrella of LLM-based news recommender systems. In this survey, we first overview the development of deep learning-based news recommender systems. Then, we review LLM-based news recommender systems based on three aspects: news-oriented modeling, user-oriented modeling, and prediction-oriented modeling. Next, we examine the challenges from various perspectives, including datasets, benchmarking tools, and methodologies. Furthermore, we conduct extensive experiments to analyze how large language model technologies affect the performance of different news recommender systems. Finally, we comprehensively explore the future directions for LLM-based news recommendations in the era of LLMs.</article>","contentLength":1534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions","url":"https://arxiv.org/abs/2502.09795","date":1739768400,"author":"","guid":1525,"unread":true,"content":"<article>arXiv:2502.09795v1 Announce Type: new \nAbstract: Planetary exploration using aerial assets has the potential for unprecedented scientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved flight in Martian atmosphere is possible, future Mars rotocrafts will require advanced navigation capabilities for long-range flights. One such critical capability is Map-based Localization (MbL) which registers an onboard image to a reference map during flight in order to mitigate cumulative drift from visual odometry. However, significant illumination differences between rotocraft observations and a reference map prove challenging for traditional MbL systems, restricting the operational window of the vehicle. In this work, we investigate a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model for image registration that is more robust under large illumination differences than prior models. The system is supported by a custom simulation framework that uses real orbital maps to produce large amounts of realistic images of the Martian terrain. Comprehensive evaluations show that our proposed system outperforms prior MbL efforts in terms of localization accuracy under significant lighting and scale variations. Furthermore, we demonstrate the validity of our approach across a simulated Martian day.</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Noise Controlled CT Super-Resolution with Conditional Diffusion Model","url":"https://arxiv.org/abs/2502.09793","date":1739768400,"author":"","guid":1526,"unread":true,"content":"<article>arXiv:2502.09793v1 Announce Type: new \nAbstract: Improving the spatial resolution of CT images is a meaningful yet challenging task, often accompanied by the issue of noise amplification. This article introduces an innovative framework for noise-controlled CT super-resolution utilizing the conditional diffusion model. The model is trained on hybrid datasets, combining noise-matched simulation data with segmented details from real data. Experimental results with real CT images validate the effectiveness of our proposed framework, showing its potential for practical applications in CT imaging.</article>","contentLength":598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MANTIS: Detection of Zero-Day Malicious Domains Leveraging Low Reputed Hosting Infrastructure","url":"https://arxiv.org/abs/2502.09788","date":1739768400,"author":"","guid":1527,"unread":true,"content":"<article>arXiv:2502.09788v1 Announce Type: new \nAbstract: Internet miscreants increasingly utilize short-lived disposable domains to launch various attacks. Existing detection mechanisms are either too late to catch such malicious domains due to limited information and their short life spans or unable to catch them due to evasive techniques such as cloaking and captcha. In this work, we investigate the possibility of detecting malicious domains early in their life cycle using a content-agnostic approach. We observe that attackers often reuse or rotate hosting infrastructures to host multiple malicious domains due to increased utilization of automation and economies of scale. Thus, it gives defenders the opportunity to monitor such infrastructure to identify newly hosted malicious domains. However, such infrastructures are often shared hosting environments where benign domains are also hosted, which could result in a prohibitive number of false positives. Therefore, one needs innovative mechanisms to better distinguish malicious domains from benign ones even when they share hosting infrastructures. In this work, we build MANTIS, a highly accurate practical system that not only generates daily blocklists of malicious domains but also is able to predict malicious domains on-demand. We design a network graph based on the hosting infrastructure that is accurate and generalizable over time. Consistently, our models achieve a precision of 99.7%, a recall of 86.9% with a very low false positive rate (FPR) of 0.1% and on average detects 19K new malicious domains per day, which is over 5 times the new malicious domains flagged daily in VirusTotal. Further, MANTIS predicts malicious domains days to weeks before they appear in popular blocklists.</article>","contentLength":1755,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TableTalk: Scaffolding Spreadsheet Development with a Language Agent","url":"https://arxiv.org/abs/2502.09787","date":1739768400,"author":"","guid":1528,"unread":true,"content":"<article>arXiv:2502.09787v1 Announce Type: new \nAbstract: Despite its ubiquity in the workforce, spreadsheet programming remains challenging as programmers need both spreadsheet-specific knowledge (e.g., APIs to write formulas) and problem-solving skills to create complex spreadsheets. Large language models (LLMs) can help automate aspects of this process, and recent advances in planning and reasoning have enabled language agents, which dynamically plan, use tools, and take iterative actions to complete complex tasks. These agents observe, plan, and act, making them well-suited to scaffold spreadsheet programming by following expert processes.\n  We present TableTalk, a language agent that helps programmers build spreadsheets conversationally. Its design reifies three design principles -- scaffolding, flexibility, and incrementality -- which we derived from two studies of seven programmers and 62 Excel templates. TableTalk structures spreadsheet development by generating step-by-step plans and suggesting three next steps users can choose from. It also integrates tools that enable incremental spreadsheet construction. A user study with 20 programmers shows that TableTalk produces spreadsheets 2.3 times more likely to be preferred over a baseline agent, while reducing cognitive load and time spent reasoning about spreadsheet actions by 12.6%. TableTalk's approach has implications for human-agent collaboration. This includes providing persistent direct manipulation interfaces for stopping or undoing agent actions, while ensuring that such interfaces for accepting actions can be deactivated.</article>","contentLength":1604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerator-assisted Floating-point ASIP for Communication and Positioning in Massive MIMO Systems","url":"https://arxiv.org/abs/2502.09785","date":1739768400,"author":"","guid":1529,"unread":true,"content":"<article>arXiv:2502.09785v1 Announce Type: new \nAbstract: This paper presents an implementation of a floating-point-capable application-specific instruction set processor (ASIP) for both communication and positioning tasks using the massive multiple-input multiple-output (MIMO) technology. The ASIP is geared with vector processing capabilities in the form of single instruction multiple data (SIMD). A dual-pronged accelerator composition assists the processor to tame the heavier mathematical workloads. A standalone systolic array accelerator accompanies the processor to aid with matrix multiplications. A parallel vector memory subsystem provides functionalities to both the processor and the systolic array. Additionally, A convolutional neural network (CNN) module accelerator, which is paired with its own separate vector memory, works hand in glove with the processor to take on the positioning task. The processor is synthesized in 22 nm fully depleted silicon-on-insulator (FD-SOI) technology running at a clock frequency of 800 MHz. The system achieves a maximum detection throughput of 2.1 Gb/s in a 128x16 massive MIMO system for the user equipment (UE) speed of 50km/h. The localization throughput settles at around 390 positionings/s.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SLICES, a scientific instrument for the networking community","url":"https://arxiv.org/abs/2502.09783","date":1739768400,"author":"","guid":1530,"unread":true,"content":"<article>arXiv:2502.09783v1 Announce Type: new \nAbstract: A science is defined by a set of encyclopedic knowledge related to facts or phenomena following rules or evidenced by experimentally-driven observations. Computer Science and in particular computer networks is a relatively new scientific domain maturing over years and adopting the best practices inherited from more fundamental disciplines. The design of past, present and future networking components and architectures have been assisted, among other methods, by experimentally-driven research and in particular by the deployment of test platforms, usually named as testbeds. However, often experimentally-driven networking research used scattered methodologies, based on ad-hoc, small-sized testbeds, producing hardly repeatable results. We believe that computer networks needs to adopt a more structured methodology, supported by appropriate instruments, to produce credible experimental results supporting radical and incremental innovations. This paper reports lessons learned from the design and operation of test platforms for the scientific community dealing with digital infrastructures. We introduce the SLICES initiative as the outcome of several years of evolution of the concept of a networking test platform transformed into a scientific instrument. We address the challenges, requirements and opportunities that our community is facing to manage the full research-life cycle necessary to support a scientific methodology.</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models","url":"https://arxiv.org/abs/2502.09782","date":1739768400,"author":"","guid":1531,"unread":true,"content":"<article>arXiv:2502.09782v1 Announce Type: new \nAbstract: The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNet's performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios.</article>","contentLength":1407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Medical Applications of Graph Convolutional Networks Using Electronic Health Records: A Survey","url":"https://arxiv.org/abs/2502.09781","date":1739768400,"author":"","guid":1532,"unread":true,"content":"<article>arXiv:2502.09781v1 Announce Type: new \nAbstract: Graph Convolutional Networks (GCNs) have emerged as a promising approach to machine learning on Electronic Health Records (EHRs). By constructing a graph representation of patient data and performing convolutions on neighborhoods of nodes, GCNs can capture complex relationships and extract meaningful insights to support medical decision making. This survey provides an overview of the current research in applying GCNs to EHR data. We identify the key medical domains and prediction tasks where these models are being utilized, common benchmark datasets, and architectural patterns to provide a comprehensive survey of this field. While this is a nascent area of research, GCNs demonstrate strong potential to leverage the complex information hidden in EHRs. Challenges and opportunities for future work are also discussed.</article>","contentLength":874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games","url":"https://arxiv.org/abs/2502.09780","date":1739768400,"author":"","guid":1533,"unread":true,"content":"<article>arXiv:2502.09780v1 Announce Type: new \nAbstract: Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt and circumstance: A word-by-word LLM prompting approach to interlinear glossing for low-resource languages","url":"https://arxiv.org/abs/2502.09778","date":1739768400,"author":"","guid":1534,"unread":true,"content":"<article>arXiv:2502.09778v1 Announce Type: new \nAbstract: Partly automated creation of interlinear glossed text (IGT) has the potential to assist in linguistic documentation. We argue that LLMs can make this process more accessible to linguists because of their capacity to follow natural-language instructions. We investigate the effectiveness of a retrieval-based LLM prompting approach to glossing, applied to the seven languages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based shared task baseline for every language in the morpheme-level score category, and we show that a simple 3-best oracle has higher word-level scores than the challenge winner (a tuned sequence model) in five languages. In a case study on Tsez, we ask the LLM to automatically create and follow linguistic instructions, reducing errors on a confusing grammatical feature. Our results thus demonstrate the potential contributions which LLMs can make in interactive systems for glossing, both in making suggestions to human annotators and following directions.</article>","contentLength":1049,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the existence of EFX allocations in multigraphs","url":"https://arxiv.org/abs/2502.09777","date":1739768400,"author":"","guid":1535,"unread":true,"content":"<article>arXiv:2502.09777v1 Announce Type: new \nAbstract: We study the problem of \"fairly\" dividing indivisible goods to several agents that have valuation set functions over the sets of goods. As fair we consider the allocations that are envy-free up to any good (EFX), i.e., no agent envies any proper subset of the goods given to any other agent. The existence or not of EFX allocations is a major open problem in Fair Division, and there are only positive results for special cases.\n  [George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023] introduced a restriction on the agents' valuations according to a graph structure: the vertices correspond to agents and the edges to goods, and each vertex/agent has zero marginal value (or in other words, they are indifferent) for the edges/goods that are not adjacent to them. The existence of EFX allocations has been shown for simple graphs with general monotone valuations [George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023], and for multigraphs for restricted additive valuations [Alireza Kaviani, Masoud Seddighin, Amir Mohammad Shahrezaei 2024].\n  In this work, we push the state-of-the-art further, and show that the EFX allocations always exists in multigraphs and general monotone valuations if any of the following three conditions hold: either (a) the multigraph is bipartite, or (b) each agent has at most $\\lceil \\frac{n}{4} \\rceil -1$ neighbors, where $n$ is the total number of agents, or (c) the shortest cycle with non-parallel edges has length at least 6.</article>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigating the Role of Situational Disruptors in Engagement with Digital Mental Health Tools","url":"https://arxiv.org/abs/2502.09776","date":1739768400,"author":"","guid":1536,"unread":true,"content":"<article>arXiv:2502.09776v1 Announce Type: new \nAbstract: Challenges in engagement with digital mental health (DMH) tools are commonly addressed through technical enhancements and algorithmic interventions. This paper shifts the focus towards the role of users' broader social context as a significant factor in engagement. Through an eight-week text messaging program aimed at enhancing psychological wellbeing, we recruited 20 participants to help us identify situational engagement disruptors (SEDs), including personal responsibilities, professional obligations, and unexpected health issues. In follow-up design workshops with 25 participants, we explored potential solutions that address such SEDs: prioritizing self-care through structured goal-setting, alternative framings for disengagement, and utilization of external resources. Our findings challenge conventional perspectives on engagement and offer actionable design implications for future DMH tools.</article>","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Knowledge-Enhanced Program Repair for Data Science Code","url":"https://arxiv.org/abs/2502.09771","date":1739768400,"author":"","guid":1537,"unread":true,"content":"<article>arXiv:2502.09771v1 Announce Type: new \nAbstract: This paper introduces DSrepair, a knowledge-enhanced program repair method designed to repair the buggy code generated by LLMs in the data science domain. DSrepair uses knowledge graph based RAG for API knowledge retrieval as well as bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to enable knowledge graph based API retrieval, we construct DS-KG (Data Science Knowledge Graph) for widely used data science libraries. For bug knowledge enrichment, we employ an abstract syntax tree (AST) to localize errors at the AST node level. DSrepair's effectiveness is evaluated against five state-of-the-art LLM-based repair baselines using four advanced LLMs on the DS-1000 dataset. The results show that DSrepair surpasses all five baselines. Specifically, when compared to the second-best baseline, DSrepair demonstrates significant improvements, fixing 44.4%, 14.2%, 20.6%, and 32.1% more buggy code snippets for each of the four evaluated LLMs, respectively. Additionally, it achieves greater efficiency, reducing the number of tokens required per code task by 17.49%, 34.24%, 24.71%, and 17.59%, respectively.</article>","contentLength":1184,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complex Network Modelling with Power-law Activating Patterns and Its Evolutionary Dynamics","url":"https://arxiv.org/abs/2502.09768","date":1739768400,"author":"","guid":1538,"unread":true,"content":"<article>arXiv:2502.09768v1 Announce Type: new \nAbstract: Complex network theory provides a unifying framework for the study of structured dynamic systems. The current literature emphasizes a widely reported phenomenon of intermittent interaction among network vertices. In this paper, we introduce a complex network model that considers the stochastic switching of individuals between activated and quiescent states at power-law rates and the corresponding evolutionary dynamics. By using the Markov chain and renewal theory, we discover a homogeneous stationary distribution of activated sizes in the network with power-law activating patterns and infer some statistical characteristics. To better understand the effect of power-law activating patterns, we study the two-person-two-strategy evolutionary game dynamics, demonstrate the absorbability of strategies, and obtain the critical cooperation conditions for prisoner's dilemmas in homogeneous networks without mutation. The evolutionary dynamics in real networks are also discussed. Our results provide a new perspective to analyze and understand social physics in time-evolving network systems.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Markovian Discrete Diffusion with Causal Language Models","url":"https://arxiv.org/abs/2502.09767","date":1739768400,"author":"","guid":1539,"unread":true,"content":"<article>arXiv:2502.09767v1 Announce Type: new \nAbstract: Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, CaDDi integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pretrained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.</article>","contentLength":1060,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Generated Microservice Implementations from RESTful API Definitions","url":"https://arxiv.org/abs/2502.09766","date":1739768400,"author":"","guid":1540,"unread":true,"content":"<article>arXiv:2502.09766v1 Announce Type: new \nAbstract: The growing need for scalable, maintainable, and fast-deploying systems has made microservice architecture widely popular in software development. This paper presents a system that uses Large Language Models (LLMs) to automate the API-first development of RESTful microservices. This system assists in creating OpenAPI specification, generating server code from it, and refining the code through a feedback loop that analyzes execution logs and error messages. By focusing on the API-first methodology, this system ensures that microservices are designed with well-defined interfaces, promoting consistency and reliability across the development life-cycle. The integration of log analysis enables the LLM to detect and address issues efficiently, reducing the number of iterations required to produce functional and robust services. This process automates the generation of microservices and also simplifies the debugging and refinement phases, allowing developers to focus on higher-level design and integration tasks. This system has the potential to benefit software developers, architects, and organizations to speed up software development cycles and reducing manual effort. To assess the potential of the system, we conducted surveys with six industry practitioners. After surveying practitioners, the system demonstrated notable advantages in enhancing development speed, automating repetitive tasks, and simplifying the prototyping process. While experienced developers appreciated its efficiency for specific tasks, some expressed concerns about its limitations in handling advanced customizations and larger scale projects. The code is publicly available at https://github.com/sirbh/code-gen</article>","contentLength":1751,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differential Adjusted Parity for Learning Fair Representations","url":"https://arxiv.org/abs/2502.09765","date":1739768400,"author":"","guid":1541,"unread":true,"content":"<article>arXiv:2502.09765v1 Announce Type: new \nAbstract: The development of fair and unbiased machine learning models remains an ongoing objective for researchers in the field of artificial intelligence. We introduce the Differential Adjusted Parity (DAP) loss to produce unbiased informative representations. It utilises a differentiable variant of the adjusted parity metric to create a unified objective function. By combining downstream task classification accuracy and its inconsistency across sensitive feature domains, it provides a single tool to increase performance and mitigate bias. A key element in this approach is the use of soft balanced accuracies. In contrast to previous non-adversarial approaches, DAP does not suffer a degeneracy where the metric is satisfied by performing equally poorly across all sensitive domains. It outperforms several adversarial models on downstream task accuracy and fairness in our analysis. Specifically, it improves the demographic parity, equalized odds and sensitive feature accuracy by as much as 22.5\\%, 44.1\\% and 40.1\\%, respectively, when compared to the best performing adversarial approaches on these metrics. Overall, the DAP loss and its associated metric can play a significant role in creating more fair machine learning models.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SoK: Come Together -- Unifying Security, Information Theory, and Cognition for a Mixed Reality Deception Attack Ontology & Analysis Framework","url":"https://arxiv.org/abs/2502.09763","date":1739768400,"author":"","guid":1542,"unread":true,"content":"<article>arXiv:2502.09763v1 Announce Type: new \nAbstract: We present a primary attack ontology and analysis framework for deception attacks in Mixed Reality (MR). This is achieved through multidisciplinary Systematization of Knowledge (SoK), integrating concepts from MR security, information theory, and cognition. While MR grows in popularity, it presents many cybersecurity challenges, particularly concerning deception attacks and their effects on humans. In this paper, we use the Borden-Kopp model of deception to develop a comprehensive ontology of MR deception attacks. Further, we derive two models to assess impact of MR deception attacks on information communication and decision-making. The first, an information-theoretic model, mathematically formalizes the effects of attacks on information communication. The second, a decision-making model, details the effects of attacks on interlaced cognitive processes. Using our ontology and models, we establish the MR Deception Analysis Framework (DAF) to assess the effects of MR deception attacks on information channels, perception, and attention. Our SoK uncovers five key findings for research and practice and identifies five research gaps to guide future work.</article>","contentLength":1215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and Deployment","url":"https://arxiv.org/abs/2502.09762","date":1739768400,"author":"","guid":1543,"unread":true,"content":"<article>arXiv:2502.09762v1 Announce Type: new \nAbstract: Adaptive teaming, the ability to collaborate with unseen teammates without prior coordination, remains an underexplored challenge in multi-robot collaboration. This paper focuses on adaptive teaming in multi-drone cooperative pursuit, a critical task with real-world applications such as border surveillance, search-and-rescue, and counter-terrorism. We first define and formalize the \\textbf{A}daptive Teaming in \\textbf{M}ulti-\\textbf{D}rone \\textbf{P}ursuit (AT-MDP) problem and introduce AT-MDP framework, a comprehensive framework that integrates simulation, algorithm training and real-world deployment. AT-MDP framework provides a flexible experiment configurator and interface for simulation, a distributed training framework with an extensive algorithm zoo (including two newly proposed baseline methods) and an unseen drone zoo for evaluating adaptive teaming, as well as a real-world deployment system that utilizes edge computing and Crazyflie drones. To the best of our knowledge, AT-MDP framework is the first adaptive framework for continuous-action decision-making in complex real-world drone tasks, enabling multiple drones to coordinate effectively with unseen teammates. Extensive experiments in four multi-drone pursuit environments of increasing difficulty confirm the effectiveness of AT-MDP framework, while real-world deployments further validate its feasibility in physical systems. Videos and code are available at https://sites.google.com/view/at-mdp.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The AI-Therapist Duo: Exploring the Potential of Human-AI Collaboration in Personalized Art Therapy for PICS Intervention","url":"https://arxiv.org/abs/2502.09757","date":1739768400,"author":"","guid":1544,"unread":true,"content":"<article>arXiv:2502.09757v1 Announce Type: new \nAbstract: Post-intensive care syndrome (PICS) is a multifaceted condition that arises from prolonged stays in an intensive care unit (ICU). While preventing PICS among ICU patients is becoming increasingly important, interventions remain limited. Building on evidence supporting the effectiveness of art exposure in addressing the psychological aspects of PICS, we propose a novel art therapy solution through a collaborative Human-AI approach that enhances personalized therapeutic interventions using state-of-the-art Visual Art Recommendation Systems. We developed two Human-in-the-Loop (HITL) personalization methods and assessed their impact through a large-scale user study (N=150). Our findings demonstrate that this Human-AI collaboration not only enhances the personalization and effectiveness of art therapy but also supports therapists by streamlining their workload. While our study centres on PICS intervention, the results suggest that human-AI collaborative Art therapy could potentially benefit other areas where emotional support is critical, such as cases of anxiety and depression.</article>","contentLength":1139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization","url":"https://arxiv.org/abs/2502.09755","date":1739768400,"author":"","guid":1545,"unread":true,"content":"<article>arXiv:2502.09755v1 Announce Type: new \nAbstract: Jailbreak attacks aim to exploit large language models (LLMs) and pose a significant threat to their proper conduct; they seek to bypass models' safeguards and often provoke transgressive behaviors. However, existing automatic jailbreak attacks require extensive computational resources and are prone to converge on suboptimal solutions. In this work, we propose \\textbf{C}ompliance \\textbf{R}efusal \\textbf{I}nitialization (CRI), a novel, attack-agnostic framework that efficiently initializes the optimization in the proximity of the compliance subspace of harmful prompts. By narrowing the initial gap to the adversarial objective, CRI substantially improves adversarial success rates (ASR) and drastically reduces computational overhead -- often requiring just a single optimization step. We evaluate CRI on the widely-used AdvBench dataset over the standard jailbreak attacks of GCG and AutoDAN. Results show that CRI boosts ASR and decreases the median steps to success by up to \\textbf{\\(\\times 60\\)}. The project page, along with the reference implementation, is publicly available at \\texttt{https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation/}.</article>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Adaptive Meshing, Mesh Density Functions, and Nonlocal Observations for Ensemble Based Data Assimilation","url":"https://arxiv.org/abs/2502.09754","date":1739768400,"author":"","guid":1546,"unread":true,"content":"<article>arXiv:2502.09754v1 Announce Type: new \nAbstract: Adaptive spatial meshing has proven invaluable for the accurate, efficient computation of solutions of time dependent partial differential equations. In a DA context the use of adaptive spatial meshes addresses several factors that place increased demands on meshing; these include the location and relative importance of observations and the use of ensemble solutions. To increase the efficiency of adaptive meshes for data assimilation, robust look ahead meshes are developed that fix the same adaptive mesh for all ensemble members for the entire time interval of the forecasts and that incorporates the observations at the next analysis time. This allows for increased vectorization of the ensemble forecasts while minimizing interpolation of solutions between different meshes. The techniques to determine these robust meshes are based upon combining metric tensors or mesh density functions to define nonuniform meshes. We illustrate the robust ensemble look ahead meshes using traveling wave solutions of a bistable reaction-diffusion equation. Observation operators based on convolution type integrals and their associated metric tensors are derived. These further the goals of making efficient use of adaptive meshes in ensemble based DA techniques, developing and employing robust meshes that are effective for a range of similar behaviors in both the ensembles and the observations, and the integration with advanced numerical PDE techniques (a quasi-Lagrangian moving mesh DG technique employing embedded pairs for time stepping). Numerical experiments with different observation scenarios are presented for a 2D inviscid Burgers' equation, a multi-component system, a 2D Shallow Water model, and for a coupled system of two 1D Kuramoto-Sivashinsky equations.</article>","contentLength":1820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vote-Tree-Planner: Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting","url":"https://arxiv.org/abs/2502.09749","date":1739768400,"author":"","guid":1547,"unread":true,"content":"<article>arXiv:2502.09749v1 Announce Type: new \nAbstract: Integrating large language models (LLMs) into closed-loop robotic task planning has become increasingly popular within embodied artificial intelligence. Previous efforts mainly focused on leveraging the strong reasoning abilities of LLMs to enhance task planning performance while often overlooking task planning efficiency and executability due to repetitive queries to LLMs. This paper addresses the synergy between LLMs and task planning systems, aiming to minimize redundancy while enhancing planning effectiveness. Specifically, building upon Prog-Prompt and the high-level concept of Tree-Planner, we propose Vote-Tree-Planner. This sampling strategy utilizes votes to guide plan traversal during the decision-making process. Our approach is motivated by a straightforward observation: assigning weights to agents during decision-making enables the evaluation of critical paths before execution. With this simple vote-tree construction, our method further improves the success rate and reduces the number of queries to LLMs. The experimental results highlight that our Vote-Tree-Planner demonstrates greater stability and shows a higher average success rate and goal condition recall on the unseen dataset compared with previous baseline methods. These findings underscore the potential of the Vote-Tree-Planner to enhance planning accuracy, reliability, and efficiency in LLM-based planning systems.</article>","contentLength":1455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Widespread Adoption of Large Language Model-Assisted Writing Across Society","url":"https://arxiv.org/abs/2502.09747","date":1739768400,"author":"","guid":1548,"unread":true,"content":"<article>arXiv:2502.09747v1 Announce Type: new \nAbstract: The recent advances in large language models (LLMs) attracted significant public and policymaker interest in its adoption patterns. In this paper, we systematically analyze LLM-assisted writing across four domains-consumer complaints, corporate communications, job postings, and international organization press releases-from January 2022 to September 2024. Our dataset includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3 million job postings, and 15,919 United Nations (UN) press releases. Using a robust population-level statistical framework, we find that LLM usage surged following the release of ChatGPT in November 2022. By late 2024, roughly 18% of financial consumer complaint text appears to be LLM-assisted, with adoption patterns spread broadly across regions and slightly higher in urban areas. For corporate press releases, up to 24% of the text is attributable to LLMs. In job postings, LLM-assisted writing accounts for just below 10% in small firms, and is even more common among younger firms. UN press releases also reflect this trend, with nearly 14% of content being generated or modified by LLMs. Although adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by 2024, reflecting either saturation in LLM adoption or increasing subtlety of more advanced models. Our study shows the emergence of a new reality in which firms, consumers and even international organizations substantially rely on generative AI for communications.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fine-Tuning Foundation Models with Federated Learning for Privacy Preserving Medical Time Series Forecasting","url":"https://arxiv.org/abs/2502.09744","date":1739768400,"author":"","guid":1549,"unread":true,"content":"<article>arXiv:2502.09744v1 Announce Type: new \nAbstract: Federated Learning (FL) provides a decentralized machine learning approach, where multiple devices or servers collaboratively train a model without sharing their raw data, thus enabling data privacy. This approach has gained significant interest in academia and industry due to its privacy-preserving properties, which are particularly valuable in the medical domain where data availability is often protected under strict regulations. A relatively unexplored area is the use of FL to fine-tune Foundation Models (FMs) for time series forecasting, potentially enhancing model efficacy by overcoming data limitation while maintaining privacy. In this paper, we fine-tuned time series FMs with Electrocardiogram (ECG) and Impedance Cardiography (ICG) data using different FL techniques. We then examined various scenarios and discussed the challenges FL faces under different data heterogeneity configurations. Our empirical results demonstrated that while FL can be effective for fine-tuning FMs on time series forecasting tasks, its benefits depend on the data distribution across clients. We highlighted the trade-offs in applying FL to FM fine-tuning.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partial Colexifications Improve Concept Embeddings","url":"https://arxiv.org/abs/2502.09743","date":1739768400,"author":"","guid":1550,"unread":true,"content":"<article>arXiv:2502.09743v1 Announce Type: new \nAbstract: While the embedding of words has revolutionized the field of Natural Language Processing, the embedding of concepts has received much less attention so far. A dense and meaningful representation of concepts, however, could prove useful for several tasks in computational linguistics, especially those involving cross-linguistic data or sparse data from low resource languages. First methods that have been proposed so far embed concepts from automatically constructed colexification networks. While these approaches depart from automatically inferred polysemies, attested across a larger number of languages, they are restricted to the word level, ignoring lexical relations that would only hold for parts of the words in a given language. Building on recently introduced methods for the inference of partial colexifications, we show how they can be used to improve concept embeddings in meaningful ways. The learned embeddings are evaluated against lexical similarity ratings, recorded instances of semantic shift, and word association data. We show that in all evaluation tasks, the inclusion of partial colexifications lead to improved concept representations and better results. Our results further show that the learned embeddings are able to capture and represent different semantic relationships between concepts.</article>","contentLength":1369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FoNE: Precise Single-Token Number Embeddings via Fourier Features","url":"https://arxiv.org/abs/2502.09741","date":1739768400,"author":"","guid":1551,"unread":true,"content":"<article>arXiv:2502.09741v1 Announce Type: new \nAbstract: Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64$\\times$ less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3$\\times$ and 6$\\times$ fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.</article>","contentLength":1459,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adjoint of Least Squares Shadowing: Existence, Uniqueness and Coarse Domain Discretization","url":"https://arxiv.org/abs/2502.09737","date":1739768400,"author":"","guid":1552,"unread":true,"content":"<article>arXiv:2502.09737v1 Announce Type: new \nAbstract: Chaotic dynamical systems are characterized by the sensitive dependence of trajectories on initial conditions. Conventional sensitivity analysis of time-averaged functionals yields unbounded sensitivities when the simulation is chaotic. The least squares shadowing (LSS) is a popular approach to computing bounded sensitivities in the presence of chaotic dynamical systems. The current paper proves the existence, uniqueness, and boundedness of the adjoint of the LSS equations. In particular, the analysis yields a sharper bound on the condition number of the LSS equations than currently demonstrated in existing literature and shows that the condition number is bounded for large integration times. The derived bound on condition number also shows a relation between the conditioning of the LSS and the time dilation factor which is consistent with the trend numerically observed in the previous LSS literature. Furthermore, using the boundedness of the condition number for large integration times, we provide an alternate proof to (Chater et al., 2017) of the convergence of the LSS sensitivity to the true sensitivity at the rate of $\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right)$ regardless of the boundary conditions imposed on the adjoint, as long as the adjoint boundary conditions are bounded. Existence and uniqueness of the solution to the continuous-in-time adjoint LSS equation ensure that the LSS equation can be discretized independently of the primal equation and that the true LSS adjoint solution is recovered as the time step is refined. This allows for the adjoint LSS equation to be discretized on a coarser time domain than that of the primal governing equation to reduce the cost of solving the linear space-time system.</article>","contentLength":1791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A CNN Approach to Automated Detection and Classification of Brain Tumors","url":"https://arxiv.org/abs/2502.09731","date":1739768400,"author":"","guid":1553,"unread":true,"content":"<article>arXiv:2502.09731v1 Announce Type: new \nAbstract: Brain tumors require an assessment to ensure timely diagnosis and effective patient treatment. Morphological factors such as size, location, texture, and variable appearance com- plicate tumor inspection. Medical imaging presents challenges, including noise and incomplete images. This research article presents a methodology for processing Magnetic Resonance Imag- ing (MRI) data, encompassing techniques for image classification and denoising. The effective use of MRI images allows medical professionals to detect brain disorders, including tumors. This research aims to categorize healthy brain tissue and brain tumors by analyzing the provided MRI data. Unlike alternative methods like Computed Tomography (CT), MRI technology offers a more detailed representation of internal anatomical components, mak- ing it a suitable option for studying data related to brain tumors. The MRI picture is first subjected to a denoising technique utilizing an Anisotropic diffusion filter. The dataset utilized for the models creation is a publicly accessible and validated Brain Tumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE was employed for data augmentation and dataset balancing. Convolutional Neural Networks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for the classification procedure. EfficientNet attained an accuracy of 98%, the highest recorded.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perch like a bird: bio-inspired optimal maneuvers and nonlinear control for Flapping-Wing Unmanned Aerial Vehicles","url":"https://arxiv.org/abs/2502.09728","date":1739768400,"author":"","guid":1554,"unread":true,"content":"<article>arXiv:2502.09728v1 Announce Type: new \nAbstract: This research endeavors to design the perching maneuver and control in ornithopter robots. By analyzing the dynamic interplay between the robot's flight dynamics, feedback loops, and the environmental constraints, we aim to advance our understanding of the perching maneuver, drawing parallels to biological systems. Inspired by the elegant control strategies observed in avian flight, we develop an optimal maneuver and a corresponding controller to achieve stable perching. The maneuver consists of a deceleration and a rapid pitch-up (vertical turn), which arises from analytically solving the optimization problem of minimal velocity at perch, subject to kinematic and dynamic constraints. The controller for the flapping frequency and tail symmetric deflection is nonlinear and adaptive, ensuring robustly stable perching. Indeed, such adaptive behavior in a sense incorporates homeostatic principles of cybernetics into the control system, enhancing the robot's ability to adapt to unexpected disturbances and maintain a stable posture during the perching maneuver. The resulting autonomous perching maneuvers -- closed-loop descent and turn -- , have been verified and validated, demonstrating excellent agreement with real bird perching trajectories reported in the literature. These findings lay the theoretical groundwork for the development of future prototypes that better imitate the skillful perching maneuvers of birds.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of Robust and Secure DNS Protocols for IoT Devices","url":"https://arxiv.org/abs/2502.09726","date":1739768400,"author":"","guid":1555,"unread":true,"content":"<article>arXiv:2502.09726v1 Announce Type: new \nAbstract: The DNS (Domain Name System) protocol has been in use since the early days of the Internet. Although DNS as a de facto networking protocol had no security considerations in its early years, there have been many security enhancements, such as DNSSec (Domain Name System Security Extensions), DoT (DNS over Transport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With all these security improvements, it is not yet clear what resource-constrained Internet-of-Things (IoT) devices should be used for robustness. In this paper, we investigate different DNS security approaches using an edge DNS resolver implemented as a Virtual Network Function (VNF) to replicate the impact of the protocol from an IoT perspective and compare their performances under different conditions. We present our results for cache-based and non-cached responses and evaluate the corresponding security benefits. Our results and framework can greatly help consumers, manufacturers, and the research community decide and implement their DNS protocols depending on the given dynamic network conditions and enable robust Internet access via DNS for different devices.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning","url":"https://arxiv.org/abs/2502.09724","date":1739768400,"author":"","guid":1556,"unread":true,"content":"<article>arXiv:2502.09724v1 Announce Type: new \nAbstract: In many real-world applications of reinforcement learning (RL), deployed policies have varied impacts on different stakeholders, creating challenges in reaching consensus on how to effectively aggregate their preferences. Generalized $p$-means form a widely used class of social welfare functions for this purpose, with broad applications in fair resource allocation, AI alignment, and decision-making. This class includes well-known welfare functions such as Egalitarian, Nash, and Utilitarian welfare. However, selecting the appropriate social welfare function is challenging for decision-makers, as the structure and outcomes of optimal policies can be highly sensitive to the choice of $p$. To address this challenge, we study the concept of an $\\alpha$-approximate portfolio in RL, a set of policies that are approximately optimal across the family of generalized $p$-means for all $p \\in [-\\infty, 1]$. We propose algorithms to compute such portfolios and provide theoretical guarantees on the trade-offs among approximation factor, portfolio size, and computational efficiency. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of our approach in summarizing the policy space induced by varying $p$ values, empowering decision-makers to navigate this landscape more effectively.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models","url":"https://arxiv.org/abs/2502.09723","date":1739768400,"author":"","guid":1557,"unread":true,"content":"<article>arXiv:2502.09723v1 Announce Type: new \nAbstract: Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to systematically examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into code-style structured query to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, ant the results show that QueryAttack achieves high attack success rates (ASRs) across LLMs with different developers and capabilities. We also evaluate QueryAttack's performance against common defenses, confirming that it is difficult to mitigate with general defensive techniques. To defend against QueryAttack, we tailor a defense method which can reduce ASR by up to 64\\% on GPT-4-1106. The code of QueryAttack can be found on https://anonymous.4open.science/r/QueryAttack-334B.</article>","contentLength":1262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NestQuant: Nested Lattice Quantization for Matrix Products and LLMs","url":"https://arxiv.org/abs/2502.09720","date":1739768400,"author":"","guid":1558,"unread":true,"content":"<article>arXiv:2502.09720v1 Announce Type: new \nAbstract: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent work have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various LLM evaluation benchmarks also show a reduction in performance degradation induced by quantization.</article>","contentLength":1056,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Carbon- and Precedence-Aware Scheduling for Data Processing Clusters","url":"https://arxiv.org/abs/2502.09717","date":1739768400,"author":"","guid":1559,"unread":true,"content":"<article>arXiv:2502.09717v1 Announce Type: new \nAbstract: As large-scale data processing workloads continue to grow, their carbon footprint raises concerns. Prior research on carbon-aware schedulers has focused on shifting computation to align with availability of low-carbon energy, but these approaches assume that each task can be executed independently. In contrast, data processing jobs have precedence constraints (i.e., outputs of one task are inputs for another) that complicate decisions, since delaying an upstream ``bottleneck'' task to a low-carbon period will also block downstream tasks, impacting the entire job's completion time. In this paper, we show that carbon-aware scheduling for data processing benefits from knowledge of both time-varying carbon and precedence constraints. Our main contribution is $\\texttt{PCAPS}$, a carbon-aware scheduler that interfaces with modern ML scheduling policies to explicitly consider the precedence-driven importance of each task in addition to carbon. To illustrate the gains due to fine-grained task information, we also study $\\texttt{CAP}$, a wrapper for any carbon-agnostic scheduler that adapts the key provisioning ideas of $\\texttt{PCAPS}$. Our schedulers enable a configurable priority between carbon reduction and job completion time, and we give analytical results characterizing the trade-off between the two. Furthermore, our Spark prototype on a 100-node Kubernetes cluster shows that a moderate configuration of $\\texttt{PCAPS}$ reduces carbon footprint by up to 32.9% without significantly impacting the cluster's total efficiency.</article>","contentLength":1594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Genetic Data Governance in Crisis: Policy Recommendations for Safeguarding Privacy and Preventing Discrimination","url":"https://arxiv.org/abs/2502.09716","date":1739768400,"author":"","guid":1560,"unread":true,"content":"<article>arXiv:2502.09716v1 Announce Type: new \nAbstract: Genetic data collection has become ubiquitous today. The ability to meaningfully interpret genetic data has motivated its widespread use, providing crucial insights into human health and ancestry while driving important public health initiatives. Easy access to genetic testing has fueled a rapid expansion of recreational direct-to-consumer offerings. However, the growth of genetic datasets and their applications has created significant privacy and discrimination risks, as our understanding of the scientific basis for genetic traits continues to evolve. In this paper, we organize the uses of genetic data along four distinct \"pillars\": clinical practice, research, forensic and government use, and recreational use. Using our scientific understanding of genetics, genetic inference methods and their associated risks, and current public protections, we build a risk assessment framework that identifies key values that any governance system must preserve. We analyze case studies using this framework to assess how well existing regulatory frameworks preserve desired values. Our investigation reveals critical gaps in these frameworks and identifies specific threats to privacy and personal liberties, particularly through genetic discrimination. We propose comprehensive policy reforms to: (1) update the legal definition of genetic data to protect against modern technological capabilities, (2) expand the Genetic Information Nondiscrimination Act (GINA) to cover currently unprotected domains, and (3) establish a unified regulatory framework under a single governing body to oversee all applications of genetic data. We conclude with three open questions about genetic data: the challenges posed by its relational nature, including consent for relatives and minors; the complexities of international data transfer; and its potential integration into large language models.</article>","contentLength":1932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating GPT's Capability in Identifying Stages of Cognitive Impairment from Electronic Health Data","url":"https://arxiv.org/abs/2502.09715","date":1739768400,"author":"","guid":1561,"unread":true,"content":"<article>arXiv:2502.09715v1 Announce Type: new \nAbstract: Identifying cognitive impairment within electronic health records (EHRs) is crucial not only for timely diagnoses but also for facilitating research. Information about cognitive impairment often exists within unstructured clinician notes in EHRs, but manual chart reviews are both time-consuming and error-prone. To address this issue, our study evaluates an automated approach using zero-shot GPT-4o to determine stage of cognitive impairment in two different tasks. First, we evaluated the ability of GPT-4o to determine the global Clinical Dementia Rating (CDR) on specialist notes from 769 patients who visited the memory clinic at Massachusetts General Hospital (MGH), and achieved a weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to differentiate between normal cognition, mild cognitive impairment (MCI), and dementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o attained a weighted kappa score of 0.91 in comparison to specialist chart reviews and 0.96 on cases that the clinical adjudicators rated with high confidence. Our findings demonstrate GPT-4o's potential as a scalable chart review tool for creating research datasets and assisting diagnosis in clinical settings in the future.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models","url":"https://arxiv.org/abs/2502.09696","date":1739768400,"author":"","guid":1562,"unread":true,"content":"<article>arXiv:2502.09696v1 Announce Type: new \nAbstract: Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.</article>","contentLength":878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Power System Electromagnetic Transient Stability: an Analysis Based on Convergent Hamiltonian","url":"https://arxiv.org/abs/2502.09695","date":1739768400,"author":"","guid":1563,"unread":true,"content":"<article>arXiv:2502.09695v1 Announce Type: new \nAbstract: Transient stability is crucial to the reliable operation of power systems. Existing theories rely on the simplified electromechanical models, substituting the detailed electromagnetic dynamics of inductor and capacitor with their impedance representations. However, this simplification is inadequate for the growing penetration of fast-switching power electronic devices. Attempts to extend the existing theories to include electromagnetic dynamics lead to overly conservative stability conditions. To tackle this problem more directly, we study the condition under which the power source and dissipation in the electromagnetic dynamics tend to balance each other asymptotically. This is equivalent to the convergence of the Hamiltonian (total stored energy) and can be shown to imply transient stability. Using contraction analysis, we prove that this property holds for a large class of time-varying port-Hamiltonian systems with (i) constant damping matrix and (ii) strictly convex Hamiltonian. Then through port-Hamiltonian modeling of the electromagnetic dynamics, we obtain that the synchronized steady state of the power system is globally stable if it exists. This result provides new insights into the reliable operation of power systems. The proposed theory is illustrated in the simulation results of a two-machine system.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Ronaldo's a poser!\": How the Use of Generative AI Shapes Debates in Online Forums","url":"https://arxiv.org/abs/2502.09693","date":1739768400,"author":"","guid":1564,"unread":true,"content":"<article>arXiv:2502.09693v1 Announce Type: new \nAbstract: Online debates can enhance critical thinking but may escalate into hostile attacks. As humans are increasingly reliant on Generative AI (GenAI) in writing tasks, we need to understand how people utilize GenAI in online debates. To examine the patterns of writing behavior while making arguments with GenAI, we created an online forum for soccer fans to engage in turn-based and free debates in a post format with the assistance of ChatGPT, arguing on the topic of \"Messi vs Ronaldo\". After 13 sessions of two-part study and semi-structured interviews with 39 participants, we conducted content and thematic analyses to integrate insights from interview transcripts, ChatGPT records, and forum posts. We found that participants prompted ChatGPT for aggressive responses, created posts with similar content and logical fallacies, and sacrificed the use of ChatGPT for better human-human communication. This work uncovers how polarized forum members work with GenAI to engage in debates online.</article>","contentLength":1040,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NeuralCFD: Deep Learning on High-Fidelity Automotive Aerodynamics Simulations","url":"https://arxiv.org/abs/2502.09692","date":1739768400,"author":"","guid":1565,"unread":true,"content":"<article>arXiv:2502.09692v1 Announce Type: new \nAbstract: Recent advancements in neural operator learning are paving the way for transformative innovations in fields such as automotive aerodynamics. However, key challenges must be overcome before neural network-based simulation surrogates can be implemented at an industry scale. First, surrogates must become scalable to large surface and volume meshes, especially when using raw geometry inputs only, i.e., without relying on the simulation mesh. Second, surrogates must be trainable with a limited number of high-fidelity numerical simulation samples while still reaching the required performance levels. To this end, we introduce Geometry-preserving Universal Physics Transformer (GP-UPT), which separates geometry encoding and physics predictions, ensuring flexibility with respect to geometry representations and surface sampling strategies. GP-UPT enables independent scaling of the respective parts of the model according to practical requirements, offering scalable solutions to open challenges. GP-UPT circumvents the creation of high-quality simulation meshes, enables accurate 3D velocity field predictions at 20 million mesh cells, and excels in transfer learning from low-fidelity to high-fidelity simulation datasets, requiring less than half of the high-fidelity data to match the performance of models trained from scratch.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes","url":"https://arxiv.org/abs/2502.09690","date":1739768400,"author":"","guid":1566,"unread":true,"content":"<article>arXiv:2502.09690v1 Announce Type: new \nAbstract: Multi-purpose Large Language Models (LLMs), a subset of generative Artificial Intelligence (AI), have recently made significant progress. While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet. To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts. This procedure was applied without any fine-tuning or calibration to document baseline LLM performance. We then adopted a two-fold mixed-methods approach to compare AI generated artifacts against the benchmark. First, we quantitatively compare the artifacts using natural language processing algorithms and find that when prompted carefully, the state-of-the-art algorithms cannot differentiate AI-generated artifacts from the human-expert benchmark. Second, we conduct a qualitative deep dive to investigate how they differ in terms of quality. We document that while the two-material appear very similar, AI generated artifacts exhibit serious failure modes that could be difficult to detect. We characterize these as: premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify. We contend that this study tells a cautionary tale about why the SE community must be more cautious adopting AI suggested feedback, at least when generated by multi-purpose LLMs.</article>","contentLength":1814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models and Provenance Metadata for Determining the Relevance of Images and Videos in News Stories","url":"https://arxiv.org/abs/2502.09689","date":1739768400,"author":"","guid":1567,"unread":true,"content":"<article>arXiv:2502.09689v1 Announce Type: new \nAbstract: The most effective misinformation campaigns are multimodal, often combining text with images and videos taken out of context -- or fabricating them entirely -- to support a given narrative. Contemporary methods for detecting misinformation, whether in deepfakes or text articles, often miss the interplay between multiple modalities. Built around a large language model, the system proposed in this paper addresses these challenges. It analyzes both the article's text and the provenance metadata of included images and videos to determine whether they are relevant. We open-source the system prototype and interactive web interface.</article>","contentLength":682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling","url":"https://arxiv.org/abs/2502.09688","date":1739768400,"author":"","guid":1568,"unread":true,"content":"<article>arXiv:2502.09688v1 Announce Type: new \nAbstract: Artificial intelligence (AI) is poised to transform healthcare by enabling personalized and efficient care through data-driven insights. Although radiology is at the forefront of AI adoption, in practice, the potential of AI models is often overshadowed by severe failures to generalize: AI models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch raises concerns that radiologists will be misled by incorrect AI predictions in practice and/or grow to distrust AI, rendering these promising technologies practically ineffectual. Exhaustive clinical trials of AI models on abundant and diverse data is thus critical to anticipate AI model degradation when encountering varied data samples. Achieving these goals, however, is challenging due to the high costs of collecting diverse data samples and corresponding annotations. To overcome these limitations, we introduce a novel conditional generative AI model designed for virtual clinical trials (VCTs) of radiology AI, capable of realistically synthesizing full-body CT images of patients with specified attributes. By learning the joint distribution of images and anatomical structures, our model enables precise replication of real-world patient populations with unprecedented detail at this scale. We demonstrate meaningful evaluation of radiology AI models through VCTs powered by our synthetic CT study populations, revealing model degradation and facilitating algorithmic auditing for bias-inducing data attributes. Our generative AI approach to VCTs is a promising avenue towards a scalable solution to assess model robustness, mitigate biases, and safeguard patient care by enabling simpler testing and evaluation of AI models in any desired range of diverse patient populations.</article>","contentLength":1882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models","url":"https://arxiv.org/abs/2502.09687","date":1739768400,"author":"","guid":1569,"unread":true,"content":"<article>arXiv:2502.09687v1 Announce Type: new \nAbstract: Be careful what you ask for, you just might get it. This saying fits with the way large language models (LLMs) are trained, which, instead of being rewarded for correctness, are increasingly rewarded for pleasing the recipient. So, they are increasingly effective at persuading us that their answers are valuable. But what tricks do they use in this persuasion? In this study, we examine what are the psycholinguistic features of the responses used by twelve different language models. By grouping response content according to rational or emotional prompts and exploring social influence principles employed by LLMs, we ask whether and how we can mitigate the risks of LLM-driven mass misinformation. We position this study within the broader discourse on human-centred AI, emphasizing the need for interdisciplinary approaches to mitigate cognitive and societal risks posed by persuasive AI responses.</article>","contentLength":952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Machine Learning and Deep Learning Techniques for Improved Pathological Staging of Prostate Cancer","url":"https://arxiv.org/abs/2502.09686","date":1739768400,"author":"","guid":1570,"unread":true,"content":"<article>arXiv:2502.09686v1 Announce Type: new \nAbstract: Prostate cancer (Pca) continues to be a leading cause of cancer-related mortality in men, and the limitations in precision of traditional diagnostic methods such as the Digital Rectal Exam (DRE), Prostate-Specific Antigen (PSA) testing, and biopsies underscore the critical importance of accurate staging detection in enhancing treatment outcomes and improving patient prognosis. This study leverages machine learning and deep learning approaches, along with feature selection and extraction methods, to enhance PCa pathological staging predictions using RNA sequencing data from The Cancer Genome Atlas (TCGA). Gene expression profiles from 486 tumors were analyzed using advanced algorithms, including Random Forest (RF), Logistic Regression (LR), Extreme Gradient Boosting (XGB), and Support Vector Machine (SVM). The performance of the study is measured with respect to the F1-score, as well as precision and recall, all of which are calculated as weighted averages. The results reveal that the highest test F1-score, approximately 83%, was achieved by the Random Forest algorithm, followed by Logistic Regression at 80%, while both Extreme Gradient Boosting (XGB) and Support Vector Machine (SVM) scored around 79%. Furthermore, deep learning models with data augmentation achieved an accuracy of 71. 23%, while PCA-based dimensionality reduction reached an accuracy of 69.86%. This research highlights the potential of AI-driven approaches in clinical oncology, paving the way for more reliable diagnostic tools that can ultimately improve patient outcomes.</article>","contentLength":1612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions","url":"https://arxiv.org/abs/2502.09685","date":1739768400,"author":"","guid":1571,"unread":true,"content":"<article>arXiv:2502.09685v1 Announce Type: new \nAbstract: Accurate demand forecasting is vital for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory, and distribution. However, forecasting contraceptive demand in developing countries presents challenges, including incomplete data, poor data quality, and the need to account for multiple geographical and product factors. Current methods often rely on simple forecasting techniques, which fail to capture demand uncertainties arising from these factors, warranting expert involvement. Our study aims to improve contraceptive demand forecasting by combining probabilistic forecasting methods with expert knowledge. We developed a hybrid model that combines point forecasts from domain-specific model with probabilistic distributions from statistical and machine learning approaches, enabling human input to fine-tune and enhance the system-generated forecasts. This approach helps address the uncertainties in demand and is particularly useful in resource-limited settings. We evaluate different forecasting methods, including time series, Bayesian, machine learning, and foundational time series methods alongside our new hybrid approach. By comparing these methods, we provide insights into their strengths, weaknesses, and computational requirements. Our research fills a gap in forecasting contraceptive demand and offers a practical framework that combines algorithmic and human expertise. Our proposed model can also be generalized to other humanitarian contexts with similar data patterns.</article>","contentLength":1591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Channel Dependence, Limited Lookback Windows, and the Simplicity of Datasets: How Biased is Time Series Forecasting?","url":"https://arxiv.org/abs/2502.09683","date":1739768400,"author":"","guid":1572,"unread":true,"content":"<article>arXiv:2502.09683v1 Announce Type: new \nAbstract: Time-series forecasting research has converged to a small set of datasets and a standardized collection of evaluation scenarios. Such a standardization is to a specific extent needed for comparable research. However, the underlying assumption is, that the considered setting is a representative for the problem as a whole. In this paper, we challenge this assumption and show that the current scenario gives a strongly biased perspective on the state of time-series forecasting research. To be more detailed, we show that the current evaluation scenario is heavily biased by the simplicity of the current datasets. We furthermore emphasize, that when the lookback-window is properly tuned, current models usually do not need any information flow across channels. However, when using more complex benchmark data, the situation changes: Here, modeling channel-interactions in a sophisticated manner indeed enhances performances. Furthermore, in this complex evaluation scenario, Crossformer, a method regularly neglected as an important baseline, is the SOTA method for time series forecasting. Based on this, we present the Fast Channel-dependent Transformer (FaCT), a simplified version of Crossformer which closes the runtime gap between Crossformer and TimeMixer, leading to an efficient model for complex forecasting datasets.</article>","contentLength":1378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Object-Centric Latent Action Learning","url":"https://arxiv.org/abs/2502.09680","date":1739768400,"author":"","guid":1573,"unread":true,"content":"<article>arXiv:2502.09680v1 Announce Type: new \nAbstract: Leveraging vast amounts of internet video data for Embodied AI is currently bottle-necked by the lack of action annotations and the presence of action-correlated distractors. We propose a novel object-centric latent action learning approach, based on VideoSaur and LAPO, that employs self-supervised decomposition of scenes into object representations and annotates video data with proxy-action labels. This method effectively disentangles causal agent-object interactions from irrelevant background noise and reduces the performance degradation of latent action learning approaches caused by distractors. Our preliminary experiments with the Distracting Control Suite show that latent action pretraining based on object decompositions improve the quality of inferred latent actions by x2.7 and efficiency of downstream fine-tuning with a small set of labeled actions, increasing return by x2.6 on average.</article>","contentLength":955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis","url":"https://arxiv.org/abs/2502.09675","date":1739768400,"author":"","guid":1574,"unread":true,"content":"<article>arXiv:2502.09675v1 Announce Type: new \nAbstract: Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by exploiting textual, acoustic, and visual modalities, and thus how to make full use of the interactions between different modalities is a central challenge of MSA. Interaction contains alignment and conflict aspects. Current works mainly emphasize alignment and the inherent differences between unimodal modalities, neglecting the fact that there are also potential conflicts between bimodal combinations. Additionally, multi-task learning-based conflict modeling methods often rely on the unstable generated labels. To address these challenges, we propose a novel multi-level conflict-aware network (MCAN) for multimodal sentiment analysis, which progressively segregates alignment and conflict constituents from unimodal and bimodal representations, and further exploits the conflict constituents with the conflict modeling branch. In the conflict modeling branch, we conduct discrepancy constraints at both the representation and predicted output levels, avoiding dependence on the generated labels. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed MCAN.</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis","url":"https://arxiv.org/abs/2502.09674","date":1739768400,"author":"","guid":1575,"unread":true,"content":"<article>arXiv:2502.09674v1 Announce Type: new \nAbstract: Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective. Code and artifacts are available at https://github.com/BMPixel/safety-residual-space.</article>","contentLength":1340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning","url":"https://arxiv.org/abs/2502.09673","date":1739768400,"author":"","guid":1576,"unread":true,"content":"<article>arXiv:2502.09673v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable success across various NLP benchmarks. However, excelling in complex tasks that require nuanced reasoning and precise decision-making demands more than raw language proficiency--LLMs must reason, i.e., think logically, draw from past experiences, and synthesize information to reach conclusions and take action. To enhance reasoning abilities, approaches such as prompting and fine-tuning have been widely explored. While these methods have led to clear improvements in reasoning, their impact on LLM safety remains less understood. In this work, we investigate the interplay between reasoning and safety in LLMs. We highlight the latent safety risks that arise as reasoning capabilities improve, shedding light on previously overlooked vulnerabilities. At the same time, we explore how reasoning itself can be leveraged to enhance safety, uncovering potential mitigation strategies. By examining both the risks and opportunities in reasoning-driven LLM safety, our study provides valuable insights for developing models that are not only more capable but also more trustworthy in real-world deployments.</article>","contentLength":1211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter","url":"https://arxiv.org/abs/2502.09672","date":1739768400,"author":"","guid":1577,"unread":true,"content":"<article>arXiv:2502.09672v1 Announce Type: new \nAbstract: 3D Multi-Object Tracking (MOT) provides the trajectories of surrounding objects, assisting robots or vehicles in smarter path planning and obstacle avoidance. Existing 3D MOT methods based on the Tracking-by-Detection framework typically use a single motion model to track an object throughout its entire tracking process. However, objects may change their motion patterns due to variations in the surrounding environment. In this paper, we introduce the Interacting Multiple Model filter in IMM-MOT, which accurately fits the complex motion patterns of individual objects, overcoming the limitation of single-model tracking in existing approaches. In addition, we incorporate a Damping Window mechanism into the trajectory lifecycle management, leveraging the continuous association status of trajectories to control their creation and termination, reducing the occurrence of overlooked low-confidence true targets. Furthermore, we propose the Distance-Based Score Enhancement module, which enhances the differentiation between false positives and true positives by adjusting detection scores, thereby improving the effectiveness of the Score Filter. On the NuScenes Val dataset, IMM-MOT outperforms most other single-modal models using 3D point clouds, achieving an AMOTA of 73.8%. Our project is available at https://github.com/Ap01lo/IMM-MOT.</article>","contentLength":1395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Science of Evaluating Foundation Models","url":"https://arxiv.org/abs/2502.09670","date":1739768400,"author":"","guid":1578,"unread":true,"content":"<article>arXiv:2502.09670v1 Announce Type: new \nAbstract: The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) Formalizing the Evaluation Process by providing a structured framework tailored to specific use-case contexts, (2) Offering Actionable Tools and Frameworks such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) Surveying Recent Work with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.</article>","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation","url":"https://arxiv.org/abs/2502.09669","date":1739768400,"author":"","guid":1579,"unread":true,"content":"<article>arXiv:2502.09669v1 Announce Type: new \nAbstract: Implicit neural representation (INR) has emerged as a promising solution for encoding volumetric data, offering continuous representations and seamless compatibility with the volume rendering pipeline. However, optimizing an INR network from randomly initialized parameters for each new volume is computationally inefficient, especially for large-scale time-varying or ensemble volumetric datasets where volumes share similar structural patterns but require independent training. To close this gap, we propose Meta-INR, a pretraining strategy adapted from meta-learning algorithms to learn initial INR parameters from partial observation of a volumetric dataset. Compared to training an INR from scratch, the learned initial parameters provide a strong prior that enhances INR generalizability, allowing significantly faster convergence with just a few gradient updates when adapting to a new volume and better interpretability when analyzing the parameters of the adapted INRs. We demonstrate that Meta-INR can effectively extract high-quality generalizable features that help encode unseen similar volume data across diverse datasets. Furthermore, we highlight its utility in tasks such as simulation parameter analysis and representative timestep selection. The code is available at https://github.com/spacefarers/MetaINR.</article>","contentLength":1374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"k-LLMmeans: Summaries as Centroids for Interpretable and Scalable LLM-Based Text Clustering","url":"https://arxiv.org/abs/2502.09667","date":1739768400,"author":"","guid":1580,"unread":true,"content":"<article>arXiv:2502.09667v1 Announce Type: new \nAbstract: We introduce k-LLMmeans, a novel modification of the k-means clustering algorithm that utilizes LLMs to generate textual summaries as cluster centroids, thereby capturing contextual and semantic nuances often lost when relying on purely numerical means of document embeddings. This modification preserves the properties of k-means while offering greater interpretability: the cluster centroid is represented by an LLM-generated summary, whose embedding guides cluster assignments. We also propose a mini-batch variant, enabling efficient online clustering for streaming text data and providing real-time interpretability of evolving cluster centroids. Through extensive simulations, we show that our methods outperform vanilla k-means on multiple metrics while incurring only modest LLM usage that does not scale with dataset size. Finally, We present a case study showcasing the interpretability of evolving cluster centroids in sequential text streams. As part of our evaluation, we compile a new dataset from StackExchange, offering a benchmark for text-stream clustering.</article>","contentLength":1124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revealing Subtle Phenotypes in Small Microscopy Datasets Using Latent Diffusion Models","url":"https://arxiv.org/abs/2502.09665","date":1739768400,"author":"","guid":1581,"unread":true,"content":"<article>arXiv:2502.09665v1 Announce Type: new \nAbstract: Identifying subtle phenotypic variations in cellular images is critical for advancing biological research and accelerating drug discovery. These variations are often masked by the inherent cellular heterogeneity, making it challenging to distinguish differences between experimental conditions. Recent advancements in deep generative models have demonstrated significant potential for revealing these nuanced phenotypes through image translation, opening new frontiers in cellular and molecular biology as well as the identification of novel biomarkers. Among these generative models, diffusion models stand out for their ability to produce high-quality, realistic images. However, training diffusion models typically requires large datasets and substantial computational resources, both of which can be limited in biological research. In this work, we propose a novel approach that leverages pre-trained latent diffusion models to uncover subtle phenotypic changes. We validate our approach qualitatively and quantitatively on several small datasets of microscopy images. Our findings reveal that our approach enables effective detection of phenotypic variations, capturing both visually apparent and imperceptible differences. Ultimately, our results highlight the promising potential of this approach for phenotype detection, especially in contexts constrained by limited data and computational capacity.</article>","contentLength":1456,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Image Super-Resolution with Guarantees via Conformal Generative Models","url":"https://arxiv.org/abs/2502.09664","date":1739768400,"author":"","guid":1582,"unread":true,"content":"<article>arXiv:2502.09664v1 Announce Type: new \nAbstract: The increasing use of generative ML foundation models for image super-resolution calls for robust and interpretable uncertainty quantification methods. We address this need by presenting a novel approach based on conformal prediction techniques to create a \"confidence mask\" capable of reliably and intuitively communicating where the generated image can be trusted. Our method is adaptable to any black-box generative model, including those locked behind an opaque API, requires only easily attainable data for calibration, and is highly customizable via the choice of a local image similarity metric. We prove strong theoretical guarantees for our method that span fidelity error control (according to our local image similarity metric), reconstruction quality, and robustness in the face of data leakage. Finally, we empirically evaluate these results and establish our method's solid performance.</article>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic Cellular Variations","url":"https://arxiv.org/abs/2502.09663","date":1739768400,"author":"","guid":1583,"unread":true,"content":"<article>arXiv:2502.09663v1 Announce Type: new \nAbstract: In recent years, deep learning models have been extensively applied to biological data across various modalities. Discriminative deep learning models have excelled at classifying images into categories (e.g., healthy versus diseased, treated versus untreated). However, these models are often perceived as black boxes due to their complexity and lack of interpretability, limiting their application in real-world biological contexts. In biological research, explainability is essential: understanding classifier decisions and identifying subtle differences between conditions are critical for elucidating the effects of treatments, disease progression, and biological processes. To address this challenge, we propose DiffEx, a method for generating visually interpretable attributes to explain classifiers and identify microscopic cellular variations between different conditions. We demonstrate the effectiveness of DiffEx in explaining classifiers trained on natural and biological images. Furthermore, we use DiffEx to uncover phenotypic differences within microscopy datasets. By offering insights into cellular variations through classifier explanations, DiffEx has the potential to advance the understanding of diseases and aid drug discovery by identifying novel biomarkers.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SIToBI - A Speech Prosody Annotation Tool for Indian Languages","url":"https://arxiv.org/abs/2502.09661","date":1739768400,"author":"","guid":1584,"unread":true,"content":"<article>arXiv:2502.09661v1 Announce Type: new \nAbstract: The availability of prosodic information from speech signals is useful in a wide range of applications. However, deriving this information from speech signals can be a laborious task involving manual intervention. Therefore, the current work focuses on developing a tool that can provide prosodic annotations corresponding to a given speech signal, particularly for Indian languages. The proposed Segmentation with Intensity, Tones and Break Indices (SIToBI) tool provides time-aligned phoneme, syllable, and word transcriptions, syllable-level pitch contour annotations, break indices, and syllable-level relative intensity indices. The tool focuses more on syllable-level annotations since Indian languages are syllable-timed. Indians, regardless of the language they speak, may exhibit influences from other languages. As a result, other languages spoken in India may also exhibit syllable-timed characteristics. The accuracy of the annotations derived from the tool is analyzed by comparing them against manual annotations and the tool is observed to perform well. While the current work focuses on three languages, namely, Tamil, Hindi, and Indian English, the tool can easily be extended to other Indian languages and possibly other syllable-timed languages as well.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Fine-grained Interactive Segmentation in Images and Videos","url":"https://arxiv.org/abs/2502.09660","date":1739768400,"author":"","guid":1585,"unread":true,"content":"<article>arXiv:2502.09660v1 Announce Type: new \nAbstract: The recent Segment Anything Models (SAMs) have emerged as foundational visual models for general interactive segmentation. Despite demonstrating robust generalization abilities, they still suffer performance degradations in scenarios demanding accurate masks. Existing methods for high-precision interactive segmentation face a trade-off between the ability to perceive intricate local details and maintaining stable prompting capability, which hinders the applicability and effectiveness of foundational segmentation models. To this end, we present an SAM2Refiner framework built upon the SAM2 backbone. This architecture allows SAM2 to generate fine-grained segmentation masks for both images and videos while preserving its inherent strengths. Specifically, we design a localization augment module, which incorporates local contextual cues to enhance global features via a cross-attention mechanism, thereby exploiting potential detailed patterns and maintaining semantic information. Moreover, to strengthen the prompting ability toward the enhanced object embedding, we introduce a prompt retargeting module to renew the embedding with spatially aligned prompt features. In addition, to obtain accurate high resolution segmentation masks, a mask refinement module is devised by employing a multi-scale cascaded structure to fuse mask features with hierarchical representations from the encoder. Extensive experiments demonstrate the effectiveness of our approach, revealing that the proposed method can produce highly precise masks for both images and videos, surpassing state-of-the-art methods.</article>","contentLength":1650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models","url":"https://arxiv.org/abs/2502.09659","date":1739768400,"author":"","guid":1586,"unread":true,"content":"<article>arXiv:2502.09659v1 Announce Type: new \nAbstract: Motivation: An adjuvant is a chemical incorporated into vaccines that enhances their efficacy by improving the immune response. Identifying adjuvant names from cancer vaccine studies is essential for furthering research and enhancing immunotherapies. However, the manual curation from the constantly expanding biomedical literature poses significant challenges. This study explores the automated recognition of vaccine adjuvant names using Large Language Models (LLMs), specifically Generative Pretrained Transformers (GPT) and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97 clinical trial records from AdjuvareDB and 290 abstracts annotated with the Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in zero-shot and few-shot learning paradigms with up to four examples per prompt. Prompts explicitly targeted adjuvant names, testing the impact of contextual information such as substances or interventions. Outputs underwent automated and manual validation for accuracy and consistency. Results: GPT-4o attained 100% Precision across all situations while exhibiting notable improve in Recall and F1-scores, particularly with incorporating interventions. On the VAC dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions, surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o reached an F1-score of 81.67% for three-shot prompting with interventions, surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings demonstrate that LLMs excel at identifying adjuvant names, including rare variations of naming representation. This study emphasizes the capability of LLMs to enhance cancer vaccine development by efficiently extracting insights. Future work aims to broaden the framework to encompass various biomedical literature and enhance model generalizability across various vaccines and adjuvants.</article>","contentLength":1961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality","url":"https://arxiv.org/abs/2502.09658","date":1739768400,"author":"","guid":1587,"unread":true,"content":"<article>arXiv:2502.09658v1 Announce Type: new \nAbstract: Knowledge representation and reasoning are critical challenges in Artificial Intelligence (AI), particularly in integrating neural and symbolic approaches to achieve explainable and transparent AI systems. Traditional knowledge representation methods often fall short of capturing complex processes and state changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a specialization of the neuro-symbolic AI approach that integrates conceptual modeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep learning to enhance question-answering (QA) quality. By converting natural language text into OPM models using in-context learning, NCAI leverages the expressive power of OPM to represent complex OPM elements-processes, objects, and states-beyond what traditional triplet-based knowledge graphs can easily capture. This rich structured knowledge representation improves reasoning transparency and answer accuracy in an OPM-QA system. We further propose transparency evaluation metrics to quantitatively measure how faithfully the predicted reasoning aligns with OPM-based conceptual logic. Our experiments demonstrate that NCAI outperforms traditional methods, highlighting its potential for advancing neuro-symbolic AI by providing rich knowledge representations, measurable transparency, and improved reasoning.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Spatiotemporal Vision Transformer into Digital Twins for High-Resolution Heat Stress Forecasting in Campus Environments","url":"https://arxiv.org/abs/2502.09657","date":1739768400,"author":"","guid":1588,"unread":true,"content":"<article>arXiv:2502.09657v1 Announce Type: new \nAbstract: Extreme heat events exacerbated by climate change pose significant challenges to urban resilience and planning. This study introduces a climate-responsive digital twin framework integrating the Spatiotemporal Vision Transformer (ST-ViT) model to enhance heat stress forecasting and decision-making. Using a Texas campus as a testbed, we synthesized high-resolution physical model simulations with spatial and meteorological data to develop fine-scale human thermal predictions. The ST-ViT-powered digital twin enables efficient, data-driven insights for planners, policymakers, and campus stakeholders, supporting targeted heat mitigation strategies and advancing climate-adaptive urban design.</article>","contentLength":743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bidirectional Diffusion Bridge Models","url":"https://arxiv.org/abs/2502.09655","date":1739768400,"author":"","guid":1589,"unread":true,"content":"<article>arXiv:2502.09655v1 Announce Type: new \nAbstract: Diffusion bridges have shown potential in paired image-to-image (I2I) translation tasks. However, existing methods are limited by their unidirectional nature, requiring separate models for forward and reverse translations. This not only doubles the computational cost but also restricts their practicality. In this work, we introduce the Bidirectional Diffusion Bridge Model (BDBM), a scalable approach that facilitates bidirectional translation between two coupled distributions using a single network. BDBM leverages the Chapman-Kolmogorov Equation for bridges, enabling it to model data distribution shifts across timesteps in both forward and backward directions by exploiting the interchangeability of the initial and target timesteps within this framework. Notably, when the marginal distribution given endpoints is Gaussian, BDBM's transition kernels in both directions possess analytical forms, allowing for efficient learning with a single network. We demonstrate the connection between BDBM and existing bridge methods, such as Doob's h-transform and variational approaches, and highlight its advantages. Extensive experiments on high-resolution I2I translation tasks demonstrate that BDBM not only enables bidirectional translation with minimal additional cost but also outperforms state-of-the-art bridge models. Our source code is available at [https://github.com/kvmduc/BDBM||https://github.com/kvmduc/BDBM].</article>","contentLength":1471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing","url":"https://arxiv.org/abs/2502.09652","date":1739768400,"author":"","guid":1590,"unread":true,"content":"<article>arXiv:2502.09652v1 Announce Type: new \nAbstract: This paper introduces a data-driven algorithm for modeling and compensating shape deviations in additive manufacturing (AM), addressing challenges in geometric accuracy and batch production. While traditional methods, such as analytical models and metrology, laid the groundwork for geometric precision, they are often impractical for large-scale production. Recent advancements in machine learning (ML) have improved compensation precision, but issues remain in generalizing across complex geometries and adapting to position-dependent variations. We present a novel approach for powder bed fusion (PBF) processes, using GraphCompNet, which is a computational framework combining graph-based neural networks with a generative adversarial network (GAN)-inspired training process. By leveraging point cloud data and dynamic graph convolutional neural networks (DGCNNs), GraphCompNet models complex shapes and incorporates position-specific thermal and mechanical factors. A two-stage adversarial training procedure iteratively refines compensated designs via a compensator-predictor architecture, offering real-time feedback and optimization. Experimental validation across diverse shapes and positions shows the framework significantly improves compensation accuracy (35 to 65 percent) across the entire print space, adapting to position-dependent variations. This work advances the development of Digital Twin technology for AM, enabling scalable, real-time monitoring and compensation, and addressing critical gaps in AM process control. The proposed method supports high-precision, automated industrial-scale design and manufacturing systems.</article>","contentLength":1694,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-VERDE: A Gateway for Egalitarian Access to Large Language Model-Based Resources For Educational Institutions","url":"https://arxiv.org/abs/2502.09651","date":1739768400,"author":"","guid":1591,"unread":true,"content":"<article>arXiv:2502.09651v1 Announce Type: new \nAbstract: We present AI-VERDE, a unified LLM-as-a-platform service designed to facilitate seamless integration of commercial, cloud-hosted, and on-premise open LLMs in academic settings. AI-VERDE streamlines access management for instructional and research groups by providing features such as robust access control, privacy-preserving mechanisms, native Retrieval-Augmented Generation (RAG) support, budget management for third-party LLM services, and both a conversational web interface and API access. In a pilot deployment at a large public university, AI-VERDE demonstrated significant engagement across diverse educational and research groups, enabling activities that would typically require substantial budgets for commercial LLM services with limited user and team management capabilities. To the best of our knowledge, AI-Verde is the first platform to address both academic and research needs for LLMs within an higher education institutional framework.</article>","contentLength":1003,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples","url":"https://arxiv.org/abs/2502.09650","date":1739768400,"author":"","guid":1592,"unread":true,"content":"<article>arXiv:2502.09650v1 Announce Type: new \nAbstract: The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Imit Diff: Semantics Guided Diffusion Transformer with Dual Resolution Fusion for Imitation Learning","url":"https://arxiv.org/abs/2502.09649","date":1739768400,"author":"","guid":1593,"unread":true,"content":"<article>arXiv:2502.09649v1 Announce Type: new \nAbstract: Visuomotor imitation learning enables embodied agents to effectively acquire manipulation skills from video demonstrations and robot proprioception. However, as scene complexity and visual distractions increase, existing methods that perform well in simple scenes tend to degrade in performance. To address this challenge, we introduce Imit Diff, a semanstic guided diffusion transformer with dual resolution fusion for imitation learning. Our approach leverages prior knowledge from vision language foundation models to translate high-level semantic instruction into pixel-level visual localization. This information is explicitly integrated into a multi-scale visual enhancement framework, constructed with a dual resolution encoder. Additionally, we introduce an implementation of Consistency Policy within the diffusion transformer architecture to improve both real-time performance and motion smoothness in embodied agent control.We evaluate Imit Diff on several challenging real-world tasks. Due to its task-oriented visual localization and fine-grained scene perception, it significantly outperforms state-of-the-art methods, especially in complex scenes with visual distractions, including zero-shot experiments focused on visual distraction and category generalization. The code will be made publicly available.</article>","contentLength":1369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UKTA: Unified Korean Text Analyzer","url":"https://arxiv.org/abs/2502.09648","date":1739768400,"author":"","guid":1594,"unread":true,"content":"<article>arXiv:2502.09648v1 Announce Type: new \nAbstract: Evaluating writing quality is complex and time-consuming often delaying feedback to learners. While automated writing evaluation tools are effective for English, Korean automated writing evaluation tools face challenges due to their inability to address multi-view analysis, error propagation, and evaluation explainability. To overcome these challenges, we introduce UKTA (Unified Korean Text Analyzer), a comprehensive Korea text analysis and writing evaluation system. UKTA provides accurate low-level morpheme analysis, key lexical features for mid-level explainability, and transparent high-level rubric-based writing scores. Our approach enhances accuracy and quadratic weighted kappa over existing baseline, positioning UKTA as a leading multi-perspective tool for Korean text analysis and writing evaluation.</article>","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unveiling Simplicities of Attention: Adaptive Long-Context Head Identification","url":"https://arxiv.org/abs/2502.09647","date":1739768400,"author":"","guid":1595,"unread":true,"content":"<article>arXiv:2502.09647v1 Announce Type: new \nAbstract: The ability to process long contexts is crucial for many natural language processing tasks, yet it remains a significant challenge. While substantial progress has been made in enhancing the efficiency of attention mechanisms, there is still a gap in understanding how attention heads function in long-context settings. In this paper, we observe that while certain heads consistently attend to local information only, others swing between attending to local and long-context information depending on the query. This raises the question: can we identify which heads require long-context information to predict the next token accurately? We demonstrate that it's possible to predict which heads are crucial for long-context processing using only local keys. The core idea here is to exploit a simple model for the long-context scores via second moment approximations. These findings unveil simple properties of attention in the context of long sequences, and open the door to potentially significant gains in efficiency.</article>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Shift or Maintenance? An Intergenerational Study of the Tibetan Community in Saudi Arabia","url":"https://arxiv.org/abs/2502.09646","date":1739768400,"author":"","guid":1596,"unread":true,"content":"<article>arXiv:2502.09646v1 Announce Type: new \nAbstract: The present study provides the first-ever report on the language shift from Tibetan to Arabic among descendants of Tibetan families who migrated from the Tibet region to Saudi Arabia around 70 years ago. The aim of this study was to determine whether three age groups had adopted different practices in terms of maintaining Tibetan or shifting to Hijazi Arabic. To this end, 96 male and female members of the Tibetan community responded to a questionnaire in which they were asked about their code choice in different domains (home, neighbourhood, friends and relatives, expressing emotion, and performing religious rituals). The data revealed significant intergenerational differences between members of the community in terms of the extent of the shift to Arabic, with Tibetan rarely used by younger members and older members making only slightly more use of it. The difference between the three age groups was significant, at a p-value of .001.</article>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models","url":"https://arxiv.org/abs/2502.09645","date":1739768400,"author":"","guid":1597,"unread":true,"content":"<article>arXiv:2502.09645v1 Announce Type: new \nAbstract: Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments.</article>","contentLength":1159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis","url":"https://arxiv.org/abs/2502.09644","date":1739768400,"author":"","guid":1598,"unread":true,"content":"<article>arXiv:2502.09644v1 Announce Type: new \nAbstract: Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in - as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer's stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.</article>","contentLength":1413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Krutrim LLM: Multilingual Foundational Model for over a Billion People","url":"https://arxiv.org/abs/2502.09642","date":1739768400,"author":"","guid":1599,"unread":true,"content":"<article>arXiv:2502.09642v1 Announce Type: new \nAbstract: India is a diverse society with unique challenges in developing AI systems, including linguistic diversity, oral traditions, data accessibility, and scalability. Existing foundation models are primarily trained on English, limiting their effectiveness for India's population. Indic languages comprise only 1 percent of Common Crawl corpora despite India representing 18 percent of the global population, leading to linguistic biases. Thousands of regional languages, dialects, and code mixing create additional representation challenges due to sparse training data.\n  We introduce Krutrim LLM, a 2 trillion token multilingual model designed for India's linguistic landscape. It incorporates the largest known Indic dataset, mitigating data scarcity and ensuring balanced performance across dialects. Krutrim outperforms or matches state-of-the-art models on Indic benchmarks while maintaining competitive English performance. Despite being significantly smaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2 on 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This evidences Krutrim's flexible multilingual fluency across diverse linguistic contexts.\n  Krutrim is integrated with real-time search to improve factual accuracy in conversational AI applications. This enhances accessibility for over 1 billion users worldwide. Through intentional design choices addressing data imbalances, Krutrim LLM signifies meaningful progress in building ethical, globally representative AI models.</article>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Social Support Detection in Spanish Social Media Texts","url":"https://arxiv.org/abs/2502.09640","date":1739768400,"author":"","guid":1600,"unread":true,"content":"<article>arXiv:2502.09640v1 Announce Type: new \nAbstract: The advent of social media has transformed communication, enabling individuals to share their experiences, seek support, and participate in diverse discussions. While extensive research has focused on identifying harmful content like hate speech, the recognition and promotion of positive and supportive interactions remain largely unexplored. This study proposes an innovative approach to detecting online social support in Spanish-language social media texts. We introduce the first annotated dataset specifically created for this task, comprising 3,189 YouTube comments classified as supportive or non-supportive. To address data imbalance, we employed GPT-4o to generate paraphrased comments and create a balanced dataset. We then evaluated social support classification using traditional machine learning models, deep learning architectures, and transformer-based models, including GPT-4o, but only on the unbalanced dataset. Subsequently, we utilized a transformer model to compare the performance between the balanced and unbalanced datasets. Our findings indicate that the balanced dataset yielded improved results for Task 2 (Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community, Women, Religion), whereas GPT-4o performed best for Task 1 (Social Support and Non-Support). This study highlights the significance of fostering a supportive online environment and lays the groundwork for future research in automated social support detection.</article>","contentLength":1516,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jailbreaking to Jailbreak","url":"https://arxiv.org/abs/2502.09638","date":1739768400,"author":"","guid":1601,"unread":true,"content":"<article>arXiv:2502.09638v1 Announce Type: new \nAbstract: Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as $J_2$ attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as $J_2$, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with $J_2$, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.</article>","contentLength":1336,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness","url":"https://arxiv.org/abs/2502.09637","date":1739768400,"author":"","guid":1602,"unread":true,"content":"<article>arXiv:2502.09637v1 Announce Type: new \nAbstract: Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, \"culture\" is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess \"cultural awareness\", and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?","url":"https://arxiv.org/abs/2502.09636","date":1739768400,"author":"","guid":1603,"unread":true,"content":"<article>arXiv:2502.09636v1 Announce Type: new \nAbstract: In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83\\% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: https://github.com/sougata-ub/reading_between_lines</article>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking","url":"https://arxiv.org/abs/2502.09635","date":1739768400,"author":"","guid":1604,"unread":true,"content":"<article>arXiv:2502.09635v1 Announce Type: new \nAbstract: Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method, Context- and Reference-augmented Reasoning and Prompting. For evidence reasoning, we construct a three-layer evidence graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to integrate three graph layers into a unified evidence embedding. For verdict prediction, we design evidence-conditioned prompt encoder, which produces unique prompt embeddings for each claim. These evidence-conditioned prompt embeddings and claims are unified for fact-checking. Experiments verify the strength of our model.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach","url":"https://arxiv.org/abs/2502.09624","date":1739768400,"author":"","guid":1605,"unread":true,"content":"<article>arXiv:2502.09624v1 Announce Type: new \nAbstract: By synergistically integrating mobile networks and embodied artificial intelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of vehicular messages or vulnerability to malicious tampering, potentially causing severe traffic accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the miner trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.</article>","contentLength":1865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["arxiv"]}