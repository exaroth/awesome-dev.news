{"id":"5EtLJWwbEiHVuC","title":"Kubernetes","displayTitle":"Kubernetes","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":4,"items":[{"title":"ChatLoopBackOff Episode 75: Exploring Trulent with Shivay Lamba","url":"https://www.youtube.com/watch?v=nia_qUP0Zu0","date":1769643877,"author":"CNCF [Cloud Native Computing Foundation]","guid":425604,"unread":true,"content":"<article>Join us LIVE as CNCF Ambassador Shivay Lamba explores TruLens for the very first time on ChatLoopBackOff!\n\nTruLens is an open source observability and evaluation framework designed to help teams understand, test, and improve Large Language Model (LLM) applications. As AI-powered systems become a core part of modern cloud-native platforms, TruLens provides critical insights into model behavior, quality, and trustworthiness—helping developers move beyond “black box” AI.\n\nIn this episode, Shivay will take a hands-on look at how TruLens works. Expect live experimentation, and an honest first look at where TruLens fits within the growing AI and cloud native ecosystem.\n\nIf you’re curious about LLM observability, responsible AI, or how open source communities are tackling AI evaluation challenges, this session is for you. Bring your questions, share your experiences, and learn alongside us as we explore TruLens together—live.</article>","contentLength":942,"flags":null,"enclosureUrl":"https://www.youtube.com/v/nia_qUP0Zu0?version=3","enclosureMime":"","commentsUrl":null},{"title":"Experimenting with Gateway API using kind","url":"https://kubernetes.io/blog/2026/01/28/experimenting-gateway-api-with-kind/","date":1769558400,"author":"","guid":424094,"unread":true,"content":"<p>This document will guide you through setting up a local experimental environment with <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API</a> on <a href=\"https://kind.sigs.k8s.io/\">kind</a>. This setup is designed for learning and testing. It helps you understand Gateway API concepts without production complexity.</p><div role=\"alert\">This is an experimentation learning setup, and should not be used for production. The components used on this document are not suited for production usage.\nOnce you're ready to deploy Gateway API in a production environment,\nselect an <a href=\"https://gateway-api.sigs.k8s.io/implementations/\">implementation</a> that suits your needs.</div><ul><li>Set up a local Kubernetes cluster using kind (Kubernetes in Docker)</li><li>Deploy <a href=\"https://github.com/kubernetes-sigs/cloud-provider-kind\">cloud-provider-kind</a>, which provides both LoadBalancer Services and a Gateway API controller</li><li>Create a Gateway and HTTPRoute to route traffic to a demo application</li><li>Test your Gateway API configuration locally</li></ul><p>This setup is ideal for learning, development, and experimentation with Gateway API concepts.</p><p>Before you begin, ensure you have the following installed on your local machine:</p><ul><li> - Required to run kind and cloud-provider-kind</li><li> - The Kubernetes command-line tool</li><li> - Kubernetes in Docker</li><li> - Required to test the routes</li></ul><p>Create a new kind cluster by running:</p><p>This will create a single-node Kubernetes cluster running in a Docker container.</p><h3>Install cloud-provider-kind</h3><ul><li>A LoadBalancer controller that assigns addresses to LoadBalancer-type Services</li><li>A Gateway API controller that implements the Gateway API specification</li></ul><p>It also automatically installs the Gateway API Custom Resource Definitions (CRDs) in your cluster.</p><p>Run cloud-provider-kind as a Docker container on the same host where you created the kind cluster:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p> On some systems, you may need elevated privileges to access the Docker socket.</p><p>Verify that cloud-provider-kind is running:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>You should see the container listed and in a running state. You can also check the logs:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><h2>Experimenting with Gateway API</h2><p>Now that your cluster is set up, you can start experimenting with Gateway API resources.</p><p>cloud-provider-kind automatically provisions a GatewayClass called . You'll use this class to create your Gateway.</p><p>It is worth noticing that while kind is not a cloud provider, the project is named as  as it provides features that simulate a cloud-enabled environment.</p><p>The following manifest will:</p><ul><li>Create a new namespace called </li><li>Deploy a Gateway that listens on port 80</li><li>Accept HTTPRoutes with hostnames matching the  pattern</li><li>Allow routes from any namespace to attach to the Gateway.\n: In real clusters, prefer Same or Selector values on the <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#fromnamespaces\"> namespace selector</a> field to limit attachments.</li></ul><p>Apply the following manifest:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Then verify that your Gateway is properly programmed and has an address assigned:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><pre tabindex=\"0\"><code>NAME CLASS ADDRESS PROGRAMMED AGE\ngateway cloud-provider-kind 172.18.0.3 True 5m6s\n</code></pre><p>The PROGRAMMED column should show True, and the ADDRESS field should contain an IP address.</p><h3>Deploy a demo application</h3><p>Next, deploy a simple echo application that will help you test your Gateway configuration. This application:</p><ul><li>Echoes back request details including path, headers, and environment variables</li><li>Runs in a namespace called </li></ul><p>Apply the following manifest:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Now create an HTTPRoute to route traffic from your Gateway to the echo application.\nThis HTTPRoute will:</p><ul><li>Respond to requests for the hostname <code>some.exampledomain.example</code></li><li>Route traffic to the echo application</li><li>Attach to the Gateway in the  namespace</li></ul><p>Apply the following manifest:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The final step is to test your route using curl. You'll make a request to the Gateway's IP address with the hostname <code>some.exampledomain.example</code>. The command below is for POSIX shell only, and may need to be adjusted for your environment:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>You should receive a JSON response similar to this:</p><div><pre tabindex=\"0\"><code data-lang=\"json\"></code></pre></div><p>If you see this response, congratulations! Your Gateway API setup is working correctly.</p><p>If something isn't working as expected, you can troubleshoot by checking the status of your resources.</p><p>First, inspect your Gateway resource:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Look at the  section for conditions. Your Gateway should have:</p><ul><li> - The Gateway was accepted by the controller</li><li> - The Gateway was successfully configured</li><li> populated with an IP address</li></ul><h3>Check the HTTPRoute status</h3><p>Next, inspect your HTTPRoute:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Check the  section for conditions. Common issues include:</p><ul><li>ResolvedRefs set to False with reason ; this means that the backend Service doesn't exist or has the wrong name</li><li>Accepted set to False; this means that the route couldn't attach to the Gateway (check namespace permissions or hostname matching)</li></ul><p>Example error when a backend is not found:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>If the resource statuses don't reveal the issue, check the cloud-provider-kind logs:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>This will show detailed logs from both the LoadBalancer and Gateway API controllers.</p><p>When you're finished with your experiments, you can clean up the resources:</p><h3>Remove Kubernetes resources</h3><p>Delete the namespaces (this will remove all resources within them):</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Stop and remove the cloud-provider-kind container:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Because the container was started with the  flag, it will be automatically removed when stopped.</p><p>Finally, delete the kind cluster:</p><p>Now that you've experimented with Gateway API locally, you're ready to explore production-ready implementations:</p><ul><li>: Explore the <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API documentation</a> to learn about advanced features like TLS, traffic splitting, and header manipulation</li><li>: Experiment with path-based routing, header matching, request mirroring and other features following <a href=\"https://gateway-api.sigs.k8s.io/guides/getting-started/\">Gateway API user guides</a></li></ul><p>This  setup is for development and learning only.\nAlways use a production-grade Gateway API implementation for real workloads.</p>","contentLength":5400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cluster API v1.12: Introducing In-place Updates and Chained Upgrades","url":"https://kubernetes.io/blog/2026/01/27/cluster-api-v1-12-release/","date":1769529600,"author":"","guid":424050,"unread":true,"content":"<p><a href=\"https://cluster-api.sigs.k8s.io/\">Cluster API</a> brings declarative management to Kubernetes cluster lifecycle, allowing users and platform teams to define the desired state of clusters and rely on controllers to continuously reconcile toward it.</p><p>Similar to how you can use StatefulSets or Deployments in Kubernetes to manage a group of Pods, in Cluster API you can use KubeadmControlPlane to manage a set of control plane Machines, or you can use MachineDeployments to manage a group of worker Nodes.</p><p>The <a href=\"https://github.com/kubernetes-sigs/cluster-api/releases/tag/v1.12.0\">Cluster API v1.12.0</a> release expands what is possible in Cluster API, reducing friction in common lifecycle operations by introducing in-place updates and chained upgrades.</p><h2>Emphasis on simplicity and usability</h2><p>With v1.12.0, the Cluster API project demonstrates once again that this community is capable of delivering a great amount of innovation, while at the same time minimizing impact for Cluster API users.</p><p>What does this mean in practice?</p><p>Users simply have to change the <a href=\"https://cluster-api.sigs.k8s.io/user/concepts#cluster\">Cluster</a> or the <a href=\"https://cluster-api.sigs.k8s.io/user/concepts#machine\">Machine</a> spec (just as with previous Cluster API releases), and Cluster API will automatically trigger in-place updates or chained upgrades when possible and advisable.</p><p>Like Kubernetes does for Pods in Deployments, when the <a href=\"https://cluster-api.sigs.k8s.io/user/concepts#machine\">Machine</a> spec changes also Cluster API performs rollouts by creating a new Machine and deleting the old one.</p><p>This approach, inspired by the principle of immutable infrastructure, has a set of considerable advantages:</p><ul><li>It is simple to explain, predictable, consistent and easy to reason about with users and engineers.</li><li>It is simple to implement, because it relies only on two core primitives, create and delete.</li><li>Implementation does not depend on Machine-specific choices, like OS, bootstrap mechanism etc.</li></ul><p>As a result, Machine rollouts drastically reduce the number of variables to be considered when managing the lifecycle of a host server that is hosting Nodes.</p><p>However, while advantages of immutability are not under discussion, both Kubernetes and Cluster API are undergoing a similar journey, introducing changes that allow users to minimize workload disruption whenever possible.</p><p>Over time, also Cluster API has introduced several improvements to immutable rollouts, including:</p><p>The new <a href=\"https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20240807-in-place-updates.md\">in-place update</a> feature in Cluster API is the next step in this journey.</p><p>With the v1.12.0 release, Cluster API introduces support for <a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/runtime-sdk/implement-in-place-update-hooks\">update extensions</a> allowing users to make changes on existing machines in-place, without deleting and re-creating the Machines.</p><p>Both KubeadmControlPlane and MachineDeployments support in-place updates based on the new update extension, and this means that the boundary of what is possible in Cluster API is now changed in a significant way.</p><p>How do in-place updates work?</p><p>The simplest way to explain it is that once the user triggers an update by changing the desired state of Machines, then Cluster API chooses the best tool to achieve the desired state.</p><p>The news is that now Cluster API can choose between immutable rollouts and in-place update extensions to perform required changes.</p><p>Importantly, this is not immutable rollouts vs in-place updates; Cluster API considers both valid options and selects the most appropriate mechanism for a given change.</p><p>From the perspective of the Cluster API maintainers, in-place updates are most useful for making changes that don't otherwise require a node drain or pod restart; for example: changing user credentials for the Machine. On the other hand, when the workload will be disrupted anyway, just do a rollout.</p><p>Nevertheless, Cluster API remains true to its extensible nature, and everyone can create their own update extension and decide when and how to use in-place updates by trading in some of the benefits of immutable rollouts.</p><p><a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/\">ClusterClass</a> and managed topologies in Cluster API jointly provided a powerful and effective framework that acts as a building block for many platforms offering Kubernetes-as-a-Service.</p><p>Now with v1.12.0 this feature is making another important step forward, by allowing users to upgrade by more than one Kubernetes minor version in a single operation, commonly referred to as a <a href=\"https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20250513-chained-and-efficient-upgrades-for-clusters-with-managed-topologies.md\">chained upgrade</a>.</p><p>This allows users to declare a target Kubernetes version and let Cluster API safely orchestrate the required intermediate steps, rather than manually managing each minor upgrade.</p><p>The simplest way to explain how chained upgrades work, is that once the user triggers an update by changing the desired version for a Cluster, Cluster API computes an upgrade plan, and then starts executing it. Rather than (for example) update the Cluster to v1.33.0 and then v1.34.0 and then v1.35.0, checking on progress at each step, a chained upgrade lets you go directly to v1.35.0.</p><p>Executing an upgrade plan means upgrading control plane and worker machines in a strictly controlled order, repeating this process as many times as needed to reach the desired state. The Cluster API is now capable of managing this for you.</p><p>Cluster API takes care of optimizing and minimizing the upgrade steps for worker machines, and in fact worker machines will skip upgrades to intermediate Kubernetes minor releases whenever allowed by the Kubernetes version skew policies.</p><p>Also in this case extensibility is at the core of this feature, and <a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/runtime-sdk/implement-upgrade-plan-hooks\">upgrade plan runtime extensions</a> can be used to influence how the upgrade plan is computed; similarly, <a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/runtime-sdk/implement-lifecycle-hooks\">lifecycle hooks</a> can be used to automate other tasks that must be performed during an upgrade, e.g. upgrading an addon after the control plane update completed.</p><p>From our perspective, chained upgrades are most useful for users that struggle to keep up with Kubernetes minor releases, and e.g. they want to upgrade only once per year and then upgrade by three versions (n-3 → n). But be warned: the fact that you can now easily upgrade by more than one minor version is not an excuse to not patch your cluster frequently!</p><p>I would like to thank all the contributors, the maintainers, and all the engineers that volunteered for the release team.</p><p>The reliability and predictability of Cluster API releases, which is one of the most appreciated features from our users, is only possible with the support, commitment, and hard work of its community.</p><p>Kudos to the entire Cluster API community for the v1.12.0 release and all the great releases delivered in 2025!\n​​\nIf you are interested in getting involved, learn about\n<a href=\"https://cluster-api.sigs.k8s.io/contributing\">Cluster API contributing guidelines</a>.</p><p>If you read the <a href=\"https://cluster-api.sigs.k8s.io/user/manifesto\">Cluster API manifesto</a>, you can see how the Cluster API subproject claims the right to remain unfinished, recognizing the need to continuously evolve, improve, and adapt to the changing needs of Cluster API’s users and the broader Cloud Native ecosystem.</p><p>As Kubernetes itself continues to evolve, the Cluster API subproject will keep advancing alongside it, focusing on safer upgrades, reduced disruption, and stronger building blocks for platforms managing Kubernetes at scale.</p><p>Innovation remains at the heart of Cluster API, stay tuned for an exciting 2026!</p>","contentLength":6886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neurodiversity Meeting - January 2026","url":"https://www.youtube.com/watch?v=2-ub9F64PxM","date":1769109116,"author":"CNCF [Cloud Native Computing Foundation]","guid":418395,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon events in Amsterdam, The Netherlands (23-26 March, 2026). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":334,"flags":null,"enclosureUrl":"https://www.youtube.com/v/2-ub9F64PxM?version=3","enclosureMime":"","commentsUrl":null}],"tags":["k8s"]}