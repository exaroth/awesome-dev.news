{"id":"5EtLJWwbEiHVuC","title":"Kubernetes","displayTitle":"Kubernetes","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":25,"items":[{"title":"Busting Myths About Cisco 700-150 ICS!","url":"https://www.reddit.com/r/kubernetes/comments/1iq3umq/busting_myths_about_cisco_700150_ics/","date":1739633610,"author":"/u/lucina_scott","guid":543,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Career transition in to Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/","date":1739626865,"author":"/u/Similar-Secretary-86","guid":544,"unread":true,"content":"<p>\"I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated </p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My new blog post comparing networking in EKS vs. GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/","date":1739617569,"author":"/u/jumiker","guid":545,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Dive into VPA Recommender","url":"https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/","date":1739615164,"author":"/u/erik_zilinsky","guid":546,"unread":true,"content":"<p>I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.</p><p>Based on my findings, I wrote a <a href=\"https://erikzilinsky.com/posts/vpa1.html\">blog post</a> about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.</p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container Networking - Kubernetes with Calico","url":"https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/","date":1739604355,"author":"/u/tkr_2020","guid":542,"unread":true,"content":"<ul><li>: VLAN 10</li><li>: VLAN 20</li></ul><p>When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:</p><p>The inner IP header reflects:</p><p>The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.</p><p>Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?</p>","contentLength":502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/","date":1739555053,"author":"/u/trouphaz","guid":547,"unread":true,"content":"<p>I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.</p><p>So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.</p><pre><code>sc() { CLUSTER_NAME=\"${1}\" export KUBECONFIG=\"~/.kube/${CLUSTER_NAME}\" } </code></pre>","contentLength":592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11","url":"https://www.youtube.com/watch?v=eHa6GhK7L0I","date":1739541570,"author":"CNCF [Cloud Native Computing Foundation]","guid":651,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eHa6GhK7L0I?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 46 (Dragonfly)","url":"https://www.youtube.com/watch?v=gd6HRgr8KcA","date":1739512616,"author":"CNCF [Cloud Native Computing Foundation]","guid":650,"unread":true,"content":"<article>Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. \n\nThis CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.</article>","contentLength":576,"flags":null,"enclosureUrl":"https://www.youtube.com/v/gd6HRgr8KcA?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1739491200,"author":"","guid":753,"unread":true,"content":"<p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the \nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial  is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><div>sequenceDiagram\nautonumber\nrect rgb(191, 223, 255)\nKubelet-&gt;&gt;+Kube-apiserver: Create Node\nNote over Kubelet: Taint: node.cloudprovider.kubernetes.io\nKube-apiserver-&gt;&gt;-Kubelet: Node Created\nend\nNote over Kube-apiserver: Node is Not Ready<p> Tainted, Missing Node Addresses*, ...\nNote over Kube-apiserver: Send Updates\nrect rgb(200, 150, 255)\nKube-apiserver-&gt;&gt;+Cloud-controller-manager: Watch: New Node Created\nNote over Cloud-controller-manager: Initialize Node:</p>Cloud Provider Labels, Node Addresses, ...\nCloud-controller-manager-&gt;&gt;-Kube-apiserver: Update Node\nend\nNote over Kube-apiserver: Node is Ready\n</div><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a  resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a  resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one “correct way” to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting “hostNetwork” to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider’s instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure’s node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.\n<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider’s documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nname: cloud-controller-manager\nnamespace: kube-system\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nannotations:\nkubernetes.io/description: Cloud controller manager for my infrastructure\nspec:\ncontainers: # the container details will depend on your specific cloud controller manager\n- name: cloud-controller-manager\ncommand:\n- /bin/my-infrastructure-cloud-controller-manager\n- --leader-elect=true\n- -v=1\nimage: registry/my-infrastructure-cloud-controller-manager@latest\nresources:\nrequests:\ncpu: 200m\nmemory: 50Mi\nhostNetwork: true # these Pods are part of the control plane\nnodeSelector:\nnode-role.kubernetes.io/control-plane: \"\"\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- topologyKey: \"kubernetes.io/hostname\"\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\ntolerations:\n- effect: NoSchedule\nkey: node-role.kubernetes.io/master\noperator: Exists\n- effect: NoExecute\nkey: node.kubernetes.io/unreachable\noperator: Exists\ntolerationSeconds: 120\n- effect: NoExecute\nkey: node.kubernetes.io/not-ready\noperator: Exists\ntolerationSeconds: 120\n- effect: NoSchedule\nkey: node.cloudprovider.kubernetes.io/uninitialized\noperator: Exists\n- effect: NoSchedule\nkey: node.kubernetes.io/not-ready\noperator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes History Inspector, with Kakeru Ishii","url":"http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii","date":1739445780,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":653,"unread":true,"content":"<p dir=\"ltr\">Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week ","contentLength":341,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Sandbox environments: Creating efficient and isolated testing realms","url":"https://www.youtube.com/watch?v=fh7-lQVmX-o","date":1739426433,"author":"CNCF [Cloud Native Computing Foundation]","guid":649,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fh7-lQVmX-o?version=3","enclosureMime":"","commentsUrl":null},{"title":"KitOps: AI Model Packaging Standards","url":"https://www.youtube.com/watch?v=1TD-e_wVe4Q","date":1739426400,"author":"CNCF [Cloud Native Computing Foundation]","guid":648,"unread":true,"content":"<article>Chat with us on Discord:  https://discord.gg/Tapeh8agYy\n\nCheck out our repos:\nKitOps      https://github.com/jozu-ai/kitops\nPyKitOps Python Library  https://github.com/jozu-ai/pykitops\nKitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin</article>","contentLength":253,"flags":null,"enclosureUrl":"https://www.youtube.com/v/1TD-e_wVe4Q?version=3","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing Kyverno policy enforcement performance for large clusters","url":"https://www.youtube.com/watch?v=DWmCAUCs3bc","date":1739211188,"author":"CNCF [Cloud Native Computing Foundation]","guid":647,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DWmCAUCs3bc?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 45 (CRI-O)","url":"https://www.youtube.com/watch?v=--eJZu3Zkbw","date":1738909199,"author":"CNCF [Cloud Native Computing Foundation]","guid":646,"unread":true,"content":"<article>CRI-O, a CNCF Graduated project, is designed to be lightweight, focusing solely on Kubernetes needs’, reducing resource overhead compared to general-purpose container runtimes. \n\nIt improves security by leveraging OCI (Open Container Initiative) standards, integrating seamlessly with tools like seccomp, SELinux, and AppArmor for enhanced isolation and compliance. With robust support for OCI-compliant image formats and runtimes like runc, CRI-O ensures compatibility with a wide range of container images. Join CNCF Ambassador Marvin Beckers test CRI-O’s efficiency, as it minimizes the feature set to what Kubernetes requires, leading to faster startup times and reduced complexity.</article>","contentLength":690,"flags":null,"enclosureUrl":"https://www.youtube.com/v/--eJZu3Zkbw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Cloud Native Live: What's latest in KubeArmor v1.5","url":"https://www.youtube.com/watch?v=OUNEu3h2V3c","date":1738821857,"author":"CNCF [Cloud Native Computing Foundation]","guid":645,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/OUNEu3h2V3c?version=3","enclosureMime":"","commentsUrl":null},{"title":"AI for observability","url":"https://www.youtube.com/watch?v=IIz8Xpyebug","date":1738821619,"author":"CNCF [Cloud Native Computing Foundation]","guid":644,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/IIz8Xpyebug?version=3","enclosureMime":"","commentsUrl":null},{"title":"TOC Meeting 2025-02-04","url":"https://www.youtube.com/watch?v=R3H_ceqR6Us","date":1738707999,"author":"CNCF [Cloud Native Computing Foundation]","guid":643,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/R3H_ceqR6Us?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 44 (k3s)","url":"https://www.youtube.com/watch?v=6vYfJ6MM9_o","date":1738302651,"author":"CNCF [Cloud Native Computing Foundation]","guid":642,"unread":true,"content":"<article>K3s, a CNCF Sandbox project, is a lightweight, certified Kubernetes distribution designed for simplicity, minimal resource consumption, and ease of use. Its small binary size, pre-bundled dependencies, and single-command installation make it ideal for edge computing, IoT, and development environments. \n\nK3s feature low hardware requirements, support for ARM devices, and optimized performance, so it runs seamlessly on resource-constrained systems. Join CNCF Ambassador Aditya Soni while he explores how K3s ensures compatibility with Kubernetes tools while lowering the complexity and cost of deployment.</article>","contentLength":607,"flags":null,"enclosureUrl":"https://www.youtube.com/v/6vYfJ6MM9_o?version=3","enclosureMime":"","commentsUrl":null},{"title":"Deaf and Hard of Hearing WG Meeting - 2025-01-28","url":"https://www.youtube.com/watch?v=Rndt_zS6v_w","date":1738249798,"author":"CNCF [Cloud Native Computing Foundation]","guid":641,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Rndt_zS6v_w?version=3","enclosureMime":"","commentsUrl":null},{"title":"Linkerd, with William Morgan","url":"http://sites.libsyn.com/419861/linkerd-with-william-morgan","date":1738110300,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":652,"unread":true,"content":"<p dir=\"ltr\"><a href=\"https://www.linkedin.com/in/wmorgan/\">William Morgan</a> is the CEO of <a href=\"https://buoyant.io/\">Buoyant</a>, the company behind Linkerd. You worked at Twitter before as a software engineer and engineering manager and you have a long experience in the field.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week  Links from the interview ","contentLength":295,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPod246.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"ASL Container","url":"https://www.youtube.com/watch?v=meUtsFU7ndo","date":1737998838,"author":"CNCF [Cloud Native Computing Foundation]","guid":640,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/meUtsFU7ndo?version=3","enclosureMime":"","commentsUrl":null},{"title":"ASL Autoscaling 2","url":"https://www.youtube.com/watch?v=cGONmC1smaM","date":1737997887,"author":"CNCF [Cloud Native Computing Foundation]","guid":639,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/cGONmC1smaM?version=3","enclosureMime":"","commentsUrl":null},{"title":"ASL Reliability","url":"https://www.youtube.com/watch?v=pQluo2FG2eA","date":1737997855,"author":"CNCF [Cloud Native Computing Foundation]","guid":638,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/pQluo2FG2eA?version=3","enclosureMime":"","commentsUrl":null},{"title":"ASL Serverless 2","url":"https://www.youtube.com/watch?v=rbyBgXqCN2k","date":1737997826,"author":"CNCF [Cloud Native Computing Foundation]","guid":637,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/rbyBgXqCN2k?version=3","enclosureMime":"","commentsUrl":null},{"title":"Spotlight on SIG Architecture: Enhancements","url":"https://kubernetes.io/blog/2025/01/21/sig-architecture-enhancements/","date":1737417600,"author":"","guid":752,"unread":true,"content":"<p><em>This is the fourth interview of a SIG Architecture Spotlight series that will cover the different\nsubprojects, and we will be covering <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/README.md#enhancements\">SIG Architecture:\nEnhancements</a>.</em></p><p>In this SIG Architecture spotlight we talked with <a href=\"https://github.com/kikisdeliveryservice\">Kirsten\nGarrison</a>, lead of the Enhancements subproject.</p><h2>The Enhancements subproject</h2><p><strong>Frederico (FSM): Hi Kirsten, very happy to have the opportunity to talk about the Enhancements\nsubproject. Let's start with some quick information about yourself and your role.</strong></p><p>: I’m a lead of the Enhancements subproject of SIG-Architecture and\ncurrently work at Google. I first got involved by contributing to the service-catalog project with\nthe help of <a href=\"https://github.com/carolynvs\">Carolyn Van Slyck</a>. With time, <a href=\"https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md\">I joined the Release\nteam</a>,\neventually becoming the Enhancements Lead and a Release Lead shadow. While on the release team, I\nworked on some ideas to make the process better for the SIGs and Enhancements team (the opt-in\nprocess) based on my team’s experiences. Eventually, I started attending Subproject meetings and\ncontributing to the Subproject’s work.</p><p><strong>FSM: You mentioned the Enhancements subproject: how would you describe its main goals and areas of\nintervention?</strong></p><p><strong>FSM: The improvement of the KEP process was (and is) one in which SIG Architecture was heavily\ninvolved. Could you explain the process to those that aren’t aware of it?</strong></p><p>: <a href=\"https://kubernetes.io/releases/release/#the-release-cycle\">Every release</a>, the SIGs let the\nRelease Team know which features they intend to work on to be put into the release. As mentioned\nabove, the prerequisite for these changes is a KEP - a standardized design document that all authors\nmust fill out and approve in the first weeks of the release cycle. Most features <a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages\">will move\nthrough 3\nphases</a>:\nalpha, beta and finally GA so approving a feature represents a significant commitment for the SIG.</p><p>The KEP serves as the full source of truth of a feature. The <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/NNNN-kep-template/README.md\">KEP\ntemplate</a>\nhas different requirements based on what stage a feature is in, but it generally requires a detailed\ndiscussion of the design and the impact as well as providing artifacts of stability and\nperformance. The KEP takes quite a bit of iterative work between authors, SIG reviewers, api review\nteam and the Production Readiness Review team before it is approved. Each set of reviewers is\nlooking to make sure that the proposal meets their standards in order to have a stable and\nperformant Kubernetes release. Only after all approvals are secured, can an author go forth and\nmerge their feature in the Kubernetes code base.</p><p><strong>FSM: I see, quite a bit of additional structure was added. Looking back, what were the most\nsignificant improvements of that approach?</strong></p><p>: In general, I think that the improvements with the most impact had to do with focusing on\nthe core intent of the KEP. KEPs exist not just to memorialize designs, but provide a structured way\nto discuss and come to an agreement about different facets of the change. At the core of the KEP\nprocess is communication and consideration.</p><p>To that end, some of the significant changes revolve around a more detailed and accessible KEP\ntemplate. A significant amount of work was put in over time to get the\n<a href=\"https://github.com/kubernetes/enhancements\">k/enhancements</a> repo into its current form -- a\ndirectory structure organized by SIG with the contours of the modern KEP template (with\nProposal/Motivation/Design Details subsections). We might take that basic structure for granted\ntoday, but it really represents the work of many people trying to get the foundation of this process\nin place over time.</p><p>As Kubernetes matures, we’ve needed to think about more than just the end goal of getting a single\nfeature merged. We need to think about things like: stability, performance, setting and meeting user\nexpectations. And as we’ve thought about those things the template has grown more detailed. The\naddition of the Production Readiness Review was major as well as the enhanced testing requirements\n(varying at different stages of a KEP’s lifecycle).</p><p>: We’re currently working on two things:</p><ol><li><em>Creating a Process KEP template.</em> Sometimes people want to harness the KEP process for\nsignificant changes that are more process oriented rather than feature oriented. We want to\nsupport this because memorializing changes is important and giving people a better tool to do so\nwill only encourage more discussion and transparency.</li><li> While our template changes aim to be as non-disruptive as possible, we\nbelieve that it will be easier to track and communicate those changes to the community better with\na versioned KEP template and the policies that go alongside such versioning.</li></ol><p>Both features will take some time to get right and fully roll out (just like a KEP feature) but we\nbelieve that they will both provide improvements that will benefit the community at large.</p><p><strong>FSM: You mentioned improvements: I remember when project boards for Enhancement tracking were\nintroduced in recent releases, to great effect and unanimous applause from release team members. Was\nthis a particular area of focus for the subproject?</strong></p><p>: The Subproject provided support to the Release Team’s Enhancement team in the migration away\nfrom using the spreadsheet to a project board. The collection and tracking of enhancements has\nalways been a logistical challenge. During my time on the Release Team, I helped with the transition\nto an opt-in system of enhancements, whereby the SIG leads \"opt-in\" KEPs for release tracking. This\nhelped to enhance communication between authors and SIGs before any significant work was undertaken\non a KEP and removed toil from the Enhancements team. This change used the existing tools to avoid\nintroducing too many changes at once to the community. Later, the Release Team approached the\nSubproject with an idea of leveraging GitHub Project Boards to further improve the collection\nprocess. This was to be a move away from the use of complicated spreadsheets to using repo-native\nlabels on <a href=\"https://github.com/kubernetes/enhancements\">k/enhancement</a> issues and project boards.</p><p><strong>FSM: That surely adds an impact on simplifying the workflow...</strong></p><p>: Removing sources of friction and promoting clear communication is very important to the\nEnhancements Subproject. At the same time, it’s important to give careful consideration to\ndecisions that impact the community as a whole. We want to make sure that changes are balanced to\ngive an upside and while not causing any regressions and pain in the rollout. We supported the\nRelease Team in ideation as well as through the actual migration to the project boards. It was a\ngreat success and exciting to see the team make high impact changes that helped everyone involved in\nthe KEP process!</p><p><strong>FSM: For those reading that might be curious and interested in helping, how would you describe the\nrequired skills for participating in the sub-project?</strong></p><p>: Familiarity with KEPs either via experience or taking time to look through the\nkubernetes/enhancements repo is helpful. All are welcome to participate if interested - we can take\nit from there.</p><p><strong>FSM: Excellent! Many thanks for your time and insight -- any final comments you would like to\nshare with our readers?</strong></p><p>: The Enhancements process is one of the most important parts of Kubernetes and requires\nenormous amounts of coordination and collaboration of people and teams across the project to make it\nsuccessful. I’m thankful and inspired by everyone’s continued hard work and dedication to making the\nproject great. This is truly a wonderful community.</p>","contentLength":7334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["k8s"]}