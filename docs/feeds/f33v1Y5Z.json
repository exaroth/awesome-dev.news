{"id":"f33v1Y5Z","title":"Latest","displayTitle":"Latest","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":871,"items":[{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-126/","date":1739782821,"author":"Mike Vizard","guid":1674,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mixture of Experts Makes Text Models Smarter: New Research Shows Better Language Understanding","url":"https://dev.to/mikeyoung44/mixture-of-experts-makes-text-models-smarter-new-research-shows-better-language-understanding-2bji","date":1739782574,"author":"Mike Young","guid":1692,"unread":true,"content":"<ul><li>Research explores combining Mixture of Experts (MoE) with text embeddings</li><li>Focuses on improving multilingual capabilities in language models</li><li>Addresses efficiency and quality trade-offs in text representation</li><li>Examines specialized expert networks for different language tasks</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p>Text embedding models turn words and sentences into numbers that computers can understand. Think of it like translating languages - each word gets converted into a special code. But doing this well for many languages at once is hard.</p><p>This paper suggests using a [mixture of exp...</p>","contentLength":583,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Models' Reasoning Skills Don't Easily Transfer to Finance, Study Shows","url":"https://dev.to/mikeyoung44/ai-models-reasoning-skills-dont-easily-transfer-to-finance-study-shows-4b9a","date":1739782537,"author":"Mike Young","guid":1691,"unread":true,"content":"<ul><li>Research evaluates transfer of reasoning capabilities from general-purpose Large Language Models (LLMs) to finance domain</li><li>Tests models fine-tuned on reasoning tasks in financial applications</li><li>Introduces Fino1 benchmark for financial reasoning assessment </li><li>Examines performance of models like GPT-4, Claude and PaLM</li><li>Shows limitations in direct transfer of reasoning skills to finance</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p><a href=\"https://aimodels.fyi/papers/arxiv/fine-tuning-smaller-language-models-question-answering\" rel=\"noopener noreferrer\">Fine-tuning language models</a> for reasoning doesn't automatically make them better at financial tasks. Think of it like teaching someone general problem-solving skills - just because they'...</p>","contentLength":600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI vs. Detective: How Well Can Language Models Solve Murder Mysteries?","url":"https://dev.to/mikeyoung44/ai-vs-detective-how-well-can-language-models-solve-murder-mysteries-aif","date":1739782501,"author":"Mike Young","guid":1690,"unread":true,"content":"<ul><li>New benchmark dataset called  for testing AI systems on mystery story comprehension</li><li>Contains 200 carefully curated mystery stories with identified culprits </li><li>Tests language models' ability to identify perpetrators and follow complex narratives</li><li>Evaluates both direct culprit detection and reasoning about evidence</li><li>Performance tested across multiple large language models like GPT-4 and Claude</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p><a href=\"https://aimodels.fyi/papers/arxiv/whodunit-evaluation-benchmark-culprit-detection-mystery-stories\" rel=\"noopener noreferrer\">Mystery story analysis</a> presents a unique challenge for artificial intelligence. Much like how humans piece together clues to solve a mystery, AI systems need to track characters...</p>","contentLength":601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Breakthrough: Universal Translator Links Images and 100+ Languages with Record Accuracy","url":"https://dev.to/mikeyoung44/ai-breakthrough-universal-translator-links-images-and-100-languages-with-record-accuracy-31hk","date":1739782465,"author":"Mike Young","guid":1689,"unread":true,"content":"<ul><li>New  model improves multilingual and multimodal embeddings using synthetic data</li><li>Creates high-quality training pairs across 100+ languages and image-text combinations</li><li>Achieves state-of-the-art performance on cross-lingual and cross-modal retrieval tasks</li><li>Uses text-to-text and image-to-text generation to expand training data</li><li>Builds on previous E5 embedding models with enhanced multilingual capabilities</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p>The mmE5 system tackles a common challenge in AI - making computers understand connections between different languages and images. Think of it like teaching a computer to be a universal translator that can match pictures with descriptions in any language.</p>","contentLength":688,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New AI Safety System Improves Language Model Safety by 25% Without Retraining","url":"https://dev.to/mikeyoung44/new-ai-safety-system-improves-language-model-safety-by-25-without-retraining-580d","date":1739782429,"author":"Mike Young","guid":1688,"unread":true,"content":"<ul><li>MetaSC optimizes AI safety specifications during runtime</li><li>Addresses challenge of making language models safer without retraining</li><li>Uses gradient-based optimization to improve safety constraints</li><li>Demonstrates effectiveness across multiple language models and safety benchmarks</li><li>Achieves 20-30% improvement in safety metrics while maintaining model performance</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p>Think of MetaSC like installing guardrails on a highway while cars are still driving. Traditional AI safety methods require rebuilding the entire road, but MetaSC adds safety features while the AI system runs.</p><p>The system works by continuously checking if the AI's responses mig...</p>","contentLength":663,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Audio Defense Blocks 98% of Voice Deepfakes While Maintaining Natural Speech Quality","url":"https://dev.to/mikeyoung44/new-audio-defense-blocks-98-of-voice-deepfakes-while-maintaining-natural-speech-quality-3oh4","date":1739782393,"author":"Mike Young","guid":1687,"unread":true,"content":"<ul><li>VocalCrypt protects voice recordings from deepfake manipulation</li><li>Uses acoustic masking to disrupt AI voice cloning</li><li>Maintains audio quality while preventing unauthorized copying</li><li>Achieves 98% success rate in blocking voice synthesis attacks</li><li>Preserves natural speech intelligibility for human listeners</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p><a href=\"https://aimodels.fyi/papers/arxiv/can-deepfake-speech-be-reliably-detected\" rel=\"noopener noreferrer\">Voice deepfakes</a> have become a serious security threat. Bad actors can clone someone's voice from just a few seconds of audio. VocalCrypt offers a solution by adding subtle acoustic masking to voice re...</p>","contentLength":532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Creates Cinematic Videos from Text with Advanced 3D Camera Control","url":"https://dev.to/mikeyoung44/ai-creates-cinematic-videos-from-text-with-advanced-3d-camera-control-onk","date":1739782356,"author":"Mike Young","guid":1686,"unread":true,"content":"<ul><li>Introduces , a new AI framework for creating cinematic videos from text descriptions</li><li>Combines 3D awareness with precise camera and motion control </li><li>Generates high-quality videos with consistent camera movements</li><li>Uses novel multi-stage architecture for better temporal consistency</li><li>Achieves state-of-the-art results in text-to-video generation</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p>CineMaster works like a virtual movie director. When given a text description, it first creates a mental picture of the 3D scene, then plans how the camera should move around it, and finally generates a smooth video that follows this plan.</p><p>Think of it like planning a movie sho...</p>","contentLength":649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Breakthrough Reduces Bias in Medical Survival Predictions by 40%","url":"https://dev.to/mikeyoung44/ai-breakthrough-reduces-bias-in-medical-survival-predictions-by-40-3lpe","date":1739782320,"author":"Mike Young","guid":1685,"unread":true,"content":"<ul><li>New method for handling censored data in survival analysis and medical research</li><li>Addresses bias in traditional statistical approaches when censoring depends on outcomes</li><li>Introduces <strong>Censor Dependent Variational Inference (CDVI)</strong> framework</li><li>Improves prediction accuracy for patient survival times and treatment outcomes</li><li>Combines deep learning with statistical theory for robust modeling</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p>Medical researchers often study how long patients survive after treatment. Sometimes they lose track of patients before knowing their final outcome - this is called censoring. Traditional methods assume this loss of follow-up happens randomly, but in reality, sicker patients mi...</p>","contentLength":692,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hidden Image Generation Powers Found Inside AI Recognition Systems","url":"https://dev.to/mikeyoung44/hidden-image-generation-powers-found-inside-ai-recognition-systems-ojb","date":1739782284,"author":"Mike Young","guid":1684,"unread":true,"content":"<ul><li>Novel technique called Direct Ascent Synthesis (DAS) extracts generative capabilities from discriminative models</li><li>Transforms discriminative neural networks into image generators without additional training</li><li>Achieves high-quality image synthesis through gradient-based optimization</li><li>Works with pre-trained classification networks and vision transformers</li><li>Demonstrates competitive results compared to traditional generative models</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p><a href=\"https://aimodels.fyi/papers/arxiv/direct-ascent-synthesis-revealing-hidden-generative-capabilities\" rel=\"noopener noreferrer\">Direct Ascent Synthesis</a> is like discovering that your microscope can also work as a projector. The research shows that neural networks trained to recognize images can also crea...</p>","contentLength":634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Image Generation 30-50% Faster with New Adaptive Sampling Method","url":"https://dev.to/mikeyoung44/ai-image-generation-30-50-faster-with-new-adaptive-sampling-method-4kfj","date":1739782248,"author":"Mike Young","guid":1683,"unread":true,"content":"<ul><li>New adaptive sampling method for diffusion transformers </li><li><strong>Region-Adaptive Sampling (RAS)</strong> dynamically adjusts attention based on image regions</li><li>Reduces computation costs by 30-50% without quality loss</li><li>Works with various diffusion transformer architectures</li><li>Improves efficiency through selective attention allocation</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p>Imagine a painter who focuses more attention on detailed areas of a canvas while using broader strokes for simpler regions. <a href=\"https://aimodels.fyi/papers/arxiv/region-adaptive-sampling-diffusion-transformers\" rel=\"noopener noreferrer\">Region-Adaptive Sampling</a> works similarly for AI image generation. The...</p>","contentLength":538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Guide on Healthcare e-Commerce: Features, Challenges & Opportunities","url":"https://dev.to/phyniks/guide-on-healthcare-e-commerce-features-challenges-opportunities-23em","date":1739782237,"author":"Phyniks","guid":1682,"unread":true,"content":"<p>The healthcare industry is at a crossroads. As consumer behavior shifts towards convenience and personalization, traditional healthcare providers are finding it harder to keep up with expectations. For startups eyeing a slice of this market, healthcare e-commerce offers a game-changing opportunity. But with great opportunities come challenges like compliance, trust, and logistics. How can startups navigate this complex yet rewarding landscape?</p><h2>\n  \n  \n  What is Healthcare E-Commerce?\n</h2><p>Healthcare e-commerce refers to the online marketplace where medical products, services, and healthcare solutions are transacted. From purchasing prescription medicines to booking telemedicine appointments, it covers a wide spectrum of services.</p><p>Unlike other sectors, e-commerce in healthcare doesn’t just prioritize convenience; it’s about accessibility, trust, and improving health outcomes. Examples of successful healthcare e-commerce platforms include:</p><ul><li>1mg and Medlife for medicine delivery.</li><li>Zocdoc and Practo for telemedicine consultations.</li><li>HealthKart for wellness and fitness products.</li></ul><p>The growth of <a href=\"https://phyniks.com/application-development-services-company\" rel=\"noopener noreferrer\">healthcare custom apps</a> has further propelled this sector, enabling startups to deliver personalized and efficient healthcare solutions.</p><h3>\n  \n  \n  Why Startups Should Care About Healthcare E-Commerce\n</h3><p>In many developed countries, healthcare systems are saturated but often outdated. In developing nations, access to quality healthcare remains a struggle. Healthcare e-commerce bridges these gaps by providing scalable, tech-driven solutions.</p><p>Today's patients are informed and prefer convenience. They expect to order medical products or consult doctors from their smartphones. This demand has fueled the rise of <a href=\"https://phyniks.com/application-development-services-company\" rel=\"noopener noreferrer\">healthcare custom apps</a> tailored for these needs.</p><p>The global healthcare e-commerce market is growing exponentially, driven by innovations in telemedicine and health product marketplaces. Startups that act now can ride this wave and establish themselves as industry leaders.</p><h3>\n  \n  \n  Key Categories of Healthcare E-Commerce Firms\n</h3><p><strong>1. Online Pharmacies: Transforming Medicine Distribution</strong></p><p>The rise of online pharmacies is perhaps one of the most significant transformations in e-commerce in healthcare. Startups can now offer prescription and over-the-counter medications to consumers directly through a digital platform. With examples like Medlife, 1mg, and PillPack, the demand for online medicine retail is soaring, driven by the convenience of home delivery and easy access.</p><p>For startups, establishing an online pharmacy platform can serve a dual purpose: meeting consumer demand for convenience while also addressing critical access gaps in underserved areas. This category offers ample room for healthcare custom apps that streamline prescriptions, medicine reminders, and even virtual consultations with licensed professionals.</p><p><strong>2. Telemedicine Platforms: Remote Healthcare Services</strong></p><p>Telemedicine is another booming category within healthcare e-commerce. Platforms like Practo and Zocdoc are connecting patients with doctors remotely, eliminating the need for in-person visits. For startups, telemedicine presents an attractive business model that capitalizes on the growing trend of virtual healthcare.</p><p>With the power of video conferencing and mobile apps, a startup could build a telemedicine platform tailored to specific niches—whether it’s mental health services, chronic care management, or general medical advice. Telemedicine not only ensures that patients have greater access to healthcare but also opens up new revenue streams for businesses in the healthcare e-commerce space.</p><p>For real estate startups, this could be an interesting opportunity to combine e-commerce in healthcare with physical spaces, offering consultation services in combination with virtual solutions.</p><p><strong>3. Health Product Marketplaces: A One-Stop Shop for Wellness</strong></p><p>A growing trend in healthcare e-commerce is the development of health product marketplaces. These platforms, like HealthKart and Amazon Healthcare, allow consumers to purchase a variety of health-related products, from medical equipment to fitness supplements.</p><p>Startups can create specialized health product marketplaces focused on specific sectors like fitness, wellness, or chronic disease management. The key to success in this space lies in offering high-quality products along with a seamless e-commerce experience. Building a healthcare custom app can be a great way to enable customers to browse, compare, and purchase products easily, while also offering features like subscription models or personalized recommendations based on health goals.</p><p>By focusing on niche markets within the broader health product category, startups can cater to specific customer needs, thus increasing customer loyalty and repeat business.</p><p><strong>4. Chronic Care Management: Subscription Models for Long-Term Health</strong></p><p>Chronic care management is a rapidly growing segment within healthcare e-commerce, especially for patients managing conditions like diabetes or hypertension. Subscription models allow patients to receive ongoing care, medications, and monitoring tools delivered to their doorsteps.</p><p>Startups offering chronic care management services can tap into this need by providing convenient, subscription-based healthcare solutions. These services can include access to medical devices, regular health check-ins, medication refills, and consultations, all via a <a href=\"https://phyniks.com/application-development-services-company\" rel=\"noopener noreferrer\">healthcare custom app</a>.</p><p>For healthcare startups, focusing on chronic conditions can yield long-term relationships with customers and steady revenue streams. This category also provides opportunities to integrate wearables and health monitoring devices, further enhancing patient care and engagement.</p><p><strong>5. Wellness and Preventive Healthcare: A Focus on Holistic Health</strong></p><p>Another exciting category within healthcare e-commerce is wellness and preventive healthcare. Startups can create platforms that offer products and services aimed at improving overall health, such as fitness equipment, dietary plans, and mental health tools.</p><p>As consumers increasingly prioritize their well-being, the demand for wellness products continues to grow. A startup could focus on creating a marketplace for fitness products, wellness supplements, or mental health resources. These platforms can also integrate healthcare custom apps that provide personalized recommendations and support for users seeking healthier lifestyles.</p><p>This category not only focuses on prevention but also emphasizes the need for startups to offer convenience and accessibility through e-commerce channels.</p><p><strong>6. B2B Medical Supplies: Supporting Healthcare Institutions</strong></p><p>Finally, B2B medical supplies is another growing category within healthcare e-commerce, especially for startups looking to cater to hospitals, clinics, and healthcare facilities. This market involves supplying medical equipment, pharmaceuticals, and other health-related products in bulk.</p><p>Startups can build platforms that connect healthcare institutions with trusted suppliers. By offering competitive prices, high-quality products, and reliable delivery systems, businesses can carve out a strong presence in the healthcare e-commerce industry. Additionally, B2B platforms can incorporate features like subscription models or automated inventory management, making the process smoother for both buyers and sellers.</p><h3>\n  \n  \n  Why should startups dive into healthcare e-commerce?\n</h3><p>*\nThe healthcare e-commerce industry is expanding rapidly, driven by the increasing adoption of digital platforms. Consumers are now looking for online pharmacies, telemedicine services, and wellness product marketplaces, making it a prime market for innovation.</p><p>\nStartups can scale quickly by tapping into online platforms that don’t require large physical infrastructure. Whether it's through offering telemedicine, wellness products, or prescription medications, businesses can start small and expand as demand grows.</p><p>\nThe integration of advanced technologies such as healthcare custom apps, telemedicine, and AI-driven solutions allows startups to bring innovative solutions to the market. With the right tech, startups can build platforms that meet the evolving needs of consumers, making the market ripe for disruption.</p><p><strong>Reduced Barriers to Entry:</strong>\nCompared to traditional healthcare businesses, starting an online healthcare platform requires lower upfront investments. E-commerce platforms are often easier and less expensive to set up, making healthcare more accessible to startups in both developed and developing markets.</p><h3>\n  \n  \n  Essential Features of a Healthcare E-Commerce Platform\n</h3><p>Building a successful healthcare e-commerce platform is not just about selling products or services; it’s about creating an experience that users trust and rely on. To build a platform that stands out in a competitive market, certain features are non-negotiable:</p><h3>\n  \n  \n  Challenges in Healthcare E-Commerce\n</h3><p>While the potential is high, e-commerce in healthcare comes with its own set of challenges that startups need to address:</p><p><strong>Legal &amp; Compliance Hurdles</strong>: One of the biggest challenges in the healthcare e-commerce industry is navigating the complex web of legal and compliance regulations. Different regions have varying rules regarding the sale of medical products, telemedicine, and data protection. Startups need to ensure their platform adheres to these regulations to avoid legal complications.</p><p><strong>Building Trust with Consumers:</strong> Trust is a major factor in healthcare e-commerce. Consumers must feel confident that the products they’re buying are genuine, that the telemedicine services are provided by licensed professionals, and that their personal data is protected. Building this trust takes time, but it can be done through transparency, strong customer support, and certifications from health authorities.</p><p> Healthcare products often need to be delivered quickly and securely, especially in rural areas where access to physical healthcare facilities may be limited. Logistics, therefore, becomes a significant challenge for startups. Ensuring timely and safe delivery of medications or medical equipment is essential to customer satisfaction.\nStrategies for Success in Healthcare E-Commerce</p><p>To stand out in the healthcare e-commerce space, startups need to focus on a few key strategies to ensure long-term success:</p><p><strong>Develop a Mobile-First Experience:</strong> A large portion of users now access healthcare services through their smartphones. As mobile usage continues to rise, startups must prioritize a mobile-first approach for their platforms. A mobile-friendly design ensures that users can easily access services like telemedicine consultations, prescription refills, or wellness product purchases on the go.</p><p><strong>Leverage AI &amp; Data Analytics:</strong> Startups can leverage AI and data analytics to offer personalized healthcare experiences. By tracking user behavior and preferences, startups can offer customized recommendations for medications, health plans, or fitness routines. Personalization can drive higher engagement, repeat business, and customer satisfaction.</p><p><strong>Build Strategic Partnerships</strong>: Collaborating with healthcare providers, pharmacies, or wellness experts can help expand the reach of your platform. These partnerships can help startups gain credibility, tap into new customer bases, and provide a broader range of services.</p><h3>\n  \n  \n  Future Trends in Healthcare E-commerce\n</h3><p>As the healthcare industry continues to evolve, healthcare e-commerce is experiencing rapid growth, driven by technological advancements and changing consumer expectations. Startups in both developed and developing markets have the opportunity to leverage these trends to build innovative platforms that offer better services and greater efficiency. Here’s a look at some of the most significant trends shaping the future of e-commerce in healthcare:</p><p><strong>1. Integration of AI, IoT, and Blockchain</strong></p><p>The integration of AI, the Internet of Things (IoT), and blockchain technology is set to revolutionize healthcare e-commerce. These technologies are enhancing the efficiency of healthcare delivery, improving patient outcomes, and simplifying transactions.</p><p>AI: <a href=\"https://phyniks.com/ai-software-development-services-company\" rel=\"noopener noreferrer\">Artificial Intelligence</a> is being used for personalized healthcare experiences, recommending the best treatments or medications based on patient data. AI can also optimize inventory management, ensuring that products are stocked according to demand, thus reducing waste and improving the customer experience.</p><p>IoT: The <a href=\"https://phyniks.com/emerging-technologies-solutions\" rel=\"noopener noreferrer\">IoT solution</a> is enabling real-time data collection from devices like wearables, providing insights into a patient’s health status. This data can be integrated into e-commerce platforms, creating a seamless connection between the patient’s physical health and their digital healthcare journey.\nBlockchain: Blockchain ensures the security and transparency of patient data, making e-commerce platforms more trustworthy. It can be used to verify the authenticity of health products, offering consumers peace of mind that they’re purchasing from reliable sources.</p><p><strong>2. Growth of Cross-Border E-commerce</strong></p><p>One of the most exciting trends in healthcare e-commerce is the growth of cross-border shopping. As more consumers demand access to healthcare products and services from around the world, cross-border e-commerce has become a key growth area. Startups in healthcare have the opportunity to tap into international markets, expanding their reach beyond local boundaries.</p><p>For example, consumers in developing countries are increasingly turning to online pharmacies or health product marketplaces in developed countries for access to quality products. E-commerce platforms are making it easier for these consumers to access medications and health products that may not be available locally. This global accessibility is creating new revenue streams and fostering innovation in how products are distributed worldwide.</p><p>To capitalize on this trend, startups should focus on ensuring their healthcare custom app is designed for seamless international transactions, including offering multi-currency and multi-language options. Additionally, partnerships with global logistics providers can ensure timely and reliable delivery of healthcare products across borders.</p><p><strong>3. The Shift Towards Holistic Healthcare Platforms</strong></p><p>Another key trend is the shift towards holistic healthcare platforms that combine physical and digital services. Consumers are increasingly looking for platforms that not only provide products but also integrate digital healthcare services like telemedicine, wellness consultations, and chronic disease management.</p><p>Telemedicine Platforms: These platforms allow patients to consult with healthcare providers remotely, making healthcare more accessible, especially in regions with limited access to in-person services.</p><p>Chronic Care Management: Subscription models for managing chronic conditions, such as diabetes and hypertension, are becoming more popular. These models provide ongoing support and medication, offering a more personalized approach to patient care.</p><p>By combining these services into a single platform, healthcare e-commerce providers can offer a comprehensive solution that meets the diverse needs of today’s healthcare consumers. The future of e-commerce in healthcare lies in creating these all-in-one solutions that bridge the gap between physical and digital healthcare services.</p><p>The future of healthcare e-commerce is filled with exciting opportunities, driven by technological advancements and changing consumer expectations. By staying on top of trends like AI integration, cross-border e-commerce, and the shift toward holistic platforms, startups can build innovative, scalable solutions that meet the needs of the modern healthcare consumer.</p><p>With the right approach, these technology will help healthcare startups establish themselves as leaders in the growing e-commerce space, creating new opportunities for growth and success in an increasingly digital world.</p><p>If you're ready to take advantage of the growing healthcare e-commerce trends and need a trusted partner to help you develop a tailored solution, our software development firm is here to bring your vision to life. Let's work together to create an innovative platform that meets the needs of today’s digital healthcare landscape. <a href=\"https://phyniks.com/contact-us\" rel=\"noopener noreferrer\">Contact us today to get started</a>!</p>","contentLength":16259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robot AI with Memory Makes Complex Tasks 45% More Successful","url":"https://dev.to/mikeyoung44/robot-ai-with-memory-makes-complex-tasks-45-more-successful-3ko6","date":1739782212,"author":"Mike Young","guid":1681,"unread":true,"content":"<ul><li>New AI agent called STMA that helps robots complete complex tasks by remembering spatial and temporal information</li><li>Combines memory storage with planning abilities for extended task sequences</li><li>Demonstrates improved performance on household tasks compared to existing methods</li><li>Uses neural networks to integrate memory with decision making</li><li>Tested successfully in simulated home environments</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2><p>STMA is like giving a robot both a good memory and the ability to plan ahead. Just as humans remember where objects are in their home and what steps they've already taken when cooking a meal, STMA helps robots keep track of important information while working on tasks.</p>","contentLength":684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepSeek Removed from South Korea App Stores Pending Privacy Review","url":"https://yro.slashdot.org/story/25/02/17/064219/deepseek-removed-from-south-korea-app-stores-pending-privacy-review?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739781240,"author":"EditorDavid","guid":1673,"unread":true,"content":"Today Seoul's Personal Information Protection Commission \"said DeepSeek would no longer be available for download until a review of its personal data collection practices was carried out,\" reports AFP.\n\nA number of countries have questioned DeepSeek's storage of user data, which the firm says is collected in \"secure servers located in the People's Republic of China\"... This month, a slew of South Korean government ministries and police said they blocked access to DeepSeek on their computers. Italy has also launched an investigation into DeepSeek's R1 model and blocked it from processing Italian users' data. Australia has banned DeepSeek from all government devices on the advice of security agencies. US lawmakers have also proposed a bill to ban DeepSeek from being used on government devices over concerns about user data security. \n\nMore details from the Associated Press:\n\nThe South Korean privacy commission, which began reviewing DeepSeek's services last month, found that the company lacked transparency about third-party data transfers and potentially collected excessive personal information, said Nam Seok [director of the South Korean commission's investigation division]... A recent analysis by Wiseapp Retail found that DeepSeek was used by about 1.2 million smartphone users in South Korea during the fourth week of January, emerging as the second-most-popular AI model behind ChatGPT.\n\n","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meet the Cybersecurity Defender of 2025 for EMEA","url":"https://blogs.cisco.com/security/meet-the-cybersecurity-defender-of-2025-for-emea/","date":1739781000,"author":"Cristina Errico","guid":1672,"unread":true,"content":"<article>Cisco's 2025 EMEA Cybersecurity Defender of the Year award goes to the team at SAP Enterprise Cloud Services, who raised the bar for overall security posture.</article>","contentLength":158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deploy and use DeepSeek R1 with Azure and .NET","url":"https://dev.to/uveta/deploy-and-use-deepseek-r1-with-azure-and-net-1fh6","date":1739778157,"author":"Uroš Miletić","guid":1659,"unread":true,"content":"<ul><li>Deploy DeepSeek R1 on Azure AI Foundry</li><li>Consume from Semantic Kernel</li></ul><p>DeepSeek models have taken technological world by surprise, demonstrating that cutting-edge AI development is no longer confined to the certain valley made out of silicon, but has become a global phenomenon. Although Microsoft has traditionally partnered with OpenAI, the users of its technologies still have reasons to be optimistic. The Azure cloud platform recently announced support for the DeepSeek R1 model through its Azure AI Foundry service. Currently in public preview, the model may be run in serverless mode and is free of charge. This article will guide you through deploying the R1 model and integrating it with .NET applications.</p><h2>\n  \n  \n  Deploy DeepSeek R1 on Azure AI Foundry\n</h2><p>Deploying the DeepSeek model on Azure is straightforward, even for those new to Azure AI Foundry (formerly Azure AI Studio).</p><p>Start by creating a new hub, which serves as a container for your AI applications and models. This can be done via AI Foundry <a href=\"https://ai.azure.com/managementCenter/allResources\" rel=\"noopener noreferrer\">Management Center</a>. Note that the region you select for your hub will impact model availability. As of February 2025, the DeepSeek R1 model is available in East US, East US 2, West US, West US 3, South Central US, and North Central US regions only.</p><p>Next, you need to create a new project. In the <a href=\"https://ai.azure.com/managementCenter/allResources\" rel=\"noopener noreferrer\">Management Center</a> select the hub you created, and click on \"New project\" button. Provide a name for your project and click \"Create.\" Your project will be ready in a few seconds.</p><p>Once you have your hub and project ready, you can deploy DeepSeek R1 model. Navigate to the <a href=\"https://ai.azure.com/explore/models\" rel=\"noopener noreferrer\">Model catalog</a> tab within your project. Search for the \"DeepSeek R1\" model, and click \"Deploy\". Provide a region unique name for your model and optionally apply content filters. Click \"Deploy\" (again) to start provisioning the model, which may take a few minutes.</p><p>After deployment finishes, you will find the model in the <a href=\"https://ai.azure.com/build/deployments/model\" rel=\"noopener noreferrer\">Models + endpoints</a> tab of your project. Select the deployment name to access detailed information, including the endpoint URL and API key, which are necessary for programmatic consumption.</p><p>Use the chat playground available in the <a href=\"https://ai.azure.com/playgrounds\" rel=\"noopener noreferrer\">Playgrounds</a> tab of your project to ensure the deployment is functioning correctly. Make sure to select DeepSeek R1 deployment before starting the conversation. This step helps verify that the model will work seamlessly when integrated programmatically.</p><p>Models deployed via Azure AI Foundry can be accessed from any programming language that supports HTTP requests. For .NET, Azure provides an SDK through the <a href=\"https://www.nuget.org/packages/Azure.AI.Inference\" rel=\"noopener noreferrer\">Azure AI Inference</a> library. To consume the model, create a chat client using the deployment endpoint URL and API key, and then run chat completion.</p><h2>\n  \n  \n  Consume from SemanticKernel\n</h2><p>For more complex applications using Semantic Kernel, consuming models deployed in Azure AI Foundry is straightforward. Utilize the <a href=\"https://www.nuget.org/packages/Microsoft.SemanticKernel.Connectors.AzureAIInference\" rel=\"noopener noreferrer\">Microsoft.SemanticKernel.Connectors.AzureAIInference</a> connector library. Register the AI Inference connector using the deployment name, endpoint URL, and API key while building the kernel. Once configured, provision the kernel and use the  service to run chat completion.</p><p>Complete .NET and Semantic Kernel chat samples are available on <a href=\"https://github.com/uveta/demo-azure-deepseek\" rel=\"noopener noreferrer\">GitHub</a>. Make sure you add the deployment name, endpoint URL, and API key where indicated in the code to run the applications without issues.</p><p>Keep in mind that the DeepSeek R1 model on Azure is still in preview and is subject to throttling and rate limiting. While it may take from couple of seconds up to few minutes to receive a meaningful response, the service is currently free, allowing for extensive experimentation.</p>","contentLength":3597,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"South Korea blocks downloads of DeepSeek from local app stores","url":"https://techcrunch.com/2025/02/16/south-korea-blocks-downloads-of-deepseek-from-local-app-stores/","date":1739777659,"author":"Kate Park","guid":1650,"unread":true,"content":"<p>South Korean officials on Saturday temporarily restricted Chinese AI Lab DeepSeek’s app from being downloaded from app stores in the country pending an assessment of how the Chinese company handles user data. The Personal Information Protection Commission (PIPC) said the Chinese app would be available to be downloaded once it complies with Korean privacy laws […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Effortless RChilli PeopleSoft Integration for Smarter Hiring 🤖📄","url":"https://dev.to/rchilli_resumeparser/effortless-rchilli-peoplesoft-integration-for-smarter-hiring-40ep","date":1739776318,"author":"Rchilli Inc","guid":1658,"unread":true,"content":"<p>Elevate your hiring process with <a href=\"https://www.rchilli.com/our-partners/rchilli-peoplesoft-integration\" rel=\"noopener noreferrer\">RChilli PeopleSoft Integration</a> 🔗. This AI-powered resume parsing solution seamlessly automates candidate data extraction 📊, eliminating manual entry and enhancing recruitment efficiency ⚡. Oracle PeopleSoft users benefit from faster screening ⏳, structured resume data 📂, and improved decision-making 🎯. Streamline talent acquisition and hire smarter with intelligent automation 🤝.<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc8e0xydogmuvg8a2i048.jpg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fc8e0xydogmuvg8a2i048.jpg\" alt=\"Image description\" width=\"800\" height=\"430\"></a></p>","contentLength":433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why DeepSeek-R1 Is so Much Better Than o3-Mini & Qwen 2.5 MAX — Here The Results","url":"https://dev.to/gaodalie_ai/why-deepseek-r1-is-so-much-better-than-o3-mini-qwen-25-max-here-the-results-37m9","date":1739776276,"author":"Gao Dalie (高達烈)","guid":1657,"unread":true,"content":"<p>As the title says, I tried using Deepseek-r1, o3-Mini and Qwen 2.5 MAX, which is getting a lot of attention. There are a lot of things being said about it,</p><p>I get the impression that by entering the market at a time when research and development were progressing and methods were being established through companies like OpenAI, DeepSeek, and Qwen, they were able to save on the huge costs of trial and error and complete the project at a low cost.</p><p>OpenAI is eager to defend its market position with the release of the O3 Mini on Friday, a direct response to Chinese startup DeepSeek’s R1 model.</p><p>Finally couldn’t sit still and launched a new inference model series o3-mini. Not only is the inference model open to free users for the first time, but the cost is also reduced by 15 times compared to the previous o1 series.</p><p>Unlike the GPT-4O and GPT model families, the “O” family of AI models focuses on reasoning tasks. They are less creative but have embedded chains of thought reasoning, making them more capable of solving complex problems, tracing back wrong analyses, and building better-structured codes.</p><p>Not to mention, Alibaba released a new version of its Qwen 2.5 artificial intelligence model, Meanwhile, Qwen is also developing an ultra-large MoE model, Qwen2.5-Max, which is trained using pre-training data of over 20 trillion tokens and a carefully designed post-processing training method.</p><p>Compared to the previous generation model, the Qwen 2.5 model has significantly improved computing power, processing speed, and versatility, and is expected to be particularly useful for business and research purposes.</p><p>However, it is rumoured that  will outperform DeepSeek-R1. In this article, we will explore the true value and future potential of  through its features and mechanisms and compare them with competing models.</p><p>By the time you finish reading, you will be excited and hopeful about the future of AI, and you will want to pick up some “information” that will help you improve business efficiency and innovation. Please stay with us until the end.</p><ul><li><p>If you like this topic and you want to support me:</p></li><li><p>like my article; that will really help me out.👏</p></li><li><p>Follow me on my YouTube channel</p></li><li><p>Subscribe to me to get the latest article.</p></li></ul><p>o3-mini is OpenAI’s first small inference model that supports developer-required functions. It inherits the low-cost and low-latency advantages of o1-mini and supports functions such as function calls, streaming, and structured output.</p><p>Developers can choose the strength of inference according to their needs, balancing the depth of thinking and response speed, but it does not support visual tasks, and visual reasoning still requires the use of o1.</p><p>Outstanding reasoning ability: Compared to its predecessor, the o1-mini, it produces more accurate and clearer answers, providing stronger reasoning ability, and allowing for deeper understanding and logical thinking, which is essential in solving complex problems.</p><p>: Response time is 24% faster than the o1-mini, at an average of 7.7 seconds. You can use it without stress even in situations where real-time response is required.</p><p>: Excels in science, math, coding, and a variety of engineering specialities.</p><p><strong>Developer-oriented features</strong>: Equipped with long-awaited features, such as function calls, structured output, and developer messages, enhancing usability as an engineering tool.</p><p><strong>Flexible inference options</strong>: Three inference effort options are available: low, medium, and high. You can choose the best performance for your situation.</p><p>: It maintains the low cost and low latency of the o1-mini while providing more advanced features, making it cost-effective and accessible to a wide range of users.</p><p>By the way, what is the performance difference between this and the O1 Pro? Of course, this is just a matter of usage, so a strict comparison cannot be made.</p><p>But the O1 Pro is slow to begin with. No matter what code you’re working on, even if you give detailed instructions for each function, it usually takes about 3 minutes.</p><p>It requires a lot of writing code, so it’s not very practical on the O1 Pro.</p><p>That’s why the o3-mini is by far the easiest to use.</p><p>If there was a situation where the o3-mini-high would give an error, I thought I would try using the o1 pro, but so far I haven’t gotten any errors so I can’t compare them</p><p>Qwen 2.5 Max is a particularly high-performance version of the series and has attracted attention for demonstrating benchmark scores that surpass DeepSeek V3. Its main features are its inference speed and ability to handle a variety of tasks, as well as enhanced integration with external services and plugins.</p><p>Qwen 2.5 Max integrates not only language understanding but also image and video generation functions. This gives it the flexibility to handle not only text-based dialogue but also multimodal input</p><p>So, what exactly is the Qwen2.5-Max? Below are its main features.</p><p>Mixture-of-Expert (MoE) ArchitectureUnlike typical transformer-based language models, this architecture uses a combination of multiple “expert” sub-models, making it easier to optimize for specific tasks and improving processing efficiency and accuracy.</p><p>: Pre-training is performed on more than 20 trillion tokens, giving it the ability to understand and generate language across a wide range of domains. It has the advantage of being able to handle a wide range of fields because it greedily learns from all kinds of text data, including news articles, academic papers, and social media posts.</p><p>Advanced fine-tuning (SFT and RLHF)By combining SFT (supervised fine-tuning) and RLHF (reinforcement learning based on human feedback), we have created a mechanism to generate responses that users expect more accurately and naturally. This makes it possible to respond in a way that takes into account the user’s “preferences” and “intentions,” rather than simply reproducing training data.</p><p>: Access is possible via API using Alibaba Cloud’s Model Studio service. It also supports OpenAI-compatible APIs, so it is highly compatible with existing development environments and tools, and you can expect quick implementation and expansion.</p><p>I couldn’t hold myself to compare it with the domestic large model DeepSeek R1 that has swept the entire large model circle.</p><p>This time, we will verify the following items.</p><div><pre><code>\nwriting a p5.js script to simulate 100 colored balls bouncing inside a sphere.\nEach ball should leave a fading trail showing its most recent path. \nThe container sphere should rotate slowly. \nMake sure to implement proper collision detection to keep the ball \ninside the sphere.\n</code></pre></div><div><pre><code>let balls = [];\nlet numBalls = 100;\nlet sphereRadius = 300;\nlet rotationAngle = 0;\n\nfunction setup() {\n  createCanvas(800, 800, WEBGL);\n  for (let i = 0; i &lt; numBalls; i++) {\n    balls.push(new Ball());\n  }\n  noStroke();\n}\n\nfunction draw() {\n  background(0);\n  rotateX(rotationAngle);\n  rotateY(rotationAngle * 0.5);\n\n  // Draw the container sphere (transparent)\n  push();\n  fill(0, 0, 0, 0);\n  stroke(255);\n  sphere(sphereRadius);\n  pop();\n\n  // Update and display balls\n  for (let ball of balls) {\n    ball.update();\n    ball.display();\n  }\n\n  // Slowly rotate the sphere\n  rotationAngle += 0.005;\n}\n\nclass Ball {\n  constructor() {\n    this.pos = p5.Vector.random3D().mult(sphereRadius * 0.8);\n    this.vel = p5.Vector.random3D().mult(2);\n    this.color = color(random(255), random(255), random(255), 200);\n    this.trail = [];\n    this.trailLength = 20;\n  }\n\n  update() {\n    // Update position\n    this.pos.add(this.vel);\n\n    // Collision detection with the sphere\n    let distance = this.pos.mag();\n    if (distance &gt; sphereRadius) {\n      let normal = this.pos.copy().normalize();\n      this.vel = p5.Vector.reflect(this.vel, normal);\n      this.pos = normal.mult(sphereRadius * 0.99); // Slightly inside the sphere\n    }\n\n    // Add current position to the trail\n    this.trail.push(this.pos.copy());\n    if (this.trail.length &gt; this.trailLength) {\n      this.trail.shift();\n    }\n  }\n\n  display() {\n    // Draw the trail\n    for (let i = 0; i &lt; this.trail.length; i++) {\n      let alpha = map(i, 0, this.trail.length, 50, 255);\n      fill(this.color.levels[0], this.color.levels[1], this.color.levels[2], alpha);\n      noStroke();\n      push();\n      translate(this.trail[i].x, this.trail[i].y, this.trail[i].z);\n      sphere(5);\n      pop();\n    }\n\n    // Draw the ball\n    fill(this.color);\n    noStroke();\n    push();\n    translate(this.pos.x, this.pos.y, this.pos.z);\n    sphere(10);\n    pop();\n  }\n}\n</code></pre></div><div><pre><code>// Global parameters\nlet balls = [];\nconst numBalls = 100;\nlet containerRadius;\nconst ballRadius = 5;      // radius for each ball\nconst maxTrailLength = 30; // number of positions to store for the fading trail\n\nfunction setup() {\n  createCanvas(windowWidth, windowHeight, WEBGL);\n  // Set the container sphere radius relative to the canvas size.\n  containerRadius = min(width, height) * 0.4;\n\n  // Create the balls with random positions (inside the sphere) and random velocities.\n  for (let i = 0; i &lt; numBalls; i++) {\n    // Get a random point that is guaranteed to be inside the container\n    let pos = randomPointInSphere(containerRadius - ballRadius);\n    // Give a random 3D direction with a random speed between 1 and 3.\n    let vel = p5.Vector.random3D().mult(random(1, 3));\n    // Random color for the ball.\n    let col = color(random(255), random(255), random(255));\n    balls.push(new Ball(pos, vel, col));\n  }\n\n  // Optional: set a stroke weight for trails\n  strokeWeight(2);\n}\n\nfunction draw() {\n  // Draw a semi-transparent black background so that previous frames slowly fade.\n  // (Adjust the second parameter to change how quickly the trails fade.)\n  background(0, 20);\n\n  // Apply a slow rotation to the entire scene.\n  rotateY(frameCount * 0.005);\n  rotateX(frameCount * 0.003);\n\n  // Update and display each ball.\n  for (let ball of balls) {\n    ball.update();\n    ball.display();\n  }\n\n  // Draw the container sphere (as a semi-transparent wireframe)\n  noFill();\n  stroke(255, 50);\n  sphere(containerRadius);\n}\n\n// Returns a random point uniformly distributed within a sphere of given max radius.\nfunction randomPointInSphere(maxRadius) {\n  let v = p5.Vector.random3D();\n  // Multiply by a random number between 0 and maxRadius to get a point inside the sphere.\n  v.mult(random(0, maxRadius));\n  return v;\n}\n\n// Ball class definition\nclass Ball {\n  constructor(pos, vel, col) {\n    this.pos = pos;\n    this.vel = vel;\n    this.col = col;\n    this.trail = []; // to store previous positions\n  }\n\n  update() {\n    // Move the ball.\n    this.pos.add(this.vel);\n\n    // Collision detection: If the ball goes outside the container sphere,\n    // reflect its velocity. (We use the ball's radius so that the entire sphere stays in.)\n    let d = this.pos.mag();\n    if (d + ballRadius &gt; containerRadius) {\n      // Get the outward normal from the sphere's center.\n      let normal = this.pos.copy().normalize();\n      // Reflect the velocity using the reflection formula:\n      //    v = v - 2*(v dot normal)*normal\n      let dot = this.vel.dot(normal);\n      this.vel.sub(p5.Vector.mult(normal, 2 * dot));\n\n      // Optionally reposition the ball to exactly be inside the container.\n      this.pos = normal.mult(containerRadius - ballRadius);\n    }\n\n    // Add the current position to the trail.\n    this.trail.push(this.pos.copy());\n    // Limit the length of the trail.\n    if (this.trail.length &gt; maxTrailLength) {\n      this.trail.shift();\n    }\n  }\n\n  display() {\n    // Draw the fading trail.\n    noFill();\n    beginShape();\n    // Draw a vertex at each point in the trail.\n    // Older positions are drawn with lower opacity.\n    for (let i = 0; i &lt; this.trail.length; i++) {\n      let pos = this.trail[i];\n      // Map the opacity so that the oldest point is more transparent.\n      let alpha = map(i, 0, this.trail.length - 1, 50, 255);\n      stroke(red(this.col), green(this.col), blue(this.col), alpha);\n      vertex(pos.x, pos.y, pos.z);\n    }\n    endShape();\n\n    // Draw the ball.\n    push();\n    translate(this.pos.x, this.pos.y, this.pos.z);\n    noStroke();\n    fill(this.col);\n    sphere(ballRadius);\n    pop();\n  }\n}\n</code></pre></div><div><pre><code>let balls = [];\nlet numBalls = 100;\nlet sphereRadius = 200;\nlet angleX = 0;\nlet angleY = 0;\n\nfunction setup() {\n  createCanvas(600, 600, WEBGL);\n  colorMode(HSB, 360, 100, 100, 100);\n\n  // Create balls\n  for (let i = 0; i &lt; numBalls; i++) {\n    balls.push(new Ball());\n  }\n}\n\nfunction draw() {\n  background(0);\n\n  // Rotate the container sphere\n  rotateX(angleX);\n  rotateY(angleY);\n  angleX += 0.005;\n  angleY += 0.01;\n\n  // Draw the container sphere\n  noFill();\n  stroke(255, 50);\n  sphere(sphereRadius * 2);\n\n  // Update and display balls\n  for (let ball of balls) {\n    ball.update();\n    ball.display();\n    ball.checkCollision();\n  }\n}\n\nclass Ball {\n  constructor() {\n    this.position = createVector(\n      random(-sphereRadius, sphereRadius),\n      random(-sphereRadius, sphereRadius),\n      random(-sphereRadius, sphereRadius)\n    );\n    this.velocity = p5.Vector.random3D().mult(random(1, 3));\n    this.color = color(random(360), 80, 100, 100);\n    this.history = [];\n    this.maxHistory = 20;\n  }\n\n  update() {\n    this.position.add(this.velocity);\n\n    // Store position history for the trail\n    this.history.push(this.position.copy());\n    if (this.history.length &gt; this.maxHistory) {\n      this.history.shift();\n    }\n  }\n\n  display() {\n    // Draw the trail\n    noFill();\n    stroke(this.color);\n    beginShape();\n    for (let i = 0; i &lt; this.history.length; i++) {\n      let pos = this.history[i];\n      vertex(pos.x, pos.y, pos.z);\n    }\n    endShape();\n\n    // Draw the ball\n    noStroke();\n    fill(this.color);\n    push();\n    translate(this.position.x, this.position.y, this.position.z);\n    sphere(5);\n    pop();\n  }\n\n  checkCollision() {\n    // Check if the ball is outside the sphere\n    if (this.position.mag() &gt; sphereRadius) {\n      // Reflect the velocity vector\n      this.velocity.reflect(this.position.normalize());\n    }\n  }\n}\n</code></pre></div><p>All the code is incorrect. For example, DeepSeek-R1 has a problem with the 100 balls — if you look at the GIF, you will see that DeepSeek-R1 misses the ball inside the circle. O3-Mini generates 100 balls, but the circle is unclear because it has a grey background. Qwen2.5-Max excels at generating the circle and 100 balls but has a size issue, making them look bigger than expected.</p><p>In conclusion, I would say that all these models require human intervention, as they cannot generate correct code with a single prompt.</p><p>Factorize the number 1757051 and explain why it is a good example \nto test a human's ability to factorize this.</p><p>Deepseek-r1 and o3-mini generate correct answers, but o3-mini is not as good at reasoning as Deepseek-r1. Even when the question is correct, the o3-mini lacks some formulas and misses important details. Qwen2.5-Max fails to generate the correct question. Personally, if I have a complex math question, I would prefer to use Deepseek-r1.</p><p>o3-mini is still far behind Deepseek, Deepseek-R1 is better at deep reasoning and complex task processing, especially in deep thinking, R1 not only has powerful performance but also has a lower price.</p><p>The Chat version of r1 is free and unlimited, while the Chat version of o3-mini is available to free members, but it is severely limited. It is just a trial version.</p><p>The API price of o3-mini is $1.1 for input and $4.4 for output, while r1 is 0.55 and $2.19. o3-mini is exactly twice that of r1</p><p>Even o3-mini is better at coding, but I will not use it for coding</p><p>Personally, I think Deepseek-r1 (math) &amp; Claude (coding), who used ❤ to represent life, is the winner.</p><p>🧙‍♂️ I am an ! If you want to collaborate on a project, drop an inquiry here or Book a  Call With Me.</p><p><strong>I would highly appreciate it if you</strong></p>","contentLength":15788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 AI-Powered Stock Recommender 📈💡","url":"https://dev.to/buildandcodewithraman/ai-powered-stock-recommender-15om","date":1739775379,"author":"Ramandeep Singh","guid":1638,"unread":true,"content":"<p>\nInvest smarter with our <strong>AI-Powered Stock Recommender</strong>! This basic version helps you analyze market trends, track real-time stock data, and get AI-driven insights. 📊🤖</p><p>🔹 \n✅ AI-based stock recommendations\n✅ News sentiment analysis<p>\n✅ Easy-to-use Streamlit UI</p></p><p>🚀 <strong>More features coming soon!</strong> Portfolio tracking, advanced AI models, and multi-exchange support are on the way! Stay tuned. 🔥</p><p>⚠️ Disclaimer: This tool is for informational and analytical purposes only. It does not provide financial advice. Please do your own research before making any investment decisions. 📢</p>","contentLength":589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Boost Your Resume Instantly – For FREE!","url":"https://dev.to/buildandcodewithraman/boost-your-resume-instantly-for-free-a83","date":1739774926,"author":"Ramandeep Singh","guid":1635,"unread":true,"content":"<p>Tired of bland resumes? I built a  that transforms your resume with ! ✨🚀<p>\nNow no need to pay any talent sites promising jobs in return of enhanced resumes.</p></p><p>✅ Upload your resume 📂 bullet points 🔥<p>\n✅ Download the improved version as a </p> 📄  </p><p>Built with , this tool ensures your resume stands out! No signups, no hassle – just instant upgrades.  </p><p>Give your resume the AI touch! 🚀💼 Let me know what you think! 👇</p>","contentLength":429,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why AI is the Future of Software Testing","url":"https://dev.to/testifytech/why-ai-is-the-future-of-software-testing-2bb0","date":1739774722,"author":"Steve Wortham","guid":1637,"unread":true,"content":"<p>In the fast-paced world of software development, ensuring high-quality products is no longer just an option — it’s a necessity. As software systems grow more complex, traditional testing methods struggle to keep up with the demand for speed, accuracy, and scalability. Enter Artificial Intelligence (AI) in software testing, a game-changing innovation that’s revolutionizing how quality assurance (QA) teams operate. From generative AI in software testing to sophisticated AI testing tools, the future of QA lies in the intelligent capabilities of these technologies.</p><h2>\n  \n  \n  The Role of AI in Software Testing\n</h2><p>Artificial Intelligence has revolutionized software testing by automating routine tasks, improving test precision, and identifying potential defects before they occur. Rather than replacing human testers, AI enhances their abilities by allowing them to concentrate on more valuable tasks. With AI-powered tools, QA teams are able to optimize workflows, reduce mistakes, and accelerate release times without sacrificing quality.</p><p>Generative AI in software testing takes it a step further by not only automating tasks but also generating test scripts, crafting realistic test data, and recognizing intricate patterns that traditional tools may overlook. These AI systems are designed to learn from past testing cycles, adjust to evolving requirements, and progressively refine testing outcomes.</p><h2>\n  \n  \n  How to Use AI in Software Testing\n</h2><p>Leveraging AI in software testing involves integrating intelligent tools and frameworks into existing QA processes. Some of the key areas where AI can make a significant impact include:</p><ul><li>: AI can automate regression, performance, and functional testing, freeing up testers to focus on exploratory and creative tasks.</li><li>: Machine learning algorithms analyze historical data to predict where bugs are likely to occur.</li><li>Intelligent Test Case Generation: AI tools can create optimized test cases, reducing redundancy and improving coverage.</li><li>: AI-powered tools can automatically detect and fix broken test scripts, minimizing manual intervention.</li><li>: AI continuously monitors software behavior, flagging anomalies and potential performance bottlenecks.</li></ul><h2>\n  \n  \n  AI and Software Testing: A Perfect Match\n</h2><p>AI and software testing work hand in hand, overcoming the limitations of conventional testing techniques. Human testers bring creativity and problem-solving skills, while AI offers speed, scalability, and data-driven accuracy. This powerful combination results in stronger software products and shorter time-to-market.</p><p>For instance, generative AI tools can quickly produce intricate test cases in mere seconds, a task that would typically require hours from human testers. Additionally, AI’s predictive analytics can pinpoint areas with a high likelihood of defects, allowing teams to tackle potential issues before they become major problems.</p><h2>\n  \n  \n  AI Testing Tools: Enhancing Modern QA Practices\n</h2><p>Modern AI testing tools are transforming QA workflows by offering advanced capabilities for automation, defect detection, and test execution. These tools are designed to seamlessly integrate with development pipelines, improving efficiency and reducing errors.</p><p>One such tool designed to address the challenges of AI in software testing is <a href=\"https://testgrid.io/cotester\" rel=\"noopener noreferrer\">CoTester</a>. Built with advanced AI capabilities, it integrates into QA workflows, adapts to team structures, and simplifies complex testing tasks. Its capabilities include analyzing test scenarios, generating test cases, executing tests across real devices, and offering actionable insights through detailed test summaries.</p><h2>\n  \n  \n  Generative AI in Software Testing: The Next Frontier\n</h2><p>Generative AI is reshaping software testing by autonomously generating test scripts, simulating real-world scenarios, and even predicting edge cases that might escape manual testing. Unlike traditional automation tools, generative AI adapts and evolves with each testing cycle, continuously improving its output.</p><p>With AI software testing tools powered by generative AI, QA teams no longer need to spend hours writing and maintaining test scripts. Instead, they can focus on refining strategies, addressing critical bugs, and ensuring the software aligns with business goals.</p><h2>\n  \n  \n  Benefits of AI in Software Testing\n</h2><p>The adoption of AI in software testing brings a multitude of benefits, including:</p><ul><li>: Automating repetitive tasks accelerates testing cycles.</li><li>: AI minimizes human errors, ensuring precise and reliable results.</li><li>: AI can handle massive datasets and complex testing environments effortlessly.</li><li>: AI tools provide insights into potential risks and areas of concern.</li><li>: Automation reduces the overall cost of testing while improving ROI.</li></ul><h2>\n  \n  \n  The Future of AI in Software Testing\n</h2><p>The integration of AI in software testing is not just a passing trend — it’s the future. As AI technologies continue to advance, we can expect even smarter tools capable of autonomously managing end-to-end testing processes.</p><p>In the coming years, AI-powered tools will become essential for organizations aiming to stay competitive in the software development landscape. From enhancing productivity to improving product quality, the benefits of AI in software testing are undeniable.</p><p>AI in software testing is paving the way for smarter, faster, and more reliable software delivery. Intelligent tools equipped with AI capabilities are transforming QA teams’ ability to predict, prevent, and resolve software issues efficiently.</p><p>In a world where software is the backbone of businesses, AI isn’t just an advantage — it’s a necessity. Adopting AI-driven tools is not just about keeping up with trends but about staying ahead in an increasingly competitive market. The future of software testing is here, and it’s powered by Artificial Intelligence.</p><p><em>: This blog was originally published at <a href=\"https://medium.com/@reedjace28/why-ai-is-the-future-of-software-testing-925a7eb4c2f2\" rel=\"noopener noreferrer\">medium.com</a></em></p>","contentLength":5848,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Credit Proposal Generation: How SimplAI’s Credit Analyst AI Agent Solves Key Challenges","url":"https://dev.to/simplai/understanding-credit-proposal-generation-how-simplais-credit-analyst-ai-agent-solves-key-4glj","date":1739773920,"author":"SimplAI","guid":1636,"unread":true,"content":"<p>In the dynamic landscape of banking and finance, navigating the intricacies of credit proposal generation is critical. Financial institutions face an ever-increasing demand for speed and accuracy in assessing creditworthiness. Amidst this backdrop, SimplAI's Credit Analyst AI Agent shines as a beacon of efficiency and emerges as a game-changer, addressing key challenges within credit management.</p><p>The Challenges in <a href=\"https://simplai.ai/blogs/understanding-credit-proposal-generation-how-simplais-credit-analyst-ai-agent-solves-key-challenges/\" rel=\"noopener noreferrer\">Credit Proposal Automation</a>\nAs banking institutions grapple with traditional methods of credit proposal generation, they encounter significant hurdles:</p><p>Inefficiencies in Data Handling: Manual credit assessments can be slow, requiring extensive data collection and processing, leading to delays in decision-making.\nHuman Error Risks: Traditional methods are often fraught with human error, impacting the accuracy of credit decisions.<p>\nBias in Decision-Making: There's an ever-present concern regarding inherent biases that may affect the fairness of credit evaluations.</p>\nComplex Regulatory Compliance: Financial institutions must adhere to stringent regulatory standards, adding layers of complexity to credit assessments.</p>","contentLength":1133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomSolarize in PyTorch","url":"https://dev.to/hyperkai/randomsolarize-in-pytorch-5a4o","date":1739773690,"author":"Super Kai (Kazuya Ito)","guid":1634,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomSolarize.html\" rel=\"noopener noreferrer\">RandomSolarize()</a> can randomly solarize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type: or ). *All pixels equal or above this value are inverted.</li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is solarized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":419,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BrushinBella: Crafting a Digital Experience to Make Parents’ Lives Easier","url":"https://dev.to/maronzalez/brushinbella-crafting-a-digital-experience-to-make-parents-lives-easier-ab8","date":1739770827,"author":"Griffin Cole","guid":1618,"unread":true,"content":"<p>In today’s fast‑paced digital landscape, a brand’s online presence is not just a storefront—it’s an experience that speaks to its audience. BrushinBella, a company dedicated to providing innovative and thoughtfully designed baby and feeding products, recognized that their website needed to embody the values of care, creativity, and reliability. The goal was to create an engaging, easy‑to‑navigate e‑commerce platform that not only showcased their products but also helped parents seamlessly integrate these solutions into their busy lives.</p><p>This article recounts the comprehensive journey behind the creation of the <a href=\"https://www.brushinbella.com/\" rel=\"noopener noreferrer\">Brush in Bella</a> website, from the initial planning and design phases to the intricate development work that involved custom integrations using C++, Java, and Python. We will explore the challenges encountered along the way, the solutions that were implemented, and share expert insights on current trends and best practices in web development.</p><ol><li>The Vision and Planning Phase\n1.1. Defining the Brand and Project Scope\nBefore any code was written or designs drafted, the BrushinBella team—comprising marketing experts, product designers, UX specialists, and software engineers—came together to define the project’s vision. The core objective was clear: to create a digital experience that makes parents’ everyday life easier by offering a curated selection of products, valuable parenting content, and a user‑friendly interface.</li></ol><p>Key objectives defined during the planning phase included:</p><p>Brand Consistency: The website needed to mirror the brand’s values of quality, trust, and innovation.\nUser Experience: Given the target audience of busy parents, simplicity and intuitive navigation were paramount.<p>\nResponsive Design: With an increasing number of users accessing websites on mobile devices, the site had to offer a seamless experience across desktops, tablets, and smartphones.</p>\nCustom Integrations: Although the e‑commerce platform was built on a robust platform (Shopify was chosen for its reliability and scalability), the team identified specific areas—such as performance‑critical image processing, advanced inventory analytics, and bespoke order processing—where custom functionality was needed. This is where languages like C++, Java, and Python played a crucial role.<p>\nScalability and Performance: The system needed to handle fluctuating traffic levels and ensure quick load times, a challenge that required both architectural planning and performance‑oriented coding practices.</p>\nSecurity and Compliance: With sensitive customer data at stake, ensuring top‑notch security from the very start was non‑negotiable.<p>\n1.2. Assembling the Cross‑Functional Team</p>\nTo tackle this multifaceted project, BrushinBella assembled a cross‑functional team. Each team member brought unique expertise:</p><p>Project Managers and Business Analysts: To capture requirements, define deliverables, and ensure alignment with business goals.\nUX/UI Designers: Charged with creating an aesthetically pleasing and intuitive design that would resonate with the target audience.<p>\nFront‑End Developers: Specialists in HTML, CSS, and JavaScript who would turn design mockups into interactive, responsive web pages.</p>\nBack‑End Developers: Experts who would build custom modules, integrate third‑party APIs, and ensure that the site’s server‑side logic was robust and secure.<p>\nDevOps and QA Engineers: Responsible for establishing CI/CD pipelines, rigorous testing protocols, and ensuring smooth deployment and scaling.</p>\n1.3. Technology Selection and Architecture Decisions<p>\nDuring planning, the team conducted a thorough analysis of available technologies. Although the core e‑commerce solution was deployed on Shopify for its proven reliability and ease of use, certain functionalities required custom development. This resulted in a hybrid architecture:</p></p><p>Shopify as the Primary Platform: Managing the storefront, shopping cart, product catalog, and checkout process.\nCustom Back‑End Services: Developed in Java and Python, these services handled complex business logic, integration with third‑party systems (such as ERP and CRM platforms), and data analytics.<p>\nPerformance‑Critical Modules in C++: For tasks such as real‑time image processing (for product images and dynamic visual elements) and computationally intensive operations, C++ was chosen to ensure maximum speed and efficiency.</p>\nAPIs and Microservices: A RESTful API layer was established to enable seamless communication between Shopify and the custom modules. This approach allowed the system to scale horizontally and adopt a microservices architecture, which is increasingly considered best practice in modern web development.</p><ol><li>The Design Phase: From Concept to Wireframe\n2.1. User-Centric Design Philosophy\nA key part of BrushinBella’s vision was to make the website accessible not only to tech‑savvy users but also to non‑experts—busy parents who need a simple, straightforward interface. The UX/UI design phase was driven by several guiding principles:</li></ol><p>Simplicity: The design was stripped of any unnecessary complexity. Clear call‑to‑action buttons, uncluttered layouts, and intuitive navigation were prioritized.\nVisual Appeal: The website needed to evoke warmth and trust. Soft color palettes, playful yet professional typography, and high‑quality images of products and happy families helped achieve this.<p>\nResponsiveness: Mobile-first design principles were followed. Prototypes were tested on multiple devices and screen sizes to ensure consistency.</p>\nAccessibility: The design adhered to accessibility guidelines, ensuring that the website was usable by people with disabilities. This included proper contrast ratios, keyboard‑navigable menus, and alternative text for images.<p>\n2.2. Wireframing and Prototyping</p>\nUsing industry‑standard tools like Sketch and Figma, the design team created detailed wireframes and prototypes. These early models allowed stakeholders to visualize the website’s structure, layout, and user flow. Key features that were highlighted included:</p><p>Homepage: Featuring a dynamic banner that communicated the brand’s message (“Making Parents’ Everyday Life Easier!”) and a streamlined product navigation menu.\nProduct Pages: Each product page was designed to provide detailed images, descriptions, customer reviews, and easy‑to‑find purchasing options.<p>\nBlog and Content Sections: Recognizing that educational content is a valuable asset, the design included a blog section with articles, parenting tips, and video content.</p>\nCheckout Flow: A simplified, secure checkout process was paramount. Wireframes detailed the progression from shopping cart to payment gateway, with minimal friction.<p>\n2.3. Design Iterations and Stakeholder Feedback</p>\nThe iterative nature of design meant that prototypes were continuously refined based on stakeholder and user feedback. Early user testing sessions were conducted with focus groups of parents, ensuring that the designs were meeting real needs. Feedback led to several important adjustments:</p><p>Simplified Navigation: Initial designs with complex menus were streamlined to a single‑level navigation bar.\nEnhanced Product Imagery: High‑resolution images and a “zoom” feature were incorporated to allow parents to inspect product details closely.<p>\nClearer Calls-to‑Action: Buttons were redesigned for better visibility and prominence.</p>\nContent Accessibility: The blog section was reorganized to make content categories and search features more intuitive.</p><ol><li>The Development Phase: Building the Backbone\n3.1. Integrating Shopify with Custom Services\nOnce the design was finalized, the development phase kicked off. The first step was to integrate the robust capabilities of Shopify with the custom-built services developed in-house. Shopify managed the storefront and basic e‑commerce functionalities. However, several requirements demanded bespoke solutions:</li></ol><p>Custom Order Processing: While Shopify provided standard order management, BrushinBella needed an advanced system to integrate real‑time inventory data, promotional logic, and customized gift‑wrapping options. For this, the team built a microservice using Java.\nData Analytics and Reporting: In order to understand customer behavior and optimize the sales process, Python‑based analytics tools were developed. These tools processed data from Shopify’s API and generated actionable insights.<p>\nImage Processing and Dynamic Visuals: To ensure that product images were optimized for speed and quality, the team implemented a performance‑critical module in C++. This module handled tasks such as real‑time image resizing, format conversion, and dynamic compression.</p>\n3.2. The Role of C++: High‑Performance Modules<p>\nAlthough C++ is not traditionally associated with web development, its use in BrushinBella’s project was pivotal for performance‑critical tasks. Key functions implemented in C++ included:</p></p><p>Image Optimization Engine: C++ was used to build an engine that automatically resized and optimized images for various devices and screen resolutions. The engine was integrated as a microservice that communicated with Shopify through RESTful APIs.\nReal‑Time Data Processing: Certain operations, such as processing and rendering high‑resolution graphics for product galleries, were computationally intensive. C++’s efficiency ensured that these tasks did not slow down the user experience.<p>\nCustom Plugins: Some interactive features, such as a dynamic “gift‑wrapping” calculator that adjusted options based on user input, were built in C++ to ensure rapid response times and minimize latency.</p>\nUsing C++ required careful management of memory and thread safety. The development team leveraged modern C++ standards (C++17/20) and robust libraries such as Boost and OpenCV for image processing. This combination not only achieved the necessary performance gains but also ensured that the codebase was maintainable and scalable.</p><p>3.3. Java: The Enterprise Workhorse\nJava was chosen for its reliability, scalability, and strong ecosystem—qualities that made it ideal for handling core business logic and integrations. Within the BrushinBella project, Java served several key roles:</p><p>Business Logic and Order Management: The custom order processing system was developed in Java. This system interfaced with Shopify’s API to synchronize order data and applied complex business rules for promotions, discounts, and gift‑wrapping options.\nAPI Gateway: Java was also used to build a RESTful API gateway that served as the communication hub between Shopify and the custom microservices. The gateway ensured secure and efficient data exchange, handling tasks like authentication, rate‑limiting, and error logging.<p>\nIntegration with Legacy Systems: Many enterprise systems—such as ERP and CRM platforms—are built on or integrate well with Java. By choosing Java for these integrations, BrushinBella ensured that their website could interface smoothly with back‑office systems, enabling real‑time inventory management and customer data synchronization.</p>\nRobust Error Handling and Monitoring: Java’s mature ecosystem provided access to powerful tools for logging (using frameworks such as Log4j) and performance monitoring. This allowed the team to identify and resolve issues quickly, ensuring minimal downtime.<p>\nThe Java development team adopted best practices such as writing modular, test‑driven code and using containerization (with Docker) for deployment. This not only improved the reliability of the application but also made scaling the service more straightforward.</p></p><p>3.4. Python: Rapid Prototyping and Data Analytics\nPython’s reputation for ease of use and rapid development made it the language of choice for a range of supporting services:</p><p>Data Analytics: Python was used extensively for developing analytical tools that processed customer behavior data, order history, and product performance metrics. Libraries such as Pandas, NumPy, and Matplotlib were leveraged to generate detailed reports that informed marketing strategies and inventory decisions.\nAutomation Scripts: Routine tasks such as data backups, report generation, and system health checks were automated using Python scripts. This helped reduce manual intervention and allowed the team to focus on higher‑value activities.<p>\nIntegration and Testing: Python’s flexibility also made it an ideal candidate for writing integration tests. Automated testing frameworks like pytest ensured that the interactions between Shopify, the Java API gateway, and the C++ image optimization engine were reliable and robust.</p>\nMicroservices Development: In some cases, Python microservices were deployed to handle tasks that required rapid iteration and experimentation. For example, the team developed a prototype recommendation engine that used machine learning algorithms (with scikit‑learn) to suggest complementary products to customers based on their browsing history.<p>\nBy combining Python’s rapid prototyping capabilities with the stability of Java and the performance of C++, BrushinBella was able to build a hybrid system that leveraged the strengths of each language.</p></p><p>3.5. Adopting a Microservices Architecture\nOne of the most critical decisions during development was to adopt a microservices architecture. Instead of building a monolithic application, the system was divided into discrete services that communicated via RESTful APIs. This offered several benefits:</p><p>Scalability: Each service could be scaled independently based on demand. For example, the image processing engine in C++ could be scaled up during high‑traffic periods without affecting the rest of the system.\nResilience: Failures in one microservice did not bring down the entire website. Robust error‑handling and fallback mechanisms ensured that the website remained operational even if one component experienced issues.<p>\nFlexibility: The architecture allowed the team to update or replace individual services without redeploying the entire application. This was particularly beneficial when iterating on features such as the recommendation engine or order processing logic.</p>\nTechnology Diversity: By decoupling services, the team could choose the most appropriate language or framework for each task without forcing a one‑size‑fits‑all solution. This technological diversity, while challenging to manage, ultimately resulted in a more robust and efficient system.</p><ol><li>Testing, Deployment, and Optimization\n4.1. Rigorous Testing Strategies\nQuality assurance was embedded in every stage of development. The BrushinBella team implemented a multi‑layered testing strategy to ensure that each component—from the front‑end user interface to the backend microservices—functioned as expected:</li></ol><p>Unit Testing: Each module, whether written in Java, Python, or C++, underwent rigorous unit testing. For Java, JUnit was employed; Python modules were tested with pytest; and C++ components were validated using Google Test.\nIntegration Testing: Automated integration tests were established to verify the seamless communication between Shopify, the Java API gateway, the Python analytics services, and the C++ performance modules.<p>\nEnd‑to‑End Testing: Tools such as Selenium and Cypress were used to simulate real‑user interactions, ensuring that the entire system worked together harmoniously.</p>\nPerformance Testing: Given the emphasis on speed and scalability, performance testing was a critical focus. Load testing simulated high‑traffic scenarios to validate that the system could handle peak loads without significant degradation in response times.<p>\nSecurity Audits: Comprehensive security testing was conducted to safeguard against common vulnerabilities such as SQL injection, cross‑site scripting (XSS), and cross‑site request forgery (CSRF). Regular code audits and penetration testing further ensured that customer data remained protected.</p>\n4.2. Continuous Integration and Deployment (CI/CD)<p>\nTo streamline the development process and ensure rapid delivery of updates, the team set up a robust CI/CD pipeline. Key components of the pipeline included:</p></p><p>Automated Builds: Every code commit triggered an automated build process that compiled the Java and C++ modules and ran the Python scripts. This ensured that errors were caught early.\nAutomated Testing: The CI system executed the full suite of tests for each build, including unit, integration, and end‑to‑end tests. Only builds that passed all tests were promoted to staging.<p>\nContainerization: Using Docker, each microservice was containerized. This ensured consistency across development, testing, and production environments and simplified the scaling process.</p>\nOrchestration: Kubernetes was employed to manage the containerized services, enabling automatic scaling, load balancing, and self‑healing capabilities.<p>\nMonitoring and Logging: Post‑deployment, comprehensive monitoring tools (such as Prometheus and Grafana) were used to track system performance, while logging frameworks (e.g., Logstash and ELK stack) helped in quickly diagnosing and resolving issues.</p>\n4.3. Performance Optimization<p>\nOptimization was an ongoing effort throughout development. Key performance enhancements included:</p></p><p>Caching Strategies: To reduce load times and database queries, caching mechanisms were implemented. This included front‑end caching using service workers for static assets and back‑end caching with Redis for frequently accessed data.\nEfficient API Design: The RESTful APIs were optimized for speed, ensuring that data was transmitted in lightweight JSON formats with minimal overhead.<p>\nCode Profiling and Optimization: Regular profiling sessions identified bottlenecks in the code. For instance, the C++ image processing engine was fine‑tuned to minimize latency, and the Java microservices were optimized to handle high volumes of concurrent requests.</p>\nResponsive and Adaptive Design: The front‑end was optimized for mobile devices by adopting a mobile‑first design philosophy, ensuring fast load times even on slower networks.<p>\nDatabase Optimization: Indexing, query optimization, and efficient data structuring in both relational and NoSQL databases helped maintain rapid data retrieval and update speeds.</p></p><ol><li>Overcoming Challenges and Implementing Solutions\nNo major web development project is without its hurdles. The creation of the BrushinBella website presented a variety of challenges, each of which was met with innovative solutions.</li></ol><p>5.1. Integration Complexity\nChallenge:<p>\nIntegrating a Shopify‑based e‑commerce platform with custom microservices written in Java, Python, and C++ proved complex. Each system had its own data formats, security protocols, and performance characteristics.</p></p><p>Solution:\nThe team implemented a robust API gateway in Java to serve as the central communication hub. This gateway standardized data formats (using JSON), handled authentication and authorization, and ensured that all services communicated seamlessly. Extensive use of RESTful API design principles and well‑documented endpoints minimized integration friction.</p><p>5.2. Performance Bottlenecks\nChallenge:<p>\nCertain operations—especially image processing and real‑time analytics—posed performance challenges, risking slow response times that could frustrate users.</p></p><p>Solution:\nCritical performance‑intensive tasks were offloaded to specialized microservices developed in C++. These modules were optimized using modern C++ standards, multithreading, and libraries like OpenCV to ensure that image processing was done swiftly. Additionally, Python’s role in handling analytics was enhanced by leveraging efficient data processing libraries and asynchronous programming techniques to reduce latency.</p><p>5.3. Maintaining a Consistent User Experience\nChallenge:<p>\nEnsuring a seamless, high‑quality user experience across multiple devices and browsers is always challenging—especially when integrating third‑party services and custom code.</p></p><p>Solution:\nThe design team adopted a mobile‑first approach and followed responsive design principles to ensure that the website adapted gracefully to different screen sizes. Rigorous cross‑browser testing, combined with adaptive UI frameworks, ensured consistency. Accessibility guidelines were also adhered to, making the site usable by a diverse audience, including those with disabilities.</p><p>5.4. Ensuring Security and Data Integrity\nChallenge:<p>\nWith customer data and payment information at stake, any security lapse could have dire consequences. The heterogeneous nature of the system, with multiple programming languages and platforms interacting, introduced several potential vulnerabilities.</p></p><p>Solution:\nSecurity was integrated from day one. The team implemented robust authentication mechanisms at the API gateway level and encrypted all sensitive data. Regular security audits, automated vulnerability scans, and adherence to best practices—such as input validation, sanitization, and the use of secure coding frameworks—helped safeguard the system. Additionally, deploying a web application firewall (WAF) and implementing SSL/TLS across all endpoints further ensured data integrity.</p><p>5.5. Managing a Diverse Technology Stack\nChallenge:<p>\nCombining multiple programming languages and frameworks increases the complexity of the codebase and the development process. Ensuring that team members could collaborate effectively across different languages was a significant challenge.</p></p><p>Solution:\nThe project adopted a microservices architecture, which naturally decoupled the different language‑specific modules. Clear documentation, code conventions, and regular cross‑team meetings ensured that everyone was on the same page. The use of containerization (with Docker) and orchestration (with Kubernetes) allowed developers to work on isolated services without interference. In addition, investing in integrated development environments (IDEs) and code review tools helped maintain code quality and consistency across the diverse stack.</p><ol><li>Expert Insights on Web Development Trends and Best Practices\nAs BrushinBella’s website evolved from concept to a fully‑functional, high‑performance digital platform, the team kept a close eye on emerging trends and industry best practices. Here are some expert insights gleaned during the project:</li></ol><p>6.1. Embracing Microservices and Containerization\nModern web development is increasingly moving away from monolithic architectures toward microservices. This approach provides several advantages:</p><p>Scalability: Each service can be scaled independently to meet demand.\nResilience: Failures in one service do not affect the entire system.<p>\nFlexibility: Developers can choose the best technology for each service without being locked into a single framework or language.</p>\nContainerization tools like Docker and orchestration platforms like Kubernetes have become essential. They enable rapid deployment, efficient resource management, and simplified scaling. For BrushinBella, this meant that the custom C++, Java, and Python services could be managed and updated independently, leading to a more resilient and adaptable platform.</p><p>6.2. The Role of Hybrid Technology Stacks\nWhile many modern websites rely heavily on JavaScript frameworks for the front‑end (such as React or Vue), integrating a hybrid technology stack can yield significant benefits. Each language and framework brings its own strengths:</p><p>C++ for Performance‑Critical Tasks: In performance‑sensitive areas such as image processing, the efficiency of C++ is unmatched.\nJava for Robust Enterprise‑Grade Logic: Java’s strong typing, mature ecosystem, and scalability make it ideal for handling complex business rules and integrations.<p>\nPython for Rapid Development and Data Analysis: Python’s ease of use and powerful libraries allow teams to quickly prototype and deploy data‑driven features.</p>\nThis multi‑language approach is becoming more common as companies seek to optimize for both performance and development speed. It is essential, however, to manage this diversity with clear interfaces, robust API designs, and comprehensive documentation.</p><p>6.3. User Experience and Accessibility as Top Priorities\nNo matter how powerful the backend or how sophisticated the custom integrations, the success of a website ultimately depends on the user experience. Best practices dictate that designers and developers should focus on:</p><p>Responsive Design: Ensuring that the website looks and functions well on all devices.\nAccessibility: Building websites that are usable by everyone, including those with disabilities.<p>\nIntuitive Navigation: Simplifying the user journey from landing on the page to completing a transaction.</p>\nVisual Appeal: Using high‑quality images, consistent branding, and engaging interactive elements to create a memorable user experience.<p>\nFor BrushinBella, rigorous user testing and iterative design refinements ensured that the website not only met functional requirements but also delighted its target audience.</p></p><p>6.4. Security in a Connected World\nSecurity remains one of the most critical aspects of web development. With increasing data breaches and cyberattacks, best practices include:</p><p>Encryption of Data: Both in transit (using SSL/TLS) and at rest.\nRegular Security Audits: Automated vulnerability scanning and penetration testing to identify and mitigate risks.<p>\nRobust Authentication: Using multi‑factor authentication (MFA) and secure API gateways.</p>\nInput Validation and Sanitization: To prevent common attacks such as SQL injection and cross‑site scripting (XSS).<p>\nImplementing these security measures is not a one‑time task but an ongoing process that requires constant vigilance.</p></p><p>6.5. The Future: Serverless, AI, and Progressive Web Apps\nLooking ahead, several trends are shaping the future of web development:</p><p>Serverless Architectures: Platforms such as AWS Lambda, Google Cloud Functions, and Azure Functions are enabling developers to run code without managing servers. This can reduce costs and simplify deployment for certain types of applications.\nArtificial Intelligence and Machine Learning: Integrating AI into web applications is becoming increasingly common. Whether for personalized recommendations, chatbots, or automated analytics, AI can dramatically enhance the user experience.<p>\nProgressive Web Apps (PWAs): PWAs combine the best features of web and mobile applications, offering offline functionality, push notifications, and fast load times. They represent the future of delivering seamless, app‑like experiences through the browser.</p>\nFor BrushinBella, these trends offer opportunities for future enhancements. For example, a serverless recommendation engine or AI‑driven customer support chatbot could further enrich the user experience.</p><ol><li>Reflections and Lessons Learned\n7.1. Collaboration Is Key\nOne of the most important lessons from the BrushinBella project was the power of cross‑functional collaboration. Bringing together designers, developers, business analysts, and marketing experts enabled the team to view the project from multiple perspectives. This collaborative approach ensured that the final product was not only technically sound but also aligned with the brand’s vision and customer needs.</li></ol><p>7.2. Flexibility in Technology Choices\nAdopting a hybrid technology stack may seem daunting at first, but it can yield enormous benefits when managed properly. By leveraging the strengths of C++, Java, and Python in different parts of the system, the team was able to optimize for performance, scalability, and rapid development. This flexibility allowed BrushinBella to build a robust platform that could evolve with changing requirements.</p><p>7.3. Iteration and Continuous Improvement\nThe project was not built in a single, linear pass. Instead, it was an iterative process where feedback was continuously gathered and incorporated. From early wireframes to final deployment, each iteration brought improvements and refinements. The use of CI/CD pipelines and automated testing ensured that each update maintained the high standards of quality required for a live e‑commerce site.</p><p>7.4. Balancing Innovation and Practicality\nWhile it was tempting to incorporate cutting‑edge technologies and ambitious features, the team also had to remain practical. Decisions were driven by both innovative ideas and real‑world constraints such as budget, timelines, and technical feasibility. This balance ensured that the website was not only modern and attractive but also reliable and maintainable over the long term.</p><ol><li>Conclusion: A Modern Web Experience for Today’s Parents\nThe creation of the BrushinBella website is a testament to what can be achieved when visionary design meets technical excellence. By thoughtfully planning each phase, embracing a hybrid development strategy, and leveraging the unique strengths of C++, Java, and Python, the team built a website that not only drives sales but also resonates with its audience.</li></ol><p>Holistic Planning: Successful projects begin with a clear vision and a well‑defined scope. Every stakeholder’s input is valuable in shaping a product that meets both business and user needs.\nUser‑Centered Design: Prioritizing the user experience—through responsive design, intuitive navigation, and accessibility—ensures that the website remains relevant and engaging.<p>\nTechnological Synergy: Using a blend of languages and frameworks allows teams to optimize different aspects of the application. C++ provided high‑performance modules, Java ensured robust enterprise‑grade processing, and Python accelerated data analytics and automation.</p>\nResilient Architecture: Adopting microservices and containerization enabled the system to scale, adapt, and remain resilient in the face of increasing demand.<p>\nOngoing Evolution: The journey does not end at launch. Continuous testing, monitoring, and optimization are crucial to keeping a website secure, fast, and responsive to changing user expectations.</p>\nAs BrushinBella continues to grow and innovate, the lessons learned from this project will serve as a roadmap for future enhancements. With a focus on emerging trends such as serverless architectures, AI integrations, and progressive web apps, the company is well‑positioned to adapt to the evolving digital landscape and maintain its commitment to making parents’ everyday life easier.</p><p>In a world where the digital experience often makes or breaks a brand, BrushinBella’s website stands as an exemplar of what thoughtful planning, creative design, and technical excellence can achieve together. Whether you’re an expert in web development or a newcomer trying to understand the complexities behind a modern e‑commerce platform, the BrushinBella story offers valuable insights into building a system that is both powerful and personable.</p><p>Expert Perspectives and Future Outlook\nIndustry experts agree that the integration of multiple programming paradigms is the future of web development. Leaders in the field emphasize the importance of:</p><p>Adopting Modular Architectures: Breaking down applications into microservices not only improves scalability but also enhances maintainability.\nInvesting in Performance Optimization: As user expectations continue to rise, ensuring fast load times and seamless interactions will remain a top priority.<p>\nFostering Interdisciplinary Collaboration: The most innovative projects arise when cross‑functional teams work together, blending design, technology, and business acumen.</p>\nEmbracing Continuous Learning: With technologies evolving at a rapid pace, staying updated with the latest tools, frameworks, and best practices is essential for success.<p>\nLooking forward, trends such as the integration of AI in personalization, the adoption of serverless computing to reduce operational overhead, and the rise of progressive web apps are expected to shape the future of web development. BrushinBella is already exploring these avenues, planning to integrate AI‑driven customer insights and further optimize the platform using serverless components.</p></p><p>Final Thoughts\nThe journey of building the BrushinBella website highlights the dynamic and ever‑evolving nature of web development. By combining the proven capabilities of established platforms like Shopify with custom‑developed microservices in C++, Java, and Python, the team created a system that is greater than the sum of its parts. This approach not only met immediate business needs but also laid a robust foundation for future growth and innovation.</p><p>For developers and business leaders alike, the BrushinBella project is a compelling case study in the effective melding of design, technology, and user‑centric strategy. It demonstrates that with careful planning, strategic technology selection, and relentless focus on the user experience, it is possible to create a digital platform that truly makes a difference.</p><p>Whether you are planning your next web development project or simply interested in learning how modern e‑commerce platforms are built, the BrushinBella story offers a wealth of insights. As technology continues to evolve, so too will the tools and methods used to create these digital experiences. The key is to remain agile, to embrace change, and to always put the user first.</p><p>In summary, the BrushinBella website is more than just an online store—it is an embodiment of a brand’s promise to simplify and enrich the lives of parents. It stands as a reminder that at the heart of every great digital experience is a commitment to quality, innovation, and user empowerment.</p>","contentLength":33591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/dhruvjoshi9/-5d00","date":1739770696,"author":"Dhruv Joshi","guid":1620,"unread":true,"content":"<h2>ChatGPT, Gemini, Copilot, or DeepSeek R1—Which One Should You Use?</h2>","contentLength":68,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"California Considers Taking Over Some Oil Refineries","url":"https://tech.slashdot.org/story/25/02/17/0511251/california-considers-taking-over-some-oil-refineries?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739769720,"author":"EditorDavid","guid":1616,"unread":true,"content":"California is \"considering state ownership of one or more oil refineries,\" reports the Los Angeles Times. \n\nThey call the idea \"one item on a list of options presented by the California Energy Commission to ensure steady gas supplies as oil companies pull back from the refinery business in the state.\"\n\n\"The state recognizes that they're on a pathway to more refinery closures,\" said Skip York, chief energy strategist at energy consultant Turner Mason &amp; Co. The risk to consumers and the state's economy, he said, is gasoline supply disappearing faster than consumer demand, resulting in fuel shortages, higher prices and severe logistical challenges. \n\nGasoline demand is falling in California, albeit slowly, for two reasons: more efficient gasoline engines, and the increasing number of electric vehicles on the road. Gasoline consumption in California peaked in 2005 and fell 15% through 2023, according to the Union of Concerned Scientists. Electric vehicles, including plug-in hybrids, now represent about 25% of annual new car sales... The drop in demand is causing fundamental strategic shifts among the state's major oil refiners: Chevron, Marathon, Phillips 66, PBF Energy and Valero. \n\nAlready, two California refineries have ceased producing gasoline to make biodiesel fuel for use in heavy-duty trucks, a cleaner-fuel alternative that enjoys rich state subsidies. More worrisome, the Phillips 66 refinery complex in Wilmington, just outside Los Angeles, plans to close down permanently by year's end. That leaves eight major refineries in California capable of producing gasoline. The closure of any one would create serious gasoline supply issues, industry analysts say. But both Chevron and Valero are contemplating permanent refinery closures. The implications? \"Demand will decline gradually,\" York said, \"but supply will fall out in chunks.\" What's unknown is how many refineries will close, and how soon, and how that will affect supply and demand... \n\nA state refinery takeover seems like a radical idea, but the fact that it's being considered demonstrates the seriousness of the supply issue. It's one of several option laid out by the California Energy Commission, which is fulfilling a legislative order to find ways to ensure \"a reliable supply of affordable and safe transportation fuels in California.\" The options list is disparate: Ship in more gasoline from Asia; regulate refineries on the order of electric utilities; cap profit margins; and many more. \n\n 92% of California's gas is produced in refineries, the Times reports. But the special gasoline blends required to reduce air pollution \"also drive up gasoline prices and raise the risk of shortages, because little such gasoline is produced outside California.\"","contentLength":2750,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Testing Aider: Practical Experience with Different Models","url":"https://dev.to/sikamikanikobg/testing-aider-practical-experience-with-different-models-58f2","date":1739769253,"author":"Arsen Apostolov","guid":1619,"unread":true,"content":"<p>I like coding agents and all kind of supportive plug-ins to make my day easier. From Continue to Copilot. I must say that the most complete solution so far for me was Cursor, yet it is not free (see the plans).</p><p>So i found this super cool alternative - Aider chat.</p><p>I tested Aider with multiple AI backends: OpenAI, Claude, and local Ollama server.</p><p>First 30 minutes were spent learning the tool - understanding commands and workflow. After this initial setup phase, development speed increased significantly.\nModel comparison from practical use:</p><p><strong>Claude: Best performance when working remotely</strong><strong>Local setup: Ollama with deepseek r1 7b and qwen coder 2.5 7b</strong>\nHome setup preference: Architect mode with Ollama models</p><p>Key observation: Local models provide good performance without cloud dependencies. The initial learning curve is worth the productivity gain.</p><p>What's your experience with Aider? Particularly interested in local model configurations and performance comparisons.</p>","contentLength":962,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multifidelity Simulation-based Inference for Computationally Expensive Simulators","url":"https://arxiv.org/abs/2502.08416","date":1739768400,"author":"","guid":895,"unread":true,"content":"<article>arXiv:2502.08416v2 Announce Type: replace-cross \nAbstract: Across many domains of science, stochastic models are an essential tool to understand the mechanisms underlying empirically observed data. Models can be of different levels of detail and accuracy, with models of high-fidelity (i.e., high accuracy) to the phenomena under study being often preferable. However, inferring parameters of high-fidelity models via simulation-based inference is challenging, especially when the simulator is computationally expensive. We introduce MF-NPE, a multifidelity approach to neural posterior estimation that leverages inexpensive low-fidelity simulations to infer parameters of high-fidelity simulators within a limited simulation budget. MF-NPE performs neural posterior estimation with limited high-fidelity resources by virtue of transfer learning, with the ability to prioritize individual observations using active learning. On one statistical task with analytical ground-truth and two real-world tasks, MF-NPE shows comparable performance to current approaches while requiring up to two orders of magnitude fewer high-fidelity simulations. Overall, MF-NPE opens new opportunities to perform efficient Bayesian inference on computationally expensive simulators.</article>","contentLength":1261,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Devil is in the Prompts: De-Identification Traces Enhance Memorization Risks in Synthetic Chest X-Ray Generation","url":"https://arxiv.org/abs/2502.07516","date":1739768400,"author":"","guid":896,"unread":true,"content":"<article>arXiv:2502.07516v2 Announce Type: replace-cross \nAbstract: Generative models, particularly text-to-image (T2I) diffusion models, play a crucial role in medical image analysis. However, these models are prone to training data memorization, posing significant risks to patient privacy. Synthetic chest X-ray generation is one of the most common applications in medical image analysis with the MIMIC-CXR dataset serving as the primary data repository for this task. This study presents the first systematic attempt to identify prompts and text tokens in MIMIC-CXR that contribute the most to training data memorization. Our analysis reveals two unexpected findings: (1) prompts containing traces of de-identification procedures (markers introduced to hide Protected Health Information) are the most memorized, and (2) among all tokens, de-identification markers contribute the most towards memorization. This highlights a broader issue with the standard anonymization practices and T2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time memorization mitigation strategies are ineffective and fail to sufficiently reduce the model's reliance on memorized text tokens. On this front, we propose actionable strategies for different stakeholders to enhance privacy and improve the reliability of generative models in medical imaging. Finally, our results provide a foundation for future work on developing and benchmarking memorization mitigation techniques for synthetic chest X-ray generation using the MIMIC-CXR dataset. The anonymized code is available at https://anonymous.4open.science/r/diffusion_memorization-8011/</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Supervised contrastive learning for cell stage classification of animal embryos","url":"https://arxiv.org/abs/2502.07360","date":1739768400,"author":"","guid":897,"unread":true,"content":"<article>arXiv:2502.07360v2 Announce Type: replace-cross \nAbstract: Video microscopy, when combined with machine learning, offers a promising approach for studying the early development of in vitro produced (IVP) embryos. However, manually annotating developmental events, and more specifically cell divisions, is time-consuming for a biologist and cannot scale up for practical applications. We aim to automatically classify the cell stages of embryos from 2D time-lapse microscopy videos with a deep learning approach. We focus on the analysis of bovine embryonic development using video microscopy, as we are primarily interested in the application of cattle breeding, and we have created a Bovine Embryos Cell Stages (ECS) dataset. The challenges are three-fold: (1) low-quality images and bovine dark cells that make the identification of cell stages difficult, (2) class ambiguity at the boundaries of developmental stages, and (3) imbalanced data distribution. To address these challenges, we introduce CLEmbryo, a novel method that leverages supervised contrastive learning combined with focal loss for training, and the lightweight 3D neural network CSN-50 as an encoder. We also show that our method generalizes well. CLEmbryo outperforms state-of-the-art methods on both our Bovine ECS dataset and the publicly available NYU Mouse Embryos dataset.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Effcient classical error correction for parity encoded spin systems","url":"https://arxiv.org/abs/2502.07170","date":1739768400,"author":"","guid":898,"unread":true,"content":"<article>arXiv:2502.07170v2 Announce Type: replace-cross \nAbstract: Fast solvers for combinatorial optimization problems (COPs) have attracted engineering interest in various industrial and social applications. Quantum annealing (QA) has emerged as a promising candidate and significant efforts have been dedicated to its development. Since COP is encoded in the Ising interaction between logical spins, its realization requires a spin system with all-to-all connectivity, which poses technical difficulties in the physical implementation of large-scale QA devices. W. Lechner, P. Hauke, and P. Zoller proposed parity-encoding (PE) architecture, consisting of a larger system of physical spins with only local connectivities between them, to avoid this diffculty in the near future QA device development. They suggested that this architecture not only reduces implementation diffculties and improves scalability, but also has intrinsic fault tolerance because logical spins are redundantly and nonlocally encoded into the physical spins. Nevertheless, it remains unclear how these advantageous features can be exploited. This paper addresses how to correct errors in a spin readout of PE architecture. Our work is based on the close connection between PE architecture and classical low-density parity-check (LDPC) codes. We have shown that independent and identically distributed errors in a spin readout can be corrected by a very simple decoding algorithm that can be regarded as a bit flipping (BF) algorithm for the LDPC codes. The BF algorithm was shown to have comparable performance to the belief propagation (BP) decoding algorithm. Furthermore, it is suggested that the introduction of post-readout BF decoding reduces the total computational cost and improves the performance of the global optimal solution search using the PE architecture. We believe that our results indicate that the PE architecture is a promising platform for near-term QA devices.</article>","contentLength":1953,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pre-Equalization Aided Grant-Free Massive Access in Massive MIMO System","url":"https://arxiv.org/abs/2502.06239","date":1739768400,"author":"","guid":899,"unread":true,"content":"<article>arXiv:2502.06239v2 Announce Type: replace-cross \nAbstract: The spatial diversity and multiplexing advantages of massive multi-input-multi-output (mMIMO) can significantly improve the capacity of massive non-orthogonal multiple access (NOMA) in machine type communications. However, state-of-the-art grant-free massive NOMA schemes for mMIMO systems require accurate estimation of random access channels to perform activity detection and the following coherent data demodulation, which suffers from excessive pilot overhead and access latency. To address this, we propose a pre-equalization aided grant-free massive access scheme for mMIMO systems, where an iterative detection scheme is conceived. Specifically, the base station (BS) firstly activates one of its antennas (i.e., beacon antenna) to broadcast a beacon signal, which facilitates the user equipment (UEs) to perform downlink channel estimation and pre-equalize the uplink random access signal with respect to the channels associated with the beacon antenna. During the uplink transmission stage, the BS detects UEs' activity and data by using the proposed iterative detection algorithm, which consists of three modules: coarse data detection (DD), data-aided channel estimation (CE), and fine DD. In the proposed algorithm, the joint activity and DD is firstly performed based on the signals received by the beacon antenna. Subsequently, the DD is further refined by iteratively performing data-aided CE module and fine DD module using signals received by all BS antennas. Our simulation results demonstrate that the proposed scheme outperforms state-of-the-art mMIMO-based grant-free massive NOMA schemes with the same access latency. Simulation codes are provided to reproduce the results in this article: https://github.com/owenwang517/tvt-2025.</article>","contentLength":1811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Less is More for Synthetic Speech Detection in the Wild","url":"https://arxiv.org/abs/2502.05674","date":1739768400,"author":"","guid":900,"unread":true,"content":"<article>arXiv:2502.05674v3 Announce Type: replace-cross \nAbstract: Driven by advances in self-supervised learning for speech, state-of-the-art synthetic speech detectors have achieved low error rates on popular benchmarks such as ASVspoof. However, prior benchmarks do not address the wide range of real-world variability in speech. Are reported error rates realistic in real-world conditions? To assess detector failure modes and robustness under controlled distribution shifts, we introduce ShiftySpeech, a benchmark with more than 3000 hours of synthetic speech from 7 domains, 6 TTS systems, 12 vocoders, and 3 languages. We found that all distribution shifts degraded model performance, and contrary to prior findings, training on more vocoders, speakers, or with data augmentation did not guarantee better generalization. In fact, we found that training on less diverse data resulted in better generalization, and that a detector fit using samples from a single carefully selected vocoder and a small number of speakers, without data augmentations, achieved state-of-the-art results on the challenging In-the-Wild benchmark.</article>","contentLength":1122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Regularized Newton Method for Nonconvex Optimization with Global and Local Complexity Guarantees","url":"https://arxiv.org/abs/2502.04799","date":1739768400,"author":"","guid":901,"unread":true,"content":"<article>arXiv:2502.04799v2 Announce Type: replace-cross \nAbstract: We consider the problem of finding an $\\epsilon$-stationary point of a nonconvex function with a Lipschitz continuous Hessian and propose a quadratic regularized Newton method incorporating a new class of regularizers constructed from the current and previous gradients. The method leverages a recently developed linear conjugate gradient approach with a negative curvature monitor to solve the regularized Newton equation. Notably, our algorithm is adaptive, requiring no prior knowledge of the Lipschitz constant of the Hessian, and achieves a global complexity of $O(\\epsilon^{-\\frac{3}{2}}) + \\tilde O(1)$ in terms of the second-order oracle calls, and $\\tilde O(\\epsilon^{-\\frac{7}{4}})$ for Hessian-vector products, respectively. Moreover, when the iterates converge to a point where the Hessian is positive definite, the method exhibits quadratic local convergence. Preliminary numerical results illustrate the competitiveness of our algorithm.</article>","contentLength":1010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedMimic: Physician-Inspired Multimodal Fusion for Early Diagnosis of Fever of Unknown Origin","url":"https://arxiv.org/abs/2502.04794","date":1739768400,"author":"","guid":902,"unread":true,"content":"<article>arXiv:2502.04794v2 Announce Type: replace-cross \nAbstract: Fever of unknown origin FUO remains a diagnostic challenge. MedMimic is introduced as a multimodal framework inspired by real-world diagnostic processes. It uses pretrained models such as DINOv2, Vision Transformer, and ResNet-18 to convert high-dimensional 18F-FDG PET/CT imaging into low-dimensional, semantically meaningful features. A learnable self-attention-based fusion network then integrates these imaging features with clinical data for classification. Using 416 FUO patient cases from Sichuan University West China Hospital from 2017 to 2023, the multimodal fusion classification network MFCN achieved macro-AUROC scores ranging from 0.8654 to 0.9291 across seven tasks, outperforming conventional machine learning and single-modality deep learning methods. Ablation studies and five-fold cross-validation further validated its effectiveness. By combining the strengths of pretrained large models and deep learning, MedMimic offers a promising solution for disease classification.</article>","contentLength":1050,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation","url":"https://arxiv.org/abs/2502.03930","date":1739768400,"author":"","guid":903,"unread":true,"content":"<article>arXiv:2502.03930v2 Announce Type: replace-cross \nAbstract: Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.</article>","contentLength":1246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gaussian Processes Regression for Uncertainty Quantification: An Introductory Tutorial","url":"https://arxiv.org/abs/2502.03090","date":1739768400,"author":"","guid":904,"unread":true,"content":"<article>arXiv:2502.03090v2 Announce Type: replace-cross \nAbstract: Gaussian Process Regression (GPR) is a powerful nonparametric regression method that is widely used in Uncertainty Quantification (UQ) for constructing surrogate models. This tutorial serves as an introductory guide for beginners, aiming to offer a structured and accessible overview of GPR's applications in UQ. We begin with an introduction to UQ and outline its key tasks, including uncertainty propagation, risk estimation, optimization under uncertainty, parameter estimation, and sensitivity analysis. We then introduce Gaussian Processes (GPs) as a surrogate modeling technique, detailing their formulation, choice of covariance kernels, hyperparameter estimation, and active learning strategies for efficient data acquisition. The tutorial further explores how GPR can be applied to different UQ tasks, including Bayesian quadrature for uncertainty propagation, active learning-based risk estimation, Bayesian optimization for optimization under uncertainty, and surrogate-based sensitivity analysis. Throughout, we emphasize how to leverage the unique formulation of GP for these UQ tasks, rather than simply using it as a standard surrogate model.</article>","contentLength":1216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Quality Control Of MRI Images Using Synthetic Motion Data","url":"https://arxiv.org/abs/2502.00160","date":1739768400,"author":"","guid":905,"unread":true,"content":"<article>arXiv:2502.00160v2 Announce Type: replace-cross \nAbstract: MRI quality control (QC) is challenging due to unbalanced and limited datasets, as well as subjective scoring, which hinder the development of reliable automated QC systems. To address these issues, we introduce an approach that pretrains a model on synthetically generated motion artifacts before applying transfer learning for QC classification. This method not only improves the accuracy in identifying poor-quality scans but also reduces training time and resource requirements compared to training from scratch. By leveraging synthetic data, we provide a more robust and resource-efficient solution for QC automation in MRI, paving the way for broader adoption in diverse research settings.</article>","contentLength":754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A topological theory for qLDPC: non-Clifford gates and magic state fountain on homological product codes with constant rate and beyond the $N^{1/3}$ distance barrier","url":"https://arxiv.org/abs/2501.19375","date":1739768400,"author":"","guid":906,"unread":true,"content":"<article>arXiv:2501.19375v2 Announce Type: replace-cross \nAbstract: We develop a unified theory for fault-tolerant quantum computation in quantum low-density parity-check (qLDPC) and topological codes. We show that there exist hidden simplicial complex structures encoding the topological data for all qLDPC and CSS codes obtained from product construction by generalizing the Freedman-Hastings code-to-manifold mapping. This is achieved by building manifolds corresponding to high-dimensional topological expanders from the Tanner graphs of the skeleton classical or quantum codes, which further form a product manifold and an associated thickened product code defined on its triangulation with only a constant qubit overhead. This suggests that qLDPC or more generally CSS codes obtained from product constructions are topological, and hence can admit cohomology operations such as cup products, physically corresponding to higher symmetries in the underlying topological quantum field theory. When applying this mapping to a 3D hypergraph product code obtained from the product of 3 copies of good classical expander codes, we obtain the first non-Clifford logical CCZ gates via constant depth circuits on a code with constant stabilizer weight $w=O(1)$, constant rate $K=\\Theta(N)$, and polynomial distance $D=\\Omega(N^{1/3})$. When applied to 3D homological product codes consisting of the product of a pair of good quantum and classical LDPC codes, we can further improve the distance to $D=\\Omega(\\sqrt{N})$ exceeding the $N^{1/3}$ distance barrier implied by the Bravyi-K\\\"onig bound for conventional topological codes. Our work suggests that it is feasible to apply native logical non-Clifford gates on qLDPC codes or directly inject high-fidelity magic states as resources (`magic state fountain') without the distillation process. For the homological product construction, the fountain can inject $\\Theta(\\sqrt{N})$ magic states in parallel in a single round.</article>","contentLength":1961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collaborative Channel Access and Transmission for NR Sidelink and Wi-Fi Coexistence over Unlicensed Spectrum","url":"https://arxiv.org/abs/2501.17878","date":1739768400,"author":"","guid":907,"unread":true,"content":"<article>arXiv:2501.17878v2 Announce Type: replace-cross \nAbstract: With the rapid development of various internet of things (IoT) applications, including industrial IoT (IIoT) and visual IoT (VIoT), the demand for direct device-to-device communication to support high data rates continues to grow. To address this demand, 5G-Advanced has introduced sidelink communication over the unlicensed spectrum (SL-U) to increase data rates. However, the primary challenge of SL-U in the unlicensed spectrum is ensuring fair coexistence with other incumbent systems, such as Wi-Fi. In this paper, we address the challenge by designing channel access mechanisms and power control strategies to mitigate interference and ensure fair coexistence. First, we propose a novel collaborative channel access (CCHA) mechanism that integrates channel access with resource allocation through collaborative interactions between base stations (BS) and SL-U users. This mechanism ensures fair coexistence with incumbent systems while improving resource utilization. Second, to further enhance the performance of the coexistence system, we develop a cooperative subgoal-based hierarchical deep reinforcement learning (C-GHDRL) algorithm framework. The framework enables SL-U users to make globally optimal decisions by leveraging cooperative operations between the BS and SL-U users, effectively overcoming the limitations of traditional optimization methods in solving joint optimization problems with nonlinear constraints. Finally, we mathematically model the joint channel access and power control problem and balance the trade-off between fairness and transmission rate in the coexistence system by defining a suitable reward function in the C-GHDRL algorithm. Simulation results demonstrate that the proposed scheme significantly enhances the performance of the coexistence system while ensuring fair coexistence between SL-U and Wi-Fi users.</article>","contentLength":1914,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artifact-free Sound Quality in DNN-based Closed-loop Systems for Audio Processing","url":"https://arxiv.org/abs/2501.04116","date":1739768400,"author":"","guid":908,"unread":true,"content":"<article>arXiv:2501.04116v2 Announce Type: replace-cross \nAbstract: Recent advances in deep neural networks (DNNs) have significantly improved various audio processing applications, including speech enhancement, synthesis, and hearing aid algorithms. DNN-based closed-loop systems have gained popularity in these applications due to their robust performance and ability to adapt to diverse conditions. Despite their effectiveness, current DNN-based closed-loop systems often suffer from sound quality degradation caused by artifacts introduced by suboptimal sampling methods. To address this challenge, we introduce dCoNNear, a novel DNN architecture designed for seamless integration into closed-loop frameworks. This architecture specifically aims to prevent the generation of spurious artifacts. We demonstrate the effectiveness of dCoNNear through a proof-of-principle example within a closed-loop framework that employs biophysically realistic models of auditory processing for both normal and hearing-impaired profiles to design personalized hearing aid algorithms. Our results show that dCoNNear not only accurately simulates all processing stages of existing non-DNN biophysical models but also eliminates audible artifacts, thereby enhancing the sound quality of the resulting hearing aid algorithms. This study presents a novel, artifact-free closed-loop framework that improves the sound quality of audio processing systems, offering a promising solution for high-fidelity applications in audio and hearing technologies.</article>","contentLength":1522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching","url":"https://arxiv.org/abs/2501.00135","date":1739768400,"author":"","guid":909,"unread":true,"content":"<article>arXiv:2501.00135v4 Announce Type: replace-cross \nAbstract: Quantum computing is an exciting non-Von Neumann paradigm, offering provable speedups over classical computing for specific problems. However, the practical limits of classical simulatability for quantum circuits remain unclear, especially with current noisy quantum devices. In this work, we explore the potential of leveraging Large Language Models (LLMs) to simulate the output of a quantum Turing machine using Grover's quantum circuits, known to provide quadratic speedups over classical counterparts. To this end, we developed GroverGPT, a specialized model based on LLaMA's 8-billion-parameter architecture, trained on over 15 trillion tokens. Unlike brute-force state-vector simulations, which demand substantial computational resources, GroverGPT employs pattern recognition to approximate quantum search algorithms without explicitly representing quantum states. Analyzing 97K quantum search instances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\\% accuracy), achieving nearly 100\\% accuracy on 6- and 10-qubit datasets when trained on 4-qubit or larger datasets. It also demonstrated strong generalization, surpassing 95\\% accuracy for systems with over 20 qubits when trained on 3- to 6-qubit data. Analysis indicates GroverGPT captures quantum features of Grover's search rather than classical patterns, supported by novel prompting strategies to enhance performance. Although accuracy declines with increasing system size, these findings offer insights into the practical boundaries of classical simulatability. This work suggests task-specific LLMs can surpass general-purpose models like GPT-4o in quantum algorithm learning and serve as powerful tools for advancing quantum research.</article>","contentLength":1771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Consolidated Volatility Prediction with Back Propagation Neural Network and Genetic Algorithm","url":"https://arxiv.org/abs/2412.07223","date":1739768400,"author":"","guid":910,"unread":true,"content":"<article>arXiv:2412.07223v4 Announce Type: replace-cross \nAbstract: This paper provides a unique approach with AI algorithms to predict emerging stock markets volatility. Traditionally, stock volatility is derived from historical volatility,Monte Carlo simulation and implied volatility as well. In this paper, the writer designs a consolidated model with back-propagation neural network and genetic algorithm to predict future volatility of emerging stock markets and found that the results are quite accurate with low errors.</article>","contentLength":518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption","url":"https://arxiv.org/abs/2412.01858","date":1739768400,"author":"","guid":911,"unread":true,"content":"<article>arXiv:2412.01858v4 Announce Type: replace-cross \nAbstract: The integration of fully homomorphic encryption (FHE) in federated learning (FL) has led to significant advances in data privacy. However, during the aggregation phase, it often results in performance degradation of the aggregated model, hindering the development of robust representational generalization. In this work, we propose a novel multimodal quantum federated learning framework that utilizes quantum computing to counteract the performance drop resulting from FHE. For the first time in FL, our framework combines a multimodal quantum mixture of experts (MQMoE) model with FHE, incorporating multimodal datasets for enriched representation and task-specific learning. Our MQMoE framework enhances performance on multimodal datasets and combined genomics and brain MRI scans, especially for underrepresented categories. Our results also demonstrate that the quantum-enhanced approach mitigates the performance degradation associated with FHE and improves classification accuracy across diverse datasets, validating the potential of quantum interventions in enhancing privacy in FL.</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Comprehensive Framework for Automated Segmentation of Perivascular Spaces in Brain MRI with the nnU-Net","url":"https://arxiv.org/abs/2411.19564","date":1739768400,"author":"","guid":912,"unread":true,"content":"<article>arXiv:2411.19564v2 Announce Type: replace-cross \nAbstract: Background: Enlargement of perivascular spaces (PVS) is common in neurodegenerative disorders including cerebral small vessel disease, Alzheimer's disease, and Parkinson's disease. PVS enlargement may indicate impaired clearance pathways and there is a need for reliable PVS detection methods which are currently lacking. Aim: To optimise a widely used deep learning model, the no-new-UNet (nnU-Net), for PVS segmentation. Methods: In 30 healthy participants (mean$\\pm$SD age: 50$\\pm$18.9 years; 13 females), T1-weighted MRI images were acquired using three different protocols on three MRI scanners (3T Siemens Tim Trio, 3T Philips Achieva, and 7T Siemens Magnetom). PVS were manually segmented across ten axial slices in each participant. Segmentations were completed using a sparse annotation strategy. In total, 11 models were compared using various strategies for image handling, preprocessing and semi-supervised learning with pseudo-labels. Model performance was evaluated using 5-fold cross validation (5FCV). The main performance metric was the Dice Similarity Coefficient (DSC). Results: The voxel-spacing agnostic model (mean$\\pm$SD DSC=64.3$\\pm$3.3%) outperformed models which resampled images to a common resolution (DSC=40.5-55%). Model performance improved substantially following iterative label cleaning (DSC=85.7$\\pm$1.2%). Semi-supervised learning with pseudo-labels (n=12,740) from 18 additional datasets improved the agreement between raw and predicted PVS cluster counts (Lin's concordance correlation coefficient=0.89, 95%CI=0.82-0.94). We extended the model to enable PVS segmentation in the midbrain (DSC=64.3$\\pm$6.5%) and hippocampus (DSC=67.8$\\pm$5%). Conclusions: Our deep learning models provide a robust and holistic framework for the automated quantification of PVS in brain MRI.</article>","contentLength":1870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Schr\\\"odinger Bridge Problem for Jump Diffusions","url":"https://arxiv.org/abs/2411.13765","date":1739768400,"author":"","guid":913,"unread":true,"content":"<article>arXiv:2411.13765v2 Announce Type: replace-cross \nAbstract: The Schr\\\"odinger bridge problem (SBP) seeks to find the measure $\\hat{\\mathbf{P}}$ on a certain path space which interpolates between state-space distributions $\\rho_0$ at time $0$ and $\\rho_T$ at time $T$ while minimizing the KL divergence (relative entropy) to a reference path measure $\\mathbf{R}$. In this work, we tackle the SBP in the case when $\\mathbf{R}$ is the path measure of a jump diffusion. Under mild assumptions, with both the operator theory approach and the stochastic calculus techniques, we establish an $h$-transform theory for jump diffusions and devise an approximation method to achieve the jump-diffusion SBP solution $\\hat{\\mathbf{P}}$ as the strong-convergence limit of a sequence of harmonic $h$-transforms. To the best of our knowledge, these results are novel in the study of SBP. Moreover, the $h$-transform framework and the approximation method developed in this work are robust and applicable to a relatively general class of jump diffusions. In addition, we examine the SBP of particular types of jump diffusions under additional regularity conditions and extend the existing results on the SBP from the diffusion case to the jump-diffusion setting.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature Learning in Chest CT Segmentation","url":"https://arxiv.org/abs/2411.13198","date":1739768400,"author":"","guid":914,"unread":true,"content":"<article>arXiv:2411.13198v2 Announce Type: replace-cross \nAbstract: In the field of medical image segmentation, challenges such as indistinct lesion features, ambiguous boundaries,and multi-scale characteristics have long revailed. This paper proposes an improved method named Intensity-Spatial Dual Masked AutoEncoder (ISD-MAE). Based on the tissue-contrast semi-masked autoencoder, a Masked AutoEncoder (MAE) branch is introduced to perform intensity masking and spatial masking operations on chest CT images for multi-scale feature learning and segmentation tasks. The model utilizes a dual-branch structure and contrastive learning to enhance the ability to learn tissue features and boundary details. Experiments are conducted on multiple 2D and 3D datasets. The results show that ISD-MAE significantly outperforms other methods in 2D pneumonia and mediastinal tumor segmentation tasks. For example, the Dice score reaches 90.10% on the COVID19 LESION dataset, and the performance is relatively stable. However, there is still room for improvement on 3D datasets. In response to this, improvement directions are proposed, including optimizing the loss function, using enhanced 3D convolution blocks, and processing datasets from multiple perspectives.Our code is available at:https://github.com/prowontheus/ISD-MAE.</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Bregman firmly nonexpansive proximal operator for baryconvex optimization","url":"https://arxiv.org/abs/2411.00928","date":1739768400,"author":"","guid":915,"unread":true,"content":"<article>arXiv:2411.00928v2 Announce Type: replace-cross \nAbstract: We present a generalization of the proximal operator defined through a convex combination of convex objectives, where the coefficients are updated in a minimax fashion. We prove that this new operator is Bregman firmly nonexpansive with respect to a Bregman divergence that combines Euclidean and information geometries. Finally, we derive the associated continuous flows.</article>","contentLength":431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MassSpecGym: A benchmark for the discovery and identification of molecules","url":"https://arxiv.org/abs/2410.23326","date":1739768400,"author":"","guid":916,"unread":true,"content":"<article>arXiv:2410.23326v3 Announce Type: replace-cross \nAbstract: The discovery and identification of molecules in biological and environmental samples is crucial for advancing biomedical and chemical sciences. Tandem mass spectrometry (MS/MS) is the leading technique for high-throughput elucidation of molecular structures. However, decoding a molecular structure from its mass spectrum is exceptionally challenging, even when performed by human experts. As a result, the vast majority of acquired MS/MS spectra remain uninterpreted, thereby limiting our understanding of the underlying (bio)chemical processes. Despite decades of progress in machine learning applications for predicting molecular structures from MS/MS spectra, the development of new methods is severely hindered by the lack of standard datasets and evaluation protocols. To address this problem, we propose MassSpecGym -- the first comprehensive benchmark for the discovery and identification of molecules from MS/MS data. Our benchmark comprises the largest publicly available collection of high-quality labeled MS/MS spectra and defines three MS/MS annotation challenges: de novo molecular structure generation, molecule retrieval, and spectrum simulation. It includes new evaluation metrics and a generalization-demanding data split, therefore standardizing the MS/MS annotation tasks and rendering the problem accessible to the broad machine learning community. MassSpecGym is publicly available at https://github.com/pluskal-lab/MassSpecGym.</article>","contentLength":1510,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Statistical Complexity of Estimating Vendi Scores from Empirical Data","url":"https://arxiv.org/abs/2410.21719","date":1739768400,"author":"","guid":917,"unread":true,"content":"<article>arXiv:2410.21719v2 Announce Type: replace-cross \nAbstract: Evaluating the diversity of generative models without access to reference data poses methodological challenges. The reference-free Vendi score offers a solution by quantifying the diversity of generated data using matrix-based entropy measures. The Vendi score is usually computed via the eigendecomposition of an $n \\times n$ kernel matrix for $n$ generated samples. However, the heavy computational cost of eigendecomposition for large $n$ often limits the sample size used in practice to a few tens of thousands. In this paper, we investigate the statistical convergence of the Vendi score. We numerically demonstrate that for kernel functions with an infinite feature map dimension, the score estimated from a limited sample size may exhibit a non-negligible bias relative to the population Vendi score, i.e., the asymptotic limit as the sample size approaches infinity. To address this, we introduce a truncation of the Vendi statistic, called the $t$-truncated Vendi statistic, which is guaranteed to converge to its asymptotic limit given $n=O(t)$ samples. We show that the existing Nystr\\\"om method and the FKEA approximation method for approximating the Vendi score both converge to the population truncated Vendi score. We perform several numerical experiments to illustrate the concentration of the Nystr\\\"om and FKEA-computed Vendi scores around the truncated Vendi and discuss how the truncated Vendi score correlates with the diversity of image and text data.</article>","contentLength":1532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computing MHD equilibria of stellarators with a flexible coordinate frame","url":"https://arxiv.org/abs/2410.17595","date":1739768400,"author":"","guid":918,"unread":true,"content":"<article>arXiv:2410.17595v3 Announce Type: replace-cross \nAbstract: For the representation of axi-symmetric plasma configurations, it is natural to use cyl. coordinates (R,Z,$\\phi$), where $\\phi$ is an independent coordinate. The same cyl. coordinates have also been widely used for representing 3D MHD equilibria of non-axisymmetric configurations (stellarators), with cross-sections, defined in RZ-planes, that vary over $\\phi$. Stellarator equilibria have been found, however, for which cyl. coordinates are not at all a natural choice, for instance certain stellarators obtained using the near-axis expansion (NAE), defined by a magn. axis curve and its Frenet frame.\n  In this contribution, we propose an alternative approach for representing the boundary in a fixed-boundary 3D MHD equil. solver, moving away from cyl. coordinates. Instead, we use planar cross-sections whose orientation is determined by a general coordinate frame (G-Frame). This frame is similar to the conventional Frenet frame, but more flexible. As an additional part of the boundary representation, it becomes an input to the equil. solve, along with the geometry of the cross-sections. We see two advantages: 1) the capability to easily represent configurations where the magn. axis is highly non-planar or even knotted 2) a reduction in the degrees of freedom needed for the boundary surface, and thus the equil. solver, enabling progress in optimization of these configurations.\n  We discuss the properties of the G-Frame, starting from the conventional Frenet frame. Then we show two exemplary ways of constructing it, first from a NAE solution and also from a given boundary surface. We present the details of the implementation of the new frame in the 3D MHD equil. solver GVEC. Furthermore, we demonstrate for a highly shaped QI-optimized stellarator that far fewer degrees of freedom are necessary to find a high quality equil. solution, compared to the solution computed in cyl. coordinates.</article>","contentLength":1970,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The shape of the brain's connections is predictive of cognitive performance: an explainable machine learning study","url":"https://arxiv.org/abs/2410.15108","date":1739768400,"author":"","guid":919,"unread":true,"content":"<article>arXiv:2410.15108v2 Announce Type: replace-cross \nAbstract: The shape of the brain's white matter connections is relatively unexplored in diffusion MRI tractography analysis. While it is known that tract shape varies in populations and across the human lifespan, it is unknown if the variability in dMRI tractography-derived shape may relate to the brain's functional variability across individuals. This work explores the potential of leveraging tractography fiber cluster shape measures to predict subject-specific cognitive performance. We implement machine learning models to predict individual cognitive performance scores. We study a large-scale database from the HCP-YA study. We apply an atlas-based fiber cluster parcellation to the dMRI tractography of each individual. We compute 15 shape, microstructure, and connectivity features for each fiber cluster. Using these features as input, we train a total of 210 models to predict 7 different NIH Toolbox cognitive performance assessments. We apply an explainable AI technique, SHAP, to assess the importance of each fiber cluster for prediction. Our results demonstrate that shape measures are predictive of individual cognitive performance. The studied shape measures, such as irregularity, diameter, total surface area, volume, and branch volume, are as effective for prediction as microstructure and connectivity measures. The overall best-performing feature is a shape feature, irregularity, which describes how different a cluster's shape is from an idealized cylinder. Further interpretation using SHAP values suggest that fiber clusters with features highly predictive of cognitive ability are widespread throughout the brain, including fiber clusters from the superficial association, deep association, cerebellar, striatal, and projection pathways. This study demonstrates the strong potential of shape descriptors to enhance the study of the brain's white matter and its relationship to cognitive function.</article>","contentLength":1975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero-shot Model-based Reinforcement Learning using Large Language Models","url":"https://arxiv.org/abs/2410.11711","date":1739768400,"author":"","guid":920,"unread":true,"content":"<article>arXiv:2410.11711v2 Announce Type: replace-cross \nAbstract: The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs' deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.</article>","contentLength":1137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Channel Distinguishability in Local Neighborhoods of the Model Space in Quantum Neural Networks","url":"https://arxiv.org/abs/2410.09470","date":1739768400,"author":"","guid":921,"unread":true,"content":"<article>arXiv:2410.09470v2 Announce Type: replace-cross \nAbstract: With the increasing interest in Quantum Machine Learning, Quantum Neural Networks (QNNs) have emerged and gained significant attention. These models have, however, been shown to be notoriously difficult to train, which we hypothesize is partially due to the architectures, called ansatzes, that are hardly studied at this point. Therefore, in this paper, we take a step back and analyze ansatzes. We initially consider their expressivity, i.e., the space of operations they are able to express, and show that the closeness to being a 2-design, the primarily used measure, fails at capturing this property. Hence, we look for alternative ways to characterize ansatzes by considering the local neighborhood of the model space, in particular, analyzing model distinguishability upon small perturbation of parameters. We derive an upper bound on their distinguishability, showcasing that QNNs with few parameters are hardly discriminable upon update. Our numerical experiments support our bounds and further indicate that there is a significant degree of variability, which stresses the need for warm-starting or clever initialization. Altogether, our work provides an ansatz-centric perspective on training dynamics and difficulties in QNNs, ultimately suggesting that iterative training of small quantum models may not be effective, which contrasts their initial motivation.</article>","contentLength":1431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CR-CTC: Consistency regularization on CTC for improved speech recognition","url":"https://arxiv.org/abs/2410.05101","date":1739768400,"author":"","guid":922,"unread":true,"content":"<article>arXiv:2410.05101v4 Announce Type: replace-cross \nAbstract: Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input speech mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC. It significantly improves the CTC performance, achieving state-of-the-art results comparable to those attained by transducer or systems combining CTC and attention-based encoder-decoder (CTC/AED). We release our code at https://github.com/k2-fsa/icefall.</article>","contentLength":1337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Performance and Robustness of Signal-Dependent vs. Signal-Independent Binaural Signal Matching with Wearable Microphone Arrays","url":"https://arxiv.org/abs/2409.11731","date":1739768400,"author":"","guid":923,"unread":true,"content":"<article>arXiv:2409.11731v3 Announce Type: replace-cross \nAbstract: The increasing popularity of spatial audio in applications such as teleconferencing, entertainment, and virtual reality has led to the recent developments of binaural reproduction methods. However, only a few of these methods are well-suited for wearable and mobile arrays, which typically consist of a small number of microphones. One such method is binaural signal matching (BSM), which has been shown to produce high-quality binaural signals for wearable arrays. However, BSM may be suboptimal in cases of high direct-to-reverberant ratio (DRR) as it is based on the diffuse sound field assumption. To overcome this limitation, previous studies incorporated sound-field models other than diffuse. However, performance may be sensitive to signal estimation errors. This paper aims to provide a systematic and comprehensive analysis of signal-dependent vs. signal-independent BSM, so that the benefits and limitations of the methods become clearer. Two signal-dependent BSM-based methods designed for high DRR scenarios that incorporate a sound field model composed of direct and reverberant components are investigated mathematically, using simulations, and finally validated by a listening test, and compared to the signal-independent BSM. The results show that signal-dependent BSM can significantly improve performance, in particular in the direction of the source, while presenting only a negligible degradation in other directions. Furthermore, when source direction estimation is inaccurate, performance of of the signal-dependent BSM degrade to equal that of the signal-independent BSM, presenting a desired robustness quality.</article>","contentLength":1695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Application of Langevin Dynamics to Advance the Quantum Natural Gradient Optimization Algorithm","url":"https://arxiv.org/abs/2409.01978","date":1739768400,"author":"","guid":924,"unread":true,"content":"<article>arXiv:2409.01978v3 Announce Type: replace-cross \nAbstract: A Quantum Natural Gradient (QNG) algorithm for optimization of variational quantum circuits has been proposed recently. In this study, we employ the Langevin equation with a QNG stochastic force to demonstrate that its discrete-time solution gives a generalized form of the above-specified algorithm, which we call Momentum-QNG. Similar to other optimization algorithms with the momentum term, such as the Stochastic Gradient Descent with momentum, RMSProp with momentum and Adam, Momentum-QNG is more effective to escape local minima and plateaus in the variational parameter space and, therefore, achieves a better convergence behavior compared to the basic QNG. In this paper we benchmark Momentum-QNG together with basic QNG, Adam and Momentum optimizers and find the optimal values of its hyperparameters. Our open-source code is available at https://github.com/borbysh/Momentum-QNG</article>","contentLength":946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Orientable and negative orientable sequences","url":"https://arxiv.org/abs/2409.00672","date":1739768400,"author":"","guid":925,"unread":true,"content":"<article>arXiv:2409.00672v5 Announce Type: replace-cross \nAbstract: Analogously to de Bruijn sequences, orientable sequences have application in automatic position-location applications and, until recently, studies of these sequences focused on the binary case. In recent work by Alhakim et al., a range of methods of construction were described for orientable sequences over arbitrary finite alphabets; some of these methods involve using negative orientable sequences as a building block. In this paper we describe three techniques for generating such negative orientable sequences, as well as upper bounds on their period. We then go on to show how these negative orientable sequences can be used to generate orientable sequences with period close to the maximum possible for every non-binary alphabet size and for every tuple length. In doing so we use two closely related approaches described by Alhakim et al.</article>","contentLength":906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hedge Fund Portfolio Construction Using PolyModel Theory and iTransformer","url":"https://arxiv.org/abs/2408.03320","date":1739768400,"author":"","guid":926,"unread":true,"content":"<article>arXiv:2408.03320v3 Announce Type: replace-cross \nAbstract: When constructing portfolios, a key problem is that a lot of financial time series data are sparse, making it challenging to apply machine learning methods. Polymodel theory can solve this issue and demonstrate superiority in portfolio construction from various aspects. To implement the PolyModel theory for constructing a hedge fund portfolio, we begin by identifying an asset pool, utilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory also involves choosing a wide-ranging set of risk factors, which includes various financial indices, currencies, and commodity prices. This comprehensive selection mirrors the complexities of the real-world environment. Leveraging on the PolyModel theory, we create quantitative measures such as Long-term Alpha, Long-term Ratio, and SVaR. We also use more classical measures like the Sharpe ratio or Morningstar's MRAR. To enhance the performance of the constructed portfolio, we also employ the latest deep learning techniques (iTransformer) to capture the upward trend, while efficiently controlling the downside, using all the features. The iTransformer model is specifically designed to address the challenges in high-dimensional time series forecasting and could largely improve our strategies. More precisely, our strategies achieve better Sharpe ratio and annualized return. The above process enables us to create multiple portfolio strategies aiming for high returns and low risks when compared to various benchmarks.</article>","contentLength":1550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MEMS and ECM Sensor Technologies for Cardiorespiratory Sound Monitoring - A Comprehensive Review","url":"https://arxiv.org/abs/2406.12432","date":1739768400,"author":"","guid":927,"unread":true,"content":"<article>arXiv:2406.12432v2 Announce Type: replace-cross \nAbstract: This paper presents a comprehensive review of cardiorespiratory auscultation sensing devices (i.e., stethoscopes), which is useful for understanding the theoretical aspects and practical design notes. In this paper, we first introduce the acoustic properties of the heart and lungs, as well as a brief history of stethoscope evolution. Then, we discuss the basic concept of electret condenser microphones (ECMs) and a stethoscope based on them. Then, we discuss the microelectromechanical systems (MEMSs) technology, particularly focusing on piezoelectric transducer sensors. This paper comprehensively reviews sensing technologies for cardiorespiratory auscultation, emphasizing MEMS-based wearable designs in the past decade. To our knowledge, this is the first paper to summarize ECM and MEMS applications for heart and lung sound analysis.</article>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Joint semi-supervised and contrastive learning enables domain generalization and multi-domain segmentation","url":"https://arxiv.org/abs/2405.05336","date":1739768400,"author":"","guid":928,"unread":true,"content":"<article>arXiv:2405.05336v2 Announce Type: replace-cross \nAbstract: Despite their effectiveness, current deep learning models face challenges with images coming from different domains with varying appearance and content. We introduce SegCLR, a versatile framework designed to segment images across different domains, employing supervised and contrastive learning simultaneously to effectively learn from both labeled and unlabeled data. We demonstrate the superior performance of SegCLR through a comprehensive evaluation involving three diverse clinical datasets of 3D retinal Optical Coherence Tomography (OCT) images, for the slice-wise segmentation of fluids with various network configurations and verification across 10 different network initializations. In an unsupervised domain adaptation context, SegCLR achieves results on par with a supervised upper-bound model trained on the intended target domain. Notably, we discover that the segmentation performance of SegCLR framework is marginally impacted by the abundance of unlabeled data from the target domain, thereby we also propose an effective domain generalization extension of SegCLR, known also as zero-shot domain adaptation, which eliminates the need for any target domain information. This shows that our proposed addition of contrastive loss in standard supervised training for segmentation leads to superior models, inherently more generalizable to both in- and out-of-domain test data. We additionally propose a pragmatic solution for SegCLR deployment in realistic scenarios with multiple domains containing labeled data. Accordingly, our framework pushes the boundaries of deep-learning based segmentation in multi-domain applications, regardless of data availability - labeled, unlabeled, or nonexistent.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Layered Uploading for Quantum Convolutional Neural Networks","url":"https://arxiv.org/abs/2404.09750","date":1739768400,"author":"","guid":929,"unread":true,"content":"<article>arXiv:2404.09750v2 Announce Type: replace-cross \nAbstract: Continuing our analysis of quantum machine learning applied to our use-case of malware detection, we investigate the potential of quantum convolutional neural networks. More precisely, we propose a new architecture where data is uploaded all along the quantum circuit. This allows us to use more features from the data, hence giving to the algorithm more information, without having to increase the number of qubits that we use for the quantum circuit. This approach is motivated by the fact that we do not always have great amounts of data, and that quantum computers are currently restricted in their number of logical qubits.</article>","contentLength":687,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computing the spectrum and pseudospectrum of infinite-volume operators from local patches","url":"https://arxiv.org/abs/2403.19055","date":1739768400,"author":"","guid":930,"unread":true,"content":"<article>arXiv:2403.19055v2 Announce Type: replace-cross \nAbstract: We show how the spectrum of normal discrete short-range infinite-volume operators can be approximated with two-sided error control using only data from finite-sized local patches. As a corollary, we prove the computability of the spectrum of such infinite-volume operators with the additional property of finite local complexity and provide an explicit algorithm. Such operators appear in many applications, e.g. as discretizations of differential operators on unbounded domains or as so-called tight-binding Hamiltonians in solid state physics. For a large class of such operators, our result allows for the first time to establish computationally also the absence of spectrum, i.e. the existence and the size of spectral gaps. We extend our results to the $\\varepsilon$-pseudospectrum of non-normal operators, proving that also the pseudospectrum of such operators is computable.</article>","contentLength":940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium","url":"https://arxiv.org/abs/2402.02303","date":1739768400,"author":"","guid":931,"unread":true,"content":"<article>arXiv:2402.02303v3 Announce Type: replace-cross \nAbstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evidence of Scaling Regimes in the Hopfield Dynamics of Whole Brain Model","url":"https://arxiv.org/abs/2401.07538","date":1739768400,"author":"","guid":932,"unread":true,"content":"<article>arXiv:2401.07538v3 Announce Type: replace-cross \nAbstract: It is shown that a Hopfield recurrent neural network exhibits a scaling regime, whose specific exponents depend on the number of parcels used and the decay length of the coupling strength. This scaling regime recovers the picture introduced by Deco et al., according to which the process of information transfer within the human brain shows spatially correlated patterns qualitatively similar to those displayed by turbulent flows, although with a more singular exponent, 1/2 instead of 2/3. Both models employ a coupling strength which decays exponentially with the Euclidean distance between the nodes, informed by experimentally derived brain topology. Nevertheless, their mathematical nature is very different, Hopf oscillators versus a Hopfield neural network, respectively. Hence, their convergence for the same data parameters, suggests an intriguing robustness of the scaling picture.Furthermore, the present analysis shows that the Hopfield model brain remains functional by removing links above about five decay lengths, corresponding to about one sixth of the size of the global brain. This suggests that, in terms of connectivity decay length, the Hopfield brain functions in a sort of intermediate ``turbulent liquid''-like state, whose essential connections are the intermediate ones between the connectivity decay length and the global brain size. The evident sensitivity of the scaling exponent to the value of the decay length, as well as to the number of brain parcels employed, leads us to take with great caution any quantitative assessment regarding the specific nature of the scaling regime.</article>","contentLength":1672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Auto.gov: Learning-based Governance for Decentralized Finance (DeFi)","url":"https://arxiv.org/abs/2302.09551","date":1739768400,"author":"","guid":933,"unread":true,"content":"<article>arXiv:2302.09551v3 Announce Type: replace-cross \nAbstract: Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional DeFi governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce \"Auto.gov\", a learning-based governance framework that employs a deep Q-network (DQN) reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto.gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto.gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric-protocol profitability. Overall, the comprehensive evaluations confirm that Auto.gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.</article>","contentLength":1552,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pitfalls of Evidence-Based AI Policy","url":"https://arxiv.org/abs/2502.09618","date":1739768400,"author":"","guid":934,"unread":true,"content":"<article>arXiv:2502.09618v2 Announce Type: replace \nAbstract: Nations across the world are working to govern AI. However, from a technical perspective, there is uncertainty and disagreement on the best way to do this. Meanwhile, recent debates over AI regulation have led to calls for \"evidence-based AI policy\" which emphasize holding regulatory action to a high evidentiary standard. Evidence is of irreplaceable value to policymaking. However, holding regulatory action to too high an evidentiary standard can lead to systematic neglect of certain risks. In historical policy debates (e.g., over tobacco ca. 1965 and fossil fuels ca. 1985) \"evidence-based policy\" rhetoric is also a well-precedented strategy to downplay the urgency of action, delay regulation, and protect industry interests. Here, we argue that if the goal is evidence-based AI policy, the first regulatory objective must be to actively facilitate the process of identifying, studying, and deliberating about AI risks. We discuss a set of 15 regulatory goals to facilitate this and show that Brazil, Canada, China, the EU, South Korea, the UK, and the USA all have substantial opportunities to adopt further evidence-seeking policies.</article>","contentLength":1197,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Score-of-Mixture Training: Training One-Step Generative Models Made Simple via Score Estimation of Mixture Distributions","url":"https://arxiv.org/abs/2502.09609","date":1739768400,"author":"","guid":935,"unread":true,"content":"<article>arXiv:2502.09609v2 Announce Type: replace \nAbstract: We propose Score-of-Mixture Training (SMT), a novel framework for training one-step generative models by minimizing a class of divergences called the $\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score of mixture distributions between real and fake samples across multiple noise levels. Similar to consistency models, our approach supports both training from scratch (SMT) and distillation using a pretrained diffusion model, which we call Score-of-Mixture Distillation (SMD). It is simple to implement, requires minimal hyperparameter tuning, and ensures stable training. Experiments on CIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even outperform existing methods.</article>","contentLength":775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering","url":"https://arxiv.org/abs/2502.09573","date":1739768400,"author":"","guid":936,"unread":true,"content":"<article>arXiv:2502.09573v2 Announce Type: replace \nAbstract: In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification systems across various domains in industry.</article>","contentLength":891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Entropy Collapse in Mobile Sensors: The Hidden Risks of Sensor-Based Security","url":"https://arxiv.org/abs/2502.09535","date":1739768400,"author":"","guid":937,"unread":true,"content":"<article>arXiv:2502.09535v2 Announce Type: replace \nAbstract: Mobile sensor data has been proposed for security-critical applications such as device pairing, proximity detection, and continuous authentication. However, the foundational assumption that these signals provide sufficient entropy remains under-explored. In this work, we systematically analyse the entropy of mobile sensor data across four diverse datasets spanning multiple application contexts. Our findings reveal pervasive biases, with single-sensor mean min-entropy values ranging from 3.408-4.483 bits (S.D.=1.018-1.574) despite Shannon entropy being several multiples higher. We further demonstrate that correlations between sensor modalities reduce the worst-case entropy of using multiple sensors by up to approx. 75% compared to average-case Shannon entropy. This brings joint min-entropy well below 10 bits in many cases and, in the best case, yielding only approx. 24 bits of min-entropy when combining 20 sensor modalities. These results call into question the widely held assumption that adding more sensors inherently yields higher security. We ultimately caution against relying on raw sensor data as a primary source of randomness.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling","url":"https://arxiv.org/abs/2502.09509","date":1739768400,"author":"","guid":938,"unread":true,"content":"<article>arXiv:2502.09509v2 Announce Type: replace \nAbstract: Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.</article>","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic Forgetting","url":"https://arxiv.org/abs/2502.09500","date":1739768400,"author":"","guid":939,"unread":true,"content":"<article>arXiv:2502.09500v2 Announce Type: replace \nAbstract: Catastrophic forgetting -- the phenomenon of a neural network learning a task t1 and losing the ability to perform it after being trained on some other task t2 -- is a long-standing problem for neural networks [McCloskey and Cohen, 1989]. We present a method, Eidetic Learning, that provably solves catastrophic forgetting. A network trained with Eidetic Learning -- here, an EideticNet -- requires no rehearsal or replay. We consider successive discrete tasks and show how at inference time an EideticNet automatically routes new instances without auxiliary task information. An EideticNet bears a family resemblance to the sparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network capacity is partitioned across tasks and the network itself performs data-conditional routing. An EideticNet is easy to implement and train, is efficient, and has time and space complexity linear in the number of parameters. The guarantee of our method holds for normalization layers of modern neural networks during both pre-training and fine-tuning. We show with a variety of network architectures and sets of tasks that EideticNets are immune to forgetting. While the practical benefits of EideticNets are substantial, we believe they can be benefit practitioners and theorists alike. The code for training EideticNets is available at https://github.com/amazon-science/eideticnet-training.</article>","contentLength":1448,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Predict Global Atrial Fibrillation Dynamics from Sparse Measurements","url":"https://arxiv.org/abs/2502.09473","date":1739768400,"author":"","guid":940,"unread":true,"content":"<article>arXiv:2502.09473v2 Announce Type: replace \nAbstract: Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all treatment with limited success in persistent AF. This may be due to our inability to map the dynamics of AF with the limited resolution and coverage provided by sequential contact mapping catheters, preventing effective patient phenotyping for personalised, targeted ablation. Here we introduce FibMap, a graph recurrent neural network model that reconstructs global AF dynamics from sparse measurements. Trained and validated on 51 non-contact whole atria recordings, FibMap reconstructs whole atria dynamics from 10% surface coverage, achieving a 210% lower mean absolute error and an order of magnitude higher performance in tracking phase singularities compared to baseline methods. Clinical utility of FibMap is demonstrated on real-world contact mapping recordings, achieving reconstruction fidelity comparable to non-contact mapping. FibMap's state-spaces and patient-specific parameters offer insights for electrophenotyping AF. Integrating FibMap into clinical practice could enable personalised AF care and improve outcomes.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of harmonic average method for interface problems with discontinuous solutions and fluxes","url":"https://arxiv.org/abs/2502.09413","date":1739768400,"author":"","guid":941,"unread":true,"content":"<article>arXiv:2502.09413v2 Announce Type: replace \nAbstract: Harmonic average method has been widely utilized to deal with heterogeneous coefficients in solving differential equations. One remarkable advantage of the harmonic averaging method is that no derivative of the coefficient is needed. Furthermore, the coefficient matrix of the finite difference equations is an M-matrix which guarantees the stability of the algorithm. It has been numerically observed but not theoretically proved that the method produces second order pointwise accuracy when the solution and flux are continuous even if the coefficient has finite discontinuities for which the method is inconsistent ($O(1)$ in the local truncation errors). It has been believed that there are some fortunate error cancellations. The harmonic average method does not converge when the solution or the flux has finite discontinuities. In this paper, not only we rigorously prove the second order convergence of the harmonic averaging method for one-dimensional interface problem when the coefficient has a finite discontinuities and the solution and the flux are continuous, but also proposed an {\\em improved harmonic average method} that is also second order accurate (in the $L^{\\infty}$ norm), which allows discontinuous solutions and fluxes along with the discontinuous coefficients. The key in the convergence proof is the construction of the Green's function. The proof shows how the error cancellations occur in a subtle way. Numerical experiments in both 1D and 2D confirmed the theoretical proof of the improved harmonic average method.</article>","contentLength":1599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code Style Sheets: CSS for Code","url":"https://arxiv.org/abs/2502.09386","date":1739768400,"author":"","guid":942,"unread":true,"content":"<article>arXiv:2502.09386v2 Announce Type: replace \nAbstract: Program text is rendered using impoverished typographic styles. Beyond choice of fonts and syntax-highlighting colors, code editors and related tools utilize very few text decorations. These limited styles are, furthermore, applied in monolithic fashion, regardless of the programs and tasks at hand.\n  We present the notion of code style sheets for styling the textual representation of programs. Motivated by analogy to cascading style sheets (CSS) for styling HTML documents, code style sheets provide mechanisms for defining rules to select and style abstract syntax trees (ASTs). Technically, code style sheets generalize essential notions from CSS over untyped HTML trees to a programming-language setting with algebraic data types (such as ASTs). Practically, code style sheets allow ASTs to be styled granularly, based on semantic information -- such as the structure of abstract syntax, static type information, and corresponding run-time values -- as well as design choices on the part of authors and readers of a program. In this paper, we design and implement a code style sheets system for a subset of Haskell, using it to illustrate several use cases involving code presentation and visualization tasks. These examples demonstrate that code style sheets provide a uniform framework for rendering programs in multivarious ways.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LoRA Training Provably Converges to a Low-Rank Global Minimum or It Fails Loudly (But it Probably Won't Fail)","url":"https://arxiv.org/abs/2502.09376","date":1739768400,"author":"","guid":943,"unread":true,"content":"<article>arXiv:2502.09376v2 Announce Type: replace \nAbstract: Low-rank adaptation (LoRA) has become a standard approach for fine-tuning large foundation models. However, our theoretical understanding of LoRA remains limited as prior analyses of LoRA's training dynamics either rely on linearization arguments or consider highly simplified setups. In this work, we analyze the LoRA loss landscape without such restrictive assumptions. We define two regimes: a ``special regime'', which includes idealized setups where linearization arguments hold, and a ``generic regime'' representing more realistic setups where linearization arguments do not hold. In the generic regime, we show that LoRA training converges to a global minimizer with low rank and small magnitude, or a qualitatively distinct solution with high rank and large magnitude. Finally, we argue that the zero-initialization and weight decay in LoRA training induce an implicit bias toward the low-rank, small-magnitude region of the parameter space -- where global minima lie -- thus shedding light on why LoRA training usually succeeds in finding global minima.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delay Performance Analysis with Short Packets in Intelligent Machine Network","url":"https://arxiv.org/abs/2502.09313","date":1739768400,"author":"","guid":944,"unread":true,"content":"<article>arXiv:2502.09313v2 Announce Type: replace \nAbstract: With the rapid development of delay-sensitive services happened in industrial manufacturing, Internet of Vehicles, and smart logistics, more stringent delay requirements are put forward for the intelligent machine (IM) network. Short packet transmissions are widely adopted to reduce delay in IM networks. However, the delay performance of an IM network has not been sufficiently analyzed. This paper applies queuing theory and stochastic geometry to construct network model and transmission model for downlink communication, respectively, proposes and derives the following three metrics, e.g., the transmission success probability (with delay as the threshold), expected delay, and delay jitter. To accurately characterize the transmission delay with short packets, the finite blocklength capacity is used to measure the channel transmission rate. Simulation results show that the increase of packet length and IM density significantly deteriorates the three metrics. Short packets are needed to improve the three metrics, especially in high IM density scenarios. The outcomes of this paper provide an important theoretical basis for the optimization design and performance improvement of IM networks.</article>","contentLength":1256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Safety for Everyone","url":"https://arxiv.org/abs/2502.09288","date":1739768400,"author":"","guid":945,"unread":true,"content":"<article>arXiv:2502.09288v2 Announce Type: replace \nAbstract: Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI systems, suggesting that work on AI safety necessarily entails serious consideration of potential existential threats. However, this framing has three potential drawbacks: it may exclude researchers and practitioners who are committed to AI safety but approach the field from different angles; it could lead the public to mistakenly view AI safety as focused solely on existential scenarios rather than addressing a wide spectrum of safety challenges; and it risks creating resistance to safety measures among those who disagree with predictions of existential AI risks. Through a systematic literature review of primarily peer-reviewed research, we find a vast array of concrete safety work that addresses immediate and practical concerns with current AI systems. This includes crucial areas like adversarial robustness and interpretability, highlighting how AI safety research naturally extends existing technological and systems safety concerns and practices. Our findings suggest the need for an epistemically inclusive and pluralistic conception of AI safety that can accommodate the full range of safety considerations, motivations, and perspectives that currently shape the field.</article>","contentLength":1390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection","url":"https://arxiv.org/abs/2502.09271","date":1739768400,"author":"","guid":946,"unread":true,"content":"<article>arXiv:2502.09271v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in modeling data with graph structures, yet recent research reveals their susceptibility to adversarial attacks. Traditional attack methodologies, which rely on manipulating the original graph or adding links to artificially created nodes, often prove impractical in real-world settings. This paper introduces a novel adversarial scenario involving the injection of an isolated subgraph to deceive both the link recommender and the node classifier within a GNN system. Specifically, the link recommender is mislead to propose links between targeted victim nodes and the subgraph, encouraging users to unintentionally establish connections and that would degrade the node classification accuracy, thereby facilitating a successful attack. To address this, we present the LiSA framework, which employs a dual surrogate model and bi-level optimization to simultaneously meet two adversarial objectives. Extensive experiments on real-world datasets demonstrate the effectiveness of our method.</article>","contentLength":1112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation","url":"https://arxiv.org/abs/2502.09268","date":1739768400,"author":"","guid":947,"unread":true,"content":"<article>arXiv:2502.09268v2 Announce Type: replace \nAbstract: With the rapid development of embodied artificial intelligence, significant progress has been made in vision-language-action (VLA) models for general robot decision-making. However, the majority of existing VLAs fail to account for the inevitable external perturbations encountered during deployment. These perturbations introduce unforeseen state information to the VLA, resulting in inaccurate actions and consequently, a significant decline in generalization performance. The classic internal model control (IMC) principle demonstrates that a closed-loop system with an internal model that includes external input signals can accurately track the reference input and effectively offset the disturbance. We propose a novel closed-loop VLA method GEVRM that integrates the IMC principle to enhance the robustness of robot visual manipulation. The text-guided video generation model in GEVRM can generate highly expressive future visual planning goals. Simultaneously, we evaluate perturbations by simulating responses, which are called internal embeddings and optimized through prototype contrastive learning. This allows the model to implicitly infer and distinguish perturbations from the external environment. The proposed GEVRM achieves state-of-the-art performance on both standard and perturbed CALVIN benchmarks and shows significant improvements in realistic robot tasks.</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Commitment Schemes from OWFs with Applications to Quantum Oblivious Transfer","url":"https://arxiv.org/abs/2502.09201","date":1739768400,"author":"","guid":948,"unread":true,"content":"<article>arXiv:2502.09201v2 Announce Type: replace \nAbstract: Commitment schemes are essential to many cryptographic protocols and schemes with applications that include privacy-preserving computation on data, privacy-preserving authentication, and, in particular, oblivious transfer protocols. For quantum oblivious transfer (qOT) protocols, unconditionally binding commitment schemes that do not rely on hardness assumptions from structured mathematical problems are required. These additional constraints severely limit the choice of commitment schemes to random oracle-based constructions or Naor's bit commitment scheme. As these protocols commit to individual bits, the use of such commitment schemes comes at a high bandwidth and computational cost. In this work, we investigate improvements to the efficiency of commitment schemes used in qOT protocols and propose an extension of Naor's commitment scheme requiring the existence of one-way functions (OWF) to reduce communication complexity for 2-bit strings. Additionally, we provide an interactive string commitment scheme with preprocessing to enable a fast and efficient computation of commitments.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An hp Multigrid Approach for Tensor-Product Space-Time Finite Element Discretizations of the Stokes Equations","url":"https://arxiv.org/abs/2502.09159","date":1739768400,"author":"","guid":949,"unread":true,"content":"<article>arXiv:2502.09159v2 Announce Type: replace \nAbstract: We present a monolithic $hp$ space-time multigrid method for tensor-product space-time finite element discretizations of the Stokes equations. Geometric and polynomial coarsening of the space-time mesh is performed, and the entire algorithm is expressed through rigorous mathematical mappings. For the discretization, we use inf-sup stable pairs $\\mathbb Q_{r+1}/\\mathbb P_{r}^{\\text{disc}}$ of elements in space and a discontinuous Galerkin (DG$(k)$) discretization in time with piecewise polynomials of order $k$. The key novelty of this work is the application of $hp$ multigrid techniques in space and time, facilitated and accelerated by the matrix-free capabilities of the deal.II library. While multigrid methods are well-established for stationary problems, their application in space-time formulations encounter unique challenges, particularly in constructing suitable smoothers. To overcome these challenges, we employ a space-time cell-wise Vanka smoother. Extensive tests on high-performance computing platforms demonstrate the efficiency of our $hp$ multigrid approach on problem sizes exceeding a trillion degrees of freedom (dofs), sustaining throughputs of hundreds of millions of dofs per second.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Social Construction of Visualizations: Practitioner Challenges and Experiences of Visualizing Race and Gender Demographic Data","url":"https://arxiv.org/abs/2502.09048","date":1739768400,"author":"","guid":950,"unread":true,"content":"<article>arXiv:2502.09048v2 Announce Type: replace \nAbstract: Data visualizations are increasingly seen as socially constructed, with several recent studies positing that perceptions and interpretations of visualization artifacts are shaped through complex sets of interactions between members of a community. However, most of these works have focused on audiences and researchers, and little is known about if and how practitioners account for the socially constructed framing of data visualization. In this paper, we study and analyze how visualization practitioners understand the influence of their beliefs, values, and biases in their design processes and the challenges they experience. In 17 semi-structured interviews with designers working with race and gender demographic data, we find that a complex mix of factors interact to inform how practitioners approach their design process, including their personal experiences, values, and their understandings of power, neutrality, and politics. Based on our findings, we suggest a series of implications for research and practice in this space.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key to Model Reasoning","url":"https://arxiv.org/abs/2502.09022","date":1739768400,"author":"","guid":951,"unread":true,"content":"<article>arXiv:2502.09022v2 Announce Type: replace \nAbstract: Transformer-based language models have achieved significant success; however, their internal mechanisms remain largely opaque due to the complexity of non-linear interactions and high-dimensional operations. While previous studies have demonstrated that these models implicitly embed reasoning trees, humans typically employ various distinct logical reasoning mechanisms to complete the same task. It is still unclear which multi-step reasoning mechanisms are used by language models to solve such tasks. In this paper, we aim to address this question by investigating the mechanistic interpretability of language models, particularly in the context of multi-step reasoning tasks. Specifically, we employ circuit analysis and self-influence functions to evaluate the changing importance of each token throughout the reasoning process, allowing us to map the reasoning paths adopted by the model. We apply this methodology to the GPT-2 model on a prediction task (IOI) and demonstrate that the underlying circuits reveal a human-interpretable reasoning process used by the model.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural Force Field: Learning Generalized Physical Representation from a Few Examples","url":"https://arxiv.org/abs/2502.08987","date":1739768400,"author":"","guid":952,"unread":true,"content":"<article>arXiv:2502.08987v2 Announce Type: replace \nAbstract: Physical reasoning is a remarkable human ability that enables rapid learning and generalization from limited experience. Current AI models, despite extensive training, still struggle to achieve similar generalization, especially in Out-of-distribution (OOD) settings. This limitation stems from their inability to abstract core physical principles from observations. A key challenge is developing representations that can efficiently learn and generalize physical dynamics from minimal data. Here we present Neural Force Field (NFF) a modeling framework built on Neural Ordinary Differential Equation (NODE) that learns interpretable force field representations which can be efficiently integrated through an Ordinary Differential Equation ( ODE) solver to predict object trajectories. Unlike existing approaches that rely on high-dimensional latent spaces, NFF captures fundamental physical concepts such as gravity, support, and collision in an interpretable manner. Experiments on two challenging physical reasoning tasks demonstrate that NFF, trained with only a few examples, achieves strong generalization to unseen scenarios. This physics-grounded representation enables efficient forward-backward planning and rapid adaptation through interactive refinement. Our work suggests that incorporating physics-inspired representations into learning systems can help bridge the gap between artificial and human physical reasoning capabilities.</article>","contentLength":1497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage","url":"https://arxiv.org/abs/2502.08966","date":1739768400,"author":"","guid":953,"unread":true,"content":"<article>arXiv:2502.08966v2 Announce Type: replace \nAbstract: Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions. Existing defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. RTBAS adapts Information Flow Control to the unique challenges presented by TBAS. We present two novel dependency screeners, using LM-as-a-judge and attention-based saliency, to overcome these challenges. Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.</article>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond the Singular: The Essential Role of Multiple Generations in Effective Benchmark Evaluation and Analysis","url":"https://arxiv.org/abs/2502.08943","date":1739768400,"author":"","guid":954,"unread":true,"content":"<article>arXiv:2502.08943v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated significant utilities in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantic prompts, enabling error detection and quality control in benchmark construction.</article>","contentLength":1300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of Off-Policy $n$-Step TD-Learning with Linear Function Approximation","url":"https://arxiv.org/abs/2502.08941","date":1739768400,"author":"","guid":955,"unread":true,"content":"<article>arXiv:2502.08941v2 Announce Type: replace \nAbstract: This paper analyzes multi-step temporal difference (TD)-learning algorithms within the ``deadly triad'' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that $n$-step TD-learning algorithms converge to a solution as the sampling horizon $n$ increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when $n$ is sufficiently large. Based on these findings, in the second part, two $n$-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free reinforcement learning counterparts of the model-based deterministic algorithms.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Siren Song: Manipulating Pose Estimation in XR Headsets Using Acoustic Attacks","url":"https://arxiv.org/abs/2502.08865","date":1739768400,"author":"","guid":956,"unread":true,"content":"<article>arXiv:2502.08865v2 Announce Type: replace \nAbstract: Extended Reality (XR) experiences involve interactions between users, the real world, and virtual content. A key step to enable these experiences is the XR headset sensing and estimating the user's pose in order to accurately place and render virtual content in the real world. XR headsets use multiple sensors (e.g., cameras, inertial measurement unit) to perform pose estimation and improve its robustness, but this provides an attack surface for adversaries to interfere with the pose estimation process. In this paper, we create and study the effects of acoustic attacks that create false signals in the inertial measurement unit (IMU) on XR headsets, leading to adverse downstream effects on XR applications. We generate resonant acoustic signals on a HoloLens 2 and measure the resulting perturbations in the IMU readings, and also demonstrate both fine-grained and coarse attacks on the popular ORB-SLAM3 and an open-source XR system (ILLIXR). With the knowledge gleaned from attacking these open-source frameworks, we demonstrate four end-to-end proof-of-concept attacks on a HoloLens 2: manipulating user input, clickjacking, zone invasion, and denial of user interaction. Our experiments show that current commercial XR headsets are susceptible to acoustic attacks, raising concerns for their security.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges","url":"https://arxiv.org/abs/2502.08859","date":1739768400,"author":"","guid":957,"unread":true,"content":"<article>arXiv:2502.08859v2 Announce Type: replace \nAbstract: As language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities, making them a unique testbed for evaluating frontier language models. We introduce EnigmaEval, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks, puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution paths. The benchmark comprises 1184 puzzles of varying complexity -- each typically requiring teams of skilled solvers hours to days to complete -- with unambiguous, verifiable solutions that enable efficient evaluation. State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning.</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Dataset Size for Recommender Systems: Evaluating Algorithms' Performance via Downsampling","url":"https://arxiv.org/abs/2502.08845","date":1739768400,"author":"","guid":958,"unread":true,"content":"<article>arXiv:2502.08845v2 Announce Type: replace \nAbstract: This thesis investigates dataset downsampling as a strategy to optimize energy efficiency in recommender systems while maintaining competitive performance. With increasing dataset sizes posing computational and environmental challenges, this study explores the trade-offs between energy efficiency and recommendation quality in Green Recommender Systems, which aim to reduce environmental impact. By applying two downsampling approaches to seven datasets, 12 algorithms, and two levels of core pruning, the research demonstrates significant reductions in runtime and carbon emissions. For example, a 30% downsampling portion can reduce runtime by 52% compared to the full dataset, leading to a carbon emission reduction of up to 51.02 KgCO2e during the training of a single algorithm on a single dataset. The analysis reveals that algorithm performance under different downsampling portions depends on factors like dataset characteristics, algorithm complexity, and the specific downsampling configuration (scenario dependent). Some algorithms, which showed lower nDCG@10 scores compared to higher-performing ones, exhibited lower sensitivity to the amount of training data, offering greater potential for efficiency in lower downsampling portions. On average, these algorithms retained 81% of full-size performance using only 50% of the training set. In certain downsampling configurations, where more users were progressively included while keeping the test set size fixed, they even showed higher nDCG@10 scores than when using the full dataset. These findings highlight the feasibility of balancing sustainability and effectiveness, providing insights for designing energy-efficient recommender systems and promoting sustainable AI practices.</article>","contentLength":1799,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Acoustic Wave Manipulation Through Sparse Robotic Actuation","url":"https://arxiv.org/abs/2502.08784","date":1739768400,"author":"","guid":959,"unread":true,"content":"<article>arXiv:2502.08784v2 Announce Type: replace \nAbstract: Recent advancements in robotics, control, and machine learning have facilitated progress in the challenging area of object manipulation. These advancements include, among others, the use of deep neural networks to represent dynamics that are partially observed by robot sensors, as well as effective control using sparse control signals. In this work, we explore a more general problem: the manipulation of acoustic waves, which are partially observed by a robot capable of influencing the waves through spatially sparse actuators. This problem holds great potential for the design of new artificial materials, ultrasonic cutting tools, energy harvesting, and other applications. We develop an efficient data-driven method for robot learning that is applicable to either focusing scattered acoustic energy in a designated region or suppressing it, depending on the desired task. The proposed method is better in terms of a solution quality and computational complexity as compared to a state-of-the-art learning based method for manipulation of dynamical systems governed by partial differential equations. Furthermore our proposed method is competitive with a classical semi-analytical method in acoustics research on the demonstrated tasks. We have made the project code publicly available, along with a web page featuring video demonstrations: https://gladisor.github.io/waves/.</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deployment-friendly Lane-changing Intention Prediction Powered by Brain-inspired Spiking Neural Networks","url":"https://arxiv.org/abs/2502.08659","date":1739768400,"author":"","guid":960,"unread":true,"content":"<article>arXiv:2502.08659v2 Announce Type: replace \nAbstract: Accurate and real-time prediction of surrounding vehicles' lane-changing intentions is a critical challenge in deploying safe and efficient autonomous driving systems in open-world scenarios. Existing high-performing methods remain hard to deploy due to their high computational cost, long training times, and excessive memory requirements. Here, we propose an efficient lane-changing intention prediction approach based on brain-inspired Spiking Neural Networks (SNN). By leveraging the event-driven nature of SNN, the proposed approach enables us to encode the vehicle's states in a more efficient manner. Comparison experiments conducted on HighD and NGSIM datasets demonstrate that our method significantly improves training efficiency and reduces deployment costs while maintaining comparable prediction accuracy. Particularly, compared to the baseline, our approach reduces training time by 75% and memory usage by 99.9%. These results validate the efficiency and reliability of our method in lane-changing predictions, highlighting its potential for safe and efficient autonomous driving systems while offering significant advantages in deployment, including reduced training time, lower memory usage, and faster inference.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and learning in neural networks","url":"https://arxiv.org/abs/2502.08644","date":1739768400,"author":"","guid":961,"unread":true,"content":"<article>arXiv:2502.08644v3 Announce Type: replace \nAbstract: The brain can rapidly adapt to new contexts and learn from limited data, a coveted characteristic that artificial intelligence algorithms have struggled to mimic. Inspired by oscillatory rhythms of the mechanical structures of neural cells, we developed a learning paradigm that is based on oscillations in link strengths and associates learning with the coordination of these oscillations. We find that this paradigm yields rapid adaptation and learning in artificial neural networks. Link oscillations can rapidly change coordination, endowing the network with the ability to sense subtle context changes in an unsupervised manner. In other words, the network generates the missing contextual tokens required to perform as a generalist AI architecture capable of predicting dynamics in multiple contexts. Oscillations also allow the network to extrapolate dynamics to never-seen-before contexts. These capabilities make our learning paradigm a powerful starting point for novel models of learning and cognition. Furthermore, learning through link coordination is agnostic to the specifics of the neural network architecture, hence our study opens the door for introducing rapid adaptation and learning capabilities into leading AI models.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LucidAtlas$: Learning Uncertainty-Aware, Covariate-Disentangled, Individualized Atlas Representations","url":"https://arxiv.org/abs/2502.08445","date":1739768400,"author":"","guid":962,"unread":true,"content":"<article>arXiv:2502.08445v2 Announce Type: replace \nAbstract: The goal of this work is to develop principled techniques to extract information from high dimensional data sets with complex dependencies in areas such as medicine that can provide insight into individual as well as population level variation. We develop $\\texttt{LucidAtlas}$, an approach that can represent spatially varying information, and can capture the influence of covariates as well as population uncertainty. As a versatile atlas representation, $\\texttt{LucidAtlas}$ offers robust capabilities for covariate interpretation, individualized prediction, population trend analysis, and uncertainty estimation, with the flexibility to incorporate prior knowledge. Additionally, we discuss the trustworthiness and potential risks of neural additive models for analyzing dependent covariates and then introduce a marginalization approach to explain the dependence of an individual predictor on the models' response (the atlas). To validate our method, we demonstrate its generalizability on two medical datasets. Our findings underscore the critical role of by-construction interpretable models in advancing scientific discovery. Our code will be publicly available upon acceptance.</article>","contentLength":1240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","url":"https://arxiv.org/abs/2502.08346","date":1739768400,"author":"","guid":963,"unread":true,"content":"<article>arXiv:2502.08346v2 Announce Type: replace \nAbstract: Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.</article>","contentLength":1129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion Rate Modeling","url":"https://arxiv.org/abs/2502.08277","date":1739768400,"author":"","guid":964,"unread":true,"content":"<article>arXiv:2502.08277v2 Announce Type: replace \nAbstract: Post-click conversion rate (CVR) estimation is a vital task in many recommender systems of revenue businesses, e.g., e-commerce and advertising. In a perspective of sample, a typical CVR positive sample usually goes through a funnel of exposure to click to conversion. For lack of post-event labels for un-clicked samples, CVR learning task commonly only utilizes clicked samples, rather than all exposed samples as for click-through rate (CTR) learning task. However, during online inference, CVR and CTR are estimated on the same assumed exposure space, which leads to a inconsistency of sample space between training and inference, i.e., sample selection bias (SSB). To alleviate SSB, previous wisdom proposes to design novel auxiliary tasks to enable the CVR learning on un-click training samples, such as CTCVR and counterfactual CVR, etc. Although alleviating SSB to some extent, none of them pay attention to the discrimination between ambiguous negative samples (un-clicked) and factual negative samples (clicked but un-converted) during modelling, which makes CVR model lacks robustness. To full this gap, we propose a novel ChorusCVR model to realize debiased CVR learning in entire-space.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TRISHUL: Towards Region Identification and Screen Hierarchy Understanding for Large VLM based GUI Agents","url":"https://arxiv.org/abs/2502.08226","date":1739768400,"author":"","guid":965,"unread":true,"content":"<article>arXiv:2502.08226v2 Announce Type: replace \nAbstract: Recent advancements in Large Vision Language Models (LVLMs) have enabled the development of LVLM-based Graphical User Interface (GUI) agents under various paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle with cross-dataset and cross-platform generalization due to their reliance on dataset-specific training. Generalist LVLMs, such as GPT-4V, employ Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires metadata like HTML source, which is not consistently available across platforms. Moreover, existing methods often specialize in singular GUI tasks rather than achieving comprehensive GUI understanding. To address these limitations, we introduce TRISHUL, a novel, training-free agentic framework that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior works that focus on either action grounding (mapping instructions to GUI elements) or GUI referring (describing GUI elements given a location), TRISHUL seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module, which work synergistically to provide multi-granular, spatially, and semantically enriched representations of GUI elements. Our results demonstrate TRISHUL's superior performance in action grounding across the ScreenSpot, VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring, TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new standard for robust and adaptable GUI comprehension.</article>","contentLength":1608,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In-Context Learning of Linear Dynamical Systems with Transformers: Error Bounds and Depth-Separation","url":"https://arxiv.org/abs/2502.08136","date":1739768400,"author":"","guid":966,"unread":true,"content":"<article>arXiv:2502.08136v2 Announce Type: replace \nAbstract: This paper investigates approximation-theoretic aspects of the in-context learning capability of the transformers in representing a family of noisy linear dynamical systems. Our first theoretical result establishes an upper bound on the approximation error of multi-layer transformers with respect to an $L^2$-testing loss uniformly defined across tasks. This result demonstrates that transformers with logarithmic depth can achieve error bounds comparable with those of the least-squares estimator. In contrast, our second result establishes a non-diminishing lower bound on the approximation error for a class of single-layer linear transformers, which suggests a depth-separation phenomenon for transformers in the in-context learning of dynamical systems. Moreover, this second result uncovers a critical distinction in the approximation power of single-layer linear transformers when learning from IID versus non-IID data.</article>","contentLength":980,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping","url":"https://arxiv.org/abs/2502.08054","date":1739768400,"author":"","guid":967,"unread":true,"content":"<article>arXiv:2502.08054v2 Announce Type: replace \nAbstract: This paper addresses the challenge of occluded robot grasping, i.e. grasping in situations where the desired grasp poses are kinematically infeasible due to environmental constraints such as surface collisions. Traditional robot manipulation approaches struggle with the complexity of non-prehensile or bimanual strategies commonly used by humans in these circumstances. State-of-the-art reinforcement learning (RL) methods are unsuitable due to the inherent complexity of the task. In contrast, learning from demonstration requires collecting a significant number of expert demonstrations, which is often infeasible. Instead, inspired by human bimanual manipulation strategies, where two hands coordinate to stabilise and reorient objects, we focus on a bimanual robotic setup to tackle this challenge. In particular, we introduce Constraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), a learning-based approach which leverages two coordinated policies: a constraint policy trained using self-supervised datasets to generate stabilising poses and a grasping policy trained using RL that reorients and grasps the target object. A key contribution lies in value function-guided policy coordination. Specifically, during RL training for the grasping policy, the constraint policy's output is refined through gradients from a jointly trained value function, improving bimanual coordination and task performance. Lastly, COMBO-Grasp employs teacher-student policy distillation to effectively deploy point cloud-based policies in real-world environments. Empirical evaluations demonstrate that COMBO-Grasp significantly improves task success rates compared to competitive baseline approaches, with successful generalisation to unseen objects in both simulated and real-world environments.</article>","contentLength":1857,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Interactive Framework for Implementing Privacy-Preserving Federated Learning: Experiments on Large Language Models","url":"https://arxiv.org/abs/2502.08008","date":1739768400,"author":"","guid":968,"unread":true,"content":"<article>arXiv:2502.08008v2 Announce Type: replace \nAbstract: Federated learning (FL) enhances privacy by keeping user data on local devices. However, emerging attacks have demonstrated that the updates shared by users during training can reveal significant information about their data. This has greatly thwart the adoption of FL methods for training robust AI models in sensitive applications. Differential Privacy (DP) is considered the gold standard for safeguarding user data. However, DP guarantees are highly conservative, providing worst-case privacy guarantees. This can result in overestimating privacy needs, which may compromise the model's accuracy. Additionally, interpretations of these privacy guarantees have proven to be challenging in different contexts. This is further exacerbated when other factors, such as the number of training iterations, data distribution, and specific application requirements, can add further complexity to this problem. In this work, we proposed a framework that integrates a human entity as a privacy practitioner to determine an optimal trade-off between the model's privacy and utility. Our framework is the first to address the variable memory requirement of existing DP methods in FL settings, where resource-limited devices (e.g., cell phones) can participate. To support such settings, we adopt a recent DP method with fixed memory usage to ensure scalable private FL. We evaluated our proposed framework by fine-tuning a BERT-based LLM model using the GLUE dataset (a common approach in literature), leveraging the new accountant, and employing diverse data partitioning strategies to mimic real-world conditions. As a result, we achieved stable memory usage, with an average accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon = 6$, when compared to the state-of-the-art DP accountant which does not support fixed memory usage.</article>","contentLength":1893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New tools for comparing classical and neural ODE models for tumor growth","url":"https://arxiv.org/abs/2502.07964","date":1739768400,"author":"","guid":969,"unread":true,"content":"<article>arXiv:2502.07964v2 Announce Type: replace \nAbstract: A new computational tool TumorGrowth$.$jl for modeling tumor growth is introduced. The tool allows the comparison of standard textbook models, such as General Bertalanffy and Gompertz, with some newer models, including, for the first time, neural ODE models. As an application, we revisit a human meta-study of non-small cell lung cancer and bladder cancer lesions, in patients undergoing two different treatment options, to determine if previously reported performance differences are statistically significant, and if newer, more complex models perform any better. In a population of examples with at least four time-volume measurements available for calibration, and an average of about 6.3, our main conclusion is that the General Bertalanffy model has superior performance, on average. However, where more measurements are available, we argue that more complex models, capable of capturing rebound and relapse behavior, may be better choices.</article>","contentLength":1000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implicit Language Models are RNNs: Balancing Parallelization and Expressivity","url":"https://arxiv.org/abs/2502.07827","date":1739768400,"author":"","guid":970,"unread":true,"content":"<article>arXiv:2502.07827v2 Announce Type: replace \nAbstract: State-space models (SSMs) and transformers dominate the language modeling landscape. However, they are constrained to a lower computational complexity than classical recurrent neural networks (RNNs), limiting their expressivity. In contrast, RNNs lack parallelization during training, raising fundamental questions about the trade off between parallelization and expressivity. We propose implicit SSMs, which iterate a transformation until convergence to a fixed point. Theoretically, we show that implicit SSMs implement the non-linear state-transitions of RNNs. Empirically, we find that only approximate fixed-point convergence suffices, enabling the design of a scalable training curriculum that largely retains parallelization, with full convergence required only for a small subset of tokens. Our approach demonstrates superior state-tracking capabilities on regular languages, surpassing transformers and SSMs. We further scale implicit SSMs to natural language reasoning tasks and pretraining of large-scale language models up to 1.3B parameters on 207B tokens - representing, to our knowledge, the largest implicit model trained to date. Notably, our implicit models outperform their explicit counterparts on standard benchmarks.</article>","contentLength":1291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Simulation-Based Framework for Leveraging Shared Autonomous Vehicles to Enhance Disaster Evacuations in Rural Regions with a Focus on Vulnerable Populations","url":"https://arxiv.org/abs/2502.07787","date":1739768400,"author":"","guid":971,"unread":true,"content":"<article>arXiv:2502.07787v2 Announce Type: replace \nAbstract: Rapid advancements in autonomous vehicles (AVs) are poised to revolutionize transportation and communities, including disaster evacuations, particularly through the deployment of Shared Autonomous Vehicles (SAVs). Despite the potential, the use of SAVs in rural disaster evacuations remains an underexplored area. To address this gap, this study proposes a simulation-based framework that integrates both mathematical programming and SUMO traffic simulation to deploy SAVs in pre- and post-disaster evacuations in rural areas. The framework prioritizes the needs of vulnerable groups, including individuals with disabilities, limited English proficiency, and elderly residents. Sumter County, Florida, serves as the case study due to its unique characteristics: a high concentration of vulnerable individuals and limited access to public transportation, making it one of the most transportation-insecure counties in the state. These conditions present significant challenges for evacuation planning in the region. To explore potential solutions, we conducted mass evacuation simulations by incorporating SAVs across seven scenarios. These scenarios represented varying SAV penetration levels, ranging from 20% to 100% of the vulnerable population, and were compared to a baseline scenario using only passenger cars. Additionally, we examined both pre-disaster and post-disaster conditions, accounting for infrastructure failures and road closures. According to the simulation results, higher SAV integration significantly improves traffic distribution and reduces congestion. Scenarios featuring more SAVs exhibited lower congestion peaks and more stable traffic flow. Conversely, mixed traffic environments demonstrate reduced average speeds attributable to interactions between SAVs and passenger cars, while exclusive use of SAVs results in higher speeds and more stable travel patterns.</article>","contentLength":1943,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EIQP: Execution-time-certified and Infeasibility-detecting QP Solver","url":"https://arxiv.org/abs/2502.07738","date":1739768400,"author":"","guid":972,"unread":true,"content":"<article>arXiv:2502.07738v2 Announce Type: replace \nAbstract: Solving real-time quadratic programming (QP) is a ubiquitous task in control engineering, such as in model predictive control and control barrier function-based QP. In such real-time scenarios, certifying that the employed QP algorithm can either return a solution within a predefined level of optimality or detect QP infeasibility before the predefined sampling time is a pressing requirement. This article considers convex QP (including linear programming) and adopts its homogeneous formulation to achieve infeasibility detection. Exploiting this homogeneous formulation, this article proposes a novel infeasible interior-point method (IPM) algorithm with the best theoretical $O(\\sqrt{n})$ iteration complexity that feasible IPM algorithms enjoy. The iteration complexity is proved to be \\textit{exact} (rather than an upper bound), \\textit{simple to calculate}, and \\textit{data independent}, with the value $\\left\\lceil\\frac{\\log(\\frac{n+1}{\\epsilon})}{-\\log(1-\\frac{0.414213}{\\sqrt{n+1}})}\\right\\rceil$ (where $n$ and $\\epsilon$ denote the number of constraints and the predefined optimality level, respectively), making it appealing to certify the execution time of online time-varying convex QPs. The proposed algorithm is simple to implement without requiring a line search procedure (uses the full Newton step), and its C-code implementation (offering MATLAB, Julia, and Python interfaces) and numerical examples are publicly available at https://github.com/liangwu2019/EIQP.</article>","contentLength":1539,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Magic 1-For-1: Generating One Minute Video Clips within One Minute","url":"https://arxiv.org/abs/2502.07701","date":1739768400,"author":"","guid":973,"unread":true,"content":"<article>arXiv:2502.07701v2 Announce Type: replace \nAbstract: In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.</article>","contentLength":1569,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving","url":"https://arxiv.org/abs/2502.07640","date":1739768400,"author":"","guid":974,"unread":true,"content":"<article>arXiv:2502.07640v2 Announce Type: replace \nAbstract: We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. Despite using only supervised fine-tuning, our final prover significantly outperforms the previous best open-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning. On the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32), surpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.</article>","contentLength":1454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bag of Tricks for Inference-time Computation of LLM Reasoning","url":"https://arxiv.org/abs/2502.07191","date":1739768400,"author":"","guid":975,"unread":true,"content":"<article>arXiv:2502.07191v3 Announce Type: replace \nAbstract: With the advancement of large language models (LLMs), solving complex reasoning tasks has gained increasing attention. Inference-time computation methods (e.g., Best-of-N, beam search, et al.) are particularly valuable as they can enhance reasoning performance without modifying model parameters or requiring additional training. However, these techniques come with implementation challenges, and most existing methods remain at the proof-of-concept stage with limited practical adoption due to their computational complexity and varying effectiveness across different tasks. In this paper, we investigate and benchmark diverse inference-time computation strategies across reasoning tasks of varying complexity. Since most current methods rely on a proposer-verifier pipeline that first generates candidate solutions (e.g., reasoning solutions) and then selects the best one based on reward signals (e.g., RLHF rewards, process rewards), our research focuses on optimizing both candidate solution generation (e.g., instructing prompts, hyperparameters such as temperature and top-p) and reward mechanisms (e.g., self-evaluation, reward types). Through extensive experiments (more than 20,000 A100-80G GPU hours with over 1,000 experiments) across a variety of models (e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation studies reveal that previously overlooked strategies can significantly enhance performance (e.g., tuning temperature can improve reasoning task performance by up to 5%). Furthermore, we establish a standardized benchmark for inference-time computation by systematically evaluating six representative methods across eight reasoning tasks. These findings provide a stronger foundation for future research. The code is available at https://github.com/usail-hkust/benchmark_inference_time_computation_LL</article>","contentLength":1892,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero-Knowledge Proof Frameworks: A Systematic Survey","url":"https://arxiv.org/abs/2502.07063","date":1739768400,"author":"","guid":976,"unread":true,"content":"<article>arXiv:2502.07063v2 Announce Type: replace \nAbstract: Zero-Knowledge Proofs (ZKPs) are a cryptographic primitive that allows a prover to demonstrate knowledge of a secret value to a verifier without revealing anything about the secret itself. ZKPs have shown to be an extremely powerful tool, as evidenced in both industry and academic settings. In recent years, the utilization of user data in practical applications has necessitated the rapid development of privacy-preserving techniques, including ZKPs. This has led to the creation of several robust open-source ZKP frameworks. However, there remains a significant gap in understanding the capabilities and real-world applications of these frameworks. Furthermore, identifying the most suitable frameworks for the developers' specific applications and settings is a challenge, given the variety of options available. The primary goal of our work is to lower the barrier to entry for understanding and building applications with open-source ZKP frameworks.\n  In this work, we survey and evaluate 25 general-purpose, prominent ZKP frameworks. Recognizing that ZKPs have various constructions and underlying arithmetic schemes, our survey aims to provide a comprehensive overview of the ZKP landscape. These systems are assessed based on their usability and performance in SHA-256 and matrix multiplication experiments. Acknowledging that setting up a functional development environment can be challenging for these frameworks, we offer a fully open-source collection of Docker containers. These containers include a working development environment and are accompanied by documented code from our experiments. We conclude our work with a thorough analysis of the practical applications of ZKPs, recommendations for ZKP settings in different application scenarios, and a discussion on the future development of ZKP frameworks.</article>","contentLength":1875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AutoSketch: VLM-assisted Style-Aware Vector Sketch Completion","url":"https://arxiv.org/abs/2502.06860","date":1739768400,"author":"","guid":977,"unread":true,"content":"<article>arXiv:2502.06860v2 Announce Type: replace \nAbstract: The ability to automatically complete a partial sketch that depicts a complex scene, e.g., \"a woman chatting with a man in the park\", is very useful. However, existing sketch generation methods create sketches from scratch; they do not complete a partial sketch in the style of the original. To address this challenge, we introduce AutoSketch, a styleaware vector sketch completion method that accommodates diverse sketch styles. Our key observation is that the style descriptions of a sketch in natural language preserve the style during automatic sketch completion. Thus, we use a pretrained vision-language model (VLM) to describe the styles of the partial sketches in natural language and replicate these styles using newly generated strokes. We initially optimize the strokes to match an input prompt augmented by style descriptions extracted from the VLM. Such descriptions allow the method to establish a diffusion prior in close alignment with that of the partial sketch. Next, we utilize the VLM to generate an executable style adjustment code that adjusts the strokes to conform to the desired style. We compare our method with existing methods across various sketch styles and prompts, performed extensive ablation studies and qualitative and quantitative evaluations, and demonstrate that AutoSketch can support various sketch scenarios.</article>","contentLength":1402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The 2021 Tokyo Olympics Multilingual News Article Dataset","url":"https://arxiv.org/abs/2502.06648","date":1739768400,"author":"","guid":978,"unread":true,"content":"<article>arXiv:2502.06648v2 Announce Type: replace \nAbstract: In this paper, we introduce a dataset of multilingual news articles covering the 2021 Tokyo Olympics. A total of 10,940 news articles were gathered from 1,918 different publishers, covering 1,350 sub-events of the 2021 Olympics, and published between July 1, 2021, and August 14, 2021. These articles are written in nine languages from different language families and in different scripts. To create the dataset, the raw news articles were first retrieved via a service that collects and analyzes news articles. Then, the articles were grouped using an online clustering algorithm, with each group containing articles reporting on the same sub-event. Finally, the groups were manually annotated and evaluated. The development of this dataset aims to provide a resource for evaluating the performance of multilingual news clustering algorithms, for which limited datasets are available. It can also be used to analyze the dynamics and events of the 2021 Tokyo Olympics from different perspectives. The dataset is available in CSV format and can be accessed from the CLARIN.SI repository.</article>","contentLength":1139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions","url":"https://arxiv.org/abs/2502.06309","date":1739768400,"author":"","guid":979,"unread":true,"content":"<article>arXiv:2502.06309v2 Announce Type: replace \nAbstract: As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. Among all the physical properties of resistive elements, the response to the pulses directly affects the training dynamics. This paper first provides a theoretical foundation for gradient-based training on AIMC hardware and studies the impact of response functions. We demonstrate that noisy update and asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty term on the objective. To overcome the issue, Tiki-Taka, a residual learning algorithm, converges exactly to a critical point by optimizing a main array and a residual array bilevelly. The conclusion is supported by simulations validating our theoretical insights.</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Occupancy-SLAM: An Efficient and Robust Algorithm for Simultaneously Optimizing Robot Poses and Occupancy Map","url":"https://arxiv.org/abs/2502.06292","date":1739768400,"author":"","guid":980,"unread":true,"content":"<article>arXiv:2502.06292v2 Announce Type: replace \nAbstract: Joint optimization of poses and features has been extensively studied and demonstrated to yield more accurate results in feature-based SLAM problems. However, research on jointly optimizing poses and non-feature-based maps remains limited. Occupancy maps are widely used non-feature-based environment representations because they effectively classify spaces into obstacles, free areas, and unknown regions, providing robots with spatial information for various tasks. In this paper, we propose Occupancy-SLAM, a novel optimization-based SLAM method that enables the joint optimization of robot trajectory and the occupancy map through a parameterized map representation. The key novelty lies in optimizing both robot poses and occupancy values at different cell vertices simultaneously, a significant departure from existing methods where the robot poses need to be optimized first before the map can be estimated. Evaluations using simulations and practical 2D laser datasets demonstrate that the proposed approach can robustly obtain more accurate robot trajectories and occupancy maps than state-of-the-art techniques with comparable computational time. Preliminary results in the 3D case further confirm the potential of the proposed method in practical 3D applications, achieving more accurate results than existing methods.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Low Tensor-Rank Adaptation of Kolmogorov--Arnold Networks","url":"https://arxiv.org/abs/2502.06153","date":1739768400,"author":"","guid":981,"unread":true,"content":"<article>arXiv:2502.06153v2 Announce Type: replace \nAbstract: Kolmogorov--Arnold networks (KANs) have demonstrated their potential as an alternative to multi-layer perceptions (MLPs) in various domains, especially for science-related tasks. However, transfer learning of KANs remains a relatively unexplored area. In this paper, inspired by Tucker decomposition of tensors and evidence on the low tensor-rank structure in KAN parameter updates, we develop low tensor-rank adaptation (LoTRA) for fine-tuning KANs. We study the expressiveness of LoTRA based on Tucker decomposition approximations. Furthermore, we provide a theoretical analysis to select the learning rates for each LoTRA component to enable efficient training. Our analysis also shows that using identical learning rates across all components leads to inefficient training, highlighting the need for an adaptive learning rate strategy. Beyond theoretical insights, we explore the application of LoTRA for efficiently solving various partial differential equations (PDEs) by fine-tuning KANs. Additionally, we propose Slim KANs that incorporate the inherent low-tensor-rank properties of KAN parameter tensors to reduce model size while maintaining superior performance. Experimental results validate the efficacy of the proposed learning rate selection strategy and demonstrate the effectiveness of LoTRA for transfer learning of KANs in solving PDEs. Further evaluations on Slim KANs for function representation and image classification tasks highlight the expressiveness of LoTRA and the potential for parameter reduction through low tensor-rank decomposition.</article>","contentLength":1619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sink-free orientations: a local sampler with applications","url":"https://arxiv.org/abs/2502.05877","date":1739768400,"author":"","guid":982,"unread":true,"content":"<article>arXiv:2502.05877v2 Announce Type: replace \nAbstract: For sink-free orientations in graphs of minimum degree at least $3$, we show that there is a deterministic approximate counting algorithm that runs in time $O((n^{73}/\\varepsilon^{72})\\log(n/\\varepsilon))$, a near-linear time sampling algorithm, and a randomised approximate counting algorithm that runs in time $O((n/\\varepsilon)^2\\log(n/\\varepsilon))$, where $n$ denotes the number of vertices of the input graph and $0&lt;\\varepsilon&lt;1$ is the desired accuracy. All three algorithms are based on a local implementation of the sink popping method (Cohn, Pemantle, and Propp, 2002) under the partial rejection sampling framework (Guo, Jerrum, and Liu, 2019).</article>","contentLength":709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Federated Learning with Reservoir State Analysis for Time Series Anomaly Detection","url":"https://arxiv.org/abs/2502.05679","date":1739768400,"author":"","guid":983,"unread":true,"content":"<article>arXiv:2502.05679v2 Announce Type: replace \nAbstract: With a growing data privacy concern, federated learning has emerged as a promising framework to train machine learning models without sharing locally distributed data. In federated learning, local model training by multiple clients and model integration by a server are repeated only through model parameter sharing. Most existing federated learning methods assume training deep learning models, which are often computationally demanding. To deal with this issue, we propose federated learning methods with reservoir state analysis to seek computational efficiency and data privacy protection simultaneously. Specifically, our method relies on Mahalanobis Distance of Reservoir States (MD-RS) method targeting time series anomaly detection, which learns a distribution of reservoir states for normal inputs and detects anomalies based on a deviation from the learned distribution. Iterative updating of statistical parameters in the MD-RS enables incremental federated learning (IncFed MD-RS). We evaluate the performance of IncFed MD-RS using benchmark datasets for time series anomaly detection. The results show that IncFed MD-RS outperforms other federated learning methods with deep learning and reservoir computing models particularly when clients' data are relatively short and heterogeneous. We demonstrate that IncFed MD-RS is robust against reduced sample data compared to other methods. We also show that the computational cost of IncFed MD-RS can be reduced by subsampling from the reservoir states without performance degradation. The proposed method is beneficial especially in anomaly detection applications where computational efficiency, algorithm simplicity, and low communication cost are required.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreeBlend: Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion","url":"https://arxiv.org/abs/2502.05606","date":1739768400,"author":"","guid":984,"unread":true,"content":"<article>arXiv:2502.05606v2 Announce Type: replace \nAbstract: Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two-Player Zero-Sum Differential Games with One-Sided Information","url":"https://arxiv.org/abs/2502.05314","date":1739768400,"author":"","guid":985,"unread":true,"content":"<article>arXiv:2502.05314v2 Announce Type: replace \nAbstract: Unlike Poker where the action space $\\mathcal{A}$ is discrete, differential games in the physical world often have continuous action spaces not amenable to discrete abstraction, rendering no-regret algorithms with $\\mathcal{O}(|\\mathcal{A}|)$ complexity not scalable. To address this challenge within the scope of two-player zero-sum (2p0s) games with one-sided information, we show that (1) a computational complexity independent of $|\\mathcal{A}|$ can be achieved by exploiting the convexification property of incomplete-information games and the Isaacs' condition that commonly holds for dynamical systems, and that (2) the computation of the two equilibrium strategies can be decoupled under one-sidedness of information. Leveraging these insights, we develop an algorithm that successfully approximates the optimal strategy in a homing game. Code available in https://github.com/ghimiremukesh/cams/tree/workshop</article>","contentLength":969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated Learning","url":"https://arxiv.org/abs/2502.04890","date":1739768400,"author":"","guid":986,"unread":true,"content":"<article>arXiv:2502.04890v2 Announce Type: replace \nAbstract: Federated Learning (FL) is notorious for its vulnerability to Byzantine attacks. Most current Byzantine defenses share a common inductive bias: among all the gradients, the densely distributed ones are more likely to be honest. However, such a bias is a poison to Byzantine robustness due to a newly discovered phenomenon in this paper - gradient skew. We discover that a group of densely distributed honest gradients skew away from the optimal gradient (the average of honest gradients) due to heterogeneous data. This gradient skew phenomenon allows Byzantine gradients to hide within the densely distributed skewed gradients. As a result, Byzantine defenses are confused into believing that Byzantine gradients are honest. Motivated by this observation, we propose a novel skew-aware attack called STRIKE: first, we search for the skewed gradients; then, we construct Byzantine gradients within the skewed gradients. Experiments on three benchmark datasets validate the effectiveness of our attack</article>","contentLength":1053,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tight Bounds for Noisy Computation of High-Influence Functions, Connectivity, and Threshold","url":"https://arxiv.org/abs/2502.04632","date":1739768400,"author":"","guid":987,"unread":true,"content":"<article>arXiv:2502.04632v2 Announce Type: replace \nAbstract: In the noisy query model, the (binary) return value of every query (possibly repeated) is independently flipped with some fixed probability $p \\in (0, 1/2)$. In this paper, we obtain tight bounds on the noisy query complexity of several fundamental problems.\n  Our first contribution is to show that any Boolean function with total influence $\\Omega(n)$ has noisy query complexity $\\Theta(n\\log n)$. Previous works often focus on specific problems, and it is of great interest to have a characterization of noisy query complexity for general functions. Our result is the first noisy query complexity lower bound of this generality, beyond what was known for random Boolean functions [Reischuk and Schmeltz, FOCS 1991].\n  Our second contribution is to prove that Graph Connectivity has noisy query complexity $\\Theta(n^2 \\log n)$. In this problem, the goal is to determine whether an undirected graph is connected using noisy edge queries. While the upper bound can be achieved by a simple algorithm, no non-trivial lower bounds were known prior to this work.\n  Last but not least, we determine the exact number of noisy queries (up to lower order terms) needed to solve the $k$-Threshold problem and the Counting problem. The $k$-Threshold problem asks to decide whether there are at least $k$ ones among $n$ bits, given noisy query access to the bits. We prove that $(1\\pm o(1)) \\frac{n\\log (\\min\\{k,n-k+1\\}/\\delta)}{(1-2p)\\log \\frac{1-p}p}$ queries are both sufficient and necessary to achieve error probability $\\delta = o(1)$. Previously, such a result was only known when $\\min\\{k,n-k+1\\}=o(n)$ [Wang, Ghaddar, Zhu and Wang, arXiv 2024]. We also show a similar $(1\\pm o(1)) \\frac{n\\log (\\min\\{k+1,n-k+1\\}/\\delta)}{(1-2p)\\log \\frac{1-p}p}$ bound for the Counting problem, where one needs to count the number of ones among $n$ bits given noisy query access and $k$ denotes the answer.</article>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt-based Depth Pruning of Large Language Models","url":"https://arxiv.org/abs/2502.04348","date":1739768400,"author":"","guid":988,"unread":true,"content":"<article>arXiv:2502.04348v2 Announce Type: replace \nAbstract: Depth pruning aims to reduce the inference cost of a large language model without any hardware-specific complications, by simply removing several less important transformer blocks. However, our empirical findings suggest that the importance of a transformer block may be highly task-dependent -- a block that is crucial for a task can be removed without degrading the accuracy on another task. Based on this observation, we develop a dynamic depth pruning algorithm, coined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which blocks to omit from the model based on the input prompt. PuDDing operates by training a lightweight router to predict the best omission set among a set of options, where this option set has also been constructed in a data-driven manner. Empirical results on commonsense reasoning benchmarks demonstrate that PuDDing effectively accelerates the inference language models, and achieves better on-task performance than static depth pruning baselines.</article>","contentLength":1044,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs","url":"https://arxiv.org/abs/2502.03824","date":1739768400,"author":"","guid":989,"unread":true,"content":"<article>arXiv:2502.03824v3 Announce Type: replace \nAbstract: LLMs have boosted progress in many AI applications. Recently, there were attempts to distill the vast knowledge of LLMs into information retrieval systems. Those distillation methods mostly use output probabilities of LLMs which are unavailable in the latest black-box LLMs. We propose Syntriever, a training framework for retrievers using synthetic data from black-box LLMs. Syntriever consists of two stages. Firstly in the distillation stage, we synthesize relevant and plausibly irrelevant passages and augmented queries using chain-of-thoughts for the given queries. LLM is asked to self-verify the synthetic data for possible hallucinations, after which retrievers are trained with a loss designed to cluster the embeddings of relevant passages. Secondly in the alignment stage, we align the retriever with the preferences of LLMs. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences with regularization which prevents the model from deviating excessively from that trained in the distillation stage. Experiments show that Syntriever achieves state-of-the-art performances on benchmark datasets from various domains in nDCG@$K$. The code is available at \\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.</article>","contentLength":1340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations","url":"https://arxiv.org/abs/2502.03752","date":1739768400,"author":"","guid":990,"unread":true,"content":"<article>arXiv:2502.03752v2 Announce Type: replace \nAbstract: Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen tasks but faces challenges in long-horizon environments. Skill-based approaches tackle this by decomposing state-action sequences into reusable skills and employing hierarchical decision-making. However, these methods are highly susceptible to noisy offline demonstrations, resulting in unstable skill learning and degraded performance. To overcome this, we propose Prioritized Refinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates exploration near noisy data to generate online trajectories and combines them with offline data. Through prioritization, PRISM extracts high-quality data to learn task-relevant skills effectively. By addressing the impact of noise, our method ensures stable skill learning and achieves superior performance in long-horizon tasks, even with noisy and sub-optimal data.</article>","contentLength":954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons","url":"https://arxiv.org/abs/2502.03391","date":1739768400,"author":"","guid":991,"unread":true,"content":"<article>arXiv:2502.03391v2 Announce Type: replace \nAbstract: *Minimal sufficient reasons* represent a prevalent form of explanation - the smallest subset of input features which, when held constant at their corresponding values, ensure that the prediction remains unchanged. Previous *post-hoc* methods attempt to obtain such explanations but face two main limitations: (1) Obtaining these subsets poses a computational challenge, leading most scalable methods to converge towards suboptimal, less meaningful subsets; (2) These methods heavily rely on sampling out-of-distribution input assignments, potentially resulting in counterintuitive behaviors. To tackle these limitations, we propose in this work a self-supervised training approach, which we term *sufficient subset training* (SST). Using SST, we train models to generate concise sufficient reasons for their predictions as an integral part of their output. Our results indicate that our framework produces succinct and faithful subsets substantially more efficiently than competing post-hoc methods, while maintaining comparable predictive performance.</article>","contentLength":1105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symmetry-Aware Bayesian Flow Networks for Crystal Generation","url":"https://arxiv.org/abs/2502.03146","date":1739768400,"author":"","guid":992,"unread":true,"content":"<article>arXiv:2502.03146v2 Announce Type: replace \nAbstract: The discovery of new crystalline materials is essential to scientific and technological progress. However, traditional trial-and-error approaches are inefficient due to the vast search space. Recent advancements in machine learning have enabled generative models to predict new stable materials by incorporating structural symmetries and to condition the generation on desired properties. In this work, we introduce SymmBFN, a novel symmetry-aware Bayesian Flow Network (BFN) for crystalline material generation that accurately reproduces the distribution of space groups found in experimentally observed crystals. SymmBFN substantially improves efficiency, generating stable structures at least 50 times faster than the next-best method. Furthermore, we demonstrate its capability for property-conditioned generation, enabling the design of materials with tailored properties. Our findings establish BFNs as an effective tool for accelerating the discovery of crystalline materials.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation Learning with Visual Observations","url":"https://arxiv.org/abs/2502.02867","date":1739768400,"author":"","guid":993,"unread":true,"content":"<article>arXiv:2502.02867v2 Announce Type: replace \nAbstract: Imitation learning (IL) enables agents to mimic expert behavior without reward signals but faces challenges in cross-domain scenarios with high-dimensional, noisy, and incomplete visual observations. To address this, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL), a novel IL method that extracts domain-invariant features from individual frames and adapts them into sequences to isolate and replicate expert behaviors. We also introduce a frame-wise time labeling technique to segment expert behaviors by timesteps and assign rewards aligned with temporal contexts, enhancing task performance. Experiments across diverse visual environments demonstrate the effectiveness of DIFF-IL in addressing complex visual tasks.</article>","contentLength":814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement Learning","url":"https://arxiv.org/abs/2502.02844","date":1739768400,"author":"","guid":994,"unread":true,"content":"<article>arXiv:2502.02844v2 Announce Type: replace \nAbstract: Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL.</article>","contentLength":753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks","url":"https://arxiv.org/abs/2502.02834","date":1739768400,"author":"","guid":995,"unread":true,"content":"<article>arXiv:2502.02834v2 Announce Type: replace \nAbstract: Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments.</article>","contentLength":844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Relative Homology Theory of Representation in Neural Networks","url":"https://arxiv.org/abs/2502.01360","date":1739768400,"author":"","guid":996,"unread":true,"content":"<article>arXiv:2502.01360v2 Announce Type: replace \nAbstract: Previous research has proven that the set of maps implemented by neural networks with a ReLU activation function is identical to the set of piecewise linear continuous maps. Furthermore, such networks induce a hyperplane arrangement splitting the input domain into convex polyhedra $G_J$ over which the network $\\Phi$ operates in an affine manner.\n  In this work, we leverage these properties to define the equivalence class of inputs $\\sim_\\Phi$, which can be split into two sets related to the local rank of $\\Phi_J$ and the intersections $\\cap \\text{Im}\\Phi_{J_i}$. We refer to the latter as the overlap decomposition $O_\\Phi$ and prove that if the intersections between each polyhedron and the input manifold are convex, the homology groups of neural representations are isomorphic to relative homology groups $H_k(\\Phi(M)) \\simeq H_k(M,O_\\Phi)$. This lets us compute Betti numbers without the choice of an external metric. We develop methods to numerically compute the overlap decomposition through linear programming and a union-find algorithm.\n  Using this framework, we perform several experiments on toy datasets showing that, compared to standard persistent homology, our relative homology-based computation of Betti numbers tracks purely topological rather than geometric features. Finally, we study the evolution of the overlap decomposition during training on various classification problems while varying network width and depth and discuss some shortcomings of our method.</article>","contentLength":1540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"`Do as I say not as I do': A Semi-Automated Approach for Jailbreak Prompt Attack against Multimodal LLMs","url":"https://arxiv.org/abs/2502.00735","date":1739768400,"author":"","guid":997,"unread":true,"content":"<article>arXiv:2502.00735v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the state-of-the-art multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. Further, to better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flanking Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"S2CFormer: Reorienting Learned Image Compression from Spatial Interaction to Channel Aggregation","url":"https://arxiv.org/abs/2502.00700","date":1739768400,"author":"","guid":998,"unread":true,"content":"<article>arXiv:2502.00700v2 Announce Type: replace \nAbstract: Transformers have achieved significant success in learned image compression (LIC), with Swin Transformers emerging as the mainstream choice for nonlinear transforms. A common belief is that their sophisticated spatial operations contribute most to their efficacy. However, the crucial role of the feed-forward network (FFN) based Channel Aggregation module within the transformer architecture has been largely overlooked, and the over-design of spatial operations leads to a suboptimal trade-off between decoding latency and R-D performance. In this paper, we reevaluate the key factors behind the competence of transformers in LIC. By replacing spatial operations with identity mapping, we are surprised to find that channel operations alone can approach the R-D performance of the leading methods. This solid lower bound of performance emphasizes that the presence of channel aggregation is more essential for the LIC model to achieve competitive performance, while the previously complex spatial interactions are partly redundant. Based on this insight, we initiate the \"S2CFormer\" paradigm, a general architecture that reorients the focus of LIC from Spatial Interaction to Channel Aggregation. We present two instantiations of the S2CFormer: S2C-Conv, and S2C-Attention. Each one incorporates a simple operator for spatial interaction and serves as nonlinear transform blocks for our LIC models. Both models demonstrate state-of-the-art (SOTA) R-D performance and significantly faster decoding speed. These results also motivate further exploration of advanced FFN structures to enhance the R-D performance while maintaining model efficiency. With these foundations, we introduce S2C-Hybrid, an enhanced LIC model that combines the strengths of different S2CFormer instantiations. This model outperforms all the existing methods on several datasets, setting a new benchmark for efficient and high-performance LIC.</article>","contentLength":1971,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improved Community Detection using Stochastic Block Models","url":"https://arxiv.org/abs/2502.00686","date":1739768400,"author":"","guid":999,"unread":true,"content":"<article>arXiv:2502.00686v3 Announce Type: replace \nAbstract: Identifying edge-dense communities that are also well-connected is an important aspect of understanding community structure. Prior work has shown that community detection methods can produce poorly connected communities, and some can even produce internally disconnected communities. In this study we evaluate the connectivity of communities obtained using Stochastic Block Models. We find that SBMs produce internally disconnected communities from real-world networks. We present a simple technique, Well-Connected Clusters (WCC), which repeatedly removes small edge cuts until the communities meet a user-specified threshold for well-connectivity. Our study using a large collection of synthetic networks based on clustered real-world networks shows that using WCC as a post-processing tool with SBM community detection typically improves clustering accuracy. WCC is fast enough to use on networks with millions of nodes and is freely available in open source form.</article>","contentLength":1020,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction, and Column Exploration","url":"https://arxiv.org/abs/2502.00675","date":1739768400,"author":"","guid":1000,"unread":true,"content":"<article>arXiv:2502.00675v2 Announce Type: replace \nAbstract: Text-to-SQL systems have unlocked easier access to critical data insights by enabling natural language queries over structured databases. However, deploying such systems in enterprise environments remains challenging due to factors such as large, complex schemas (&gt; 3000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake) and sophisticated query requirements (e.g., transformation, analytics). Current state-of-the-art performance on the Spider 2.0 dataset -- a benchmark built to mimic such complex environments -- remains limited at 20%. Key limitations include inadequate instruction-following, poor long-context comprehension, weak self-refinement, and insufficient dialect-specific knowledge. To address these gaps, we propose ReFoRCE (Self-Refinement Agent with Format Restriction and Column Exploration) which introduces (1) table compression to mitigate long-context limitations (2) format restriction to ensure accurate answer format, and (3) iterative column exploration for enhanced schema understanding. Additionally, it employs self-refinement pipeline consisting of (1) parallelized workflows with voting mechanisms and (2) a Common Table Expression (CTE) based refinement approach to handle unresolved cases. ReFoRCE achieves state-of-the-art results scoring 31.26 on the Spider 2.0-Snow and scoring 30.35 on the Spider 2.0-Lite tasks.</article>","contentLength":1411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Scalable Solver for 2p0s Differential Games with One-Sided Payoff Information and Continuous Actions, States, and Time","url":"https://arxiv.org/abs/2502.00560","date":1739768400,"author":"","guid":1001,"unread":true,"content":"<article>arXiv:2502.00560v2 Announce Type: replace \nAbstract: Existing solvers for imperfect-information extensive-form games (IIEFGs) often struggle with scalability in terms of action and state space sizes and the number of time steps. However, many real-world games involve continuous action and state spaces and occur in continuous time, making them differential in nature. This paper addresses the scalability challenges for a representative class of two-player zero-sum (2p0s) differential games where the informed player knows the game type (payoff) while the uninformed one only has a prior belief over the set of possible types. Such games encompass a wide range of attack-defense scenarios, where the defender adapts based on their belief about the attacker's target. We make the following contributions: (1) We show that under the Isaacs' condition, the complexity of computing the Nash equilibrium for these games is not related to the action space size; and (2) we propose a multigrid approach to effectively reduce the cost of these games when many time steps are involved. Code for this work is available at https://github.com/ghimiremukesh/cams/tree/conf_sub.</article>","contentLength":1166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited Communication","url":"https://arxiv.org/abs/2502.00558","date":1739768400,"author":"","guid":1002,"unread":true,"content":"<article>arXiv:2502.00558v2 Announce Type: replace \nAbstract: We consider the problem setting in which multiple autonomous agents must cooperatively navigate and perform tasks in an unknown, communication-constrained environment. Traditional multi-agent reinforcement learning (MARL) approaches assume synchronous communications and perform poorly in such environments. We propose AsynCoMARL, an asynchronous MARL approach that uses graph transformers to learn communication protocols from dynamic graphs. AsynCoMARL can accommodate infrequent and asynchronous communications between agents, with edges of the graph only forming when agents communicate with each other. We show that AsynCoMARL achieves similar success and collision rates as leading baselines, despite 26\\% fewer messages being passed between agents.</article>","contentLength":808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Line Balancing in the Modern Garment Industry","url":"https://arxiv.org/abs/2502.00455","date":1739768400,"author":"","guid":1003,"unread":true,"content":"<article>arXiv:2502.00455v2 Announce Type: replace \nAbstract: This article presents applied research on line balancing within the modern garment industry, focusing on the significant impact of intelligent hanger systems and hanger lines on the stitching process, by Lean Methodology for garment modernization. It explores the application of line balancing in the modern garment industry, focusing on the significant impact of intelligent hanger systems and hanger lines on the stitching process. It aligns with Lean Methodology principles for garment modernization. Without the implementation of line balancing technology, the garment manufacturing process using hanger systems cannot improve output rates. The case study demonstrates that implementing intelligent line balancing in a straightforward practical setup facilitates lean practices combined with a digitalization system and automaton. This approach illustrates how to enhance output and reduce accumulated work in progress.</article>","contentLength":976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Middleman Bias in Advertising: Aligning Relevance of Keyphrase Recommendations with Search","url":"https://arxiv.org/abs/2502.00131","date":1739768400,"author":"","guid":1004,"unread":true,"content":"<article>arXiv:2502.00131v2 Announce Type: replace \nAbstract: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). Keyphrases must be pertinent to items; otherwise, it can result in seller dissatisfaction and poor targeting -- towards that end relevance filters are employed. In this work, we describe the shortcomings of training relevance filter models on biased click/sales signals. We re-conceptualize advertiser keyphrase relevance as interaction between two dynamical systems -- Advertising which produces the keyphrases and Search which acts as a middleman to reach buyers. We discuss the bias of search relevance systems (middleman bias) and the need to align advertiser keyphrases with search relevance signals. We also compare the performance of cross encoders and bi-encoders in modeling this alignment and the scalability of such a solution for sellers at eBay.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Large Language Models to Enhance Machine Learning Interpretability and Predictive Performance: A Case Study on Emergency Department Returns for Mental Health Patients","url":"https://arxiv.org/abs/2502.00025","date":1739768400,"author":"","guid":1005,"unread":true,"content":"<article>arXiv:2502.00025v3 Announce Type: replace \nAbstract: Importance: Emergency department (ED) returns for mental health conditions pose a major healthcare burden, with 24-27% of patients returning within 30 days. Traditional machine learning models for predicting these returns often lack interpretability for clinical use.\n  Objective: To assess whether integrating large language models (LLMs) with machine learning improves predictive accuracy and clinical interpretability of ED mental health return risk models.\n  Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an academic medical center in the Deep South from January 2018 to December 2022.\n  Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30-day ED return prediction accuracy and (2) model interpretability using a novel LLM-enhanced framework integrating SHAP (SHapley Additive exPlanations) values with clinical knowledge.\n  Results: For chief complaint classification, LLaMA 3 (8B) with 10-shot learning outperformed traditional models (accuracy: 0.882, F1-score: 0.86). In SDoH classification, LLM-based models achieved 0.95 accuracy and 0.96 F1-score, with Alcohol, Tobacco, and Substance Abuse performing best (F1: 0.96-0.89), while Exercise and Home Environment showed lower performance (F1: 0.70-0.67). The LLM-based interpretability framework achieved 99% accuracy in translating model predictions into clinically relevant explanations. LLM-extracted features improved XGBoost AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61.\n  Conclusions and Relevance: Integrating LLMs with machine learning models yielded modest but consistent accuracy gains while significantly enhancing interpretability through automated, clinically relevant explanations. This approach provides a framework for translating predictive analytics into actionable clinical insights.</article>","contentLength":1904,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reward-Guided Speculative Decoding for Efficient LLM Reasoning","url":"https://arxiv.org/abs/2501.19324","date":1739768400,"author":"","guid":1006,"unread":true,"content":"<article>arXiv:2501.19324v2 Announce Type: replace \nAbstract: We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios. The code is available at https://github.com/BaohaoLiao/RSD.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Communication Framework for Compositional Generation","url":"https://arxiv.org/abs/2501.19182","date":1739768400,"author":"","guid":1007,"unread":true,"content":"<article>arXiv:2501.19182v2 Announce Type: replace \nAbstract: Compositionality and compositional generalization--the ability to understand novel combinations of known concepts--are central characteristics of human language and are hypothesized to be essential for human cognition. In machine learning, the emergence of this property has been studied in a communication game setting, where independent agents (a sender and a receiver) converge to a shared encoding policy from a set of states to a space of discrete messages, where the receiver can correctly reconstruct the states observed by the sender using only the sender's messages. The use of communication games in generation tasks is still largely unexplored, with recent methods for compositional generation focusing mainly on the use of supervised guidance (either through class labels or text). In this work, we take the first steps to fill this gap, and we present a self-supervised generative communication game-based framework for creating compositional encodings in learned representations from pre-trained encoder-decoder models. In an Iterated Learning (IL) protocol involving a sender and a receiver, we apply alternating pressures for compression and diversity of encoded discrete messages, so that the protocol converges to an efficient but unambiguous encoding. Approximate message entropy regularization is used to favor compositional encodings. Our framework is based on rigorous justifications and proofs of defining and balancing the concepts of Efficiency, Unambiguity and Non-Holisticity in encoding. We test our method on the compositional image dataset Shapes3D, demonstrating robust performance in both reconstruction and compositionality metrics, surpassing other tested discrete message frameworks.</article>","contentLength":1771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Neural Function Approximation: The XNet Outperforming KAN","url":"https://arxiv.org/abs/2501.18959","date":1739768400,"author":"","guid":1008,"unread":true,"content":"<article>arXiv:2501.18959v2 Announce Type: replace \nAbstract: XNet is a single-layer neural network architecture that leverages Cauchy integral-based activation functions for high-order function approximation. Through theoretical analysis, we show that the Cauchy activation functions used in XNet can achieve arbitrary-order polynomial convergence, fundamentally outperforming traditional MLPs and Kolmogorov-Arnold Networks (KANs) that rely on increased depth or B-spline activations. Our extensive experiments on function approximation, PDE solving, and reinforcement learning demonstrate XNet's superior performance - reducing approximation error by up to 50000 times and accelerating training by up to 10 times compared to existing approaches. These results establish XNet as a highly efficient architecture for both scientific computing and AI applications.</article>","contentLength":854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building Bridges, Not Walls -- Advancing Interpretability by Unifying Feature, Data, and Model Component Attribution","url":"https://arxiv.org/abs/2501.18887","date":1739768400,"author":"","guid":1009,"unread":true,"content":"<article>arXiv:2501.18887v2 Announce Type: replace \nAbstract: The increasing complexity of AI systems has made understanding their behavior a critical challenge. Numerous methods have been developed to attribute model behavior to three key aspects: input features, training data, and internal model components. However, these attribution methods are studied and applied rather independently, resulting in a fragmented landscape of approaches and terminology. This position paper argues that feature, data, and component attribution methods share fundamental similarities, and bridging them can benefit interpretability research. We conduct a detailed analysis of successful methods of these three attribution aspects and present a unified view to demonstrate that these seemingly distinct methods employ similar approaches, such as perturbations, gradients, and linear approximations, differing primarily in their perspectives rather than core techniques. Our unified perspective enhances understanding of existing attribution methods, identifies shared concepts and challenges, makes this field more accessible to newcomers, and highlights new directions not only for attribution and interpretability but also for broader AI research, including model editing, steering, and regulation.</article>","contentLength":1277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Echoes of Discord: Forecasting Hater Reactions to Counterspeech","url":"https://arxiv.org/abs/2501.16235","date":1739768400,"author":"","guid":1010,"unread":true,"content":"<article>arXiv:2501.16235v2 Announce Type: replace \nAbstract: Hate speech (HS) erodes the inclusiveness of online users and propagates negativity and division. Counterspeech has been recognized as a way to mitigate the harmful consequences. While some research has investigated the impact of user-generated counterspeech on social media platforms, few have examined and modeled haters' reactions toward counterspeech, despite the immediate alteration of haters' attitudes being an important aspect of counterspeech. This study fills the gap by analyzing the impact of counterspeech from the hater's perspective, focusing on whether the counterspeech leads the hater to reenter the conversation and if the reentry is hateful. We compile the Reddit Echoes of Hate dataset (ReEco), which consists of triple-turn conversations featuring haters' reactions, to assess the impact of counterspeech. To predict haters' behaviors, we employ two strategies: a two-stage reaction predictor and a three-way classifier. The linguistic analysis sheds insights on the language of counterspeech to hate eliciting different haters' reactions. Experimental results demonstrate that the 3-way classification model outperforms the two-stage reaction predictor, which first predicts reentry and then determines the reentry type. We conclude the study with an assessment showing the most common errors identified by the best-performing model.</article>","contentLength":1410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection","url":"https://arxiv.org/abs/2501.15451","date":1739768400,"author":"","guid":1011,"unread":true,"content":"<article>arXiv:2501.15451v2 Announce Type: replace \nAbstract: The proliferation of hate speech has caused significant harm to society. The intensity and directionality of hate are closely tied to the target and argument it is associated with. However, research on hate speech detection in Chinese has lagged behind, and existing datasets lack span-level fine-grained annotations. Furthermore, the lack of research on Chinese hateful slang poses a significant challenge. In this paper, we provide a solution for fine-grained detection of Chinese hate speech. First, we construct a dataset containing Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first span-level Chinese hate speech dataset. Secondly, we evaluate the span-level hate speech detection performance of existing models using STATE ToxiCN. Finally, we conduct the first study on Chinese hateful slang and evaluate the ability of LLMs to detect such expressions. Our work contributes valuable resources and insights to advance span-level hate speech detection in Chinese.</article>","contentLength":1046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models","url":"https://arxiv.org/abs/2501.15140","date":1739768400,"author":"","guid":1012,"unread":true,"content":"<article>arXiv:2501.15140v2 Announce Type: replace \nAbstract: Multi-modal large language models (MLLMs) have shown remarkable abilities in various visual understanding tasks. However, MLLMs still struggle with fine-grained visual recognition (FGVR), which aims to identify subordinate-level categories from images. This can negatively impact more advanced capabilities of MLLMs, such as object-centric visual question answering and reasoning. In our study, we revisit three quintessential capabilities of MLLMs for FGVR, including object information extraction, category knowledge reserve, object-category alignment, and position of the root cause as a misalignment problem. To address this issue, we present Finedefics, an MLLM that enhances the model's FGVR capability by incorporating informative attribute descriptions of objects into the training phase. We employ contrastive learning on object-attribute pairs and attribute-category pairs simultaneously and use examples from similar but incorrect categories as hard negatives, naturally bringing representations of visual objects and category names closer. Extensive evaluations across multiple popular FGVR datasets demonstrate that Finedefics outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy. The code is available at https://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Center Cooling System Optimization Using Offline Reinforcement Learning","url":"https://arxiv.org/abs/2501.15085","date":1739768400,"author":"","guid":1013,"unread":true,"content":"<article>arXiv:2501.15085v2 Announce Type: replace \nAbstract: The recent advances in information technology and artificial intelligence have fueled a rapid expansion of the data center (DC) industry worldwide, accompanied by an immense appetite for electricity to power the DCs. In a typical DC, around 30~40% of the energy is spent on the cooling system rather than on computer servers, posing a pressing need for developing new energy-saving optimization technologies for DC cooling systems. However, optimizing such real-world industrial systems faces numerous challenges, including but not limited to a lack of reliable simulation environments, limited historical data, and stringent safety and control robustness requirements. In this work, we present a novel physics-informed offline reinforcement learning (RL) framework for energy efficiency optimization of DC cooling systems. The proposed framework models the complex dynamical patterns and physical dependencies inside a server room using a purposely designed graph neural network architecture that is compliant with the fundamental time-reversal symmetry. Because of its well-behaved and generalizable state-action representations, the model enables sample-efficient and robust latent space offline policy learning using limited real-world operational data. Our framework has been successfully deployed and verified in a large-scale production DC for closed-loop control of its air-cooling units (ACUs). We conducted a total of 2000 hours of short and long-term experiments in the production DC environment. The results show that our method achieves 14~21% energy savings in the DC cooling system, without any violation of the safety or operational constraints. Our results have demonstrated the significant potential of offline RL in solving a broad range of data-limited, safety-critical real-world industrial control problems.</article>","contentLength":1882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation","url":"https://arxiv.org/abs/2501.14679","date":1739768400,"author":"","guid":1014,"unread":true,"content":"<article>arXiv:2501.14679v4 Announce Type: replace \nAbstract: Attention-based methods have demonstrated exceptional performance in modelling long-range dependencies on spherical cortical surfaces, surpassing traditional Geometric Deep Learning (GDL) models. However, their extensive inference time and high memory demands pose challenges for application to large datasets with limited computing resources. Inspired by the state space model in computer vision, we introduce the attention-free Vision Mamba (Vim) to spherical surfaces, presenting a domain-agnostic architecture for analyzing data on spherical manifolds. Our method achieves surface patching by representing spherical data as a sequence of triangular patches derived from a subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on multiple neurodevelopmental phenotype regression tasks using cortical surface metrics from neonatal brains. Experimental results demonstrate that SiM outperforms both attention- and GDL-based methods, delivering 4.8 times faster inference and achieving 91.7% lower memory consumption compared to the Surface Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity analysis further underscores the potential of SiM to identify subtle cognitive developmental patterns. The code is available at https://github.com/Rongzhao-He/surface-vision-mamba.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating and Improving Graph to Text Generation with Large Language Models","url":"https://arxiv.org/abs/2501.14497","date":1739768400,"author":"","guid":1015,"unread":true,"content":"<article>arXiv:2501.14497v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated immense potential across various tasks. However, research for exploring and improving the capabilities of LLMs in interpreting graph structures remains limited. To address this gap, we conduct a comprehensive evaluation of prompting current open-source LLMs on graph-to-text generation tasks. Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found that the improvements from tuning-free approaches were incremental, as LLMs struggle with planning on complex graphs, particularly those with a larger number of triplets. To further improve LLMs in planning with graph sequences and grounding in truth, we introduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks: reordering and attribution. Through extensive automatic and human evaluations, we demonstrate significant improvements in the quality of generated text from both few-shot learning and fine-tuning perspectives using the PlanGTG dataset. Our study paves the way for new research directions in graph-to-text generation. PlanGTG datasets can be found in https://github.com/probe2/kg_text.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Inverse Linear Optimization: Improved Regret Bound, Robustness to Suboptimality, and Toward Tight Regret Analysis","url":"https://arxiv.org/abs/2501.14349","date":1739768400,"author":"","guid":1016,"unread":true,"content":"<article>arXiv:2501.14349v5 Announce Type: replace \nAbstract: We study an online learning problem where, over $T$ rounds, a learner observes both time-varying sets of feasible actions and an agent's optimal actions, selected by solving linear optimization over the feasible actions. The learner sequentially makes predictions of the agent's underlying linear objective function, and their quality is measured by the regret, the cumulative gap between optimal objective values and those achieved by following the learner's predictions. A seminal work by B\\\"armann et al. (ICML 2017) showed that online learning methods can be applied to this problem to achieve regret bounds of $O(\\sqrt{T})$. Recently, Besbes et al. (COLT 2021, Oper. Res. 2023) significantly improved the result by achieving an $O(n^4\\ln T)$ regret bound, where $n$ is the dimension of the ambient space of objective vectors. Their method, based on the ellipsoid method, runs in polynomial time but is inefficient for large $n$ and $T$. In this paper, we obtain an $O(n\\ln T)$ regret bound, improving upon the previous bound of $O(n^4\\ln T)$ by a factor of $n^3$. Our method is simple and efficient: we apply the online Newton step (ONS) to appropriate exp-concave loss functions. Moreover, for the case where the agent's actions are possibly suboptimal, we establish an $O(n\\ln T+\\sqrt{\\Delta_Tn\\ln T})$ regret bound, where $\\Delta_T$ is the cumulative suboptimality of the agent's actions. This bound is achieved by using MetaGrad, which runs ONS with $\\Theta(\\ln T)$ different learning rates in parallel. We also provide a simple instance that implies an $\\Omega(n)$ lower bound, showing that our $O(n\\ln T)$ bound is tight up to an $O(\\ln T)$ factor. This gives rise to a natural question: can the $O(\\ln T)$ factor in the upper bound be removed? For the special case of $n=2$, we show that an $O(1)$ regret bound is possible, while we delineate challenges in extending this result to higher dimensions.</article>","contentLength":1965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS","url":"https://arxiv.org/abs/2501.13238","date":1739768400,"author":"","guid":1017,"unread":true,"content":"<article>arXiv:2501.13238v2 Announce Type: replace \nAbstract: This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary search (BS) locking, designed to cover a broad frequency range from 533 MHz to 4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a linear to a logarithmic function, completing in B+1 cycles, where B represents the digital-to-analog (DAC) resolution controlling the voltage-controlled delay line (VCDL). At the start of the BS process, large step sizes can cause significant bias overshoots, potentially leading to clock failure conditions (i.e., clocks fail to propagate through the VCDL). To address this issue, a toggle detector is introduced to monitor clock activity and adjust the binary search controller. Upon detecting a stalled clock, the controller reverts the DAC code to the previous working code and resumes the BS with a reduced step size. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a locking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at 4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with a static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter of 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit (FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\\cdot ns^2$.</article>","contentLength":1353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small Language Models for Biomedical Question Answering","url":"https://arxiv.org/abs/2501.12746","date":1739768400,"author":"","guid":1018,"unread":true,"content":"<article>arXiv:2501.12746v4 Announce Type: replace \nAbstract: When addressing professional questions in the biomedical domain, humans typically acquire multiple pieces of information as evidence and engage in multifaceted analysis to provide high-quality answers. Current LLM-based question answering methods lack a detailed definition and learning process for evidence analysis, leading to the risk of error propagation and hallucinations while using evidence. Although increasing the parameter size of LLMs can alleviate these issues, it also presents challenges in training and deployment with limited resources. In this study, we propose EvidenceMap, which aims to enable a tiny pre-trained language model to explicitly learn multiple aspects of biomedical evidence, including supportive evaluation, logical correlation and content summarization, thereby latently guiding a small generative model (around 3B parameters) to provide textual responses. Experimental results demonstrate that our method, learning evidence analysis by fine-tuning a model with only 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in reference-based quality and accuracy, respectively.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Growth strategies for arbitrary DAG neural architectures","url":"https://arxiv.org/abs/2501.12690","date":1739768400,"author":"","guid":1019,"unread":true,"content":"<article>arXiv:2501.12690v2 Announce Type: replace \nAbstract: Deep learning has shown impressive results obtained at the cost of training huge neural networks. However, the larger the architecture, the higher the computational, financial, and environmental costs during training and inference. We aim at reducing both training and inference durations. We focus on Neural Architecture Growth, which can increase the size of a small model when needed, directly during training using information from the backpropagation. We expand existing work and freely grow neural networks in the form of any Directed Acyclic Graph by reducing expressivity bottlenecks in the architecture. We explore strategies to reduce excessive computations and steer network growth toward more parameter-efficient architectures.</article>","contentLength":792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL","url":"https://arxiv.org/abs/2501.12372","date":1739768400,"author":"","guid":1020,"unread":true,"content":"<article>arXiv:2501.12372v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.\n  In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve strong performances on various benchmark datasets without finetuning and expensive self-consistency based techniques.</article>","contentLength":1494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Generalized Chernoff-Stein Lemma, Applications and Examples","url":"https://arxiv.org/abs/2501.12066","date":1739768400,"author":"","guid":1021,"unread":true,"content":"<article>arXiv:2501.12066v2 Announce Type: replace \nAbstract: In this manuscript we define the notion of \"$\\delta$-typicality\" for both entropy and relative entropy, as well as a notion of $\\epsilon$-goodness and provide an extension to Stein's lemma for continuous quantities as well as correlated setups. We apply the derived results on the Gaussian hypothesis testing problem where the observations are possibly correlated.</article>","contentLength":417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DynoSAM: Open-Source Smoothing and Mapping Framework for Dynamic SLAM","url":"https://arxiv.org/abs/2501.11893","date":1739768400,"author":"","guid":1022,"unread":true,"content":"<article>arXiv:2501.11893v2 Announce Type: replace \nAbstract: Traditional Visual Simultaneous Localization and Mapping (vSLAM) systems focus solely on static scene structures, overlooking dynamic elements in the environment. Although effective for accurate visual odometry in complex scenarios, these methods discard crucial information about moving objects. By incorporating this information into a Dynamic SLAM framework, the motion of dynamic entities can be estimated, enhancing navigation whilst ensuring accurate localization. However, the fundamental formulation of Dynamic SLAM remains an open challenge, with no consensus on the optimal approach for accurate motion estimation within a SLAM pipeline. Therefore, we developed DynoSAM, an open-source framework for Dynamic SLAM that enables the efficient implementation, testing, and comparison of various Dynamic SLAM optimization formulations. DynoSAM integrates static and dynamic measurements into a unified optimization problem solved using factor graphs, simultaneously estimating camera poses, static scene, object motion or poses, and object structures. We evaluate DynoSAM across diverse simulated and real-world datasets, achieving state-of-the-art motion estimation in indoor and outdoor environments, with substantial improvements over existing systems. Additionally, we demonstrate DynoSAM utility in downstream applications, including 3D reconstruction of dynamic scenes and trajectory prediction, thereby showcasing potential for advancing dynamic object-aware SLAM systems. DynoSAM is open-sourced at https://github.com/ACFR-RPG/DynOSAM.</article>","contentLength":1601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ParkView: Visualizing Monotone Interleavings","url":"https://arxiv.org/abs/2501.10728","date":1739768400,"author":"","guid":1023,"unread":true,"content":"<article>arXiv:2501.10728v2 Announce Type: replace \nAbstract: Merge trees are a powerful tool from topological data analysis that is frequently used to analyze scalar fields. The similarity between two merge trees can be captured by an interleaving: a pair of maps between the trees that jointly preserve ancestor relations in the trees. Interleavings can have a complex structure; visualizing them requires a sense of (drawing) order which is not inherent in this purely topological concept. However, in practice it is often desirable to introduce additional geometric constraints, which leads to variants such as labeled or monotone interleavings. Monotone interleavings respect a given order on the leaves of the merge trees and hence have the potential to be visualized in a clear and comprehensive manner.\n  In this paper, we introduce ParkView: a schematic, scalable encoding for monotone interleavings. ParkView captures both maps of the interleaving using an optimal decomposition of both trees into paths and corresponding branches. We prove several structural properties of monotone interleavings, which support a sparse visual encoding using active paths and hedges that can be linked using a maximum of 6 colors for merge trees of arbitrary size. We show how to compute an optimal path-branch decomposition in linear time and illustrate ParkView on a number of real-world datasets.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation using Tree-based Search","url":"https://arxiv.org/abs/2501.10053","date":1739768400,"author":"","guid":1024,"unread":true,"content":"<article>arXiv:2501.10053v2 Announce Type: replace \nAbstract: Leveraging the autonomous decision-making capabilities of large language models (LLMs) has demonstrated superior performance in reasoning tasks. However, despite the success of iterative or recursive retrieval-augmented generation (RAG) techniques, these methods are often constrained to a single solution space when confronted with complex problems. In this paper, we propose a novel thinking pattern in RAG that integrates system analysis with efficient reasoning actions, significantly activating intrinsic reasoning capabilities and expanding the solution space of specific tasks via Monte Carlo Tree Search (MCTS), which we refer to as AirRAG. Specifically, our approach designs five fundamental reasoning actions, which are expanded to a broad tree-based reasoning space using MCTS. The approach also incorporates self-consistency verification to explore potential reasoning paths and inference scaling law. Additionally, computationally optimal strategies are employed to allocate more inference resources to key actions, thereby enhancing overall performance. Experimental results demonstrate the effectiveness of AirRAG, showing significant performance gains on complex question-answering datasets. Furthermore, AirRAG is flexible and lightweight, making it easy to integrate with other advanced technologies.</article>","contentLength":1371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities","url":"https://arxiv.org/abs/2501.08648","date":1739768400,"author":"","guid":1025,"unread":true,"content":"<article>arXiv:2501.08648v2 Announce Type: replace \nAbstract: While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, unidirectional and bidirectional models are typically trained separately with distinct objectives (generation and representation learning). This separation overlooks the opportunity for developing a more versatile language model and for these objectives to complement each other. In this work, we propose MAGNET, a method for adapting decoder-only LLMs to generate robust representations and infill missing text spans. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. Our results demonstrate that LLMs adapted with MAGNET (1) surpass strong text encoders on token-level and sentence-level representation learning tasks, (2) generate contextually appropriate text infills by leveraging past and future contexts, (3) perform open-ended text generation without excessive repetition of words or phrases, and (4) preserve the knowledge and reasoning capability gained by the LLM during pretraining.</article>","contentLength":1267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification","url":"https://arxiv.org/abs/2501.02844","date":1739768400,"author":"","guid":1026,"unread":true,"content":"<article>arXiv:2501.02844v3 Announce Type: replace \nAbstract: Text classification is a fundamental task in data mining, pivotal to various applications such as tabular understanding and recommendation. Although neural network-based models, such as CNN and BERT, have demonstrated remarkable performance in text classification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shot text classification, where labeled data is scarce, and new target labels frequently appear based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding ability. Current approaches provide LLMs with text inputs, candidate labels, and additional side information (e.g., descriptions) to classify texts. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shot text classification. Rather than treating each input independently, GORAG constructs and maintains a weighted graph by extracting side information across all target texts. In this graph, text keywords and labels are represented as nodes, with edges indicating the correlations between them. To model these correlations, GORAG employs an edge weighting mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for each text input. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and precise contextual information.</article>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations","url":"https://arxiv.org/abs/2501.02074","date":1739768400,"author":"","guid":1027,"unread":true,"content":"<article>arXiv:2501.02074v2 Announce Type: replace \nAbstract: Societal stereotypes are at the center of a myriad of responsible AI interventions targeted at reducing the generation and propagation of potentially harmful outcomes. While these efforts are much needed, they tend to be fragmented and often address different parts of the issue without taking in a unified or holistic approach about social stereotypes and how they impact various parts of the machine learning pipeline. As a result, it fails to capitalize on the underlying mechanisms that are common across different types of stereotypes, and to anchor on particular aspects that are relevant in certain cases. In this paper, we draw on social psychological research, and build on NLP data and methods, to propose a unified framework to operationalize stereotypes in generative AI evaluations. Our framework identifies key components of stereotypes that are crucial in AI evaluation, including the target group, associated attribute, relationship characteristics, perceiving group, and relevant context. We also provide considerations and recommendations for its responsible use.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Refinement Strategies for LLM-based Product Attribute Value Extraction","url":"https://arxiv.org/abs/2501.01237","date":1739768400,"author":"","guid":1028,"unread":true,"content":"<article>arXiv:2501.01237v2 Announce Type: replace \nAbstract: Structured product data, in the form of attribute-value pairs, is essential for e-commerce platforms to support features such as faceted product search and attribute-based product comparison. However, vendors often provide unstructured product descriptions, making attribute value extraction necessary to ensure data consistency and usability. Large language models (LLMs) have demonstrated their potential for product attribute value extraction in few-shot scenarios. Recent research has shown that self-refinement techniques can improve the performance of LLMs on tasks such as code generation and text-to-SQL translation. For other tasks, the application of these techniques has resulted in increased costs due to processing additional tokens, without achieving any improvement in performance. This paper investigates applying two self-refinement techniques (error-based prompt rewriting and self-correction) to the product attribute value extraction task. The self-refinement techniques are evaluated across zero-shot, few-shot in-context learning, and fine-tuning scenarios using GPT-4o. The experiments show that both self-refinement techniques fail to significantly improve the extraction performance while substantially increasing processing costs. For scenarios with development data, fine-tuning yields the highest performance, while the ramp-up costs of fine-tuning are balanced out as the amount of product descriptions increases.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reduced Order Models and Conditional Expectation -- Analysing Parametric Low-Order Approximations","url":"https://arxiv.org/abs/2412.19836","date":1739768400,"author":"","guid":1029,"unread":true,"content":"<article>arXiv:2412.19836v2 Announce Type: replace \nAbstract: Systems may depend on parameters which one may control, or which serve to optimise the system, or are imposed externally, or they could be uncertain. This last case is taken as the ``Leitmotiv'' for the following. A reduced order model is produced from the full order model by some kind of projection onto a relatively low-dimensional manifold or subspace. The parameter dependent reduction process produces a function of the parameters into the manifold. One now wants to examine the relation between the full and the reduced state for all possible parameter values of interest. Similarly, in the field of machine learning, also a function of the parameter set into the image space of the machine learning model is learned on a training set of samples, typically minimising the mean-square error. This set may be seen as a sample from some probability distribution, and thus the training is an approximate computation of the expectation, giving an approximation to the conditional expectation, a special case of an Bayesian updating where the Bayesian loss function is the mean-square error. This offers the possibility of having a combined look at these methods, and also of introducing more general loss functions.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations","url":"https://arxiv.org/abs/2412.18940","date":1739768400,"author":"","guid":1030,"unread":true,"content":"<article>arXiv:2412.18940v2 Announce Type: replace \nAbstract: Songwriting is often driven by multimodal inspirations, such as imagery, narratives, or existing music, yet songwriters remain unsupported by current music AI systems in incorporating these multimodal inputs into their creative processes. We introduce Amuse, a songwriting assistant that transforms multimodal (image, text, or audio) inputs into chord progressions that can be seamlessly incorporated into songwriters' creative processes. A key feature of Amuse is its novel method for generating coherent chords that are relevant to music keywords in the absence of datasets with paired examples of multimodal inputs and chords. Specifically, we propose a method that leverages multimodal large language models (LLMs) to convert multimodal inputs into noisy chord suggestions and uses a unimodal chord model to filter the suggestions. A user study with songwriters shows that Amuse effectively supports transforming multimodal ideas into coherent musical suggestions, enhancing users' agency and creativity throughout the songwriting process.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty-Aware Critic Augmentation for Hierarchical Multi-Agent EV Charging Control","url":"https://arxiv.org/abs/2412.18047","date":1739768400,"author":"","guid":1031,"unread":true,"content":"<article>arXiv:2412.18047v2 Announce Type: replace \nAbstract: The advanced bidirectional EV charging and discharging technology, aimed at supporting grid stability and emergency operations, has driven a growing interest in workplace applications. It not only reduces electricity expenses but also enhances the resilience in handling practical matters, such as peak power limitation, fluctuating energy prices, and unpredictable EV departures. Considering these factors systematically can benefit energy efficiency in office buildings and for EV users simultaneously. To employ AI to address these issues, we propose HUCA, a novel real-time charging control for regulating energy demands for both the building and EVs. HUCA employs hierarchical actor-critic networks to dynamically reduce electricity costs in buildings, accounting for the needs of EV charging in the dynamic pricing scenario. To tackle the uncertain EV departures, we introduce a new critic augmentation to account for departure uncertainties in evaluating the charging decisions, while maintaining the robustness of the charging control. Experiments on real-world electricity datasets under both simulated certain and uncertain departure scenarios demonstrate that HUCA outperforms baselines in terms of total electricity costs while maintaining competitive performance in fulfilling EV charging requirements. A case study also manifests that HUCA effectively balances energy supply between the building and EVs based on real-time information, showcasing its potential as a key AI-driven solution for vehicle charging control.</article>","contentLength":1585,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ArchComplete: Autoregressive 3D Architectural Design Generation with Hierarchical Diffusion-Based Upsampling","url":"https://arxiv.org/abs/2412.17957","date":1739768400,"author":"","guid":1032,"unread":true,"content":"<article>arXiv:2412.17957v2 Announce Type: replace \nAbstract: Recent advances in 3D generative models have shown promising results but often fall short in capturing the complexity of architectural geometries and topologies and fine geometric details at high resolutions. To tackle this, we present ArchComplete, a two-stage voxel-based 3D generative pipeline consisting of a vector-quantised model, whose composition is modelled with an autoregressive transformer for generating coarse shapes, followed by a hierarchical upsampling strategy for further enrichment with fine structures and details. Key to our pipeline is (i) learning a contextually rich codebook of local patch embeddings, optimised alongside a 2.5D perceptual loss that captures global spatial correspondence of projections onto three axis-aligned orthogonal planes, and (ii) redefining upsampling as a set of conditional diffusion models learning from a hierarchy of randomly cropped coarse-to-fine local volumetric patches. Trained on our introduced dataset of 3D house models with fully modelled exterior and interior, ArchComplete autoregressively generates models at the resolution of $64^{3}$ and progressively refines them up to $512^{3}$, with voxel sizes as small as $ \\approx 9\\text{cm}$. ArchComplete solves a variety of tasks, including genetic interpolation and variation, unconditional synthesis, shape and plan-drawing completion, as well as geometric detailisation, while achieving state-of-the-art performance in quality, diversity, and computational efficiency.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UAV Communications: Impact of Obstacles on Channel Characteristics","url":"https://arxiv.org/abs/2412.17934","date":1739768400,"author":"","guid":1033,"unread":true,"content":"<article>arXiv:2412.17934v3 Announce Type: replace \nAbstract: In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as effective platforms for carrying Wi-Fi Access Points (APs) and cellular Base Stations (BSs), enabling low-cost, agile, and flexible wireless networks with high Quality of Service (QoS). The next generation of wireless communications will rely on increasingly higher frequencies, which are easily obstructed by obstacles. One of the most critical concepts yet to be fully addressed is positioning the UAV at optimal coordinates while accounting for obstacles. To ensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS, and establish reliable wireless links with maximum coverage, obstacles must be integrated into the proposed placement algorithms. This paper introduces a simulation-based measurement approach for characterizing an air-to-ground (AG) channel in a simple scenario. By considering obstacles, we present a novel perspective on channel characterization. The results, in terms of throughput, packet delivery, packet loss, and delay, are compared using the proposed positioning approach.</article>","contentLength":1146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero Shot Time Series Forecasting Using Kolmogorov Arnold Networks","url":"https://arxiv.org/abs/2412.17853","date":1739768400,"author":"","guid":1034,"unread":true,"content":"<article>arXiv:2412.17853v2 Announce Type: replace \nAbstract: Accurate energy price forecasting is crucial for participants in day-ahead energy markets, as it significantly influences their decision-making processes. While machine learning-based approaches have shown promise in enhancing these forecasts, they often remain confined to the specific markets on which they are trained, thereby limiting their adaptability to new or unseen markets. In this paper, we introduce a cross-domain adaptation model designed to forecast energy prices by learning market-invariant representations across different markets during the training phase. We propose a doubly residual N-BEATS network with Kolmogorov Arnold networks at its core for time series forecasting. These networks, grounded in the Kolmogorov-Arnold representation theorem, offer a powerful way to approximate multivariate continuous functions. The cross domain adaptation model was generated with an adversarial framework. The model's effectiveness was tested in predicting day-ahead electricity prices in a zero shot fashion. In comparison with baseline models, our proposed framework shows promising results. By leveraging the Kolmogorov-Arnold networks, our model can potentially enhance its ability to capture complex patterns in energy price data, thus improving forecast accuracy across diverse market conditions. This addition not only enriches the model's representational capacity but also contributes to a more robust and flexible forecasting tool adaptable to various energy markets.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Examining Imbalance Effects on Performance and Demographic Fairness of Clinical Language Models","url":"https://arxiv.org/abs/2412.17803","date":1739768400,"author":"","guid":1035,"unread":true,"content":"<article>arXiv:2412.17803v2 Announce Type: replace \nAbstract: Data imbalance is a fundamental challenge in applying language models to biomedical applications, particularly in ICD code prediction tasks where label and demographic distributions are uneven. While state-of-the-art language models have been increasingly adopted in biomedical tasks, few studies have systematically examined how data imbalance affects model performance and fairness across demographic groups. This study fills the gap by statistically probing the relationship between data imbalance and model performance in ICD code prediction. We analyze imbalances in a standard benchmark data across gender, age, ethnicity, and social determinants of health by state-of-the-art biomedical language models. By deploying diverse performance metrics and statistical analyses, we explore the influence of data imbalance on performance variations and demographic fairness. Our study shows that data imbalance significantly impacts model performance and fairness, but feature similarity to the majority class may be a more critical factor. We believe this study provides valuable insights for developing more equitable and robust language models in healthcare applications.</article>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QTSeg: A Query Token-Based Dual-Mix Attention Framework with Multi-Level Feature Distribution for Medical Image Segmentation","url":"https://arxiv.org/abs/2412.17241","date":1739768400,"author":"","guid":1036,"unread":true,"content":"<article>arXiv:2412.17241v2 Announce Type: replace \nAbstract: Medical image segmentation plays a crucial role in assisting healthcare professionals with accurate diagnoses and enabling automated diagnostic processes. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies, while transformer-based architectures, despite their effectiveness, come with increased computational complexity. Recent efforts have focused on combining CNNs and transformers to balance performance and efficiency, but existing approaches still face challenges in achieving high segmentation accuracy while maintaining low computational costs. Furthermore, many methods underutilize the CNN encoder's capability to capture local spatial information, concentrating primarily on mitigating long-range dependency issues. To address these limitations, we propose QTSeg, a novel architecture for medical image segmentation that effectively integrates local and global information. QTSeg features a dual-mix attention decoder designed to enhance segmentation performance through: (1) a cross-attention mechanism for improved feature alignment, (2) a spatial attention module to capture long-range dependencies, and (3) a channel attention block to learn inter-channel relationships. Additionally, we introduce a multi-level feature distribution module, which adaptively balances feature propagation between the encoder and decoder, further boosting performance. Extensive experiments on five publicly available datasets covering diverse segmentation tasks, including lesion, polyp, breast cancer, cell, and retinal vessel segmentation, demonstrate that QTSeg outperforms state-of-the-art methods across multiple evaluation metrics while maintaining lower computational costs. Our implementation can be found at: https://github.com/tpnam0901/QTSeg (v1.0.0)</article>","contentLength":1862,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learn2Mix: Training Neural Networks Using Adaptive Data Integration","url":"https://arxiv.org/abs/2412.16482","date":1739768400,"author":"","guid":1037,"unread":true,"content":"<article>arXiv:2412.16482v2 Announce Type: replace \nAbstract: Accelerating model convergence within resource-constrained environments is critical to ensure fast and efficient neural network training. This work presents learn2mix, a novel training strategy that adaptively adjusts class proportions within batches, focusing on classes with higher error rates. Unlike classical training methods that use static class proportions, learn2mix continually adapts class proportions during training, leading to faster convergence. Empirical evaluations conducted on benchmark datasets show that neural networks trained with learn2mix converge faster than those trained with existing approaches, achieving improved results for classification, regression, and reconstruction tasks under limited training resources and with imbalanced classes. Our empirical findings are supported by theoretical analysis.</article>","contentLength":885,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous Driving Systems","url":"https://arxiv.org/abs/2412.16265","date":1739768400,"author":"","guid":1038,"unread":true,"content":"<article>arXiv:2412.16265v3 Announce Type: replace \nAbstract: Existing Autonomous Driving Systems (ADS) independently make driving decisions, but they face two significant limitations. First, in complex scenarios, ADS may misinterpret the environment and make inappropriate driving decisions. Second, these systems are unable to incorporate human driving preferences in their decision-making processes. This paper proposes Autoware$.$Flex, a novel ADS system that incorporates human input into the driving process, allowing users to guide the ADS in making more appropriate decisions and ensuring their preferences are satisfied. Achieving this needs to address two key challenges: (1) translating human instructions, expressed in natural language, into a format the ADS can understand, and (2) ensuring these instructions are executed safely and consistently within the ADS' s decision-making framework. For the first challenge, we employ a Large Language Model (LLM) assisted by an ADS-specialized knowledge base to enhance domain-specific translation. For the second challenge, we design a validation mechanism to ensure that human instructions result in safe and consistent driving behavior. Experiments conducted on both simulators and a real-world autonomous vehicle demonstrate that Autoware$.$Flex effectively interprets human instructions and executes them safely.</article>","contentLength":1364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continual Learning with Strategic Selection and Forgetting for Network Intrusion Detection","url":"https://arxiv.org/abs/2412.16264","date":1739768400,"author":"","guid":1039,"unread":true,"content":"<article>arXiv:2412.16264v3 Announce Type: replace \nAbstract: Intrusion Detection Systems (IDS) are crucial for safeguarding digital infrastructure. In dynamic network environments, both threat landscapes and normal operational behaviors are constantly changing, resulting in concept drift. While continuous learning mitigates the adverse effects of concept drift, insufficient attention to drift patterns and excessive preservation of outdated knowledge can still hinder the IDS's adaptability. In this paper, we propose SSF (Strategic Selection and Forgetting), a novel continual learning method for IDS, providing continuous model updates with a constantly refreshed memory buffer. Our approach features a strategic sample selection algorithm to select representative new samples and a strategic forgetting mechanism to drop outdated samples. The proposed strategic sample selection algorithm prioritizes new samples that cause the `drifted' pattern, enabling the model to better understand the evolving landscape. Additionally, we introduce strategic forgetting upon detecting significant drift by discarding outdated samples to free up memory, allowing the incorporation of more recent data. SSF captures evolving patterns effectively and ensures the model is aligned with the change of data patterns, significantly enhancing the IDS's adaptability to concept drift. The state-of-the-art performance of SSF on NSL-KDD and UNSW-NB15 datasets demonstrates its superior adaptability to concept drift for network intrusion detection. The code is released at https://github.com/xinchen930/SSF-Strategic-Selection-and-Forgetting.</article>","contentLength":1619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WigglyEyes: Inferring Eye Movements from Keypress Data","url":"https://arxiv.org/abs/2412.15669","date":1739768400,"author":"","guid":1040,"unread":true,"content":"<article>arXiv:2412.15669v2 Announce Type: replace \nAbstract: We present a model for inferring where users look during interaction based on keypress data only. Given a key log, it outputs a scanpath that tells, moment-by-moment, how the user had moved eyes while entering those keys. The model can be used as a proxy for human data in cases where collecting real eye tracking data is expensive or impossible. Our technical insight is three-fold: first, we present an inference architecture that considers the individual characteristics of the user, inferred as a low-dimensional parameter vector; second, we present a novel loss function for synchronizing inferred eye movements with the keypresses; third, we train the model using a hybrid approach with both human data and synthetically generated data. The approach can be applied in interactive systems where predictive models of user behavior are available. We report results from evaluation in the challenging case of touchscreen typing, where the model accurately inferred real eye movements.</article>","contentLength":1039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation","url":"https://arxiv.org/abs/2412.14050","date":1739768400,"author":"","guid":1041,"unread":true,"content":"<article>arXiv:2412.14050v3 Announce Type: replace \nAbstract: Recent generative large language models (LLMs) show remarkable performance in non-English languages, but when prompted in those languages they tend to express higher harmful social biases and toxicity levels. Prior work has shown that finetuning on specialized datasets can mitigate this behavior, and doing so in English can transfer to other languages. In this work, we investigate the impact of different finetuning methods on the model's bias and toxicity, but also on its ability to produce fluent and diverse text. We reduce biases by finetuning on curated non-harmful text, but find only direct preference optimization to be effective for mitigating toxicity. The mitigation caused by applying these methods in English also transfers to non-English languages. We find evidence that the extent to which transfer takes place can be predicted by the amount of data in a given language present in the model's pretraining data. However, this transfer of bias and toxicity mitigation often comes at the expense of decreased language generation ability in non-English languages, highlighting the importance of developing language-specific bias and toxicity mitigation methods.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation","url":"https://arxiv.org/abs/2412.13877","date":1739768400,"author":"","guid":1042,"unread":true,"content":"<article>arXiv:2412.13877v2 Announce Type: replace \nAbstract: In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot Manipulation), a dataset containing 107k demonstration trajectories across 479 diverse tasks involving 96 object classes. RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related information, including multi-view observations, proprioceptive robot state information, and linguistic task descriptions. To ensure data consistency and reliability for imitation learning, RoboMIND is built on a unified data collection platform and a standardized protocol, covering four distinct robotic embodiments: the Franka Emika Panda, the UR5e, the AgileX dual-arm robot, and a humanoid robot with dual dexterous hands. Our dataset also includes 5k real-world failure demonstrations, each accompanied by detailed causes, enabling failure reflection and correction during policy learning. Additionally, we created a digital twin environment in the Isaac Sim simulator, replicating the real-world tasks and assets, which facilitates the low-cost collection of additional training data and enables efficient evaluation. To demonstrate the quality and diversity of our dataset, we conducted extensive experiments using various imitation learning methods for single-task settings and state-of-the-art Vision-Language-Action (VLA) models for multi-task scenarios. By leveraging RoboMIND, the VLA models achieved high manipulation success rates and demonstrated strong generalization capabilities. To the best of our knowledge, RoboMIND is the largest multi-embodiment teleoperation dataset collected on a unified platform, providing large-scale and high-quality robotic training data. Our project is at https://x-humanoid-robomind.github.io/.</article>","contentLength":1807,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RareAgents: Advancing Rare Disease Care through LLM-Empowered Multi-disciplinary Team","url":"https://arxiv.org/abs/2412.12475","date":1739768400,"author":"","guid":1043,"unread":true,"content":"<article>arXiv:2412.12475v2 Announce Type: replace \nAbstract: Rare diseases, despite their low individual incidence, collectively impact around 300 million people worldwide due to the vast number of diseases. The involvement of multiple organs and systems, and the shortage of specialized doctors with relevant experience make diagnosing and treating rare diseases more challenging than common diseases. Recently, agents powered by large language models (LLMs) have demonstrated notable applications across various domains. In the medical field, some agent methods have outperformed direct prompts in question-answering tasks from medical examinations. However, current agent frameworks are not well-adapted to real-world clinical scenarios, especially those involving the complex demands of rare diseases. To bridge this gap, we introduce RareAgents, the first LLM-driven multi-disciplinary team framework designed specifically for the complex clinical context of rare diseases. RareAgents integrates advanced Multidisciplinary Team (MDT) coordination, memory mechanisms, and medical tools utilization, leveraging Llama-3.1-8B/70B as the base model. Experimental results show that RareAgents outperforms state-of-the-art domain-specific models, GPT-4o, and current agent frameworks in differential diagnosis and medication recommendation for rare diseases. Furthermore, we contribute a novel rare disease dataset, MIMIC-IV-Ext-Rare, to support further advancements in this field.</article>","contentLength":1471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"THESAURUS: Contrastive Graph Clustering by Swapping Fused Gromov-Wasserstein Couplings","url":"https://arxiv.org/abs/2412.11550","date":1739768400,"author":"","guid":1044,"unread":true,"content":"<article>arXiv:2412.11550v3 Announce Type: replace \nAbstract: Graph node clustering is a fundamental unsupervised task. Existing methods typically train an encoder through selfsupervised learning and then apply K-means to the encoder output. Some methods use this clustering result directly as the final assignment, while others initialize centroids based on this initial clustering and then finetune both the encoder and these learnable centroids. However, due to their reliance on K-means, these methods inherit its drawbacks when the cluster separability of encoder output is low, facing challenges from the Uniform Effect and Cluster Assimilation. We summarize three reasons for the low cluster separability in existing methods: (1) lack of contextual information prevents discrimination between similar nodes from different clusters; (2) training tasks are not sufficiently aligned with the downstream clustering task; (3) the cluster information in the graph structure is not appropriately exploited. To address these issues, we propose conTrastive grapH clustEring by SwApping fUsed gRomov-wasserstein coUplingS (THESAURUS). Our method introduces semantic prototypes to provide contextual information, and employs a cross-view assignment prediction pretext task that aligns well with the downstream clustering task. Additionally, it utilizes Gromov-Wasserstein Optimal Transport (GW-OT) along with the proposed prototype graph to thoroughly exploit cluster information in the graph structure. To adapt to diverse real-world data, THESAURUS updates the prototype graph and the prototype marginal distribution in OT by using momentum. Extensive experiments demonstrate that THESAURUS achieves higher cluster separability than the prior art, effectively mitigating the Uniform Effect and Cluster Assimilation issues</article>","contentLength":1810,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Image Forgery Localization with State Space Models","url":"https://arxiv.org/abs/2412.11214","date":1739768400,"author":"","guid":1045,"unread":true,"content":"<article>arXiv:2412.11214v2 Announce Type: replace \nAbstract: Pixel dependency modeling from tampered images is pivotal for image forgery localization. Current approaches predominantly rely on Convolutional Neural Networks (CNNs) or Transformer-based models, which often either lack sufficient receptive fields or entail significant computational overheads. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, we propose LoMa, a novel image forgery localization method that leverages the selective SSMs. Specifically, LoMa initially employs atrous selective scan to traverse the spatial domain and convert the tampered image into ordered patch sequences, and subsequently applies multi-directional state space modeling. In addition, an auxiliary convolutional branch is introduced to enhance local feature extraction. Extensive experimental results validate the superiority of LoMa over CNN-based and Transformer-based state-of-the-arts. To our best knowledge, this is the first image forgery localization model constructed based on the SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based forgery localization models. Code is available at https://github.com/multimediaFor/LoMa.</article>","contentLength":1432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SEW: Self-calibration Enhanced Whole Slide Pathology Image Analysis","url":"https://arxiv.org/abs/2412.10853","date":1739768400,"author":"","guid":1046,"unread":true,"content":"<article>arXiv:2412.10853v2 Announce Type: replace \nAbstract: Pathology images are considered the ``gold standard\" for cancer diagnosis and treatment, with gigapixel images providing extensive tissue and cellular information. Existing methods fail to simultaneously extract global structural and local detail features for comprehensive pathology image analysis efficiently. To address these limitations, we propose a self-calibration enhanced framework for whole slide pathology image analysis, comprising three components: a global branch, a focus predictor, and a detailed branch. The global branch initially classifies using the pathological thumbnail, while the focus predictor identifies relevant regions for classification based on the last layer features of the global branch. The detailed extraction branch then assesses whether the magnified regions correspond to the lesion area. Finally, a feature consistency constraint between the global and detail branches ensures that the global branch focuses on the appropriate region and extracts sufficient discriminative features for final identification. These focused discriminative features prove invaluable for uncovering novel prognostic tumor markers from the perspective of feature cluster uniqueness and tissue spatial distribution. Extensive experiment results demonstrate that the proposed framework can rapidly deliver accurate and explainable results for pathological grading and prognosis tasks.</article>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SuperMerge: An Approach For Gradient-Based Model Merging","url":"https://arxiv.org/abs/2412.10416","date":1739768400,"author":"","guid":1047,"unread":true,"content":"<article>arXiv:2412.10416v2 Announce Type: replace \nAbstract: Large language models, such as ChatGPT, Claude, or LLaMA, are gigantic, monolithic, and possess the superpower to simultaneously support thousands of tasks. However, high-throughput applications often prefer smaller task-specific models because of their lower latency and cost. One challenge of using task-specific models is the incremental need for solving newer tasks after the model is already deployed for existing tasks. A straightforward solution requires fine-tuning the model again for both existing and new tasks, which is computationally expensive and time-consuming. To address this issue, we propose a model merging based approach called SUPERMERGE. SUPERMERGE is a gradient-based method to systematically merge several fine-tuned models trained on existing and new tasks. SUPERMERGE is designed to be lightweight and fast, and the merged model achieves similar performance to fully fine-tuned models on all tasks. Furthermore, we proposed a hierarchical model merging strategy to reduce the peak space requirement without sacrificing the performance of the merged model. We experimentally demonstrate that SUPERMERGE outperforms existing model merging methods on common natural language processing and computer vision tasks.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\textrm{A}^{\\textrm{2}}$RNet: Adversarial Attack Resilient Network for Robust Infrared and Visible Image Fusion","url":"https://arxiv.org/abs/2412.09954","date":1739768400,"author":"","guid":1048,"unread":true,"content":"<article>arXiv:2412.09954v3 Announce Type: replace \nAbstract: Infrared and visible image fusion (IVIF) is a crucial technique for enhancing visual performance by integrating unique information from different modalities into one fused image. Exiting methods pay more attention to conducting fusion with undisturbed data, while overlooking the impact of deliberate interference on the effectiveness of fusion results. To investigate the robustness of fusion models, in this paper, we propose a novel adversarial attack resilient network, called $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial paradigm with an anti-attack loss function to implement adversarial attacks and training. It is constructed based on the intrinsic nature of IVIF and provide a robust foundation for future research advancements. We adopt a Unet as the pipeline with a transformer-based defensive refinement module (DRM) under this paradigm, which guarantees fused image quality in a robust coarse-to-fine manner. Compared to previous works, our method mitigates the adverse effects of adversarial perturbations, consistently maintaining high-fidelity fusion results. Furthermore, the performance of downstream tasks can also be well maintained under adversarial attacks. Code is available at https://github.com/lok-18/A2RNet.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning","url":"https://arxiv.org/abs/2412.09078","date":1739768400,"author":"","guid":1049,"unread":true,"content":"<article>arXiv:2412.09078v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable abilities across various language tasks, but solving complex reasoning problems remains a significant challenge. While existing methods, such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT), enhance reasoning by decomposing problems or structuring prompts, they typically perform a single pass of reasoning and may fail to revisit flawed paths, compromising accuracy. To address this limitation, we propose a novel reasoning framework called Forest-of-Thought (FoT), which integrates multiple reasoning trees to leverage collective decision-making for solving complex logical problems. FoT employs sparse activation strategies to select the most relevant reasoning paths, improving both efficiency and accuracy. Additionally, we introduce a dynamic self-correction strategy that enables real-time error correction, along with consensus-guided decision-making strategies to optimize both correctness and computational resources. Experimental results demonstrate that the FoT framework, combined with these strategies, significantly enhances the reasoning capabilities of LLMs, enabling them to solve complex tasks with greater precision and efficiency.Code will be available at https://github.com/iamhankai/Forest-of-Thought.</article>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PhishIntel: Toward Practical Deployment of Reference-Based Phishing Detection","url":"https://arxiv.org/abs/2412.09057","date":1739768400,"author":"","guid":1050,"unread":true,"content":"<article>arXiv:2412.09057v2 Announce Type: replace \nAbstract: Phishing is a critical cyber threat, exploiting deceptive tactics to compromise victims and cause significant financial losses. While reference-based phishing detectors (RBPDs) have achieved notable advancements in detection accuracy, their real-world deployment is hindered by challenges such as high latency and inefficiency in URL analysis. To address these limitations, we present PhishIntel, an end-to-end phishing detection system for real-world deployment. PhishIntel intelligently determines whether a URL can be processed immediately or not, segmenting the detection process into two distinct tasks: a fast task that checks against local blacklists and result cache, and a slow task that conducts online blacklist verification, URL crawling, and webpage analysis using an RBPD. This fast-slow task system architecture ensures low response latency while retaining the robust detection capabilities of RBPDs for zero-day phishing threats. Furthermore, we develop two downstream applications based on PhishIntel: a phishing intelligence platform and a phishing email detection plugin for Microsoft Outlook, demonstrating its practical efficacy and utility.</article>","contentLength":1215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What You See Is Not Always What You Get: An Empirical Study of Code Comprehension by Large Language Models","url":"https://arxiv.org/abs/2412.08098","date":1739768400,"author":"","guid":1051,"unread":true,"content":"<article>arXiv:2412.08098v2 Announce Type: replace \nAbstract: Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering tasks, including code generation and comprehension. While LLMs have shown significant potential in assisting with coding, it is perceived that LLMs are vulnerable to adversarial attacks. In this paper, we investigate the vulnerability of LLMs to imperceptible attacks, where hidden character manipulation in source code misleads LLMs' behaviour while remaining undetectable to human reviewers. We devise these attacks into four distinct categories and analyse their impacts on code analysis and comprehension tasks. These four types of imperceptible coding character attacks include coding reordering, invisible coding characters, code deletions, and code homoglyphs. To comprehensively benchmark the robustness of current LLMs solutions against the attacks, we present a systematic experimental evaluation on multiple state-of-the-art LLMs. Our experimental design introduces two key performance metrics, namely model confidence using log probabilities of response, and the response correctness. A set of controlled experiments are conducted using a large-scale perturbed and unperturbed code snippets as the primary prompt input. Our findings confirm the susceptibility of LLMs to imperceptible coding character attacks, while different LLMs present different negative correlations between perturbation magnitude and performance. These results highlight the urgent need for robust LLMs capable of manoeuvring behaviours under imperceptible adversarial conditions. We anticipate this work provides valuable insights for enhancing the security and trustworthiness of LLMs in software engineering applications.</article>","contentLength":1776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parameter optimization for restarted mixed precision iterative sparse solver","url":"https://arxiv.org/abs/2412.08059","date":1739768400,"author":"","guid":1052,"unread":true,"content":"<article>arXiv:2412.08059v3 Announce Type: replace \nAbstract: We consider the problem of optimizing the parameter of a two-stage algorithm for approximate solution of a system of linear algebraic equations with a sparse $n\\times n$-matrix, i.e., with one in which the number of nonzero elements is $m\\!=\\!O(n)$. The two-stage algorithm uses conjugate gradient method at its stages. At the 1st stage, an approximate solution with accuracy $\\varepsilon_1$ is found for zero initial vector. All numerical values used at this stage are represented as single-precision numbers. The obtained solution is used as initial approximation for an approximate solution with a given accuracy $\\varepsilon_2$ that we obtain at the 2nd stage, where double-precision numbers are used. Based on the values of some matrix parameters, computed in a time not exceeding $O(m)$, we need to determine the value $\\varepsilon_1$ which minimizes the total computation time at two stages.\n  Using single-precision numbers for computations at the 1st stage is advantageous, since the execution time of one iteration will be approximately half that of one iteration at the 2nd stage. At the same time, using machine numbers with half the mantissa length accelerates the growth of the rounding error per iteration of the conjugate gradient method at the 1st stage, which entails an increase in the number of iterations performed at 2nd stage.\n  As parameters that allow us to determine $\\varepsilon_1$ for the input matrix, we use $n$, $m$, an estimate of the diameter of the graph associated with the matrix, an estimate of the spread of the matrix' eigenvalues, and estimates of its maximum eigenvalue. The optimal or close to the optimal value of $\\varepsilon_1$ can be determined for matrix with such a vector of parameters using the nearest neighbor regression or some other type of regression.</article>","contentLength":1859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Automated Cross-domain Exploratory Data Analysis through Large Language Models","url":"https://arxiv.org/abs/2412.07214","date":1739768400,"author":"","guid":1053,"unread":true,"content":"<article>arXiv:2412.07214v3 Announce Type: replace \nAbstract: Exploratory data analysis (EDA), coupled with SQL, is essential for data analysts involved in data exploration and analysis. However, data analysts often encounter two primary challenges: (1) the need to craft SQL queries skillfully, and (2) the requirement to generate suitable visualization types that enhance the interpretation of query results. Due to its significance, substantial research efforts have been made to explore different approaches to address these challenges, including leveraging large language models (LLMs). However, existing methods fail to meet real-world data exploration requirements primarily due to (1) complex database schema; (2) unclear user intent; (3) limited cross-domain generalization capability; and (4) insufficient end-to-end text-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain exploratory data analysis system. First, we propose hierarchical data context (i.e., HDC), which leverages LLMs to summarize the contexts related to the database schema, which is crucial for open-world EDA systems to generalize across data domains. Second, the EDA system is divided into four components (i.e., stages): HDC generation, question clarification and decomposition, text-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Finally, we implemented an end-to-end EDA system with a user-friendly GUI interface in the production environment at PingCAP. We have also open-sourced all APIs of TiInsight to facilitate research within the EDA community. Through extensive evaluations by a real-world user study, we demonstrate that TiInsight offers remarkable performance compared to human experts. Specifically, TiSQL achieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It also demonstrates state-of-the-art performance on the Bird dataset.</article>","contentLength":1910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stabilizing and Solving Inverse Problems using Data and Machine Learning","url":"https://arxiv.org/abs/2412.04409","date":1739768400,"author":"","guid":1054,"unread":true,"content":"<article>arXiv:2412.04409v2 Announce Type: replace \nAbstract: We consider an inverse problem involving the reconstruction of the solution to a nonlinear partial differential equation (PDE) with unknown boundary conditions. Instead of direct boundary data, we are provided with a large dataset of boundary observations for typical solutions (collective data) and a bulk measurement of a specific realization. To leverage this collective data, we first compress the boundary data using proper orthogonal decomposition (POD) in a linear expansion. Next, we identify a possible nonlinear low-dimensional structure in the expansion coefficients using an autoencoder, which provides a parametrization of the dataset in a lower-dimensional latent space. We then train an operator network to map the expansion coefficients representing the boundary data to the finite element solution of the PDE. Finally, we connect the autoencoder's decoder to the operator network which enables us to solve the inverse problem by optimizing a data-fitting term over the latent space. We analyze the underlying stabilized finite element method in the linear setting and establish an optimal error estimate in the $H^1$-norm. The nonlinear problem is then studied numerically, demonstrating the effectiveness of our approach.</article>","contentLength":1292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Preference Optimization for Reasoning with Pseudo Feedback","url":"https://arxiv.org/abs/2411.16345","date":1739768400,"author":"","guid":1055,"unread":true,"content":"<article>arXiv:2411.16345v2 Announce Type: replace \nAbstract: Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited. In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated test cases. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case. We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.</article>","contentLength":1399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Next Tokens via Second-to-Last Predictions with Generate and Refine","url":"https://arxiv.org/abs/2411.15661","date":1739768400,"author":"","guid":1056,"unread":true,"content":"<article>arXiv:2411.15661v2 Announce Type: replace \nAbstract: Autoregressive language models like GPT aim to predict next tokens, while autoencoding models such as BERT are trained on tasks such as predicting masked tokens. We train a decoder-only architecture for predicting the second to last token for a sequence of tokens. Our approach yields higher computational training efficiency than BERT-style models by employing a structured deterministic approach to masking tokens. We use our model to improve the next token predictions of a standard GPT by combining both predictions in a ``generate-then-refine'' approach. We demonstrate on different variants of GPT-2 and different datasets that (not unexpectedly) second to last token predictions are much more accurate, i.e., more than 15\\% higher accuracy than standard next token predictions. The ``generate-then-refine'' approach also demonstrates notable improvements in next-token predictions, yielding smaller yet consistent and significant gains.</article>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tulu 3: Pushing Frontiers in Open Language Model Post-Training","url":"https://arxiv.org/abs/2411.15124","date":1739768400,"author":"","guid":1057,"unread":true,"content":"<article>arXiv:2411.15124v4 Announce Type: replace \nAbstract: Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.\n  In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains.</article>","contentLength":1707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","url":"https://arxiv.org/abs/2411.14695","date":1739768400,"author":"","guid":1058,"unread":true,"content":"<article>arXiv:2411.14695v2 Announce Type: replace \nAbstract: Regular unsupervised domain adaptive person re-identification (ReID) focuses on adapting a model from a source domain to a fixed target domain. However, an adapted ReID model can hardly retain previously-acquired knowledge and generalize to unseen data. In this paper, we propose a Dual-level Joint Adaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a model to new domains without forgetting source domain and each adapted target domain. We explore the possibility of using prototype and instance-level consistency to mitigate the forgetting during the adaptation. Specifically, we store a small number of representative image samples and corresponding cluster prototypes in a memory buffer, which is updated at each adaptation step. With the buffered images and prototypes, we regularize the image-to-image similarity and image-to-prototype similarity to rehearse old knowledge. After the multi-step adaptation, the model is tested on all seen domains and several unseen domains to validate the generalization ability of our method. Extensive experiments demonstrate that our proposed method significantly improves the anti-forgetting, generalization and backward-compatible ability of an unsupervised person ReID model.</article>","contentLength":1299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers","url":"https://arxiv.org/abs/2411.12118","date":1739768400,"author":"","guid":1059,"unread":true,"content":"<article>arXiv:2411.12118v2 Announce Type: replace \nAbstract: In this paper, I introduce the retrieval problem, a simple yet common reasoning task that can be solved only by transformers with a minimum number of layers, which grows logarithmically with the input size. I empirically show that large language models can solve the task under different prompting formulations without any fine-tuning. To understand how transformers solve the retrieval problem, I train several transformers on a minimal formulation. Successful learning occurs only under the presence of an implicit curriculum. I uncover the learned mechanisms by studying the attention maps in the trained transformers. I also study the training process, uncovering that attention heads always emerge in a specific sequence guided by the implicit curriculum.</article>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with Semantic-Aware Views for Efficient Visual Representation","url":"https://arxiv.org/abs/2411.09858","date":1739768400,"author":"","guid":1060,"unread":true,"content":"<article>arXiv:2411.09858v2 Announce Type: replace \nAbstract: This paper proposes a scalable and straightforward pre-training paradigm for efficient visual conceptual representation called occluded image contrastive learning (OCL). Our OCL approach is simple: we randomly mask patches to generate different views within an image and contrast them among a mini-batch of images. The core idea behind OCL consists of two designs. First, masked tokens have the potential to significantly diminish the conceptual redundancy inherent in images, and create distinct views with substantial fine-grained differences on the semantic concept level instead of the instance level. Second, contrastive learning is adept at extracting high-level semantic conceptual features during the pre-training, circumventing the high-frequency interference and additional costs associated with image reconstruction. Importantly, OCL learns highly semantic conceptual representations efficiently without relying on hand-crafted data augmentations or additional auxiliary modules. Empirically, OCL demonstrates high scalability with Vision Transformers, as the ViT-L/16 can complete pre-training in 133 hours using only 4 A100 GPUs, achieving 85.8\\% accuracy in downstream fine-tuning tasks. Code is available at https://anonymous.4open.science/r/OLRS/.</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation","url":"https://arxiv.org/abs/2411.08300","date":1739768400,"author":"","guid":1061,"unread":true,"content":"<article>arXiv:2411.08300v4 Announce Type: replace \nAbstract: Achieving low remote memory access latency remains the primary challenge in realizing memory disaggregation over Ethernet within the datacenters. We present EDM that attempts to overcome this challenge using two key ideas. First, while existing network protocols for remote memory access over the Ethernet, such as TCP/IP and RDMA, are implemented on top of the MAC layer, EDM takes a radical approach by implementing the entire network protocol stack for remote memory access within the Physical layer (PHY) of the Ethernet. This overcomes fundamental latency and bandwidth overheads imposed by the MAC layer, especially for small memory messages. Second, EDM implements a centralized, fast, in-network scheduler for memory traffic within the PHY of the Ethernet switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm, the scheduler dynamically reserves bandwidth between compute and memory nodes by creating virtual circuits in the PHY, thus eliminating queuing delay and layer 2 packet processing delay at the switch for memory traffic, while maintaining high bandwidth utilization. Our FPGA testbed demonstrates that EDM's network fabric incurs a latency of only $\\sim$300 ns for remote memory access in an unloaded network, which is an order of magnitude lower than state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to emerging PCIe-based solutions such as CXL. Larger-scale network simulations indicate that even at high network loads, EDM's average latency remains within 1.3$\\times$ its unloaded latency.</article>","contentLength":1608,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks","url":"https://arxiv.org/abs/2411.07902","date":1739768400,"author":"","guid":1062,"unread":true,"content":"<article>arXiv:2411.07902v2 Announce Type: replace \nAbstract: Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by generating an ensemble of predictive distributions. However, inference via ensembling is resource-intensive, requiring additional entropy sources to generate stochasticity which increases resource consumption. We introduce Bayes2IMC, an in-memory computing (IMC) architecture designed for binary Bayesian neural networks that leverage nanoscale device stochasticity to generate desired distributions. Our novel approach utilizes Phase-Change Memory (PCM) to harness inherent noise characteristics, enabling the creation of a binary neural network. This design eliminates the necessity for a pre-neuron Analog-to-Digital Converter (ADC), significantly improving power and area efficiency. We also develop a hardware-software co-optimized correction method applied solely on the logits in the final layer to reduce device-induced accuracy variations across deployments on hardware. Additionally, we devise a simple compensation technique that ensures no drop in classification accuracy despite conductance drift of PCM. We validate the effectiveness of our approach on the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy metrics comparable to ideal software implementations as well as results reported in the literature using other technologies. Finally, we present a complete core architecture and compare its projected power, performance, and area efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6 \\times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6 \\times$ improvement in power efficiency (in GOPS/W). In addition, the projected hardware performance of Bayes2IMC surpasses that of most of the BNN architectures based on memristive devices reported in the literature, and achieves up to $20\\%$ higher power efficiency compared to the state-of-the-art.</article>","contentLength":1950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Maritime Search and Rescue Missions with Aerial Images: A Survey","url":"https://arxiv.org/abs/2411.07649","date":1739768400,"author":"","guid":1063,"unread":true,"content":"<article>arXiv:2411.07649v2 Announce Type: replace \nAbstract: The speed of response by search and rescue teams at sea is of vital importance, as survival may depend on it. Recent technological advancements have led to the development of more efficient systems for locating individuals involved in a maritime incident, such as the use of Unmanned Aerial Vehicles (UAVs) equipped with cameras and other integrated sensors. Over the past decade, several researchers have contributed to the development of automatic systems capable of detecting people using aerial images, particularly by leveraging the advantages of deep learning. In this article, we provide a comprehensive review of the existing literature on this topic. We analyze the methods proposed to date, including both traditional techniques and more advanced approaches based on machine learning and neural networks. Additionally, we take into account the use of synthetic data to cover a wider range of scenarios without the need to deploy a team to collect data, which is one of the major obstacles for these systems. Overall, this paper situates the reader in the field of detecting people at sea using aerial images by quickly identifying the most suitable methodology for each scenario, as well as providing an in-depth discussion and direction for future trends.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains","url":"https://arxiv.org/abs/2411.06426","date":1739768400,"author":"","guid":1064,"unread":true,"content":"<article>arXiv:2411.06426v2 Announce Type: replace \nAbstract: As the integration of the Large Language Models (LLMs) into various applications increases, so does their susceptibility to misuse, raising significant security concerns. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks mainly rely on scenario camouflage, prompt obfuscation, prompt optimization, and prompt iterative optimization to conceal malicious prompts. In particular, sequential prompt chains in a single query can lead LLMs to focus on certain prompts while ignoring others, facilitating context manipulation. This paper introduces SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses. The distinct narrative structures of these scenarios show that SequentialBreak is flexible enough to adapt to various prompt formats beyond those discussed. Extensive experiments demonstrate that SequentialBreak uses only a single query to achieve a substantial gain of attack success rate over existing baselines against both open-source and closed-source models. Through our research, we highlight the urgent need for more robust and resilient safeguards to enhance LLM security and prevent potential misuse. All the result files and website associated with this research are available in this GitHub repository: https://anonymous.4open.science/r/JailBreakAttack-4F3B/.</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Variational Inference on the Boolean Hypercube with the Quantum Entropy","url":"https://arxiv.org/abs/2411.03759","date":1739768400,"author":"","guid":1065,"unread":true,"content":"<article>arXiv:2411.03759v2 Announce Type: replace \nAbstract: In this paper, we derive variational inference upper-bounds on the log-partition function of pairwise Markov random fields on the Boolean hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We then propose an efficient algorithm to compute these bounds based on primal-dual optimization. An improvement of these bounds through the use of ''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and we present a greedy algorithm to select among these relaxations. We carry extensive numerical experiments and compare with state-of-the-art methods for this inference problem.</article>","contentLength":668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit Quality Estimation","url":"https://arxiv.org/abs/2411.00843","date":1739768400,"author":"","guid":1066,"unread":true,"content":"<article>arXiv:2411.00843v2 Announce Type: replace \nAbstract: Logic synthesis is a crucial phase in the circuit design process, responsible for transforming hardware description language (HDL) designs into optimized netlists. However, traditional logic synthesis methods are computationally intensive, restricting their iterative use in refining chip designs. Recent advancements in large language models (LLMs), particularly those fine-tuned on programming languages, present a promising alternative. This work proposes augmenting LLMs with predictor networks trained to estimate circuit quality directly from HDL code. To enhance performance, the model is regularized using embeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT) graphs, thereby incorporating lower-level circuit insights. The proposed method demonstrates superior performance compared to existing graph-based RTL-level estimation techniques on the established benchmark OpenABCD, while providing instant feedback on HDL code quality.</article>","contentLength":1014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Consumer Segmentation and Participation Drivers in Community-Supported Agriculture: A Choice Experiment and PLS-SEM Approach","url":"https://arxiv.org/abs/2411.00010","date":1739768400,"author":"","guid":1067,"unread":true,"content":"<article>arXiv:2411.00010v2 Announce Type: replace \nAbstract: As the global food system faces increasing challenges from sustainability, climate change, and food security issues, alternative food networks like Community-Supported Agriculture (CSA) play an essential role in fostering stronger connections between consumers and producers. However, understanding consumer engagement with CSA is fragmented, particularly in Japan where CSA participation is still emerging. This study aims to identify potential CSA participants in Japan and validate existing theories on CSA participation through a quantitative analysis of 2,484 Japanese consumers. Using choice experiments, Latent Class Analysis, and Partial Least Squares Structural Equation Modeling, we identified five distinct consumer segments. The \"Sustainable Food Seekers\" group showed the highest positive utility for CSA, driven primarily by \"Food Education and Learning Opportunities\" and \"Contribution to Environmental and Social Issues.\" These factors were consistently significant across all segments, suggesting that many Japanese consumers value CSA for its educational and environmental benefits. In contrast, factors related to \"Variety of Ingredients\" were less influential in determining participation intentions. The findings suggest that promoting CSA in Japan may be most effective by emphasizing its role in environmental and social impact, rather than focusing solely on product attributes like organic certification, which is readily available in supermarkets. This reflects a key distinction between CSA adoption in Japan and in other cultural contexts, where access to organic produce is a primary driver. For \"Sustainable Food Seekers,\" CSA offers a way to contribute to broader societal goals rather than just securing organic products.</article>","contentLength":1806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point Clouds","url":"https://arxiv.org/abs/2410.22099","date":1739768400,"author":"","guid":1068,"unread":true,"content":"<article>arXiv:2410.22099v4 Announce Type: replace \nAbstract: Brain imaging studies have demonstrated that diffusion MRI tractography geometric shape descriptors can inform the study of the brain's white matter pathways and their relationship to brain function. In this work, we investigate the possibility of utilizing a deep learning model to compute shape measures of the brain's white matter connections. We introduce a novel framework, TractShapeNet, that leverages a point cloud representation of tractography to compute five shape measures: length, span, volume, total surface area, and irregularity. We assess the performance of the method on a large dataset including 1065 healthy young adults. Experiments for shape measure computation demonstrate that our proposed TractShapeNet outperforms other point cloud-based neural network models in both the Pearson correlation coefficient and normalized error metrics. We compare the inference runtime results with the conventional shape computation tool DSI-Studio. Our results demonstrate that a deep learning approach enables faster and more efficient shape measure computation. We also conduct experiments on two downstream language cognition prediction tasks, showing that shape measures from TractShapeNet perform similarly to those computed by DSI-Studio. Our code will be available at: https://github.com/SlicerDMRI/TractShapeNet.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flaming-hot Initiation with Regular Execution Sampling for Large Language Models","url":"https://arxiv.org/abs/2410.21236","date":1739768400,"author":"","guid":1069,"unread":true,"content":"<article>arXiv:2410.21236v2 Announce Type: replace \nAbstract: Since the release of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities across various domains. A key challenge in developing these general capabilities is efficiently sourcing diverse, high-quality data. This becomes especially critical in reasoning-related tasks with sandbox checkers, such as math or code, where the goal is to generate correct solutions to specific problems with higher probability. In this work, we introduce Flaming-hot Initiation with Regular Execution (FIRE) sampling, a simple yet highly effective method to efficiently find good responses. Our empirical findings show that FIRE sampling enhances inference-time generation quality and also benefits training in the alignment stage. Furthermore, we explore how FIRE sampling improves performance by promoting diversity and analyze the impact of employing FIRE at different positions within a response.</article>","contentLength":958,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strada-LLM: Graph LLM for traffic prediction","url":"https://arxiv.org/abs/2410.20856","date":1739768400,"author":"","guid":1070,"unread":true,"content":"<article>arXiv:2410.20856v2 Announce Type: replace \nAbstract: Traffic prediction is a vital component of intelligent transportation systems. By reasoning about traffic patterns in both the spatial and temporal dimensions, accurate and interpretable predictions can be provided. A considerable challenge in traffic prediction lies in handling the diverse data distributions caused by vastly different traffic conditions occurring at different locations. LLMs have been a dominant solution due to their remarkable capacity to adapt to new datasets with very few labeled data samples, i.e., few-shot adaptability. However, existing forecasting techniques mainly focus on extracting local graph information and forming a text-like prompt, leaving LLM- based traffic prediction an open problem. This work presents a probabilistic LLM for traffic forecasting with three highlights. We propose a graph-aware LLM for traffic prediction that considers proximal traffic information. Specifically, by considering the traffic of neighboring nodes as covariates, our model outperforms the corresponding time-series LLM. Furthermore, we adopt a lightweight approach for efficient domain adaptation when facing new data distributions in few-shot fashion. The comparative experiment demonstrates the proposed method outperforms the state-of-the-art LLM-based methods and the traditional GNN- based supervised approaches. Furthermore, Strada-LLM can be easily adapted to different LLM backbones without a noticeable performance drop.</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update","url":"https://arxiv.org/abs/2410.20299","date":1739768400,"author":"","guid":1071,"unread":true,"content":"<article>arXiv:2410.20299v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated impressive capabilities in language tasks, but they require high computing power and rely on static knowledge. To overcome these limitations, Retrieval-Augmented Generation (RAG) incorporates up-to-date external information into LLMs without extensive fine-tuning. Meanwhile, small language models (SLMs) deployed on edge devices offer efficiency and low latency but often struggle with complex reasoning tasks. Unfortunately, current RAG approaches are predominantly based on centralized databases and have not been adapted to address the distinct constraints associated with deploying SLMs in edge environments. To bridge this gap, we propose Edge-Assisted and Collaborative RAG (EACO-RAG), a lightweight framework that leverages distributed edge nodes for adaptive knowledge updates and retrieval. EACO-RAG also employs a hierarchical collaborative gating mechanism to dynamically select among local, edge-assisted, and cloud-based strategies, with a carefully designed algorithm based on Safe Online Bayesian Optimization to maximize the potential performance enhancements. Experimental results demonstrate that EACO-RAG matches the accuracy of cloud-based knowledge graph RAG systems while reducing total costs by up to 84.6% under relaxed delay constraints and by 65.3% under stricter delay requirements. This work represents our initial effort toward achieving a distributed and scalable tiered LLM deployments, with EACO-RAG serving as a promising first step in unlocking the full potential of hybrid edge-cloud intelligence.</article>","contentLength":1633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FRTree Planner: Robot Navigation in Cluttered and Unknown Environments with Tree of Free Regions","url":"https://arxiv.org/abs/2410.20230","date":1739768400,"author":"","guid":1072,"unread":true,"content":"<article>arXiv:2410.20230v2 Announce Type: replace \nAbstract: In this work, we present FRTree planner, a novel robot navigation framework that leverages a tree structure of free regions, specifically designed for navigation in cluttered and unknown environments with narrow passages. The framework continuously incorporates real-time perceptive information to identify distinct navigation options and dynamically expands the tree toward explorable and traversable directions. This dynamically constructed tree incrementally encodes the geometric and topological information of the collision-free space, enabling efficient selection of the intermediate goals, navigating around dead-end situations, and avoidance of dynamic obstacles without a prior map. Crucially, our method performs a comprehensive analysis of the geometric relationship between free regions and the robot during online replanning. In particular, the planner assesses the accessibility of candidate passages based on the robot's geometries, facilitating the effective selection of the most viable intermediate goals through accessible narrow passages while minimizing unnecessary detours. By combining the free region information with a bi-level trajectory optimization tailored for robots with specific geometries, our approach generates robust and adaptable obstacle avoidance strategies in confined spaces. Through extensive simulations and real-world experiments, FRTree demonstrates its superiority over benchmark methods in generating safe, efficient motion plans through highly cluttered and unknown terrains with narrow gaps.</article>","contentLength":1593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A distributional simplicity bias in the learning dynamics of transformers","url":"https://arxiv.org/abs/2410.19637","date":1739768400,"author":"","guid":1073,"unread":true,"content":"<article>arXiv:2410.19637v2 Announce Type: replace \nAbstract: The remarkable capability of over-parameterised neural networks to generalise effectively has been explained by invoking a ``simplicity bias'': neural networks prevent overfitting by initially learning simple classifiers before progressing to more complex, non-linear functions. While simplicity biases have been described theoretically and experimentally in feed-forward networks for supervised learning, the extent to which they also explain the remarkable success of transformers trained with self-supervised techniques remains unclear. In our study, we demonstrate that transformers, trained on natural language data, also display a simplicity bias. Specifically, they sequentially learn many-body interactions among input tokens, reaching a saturation point in the prediction error for low-degree interactions while continuing to learn high-degree interactions. To conduct this analysis, we develop a procedure to generate \\textit{clones} of a given natural language data set, which rigorously capture the interactions between tokens up to a specified order. This approach opens up the possibilities of studying how interactions of different orders in the data affect learning, in natural language processing and beyond.</article>","contentLength":1278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Overcoming Non-Submodularity: Towards Constant Approximation for Network Immunization","url":"https://arxiv.org/abs/2410.19205","date":1739768400,"author":"","guid":1074,"unread":true,"content":"<article>arXiv:2410.19205v4 Announce Type: replace \nAbstract: Given a network with an ongoing epidemic, the network immunization problem seeks to identify a fixed number of nodes to immunize in order to maximize the number of infections prevented. A fundamental computational challenge in network immunization is that the objective function is generally neither submodular nor supermodular. Consequently, no efficient algorithm is known to consistently achieve a constant-factor approximation. Traditionally, this problem is partially addressed using proxy objectives that offer better approximation properties, but these indirect optimizations often introduce losses in effectiveness due to gaps between the proxy and natural objectives.\n  In this paper, we overcome these fundamental barriers by leveraging the underlying stochastic structure of the diffusion process. Similar to the traditional influence objective, the immunization objective is an expectation expressed as a sum over deterministic instances. However, unlike the former, some of these terms are not submodular. Our key step is to prove that this sum has a bounded deviation from submodularity, enabling the classic greedy algorithm to achieve a constant-factor approximation for any sparse cascading network. We demonstrate that this approximation holds across various immunization settings and spread models.</article>","contentLength":1370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Models, Humans, Predictive Models: Who Is Worse at High-Stakes Decision Making?","url":"https://arxiv.org/abs/2410.15471","date":1739768400,"author":"","guid":1075,"unread":true,"content":"<article>arXiv:2410.15471v2 Announce Type: replace \nAbstract: Despite strong advisory against it, large generative models (LMs) are already being used for decision making tasks that were previously done by predictive models or humans. We put popular LMs to the test in a high-stakes decision making task: recidivism prediction. Studying three closed-access and open-source LMs, we analyze the LMs not exclusively in terms of accuracy, but also in terms of agreement with (imperfect, noisy, and sometimes biased) human predictions or existing predictive models. We conduct experiments that assess how providing different types of information, including distractor information such as photos, can influence LM decisions. We also stress test techniques designed to either increase accuracy or mitigate bias in LMs, and find that some to have unintended consequences on LM decisions. Our results provide additional quantitative evidence to the wisdom that current LMs are not the right tools for these types of tasks.</article>","contentLength":1004,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Context-Aware or Context-Insensitive? Assessing LLMs' Performance in Document-Level Translation","url":"https://arxiv.org/abs/2410.14391","date":1739768400,"author":"","guid":1076,"unread":true,"content":"<article>arXiv:2410.14391v2 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly strong contenders in machine translation. In this work, we focus on document-level translation, where some words cannot be translated without context from outside the sentence. Specifically, we investigate the ability of prominent LLMs to utilize the document context during translation through a perturbation analysis (analyzing models' robustness to perturbed and randomized document context) and an attribution analysis (examining the contribution of relevant context to the translation). We conduct an extensive evaluation across nine LLMs from diverse model families and training paradigms, including translation-specialized LLMs, alongside two encoder-decoder transformer baselines. We find that LLMs' improved document-translation performance compared to encoder-decoder models is not reflected in pronoun translation performance. Our analysis highlight the need for context-aware finetuning of LLMs with a focus on relevant parts of the context to improve their reliability for document-level translation.</article>","contentLength":1112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sliding Puzzles Gym: A Scalable Benchmark for State Representation in Visual Reinforcement Learning","url":"https://arxiv.org/abs/2410.14038","date":1739768400,"author":"","guid":1077,"unread":true,"content":"<article>arXiv:2410.14038v3 Announce Type: replace \nAbstract: Learning effective visual representations enables agents to extract meaningful information from raw sensory inputs, which is essential for generalizing across different tasks. However, evaluating representation learning separately from policy learning remains a challenge with most reinforcement learning (RL) benchmarks. To address this gap, we introduce the Sliding Puzzles Gym (SPGym), a novel benchmark that reimagines the classic 8-tile puzzle with a visual observation space of images sourced from arbitrarily large datasets. SPGym provides precise control over representation complexity through visual diversity, allowing researchers to systematically scale the representation learning challenge while maintaining consistent environment dynamics. Despite the apparent simplicity of the task, our experiments with both model-free and model-based RL algorithms reveal fundamental limitations in current methods. As we increase visual diversity by expanding the pool of possible images, all tested algorithms show significant performance degradation, with even state-of-the-art methods struggling to generalize across different visual inputs while maintaining consistent puzzle-solving capabilities. These results highlight critical gaps in visual representation learning for RL and provide clear directions for improving robustness and generalization in decision-making systems.</article>","contentLength":1436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Kuramoto Oscillatory Neurons","url":"https://arxiv.org/abs/2410.13821","date":1739768400,"author":"","guid":1078,"unread":true,"content":"<article>arXiv:2410.13821v2 Announce Type: replace \nAbstract: It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations. Code: https://github.com/autonomousvision/akorn Project page: https://github.com/takerum/akorn_project_page</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling","url":"https://arxiv.org/abs/2410.13610","date":1739768400,"author":"","guid":1079,"unread":true,"content":"<article>arXiv:2410.13610v2 Announce Type: replace \nAbstract: Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual's health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.</article>","contentLength":1503,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs","url":"https://arxiv.org/abs/2410.13502","date":1739768400,"author":"","guid":1080,"unread":true,"content":"<article>arXiv:2410.13502v3 Announce Type: replace \nAbstract: Large language models (LLMs) can solve arithmetic word problems with high accuracy, but little is known about how well they generalize to more complex problems. This is difficult to study, as (i) much of the available evaluation data has already been seen by the most capable models during training, and (ii) existing benchmarks do not capture how problem proofs may be arbitrarily complex in various ways. In this paper, we present a data-generation framework for evaluating LLMs on problems with arbitrarily complex arithmetic proofs, called MathGAP. MathGAP generates problem statements and chain-of-thought reasoning traces according to specifications about their arithmetic proof structure, enabling systematic studies on easy-to-hard generalization with respect to complexity of proof trees. Using MathGAP, we find that LLMs show a significant decrease in performance as proofs get deeper and wider. This effect is more pronounced in complex, nonlinear proof structures, which are challenging even for the most capable models. The models are also sensitive to simple changes in sentence ordering. However, they remain capable of solving some complex problems, suggesting that reasoning generalization is noisy.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Federated Temporal Graph Clustering","url":"https://arxiv.org/abs/2410.12343","date":1739768400,"author":"","guid":1081,"unread":true,"content":"<article>arXiv:2410.12343v2 Announce Type: replace \nAbstract: Temporal graph clustering is a complex task that involves discovering meaningful structures in dynamic graphs where relationships and entities change over time. Existing methods typically require centralized data collection, which poses significant privacy and communication challenges. In this work, we introduce a novel Federated Temporal Graph Clustering (FTGC) framework that enables decentralized training of graph neural networks (GNNs) across multiple clients, ensuring data privacy throughout the process. Our approach incorporates a temporal aggregation mechanism to effectively capture the evolution of graph structures over time and a federated optimization strategy to collaboratively learn high-quality clustering representations. By preserving data privacy and reducing communication overhead, our framework achieves competitive performance on temporal graph datasets, making it a promising solution for privacy-sensitive, real-world applications involving dynamic data.</article>","contentLength":1037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CATCH: Channel-Aware multivariate Time Series Anomaly Detection via Frequency Patching","url":"https://arxiv.org/abs/2410.12261","date":1739768400,"author":"","guid":1082,"unread":true,"content":"<article>arXiv:2410.12261v2 Announce Type: replace \nAbstract: Anomaly detection in multivariate time series is challenging as heterogeneous subsequence anomalies may occur. Reconstruction-based methods, which focus on learning normal patterns in the frequency domain to detect diverse abnormal subsequences, achieve promising results, while still falling short on capturing fine-grained frequency characteristics and channel correlations. To contend with the limitations, we introduce CATCH, a framework based on frequency patching. We propose to patchify the frequency domain into frequency bands, which enhances its ability to capture fine-grained frequency characteristics. To perceive appropriate channel correlations, we propose a Channel Fusion Module (CFM), which features a patch-wise mask generator and a masked-attention mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM is encouraged to iteratively discover appropriate patch-wise channel correlations, and to cluster relevant channels while isolating adverse effects from irrelevant channels. Extensive experiments on 10 real-world datasets and 12 synthetic datasets demonstrate that CATCH achieves state-of-the-art performance. We make our code and datasets available at https://github.com/decisionintelligence/CATCH.</article>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DR-MPC: Deep Residual Model Predictive Control for Real-world Social Navigation","url":"https://arxiv.org/abs/2410.10646","date":1739768400,"author":"","guid":1083,"unread":true,"content":"<article>arXiv:2410.10646v2 Announce Type: replace \nAbstract: How can a robot safely navigate around people with complex motion patterns? Deep Reinforcement Learning (DRL) in simulation holds some promise, but much prior work relies on simulators that fail to capture the nuances of real human motion. Thus, we propose Deep Residual Model Predictive Control (DR-MPC) to enable robots to quickly and safely perform DRL from real-world crowd navigation data. By blending MPC with model-free DRL, DR-MPC overcomes the DRL challenges of large data requirements and unsafe initial behavior. DR-MPC is initialized with MPC-based path tracking, and gradually learns to interact more effectively with humans. To further accelerate learning, a safety component estimates out-of-distribution states to guide the robot away from likely collisions. In simulation, we show that DR-MPC substantially outperforms prior work, including traditional DRL and residual DRL models. Hardware experiments show our approach successfully enables a robot to navigate a variety of crowded situations with few errors using less than 4 hours of training data.</article>","contentLength":1121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model-Based Privacy-Preserving Knowledge Transfer for Large Language Models","url":"https://arxiv.org/abs/2410.10481","date":1739768400,"author":"","guid":1084,"unread":true,"content":"<article>arXiv:2410.10481v2 Announce Type: replace \nAbstract: As large language models (LLMs) become more prevalent, effectively utilizing domain-specific knowledge while ensuring privacy has become critical. Existing methods often struggle to balance utility and privacy. For instance, retrieval-augmented generation (RAG) enables LLMs to access domain-specific knowledge but compromises the privacy of sensitive data. On the other hand, differentially private data synthesis techniques offer strong privacy guarantees but often result in poor utility. To address this challenge, we propose Llamdex, a novel framework that enhances LLMs using only models trained on domain-specific data, integrated into LLMs through carefully designed connection modules. Our approach significantly enhances the accuracy of domain-specific tasks, achieving up to a 26% accuracy improvement compared to state-of-the-art data synthesis methods under the same differential privacy constraints. Experimental results show that Llamdex not only improves the accuracy of LLM responses but also maintains comparable inference efficiency to the original LLM, highlighting its potential for real applications.</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Approach to Routing and Cascading for LLMs","url":"https://arxiv.org/abs/2410.10347","date":1739768400,"author":"","guid":1085,"unread":true,"content":"<article>arXiv:2410.10347v2 Announce Type: replace \nAbstract: The availability of a wide range of large language models (LLMs) embedded in various agentic systems has significantly increased the potential of model selection strategies to improve the cost-performance tradeoff. Existing strategies involve either routing, where a single model is chosen per query, or cascading, which sequentially runs increasingly larger models until a satisfactory answer is found. However, current approaches face three key limitations: they (1) lack formal proofs of optimality, (2) fail to identify the conditions under which these strategies are most effective to improve the cost-performance tradeoff, and (3) are unable to combine both paradigms for further improvements. To address these issues, we first derive a novel optimal strategy for cascading and prove the optimality of an existing routing strategy. Further, we propose cascade routing, a unified framework that integrates routing and cascading into a theoretically optimal strategy. Through our analysis, we identify good quality estimators as the critical factor for the success of model selection paradigms. Finally, in our experiments, we show that cascade routing consistently outperforms the individual approaches by a large margin and we analyze quality estimators to determine when routing and/or cascading are useful paradigms for model selection.</article>","contentLength":1397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning","url":"https://arxiv.org/abs/2410.09536","date":1739768400,"author":"","guid":1086,"unread":true,"content":"<article>arXiv:2410.09536v3 Announce Type: replace \nAbstract: This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TinyEmo: Scaling down Emotional Reasoning via Metric Projection","url":"https://arxiv.org/abs/2410.07062","date":1739768400,"author":"","guid":1087,"unread":true,"content":"<article>arXiv:2410.07062v4 Announce Type: replace \nAbstract: This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems. We release code, models, and dataset at https://github.com/ggcr/TinyEmo</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning","url":"https://arxiv.org/abs/2410.05894","date":1739768400,"author":"","guid":1088,"unread":true,"content":"<article>arXiv:2410.05894v2 Announce Type: replace \nAbstract: In the realm of computational physics, an enduring topic is the numerical solutions to partial differential equations (PDEs). Recently, the attention of researchers has shifted towards Neural Operator methods, renowned for their capability to approximate ``operators'' -- mappings from functions to functions. Despite the universal approximation theorem within neural operators, ensuring error bounds often requires employing numerous Fourier layers. However, what about lightweight models? In response to this question, we introduce DimOL (Dimension-aware Operator Learning), drawing insights from dimensional analysis. To implement DimOL, we propose the ProdLayer, which can be seamlessly integrated into FNO-based and Transformer-based PDE solvers, enhancing their ability to handle sum-of-products structures inherent in many physical systems. Empirically, DimOL models achieve up to 48% performance gain within the PDE datasets. Furthermore, by analyzing Fourier components' weights, we can symbolically discern the physical significance of each term. This sheds light on the opaque nature of neural networks, unveiling underlying physical principles.</article>","contentLength":1209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?","url":"https://arxiv.org/abs/2410.05584","date":1739768400,"author":"","guid":1089,"unread":true,"content":"<article>arXiv:2410.05584v5 Announce Type: replace \nAbstract: Reward Models (RMs) are crucial for aligning language models with human preferences. Currently, the evaluation of RMs depends on measuring accuracy against a validation set of manually annotated preference data. Although this method is straightforward and widely adopted, the relationship between RM accuracy and downstream policy performance remains under-explored. In this work, we conduct experiments in a synthetic setting to investigate how differences in RM measured by accuracy translate into gaps in optimized policy performance. Our findings reveal that while there is a weak positive correlation between accuracy and downstream performance, policies optimized towards RMs with similar accuracy can exhibit quite different performance. Moreover, we discover that the way of measuring accuracy significantly impacts its ability to predict the final policy performance. Through the lens of the Regressional Goodhart effect, we recognize that accuracy, when used for measuring RM quality, can fail to fully capture the potential RM overoptimization. This underscores the inadequacy of relying solely on accuracy to reflect their impact on policy optimization.</article>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models","url":"https://arxiv.org/abs/2410.04634","date":1739768400,"author":"","guid":1090,"unread":true,"content":"<article>arXiv:2410.04634v2 Announce Type: replace \nAbstract: Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose Concept2Concept, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. A demo is available at https://tinyurl.com/Concept2ConceptDemo.</article>","contentLength":1073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A scalable, gradient-stable approach to multi-step, nonlinear system identification using first-order methods","url":"https://arxiv.org/abs/2410.03544","date":1739768400,"author":"","guid":1091,"unread":true,"content":"<article>arXiv:2410.03544v2 Announce Type: replace \nAbstract: This paper presents three main contributions to the field of multi-step system identification. First, drawing inspiration from Neural Network (NN) training, it introduces a tool for solving identification problems by leveraging first-order optimization and Automatic Differentiation (AD). The proposed method exploits gradients with respect to the parameters to be identified and leverages Linear Parameter-Varying (LPV) sensitivity equations to model gradient evolution. Second, it demonstrates that the computational complexity of the proposed method is linear in both the multi-step horizon length and the parameter size, ensuring scalability for large identification problems. Third, it formally addresses the \"exploding gradient\" issue: via a stability analysis of the LPV equations, it derives conditions for a reliable and efficient optimization and identification process for dynamical systems. Simulation results indicate that the proposed method is both effective and efficient, making it a promising tool for future research and applications in nonlinear system identification and non-convex optimization.</article>","contentLength":1169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spatial-aware decision-making with ring attractors in reinforcement learning systems","url":"https://arxiv.org/abs/2410.03119","date":1739768400,"author":"","guid":1092,"unread":true,"content":"<article>arXiv:2410.03119v2 Announce Type: replace \nAbstract: This paper explores the integration of ring attractors, a mathematical model inspired by neural circuit dynamics, into the Reinforcement Learning (RL) action selection process. Serving as specialized brain-inspired structures that encode spatial information and uncertainty, ring attractors offer a biologically plausible mechanism to improve learning speed and accuracy in RL. They do so by explicitly encoding the action space, facilitating the organization of neural activity, and enabling the distribution of spatial representations across the neural network in the context of Deep Reinforcement Learning (DRL). For example, preserving the continuity between rotation angles in robotic control or adjacency between tactical moves in game-like environments. The application of ring attractors in the action selection process involves mapping actions to specific locations on the ring and decoding the selected action based on neural activity. We investigate the application of ring attractors by both building an exogenous model and integrating them as part of DRL agents. Our approach significantly improves state-of-the-art performance on the Atari 100k benchmark, achieving a 53\\% increase in performance across selected state-of-the-art baselines. Codebase available at https://anonymous.4open.science/r/RA_RL-8026.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DomainLynx: Leveraging Large Language Models for Enhanced Domain Squatting Detection","url":"https://arxiv.org/abs/2410.02095","date":1739768400,"author":"","guid":1093,"unread":true,"content":"<article>arXiv:2410.02095v3 Announce Type: replace \nAbstract: Domain squatting poses a significant threat to Internet security, with attackers employing increasingly sophisticated techniques. This study introduces DomainLynx, an innovative compound AI system leveraging Large Language Models (LLMs) for enhanced domain squatting detection. Unlike existing methods focusing on predefined patterns for top-ranked domains, DomainLynx excels in identifying novel squatting techniques and protecting less prominent brands. The system's architecture integrates advanced data processing, intelligent domain pairing, and LLM-powered threat assessment. Crucially, DomainLynx incorporates specialized components that mitigate LLM hallucinations, ensuring reliable and context-aware detection. This approach enables efficient analysis of vast security data from diverse sources, including Certificate Transparency logs, Passive DNS records, and zone files. Evaluated on a curated dataset of 1,649 squatting domains, DomainLynx achieved 94.7\\% accuracy using Llama-3-70B. In a month-long real-world test, it detected 34,359 squatting domains from 2.09 million new domains, outperforming baseline methods by 2.5 times. This research advances Internet security by providing a versatile, accurate, and adaptable tool for combating evolving domain squatting threats. DomainLynx's approach paves the way for more robust, AI-driven cybersecurity solutions, enhancing protection for a broader range of online entities and contributing to a safer digital ecosystem.</article>","contentLength":1536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conformal Prediction Sets Can Cause Disparate Impact","url":"https://arxiv.org/abs/2410.01888","date":1739768400,"author":"","guid":1094,"unread":true,"content":"<article>arXiv:2410.01888v2 Announce Type: replace \nAbstract: Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact.</article>","contentLength":1063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lines of Thought in Large Language Models","url":"https://arxiv.org/abs/2410.01545","date":1739768400,"author":"","guid":1095,"unread":true,"content":"<article>arXiv:2410.01545v3 Announce Type: replace \nAbstract: Large Language Models achieve next-token prediction by transporting a vectorized piece of text (prompt) across an accompanying embedding space under the action of successive transformer layers. The resulting high-dimensional trajectories realize different contextualization, or 'thinking', steps, and fully determine the output probability distribution. We aim to characterize the statistical properties of ensembles of these 'lines of thought.' We observe that independent trajectories cluster along a low-dimensional, non-Euclidean manifold, and that their path can be well approximated by a stochastic equation with few parameters extracted from data. We find it remarkable that the vast complexity of such large models can be reduced to a much simpler form, and we reflect on implications.</article>","contentLength":846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Stochastic Dynamics from Snapshots through Regularized Unbalanced Optimal Transport","url":"https://arxiv.org/abs/2410.00844","date":1739768400,"author":"","guid":1096,"unread":true,"content":"<article>arXiv:2410.00844v2 Announce Type: replace \nAbstract: Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data. Theoretically, we explore the connections between the RUOT and Schr\\\"odinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: https://github.com/zhenyiizhang/DeepRUOT.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models","url":"https://arxiv.org/abs/2409.19471","date":1739768400,"author":"","guid":1097,"unread":true,"content":"<article>arXiv:2409.19471v2 Announce Type: replace \nAbstract: Despite significant advancements in large language models (LLMs) that enhance robot agents' understanding and execution of natural language (NL) commands, ensuring the agents adhere to user-specified constraints remains challenging, particularly for complex commands and long-horizon tasks. To address this challenge, we present three key insights, equivalence voting, constrained decoding, and domain-specific fine-tuning, which significantly enhance LLM planners' capability in handling complex tasks. Equivalence voting ensures consistency by generating and sampling multiple Linear Temporal Logic (LTL) formulas from NL commands, grouping equivalent LTL formulas, and selecting the majority group of formulas as the final LTL formula. Constrained decoding then uses the generated LTL formula to enforce the autoregressive inference of plans, ensuring the generated plans conform to the LTL. Domain-specific fine-tuning customizes LLMs to produce safe and efficient plans within specific task domains. Our approach, Safe Efficient LLM Planner (SELP), combines these insights to create LLM planners to generate plans adhering to user commands with high confidence. We demonstrate the effectiveness and generalizability of SELP across different robot agents and tasks, including drone navigation and robot manipulation. For drone navigation tasks, SELP outperforms state-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks conforming to NL commands) and by 19.8% in plan efficiency. For robot manipulation tasks, SELP achieves 20.4% improvement in safety rate. Our datasets for evaluating NL-to-LTL and robot task planning will be released in github.com/lt-asset/selp.</article>","contentLength":1737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Strategy Representation for Imitation Learning in Multi-Agent Games","url":"https://arxiv.org/abs/2409.19363","date":1739768400,"author":"","guid":1098,"unread":true,"content":"<article>arXiv:2409.19363v2 Announce Type: replace \nAbstract: The offline datasets for imitation learning (IL) in multi-agent games typically contain player trajectories exhibiting diverse strategies, which necessitate measures to prevent learning algorithms from acquiring undesirable behaviors. Learning representations for these trajectories is an effective approach to depicting the strategies employed by each demonstrator. However, existing learning strategies often require player identification or rely on strong assumptions, which are not appropriate for multi-agent games. Therefore, in this paper, we introduce the Strategy Representation for Imitation Learning (STRIL) framework, which (1) effectively learns strategy representations in multi-agent games, (2) estimates proposed indicators based on these representations, and (3) filters out sub-optimal data using the indicators. STRIL is a plug-in method that can be integrated into existing IL algorithms. We demonstrate the effectiveness of STRIL across competitive multi-agent scenarios, including Two-player Pong, Limit Texas Hold'em, and Connect Four. Our approach successfully acquires strategy representations and indicators, thereby identifying dominant trajectories and significantly enhancing existing IL performance across these environments.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent","url":"https://arxiv.org/abs/2409.18997","date":1739768400,"author":"","guid":1099,"unread":true,"content":"<article>arXiv:2409.18997v2 Announce Type: replace \nAbstract: Propaganda plays a critical role in shaping public opinion and fueling disinformation. While existing research primarily focuses on identifying propaganda techniques, it lacks the ability to capture the broader motives and the impacts of such content. To address these challenges, we introduce propainsight, a conceptual framework grounded in foundational social science research, which systematically dissects propaganda into techniques, arousal appeals, and underlying intent. propainsight offers a more granular understanding of how propaganda operates across different contexts. Additionally, we present propagaze, a novel dataset that combines human-annotated data with high-quality synthetic data generated through a meticulously designed pipeline. Our experiments show that off-the-shelf LLMs struggle with propaganda analysis, but training with propagaze significantly improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span IoU in technique identification and 66.2% higher BertScore in appeal analysis compared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited human-annotated data in data-sparse and cross-domain scenarios, showing its potential for comprehensive and generalizable propaganda analysis.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base","url":"https://arxiv.org/abs/2409.18472","date":1739768400,"author":"","guid":1100,"unread":true,"content":"<article>arXiv:2409.18472v3 Announce Type: replace \nAbstract: URIEL is a knowledge base offering geographical, phylogenetic, and typological vector representations for 7970 languages. It includes distance measures between these vectors for 4005 languages, which are accessible via the lang2vec tool. Despite being frequently cited, URIEL is limited in terms of linguistic inclusion and overall usability. To tackle these challenges, we introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses these limitations. In addition to expanding typological feature coverage for 2898 languages, URIEL+ improves the user experience with robust, customizable distance calculations to better suit the needs of users. These upgrades also offer competitive performance on downstream tasks and provide distances that better align with linguistic distance studies.</article>","contentLength":857,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale","url":"https://arxiv.org/abs/2409.17115","date":1739768400,"author":"","guid":1101,"unread":true,"content":"<article>arXiv:2409.17115v2 Announce Type: replace \nAbstract: Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb, FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training. We are open-sourcing ProX with &gt;500B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX</article>","contentLength":1951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling","url":"https://arxiv.org/abs/2409.15156","date":1739768400,"author":"","guid":1102,"unread":true,"content":"<article>arXiv:2409.15156v2 Announce Type: replace \nAbstract: The remarkable success of large language pretraining and the discovery of scaling laws signify a paradigm shift in machine learning. Notably, the primary objective has evolved from minimizing generalization error to reducing approximation error, and the most effective strategy has transitioned from regularization (in a broad sense) to scaling up models. This raises a critical question:\n  Do the established principles that proved successful in the generalization-centric era remain valid in this new era of scaling?\n  This paper examines several influential regularization-based principles that may no longer hold true in the scaling-centric, large language model (LLM) era. These principles include explicit L2 regularization and implicit regularization through small batch sizes and large learning rates. Additionally, we identify a new phenomenon termed ``scaling law crossover,'' where two scaling curves intersect at a certain scale, implying that methods effective at smaller scales may not generalize to larger ones. Together, these observations highlight two fundamental questions within this new paradigm:\n  $\\bullet$ Guiding Principles for Scaling: If regularization is no longer the primary guiding principle for model design, what new principles are emerging to guide scaling?\n  $\\bullet$ Model Comparison at Scale: How to reliably and effectively compare models at the scale where only a single experiment is feasible?</article>","contentLength":1487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LATTE: Improving Latex Recognition for Tables and Formulae with Iterative Refinement","url":"https://arxiv.org/abs/2409.14201","date":1739768400,"author":"","guid":1103,"unread":true,"content":"<article>arXiv:2409.14201v2 Announce Type: replace \nAbstract: Portable Document Format (PDF) files are dominantly used for storing and disseminating scientific research, legal documents, and tax information. LaTeX is a popular application for creating PDF documents. Despite its advantages, LaTeX is not WYSWYG -- what you see is what you get, i.e., the LaTeX source and rendered PDF images look drastically different, especially for formulae and tables. This gap makes it hard to modify or export LaTeX sources for formulae and tables from PDF images, and existing work is still limited. First, prior work generates LaTeX sources in a single iteration and struggles with complex LaTeX formulae. Second, existing work mainly recognizes and extracts LaTeX sources for formulae; and is incapable or ineffective for tables. This paper proposes LATTE, the first iterative refinement framework for LaTeX recognition. Specifically, we propose delta-view as feedback, which compares and pinpoints the differences between a pair of rendered images of the extracted LaTeX source and the expected correct image. Such delta-view feedback enables our fault localization model to localize the faulty parts of the incorrect recognition more accurately and enables our LaTeX refinement model to repair the incorrect extraction more accurately. LATTE improves the LaTeX source extraction accuracy of both LaTeX formulae and tables, outperforming existing techniques as well as GPT-4V by at least 7.03% of exact match, with a success refinement rate of 46.08% (formula) and 25.51% (table).</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Representations and Interventions in Time Series Foundation Models","url":"https://arxiv.org/abs/2409.12915","date":1739768400,"author":"","guid":1104,"unread":true,"content":"<article>arXiv:2409.12915v3 Announce Type: replace \nAbstract: Time series foundation models (TSFMs) promise to be powerful tools for a wide range of applications. However, their internal representations and learned concepts are still not well understood. In this study, we investigate the structure and redundancy of representations across various TSFMs, examining the self-similarity of model layers within and across different model sizes. This analysis reveals block-like redundancy in the representations, which can be utilized for informed pruning to improve inference speed and efficiency. Additionally, we explore the concepts learned by these models - such as periodicity and trends - and how these can be manipulated through latent space steering to influence model behavior. Our experiments show that steering interventions can introduce new features, e.g., adding periodicity or trends to signals that initially lacked them. These findings underscore the value of representational analysis for optimizing models and demonstrate how conceptual steering offers new possibilities for more controlled and efficient time series analysis with TSFMs.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference","url":"https://arxiv.org/abs/2409.12467","date":1739768400,"author":"","guid":1105,"unread":true,"content":"<article>arXiv:2409.12467v2 Announce Type: replace \nAbstract: Surgical phase recognition is critical for assisting surgeons in understanding surgical videos. Existing studies focused more on online surgical phase recognition, by leveraging preceding frames to predict the current frame. Despite great progress, they formulated the task as a series of frame-wise classification, which resulted in a lack of global context of the entire procedure and incoherent predictions. Moreover, besides online analysis, accurate offline surgical phase recognition is also in significant clinical need for retrospective analysis, and existing online algorithms do not fully analyze the entire video, thereby limiting accuracy in offline analysis. To overcome these challenges and enhance both online and offline inference capabilities, we propose a universal Surgical Phase Localization Network, named SurgPLAN++, with the principle of temporal detection. To ensure a global understanding of the surgical procedure, we devise a phase localization strategy for SurgPLAN++ to predict phase segments across the entire video through phase proposals. For online analysis, to generate high-quality phase proposals, SurgPLAN++ incorporates a data augmentation strategy to extend the streaming video into a pseudo-complete video through mirroring, center-duplication, and down-sampling. For offline analysis, SurgPLAN++ capitalizes on its global phase prediction framework to continuously refine preceding predictions during each online inference step, thereby significantly improving the accuracy of phase recognition. We perform extensive experiments to validate the effectiveness, and our SurgPLAN++ achieves remarkable performance in both online and offline modes, which outperforms state-of-the-art methods. The source code is available at https://github.com/franciszchen/SurgPLAN-Plus.</article>","contentLength":1861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Overcoming Ambient Drift and Negative-Bias Temperature Instability in Foundry Carbon Nanotube Transistors","url":"https://arxiv.org/abs/2409.11297","date":1739768400,"author":"","guid":1106,"unread":true,"content":"<article>arXiv:2409.11297v3 Announce Type: replace \nAbstract: Back-end-of-line (BEOL) logic integration is emerging as a complementary scaling path to supplement front-end-of-line (FEOL) Silicon. Among various options for BEOL logic, Carbon Nanotube Field-Effect Transistors (CNFETs) have been integrated within commercial silicon foundries, and complex CNFET circuits (e.g., RISC-V core, SRAM arrays) have been demonstrated. However, there lacks comprehensive studies that analyze the ambient drift (i.e., air-stability) and reliability of CNFETs. Here, for the first time, we thoroughly characterize and demonstrate how to overcome ambient drift and negative bias temperature instability (NBTI) in CNFETs using the following techniques: (1) Silicon Nitride encapsulation to limit ambient atmosphere induced threshold voltage shift (~8x reduction of median VT shift over 90 days) and (2) AC/pulsed operation to significantly improve CNFET NBTI vs. DC operation across a wide frequency range (e.g., 20% duty cycle AC operation at 10 MHz could extend CNFET NBTI time-to-failure by &gt;10000x vs. DC for a target VT shift tolerance &lt; 100 mV with gate stress bias VGS,stress = -1.2 V at 125 C).</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Steinmetz Neural Networks for Complex-Valued Data","url":"https://arxiv.org/abs/2409.10075","date":1739768400,"author":"","guid":1107,"unread":true,"content":"<article>arXiv:2409.10075v3 Announce Type: replace \nAbstract: We introduce a new approach to processing complex-valued data using DNNs consisting of parallel real-valued subnetworks with coupled outputs. Our proposed class of architectures, referred to as Steinmetz Neural Networks, incorporates multi-view learning to construct more interpretable representations in the latent space. Moreover, we present the Analytic Neural Network, which incorporates a consistency penalty that encourages analytic signal representations in the latent space of the Steinmetz neural network. This penalty enforces a deterministic and orthogonal relationship between the real and imaginary components. Using an information-theoretic construction, we demonstrate that the generalization gap upper bound posited by the analytic neural network is lower than that of the general class of Steinmetz neural networks. Our numerical experiments depict the improved performance and robustness to additive noise, afforded by our proposed networks on benchmark datasets and synthetic examples.</article>","contentLength":1057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AACessTalk: Fostering Communication between Minimally Verbal Autistic Children and Parents with Contextual Guidance and Card Recommendation","url":"https://arxiv.org/abs/2409.09641","date":1739768400,"author":"","guid":1108,"unread":true,"content":"<article>arXiv:2409.09641v4 Announce Type: replace \nAbstract: As minimally verbal autistic (MVA) children communicate with parents through few words and nonverbal cues, parents often struggle to encourage their children to express subtle emotions and needs and to grasp their nuanced signals. We present AACessTalk, a tablet-based, AI-mediated communication system that facilitates meaningful exchanges between an MVA child and a parent. AACessTalk provides real-time guides to the parent to engage the child in conversation and, in turn, recommends contextual vocabulary cards to the child. Through a two-week deployment study with 11 MVA child-parent dyads, we examine how AACessTalk fosters everyday conversation practice and mutual engagement. Our findings show high engagement from all dyads, leading to increased frequency of conversation and turn-taking. AACessTalk also encouraged parents to explore their own interaction strategies and empowered the children to have more agency in communication. We discuss the implications of designing technologies for balanced communication dynamics in parent-MVA child interaction.</article>","contentLength":1119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent","url":"https://arxiv.org/abs/2409.08770","date":1739768400,"author":"","guid":1109,"unread":true,"content":"<article>arXiv:2409.08770v4 Announce Type: replace \nAbstract: The performance of mini-batch stochastic gradient descent (SGD) strongly depends on setting the batch size and learning rate to minimize the empirical loss in training the deep neural network. In this paper, we present theoretical analyses of mini-batch SGD with four schedulers: (i) constant batch size and decaying learning rate scheduler, (ii) increasing batch size and decaying learning rate scheduler, (iii) increasing batch size and increasing learning rate scheduler, and (iv) increasing batch size and warm-up decaying learning rate scheduler. We show that mini-batch SGD using scheduler (i) does not always minimize the expectation of the full gradient norm of the empirical loss, whereas it does using any of schedulers (ii), (iii), and (iv). Furthermore, schedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides numerical results of supporting analyses showing that using scheduler (iii) or (iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or (ii).</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contrastive Federated Learning with Tabular Data Silos","url":"https://arxiv.org/abs/2409.06123","date":1739768400,"author":"","guid":1110,"unread":true,"content":"<article>arXiv:2409.06123v2 Announce Type: replace \nAbstract: Learning from vertical partitioned data silos is challenging due to the segmented nature of data, sample misalignment, and strict privacy concerns. Federated learning has been proposed as a solution. However, sample misalignment across silos often hinders optimal model performance and suggests data sharing within the model, which breaks privacy. Our proposed solution is Contrastive Federated Learning with Tabular Data Silos (CFL), which offers a solution for data silos with sample misalignment without the need for sharing original or representative data to maintain privacy. CFL begins with local acquisition of contrastive representations of the data within each silo and aggregates knowledge from other silos through the federated learning algorithm. Our experiments demonstrate that CFL solves the limitations of existing algorithms for data silos and outperforms existing tabular contrastive learning. CFL provides performance improvements without loosening privacy.</article>","contentLength":1029,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HelmetPoser: A Helmet-Mounted IMU Dataset for Data-Driven Estimation of Human Head Motion in Diverse Conditions","url":"https://arxiv.org/abs/2409.05006","date":1739768400,"author":"","guid":1111,"unread":true,"content":"<article>arXiv:2409.05006v2 Announce Type: replace \nAbstract: Helmet-mounted wearable positioning systems are crucial for enhancing safety and facilitating coordination in industrial, construction, and emergency rescue environments. These systems, including LiDAR-Inertial Odometry (LIO) and Visual-Inertial Odometry (VIO), often face challenges in localization due to adverse environmental conditions such as dust, smoke, and limited visual features. To address these limitations, we propose a novel head-mounted Inertial Measurement Unit (IMU) dataset with ground truth, aimed at advancing data-driven IMU pose estimation. Our dataset captures human head motion patterns using a helmet-mounted system, with data from ten participants performing various activities. We explore the application of neural networks, specifically Long Short-Term Memory (LSTM) and Transformer networks, to correct IMU biases and improve localization accuracy. Additionally, we evaluate the performance of these methods across different IMU data window dimensions, motion patterns, and sensor types. We release a publicly available dataset, demonstrate the feasibility of advanced neural network approaches for helmet-based localization, and provide evaluation metrics to establish a baseline for future studies in this field. Data and code can be found at https://lqiutong.github.io/HelmetPoser.github.io/.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection","url":"https://arxiv.org/abs/2409.04796","date":1739768400,"author":"","guid":1112,"unread":true,"content":"<article>arXiv:2409.04796v2 Announce Type: replace \nAbstract: Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods. Code is released at https://github.com/AuroraZengfh/Local-Prompt.</article>","contentLength":1670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Willingness to Read AI-Generated News Is Not Driven by Their Perceived Quality","url":"https://arxiv.org/abs/2409.03500","date":1739768400,"author":"","guid":1113,"unread":true,"content":"<article>arXiv:2409.03500v3 Announce Type: replace \nAbstract: The advancement of artificial intelligence has led to its application in many areas, including news media, which makes it crucial to understand public reception of AI-generated news. This preregistered study investigates (i) the perceived quality of AI-assisted and AI-generated versus human-generated news articles, (ii) whether disclosure of AI's involvement in generating these news articles influences engagement with them, and (iii) whether such awareness affects the willingness to read AI-generated articles in the future. We conducted a survey experiment with 599 Swiss participants, who evaluated the credibility, readability, and expertise of news articles either written by journalists (control group), rewritten by AI (AI-assisted group), or entirely written by AI (AI-generated group). Our results indicate that all articles were perceived to be of equal quality. When participants in the treatment groups were subsequently made aware of AI's role, they expressed a higher willingness to continue reading the articles than participants in the control group. However, they were not more willing to read AI-generated news in the future. These results suggest that aversion to AI usage in news media is not primarily rooted in a perceived lack of quality, and that by disclosing using AI, journalists could induce more short-term engagement.</article>","contentLength":1404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review","url":"https://arxiv.org/abs/2409.02375","date":1739768400,"author":"","guid":1114,"unread":true,"content":"<article>arXiv:2409.02375v4 Announce Type: replace \nAbstract: The recent advances in large language models (LLMs) have significantly expanded their applications across various fields such as language generation, summarization, and complex question answering. However, their application to privacy compliance and technical privacy reviews remains under-explored, raising critical concerns about their ability to adhere to global privacy standards and protect sensitive user data. This paper seeks to address this gap by providing a comprehensive case study evaluating LLMs' performance in privacy-related tasks such as privacy information extraction (PIE), legal and regulatory key point detection (KPD), and question answering (QA) with respect to privacy policies and data protection regulations. We introduce a Privacy Technical Review (PTR) framework, highlighting its role in mitigating privacy risks during the software development life-cycle. Through an empirical assessment, we investigate the capacity of several prominent LLMs, including BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks and technical privacy reviews. Our experiments benchmark the models across multiple dimensions, focusing on their precision, recall, and F1-scores in extracting privacy-sensitive information and detecting key regulatory compliance points. While LLMs show promise in automating privacy reviews and identifying regulatory discrepancies, significant gaps persist in their ability to fully comply with evolving legal standards. We provide actionable recommendations for enhancing LLMs' capabilities in privacy compliance, emphasizing the need for robust model improvements and better integration with legal and regulatory requirements. This study underscores the growing importance of developing privacy-aware LLMs that can both support businesses in compliance efforts and safeguard user privacy rights.</article>","contentLength":1918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Large Language Models Possess Sensitive to Sentiment?","url":"https://arxiv.org/abs/2409.02370","date":1739768400,"author":"","guid":1115,"unread":true,"content":"<article>arXiv:2409.02370v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) have recently displayed their extraordinary capabilities in language understanding. However, how to comprehensively assess the sentiment capabilities of LLMs continues to be a challenge. This paper investigates the ability of LLMs to detect and react to sentiment in text modal. As the integration of LLMs into diverse applications is on the rise, it becomes highly critical to comprehend their sensitivity to emotional tone, as it can influence the user experience and the efficacy of sentiment-driven tasks. We conduct a series of experiments to evaluate the performance of several prominent LLMs in identifying and responding appropriately to sentiments like positive, negative, and neutral emotions. The models' outputs are analyzed across various sentiment benchmarks, and their responses are compared with human evaluations. Our discoveries indicate that although LLMs show a basic sensitivity to sentiment, there are substantial variations in their accuracy and consistency, emphasizing the requirement for further enhancements in their training processes to better capture subtle emotional cues. Take an example in our findings, in some cases, the models might wrongly classify a strongly positive sentiment as neutral, or fail to recognize sarcasm or irony in the text. Such misclassifications highlight the complexity of sentiment analysis and the areas where the models need to be refined. Another aspect is that different LLMs might perform differently on the same set of data, depending on their architecture and training datasets. This variance calls for a more in-depth study of the factors that contribute to the performance differences and how they can be optimized.</article>","contentLength":1765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey","url":"https://arxiv.org/abs/2409.01980","date":1739768400,"author":"","guid":1116,"unread":true,"content":"<article>arXiv:2409.01980v3 Announce Type: replace \nAbstract: Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers.</article>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Cross-Lingual Explanation of Artwork in Large-scale Vision Language Models","url":"https://arxiv.org/abs/2409.01584","date":1739768400,"author":"","guid":1117,"unread":true,"content":"<article>arXiv:2409.01584v2 Announce Type: replace \nAbstract: As the performance of Large-scale Vision Language Models (LVLMs) improves, they are increasingly capable of responding in multiple languages, and there is an expectation that the demand for explanations generated by LVLMs will grow. However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English. In addition, multilingual QA benchmarks that create datasets using machine translation have cultural differences and biases, remaining issues for use as evaluation tasks. To address these challenges, this study created an extended dataset in multiple languages without relying on machine translation. This dataset that takes into account nuances and country-specific phrases was then used to evaluate the generation explanation abilities of LVLMs. Furthermore, this study examined whether Instruction-Tuning in resource-rich English improves performance in other languages. Our findings indicate that LVLMs perform worse in languages other than English compared to English. In addition, it was observed that LVLMs struggle to effectively manage the knowledge learned from English data. Our dataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified, Practical, and Understandable Summary of Non-transactional Consistency Levels in Distributed Replication","url":"https://arxiv.org/abs/2409.01576","date":1739768400,"author":"","guid":1118,"unread":true,"content":"<article>arXiv:2409.01576v3 Announce Type: replace \nAbstract: We present a summary of practical non-transactional consistency levels in the context of distributed data replication. Unlike prior work, we build upon a simple Shared Object Pool (SOP) model and define consistency levels in a unified framework centered around the concept of ordering. This model naturally reflects modern cloud object storage services and is thus easy to understand. We show that a consistency level can be intuitively defined by specifying two types of constraints on the validity of orderings allowed by the level: convergence, which bounds the lineage shape of the ordering, and relationship, which bounds the relative positions between operations. We give examples of representative protocols and systems, and discuss their availability upper bound.</article>","contentLength":824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"High-order finite element methods for three-dimensional multicomponent convection-diffusion","url":"https://arxiv.org/abs/2408.17390","date":1739768400,"author":"","guid":1119,"unread":true,"content":"<article>arXiv:2408.17390v2 Announce Type: replace \nAbstract: We derive and analyze a broad class of finite element methods for numerically simulating the stationary, low Reynolds number flow of concentrated mixtures of several distinct chemical species in a common thermodynamic phase. The underlying partial differential equations that we discretize are the Stokes$\\unicode{x2013}$Onsager$\\unicode{x2013}$Stefan$\\unicode{x2013}$Maxwell (SOSM) equations, which model bulk momentum transport and multicomponent diffusion within ideal and non-ideal mixtures. Unlike previous approaches, the methods are straightforward to implement in two and three spatial dimensions, and allow for high-order finite element spaces to be employed. The key idea in deriving the discretization is to suitably reformulate the SOSM equations in terms of the species mass fluxes and chemical potentials, and discretize these unknown fields using stable $H(\\textrm{div}) \\unicode{x2013} L^2$ finite element pairs. We prove that the methods are convergent and yield a symmetric linear system for a Picard linearization of the SOSM equations, which staggers the updates for concentrations and chemical potentials. We also discuss how the proposed approach can be extended to the Newton linearization of the SOSM equations, which requires the simultaneous solution of mole fractions, chemical potentials, and other variables. Our theoretical results are supported by numerical experiments and we present an example of a physical application involving the microfluidic non-ideal mixing of hydrocarbons.</article>","contentLength":1566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Error-controlled non-additive interaction discovery in machine learning models","url":"https://arxiv.org/abs/2408.17016","date":1739768400,"author":"","guid":1120,"unread":true,"content":"<article>arXiv:2408.17016v2 Announce Type: replace \nAbstract: Machine learning (ML) models are powerful tools for detecting complex patterns within data, yet their \"black box\" nature limits their interpretability, hindering their use in critical domains like healthcare and finance. To address this challenge, interpretable ML methods have been developed to explain how features influence model predictions. However, these methods often focus on univariate feature importance, overlooking the complex interactions between features that ML models are capable of capturing. Recognizing this limitation, recent efforts have aimed to extend these methods to discover feature interactions, but existing approaches struggle with robustness and error control, especially under data perturbations. In this study, we introduce Diamond, a novel method for trustworthy feature interaction discovery. Diamond uniquely integrates the model-X knockoffs framework to control the false discovery rate (FDR), ensuring that the proportion of falsely discovered interactions remains low. A key innovation in Diamond is its non-additivity distillation procedure, which refines existing interaction importance measures to distill non-additive interaction effects, ensuring that FDR control is maintained. This approach addresses the limitations of off-the-shelf interaction measures, which, when used naively, can lead to inaccurate discoveries. Diamond's applicability spans a wide range of ML models, including deep neural networks, transformer models, tree-based models, and factorization-based models. Our empirical evaluations on both simulated and real datasets across various biomedical studies demonstrate Diamond's utility in enabling more reliable data-driven scientific discoveries. This method represents a significant step forward in the deployment of ML models for scientific innovation and hypothesis generation.</article>","contentLength":1897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CrossFi: A Cross Domain Wi-Fi Sensing Framework Based on Siamese Network","url":"https://arxiv.org/abs/2408.10919","date":1739768400,"author":"","guid":1121,"unread":true,"content":"<article>arXiv:2408.10919v4 Announce Type: replace \nAbstract: In recent years, Wi-Fi sensing has garnered significant attention due to its numerous benefits, such as privacy protection, low cost, and penetration ability. Extensive research has been conducted in this field, focusing on areas such as gesture recognition, people identification, and fall detection. However, many data-driven methods encounter challenges related to domain shift, where the model fails to perform well in environments different from the training data. One major factor contributing to this issue is the limited availability of Wi-Fi sensing datasets, which makes models learn excessive irrelevant information and over-fit to the training set. Unfortunately, collecting large-scale Wi-Fi sensing datasets across diverse scenarios is a challenging task. To address this problem, we propose CrossFi, a siamese network-based approach that excels in both in-domain scenario and cross-domain scenario, including few-shot, zero-shot scenarios, and even works in few-shot new-class scenario where testing set contains new categories. The core component of CrossFi is a sample-similarity calculation network called CSi-Net, which improves the structure of the siamese network by using an attention mechanism to capture similarity information, instead of simply calculating the distance or cosine similarity. Based on it, we develop an extra Weight-Net that can generate a template for each class, so that our CrossFi can work in different scenarios. Experimental results demonstrate that our CrossFi achieves state-of-the-art performance across various scenarios. In gesture recognition task, our CrossFi achieves an accuracy of 98.17% in in-domain scenario, 91.72% in one-shot cross-domain scenario, 64.81% in zero-shot cross-domain scenario, and 84.75% in one-shot new-class scenario. The code for our model is publicly available at https://github.com/RS2002/CrossFi.</article>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improved Community Detection using Stochastic Block Models","url":"https://arxiv.org/abs/2408.10464","date":1739768400,"author":"","guid":1122,"unread":true,"content":"<article>arXiv:2408.10464v2 Announce Type: replace \nAbstract: Community detection approaches resolve complex networks into smaller groups (communities) that are expected to be relatively edge-dense and well-connected. The stochastic block model (SBM) is one of several approaches used to uncover community structure in graphs. In this study, we demonstrate that SBM software applied to various real-world and synthetic networks produces poorly-connected to disconnected clusters. We present simple modifications to improve the connectivity of SBM clusters, and show that the modifications improve accuracy using simulated networks.</article>","contentLength":622,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HaSPeR: An Image Repository for Hand Shadow Puppet Recognition","url":"https://arxiv.org/abs/2408.10360","date":1739768400,"author":"","guid":1123,"unread":true,"content":"<article>arXiv:2408.10360v5 Announce Type: replace \nAbstract: Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of theatrical art and storytelling where hand shadows are projected onto flat surfaces to create illusions of living creatures. The skilled performers create these silhouettes by hand positioning, finger movements, and dexterous gestures to resemble shadows of animals and objects. Due to the lack of practitioners and a seismic shift in people's entertainment standards, this art form is on the verge of extinction. To facilitate its preservation and proliferate it to a wider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset consisting of 15,000 images of hand shadow puppets across 15 classes extracted from both professional and amateur hand shadow puppeteer clips. We provide a detailed statistical analysis of the dataset and employ a range of pretrained image classification models to establish baselines. Our findings show a substantial performance superiority of skip-connected convolutional models over attention-based transformer architectures. We also find that lightweight models, such as MobileNetV2, suited for mobile applications and embedded devices, perform comparatively well. We surmise that such low-latency architectures can be useful in developing ombromanie teaching tools, and we create a prototype application to explore this surmission. Keeping the best-performing model ResNet34 under the limelight, we conduct comprehensive feature-spatial, explainability, and error analyses to gain insights into its decision-making process. To the best of our knowledge, this is the first documented dataset and research endeavor to preserve this dying art for future generations, with computer vision approaches. Our code and data will be publicly available.</article>","contentLength":1825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation","url":"https://arxiv.org/abs/2408.08192","date":1739768400,"author":"","guid":1124,"unread":true,"content":"<article>arXiv:2408.08192v2 Announce Type: replace \nAbstract: Mean field games (MFGs) model interactions in large-population multi-agent systems through population distributions. Traditional learning methods for MFGs are based on fixed-point iteration (FPI), where policy updates and induced population distributions are computed separately and sequentially. However, FPI-type methods may suffer from inefficiency and instability due to potential oscillations caused by this forward-backward procedure. In this work, we propose a novel perspective that treats the policy and population as a unified parameter controlling the game dynamics. By applying stochastic parameter approximation to this unified parameter, we develop SemiSGD, a simple stochastic gradient descent (SGD)-type method, where an agent updates its policy and population estimates simultaneously and fully asynchronously. Building on this perspective, we further apply linear function approximation (LFA) to the unified parameter, resulting in the first population-aware LFA (PA-LFA) for learning MFGs on continuous state-action spaces. A comprehensive finite-time convergence analysis is provided for SemiSGD with PA-LFA, including its convergence to the equilibrium for linear MFGs -- a class of MFGs with a linear structure concerning the population -- under the standard contractivity condition, and to a neighborhood of the equilibrium under a more practical condition. We also characterize the approximation error for non-linear MFGs. We validate our theoretical findings with six experiments on three MFGs.</article>","contentLength":1572,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Expressway Ramp Merge Safety and Efficiency via Spatiotemporal Cooperative Control","url":"https://arxiv.org/abs/2408.08121","date":1739768400,"author":"","guid":1125,"unread":true,"content":"<article>arXiv:2408.08121v3 Announce Type: replace \nAbstract: In the context of autonomous driving on expressways, the issue of ensuring safe and efficient ramp merging remains a significant challenge. Existing systems often struggle to accurately assess the status and intentions of other vehicles, leading to a persistent occurrence of accidents despite efforts to maintain safe distances. This study proposes a novel spatiotemporal cooperative control approach integrating vehicle-road coordination to address this critical issue. A comprehensive methodology is developed, beginning with the calculation of safe distances under varying spatiotemporal conditions. This involves considering multiple factors, including vehicle speed differentials, positioning errors, and clock synchronization errors. Subsequently, an advanced vehicle conflict risk evaluation model is constructed. By incorporating collision acceleration and emergency acceleration as key parameters, this model offers a more accurate and detailed evaluation of potential risks during the ramp merging process. Based on the calculated safe distances and conflict risk evaluations, a mainline priority coordinated control method is formulated. This method enables the pre-planning of vehicle trajectories, effectively reducing conflicts among vehicles. Through rigorous simulations using diverse traffic volume and speed scenarios, the efficacy of the proposed strategy is validated. The results demonstrate remarkable improvements, with the average delay time reduced by an impressive 97.96% and fuel consumption decreased by 6.01%. These outcomes indicate that the proposed approach not only enhances the speed of vehicle merging but also significantly reduces latency and fuel consumption, thereby enhancing the overall performance of ramp merging operations.</article>","contentLength":1821,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Video Coding Meets Multimodal Large Language Models: A Unified Paradigm for Video Coding","url":"https://arxiv.org/abs/2408.08093","date":1739768400,"author":"","guid":1126,"unread":true,"content":"<article>arXiv:2408.08093v3 Announce Type: replace \nAbstract: Existing codecs are designed to eliminate intrinsic redundancies to create a compact representation for compression. However, strong external priors from Multimodal Large Language Models (MLLMs) have not been explicitly explored in video compression. Herein, we introduce a unified paradigm for Cross-Modality Video Coding (CMVC), which is a pioneering approach to explore multimodality representation and video generative models in video coding. Specifically, on the encoder side, we disentangle a video into spatial content and motion components, which are subsequently transformed into distinct modalities to achieve very compact representation by leveraging MLLMs. During decoding, previously encoded components and video generation models are leveraged to create multiple encoding-decoding modes that optimize video reconstruction quality for specific decoding requirements, including Text-Text-to-Video (TT2V) mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V) mode to achieve superb perceptual consistency. In addition, we propose an efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA) tuning to guarantee perceptual quality, which allows the generated motion cues to behave smoothly. Experiments on benchmarks indicate that TT2V achieves effective semantic reconstruction, while IT2V exhibits competitive perceptual consistency. These results highlight potential directions for future research in video coding.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CTE-MLO: Continuous-time and Efficient Multi-LiDAR Odometry with Localizability-aware Point Cloud Sampling","url":"https://arxiv.org/abs/2408.04901","date":1739768400,"author":"","guid":1127,"unread":true,"content":"<article>arXiv:2408.04901v2 Announce Type: replace \nAbstract: In recent years, LiDAR-based localization and mapping methods have achieved significant progress thanks to their reliable and real-time localization capability. Considering single LiDAR odometry often faces hardware failures and degeneracy in practical scenarios, Multi-LiDAR Odometry (MLO), as an emerging technology, is studied to enhance the performance of LiDAR-based localization and mapping systems. However, MLO can suffer from high computational complexity introduced by dense point clouds that are fused from multiple LiDARs, and the continuous-time measurement characteristic is constantly neglected by existing LiDAR odometry. This motivates us to develop a Continuous-Time and Efficient MLO, namely CTE-MLO, which can achieve accurate and real-time estimation using multi-LiDAR measurements through a continuous-time perspective. In this paper, the Gaussian process estimation is naturally combined with the Kalman filter, which enables each LiDAR point in a point stream to query the corresponding continuous-time trajectory using its time instants. A decentralized multi-LiDAR synchronization scheme is also devised to combine points from separate LiDARs into a single point cloud without the primary LiDAR assignment. Moreover, with the aim of improving the real-time performance of MLO without sacrificing robustness, a point cloud sampling strategy is designed with the consideration of localizability. To this end, CTE-MLO integrates synchronization, localizability-aware sampling, continuous-time estimation, and voxel map management within a Kalman filter framework, which can achieve high accuracy and robust continuous-time estimation within only a few linear iterations. The effectiveness of the proposed method is demonstrated through various scenarios, including public datasets and real-world applications. The code is available at https://github.com/shenhm516/CTE-MLO to benefit the community.</article>","contentLength":1973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering","url":"https://arxiv.org/abs/2407.20908","date":1739768400,"author":"","guid":1128,"unread":true,"content":"<article>arXiv:2407.20908v2 Announce Type: replace \nAbstract: Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.</article>","contentLength":1410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Global Solver based on the Sperner-Lemma and Mazurkewicz-Knaster-Kuratowski-Lemma based proof of the Brouwer Fixed-Point Theorem","url":"https://arxiv.org/abs/2407.18816","date":1739768400,"author":"","guid":1129,"unread":true,"content":"<article>arXiv:2407.18816v5 Announce Type: replace \nAbstract: In this paper a fixed-point solver for mappings from a Simplex into itself that is gradient-free, global and requires $d$ function evaluations for halvening the error is presented, where $d$ is the dimension. It is based on topological arguments and uses the constructive proof of the Mazurkewicz-Knaster-Kuratowski lemma as used as part of the proof for Brouwers Fixed-Point theorem.</article>","contentLength":437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transformer-based Graph Neural Networks for Battery Range Prediction in AIoT Battery-Swap Services","url":"https://arxiv.org/abs/2407.16115","date":1739768400,"author":"","guid":1130,"unread":true,"content":"<article>arXiv:2407.16115v2 Announce Type: replace \nAbstract: The concept of the sharing economy has gained broad recognition, and within this context, Sharing E-Bike Battery (SEB) have emerged as a focal point of societal interest. Despite the popularity, a notable discrepancy remains between user expectations regarding the remaining battery range of SEBs and the reality, leading to a pronounced inclination among users to find an available SEB during emergency situations. In response to this challenge, the integration of Artificial Intelligence of Things (AIoT) and battery-swap services has surfaced as a viable solution. In this paper, we propose a novel structural Transformer-based model, referred to as the SEB-Transformer, designed specifically for predicting the battery range of SEBs. The scenario is conceptualized as a dynamic heterogeneous graph that encapsulates the interactions between users and bicycles, providing a comprehensive framework for analysis. Furthermore, we incorporate the graph structure into the SEB-Transformer to facilitate the estimation of the remaining e-bike battery range, in conjunction with mean structural similarity, enhancing the prediction accuracy. By employing the predictions made by our model, we are able to dynamically adjust the optimal cycling routes for users in real-time, while also considering the strategic locations of charging stations, thereby optimizing the user experience. Empirically our results on real-world datasets demonstrate the superiority of our model against nine competitive baselines. These innovations, powered by AIoT, not only bridge the gap between user expectations and the physical limitations of battery range but also significantly improve the operational efficiency and sustainability of SEB services. Through these advancements, the shared electric bicycle ecosystem is evolving, making strides towards a more reliable, user-friendly, and sustainable mode of transportation.</article>","contentLength":1957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships","url":"https://arxiv.org/abs/2407.12543","date":1739768400,"author":"","guid":1131,"unread":true,"content":"<article>arXiv:2407.12543v2 Announce Type: replace \nAbstract: While interpretability methods identify a model's learned concepts, they overlook the relationships between concepts that make up its abstractions and inform its ability to generalize to new data. To assess whether models' have learned human-aligned abstractions, we introduce abstraction alignment, a methodology to compare model behavior against formal human knowledge. Abstraction alignment externalizes domain-specific human knowledge as an abstraction graph, a set of pertinent concepts spanning levels of abstraction. Using the abstraction graph as a ground truth, abstraction alignment measures the alignment of a model's behavior by determining how much of its uncertainty is accounted for by the human abstractions. By aggregating abstraction alignment across entire datasets, users can test alignment hypotheses, such as which human concepts the model has learned and where misalignments recur. In evaluations with experts, abstraction alignment differentiates seemingly similar errors, improves the verbosity of existing model-quality metrics, and uncovers improvements to current human abstractions.</article>","contentLength":1164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detect, Investigate, Judge and Determine: A Knowledge-guided Framework for Few-shot Fake News Detection","url":"https://arxiv.org/abs/2407.08952","date":1739768400,"author":"","guid":1132,"unread":true,"content":"<article>arXiv:2407.08952v2 Announce Type: replace \nAbstract: Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Knowledge-guided Fake News Detection (DKFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DKFND first identifies the knowledge concepts of each news article through a Detection Module. Subsequently, DKFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to evaluate the relevance and confidence of them. Finally, a Determination Module further derives two respective predictions and obtain the final result. Extensive experiments on two public datasets show the efficacy of our proposed method, particularly in low-resource settings.</article>","contentLength":1387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fair and Truthful Allocations Under Leveled Valuations","url":"https://arxiv.org/abs/2407.05891","date":1739768400,"author":"","guid":1133,"unread":true,"content":"<article>arXiv:2407.05891v2 Announce Type: replace \nAbstract: We study the problem of fairly allocating indivisible goods among agents which are equipped with {\\em leveled} valuation functions. Such preferences, that have been studied before in economics and fair division literature, capture a simple and intuitive economic behavior; larger bundles are always preferred to smaller ones. We provide a fine-grained analysis for various subclasses of leveled valuations focusing on two extensively studied notions of fairness, (approximate) MMS and EFX. In particular, we present a general positive result, showing the existence of $2/3$-MMS allocations under valuations that are both leveled and submodular. We also show how some of our ideas can be used beyond the class of leveled valuations; for the case of two submodular (not necessarily leveled) agents we show that there always exists a $2/3$-MMS allocation, complementing a recent impossibility result. Then, we switch to the case of subadditive and fractionally subadditive leveled agents, where we are able to show tight (lower and upper) bounds of $1/2$ on the approximation factor of MMS. Moreover, we show the existence of exact EFX allocations under general leveled valuations via a simple protocol that in addition satisfies several natural economic properties. Finally, we take a mechanism design approach and we propose protocols that are both truthful and approximately fair under leveled valuations.</article>","contentLength":1458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Associative Recurrent Memory Transformer","url":"https://arxiv.org/abs/2407.04841","date":1739768400,"author":"","guid":1134,"unread":true,"content":"<article>arXiv:2407.04841v2 Announce Type: replace \nAbstract: This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.</article>","contentLength":779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hardware-Friendly Shuffling Countermeasure Against Side-Channel Attacks for Kyber","url":"https://arxiv.org/abs/2407.02452","date":1739768400,"author":"","guid":1135,"unread":true,"content":"<article>arXiv:2407.02452v3 Announce Type: replace \nAbstract: CRYSTALS-Kyber has been standardized as the only key-encapsulation mechanism (KEM) scheme by NIST to withstand attacks by large-scale quantum computers. However, the side-channel attacks (SCAs) on its implementation are still needed to be well considered for the upcoming migration. In this brief, we propose a secure and efficient hardware implementation for Kyber by incorporating a novel compact shuffling architecture. First of all, we modify the Fisher-Yates shuffle to make it more hardware-friendly. We then design an optimized shuffling architecture for the well-known open-source Kyber hardware implementation to enhance the security of all known and potential side-channel leakage points. Finally, we implement the modified Kyber design on FPGA and evaluate its security and performance. The security is verified by conducting correlation power analysis (CPA) and test vector leakage assessment (TVLA) on the hardware. Meanwhile, FPGA place-and-route results show that the proposed design reports only 8.7% degradation on the hardware efficiency compared with the original unprotected version, much better than existing hardware hiding schemes.</article>","contentLength":1207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking Predictive Coding Networks -- Made Simple","url":"https://arxiv.org/abs/2407.01163","date":1739768400,"author":"","guid":1136,"unread":true,"content":"<article>arXiv:2407.01163v2 Announce Type: replace \nAbstract: In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we propose a library, called PCX, that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library and a comprehensive set of benchmarks would address all these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii)~reach new state-of-the-art results in all of the tasks and datasets provided; (iii)~clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we release code, tests, and benchmarks. Link to the library: https://github.com/liukidar/pcx</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents","url":"https://arxiv.org/abs/2406.11132","date":1739768400,"author":"","guid":1137,"unread":true,"content":"<article>arXiv:2406.11132v2 Announce Type: replace \nAbstract: In the past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and their capacity is further expanded into the so-called LLM agents when connected with external tools. In all domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering (APE) has become an important question for many researchers and users of LLMs. However, previous works in APE rely on a final checker to evaluate the performance of the given prompt -- a requirement that is hard to meet in the case of LLM agents, where intermediate feedback is easier to obtain, and the final evaluation could be expensive, inaccurate, or even missing. In this paper, we propose a novel method, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to optimize the step-by-step instructions in the prompts given to LLM agents, based on the chat history obtained from interactions and reflections with LLM agents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the prompt without the need for a final solution checker. We evaluate our approach on PDDL generation, TravelPlanner, and Meeting Planning to show that our method could generally improve performance for different reasoning tasks.</article>","contentLength":1437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents","url":"https://arxiv.org/abs/2406.10291","date":1739768400,"author":"","guid":1138,"unread":true,"content":"<article>arXiv:2406.10291v2 Announce Type: replace \nAbstract: Large language models (LLMs) excel across many natural language processing tasks but face challenges in domain-specific, analytical tasks such as conducting research surveys. This study introduces ResearchArena, a benchmark designed to evaluate LLMs' capabilities in conducting academic surveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papers' relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps. Notably, mind-map construction is treated as a bonus task, reflecting its supplementary role in survey-writing. To support these evaluations, we construct an offline environment of 12M full-text academic papers and 7.9K survey papers. To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, underscoring significant opportunities for advancing LLMs in autonomous research.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RASPNet: A Benchmark Dataset for Radar Adaptive Signal Processing Applications","url":"https://arxiv.org/abs/2406.09638","date":1739768400,"author":"","guid":1139,"unread":true,"content":"<article>arXiv:2406.09638v2 Announce Type: replace \nAbstract: We present a large-scale dataset for radar adaptive signal processing (RASP) applications to support the development of data-driven models within the adaptive radar community. The dataset, RASPNet, exceeds 16 TB in size and comprises 100 realistic scenarios compiled over a variety of topographies and land types from across the contiguous United States. For each scenario, RASPNet consists of 10,000 clutter realizations from an airborne radar setting, which can be used to benchmark radar and complex-valued learning algorithms. RASPNet intends to fill a prominent gap in the availability of a large-scale, realistic dataset that standardizes the evaluation of adaptive radar processing techniques and complex-valued neural networks. We outline its construction, organization, and several applications, including a transfer learning example to demonstrate how RASPNet can be used for realistic adaptive radar processing scenarios.</article>","contentLength":985,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentially Private Prototypes for Imbalanced Transfer Learning","url":"https://arxiv.org/abs/2406.08039","date":1739768400,"author":"","guid":1140,"unread":true,"content":"<article>arXiv:2406.08039v3 Announce Type: replace \nAbstract: Machine learning (ML) models have been shown to leak private information from their training datasets. Differential Privacy (DP), typically implemented through the differential private stochastic gradient descent algorithm (DP-SGD), has become the standard solution to bound leakage from the models. Despite recent improvements, DP-SGD-based approaches for private learning still usually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes, and when the private training datasets are imbalanced. To overcome these limitations, we propose Differentially Private Prototype Learning (DPPL) as a new paradigm for private transfer learning. DPPL leverages publicly pre-trained encoders to extract features from private data and generates DP prototypes that represent each private class in the embedding space and can be publicly released for inference. Since our DP prototypes can be obtained from only a few private training data points and without iterative noise addition, they offer high-utility predictions and strong privacy guarantees even under the notion of \\textit{pure DP}. We additionally show that privacy-utility trade-offs can be further improved when leveraging the public data beyond pre-training of the encoder: in particular, we can privately sample our DP prototypes from the publicly available data points used to train the encoder. Our experimental evaluation with four state-of-the-art encoders, four vision datasets, and under different data and imbalancedness regimes demonstrate DPPL's high performance under strong privacy guarantees in challenging private learning setups</article>","contentLength":1665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Critical Look At Tokenwise Reward-Guided Text Generation","url":"https://arxiv.org/abs/2406.07780","date":1739768400,"author":"","guid":1141,"unread":true,"content":"<article>arXiv:2406.07780v2 Announce Type: replace \nAbstract: Large language models (LLMs) can be improved by aligning with human preferences through fine-tuning -- the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users. Due to their ability to bypass LLM fine-tuning, prediction-time tokenwise reward-guided text generation (RGTG) methods have recently been proposed. They use a reward model trained on full sequences to score partial sequences during decoding in a bid to steer the generation towards sequences with high rewards. However, these methods have so far been only heuristically motivated and poorly analyzed. In this work, we show that reward models trained on full sequences are not compatible with scoring partial sequences. To alleviate this issue, we propose to train a Bradley-Terry reward model on partial sequences explicitly, and autoregressively sample from the implied tokenwise policy during decoding time. We study the properties of this reward model and the resulting policy: we show that this policy is proportional to the ratio of two distinct RLHF policies. Our simple approach outperforms previous RGTG methods and performs similarly to strong offline baselines without large-scale LLM finetuning.</article>","contentLength":1298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delving into LLM-assisted writing in biomedical publications through excess vocabulary","url":"https://arxiv.org/abs/2406.07016","date":1739768400,"author":"","guid":1142,"unread":true,"content":"<article>arXiv:2406.07016v3 Announce Type: replace \nAbstract: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.</article>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Latent Diffusion Model-Enabled Low-Latency Semantic Communication in the Presence of Semantic Ambiguities and Wireless Channel Noises","url":"https://arxiv.org/abs/2406.06644","date":1739768400,"author":"","guid":1143,"unread":true,"content":"<article>arXiv:2406.06644v4 Announce Type: replace \nAbstract: Deep learning (DL)-based Semantic Communications (SemCom) is becoming critical to maximize overall efficiency of communication networks. Nevertheless, SemCom is sensitive to wireless channel uncertainties, source outliers, and suffer from poor generalization bottlenecks. To address the mentioned challenges, this paper develops a latent diffusion model-enabled SemCom system with three key contributions, i.e., i) to handle potential outliers in the source data, semantic errors obtained by projected gradient descent based on the vulnerabilities of DL models, are utilized to update the parameters and obtain an outlier-robust encoder, ii) a lightweight single-layer latent space transformation adapter completes one-shot learning at the transmitter and is placed before the decoder at the receiver, enabling adaptation for out-of-distribution data and enhancing human-perceptual quality, and iii) an end-to-end consistency distillation (EECD) strategy is used to distill the diffusion models trained in latent space, enabling deterministic single or few-step low-latency denoising in various noisy channels while maintaining high semantic quality. Extensive numerical experiments across different datasets demonstrate the superiority of the proposed SemCom system, consistently proving its robustness to outliers, the capability to transmit data with unknown distributions, and the ability to perform real-time channel denoising tasks while preserving high human perceptual quality, outperforming the existing denoising approaches in semantic metrics such as multi-scale structural similarity index measure (MS-SSIM) and learned perceptual image path similarity (LPIPS).</article>","contentLength":1726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Verbalized Machine Learning: Revisiting Machine Learning with Language Models","url":"https://arxiv.org/abs/2406.04344","date":1739768400,"author":"","guid":1144,"unread":true,"content":"<article>arXiv:2406.04344v3 Announce Type: replace \nAbstract: Motivated by the progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning (ML) models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical ML problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias: prior knowledge about the problem and hypothesis class can be encoded in natural language and fed into the LLM-parameterized learner; (2) automatic model class selection: the optimizer can automatically select a model class based on data and verbalized prior knowledge, and it can update the model class during training; and (3) interpretable learner updates: the LLM-parameterized optimizer can provide explanations for why an update is performed. We empirically verify the effectiveness of VML, and hope that VML can serve as a stepping stone to stronger interpretability.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shield Synthesis for LTL Modulo Theories","url":"https://arxiv.org/abs/2406.04184","date":1739768400,"author":"","guid":1145,"unread":true,"content":"<article>arXiv:2406.04184v2 Announce Type: replace \nAbstract: In recent years, Machine Learning (ML) models have achieved remarkable success in various domains. However, these models also tend to demonstrate unsafe behaviors, precluding their deployment in safety-critical systems. To cope with this issue, ample research focuses on developing methods that guarantee the safe behaviour of a given ML model. A prominent example is shielding which incorporates an external component (a ``shield'') that blocks unwanted behavior. Despite significant progress, shielding suffers from a main setback: it is currently geared towards properties encoded solely in propositional logics (e.g., LTL) and is unsuitable for richer logics. This, in turn, limits the widespread applicability of shielding in many real-world systems. In this work, we address this gap, and extend shielding to LTL modulo theories, by building upon recent advances in reactive synthesis modulo theories. This allowed us to develop a novel approach for generating shields conforming to complex safety specifications in these more expressive, logics. We evaluated our shields and demonstrate their ability to handle rich data with temporal dynamics. To the best of our knowledge, this is the first approach for synthesizing shields for such expressivity.</article>","contentLength":1309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning","url":"https://arxiv.org/abs/2406.03519","date":1739768400,"author":"","guid":1146,"unread":true,"content":"<article>arXiv:2406.03519v4 Announce Type: replace \nAbstract: High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients aggregation weights proportional to their privacy parameters will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here.</article>","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection","url":"https://arxiv.org/abs/2406.00625","date":1739768400,"author":"","guid":1147,"unread":true,"content":"<article>arXiv:2406.00625v4 Announce Type: replace \nAbstract: Visual anomaly detection is vital in real-world applications, such as industrial defect detection and medical diagnosis. However, most existing methods focus on local structural anomalies and fail to detect higher-level functional anomalies under logical conditions. Although recent studies have explored logical anomaly detection, they can only address simple anomalies like missing or addition and show poor generalizability due to being heavily data-driven. To fill this gap, we propose SAM-LAD, a zero-shot, plug-and-play framework for logical anomaly detection in any scene. First, we obtain a query image's feature map using a pre-trained backbone. Simultaneously, we retrieve the reference images and their corresponding feature maps via the nearest neighbor search of the query image. Then, we introduce the Segment Anything Model (SAM) to obtain object masks of the query and reference images. Each object mask is multiplied with the entire image's feature map to obtain object feature maps. Next, an Object Matching Model (OMM) is proposed to match objects in the query and reference images. To facilitate object matching, we further propose a Dynamic Channel Graph Attention (DCGA) module, treating each object as a keypoint and converting its feature maps into feature vectors. Finally, based on the object matching relations, an Anomaly Measurement Model (AMM) is proposed to detect objects with logical anomalies. Structural anomalies in the objects can also be detected. We validate our proposed SAM-LAD using various benchmarks, including industrial datasets (MVTec Loco AD, MVTec AD), and the logical dataset (DigitAnatomy). Extensive experimental results demonstrate that SAM-LAD outperforms existing SoTA methods, particularly in detecting logical anomalies.</article>","contentLength":1830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentially Private Clustered Federated Learning","url":"https://arxiv.org/abs/2405.19272","date":1739768400,"author":"","guid":1148,"unread":true,"content":"<article>arXiv:2405.19272v4 Announce Type: replace \nAbstract: Federated learning (FL), which is a decentralized machine learning (ML) approach, often incorporates differential privacy (DP) to provide rigorous data privacy guarantees. Previous works attempted to address high structured data heterogeneity in vanilla FL settings through clustering clients (a.k.a clustered FL), but these methods remain sensitive and prone to errors, further exacerbated by the DP noise. This vulnerability makes the previous methods inappropriate for differentially private FL (DPFL) settings with structured data heterogeneity. To address this gap, we propose an algorithm for differentially private clustered FL, which is robust to the DP noise in the system and identifies the underlying clients' clusters correctly. To this end, we propose to cluster clients based on both their model updates and training loss values. Furthermore, for clustering clients' model updates at the end of the first round, our proposed approach addresses the server's uncertainties by employing large batch sizes as well as Gaussian Mixture Models (GMM) to reduce the impact of DP and stochastic noise and avoid potential clustering errors. This idea is efficient especially in privacy-sensitive scenarios with more DP noise. We provide theoretical analysis to justify our approach and evaluate it across diverse data distributions and privacy budgets. Our experimental results show its effectiveness in addressing large structured data heterogeneity in DPFL.</article>","contentLength":1515,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LeDex: Training LLMs to Better Self-Debug and Explain Code","url":"https://arxiv.org/abs/2405.18649","date":1739768400,"author":"","guid":1149,"unread":true,"content":"<article>arXiv:2405.18649v2 Announce Type: replace \nAbstract: In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose LeDex, a training framework that significantly improves the self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92% and pass@10 by 9.30% over four benchmarks. RL training brings additional up to 3.54% improvement on pass@1 and 2.55% improvement on pass@10. The trained LLMs show iterative refinement ability and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.</article>","contentLength":1651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model Cascading for Code: A Cascaded Black-Box Multi-Model Framework for Cost-Efficient Code Completion with Self-Testing","url":"https://arxiv.org/abs/2405.15842","date":1739768400,"author":"","guid":1150,"unread":true,"content":"<article>arXiv:2405.15842v2 Announce Type: replace \nAbstract: The rapid advancement of large language models (LLMs) has significantly improved code completion tasks, yet the trade-off between accuracy and computational cost remains a critical challenge. While using larger models and incorporating inference-time self-testing algorithms can significantly improve output accuracy, they incur substantial computational expenses at the same time. Furthermore, servers in real-world scenarios usually have a dynamic preference on the cost-accuracy tradeoff, depending on the budget, bandwidth, the concurrent user volume, and users' sensitivity to wrong answers. In this work, we introduce a novel framework combining model cascading and inference-time self-feedback algorithms to find multiple near-optimal self-testing options on the cost-accuracy tradeoff in LLM-based code generation. Our approach leverages self-generated tests to both enhance accuracy and evaluate model cascading decisions. As a blackbox inference-time method, it requires no access to internal model parameters. We further propose a threshold-based algorithm to determine when to deploy larger models and a heuristic to optimize the number of solutions, test cases, and test lines generated per model, based on budget constraints. Experimental results show that our cascading approach reduces costs by an average of 26%, and up to 70% in the best case, across various model families and datasets, while maintaining or improving accuracy in natural language generation tasks compared to both random and optimal single-model self-testing schemes. To our knowledge, this is the first work to provide a series of choices for optimizing the cost-accuracy trade-off in LLM code generation with self-testing.</article>","contentLength":1763,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code","url":"https://arxiv.org/abs/2405.15568","date":1739768400,"author":"","guid":1151,"unread":true,"content":"<article>arXiv:2405.15568v3 Announce Type: replace \nAbstract: Open-ended and AI-generating algorithms aim to continuously generate and solve increasingly complex tasks indefinitely, offering a promising path toward more general intelligence. To accomplish this grand vision, learning must occur within a vast array of potential tasks. Existing approaches to automatically generating environments are constrained within manually predefined, often narrow distributions of environment, limiting their ability to create any learning environment. To address this limitation, we introduce a novel framework, OMNI-EPIC, that augments previous work in Open-endedness via Models of human Notions of Interestingness (OMNI) with Environments Programmed in Code (EPIC). OMNI-EPIC leverages foundation models to autonomously generate code specifying the next learnable (i.e., not too easy or difficult for the agent's current skill set) and interesting (e.g., worthwhile and novel) tasks. OMNI-EPIC generates both environments (e.g., an obstacle course) and reward functions (e.g., progress through the obstacle course quickly without touching red objects), enabling it, in principle, to create any simulatable learning task. We showcase the explosive creativity of OMNI-EPIC, which continuously innovates to suggest new, interesting learning challenges. We also highlight how OMNI-EPIC can adapt to reinforcement learning agents' learning progress, generating tasks that are of suitable difficulty. Overall, OMNI-EPIC can endlessly create learnable and interesting environments, further propelling the development of self-improving AI systems and AI-Generating Algorithms. Project website with videos: https://dub.sh/omniepic</article>","contentLength":1704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linear Mode Connectivity in Differentiable Tree Ensembles","url":"https://arxiv.org/abs/2405.14596","date":1739768400,"author":"","guid":1152,"unread":true,"content":"<article>arXiv:2405.14596v2 Announce Type: replace \nAbstract: Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Space-aware Socioeconomic Indicator Inference with Heterogeneous Graphs","url":"https://arxiv.org/abs/2405.14135","date":1739768400,"author":"","guid":1153,"unread":true,"content":"<article>arXiv:2405.14135v2 Announce Type: replace \nAbstract: Regional socioeconomic indicators are critical across various domains, yet their acquisition can be costly. Inferring global socioeconomic indicators from a limited number of regional samples is essential for enhancing management and sustainability in urban areas and human settlements. Current inference methods typically rely on spatial interpolation based on the assumption of spatial continuity, which does not adequately address the complex variations present within regional spaces. In this paper, we present GeoHG, the first space-aware socioeconomic indicator inference method that utilizes a heterogeneous graph-based structure to represent geospace for non-continuous inference. Extensive experiments demonstrate the effectiveness of GeoHG in comparison to existing methods, achieving an $R^2$ score exceeding 0.8 under extreme data scarcity with a masked ratio of 95\\%.</article>","contentLength":933,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"City-Scale Multi-Camera Vehicle Tracking System with Improved Self-Supervised Camera Link Model","url":"https://arxiv.org/abs/2405.11345","date":1739768400,"author":"","guid":1154,"unread":true,"content":"<article>arXiv:2405.11345v3 Announce Type: replace \nAbstract: Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms the basis for numerous future city-wide systems (e.g. traffic management, crash detection, etc.). However, the challenge of matching vehicle trajectories across different cameras based solely on feature extraction poses significant difficulties. This article introduces an innovative multi-camera vehicle tracking system that utilizes a self-supervised camera link model. In contrast to related works that rely on manual spatial-temporal annotations, our model automatically extracts crucial multi-camera relationships for vehicle matching. The camera link is established through a pre-matching process that evaluates feature similarities, pair numbers, and time variance for high-quality tracks. This process calculates the probability of spatial linkage for all camera combinations, selecting the highest scoring pairs to create camera links. Our approach significantly improves deployment times by eliminating the need for human annotation, offering substantial improvements in efficiency and cost-effectiveness when it comes to real-world application. This pairing process supports cross camera matching by setting spatial-temporal constraints, reducing the searching space for potential vehicle matches. According to our experimental results, the proposed method achieves a new state-of-the-art among automatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1 Score.</article>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solving the enigma: Enhancing faithfulness and comprehensibility in explanations of deep networks","url":"https://arxiv.org/abs/2405.10008","date":1739768400,"author":"","guid":1155,"unread":true,"content":"<article>arXiv:2405.10008v2 Announce Type: replace \nAbstract: The accelerated progress of artificial intelligence (AI) has popularized deep learning models across various domains, yet their inherent opacity poses challenges, particularly in critical fields like healthcare, medicine, and the geosciences. Explainable AI (XAI) has emerged to shed light on these 'black box' models, aiding in deciphering their decision-making processes. However, different XAI methods often produce significantly different explanations, leading to high inter-method variability that increases uncertainty and undermines trust in deep networks' predictions. In this study, we address this challenge by introducing a novel framework designed to enhance the explainability of deep networks through a dual focus on maximizing both accuracy and comprehensibility in the explanations. Our framework integrates outputs from multiple established XAI methods and leverages a non-linear neural network model, termed the 'explanation optimizer,' to construct a unified, optimal explanation. The optimizer evaluates explanations using two key metrics: faithfulness (accuracy in reflecting the network's decisions) and complexity (comprehensibility). By balancing these, it provides accurate and accessible explanations, addressing a key XAI limitation. Experiments on multi-class and binary classification in 2D object and 3D neuroscience imaging confirm its efficacy. Our optimizer achieved faithfulness scores 155% and 63% higher than the best XAI methods in 3D and 2D tasks, respectively, while also reducing complexity for better understanding. These results demonstrate that optimal explanations based on specific quality criteria are achievable, offering a solution to the issue of inter-method variability in the current XAI literature and supporting more trustworthy deep network predictions</article>","contentLength":1860,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Personalized Content Synthesis with Diffusion Models","url":"https://arxiv.org/abs/2405.05538","date":1739768400,"author":"","guid":1156,"unread":true,"content":"<article>arXiv:2405.05538v2 Announce Type: replace \nAbstract: Recent advancements in generative models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS). With a small set of user-provided examples, PCS aims to customize the subject of interest to specific user-defined prompts. Over the past two years, more than 150 methods have been proposed. However, existing surveys mainly focus on text-to-image generation, with few providing up-to-date summaries on PCS. This paper offers a comprehensive survey of PCS, with a particular focus on the diffusion models. Specifically, we introduce the generic frameworks of PCS research, which can be broadly classified into optimization-based and learning-based approaches. We further categorize and analyze these methodologies, discussing their strengths, limitations, and key techniques. Additionally, we delve into specialized tasks within the field, such as personalized object generation, face synthesis, and style personalization, highlighting their unique challenges and innovations. Despite encouraging progress, we also present an analysis of the challenges such as overfitting and the trade-off between subject fidelity and text alignment. Through this detailed overview and analysis, we propose future directions to advance the development of PCS.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Reliable Empirical Machine Unlearning Evaluation: A Cryptographic Game Perspective","url":"https://arxiv.org/abs/2404.11577","date":1739768400,"author":"","guid":1157,"unread":true,"content":"<article>arXiv:2404.11577v3 Announce Type: replace \nAbstract: Machine unlearning updates machine learning models to remove information from specific training samples, complying with data protection regulations that allow individuals to request the removal of their personal data. Despite the recent development of numerous unlearning algorithms, reliable evaluation of these algorithms remains an open research question. In this work, we focus on membership inference attack (MIA) based evaluation, one of the most common approaches for evaluating unlearning algorithms, and address various pitfalls of existing evaluation metrics lacking theoretical understanding and reliability. Specifically, by modeling the proposed evaluation process as a \\emph{cryptographic game} between unlearning algorithms and MIA adversaries, the naturally-induced evaluation metric measures the data removal efficacy of unlearning algorithms and enjoys provable guarantees that existing evaluation metrics fail to satisfy. Furthermore, we propose a practical and efficient approximation of the induced evaluation metric and demonstrate its effectiveness through both theoretical analysis and empirical experiments. Overall, this work presents a novel and reliable approach to empirically evaluating unlearning algorithms, paving the way for the development of more effective unlearning techniques.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GroundCocoa: A Benchmark for Evaluating Compositional & Conditional Reasoning in Language Models","url":"https://arxiv.org/abs/2404.04237","date":1739768400,"author":"","guid":1158,"unread":true,"content":"<article>arXiv:2404.04237v2 Announce Type: replace \nAbstract: The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their reasoning to address complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two aspects that are central to human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.</article>","contentLength":1128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minimizing the Number of Tardy Jobs and Maximal Tardiness on a Single Machine is NP-hard","url":"https://arxiv.org/abs/2404.02784","date":1739768400,"author":"","guid":1159,"unread":true,"content":"<article>arXiv:2404.02784v2 Announce Type: replace \nAbstract: This paper resolves a long-standing open question in bicriteria scheduling regarding the complexity of a single machine scheduling problem which combines the number of tardy jobs and the maximal tardiness criteria. We use the lexicographic approach with the maximal tardiness being the primary criterion. Accordingly, the objective is to find, among all solutions minimizing the maximal tardiness, the one which has the minimum number of tardy jobs. The complexity of this problem has been open for over thirty years, and has been known since then to be one of the most challenging open questions in multicriteria scheduling. We resolve this question by proving that the problem is strongly NP-hard. We also prove that the problem is at least weakly NP-hard when we switch roles between the two criteria (i.e., when the number of tardy jobs is the primary criterion). Finally, we provide hardness results for two other approaches (constraint and a priori approaches) to deal with these two criteria.</article>","contentLength":1052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AgentStudio: A Toolkit for Building General Virtual Agents","url":"https://arxiv.org/abs/2403.17918","date":1739768400,"author":"","guid":1160,"unread":true,"content":"<article>arXiv:2403.17918v3 Announce Type: replace \nAbstract: General virtual agents need to handle multimodal observations, master complex action spaces, and self-improve in dynamic, open-domain environments. However, existing environments are often domain-specific and require complex setups, which limits agent development and evaluation in real-world settings. As a result, current evaluations lack in-depth analyses that decompose fundamental agent capabilities. We introduce AgentStudio, a trinity of environments, tools, and benchmarks to address these issues. AgentStudio provides a lightweight, interactive environment with highly generic observation and action spaces, e.g., video observations and GUI/API actions. It integrates tools for creating online benchmark tasks, annotating GUI elements, and labeling actions in videos. Based on our environment and tools, we curate an online task suite that benchmarks both GUI interactions and function calling with efficient auto-evaluation. We also reorganize existing datasets and collect new ones using our tools to establish three datasets: GroundUI, IDMBench, and CriticBench. These datasets evaluate fundamental agent abilities, including GUI grounding, learning from videos, and success detection, pointing to the desiderata for robust, general, and open-ended virtual agents.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty-Aware Explanations Through Probabilistic Self-Explainable Neural Networks","url":"https://arxiv.org/abs/2403.13740","date":1739768400,"author":"","guid":1161,"unread":true,"content":"<article>arXiv:2403.13740v2 Announce Type: replace \nAbstract: The lack of transparency of Deep Neural Networks continues to be a limitation that severely undermines their reliability and usage in high-stakes applications. Promising approaches to overcome such limitations are Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions rely on the similarity between the input at hand and a set of prototypical representations of the output classes, offering therefore a deep, yet transparent-by-design, architecture. In this paper, we introduce a probabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for the prototypes with probability distributions over their values. This provides not only a more flexible framework for an end-to-end learning of prototypes, but can also capture the explanatory uncertainty of the model, which is a missing feature in previous approaches. In addition, since the prototypes determine both the explanation and the prediction, Prob-PSENNs allow us to detect when the model is making uninformed or uncertain predictions, and to obtain valid explanations for them. Our experiments demonstrate that Prob-PSENNs provide more meaningful and robust explanations than their non-probabilistic counterparts, while remaining competitive in terms of predictive performance, thus enhancing the explainability and reliability of the models.</article>","contentLength":1404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Deep Learning finally better than Decision Trees on Tabular Data?","url":"https://arxiv.org/abs/2402.03970","date":1739768400,"author":"","guid":1162,"unread":true,"content":"<article>arXiv:2402.03970v2 Announce Type: replace \nAbstract: Tabular data is a ubiquitous data modality due to its versatility and ease of use in many real-world applications. The predominant heuristics for handling classification tasks on tabular data rely on classical machine learning techniques, as the superiority of deep learning models has not yet been demonstrated. This raises the question of whether new deep learning paradigms can surpass classical approaches. Recent studies on tabular data offer a unique perspective on the limitations of neural networks in this domain and highlight the superiority of gradient boosted decision trees (GBDTs) in terms of scalability and robustness across various datasets. However, novel foundation models have not been thoroughly assessed regarding quality or fairly compared to existing methods for tabular classification. Our study categorizes ten state-of-the-art neural models based on their underlying learning paradigm, demonstrating specifically that meta-learned foundation models outperform GBDTs in small data regimes. Although dataset-specific neural networks generally outperform LLM-based tabular classifiers, they are surpassed by an AutoML library which exhibits the best performance but at the cost of higher computational demands.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simultaneous Computation and Communication over MAC","url":"https://arxiv.org/abs/2401.16751","date":1739768400,"author":"","guid":1163,"unread":true,"content":"<article>arXiv:2401.16751v4 Announce Type: replace \nAbstract: We study communication over a Gaussian multiple-access channel (MAC) with two types of transmitters: Digital transmitters hold a message from a discrete set that needs to be communicated to the receiver with vanishing error probability. Analog transmitters hold sequences of analog values. Some functions of these distributed values (but not the values themselves) need to be conveyed to the receiver, subject to a fidelity criterion such as mean squared error (MSE) or a certain maximum error with given confidence. For the case in which the computed function for the analog transmitters is a sum of values in [-1,1], we derive inner and outer bounds for the tradeoff of digital and analog rates of communication under peak and average power constraints for digital transmitters and a peak power constraint for analog transmitters. We then extend the achievability result to a class of functions that includes all linear and some non-linear functions. This extended scheme works over fading channels as long as full channel state information is available at the transmitter. The practicality of our proposed communication scheme is shown in channel simulations that use a version of the scheme based on low density parity check (LDPC) coding. We evaluate the system performance for different block lengths and Gaussian as well as non-Gaussian noise distributions.</article>","contentLength":1417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Game-Theoretic Cybersecurity: the Good, the Bad and the Ugly","url":"https://arxiv.org/abs/2401.13815","date":1739768400,"author":"","guid":1164,"unread":true,"content":"<article>arXiv:2401.13815v2 Announce Type: replace \nAbstract: Given the scale of consequences attributable to cyber attacks, the field of cybersecurity has long outgrown ad-hoc decision-making. A popular choice to provide disciplined decision-making in cybersecurity is Game Theory, which seeks to mathematically understand strategic interaction. In practice though, game-theoretic approaches are scarcely utilized (to our knowledge), highlighting the need to understand the deficit between the existing state-of-the-art and the needs of cybersecurity practitioners. Therefore, we develop a framework to characterize the function and assumptions of existing works as applied to cybersecurity and leverage it to characterize 80 unique technical papers. Then, we leverage this information to analyze the capabilities of the proposed models in comparison to the application-specific needs they are meant to serve, as well as the practicality of implementing the proposed solution. Our main finding is that Game Theory largely fails to incorporate notions of uncertainty critical to the application being considered. To remedy this, we provide guidance in terms of how to incorporate uncertainty in a model, what forms of uncertainty are critical to consider in each application area, and how to model the information that is available in each application area.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Test Case Repair Using Language Models","url":"https://arxiv.org/abs/2401.06765","date":1739768400,"author":"","guid":1165,"unread":true,"content":"<article>arXiv:2401.06765v4 Announce Type: replace \nAbstract: Ensuring the quality of software systems through testing is essential, yet maintaining test cases poses significant challenges and costs. The need for frequent updates to align with the evolving system under test often entails high complexity and cost for maintaining these test cases. Further, unrepaired broken test cases can degrade test suite quality and disrupt the software development process, wasting developers' time. To address this challenge, we present TaRGET (Test Repair GEneraTor), a novel approach leveraging pre-trained code language models for automated test case repair. TaRGET treats test repair as a language translation task, employing a two-step process to fine-tune a language model based on essential context data characterizing the test breakage. To evaluate our approach, we introduce TaRBench, a comprehensive benchmark we developed covering 45,373 broken test repairs across 59 open-source projects. Our results demonstrate TaRGET's effectiveness, achieving a 66.1% exact match accuracy. Furthermore, our study examines the effectiveness of TaRGET across different test repair scenarios. We provide a practical guide to predict situations where the generated test repairs might be less reliable. We also explore whether project-specific data is always necessary for fine-tuning and if our approach can be effective on new projects.</article>","contentLength":1413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning a Diffusion Model Policy from Rewards via Q-Score Matching","url":"https://arxiv.org/abs/2312.11752","date":1739768400,"author":"","guid":1166,"unread":true,"content":"<article>arXiv:2312.11752v5 Announce Type: replace \nAbstract: Diffusion models have become a popular choice for representing actor policies in behavior cloning and offline reinforcement learning. This is due to their natural ability to optimize an expressive class of distributions over a continuous space. However, previous works fail to exploit the score-based structure of diffusion models, and instead utilize a simple behavior cloning term to train the actor, limiting their ability in the actor-critic setting. In this paper, we present a theoretical framework linking the structure of diffusion model policies to a learned Q-function, by linking the structure between the score of the policy to the action gradient of the Q-function. We focus on off-policy reinforcement learning and propose a new policy update method from this theory, which we denote Q-score matching. Notably, this algorithm only needs to differentiate through the denoising model rather than the entire diffusion model evaluation, and converged policies through Q-score matching are implicitly multi-modal and explorative in continuous domains. We conduct experiments in simulated environments to demonstrate the viability of our proposed method and compare to popular baselines. Source code is available from the project website: https://michaelpsenka.io/qsm.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning county from pixels: Corn yield prediction with attention-weighted multiple instance learning","url":"https://arxiv.org/abs/2312.01001","date":1739768400,"author":"","guid":1167,"unread":true,"content":"<article>arXiv:2312.01001v3 Announce Type: replace \nAbstract: Remote sensing technology has become a promising tool in yield prediction. Most prior work employs satellite imagery for county-level corn yield prediction by spatially aggregating all pixels within a county into a single value, potentially overlooking the detailed information and valuable insights offered by more granular data. To this end, this research examines each county at the pixel level and applies multiple instance learning to leverage detailed information within a county. In addition, our method addresses the \"mixed pixel\" issue caused by the inconsistent resolution between feature datasets and crop mask, which may introduce noise into the model and therefore hinder accurate yield prediction. Specifically, the attention mechanism is employed to automatically assign weights to different pixels, which can mitigate the influence of mixed pixels. The experimental results show that the developed model outperforms four other machine learning models over the past five years in the U.S. corn belt and demonstrates its best performance in 2022, achieving a coefficient of determination (R2) value of 0.84 and a root mean square error (RMSE) of 0.83. This paper demonstrates the advantages of our approach from both spatial and temporal perspectives. Furthermore, through an in-depth study of the relationship between mixed pixels and attention, it is verified that our approach can capture critical feature information while filtering out noise from mixed pixels.</article>","contentLength":1532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering","url":"https://arxiv.org/abs/2311.17331","date":1739768400,"author":"","guid":1168,"unread":true,"content":"<article>arXiv:2311.17331v4 Announce Type: replace \nAbstract: Recently, to comprehensively improve Vision Language Models (VLMs) for Visual Question Answering (VQA), several methods have been proposed to further reinforce the inference capabilities of VLMs to independently tackle VQA tasks rather than some methods that only utilize VLMs as aids to Large Language Models (LLMs). However, these methods ignore the rich common-sense knowledge inside the given VQA image sampled from the real world. Thus, they cannot fully use the powerful VLM for the given VQA question to achieve optimal performance. Attempt to overcome this limitation and inspired by the human top-down reasoning process, i.e., systematically exploring relevant issues to derive a comprehensive answer, this work introduces a novel, explainable multi-agent collaboration framework by leveraging the expansive knowledge of Large Language Models (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our framework comprises three agents, i.e., Responder, Seeker, and Integrator, to collaboratively answer the given VQA question by seeking its relevant issues and generating the final answer in such a top-down reasoning process. The VLM-based Responder agent generates the answer candidates for the question and responds to other relevant issues. The Seeker agent, primarily based on LLM, identifies relevant issues related to the question to inform the Responder agent and constructs a Multi-View Knowledge Base (MVKB) for the given visual scene by leveraging the build-in world knowledge of LLM. The Integrator agent combines knowledge from the Seeker agent and the Responder agent to produce the final VQA answer. Extensive and comprehensive evaluations on diverse VQA datasets with a variety of VLMs demonstrate the superior performance and interpretability of our framework over the baseline method in the zero-shot setting without extra training cost.</article>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry","url":"https://arxiv.org/abs/2311.16466","date":1739768400,"author":"","guid":1169,"unread":true,"content":"<article>arXiv:2311.16466v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. We analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We find that LLM usage is associated with an increased likelihood of obtaining relief from financial firms. To investigate this relationship, we employ an instrumental variable approach to mitigate endogeneity concerns around LLM adoption. Although instrumental variables suggest a potential causal link, they cannot fully capture all unobserved heterogeneity. To further establish this causal relationship, we conducted controlled experiments, which support that LLMs can enhance the clarity and persuasiveness of consumer narratives, thereby increasing the likelihood of obtaining relief. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.</article>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ephemeral Rollups are All you Need","url":"https://arxiv.org/abs/2311.02650","date":1739768400,"author":"","guid":1170,"unread":true,"content":"<article>arXiv:2311.02650v4 Announce Type: replace \nAbstract: In the realm of open and composable gaming, we envision platforms where users actively expand, create, engage, and immerse themselves in a rich world of entertainment. One promising avenue for achieving this vision is through fully on-chain (FOC) games, where both game state and logic reside on the blockchain, maximizing composability. However, we must grapple with inherent limitations and trade-offs, particularly in terms of costs and scalability. This paper proposes a framework that leverages the Solana Virtual Machine (SVM) to scale FOC games without state fragmentation or compromised trust assumptions. The framework introduces a systematic approach for discovering, utilizing, and publishing modular pieces of logic as components deeply rooted in the Entity-Component-System (ECS) pattern. To enhance scalability and resource optimization, we introduce the concept of Ephemeral Rollups (ERs) that overcome the tradeoffs of L2 horizontal scaling. These dedicated runtimes can be customized to provide higher operational speed, configurable ticking mechanisms, provable sessions and gasless transactions without composability-scalability tradeoffs.</article>","contentLength":1211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos","url":"https://arxiv.org/abs/2311.02076","date":1739768400,"author":"","guid":1171,"unread":true,"content":"<article>arXiv:2311.02076v2 Announce Type: replace \nAbstract: In gradient descent dynamics of neural networks, the top eigenvalue of the loss Hessian (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later time behavior such as progressive sharpening and edge of stability. We demonstrate that a simple $2$-layer linear network (UV model) trained on a single training example exhibits all of the essential sharpness phenomenology observed in real-world scenarios. By analyzing the structure of dynamical fixed points in function space and the vector field of function updates, we uncover the underlying mechanisms behind these sharpness trends. Our analysis reveals (i) the mechanism behind early sharpness reduction and progressive sharpening, (ii) the required conditions for edge of stability, (iii) the crucial role of initialization and parameterization, and (iv) a period-doubling route to chaos on the edge of stability manifold as learning rate is increased. Finally, we demonstrate that various predictions from this simplified model generalize to real-world scenarios and discuss its limitations.</article>","contentLength":1245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Experiencing Urban Air Mobility: How Passengers evaluate a simulated flight with an Air Taxi","url":"https://arxiv.org/abs/2311.01079","date":1739768400,"author":"","guid":1172,"unread":true,"content":"<article>arXiv:2311.01079v2 Announce Type: replace \nAbstract: For the successful development and implementation of novel concepts and technology, the acceptance of potential users is crucial. Therefore, within the project HorizonUAM of the German Aerospace Center (DLR), we investigated passengers' acceptance of air taxis. One challenge is that not many people have real experiences with urban air mobility (UAM) at the moment and thus requirements formulated by potential users refer to rather abstract concepts. To allow participants to gain realistic impressions of UAM concepts, a Mixed Reality (MR) Air Taxi Simulator was set up. In a study, 30 participants experienced an inner-city business shuttle flight. We assessed the influence of another person on board on wellbeing and information needs in nominal (experiment 1) and non-nominal situations (experiment 2). For the latter, participants experienced a re-routing of the flight due to landing side unavailability. During and after the flights, participants answered questionnaires and extensive interviews were conducted. The study produced empirical data on relevant factors regarding interaction, information needs and comfort within an air taxi. The findings show that passengers want to be informed about intentions of the vehicle. The presence of a flight attendant on board is not necessary but can increase wellbeing especially during non-nominal situations.</article>","contentLength":1418,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A polynomial bound on the number of minimal separators and potential maximal cliques in $P_6$-free graphs of bounded clique number","url":"https://arxiv.org/abs/2310.11573","date":1739768400,"author":"","guid":1173,"unread":true,"content":"<article>arXiv:2310.11573v2 Announce Type: replace \nAbstract: In this note we show a polynomial bound on the number of minimal separators and potential maximal cliques in $P_6$-free graphs of bounded clique number.</article>","contentLength":205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Local Reasoning about Probabilistic Behaviour for Classical-Quantum Programs","url":"https://arxiv.org/abs/2308.04741","date":1739768400,"author":"","guid":1174,"unread":true,"content":"<article>arXiv:2308.04741v3 Announce Type: replace \nAbstract: Verifying the functional correctness of programs with both classical and quantum constructs is a challenging task. The presence of probabilistic behaviour entailed by quantum measurements and unbounded while loops complicate the verification task greatly. We propose a new quantum Hoare logic for local reasoning about probabilistic behaviour by introducing distribution formulas to specify probabilistic properties. We show that the proof rules in the logic are sound with respect to a denotational semantics. To demonstrate the effectiveness of the logic, we formally verify the correctness of non-trivial quantum algorithms including the HHL and Shor's algorithms. Moreover, we embed our logic into the proof assistant Coq. The resulting logical framework, called CoqQLR, can facilitate semi-automated reasoning about classical--quantum programs.</article>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compositional Shape Analysis with Shared Abduction and Biabductive Loop Acceleration (Extended Version)","url":"https://arxiv.org/abs/2307.06346","date":1739768400,"author":"","guid":1175,"unread":true,"content":"<article>arXiv:2307.06346v5 Announce Type: replace \nAbstract: Biabduction-based shape analysis is a compositional verification and analysis technique that can prove memory safety in the presence of complex, linked data structures. Despite its usefulness, several open problems persist for this kind of analysis; two of which we address in this paper. On the one hand, the original analysis is path-sensitive but cannot combine safety requirements for related branches. This causes the analysis to require additional soundness checks and decreases the analysis' precision. We extend the underlying symbolic execution and propose a framework for shared abduction where a common pre-condition is maintained for related computation branches. On the other hand, prior implementations lift loop acceleration methods from forward analysis to biabduction analysis by applying them separately on the pre- and post-condition, which can lead to imprecise or even unsound acceleration results that do not form a loop invariant. In contrast, we propose biabductive loop acceleration, which explicitly constructs and checks candidate loop invariants. For this, we also introduce a novel heuristic called shape extrapolation. This heuristic takes advantage of locality in the handling of list-like data structures (which are the most common data structures found in low-level code) and jointly accelerates pre- and post-conditions by extrapolating the related shapes. In addition to making the analysis more precise, our techniques also make biabductive analysis more efficient since they are sound in just one analysis phase. In contrast, prior techniques always require two phases (as the first phase can produce contracts that are unsound and must hence be verified). We experimentally confirm that our techniques improve on prior techniques; both in terms of precision and runtime of the analysis.</article>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Applying Process Mining on Scientific Workflows: a Case Study on High Performance Computing Data","url":"https://arxiv.org/abs/2307.02833","date":1739768400,"author":"","guid":1176,"unread":true,"content":"<article>arXiv:2307.02833v2 Announce Type: replace \nAbstract: Computer-based scientific experiments are becoming increasingly data-intensive, necessitating the use of High-Performance Computing (HPC) clusters to handle large scientific workflows. These workflows result in complex data and control flows within the system, making analysis challenging. This paper focuses on the extraction of case IDs from SLURM-based HPC cluster logs, a crucial step for applying mainstream process mining techniques. The core contribution is the development of methods to correlate jobs in the system, whether their interdependencies are explicitly specified or not. We present our log extraction and correlation techniques, supported by experiments that validate our approach, enabling comprehensive documentation of workflows and identification of performance bottlenecks.</article>","contentLength":850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption","url":"https://arxiv.org/abs/2307.00677","date":1739768400,"author":"","guid":1177,"unread":true,"content":"<article>arXiv:2307.00677v5 Announce Type: replace \nAbstract: Density-based clustering is the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as they are separated by low-density regions. However, a high-density region that is not separated by low-density ones might also have different structures belonging to multiple clusters. As far as we know, all previous density-based clustering algorithms fail to detect such structures. In this paper, we provide a novel density-based clustering scheme to address this problem. It is the rst clustering algorithm that can detect meticulous structures in a high-density region that is not separated by low-density ones and thus extends the range of applications of clustering. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, called Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption, dubbed SDC-HSDD-NDSA. Experiments on synthetic and real datasets are implemented to verify the effectiveness, robustness, and granularity independence of the algorithm, and the scheme is compared to unsupervised schemes in the Python package Scikit-learn. Results demonstrate that our algorithm outperforms previous ones in many situations, especially significantly when clusters have regular internal structures. For example, averaging over the eight noiseless synthetic datasets with structures employing ARI and NMI criteria, previous algorithms obtain scores below 0.6 and 0.7, while the presented algorithm obtains scores higher than 0.9 and 0.95, respectively.</article>","contentLength":1674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does my medical AI look at pictures of birds? Exploring the efficacy of transfer learning across domain boundaries","url":"https://arxiv.org/abs/2306.17555","date":1739768400,"author":"","guid":1178,"unread":true,"content":"<article>arXiv:2306.17555v2 Announce Type: replace \nAbstract: It is an open secret that ImageNet is treated as the panacea of pretraining. Particularly in medical machine learning, models not trained from scratch are often finetuned based on ImageNet-pretrained models. We posit that pretraining on data from the domain of the downstream task should almost always be preferred instead. We leverage RadNet-12M, a dataset containing more than 12 million computed tomography (CT) image slices, to explore the efficacy of self-supervised pretraining on medical and natural images. Our experiments cover intra- and cross-domain transfer scenarios, varying data scales, finetuning vs. linear evaluation, and feature space analysis. We observe that intra-domain transfer compares favorably to cross-domain transfer, achieving comparable or improved performance (0.44% - 2.07% performance increase using RadNet pretraining, depending on the experiment) and demonstrate the existence of a domain boundary-related generalization gap and domain-specific learned features.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detecting Phishing Sites Using ChatGPT","url":"https://arxiv.org/abs/2306.05816","date":1739768400,"author":"","guid":1179,"unread":true,"content":"<article>arXiv:2306.05816v3 Announce Type: replace \nAbstract: The emergence of Large Language Models (LLMs), including ChatGPT, is having a significant impact on a wide range of fields. While LLMs have been extensively researched for tasks such as code generation and text synthesis, their application in detecting malicious web content, particularly phishing sites, has been largely unexplored. To combat the rising tide of cyber attacks due to the misuse of LLMs, it is important to automate detection by leveraging the advanced capabilities of LLMs.\n  In this paper, we propose a novel system called ChatPhishDetector that utilizes LLMs to detect phishing sites. Our system involves leveraging a web crawler to gather information from websites, generating prompts for LLMs based on the crawled data, and then retrieving the detection results from the responses generated by the LLMs. The system enables us to detect multilingual phishing sites with high accuracy by identifying impersonated brands and social engineering techniques in the context of the entire website, without the need to train machine learning models. To evaluate the performance of our system, we conducted experiments on our own dataset and compared it with baseline systems and several LLMs. The experimental results using GPT-4V demonstrated outstanding performance, with a precision of 98.7% and a recall of 99.6%, outperforming the detection results of other LLMs and existing systems. These findings highlight the potential of LLMs for protecting users from online fraudulent activities and have important implications for enhancing cybersecurity measures.</article>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pavlok-Nudge: A Feedback Mechanism for Atomic Behaviour Modification with Snoring Usecase","url":"https://arxiv.org/abs/2305.06110","date":1739768400,"author":"","guid":1180,"unread":true,"content":"<article>arXiv:2305.06110v4 Announce Type: replace \nAbstract: This paper proposes an atomic behaviour intervention strategy using Pavlok device. Pavlok utilises beeps, vibration and shocks as a mode of aversion technique to help individuals with behaviour modification. While the device can be useful in certain periodic daily life situations, like alarms and exercise notifications, the device relies on manual operations that limit its usage. To automate behaviour modification, we propose a framework that first detects targeted behaviours through a lightweight deep learning model and subsequently nudges the user through Pavlok. Our proposed solution is implemented and verified in the context of snoring, which captures audio from the environment following a prediction of whether the audio content is a snore or not using a 1D convolutional neural network. Based on the prediction, we use Pavlok to nudge users for preventive measures, such as a change in sleeping posture. We believe that this simple solution can help people change their atomic habits, which may lead to long-term health benefits. Our proposed real-time, lightweight model (99.8% fewer parameters over SOTA; 1,278,049 --&gt; 1337) achieves SOTA performance (test accuracy of 0.99) on a public benchmark. The code and model are publicly available at https://github.com/hasan-rakibul/pavlok-nudge-snore.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cure is in the Cause: A Filesystem for Container Debloating","url":"https://arxiv.org/abs/2305.04641","date":1739768400,"author":"","guid":1181,"unread":true,"content":"<article>arXiv:2305.04641v3 Announce Type: replace \nAbstract: Containers have become a standard for deploying applications due to their convenience, but they often suffer from significant software bloat-unused files that inflate image sizes, increase provisioning times, and waste resources. These inefficiencies are particularly problematic in serverless and edge computing scenarios, where resources are constrained, and performance is critical. Existing debloating tools are limited in scope and effectiveness, failing to address the widespread issue of container bloat at scale. In this paper, we conduct a large-scale evaluation of container bloat, analyzing the top 20 most downloaded containers on DockerHub. We evaluate two state-of-the-art debloating tools, identify their limitations, and propose a novel solution, BAFFS, which addresses bloat at the filesystem level by introducing a flexible debloating layer that preserves the layered structure of container filesystems. The debloating layer can be organized in different ways to meet diverse requirements. Our evaluation demonstrates that over 50% of the top-downloaded containers have more than 60% bloat, and BAFFS reduces container sizes significantly while maintaining functionality. For serverless functions, BAFFS reduces cold start latency by up to 68%. Additionally, when combined with lazy-loading snapshotters, BAFFS enhances provisioning efficiency, reducing conversion times by up to 93% and provisioning times by up to 19%.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coevolution of Camouflage","url":"https://arxiv.org/abs/2304.11793","date":1739768400,"author":"","guid":1182,"unread":true,"content":"<article>arXiv:2304.11793v3 Announce Type: replace \nAbstract: Camouflage in nature seems to arise from competition between predator and prey. To survive, predators must find prey, and prey must avoid being found. This work simulates an abstract model of that adversarial relationship. It looks at crypsis through evolving prey camouflage patterns (as color textures) in competition with evolving predator vision. During their \"lifetime\" predators learn to better locate camouflaged prey. The environment for this 2D simulation is provided by a set of photographs, typically of natural scenes. This model is based on two evolving populations, one of prey and another of predators. Mutual conflict between these populations can produce both effective prey camouflage and predators skilled at \"breaking\" camouflage. The result is an open source artificial life model to help study camouflage in nature, and the perceptual phenomenon of camouflage more generally.</article>","contentLength":950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models","url":"https://arxiv.org/abs/2304.00228","date":1739768400,"author":"","guid":1183,"unread":true,"content":"<article>arXiv:2304.00228v3 Announce Type: replace \nAbstract: Search engines increasingly leverage large language models (LLMs) to generate direct answers, and AI chatbots now access the Internet for fresh data. As information curators for billions of users, LLMs must assess the accuracy and reliability of different sources. This paper audits nine widely used LLMs from three leading providers -- OpenAI, Google, and Meta -- to evaluate their ability to discern credible and high-quality information sources from low-credibility ones. We find that while LLMs can rate most tested news outlets, larger models more frequently refuse to provide ratings due to insufficient information, whereas smaller models are more prone to making errors in their ratings. For sources where ratings are provided, LLMs exhibit a high level of agreement among themselves (average Spearman's $\\rho = 0.79$), but their ratings align only moderately with human expert evaluations (average $\\rho = 0.50$). Analyzing news sources with different political leanings in the US, we observe a liberal bias in credibility ratings yielded by all LLMs in default configurations. Additionally, assigning partisan roles to LLMs consistently induces strong politically congruent bias in their ratings. These findings have important implications for the use of LLMs in curating news and political information.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Connected Trading Cycles","url":"https://arxiv.org/abs/2303.09759","date":1739768400,"author":"","guid":1184,"unread":true,"content":"<article>arXiv:2303.09759v2 Announce Type: replace \nAbstract: Incentivizing the existing participants to invite new participants to join an auction, matching or cooperative game have been extensively studied recently. One common challenge to design such incentive in these games is that the invitees and inviters are competitors. To have such an incentive, we normally have to sacrifice some of the traditional properties. Especially, in a housing market (one kind of one-sided matching), we cannot maintain the traditional stability and optimality. The previous studies proposed some new matching mechanisms to have the invitation incentive (part of the incentive compatibility), but did not have any guarantee on stability and optimality.\n  In this paper, we propose new notions of stability and optimality which are achievable with incentive compatibility. We weaken stability and optimality on a special structure (complete components) on networks. We first prove that the weakened notions are the best we can achieve with incentive compatibility. Then, we propose three mechanisms (Swap With Neighbors, Leave and Share, and Connected Trading Cycles) to satisfy the desirable properties. Connected Trading Cycles is the first mechanism to satisfy the best stability and optimality compatible with incentive compatibility.</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Task Aware Dreamer for Task Generalization in Reinforcement Learning","url":"https://arxiv.org/abs/2303.05092","date":1739768400,"author":"","guid":1185,"unread":true,"content":"<article>arXiv:2303.05092v4 Announce Type: replace \nAbstract: A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. The ability to generalize across tasks is important as it determines an agent's adaptability to real-world scenarios where reward mechanisms might vary. In this work, we first show that training a general world model can utilize similar structures in these tasks and help train more generalizable agents. Extending world models into the task generalization setting, we introduce a novel method named Task Aware Dreamer (TAD), which integrates reward-informed features to identify consistent latent characteristics across tasks. Within TAD, we compute the variational lower bound of sample data log-likelihood, which introduces a new term designed to differentiate tasks using their states, as the optimization objective of our reward-informed world models. To demonstrate the advantages of the reward-informed policy in TAD, we introduce a new metric called Task Distribution Relevance (TDR) which quantitatively measures the relevance of different tasks. For tasks exhibiting a high TDR, i.e., the tasks differ significantly, we illustrate that Markovian policies struggle to distinguish them, thus it is necessary to utilize reward-informed policies in TAD. Extensive experiments in both image-based and state-based tasks show that TAD can significantly improve the performance of handling different tasks simultaneously, especially for those with high TDR, and display a strong generalization ability to unseen tasks.</article>","contentLength":1680,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intermittently Observable Markov Decision Processes","url":"https://arxiv.org/abs/2302.11761","date":1739768400,"author":"","guid":1186,"unread":true,"content":"<article>arXiv:2302.11761v2 Announce Type: replace \nAbstract: This paper investigates MDPs with intermittent state information. We consider a scenario where the controller perceives the state information of the process via an unreliable communication channel. The transmissions of state information over the whole time horizon are modeled as a Bernoulli lossy process. Hence, the problem is finding an optimal policy for selecting actions in the presence of state information losses. We first formulate the problem as a belief MDP to establish structural results. The effect of state information losses on the expected total discounted reward is studied systematically. Then, we reformulate the problem as a tree MDP whose state space is organized in a tree structure. Two finite-state approximations to the tree MDP are developed to find near-optimal policies efficiently. Finally, we put forth a nested value iteration algorithm for the finite-state approximations, which is proved to be faster than standard value iteration. Numerical results demonstrate the effectiveness of our methods.</article>","contentLength":1082,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do PAC-Learners Learn the Marginal Distribution?","url":"https://arxiv.org/abs/2302.06285","date":1739768400,"author":"","guid":1187,"unread":true,"content":"<article>arXiv:2302.06285v2 Announce Type: replace \nAbstract: The Fundamental Theorem of PAC Learning asserts that learnability of a concept class $H$ is equivalent to the $\\textit{uniform convergence}$ of empirical error in $H$ to its mean, or equivalently, to the problem of $\\textit{density estimation}$, learnability of the underlying marginal distribution with respect to events in $H$. This seminal equivalence relies strongly on PAC learning's `distribution-free' assumption, that the adversary may choose any marginal distribution over data. Unfortunately, the distribution-free model is known to be overly adversarial in practice, failing to predict the success of modern machine learning algorithms, but without the Fundamental Theorem our theoretical understanding of learning under distributional constraints remains highly limited.\n  In this work, we revisit the connection between PAC learning, uniform convergence, and density estimation beyond the distribution-free setting when the adversary is restricted to choosing a marginal distribution from a known family $\\mathscr{P}$. We prove that while the traditional Fundamental Theorem indeed fails, a finer-grained connection between the three fundamental notions continues to hold:\n  1. PAC-Learning is strictly sandwiched between two refined models of density estimation, both equivalent to standard density estimation in the distribution-free case, differing only in whether the learner $\\textit{knows}$ the set of well-estimated events in $H$.\n  2. Under reasonable assumptions on $H$ and $\\mathscr{P}$, density estimation is equivalent to \\emph{uniform estimation}, a relaxation of uniform convergence allowing non-empirical estimators.\n  Together, our results give a clearer picture of how the Fundamental Theorem extends beyond the distribution-free setting and shed new light on the classically challenging problem of learning under arbitrary distributional assumptions.</article>","contentLength":1934,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated Learning","url":"https://arxiv.org/abs/2302.01706","date":1739768400,"author":"","guid":1188,"unread":true,"content":"<article>arXiv:2302.01706v2 Announce Type: replace \nAbstract: This paper presents the application of Vertical Federated Learning (VFL) to generate synthetic tabular data using Generative Adversarial Networks (GANs). VFL is a collaborative approach to train machine learning models among distinct tabular data holders, such as financial institutions, who possess disjoint features for the same group of customers. In this paper we introduce the VT-GAN framework, Vertical federated Tabular GAN, and demonstrate that VFL can be successfully used to implement GANs for distributed tabular data in privacy-preserving manner, with performance close to centralized GANs that assume shared data. We make design choices with respect to the distribution of GAN generator and discriminator models and introduce a training-with-shuffling technique so that no party can reconstruct training data from the GAN conditional vector. The paper presents (1) an implementation of VT-GAN, (2) a detailed quality evaluation of the VT-GAN-generated synthetic data, (3) an overall scalability examination of VT-GAN framework, (4) a security analysis on VT-GAN's robustness against Membership Inference Attack with different settings of Differential Privacy, for a range of datasets with diverse distribution characteristics. Our results demonstrate that VT-GAN can consistently generate high-fidelity synthetic tabular data of comparable quality to that generated by a centralized GAN algorithm. The difference in machine learning utility can be as low as 2.7%, even under extremely imbalanced data distributions across clients or with different numbers of clients.</article>","contentLength":1633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Decouple Complex Systems","url":"https://arxiv.org/abs/2302.01581","date":1739768400,"author":"","guid":1189,"unread":true,"content":"<article>arXiv:2302.01581v2 Announce Type: replace \nAbstract: A complex system with cluttered observations may be a coupled mixture of multiple simple sub-systems corresponding to latent entities. Such sub-systems may hold distinct dynamics in the continuous-time domain; therein, complicated interactions between sub-systems also evolve over time. This setting is fairly common in the real world but has been less considered. In this paper, we propose a sequential learning approach under this setting by decoupling a complex system for handling irregularly sampled and cluttered sequential observations. Such decoupling brings about not only subsystems describing the dynamics of each latent entity but also a meta-system capturing the interaction between entities over time. Specifically, we argue that the meta-system evolving within a simplex is governed by projected differential equations (ProjDEs). We further analyze and provide neural-friendly projection operators in the context of Bregman divergence. Experimental results on synthetic and real-world datasets show the advantages of our approach when facing complex and cluttered sequential data compared to the state-of-the-art.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Typed Lambda-Calculus for Establishing Trust in Probabilistic Programs","url":"https://arxiv.org/abs/2302.00958","date":1739768400,"author":"","guid":1190,"unread":true,"content":"<article>arXiv:2302.00958v3 Announce Type: replace \nAbstract: The extensive deployment of probabilistic algorithms has radically changed our perspective on several well-established computational notions. Correctness is probably the most basic one. While a typical probabilistic program cannot be said to compute the correct result, we often have quite strong expectations about the frequency with which it should return certain outputs. In these cases, trust as a generalisation of correctness fares better. One way to understand it is to say that a probabilistic computational process is trustworthy if the frequency of its outputs is compliant with a probability distribution which models its expected behaviour. We present a formal computational framework that formalises this idea. In order to do so, we define a typed lambda-calculus that features operators for conducting experiments at runtime on probabilistic programs and for evaluating whether they compute outputs as determined by a target probability distribution. After proving some fundamental computational properties of the calculus, such as progress and termination, we define a static notion of confidence that allows to prove that our notion of trust behaves correctly with respect to the basic tenets of probability theory.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Training Neural Networks on Data Sources with Unknown Reliability","url":"https://arxiv.org/abs/2212.02895","date":1739768400,"author":"","guid":1191,"unread":true,"content":"<article>arXiv:2212.02895v4 Announce Type: replace \nAbstract: When data is generated by multiple sources, conventional training methods update models assuming equal reliability for each source and do not consider their individual data quality. However, in many applications, sources have varied levels of reliability that can have negative effects on the performance of a neural network. A key issue is that often the quality of the data for individual sources is not known during training. Previous methods for training models in the presence of noisy data do not make use of the additional information that the source label can provide. Focusing on supervised learning, we aim to train neural networks on each data source for a number of steps proportional to the source's estimated reliability by using a dynamic re-weighting strategy motivated by likelihood tempering. This way, we allow training on all sources during the warm-up and reduce learning on less reliable sources during the final training stages, when it has been shown that models overfit to noise. We show through diverse experiments that this can significantly improve model performance when trained on mixtures of reliable and unreliable data sources, and maintain performance when models are trained on reliable sources only.</article>","contentLength":1288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Inclusion for Boundedly-Ambiguous Vector Addition Systems is Decidable","url":"https://arxiv.org/abs/2202.08033","date":1739768400,"author":"","guid":1192,"unread":true,"content":"<article>arXiv:2202.08033v5 Announce Type: replace \nAbstract: We consider the problems of language inclusion and language equivalence for Vector Addition Systems with States (VASS) with the acceptance condition defined by the set of accepting states (and more generally by some upward-closed conditions). In general, the problem of language equivalence is undecidable even for one-dimensional VASS, thus to get decidability we investigate restricted subclasses. On the one hand, we show that the problem of language inclusion of a VASS in k-ambiguous VASS (for any natural k) is decidable and even in Ackermann. On the other hand, we prove that the language equivalence problem is already Ackermann-hard for deterministic VASS. These two results imply Ackermann-completeness for language inclusion and equivalence in several possible restrictions. Some of our techniques can be also applied in much broader generality in infinite-state systems, namely for some subclass of well-structured transition systems.</article>","contentLength":999,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proof complexity of positive branching programs","url":"https://arxiv.org/abs/2102.06673","date":1739768400,"author":"","guid":1193,"unread":true,"content":"<article>arXiv:2102.06673v4 Announce Type: replace \nAbstract: We investigate the proof complexity of systems based on positive branching programs, i.e. non-deterministic branching programs (NBPs) where, for any 0-transition between two nodes, there is also a 1-transition. Positive NBPs compute monotone Boolean functions, just like negation-free circuits or formulas, but constitute a positive version of (non-uniform) NL, rather than P or NC1, respectively.\n  The proof complexity of NBPs was investigated in previous work by Buss, Das and Knop, using extension variables to represent the dag-structure, over a language of (non-deterministic) decision trees, yielding the system eLNDT. Our system eLNDT+ is obtained by restricting their systems to a positive syntax, similarly to how the 'monotone sequent calculus' MLK is obtained from the usual sequent calculus LK by restricting to negation-free formulas.\n  Our main result is that eLNDT+ polynomially simulates eLNDT over positive sequents. Our proof method is inspired by a similar result for MLK by Atserias, Galesi and Pudl\\'ak, that was recently improved to a bona fide polynomial simulation via works of Je\\v{r}\\'abek and Buss, Kabanets, Kolokolova and Kouck\\'y. Along the way we formalise several properties of counting functions within eLNDT+ by polynomial-size proofs and, as a case study, give explicit polynomial-size poofs of the propositional pigeonhole principle.</article>","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zone Theorem for Arrangements in three dimensions","url":"https://arxiv.org/abs/2006.01428","date":1739768400,"author":"","guid":1194,"unread":true,"content":"<article>arXiv:2006.01428v2 Announce Type: replace \nAbstract: In this note, a simple description of zone theorem in three dimensions is given.Arrangements in three dimensions are useful for constructing higher-order Voronoi diagrams in plane. An elementary and very intuitive treatment of this result is also given.</article>","contentLength":306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Euler Factors of Elliptic Curves","url":"https://arxiv.org/abs/2502.10357","date":1739768400,"author":"","guid":1195,"unread":true,"content":"<article>arXiv:2502.10357v1 Announce Type: cross \nAbstract: We apply transformer models and feedforward neural networks to predict Frobenius traces $a_p$ from elliptic curves given other traces $a_q$. We train further models to predict $a_p \\bmod 2$ from $a_q \\bmod 2$, and cross-analysis such as $a_p \\bmod 2$ from $a_q$. Our experiments reveal that these models achieve high accuracy, even in the absence of explicit number-theoretic tools like functional equations of $L$-functions. We also present partial interpretability findings.</article>","contentLength":527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Studying number theory with deep learning: a case study with the M\\\"obius and squarefree indicator functions","url":"https://arxiv.org/abs/2502.10335","date":1739768400,"author":"","guid":1196,"unread":true,"content":"<article>arXiv:2502.10335v1 Announce Type: cross \nAbstract: Building on work of Charton, we train small transformer models to calculate the M\\\"obius function $\\mu(n)$ and the squarefree indicator function $\\mu^2(n)$. The models attain nontrivial predictive power. We then iteratively train additional models to understand how the model functions, ultimately finding a theoretical explanation.</article>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalised Parallel Tempering: Flexible Replica Exchange via Flows and Diffusions","url":"https://arxiv.org/abs/2502.10328","date":1739768400,"author":"","guid":1197,"unread":true,"content":"<article>arXiv:2502.10328v1 Announce Type: cross \nAbstract: Parallel Tempering (PT) is a classical MCMC algorithm designed for leveraging parallel computation to sample efficiently from high-dimensional, multimodal or otherwise complex distributions via annealing. One limitation of the standard formulation of PT is the growth of computational resources required to generate high-quality samples, as measured by effective sample size or round trip rate, for increasingly challenging distributions. To address this issue, we propose the framework: Generalised Parallel Tempering (GePT) which allows for the incorporation of recent advances in modern generative modelling, such as normalising flows and diffusion models, within Parallel Tempering, while maintaining the same theoretical guarantees as MCMC-based methods. For instance, we show that this allows us to utilise diffusion models in a parallelised manner, bypassing the usual computational cost of a large number of steps to generate quality samples. Further, we empirically demonstrate that GePT can improve sample quality and reduce the growth of computational resources required to handle complex distributions over the classical algorithm.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting","url":"https://arxiv.org/abs/2502.10235","date":1739768400,"author":"","guid":1198,"unread":true,"content":"<article>arXiv:2502.10235v1 Announce Type: cross \nAbstract: Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Network fault costs based on minimum leaf spanning trees","url":"https://arxiv.org/abs/2502.10213","date":1739768400,"author":"","guid":1199,"unread":true,"content":"<article>arXiv:2502.10213v1 Announce Type: cross \nAbstract: We study the fault-tolerance of networks from both the structural and computational point of view using the minimum leaf number of the corresponding graph $G$, i.e. the minimum number of leaves of the spanning trees of $G$, and its vertex-deleted subgraphs. We investigate networks that are leaf-guaranteed, i.e. which satisfy a certain stability condition with respect to minimum leaf numbers and vertex-deletion. Next to this, our main notion is the so-called fault cost, which is based on the number of vertices that have different degrees in minimum leaf spanning trees of the network and its vertex-deleted subgraphs. We characterise networks with vanishing fault cost via leaf-guaranteed graphs and describe, for any given network $N$, leaf-guaranteed networks containing $N$. We determine for all non-negative integers $k \\le 8$ except $1$ the smallest network with fault cost $k$. We also give a detailed treatment of the fault cost $1$ case, prove that there are infinitely many $3$-regular networks with fault cost $3$, and show that for any non-negative integer $k$ there exists a network with fault cost exactly $k$.</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic End-to-End De Novo Protein Design for Tailored Dynamics Using a Language Diffusion Model","url":"https://arxiv.org/abs/2502.10173","date":1739768400,"author":"","guid":1200,"unread":true,"content":"<article>arXiv:2502.10173v1 Announce Type: cross \nAbstract: Proteins are dynamic molecular machines whose biological functions, spanning enzymatic catalysis, signal transduction, and structural adaptation, are intrinsically linked to their motions. Designing proteins with targeted dynamic properties, however, remains a challenge due to the complex, degenerate relationships between sequence, structure, and molecular motion. Here, we introduce VibeGen, a generative AI framework that enables end-to-end de novo protein design conditioned on normal mode vibrations. VibeGen employs an agentic dual-model architecture, comprising a protein designer that generates sequence candidates based on specified vibrational modes and a protein predictor that evaluates their dynamic accuracy. This approach synergizes diversity, accuracy, and novelty during the design process. Via full-atom molecular simulations as direct validation, we demonstrate that the designed proteins accurately reproduce the prescribed normal mode amplitudes across the backbone while adopting various stable, functionally relevant structures. Notably, generated sequences are de novo, exhibiting no significant similarity to natural proteins, thereby expanding the accessible protein space beyond evolutionary constraints. Our work integrates protein dynamics into generative protein design, and establishes a direct, bidirectional link between sequence and vibrational behavior, unlocking new pathways for engineering biomolecules with tailored dynamical and functional properties. This framework holds broad implications for the rational design of flexible enzymes, dynamic scaffolds, and biomaterials, paving the way toward dynamics-informed AI-driven protein engineering.</article>","contentLength":1736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing anomaly detection with topology-aware autoencoders","url":"https://arxiv.org/abs/2502.10163","date":1739768400,"author":"","guid":1201,"unread":true,"content":"<article>arXiv:2502.10163v1 Announce Type: cross \nAbstract: Anomaly detection in high-energy physics is essential for identifying new physics beyond the Standard Model. Autoencoders provide a signal-agnostic approach but are limited by the topology of their latent space. This work explores topology-aware autoencoders, embedding phase-space distributions onto compact manifolds that reflect energy-momentum conservation. We construct autoencoders with spherical ($S^n$), product ($S^2 \\otimes S^2$), and projective ($\\mathbb{RP}^2$) latent spaces and compare their anomaly detection performance against conventional Euclidean embeddings. Our results show that autoencoders with topological priors significantly improve anomaly separation by preserving the global structure of the data manifold and reducing spurious reconstruction errors. Applying our approach to simulated hadronic top-quark decays, we show that latent spaces with appropriate topological constraints enhance sensitivity and robustness in detecting anomalous events. This study establishes topology-aware autoencoders as a powerful tool for unsupervised searches for new physics in particle-collision data.</article>","contentLength":1166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses","url":"https://arxiv.org/abs/2502.10161","date":1739768400,"author":"","guid":1202,"unread":true,"content":"<article>arXiv:2502.10161v1 Announce Type: cross \nAbstract: Reasoning about fairness through correlation-based notions is rife with pitfalls. The 1973 University of California, Berkeley graduate school admissions case from Bickel et. al. (1975) is a classic example of one such pitfall, namely Simpson's paradox. The discrepancy in admission rates among males and female applicants, in the aggregate data over all departments, vanishes when admission rates per department are examined. We reason about the Berkeley graduate school admissions case through a causal lens. In the process, we introduce a statistical test for causal hypothesis testing based on Pearl's instrumental-variable inequalities (Pearl 1995). We compare different causal notions of fairness that are based on graphical, counterfactual and interventional queries on the causal model, and develop statistical tests for these notions that use only observational data. We study the logical relations between notions, and show that while notions may not be equivalent, their corresponding statistical tests coincide for the case at hand. We believe that a thorough case-based causal analysis helps develop a more principled understanding of both causal hypothesis testing and fairness.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Combinatorial Reinforcement Learning with Preference Feedback","url":"https://arxiv.org/abs/2502.10158","date":1739768400,"author":"","guid":1203,"unread":true,"content":"<article>arXiv:2502.10158v1 Announce Type: cross \nAbstract: In this paper, we consider combinatorial reinforcement learning with preference feedback, where a learning agent sequentially offers an action--an assortment of multiple items to--a user, whose preference feedback follows a multinomial logistic (MNL) model. This framework allows us to model real-world scenarios, particularly those involving long-term user engagement, such as in recommender systems and online advertising. However, this framework faces two main challenges: (1) the unknown value of each item, unlike traditional MNL bandits that only address single-step preference feedback, and (2) the difficulty of ensuring optimism while maintaining tractable assortment selection in the combinatorial action space with unknown values. In this paper, we assume a contextual MNL preference model, where the mean utilities are linear, and the value of each item is approximated by a general function. We propose an algorithm, MNL-VQL, that addresses these challenges, making it both computationally and statistically efficient. As a special case, for linear MDPs (with the MNL preference feedback), we establish the first regret lower bound in this framework and show that MNL-VQL achieves nearly minimax-optimal regret. To the best of our knowledge, this is the first work to provide statistical guarantees in combinatorial RL with preference feedback.</article>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Characterization of Logarithmic Fekete Critical Configurations of at Most Six Points in All Dimensions","url":"https://arxiv.org/abs/2502.10152","date":1739768400,"author":"","guid":1204,"unread":true,"content":"<article>arXiv:2502.10152v1 Announce Type: cross \nAbstract: We consider the logarithmic Fekete problem, which consists of placing a fixed number of points on the unit sphere in $\\mathbb{R}^d$, in such a way that the product of all pairs of mutual Euclidean distances is maximized or, equivalently, so that their logarithmic energy is minimized. Using tools from Computational Algebraic Geometry, we find and classify all critical configurations for this problem when considering at most six points in every dimension $d$. Our results discover some previously unknown optimal configurations and give the first reported case of a spurious local minimum for the Fekete problem.</article>","contentLength":665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pangraphs as models of higher-order interactions","url":"https://arxiv.org/abs/2502.10141","date":1739768400,"author":"","guid":1205,"unread":true,"content":"<article>arXiv:2502.10141v1 Announce Type: cross \nAbstract: Graphs depict pairwise relationships between objects within a system. Higher-order interactions (HOIs), which involve more than two objects simultaneously, are common in nature. Such interactions can change the stability of a complex system. Hypergraphs can represent an HOI as an arbitrary subset of vertices. However, they fail to capture the specific roles of the vertices involved, which can be highly asymmetric, particularly in the case of interaction modifications.\n  We introduce pangraphs, a robust and quantitative generalisation of graphs that accurately captures arbitrarily complex higher-order interactions. We demonstrate that several higher-order representations proposed in the literature are specific instances of pangraphs. Additionally, we introduce an incidence multilayer digraph representation of a pangraph, referred to as Levi digraph. We adapt degree and Katz centrality measures to the pangraph framework and show that a consistent generalisation of recursive graph measures cannot be simplified to a Levi digraph of a pangraph.\n  We construct a pangraph for a real-world coffee agroecosystem and compare Katz centrality between its dihypergraph and pangraph representations, both analytically and numerically. The choice of representation significantly affects centrality values and alters vertex ranks. Additionally, we emphasise the use of real-valued incidence matrices to quantify interaction strengths and the roles of vertices within the system.</article>","contentLength":1530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Consecutive and quasi-consecutive patterns: $\\mathrm{des}$-Wilf classifications and generating functions","url":"https://arxiv.org/abs/2502.10128","date":1739768400,"author":"","guid":1206,"unread":true,"content":"<article>arXiv:2502.10128v1 Announce Type: cross \nAbstract: Motivated by a correlation between the distribution of descents over permutations that avoid a consecutive pattern and those avoiding the respective quasi-consecutive pattern, as established in this paper, we obtain a complete $\\des$-Wilf classification for quasi-consecutive patterns of length up to 4. For equivalence classes containing more than one pattern, we construct various descent-preserving bijections to establish the equivalences, which lead to the provision of proper versions of two incomplete bijective arguments previously published in the literature. Additionally, for two singleton classes, we derive explicit bivariate generating functions using the generalized run theorem.</article>","contentLength":745,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strain-Induced Optical and Molecular Transformations in PET Films for Organic Electronic Applications","url":"https://arxiv.org/abs/2502.10113","date":1739768400,"author":"","guid":1207,"unread":true,"content":"<article>arXiv:2502.10113v1 Announce Type: cross \nAbstract: Poly(ethylene terephthalate) (PET) films are widely used in flexible electronics and optoelectronics, where their mechanical durability and optical performance under strain are essential for device reliability. This study investigates the impact of applied mechanical strain on the optical and molecular properties of PET at room temperature,using UV-Vis absorption and Raman spectroscopy. The work explores how varying strain levels, from 0% (unstretched) to 30%, affect the transparency, vibrational modes, and molecular reorganization within PET films. UV-Vis absorbance measurements reveal that strain induces significant changes in the light transmission properties of PET, particularly in the visible range, and increases absorption in the UVA and visible region by up to 100%. Raman spectra indicate that strain levels higher than 5% lead to irreversible shifts of vibrational lines, accompanied by an increase of their full width at half maximum (FWHM), suggesting molecular reorientation and crystallinity changes. The phonon mode coupled with C-O stretching [O-CH2] shows the strongest response to applied mechanical stress. This study provides a comprehensive understanding of strain-induced optical and structural alterations in PET, with implications for improving the mechanical and optical performance of PET-based devices in strainsensitive applications, such as organic solar cells (OSCs), organic light-emitting diodes (OLEDs), and flexible sensors.</article>","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An adaptive importance sampling algorithm for risk-averse optimization","url":"https://arxiv.org/abs/2502.10084","date":1739768400,"author":"","guid":1208,"unread":true,"content":"<article>arXiv:2502.10084v1 Announce Type: cross \nAbstract: Adaptive sampling algorithms are modern and efficient methods that dynamically adjust the sample size throughout the optimization process. However, they may encounter difficulties in risk-averse settings, particularly due to the challenge of accurately sampling from the tails of the underlying distribution of random inputs. This often leads to a much faster growth of the sample size compared to risk-neutral problems. In this work, we propose a novel adaptive sampling algorithm that adapts both the sample size and the sampling distribution at each iteration. The biasing distributions are constructed on the fly, leveraging a reduced-order model of the objective function to be minimized, and are designed to oversample a so-called risk region. As a result, a reduction of the variance of the gradients is achieved, which permits to use fewer samples per iteration compared to a standard algorithm, while still preserving the asymptotic convergence rate. Our focus is on the minimization of the Conditional Value-at-Risk (CVaR), and we establish the convergence of the proposed computational framework. Numerical experiments confirm the substantial computational savings achieved by our approach.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improved Online Confidence Bounds for Multinomial Logistic Bandits","url":"https://arxiv.org/abs/2502.10020","date":1739768400,"author":"","guid":1209,"unread":true,"content":"<article>arXiv:2502.10020v1 Announce Type: cross \nAbstract: In this paper, we propose an improved online confidence bound for multinomial logistic (MNL) models and apply this result to MNL bandits, achieving variance-dependent optimal regret. Recently, Lee &amp; Oh (2024) established an online confidence bound for MNL models and achieved nearly minimax-optimal regret in MNL bandits. However, their results still depend on the norm-boundedness of the unknown parameter $B$ and the maximum size of possible outcomes $K$. To address this, we first derive an online confidence bound of $O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over the previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee &amp; Oh, 2024). This is mainly achieved by establishing tighter self-concordant properties of the MNL loss and introducing a novel intermediary term to bound the estimation error. Using this new online confidence bound, we propose a constant-time algorithm, OFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log T \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $ for sufficiently large $T$, where $\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$ is the dimension of the contexts, and $T$ is the total number of rounds. Furthermore, we introduce an Maximum Likelihood Estimation (MLE)-based algorithm that achieves an anytime, OFU-MN$^2$L, poly($(B)$)-free regret of $O \\Big( d \\log (BT) \\sqrt{ \\smash[b]{\\sum_{t=1}^T} \\sigma_t^2 } \\Big) $.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strength and partition rank under limits and field extensions","url":"https://arxiv.org/abs/2502.10007","date":1739768400,"author":"","guid":1210,"unread":true,"content":"<article>arXiv:2502.10007v1 Announce Type: cross \nAbstract: The strength of a multivariate homogeneous polynomial is the minimal number of terms in an expression as a sum of products of lower-degree homogeneous polynomials. Partition rank is the analogue for multilinear forms. Both ranks can drop under field extensions, and both can jump in a limit. We show that, for fixed degree and under mild conditions on the characteristic of the ground field, the strength is at most a polynomial in the border strength. We also establish an analogous result for partition rank. Our results control both the jump under limits and the drop under field extensions.</article>","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Estimation of the Learning Coefficient Using Empirical Loss","url":"https://arxiv.org/abs/2502.09998","date":1739768400,"author":"","guid":1211,"unread":true,"content":"<article>arXiv:2502.09998v1 Announce Type: cross \nAbstract: The learning coefficient plays a crucial role in analyzing the performance of information criteria, such as the Widely Applicable Information Criterion (WAIC) and the Widely Applicable Bayesian Information Criterion (WBIC), which Sumio Watanabe developed to assess model generalization ability. In regular statistical models, the learning coefficient is given by d/2, where d is the dimension of the parameter space. More generally, it is defined as the absolute value of the pole order of a zeta function derived from the Kullback-Leibler divergence and the prior distribution. However, except for specific cases such as reduced-rank regression, the learning coefficient cannot be derived in a closed form. Watanabe proposed a numerical method to estimate the learning coefficient, which Imai further refined to enhance its convergence properties. These methods utilize the asymptotic behavior of WBIC and have been shown to be statistically consistent as the sample size grows. In this paper, we propose a novel numerical estimation method that fundamentally differs from previous approaches and leverages a new quantity, \"Empirical Loss,\" which was introduced by Watanabe. Through numerical experiments, we demonstrate that our proposed method exhibits both lower bias and lower variance compared to those of Watanabe and Imai. Additionally, we provide a theoretical analysis that elucidates why our method outperforms existing techniques and present empirical evidence that supports our findings.</article>","contentLength":1551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Volume Minimization in Conformal Regression","url":"https://arxiv.org/abs/2502.09985","date":1739768400,"author":"","guid":1212,"unread":true,"content":"<article>arXiv:2502.09985v1 Announce Type: cross \nAbstract: We study the question of volume optimality in split conformal regression, a topic still poorly understood in comparison to coverage control. Using the fact that the calibration step can be seen as an empirical volume minimization problem, we first derive a finite-sample upper-bound on the excess volume loss of the interval returned by the classical split method. This important quantity measures the difference in length between the interval obtained with the split method and the shortest oracle prediction interval. Then, we introduce EffOrt, a methodology that modifies the learning step so that the base prediction function is selected in order to minimize the length of the returned intervals. In particular, our theoretical analysis of the excess volume loss of the prediction sets produced by EffOrt reveals the links between the learning and calibration steps, and notably the impact of the choice of the function class of the base predictor. We also introduce Ad-EffOrt, an extension of the previous method, which produces intervals whose size adapts to the value of the covariate. Finally, we evaluate the empirical performance and the robustness of our methodologies.</article>","contentLength":1231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Universal Machine Learning Interatomic Potentials are Ready for Solid Ion Conductors","url":"https://arxiv.org/abs/2502.09970","date":1739768400,"author":"","guid":1213,"unread":true,"content":"<article>arXiv:2502.09970v1 Announce Type: cross \nAbstract: With the rapid development of energy storage technology, high-performance solid-state electrolytes (SSEs) have become critical for next-generation lithium-ion batteries. These materials require high ionic conductivity, excellent electrochemical stability, and good mechanical properties to meet the demands of electric vehicles and portable electronics. However, traditional methods like density functional theory (DFT) and empirical force fields face challenges such as high computational costs, poor scalability, and limited accuracy across material systems. Universal machine learning interatomic potentials (uMLIPs) offer a promising solution with their efficiency and near-DFT-level accuracy.This study systematically evaluates six advanced uMLIP models (MatterSim, MACE, SevenNet, CHGNet, M3GNet, and ORBFF) in terms of energy, forces, thermodynamic properties, elastic moduli, and lithium-ion diffusion behavior. The results show that MatterSim outperforms others in nearly all metrics, particularly in complex material systems, demonstrating superior accuracy and physical consistency. Other models exhibit significant deviations due to issues like energy inconsistency or insufficient training data coverage.Further analysis reveals that MatterSim achieves excellent agreement with reference values in lithium-ion diffusivity calculations, especially at room temperature. Studies on Li3YCl6 and Li6PS5Cl uncover how crystal structure, anion disorder levels, and Na/Li arrangements influence ionic conductivity. Appropriate S/Cl disorder levels and optimized Na/Li arrangements enhance diffusion pathway connectivity, improving overall ionic transport performance.</article>","contentLength":1723,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Learning for Phase Estimation in Satellite-to-Earth Quantum Communication","url":"https://arxiv.org/abs/2502.09920","date":1739768400,"author":"","guid":1214,"unread":true,"content":"<article>arXiv:2502.09920v1 Announce Type: cross \nAbstract: A global continuous-variable quantum key distribution (CV-QKD) network can be established using a series of satellite-to-Earth channels. Increased performance in such a network is provided by performing coherent measurement of the optical quantum signals using a real local oscillator, calibrated locally by encoding known information on transmitted reference pulses and using signal phase error estimation algorithms. The speed and accuracy of the signal phase error estimation algorithm are vital to practical CV-QKD implementation. Our work provides a framework to analyze long short-term memory neural network (NN) architecture parameterization, with respect to the quantum Cram\\'er-Rao uncertainty bound of the signal phase error estimation, with a focus on reducing the model complexity. More specifically, we demonstrate that signal phase error estimation can be achieved using a low-complexity NN architecture, without significantly sacrificing accuracy. Our results significantly improve the real-time performance of practical CV-QKD systems deployed over satellite-to-Earth channels, thereby contributing to the ongoing development of the Quantum Internet.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic-Computed Tomography Angiography for Cerebral Vessel Templates and Segmentation","url":"https://arxiv.org/abs/2502.09893","date":1739768400,"author":"","guid":1215,"unread":true,"content":"<article>arXiv:2502.09893v1 Announce Type: cross \nAbstract: Background: Computed Tomography Angiography (CTA) is crucial for cerebrovascular disease diagnosis. Dynamic CTA is a type of imaging that captures temporal information about the We aim to develop and evaluate two segmentation techniques to segment vessels directly on CTA images: (1) creating and registering population-averaged vessel atlases and (2) using deep learning (DL). Methods: We retrieved 4D-CT of the head from our institutional research database, with bone and soft tissue subtracted from post-contrast images. An Advanced Normalization Tools pipeline was used to create angiographic atlases from 25 patients. Then, atlas-driven ROIs were identified by a CT attenuation threshold to generate segmentation of the arteries and veins using non-linear registration. To create DL vessel segmentations, arterial and venous structures were segmented using the MRA vessel segmentation tool, iCafe, in 29 patients. These were then used to train a DL model, with bone-in CT images as input. Multiple phase images in the 4D-CT were used to increase the training and validation dataset. Both segmentation approaches were evaluated on a test 4D-CT dataset of 11 patients which were also processed by iCafe and validated by a neuroradiologist. Specifically, branch-wise segmentation accuracy was quantified with 20 labels for arteries and one for veins. DL outperformed the atlas-based segmentation models for arteries (average modified dice coefficient (amDC) 0.856 vs. 0.324) and veins (amDC 0.743 vs. 0.495) overall. For ICAs, vertebral and basilar arteries, DL and atlas -based segmentation had an amDC of 0.913 and 0.402, respectively. The amDC for MCA-M1, PCA-P1, and ACA-A1 segments were 0.932 and 0.474, respectively. Conclusion: Angiographic CT templates are developed for the first time in literature. Using 4D-CTA enables the use of tools like iCafe, lessening the burden of manual annotation.</article>","contentLength":1954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpretable Early Warnings using Machine Learning in an Online Game-experiment","url":"https://arxiv.org/abs/2502.09880","date":1739768400,"author":"","guid":1216,"unread":true,"content":"<article>arXiv:2502.09880v1 Announce Type: cross \nAbstract: Stemming from physics and later applied to other fields such as ecology, the theory of critical transitions suggests that some regime shifts are preceded by statistical early warning signals. Reddit's r/place experiment, a large-scale social game, provides a unique opportunity to test these signals consistently across thousands of subsystems undergoing critical transitions. In r/place, millions of users collaboratively created compositions, or pixel-art drawings, in which transitions occur when one composition rapidly replaces another. We develop a machine-learning-based early warning system that combines the predictive power of multiple system-specific time series via gradient-boosted decision trees with memory-retaining features. Our method significantly outperforms standard early warning indicators. Trained on the 2022 r/place data, our algorithm detects half of the transitions occurring within 20 minutes at a false positive rate of just 3.7%. Its performance remains robust when tested on the 2023 r/place event, demonstrating generalizability across different contexts. Using SHapley Additive exPlanations (SHAP) for interpreting the predictions, we investigate the underlying drivers of warnings, which could be relevant to other complex systems, especially online social systems. We reveal an interplay of patterns preceding transitions, such as critical slowing down or speeding up, a lack of innovation or coordination, turbulent histories, and a lack of image complexity. These findings show the potential of machine learning indicators in socio-ecological systems for predicting regime shifts and understanding their dynamics.</article>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gradient GA: Gradient Genetic Algorithm for Drug Molecular Design","url":"https://arxiv.org/abs/2502.09860","date":1739768400,"author":"","guid":1217,"unread":true,"content":"<article>arXiv:2502.09860v1 Announce Type: cross \nAbstract: Molecular discovery has brought great benefits to the chemical industry. Various molecule design techniques are developed to identify molecules with desirable properties. Traditional optimization methods, such as genetic algorithms, continue to achieve state-of-the-art results across multiple molecular design benchmarks. However, these techniques rely solely on random walk exploration, which hinders both the quality of the final solution and the convergence speed. To address this limitation, we propose a novel approach called Gradient Genetic Algorithm (Gradient GA), which incorporates gradient information from the objective function into genetic algorithms. Instead of random exploration, each proposed sample iteratively progresses toward an optimal solution by following the gradient direction. We achieve this by designing a differentiable objective function parameterized by a neural network and utilizing the Discrete Langevin Proposal to enable gradient guidance in discrete molecular spaces. Experimental results demonstrate that our method significantly improves both convergence speed and solution quality, outperforming cutting-edge techniques. For example, it achieves up to a 25% improvement in the top-10 score over the vanilla genetic algorithm. The code is publicly available at https://github.com/debadyuti23/GradientGA.</article>","contentLength":1396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Algorithmic contiguity from low-degree conjecture and applications in correlated random graphs","url":"https://arxiv.org/abs/2502.09832","date":1739768400,"author":"","guid":1218,"unread":true,"content":"<article>arXiv:2502.09832v1 Announce Type: cross \nAbstract: In this paper, assuming a natural strengthening of the low-degree conjecture, we provide evidence of computational hardness for two problems: (1) the (partial) matching recovery problem in the sparse correlated Erd\\H{o}s-R\\'enyi graphs $\\mathcal G(n,q;\\rho)$ when the edge-density $q=n^{-1+o(1)}$ and the correlation $\\rho&lt;\\sqrt{\\alpha}$ lies below the Otter's threshold, solving a remaining problem in \\cite{DDL23+}; (2) the detection problem between the correlated sparse stochastic block model $\\mathcal S(n,\\tfrac{\\lambda}{n};k,\\epsilon;s)$ and a pair of independent stochastic block models $\\mathcal S(n,\\tfrac{\\lambda s}{n};k,\\epsilon)$ when $\\epsilon^2 \\lambda s&lt;1$ lies below the Kesten-Stigum (KS) threshold and $s&lt;\\sqrt{\\alpha}$ lies below the Otter's threshold, solving a remaining problem in \\cite{CDGL24+}.\n  One of the main ingredient in our proof is to derive certain forms of \\emph{algorithmic contiguity} between two probability measures based on bounds on their low-degree advantage. To be more precise, consider the high-dimensional hypothesis testing problem between two probability measures $\\mathbb{P}$ and $\\mathbb{Q}$ based on the sample $\\mathsf Y$. We show that if the low-degree advantage $\\mathsf{Adv}_{\\leq D} \\big( \\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{Q}} \\big)=O(1)$, then (assuming the low-degree conjecture) there is no efficient algorithm $\\mathcal A$ such that $\\mathbb{Q}(\\mathcal A(\\mathsf Y)=0)=1-o(1)$ and $\\mathbb{P}(\\mathcal A(\\mathsf Y)=1)=\\Omega(1)$. This framework provides a useful tool for performing reductions between different inference tasks.</article>","contentLength":1652,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\Lambda$CDM and early dark energy in latent space: a data-driven parametrization of the CMB temperature power spectrum","url":"https://arxiv.org/abs/2502.09810","date":1739768400,"author":"","guid":1219,"unread":true,"content":"<article>arXiv:2502.09810v1 Announce Type: cross \nAbstract: Finding the best parametrization for cosmological models in the absence of first-principle theories is an open question. We propose a data-driven parametrization of cosmological models given by the disentangled 'latent' representation of a variational autoencoder (VAE) trained to compress cosmic microwave background (CMB) temperature power spectra. We consider a broad range of $\\Lambda$CDM and beyond-$\\Lambda$CDM cosmologies with an additional early dark energy (EDE) component. We show that these spectra can be compressed into 5 ($\\Lambda$CDM) or 8 (EDE) independent latent parameters, as expected when using temperature power spectra alone, and which reconstruct spectra at an accuracy well within the Planck errors. These latent parameters have a physical interpretation in terms of well-known features of the CMB temperature spectrum: these include the position, height and even-odd modulation of the acoustic peaks, as well as the gravitational lensing effect. The VAE also discovers one latent parameter which entirely isolates the EDE effects from those related to $\\Lambda$CDM parameters, thus revealing a previously unknown degree of freedom in the CMB temperature power spectrum. We further showcase how to place constraints on the latent parameters using Planck data as typically done for cosmological parameters, obtaining latent values consistent with previous $\\Lambda$CDM and EDE cosmological constraints. Our work demonstrates the potential of a data-driven reformulation of current beyond-$\\Lambda$CDM phenomenological models into the independent degrees of freedom to which the data observables are sensitive.</article>","contentLength":1683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prioritized Ranking Experimental Design Using Recommender Systems in Two-Sided Platforms","url":"https://arxiv.org/abs/2502.09806","date":1739768400,"author":"","guid":1220,"unread":true,"content":"<article>arXiv:2502.09806v1 Announce Type: cross \nAbstract: Interdependencies between units in online two-sided marketplaces complicate estimating causal effects in experimental settings. We propose a novel experimental design to mitigate the interference bias in estimating the total average treatment effect (TATE) of item-side interventions in online two-sided marketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the recommender system as an instrument for experimentation. TSPR strategically prioritizes items based on their treatment status in the listings displayed to users. We designed TSPR to provide users with a coherent platform experience by ensuring access to all items and a consistent realization of their treatment by all users. We evaluate our experimental design through simulations using a search impression dataset from an online travel agency. Our methodology closely estimates the true simulated TATE, while a baseline item-side estimator significantly overestimates TATE.</article>","contentLength":1001,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Patient-Specific Surgical Planning for Bicuspid Aortic Valve Repair: Fully Automated Segmentation of the Aortic Valve in 4D CT","url":"https://arxiv.org/abs/2502.09805","date":1739768400,"author":"","guid":1221,"unread":true,"content":"<article>arXiv:2502.09805v1 Announce Type: cross \nAbstract: The bicuspid aortic valve (BAV) is the most prevalent congenital heart defect and may require surgery for complications such as stenosis, regurgitation, and aortopathy. BAV repair surgery is effective but challenging due to the heterogeneity of BAV morphology. Multiple imaging modalities can be employed to assist the quantitative assessment of BAVs for surgical planning. Contrast-enhanced 4D computed tomography (CT) produces volumetric temporal sequences with excellent contrast and spatial resolution. Segmentation of the aortic cusps and root in these images is an essential step in creating patient specific models for visualization and quantification. While deep learning-based methods are capable of fully automated segmentation, no BAV-specific model exists. Among valve segmentation studies, there has been limited quantitative assessment of the clinical usability of the segmentation results. In this work, we developed a fully auto- mated multi-label BAV segmentation pipeline based on nnU-Net. The predicted segmentations were used to carry out surgically relevant morphological measurements including geometric cusp height, commissural angle and annulus diameter, and the results were compared against manual segmentation. Automated segmentation achieved average Dice scores of over 0.7 and symmetric mean distance below 0.7 mm for all three aortic cusps and the root wall. Clinically relevant benchmarks showed good consistency between manual and predicted segmentations. Overall, fully automated BAV segmentation of 3D frames in 4D CT can produce clinically usable measurements for surgical risk stratification, but the temporal consistency of segmentations needs to be improved.</article>","contentLength":1747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Acute Lymphoblastic Leukemia Diagnosis Employing YOLOv11, YOLOv8, ResNet50, and Inception-ResNet-v2 Deep Learning Models","url":"https://arxiv.org/abs/2502.09804","date":1739768400,"author":"","guid":1222,"unread":true,"content":"<article>arXiv:2502.09804v1 Announce Type: cross \nAbstract: Thousands of individuals succumb annually to leukemia alone. As artificial intelligence-driven technologies continue to evolve and advance, the question of their applicability and reliability remains unresolved. This study aims to utilize image processing and deep learning methodologies to achieve state-of-the-art results for the detection of Acute Lymphoblastic Leukemia (ALL) using data that best represents real-world scenarios. ALL is one of several types of blood cancer, and it is an aggressive form of leukemia. In this investigation, we examine the most recent advancements in ALL detection, as well as the latest iteration of the YOLO series and its performance. We address the question of whether white blood cells are malignant or benign. Additionally, the proposed models can identify different ALL stages, including early stages. Furthermore, these models can detect hematogones despite their frequent misclassification as ALL. By utilizing advanced deep learning models, namely, YOLOv8, YOLOv11, ResNet50 and Inception-ResNet-v2, the study achieves accuracy rates as high as 99.7%, demonstrating the effectiveness of these algorithms across multiple datasets and various real-world situations.</article>","contentLength":1260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reconstruction of frequency-localized functions from pointwise samples via least squares and deep learning","url":"https://arxiv.org/abs/2502.09794","date":1739768400,"author":"","guid":1223,"unread":true,"content":"<article>arXiv:2502.09794v1 Announce Type: cross \nAbstract: Recovering frequency-localized functions from pointwise data is a fundamental task in signal processing. We examine this problem from an approximation-theoretic perspective, focusing on least squares and deep learning-based methods. First, we establish a novel recovery theorem for least squares approximations using the Slepian basis from uniform random samples in low dimensions, explicitly tracking the dependence of the bandwidth on the sampling complexity. Building on these results, we then present a recovery guarantee for approximating bandlimited functions via deep learning from pointwise data. This result, framed as a practical existence theorem, provides conditions on the network architecture, training procedure, and data acquisition sufficient for accurate approximation. To complement our theoretical findings, we perform numerical comparisons between least squares and deep learning for approximating one- and two-dimensional functions. We conclude with a discussion of the theoretical limitations and the practical gaps between theory and implementation.</article>","contentLength":1124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Atom identification in bilayer moire materials with Gomb-Net","url":"https://arxiv.org/abs/2502.09791","date":1739768400,"author":"","guid":1224,"unread":true,"content":"<article>arXiv:2502.09791v1 Announce Type: cross \nAbstract: Moire patterns in van der Waals bilayer materials complicate the analysis of atomic-resolution images, hindering the atomic-scale insight typically attainable with scanning transmission electron microscopy. Here, we report a method to detect the positions and identity of atoms in each of the individual layers that compose bilayer heterostructures. We developed a deep learning model, Gomb-Net, which can distinguish atomic species in each individual layer, effectively deconvoluting the moire pattern to enable layer-specific mapping of strain and dopant distributions, unlike other methods which struggle with moire-induced complexity. Using this approach, we explored Se atom substitutional sites in a twisted fractional Janus WS2-WS2(1-x)Se2x heterostructure and found that layer specific implantation sites are unaffected by the moire pattern's local energetic or electronic modulation. This advancement enables atom-identification within material regimes where it was not possible before, opening new insights into previously inaccessible material physics.</article>","contentLength":1114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ExoMiner++ on TESS with Transfer Learning from Kepler: Transit Classification and Vetting Catalog for 2-min Data","url":"https://arxiv.org/abs/2502.09790","date":1739768400,"author":"","guid":1225,"unread":true,"content":"<article>arXiv:2502.09790v1 Announce Type: cross \nAbstract: We present ExoMiner++, an enhanced deep learning model that builds on the success of ExoMiner to improve transit signal classification in 2-minute TESS data. ExoMiner++ incorporates additional diagnostic inputs, including periodogram, flux trend, difference image, unfolded flux, and spacecraft attitude control data, all of which are crucial for effectively distinguishing transit signals from more challenging sources of false positives. To further enhance performance, we leverage transfer learning from high-quality labeled data from the Kepler space telescope, mitigating the impact of TESS's noisier and more ambiguous labels. ExoMiner++ achieves high accuracy across various classification and ranking metrics, significantly narrowing the search space for follow-up investigations to confirm new planets. To serve the exoplanet community, we introduce new TESS catalogs containing ExoMiner++ classifications and confidence scores for each transit signal. Among the 147,568 unlabeled TCEs, ExoMiner++ identifies 7,330 as planet candidates, with the remainder classified as false positives. These 7,330 planet candidates correspond to 1,868 existing TESS Objects of Interest (TOIs), 69 Community TESS Objects of Interest (CTOIs), and 50 newly introduced CTOIs. 1,797 out of the 2,506 TOIs previously labeled as planet candidates in ExoFOP are classified as planet candidates by ExoMiner++. This reduction in plausible candidates combined with the excellent ranking quality of ExoMiner++ allows the follow-up efforts to be focused on the most likely candidates, increasing the overall planet yield.</article>","contentLength":1653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Muscle and Fat Segmentation in Computed Tomography for Comprehensive Body Composition Analysis","url":"https://arxiv.org/abs/2502.09779","date":1739768400,"author":"","guid":1226,"unread":true,"content":"<article>arXiv:2502.09779v1 Announce Type: cross \nAbstract: Body composition assessment using CT images can potentially be used for a number of clinical applications, including the prognostication of cardiovascular outcomes, evaluation of metabolic health, monitoring of disease progression, assessment of nutritional status, prediction of treatment response in oncology, and risk stratification for surgical and critical care outcomes. While multiple groups have developed in-house segmentation tools for this analysis, there are very limited publicly available tools that could be consistently used across different applications. To mitigate this gap, we present a publicly accessible, end-to-end segmentation and feature calculation model specifically for CT body composition analysis. Our model performs segmentation of skeletal muscle, subcutaneous adipose tissue (SAT), and visceral adipose tissue (VAT) across the chest, abdomen, and pelvis area in axial CT images. It also provides various body composition metrics, including muscle density, visceral-to-subcutaneous fat (VAT/SAT) ratio, muscle area/volume, and skeletal muscle index (SMI), supporting both 2D and 3D assessments. The model is shared for public use. To evaluate the model, the segmentation was applied to both internal and external datasets, with body composition metrics analyzed across different age, sex, and race groups. The model achieved high dice coefficients on both internal and external datasets, exceeding 89% for skeletal muscle, SAT, and VAT segmentation. The model outperforms the benchmark by 2.40% on skeletal muscle and 10.26% on SAT compared to the manual annotations given by the publicly available dataset. Body composition metrics show mean relative absolute errors (MRAEs) under 10% for all measures. Furthermore, the model provided muscular fat segmentation with a Dice coefficient of 56.27%, which can be utilized for additional analyses as needed.</article>","contentLength":1937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CellFlow: Simulating Cellular Morphology Changes via Flow Matching","url":"https://arxiv.org/abs/2502.09775","date":1739768400,"author":"","guid":1227,"unread":true,"content":"<article>arXiv:2502.09775v1 Announce Type: cross \nAbstract: Building a virtual cell capable of accurately simulating cellular behaviors in silico has long been a dream in computational biology. We introduce CellFlow, an image-generative model that simulates cellular morphology changes induced by chemical and genetic perturbations using flow matching. Unlike prior methods, CellFlow models distribution-wise transformations from unperturbed to perturbed cell states, effectively distinguishing actual perturbation effects from experimental artifacts such as batch effects -- a major challenge in biological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined perturbation (JUMP) datasets, CellFlow generates biologically meaningful cell images that faithfully capture perturbation-specific morphological changes, achieving a 35% improvement in FID scores and a 12% increase in mode-of-action prediction accuracy over existing methods. Additionally, CellFlow enables continuous interpolation between cellular states, providing a potential tool for studying perturbation dynamics. These capabilities mark a significant step toward realizing virtual cell modeling for biomedical research.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implementation and Analysis of Regev's Quantum Factorization Algorithm","url":"https://arxiv.org/abs/2502.09772","date":1739768400,"author":"","guid":1228,"unread":true,"content":"<article>arXiv:2502.09772v1 Announce Type: cross \nAbstract: Quantum computing represents a significant advancement in computational capabilities. Of particular concern is its impact on asymmetric cryptography through, notably, Shor's algorithm and the more recently developed Regev's algorithm for factoring composite numbers. We present our implementation of the latter. Our analysis encompasses both quantum simulation results and classical component examples, with particular emphasis on comparative cases between Regev's and Shor's algorithms. Our experimental results reveal that Regev's algorithm indeed outperforms Shor's algorithm for certain composite numbers in practice. However, we observed significant performance variations across different input values. Despite Regev's algorithm's theoretical asymptotic efficiency advantage, our implementation exhibited execution times longer than Shor's algorithm for small integer factorization in both quantum and classical components. These findings offer insights into the practical challenges and performance characteristics of implementing Regev's algorithm in realistic quantum computing scenarios.</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-consistent bounds method for dissipative PDEs","url":"https://arxiv.org/abs/2502.09760","date":1739768400,"author":"","guid":1229,"unread":true,"content":"<article>arXiv:2502.09760v1 Announce Type: cross \nAbstract: We discuss the method of self-consistent bounds for dissipative PDEs with periodic boundary conditions. We prove convergence theorems for a class of dissipative PDEs, which constitute a theoretical basis of a general framework for construction of an algorithm that computes bounds for the solutions of the underlying PDE and its dependence on initial conditions.\n  We also show, that the classical examples of parabolic PDEs including Kuramoto-Sivashinsky equation and the Navier-Stokes on the torus fit into this framework.</article>","contentLength":575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast Inexact Bilevel Optimization for Analytical Deep Image Priors","url":"https://arxiv.org/abs/2502.09758","date":1739768400,"author":"","guid":1230,"unread":true,"content":"<article>arXiv:2502.09758v1 Announce Type: cross \nAbstract: The analytical deep image prior (ADP) introduced by Dittmer et al. (2020) establishes a link between deep image priors and classical regularization theory via bilevel optimization. While this is an elegant construction, it involves expensive computations if the lower-level problem is to be solved accurately. To overcome this issue, we propose to use adaptive inexact bilevel optimization to solve ADP problems. We discuss an extension of a recent inexact bilevel method called the method of adaptive inexact descent of Salehi et al.(2024) to an infinite-dimensional setting required by the ADP framework. In our numerical experiments we demonstrate that the computational speed-up achieved by adaptive inexact bilevel optimization allows one to use ADP on larger-scale problems than in the previous literature, e.g. in deblurring of 2D color images.</article>","contentLength":902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contracting Strategies for Electrolyzers to Secure Grid Connection: The Dutch Case","url":"https://arxiv.org/abs/2502.09748","date":1739768400,"author":"","guid":1231,"unread":true,"content":"<article>arXiv:2502.09748v1 Announce Type: cross \nAbstract: In response to increasing grid congestion in the Netherlands, non-firm connection and transport agreements (CTAs) and capacity restriction contracts (CRCs) have been introduced, allowing consumer curtailment in exchange for grid tariff discounts or per-MW compensations. This study examines the interaction between an electrolyzer project, facing sizing and contracting decisions, and a network operator, responsible for contract activations and determining grid connection capacity, under the new Dutch regulations. The interaction is modeled using two bilevel optimization problems with alternating leader-follower roles. Results highlight a trade-off between CRC income and non-firm CTA tariff discounts, showing that voluntary congestion management by the network operator increases electrolyzer profitability at CRC prices below 10 euro per MW but reduces it at higher prices. Furthermore, the network operator benefits more from reacting to the electrolyzer owner's CTA decisions than from leading the interaction at CRC prices above 10 euro per MW. Ignoring the other party's optimization problem overestimates profits for both the network operator and the electrolyzer owner, emphasizing the importance of coordinated decision-making.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Iterative quantum optimisation with a warm-started quantum state","url":"https://arxiv.org/abs/2502.09704","date":1739768400,"author":"","guid":1232,"unread":true,"content":"<article>arXiv:2502.09704v1 Announce Type: cross \nAbstract: We provide a method to prepare a warm-started quantum state from measurements with an iterative framework to enhance the quantum approximate optimisation algorithm (QAOA). The numerical simulations show the method can effectively address the \"stuck issue\" of the standard QAOA using a single-string warm-started initial state described in [Cain et al., 2023]. When applied to the $3$-regular MaxCut problem, our approach achieves an improved approximation ratio, with a lower bound that iteratively converges toward the best classical algorithms for $p=1$ standard QAOA. Additionally, in the context of the discrete global minimal variance portfolio (DGMVP) model, simulations reveal a more favourable scaling of identifying the global minimal compared to the QAOA standalone, the single-string warm-started QAOA and a classical constrained sampling approach.</article>","contentLength":910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lifespan tree of brain anatomy: diagnostic values for motor and cognitive neurodegenerative diseases","url":"https://arxiv.org/abs/2502.09682","date":1739768400,"author":"","guid":1233,"unread":true,"content":"<article>arXiv:2502.09682v1 Announce Type: cross \nAbstract: The differential diagnosis of neurodegenerative diseases, characterized by overlapping symptoms, may be challenging. Brain imaging coupled with artificial intelligence has been previously proposed for diagnostic support, but most of these methods have been trained to discriminate only isolated diseases from controls. Here, we develop a novel machine learning framework, named lifespan tree of brain anatomy, dedicated to the differential diagnosis between multiple diseases simultaneously. It integrates the modeling of volume changes for 124 brain structures during the lifespan with non-linear dimensionality reduction and synthetic sampling techniques to create easily interpretable representations of brain anatomy over the course of disease progression. As clinically relevant proof- of-concept applications, we constructed a cognitive lifespan tree of brain anatomy for the differential diagnosis of six causes of neurodegenerative dementia and a motor lifespan tree of brain anatomy for the differential diagnosis of four causes of parkinsonism using 37594 MRI as a training dataset. This original approach enhanced significantly the efficiency of differential diagnosis in the external validation cohort of 1754 cases, outperforming existing state-of-the art machine learning techniques. Lifespan tree holds promise as a valuable tool for differential diagnostic in relevant clinical conditions, especially for diseases still lacking effective biological markers.</article>","contentLength":1524,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalizable Cervical Cancer Screening via Large-scale Pretraining and Test-Time Adaptation","url":"https://arxiv.org/abs/2502.09662","date":1739768400,"author":"","guid":1234,"unread":true,"content":"<article>arXiv:2502.09662v1 Announce Type: cross \nAbstract: Cervical cancer is a leading malignancy in female reproductive system. While AI-assisted cytology offers a cost-effective and non-invasive screening solution, current systems struggle with generalizability in complex clinical scenarios. To address this issue, we introduced Smart-CCS, a generalizable Cervical Cancer Screening paradigm based on pretraining and adaptation to create robust and generalizable screening systems. To develop and validate Smart-CCS, we first curated a large-scale, multi-center dataset named CCS-127K, which comprises a total of 127,471 cervical cytology whole-slide images collected from 48 medical centers. By leveraging large-scale self-supervised pretraining, our CCS models are equipped with strong generalization capability, potentially generalizing across diverse scenarios. Then, we incorporated test-time adaptation to specifically optimize the trained CCS model for complex clinical settings, which adapts and refines predictions, improving real-world applicability. We conducted large-scale system evaluation among various cohorts. In retrospective cohorts, Smart-CCS achieved an overall area under the curve (AUC) value of 0.965 and sensitivity of 0.913 for cancer screening on 11 internal test datasets. In external testing, system performance maintained high at 0.950 AUC across 6 independent test datasets. In prospective cohorts, our Smart-CCS achieved AUCs of 0.947, 0.924, and 0.986 in three prospective centers, respectively. Moreover, the system demonstrated superior sensitivity in diagnosing cervical cancer, confirming the accuracy of our cancer screening results by using histology findings for validation. Interpretability analysis with cell and slide predictions further indicated that the system's decision-making aligns with clinical practice. Smart-CCS represents a significant advancement in cancer screening across diverse clinical contexts.</article>","contentLength":1951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Omics Fusion with Soft Labeling for Enhanced Prediction of Distant Metastasis in Nasopharyngeal Carcinoma Patients after Radiotherapy","url":"https://arxiv.org/abs/2502.09656","date":1739768400,"author":"","guid":1235,"unread":true,"content":"<article>arXiv:2502.09656v1 Announce Type: cross \nAbstract: Omics fusion has emerged as a crucial preprocessing approach in the field of medical image processing, providing significant assistance to several studies. One of the challenges encountered in the integration of omics data is the presence of unpredictability arising from disparities in data sources and medical imaging equipment. In order to overcome this challenge and facilitate the integration of their joint application to specific medical objectives, this study aims to develop a fusion methodology that mitigates the disparities inherent in omics data. The utilization of the multi-kernel late-fusion method has gained significant popularity as an effective strategy for addressing this particular challenge. An efficient representation of the data may be achieved by utilizing a suitable single-kernel function to map the inherent features and afterward merging them in a space with a high number of dimensions. This approach effectively addresses the differences noted before. The inflexibility of label fitting poses a constraint on the use of multi-kernel late-fusion methods in complex nasopharyngeal carcinoma (NPC) datasets, hence affecting the efficacy of general classifiers in dealing with high-dimensional characteristics. This innovative methodology aims to increase the disparity between the two cohorts, hence providing a more flexible structure for the allocation of labels. The examination of the NPC-ContraParotid dataset demonstrates the model's robustness and efficacy, indicating its potential as a valuable tool for predicting distant metastases in patients with nasopharyngeal carcinoma (NPC).</article>","contentLength":1673,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Heterogeneous Mixture of Experts for Remote Sensing Image Super-Resolution","url":"https://arxiv.org/abs/2502.09654","date":1739768400,"author":"","guid":1236,"unread":true,"content":"<article>arXiv:2502.09654v1 Announce Type: cross \nAbstract: Remote sensing image super-resolution (SR) aims to reconstruct high-resolution remote sensing images from low-resolution inputs, thereby addressing limitations imposed by sensors and imaging conditions. However, the inherent characteristics of remote sensing images, including diverse ground object types and complex details, pose significant challenges to achieving high-quality reconstruction. Existing methods typically employ a uniform structure to process various types of ground objects without distinction, making it difficult to adapt to the complex characteristics of remote sensing images. To address this issue, we introduce a Mixture of Experts (MoE) model and design a set of heterogeneous experts. These experts are organized into multiple expert groups, where experts within each group are homogeneous while being heterogeneous across groups. This design ensures that specialized activation parameters can be employed to handle the diverse and intricate details of ground objects effectively. To better accommodate the heterogeneous experts, we propose a multi-level feature aggregation strategy to guide the routing process. Additionally, we develop a dual-routing mechanism to adaptively select the optimal expert for each pixel. Experiments conducted on the UCMerced and AID datasets demonstrate that our proposed method achieves superior SR reconstruction accuracy compared to state-of-the-art methods. The code will be available at https://github.com/Mr-Bamboo/MFG-HMoE.</article>","contentLength":1541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SASVi - Segment Any Surgical Video","url":"https://arxiv.org/abs/2502.09653","date":1739768400,"author":"","guid":1237,"unread":true,"content":"<article>arXiv:2502.09653v1 Announce Type: cross \nAbstract: Purpose: Foundation models, trained on multitudes of public datasets, often require additional fine-tuning or re-prompting mechanisms to be applied to visually distinct target domains such as surgical videos. Further, without domain knowledge, they cannot model the specific semantics of the target domain. Hence, when applied to surgical video segmentation, they fail to generalise to sections where previously tracked objects leave the scene or new objects enter. Methods: We propose SASVi, a novel re-prompting mechanism based on a frame-wise Mask R-CNN Overseer model, which is trained on a minimal amount of scarcely available annotations for the target domain. This model automatically re-prompts the foundation model SAM2 when the scene constellation changes, allowing for temporally smooth and complete segmentation of full surgical videos. Results: Re-prompting based on our Overseer model significantly improves the temporal consistency of surgical video segmentation compared to similar prompting techniques and especially frame-wise segmentation, which neglects temporal information, by at least 1.5%. Our proposed approach allows us to successfully deploy SAM2 to surgical videos, which we quantitatively and qualitatively demonstrate for three different cholecystectomy and cataract surgery datasets. Conclusion: SASVi can serve as a new baseline for smooth and temporally consistent segmentation of surgical videos with scarcely available annotation data. Our method allows us to leverage scarce annotations and obtain complete annotations for full videos of the large-scale counterpart datasets. We make those annotations publicly available, providing extensive annotation data for the future development of surgical data science models.</article>","contentLength":1804,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Volumetric Temporal Texture Synthesis for Smoke Stylization using Neural Cellular Automata","url":"https://arxiv.org/abs/2502.09631","date":1739768400,"author":"","guid":1238,"unread":true,"content":"<article>arXiv:2502.09631v1 Announce Type: cross \nAbstract: Artistic stylization of 3D volumetric smoke data is still a challenge in computer graphics due to the difficulty of ensuring spatiotemporal consistency given a reference style image, and that within reasonable time and computational resources. In this work, we introduce Volumetric Neural Cellular Automata (VNCA), a novel model for efficient volumetric style transfer that synthesizes, in real-time, multi-view consistent stylizing features on the target smoke with temporally coherent transitions between stylized simulation frames. VNCA synthesizes a 3D texture volume with color and density stylization and dynamically aligns this volume with the intricate motion patterns of the smoke simulation under the Eulerian framework. Our approach replaces the explicit fluid advection modeling and the inter-frame smoothing terms with the self-emerging motion of the underlying cellular automaton, thus reducing the training time by over an order of magnitude. Beyond smoke simulations, we demonstrate the versatility of our approach by showcasing its applicability to mesh stylization.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Bias, Fairness, and Bias Mitigation for a Wearable-based Freezing of Gait Detection in Parkinson's Disease","url":"https://arxiv.org/abs/2502.09626","date":1739768400,"author":"","guid":1239,"unread":true,"content":"<article>arXiv:2502.09626v1 Announce Type: cross \nAbstract: Freezing of gait (FOG) is a debilitating feature of Parkinson's disease (PD), which is a cause of injurious falls among PD patients. Recent advances in wearable-based human activity recognition (HAR) technology have enabled the detection of FOG subtypes across benchmark datasets. Since FOG manifestation is heterogeneous, developing models that quantify FOG consistently across patients with varying demographics, FOG types, and PD conditions is important. Bias and fairness in FOG models remain understudied in HAR, with research focused mainly on FOG detection using single benchmark datasets. We evaluated the bias and fairness of HAR models for wearable-based FOG detection across demographics and PD conditions using multiple datasets and the effectiveness of transfer learning as a potential bias mitigation approach. Our evaluation using demographic parity ratio (DPR) and equalized odds ratio (EOR) showed model bias (DPR &amp; EOR &lt; 0.8) for all stratified demographic variables, including age, sex, and disease duration. Our experiments demonstrated that transfer learning from multi-site datasets and generic human activity representations significantly improved fairness (average change in DPR +0.027, +0.039, respectively) and performance (average change in F1-score +0.026, +0.018, respectively) across attributes, supporting the hypothesis that generic human activity representations learn fairer representations applicable to health analytics.</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transformer Based Time-Series Forecasting for Stock","url":"https://arxiv.org/abs/2502.09625","date":1739768400,"author":"","guid":1240,"unread":true,"content":"<article>arXiv:2502.09625v1 Announce Type: cross \nAbstract: To the naked eye, stock prices are considered chaotic, dynamic, and unpredictable. Indeed, it is one of the most difficult forecasting tasks that hundreds of millions of retail traders and professional traders around the world try to do every second even before the market opens. With recent advances in the development of machine learning and the amount of data the market generated over years, applying machine learning techniques such as deep learning neural networks is unavoidable. In this work, we modeled the task as a multivariate forecasting problem, instead of a naive autoregression problem. The multivariate analysis is done using the attention mechanism via applying a mutated version of the Transformer, \"Stockformer\", which we created.</article>","contentLength":801,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"iFormer: Integrating ConvNet and Transformer for Mobile Application","url":"https://arxiv.org/abs/2501.15369","date":1739768400,"author":"","guid":1241,"unread":true,"content":"<article>arXiv:2501.15369v1 Announce Type: cross \nAbstract: We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming a standard convolutional network, \\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks. Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding","url":"https://arxiv.org/abs/2502.10392","date":1739768400,"author":"","guid":1242,"unread":true,"content":"<article>arXiv:2502.10392v1 Announce Type: new \nAbstract: In this paper, we propose an efficient multi-level convolution architecture for 3D visual grounding. Conventional methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in 3D visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, our method achieves top inference speed and surpasses previous fastest method by 100\\% FPS. Our method also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code is available at \\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.</article>","contentLength":1671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MM-RLHF: The Next Step Forward in Multimodal LLM Alignment","url":"https://arxiv.org/abs/2502.10391","date":1739768400,"author":"","guid":1243,"unread":true,"content":"<article>arXiv:2502.10391v1 Announce Type: new \nAbstract: Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across $\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a $\\mathbf{19.5}$% increase in conversational abilities and a $\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io.</article>","contentLength":1955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"(How) Can Transformers Predict Pseudo-Random Numbers?","url":"https://arxiv.org/abs/2502.10390","date":1739768400,"author":"","guid":1244,"unread":true,"content":"<article>arXiv:2502.10390v1 Announce Type: new \nAbstract: Transformers excel at discovering patterns in sequential data, yet their fundamental limitations and learning mechanisms remain crucial topics of investigation. In this paper, we study the ability of Transformers to learn pseudo-random number sequences from linear congruential generators (LCGs), defined by the recurrence relation $x_{t+1} = a x_t + c \\;\\mathrm{mod}\\; m$. Our analysis reveals that with sufficient architectural capacity and training data variety, Transformers can perform in-context prediction of LCG sequences with unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding layers and attention patterns, we uncover how Transformers develop algorithmic structures to learn these sequences in two scenarios of increasing complexity. First, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but fixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our analysis reveals that models learn to factorize the modulus and utilize digit-wise number representations to make sequential predictions. In the second, more challenging scenario of unseen moduli, we show that Transformers can generalize to unseen moduli up to $m_{\\text{test}} = 2^{16}$. In this case, the model employs a two-step strategy: first estimating the unknown modulus from the context, then utilizing prime factorizations to generate predictions. For this task, we observe a sharp transition in the accuracy at a critical depth $=3$. We also find that the number of in-context sequence elements needed to reach high accuracy scales sublinearly with the modulus.</article>","contentLength":1646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Region-Adaptive Sampling for Diffusion Transformers","url":"https://arxiv.org/abs/2502.10389","date":1739768400,"author":"","guid":1245,"unread":true,"content":"<article>arXiv:2502.10389v1 Announce Type: new \nAbstract: Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.</article>","contentLength":1687,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction","url":"https://arxiv.org/abs/2502.10388","date":1739768400,"author":"","guid":1246,"unread":true,"content":"<article>arXiv:2502.10388v1 Announce Type: new \nAbstract: Recent progress in large language models (LLMs) has enabled the automated processing of lengthy documents even without supervised training on a task-specific dataset. Yet, their zero-shot performance in complex tasks as opposed to straightforward information extraction tasks remains suboptimal. One feasible approach for tasks with lengthy, complex input is to first summarize the document and then apply supervised fine-tuning to the summary. However, the summarization process inevitably results in some loss of information. In this study we present a method for processing the summaries of long documents aimed to capture different important aspects of the original document. We hypothesize that LLM summaries generated with different aspect-oriented prompts contain different \\textit{information signals}, and we propose methods to measure these differences. We introduce approaches to effectively integrate signals from these different summaries for supervised training of transformer models. We validate our hypotheses on a high-impact task -- 30-day readmission prediction from a psychiatric discharge -- using real-world data from four hospitals, and show that our proposed method increases the prediction performance for the complex task of predicting patient outcome.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simplifying DINO via Coding Rate Regularization","url":"https://arxiv.org/abs/2502.10385","date":1739768400,"author":"","guid":1247,"unread":true,"content":"<article>arXiv:2502.10385v1 Announce Type: new \nAbstract: DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-the-art performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable -- many hyperparameters need to be carefully tuned to ensure that the representations do not collapse -- which poses considerable difficulty to improving them or adapting them to new domains. In this work, we posit that we can remove most such-motivated idiosyncrasies in the pre-training pipelines, and only need to add an explicit coding rate term in the loss function to avoid collapse of the representations. As a result, we obtain highly simplified variants of the DINO and DINOv2 which we call SimDINO and SimDINOv2, respectively. Remarkably, these simplified models are more robust to different design choices, such as network architecture and hyperparameters, and they learn even higher-quality representations, measured by performance on downstream tasks, offering a Pareto improvement over the corresponding DINO and DINOv2 models. This work highlights the potential of using simplifying design principles to improve the empirical practice of deep learning.</article>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Representation and Interpretation in Artificial and Natural Computing","url":"https://arxiv.org/abs/2502.10383","date":1739768400,"author":"","guid":1248,"unread":true,"content":"<article>arXiv:2502.10383v1 Announce Type: new \nAbstract: Artificial computing machinery transforms representations through an objective process, to be interpreted subjectively by humans, so the machine and the interpreter are different entities, but in the putative natural computing both processes are performed by the same agent. The method or process that transforms a representation is called here \\emph{the mode of computing}. The mode used by digital computers is the algorithmic one, but there are others, such as quantum computers and diverse forms of non-conventional computing, and there is an open-ended set of representational formats and modes that could be used in artificial and natural computing. A mode based on a notion of computing different from Turing's may perform feats beyond what the Turing Machine does but the modes would not be of the same kind and could not be compared. For a mode of computing to be more powerful than the algorithmic one, it ought to compute functions lacking an effective algorithm, and Church Thesis would not hold. Here, a thought experiment including a computational demon using a hypothetical mode for such an effect is presented. If there is natural computing, there is a mode of natural computing whose properties may be causal to the phenomenological experience. Discovering it would come with solving the hard problem of consciousness; but if it turns out that such a mode does not exist, there is no such thing as natural computing, and the mind is not a computational process.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Balancing the Scales: A Theoretical and Algorithmic Framework for Learning from Imbalanced Data","url":"https://arxiv.org/abs/2502.10381","date":1739768400,"author":"","guid":1249,"unread":true,"content":"<article>arXiv:2502.10381v1 Announce Type: new \nAbstract: Class imbalance remains a major challenge in machine learning, especially in multi-class problems with long-tailed distributions. Existing methods, such as data resampling, cost-sensitive techniques, and logistic loss modifications, though popular and often effective, lack solid theoretical foundations. As an example, we demonstrate that cost-sensitive methods are not Bayes consistent. This paper introduces a novel theoretical framework for analyzing generalization in imbalanced classification. We propose a new class-imbalanced margin loss function for both binary and multi-class settings, prove its strong $H$-consistency, and derive corresponding learning guarantees based on empirical loss and a new notion of class-sensitive Rademacher complexity. Leveraging these theoretical results, we devise novel and general learning algorithms, IMMAX (Imbalanced Margin Maximization), which incorporate confidence margins and are applicable to various hypothesis sets. While our focus is theoretical, we also present extensive empirical results demonstrating the effectiveness of our algorithms compared to existing baselines.</article>","contentLength":1176,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unknown Word Detection for English as a Second Language (ESL) Learners Using Gaze and Pre-trained Language Models","url":"https://arxiv.org/abs/2502.10378","date":1739768400,"author":"","guid":1250,"unread":true,"content":"<article>arXiv:2502.10378v1 Announce Type: new \nAbstract: English as a Second Language (ESL) learners often encounter unknown words that hinder their text comprehension. Automatically detecting these words as users read can enable computing systems to provide just-in-time definitions, synonyms, or contextual explanations, thereby helping users learn vocabulary in a natural and seamless manner. This paper presents EyeLingo, a transformer-based machine learning method that predicts the probability of unknown words based on text content and eye gaze trajectory in real time with high accuracy. A 20-participant user study revealed that our method can achieve an accuracy of 97.6%, and an F1-score of 71.1%. We implemented a real-time reading assistance prototype to show the effectiveness of EyeLingo. The user study shows improvement in willingness to use and usefulness compared to baseline methods.</article>","contentLength":895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences","url":"https://arxiv.org/abs/2502.10377","date":1739768400,"author":"","guid":1251,"unread":true,"content":"<article>arXiv:2502.10377v1 Announce Type: new \nAbstract: We introduce ReStyle3D, a novel framework for scene-level appearance transfer from a single style image to a real-world scene represented by multiple views. The method combines explicit semantic correspondences with multi-view consistency to achieve precise and coherent stylization. Unlike conventional stylization methods that apply a reference style globally, ReStyle3D uses open-vocabulary segmentation to establish dense, instance-level correspondences between the style and real-world images. This ensures that each object is stylized with semantically matched textures. It first transfers the style to a single view using a training-free semantic-attention mechanism in a diffusion model. It then lifts the stylization to additional views via a learned warp-and-refine network guided by monocular depth and pixel-wise correspondences. Experiments show that ReStyle3D consistently outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence. User studies further validate its ability to produce photo-realistic, semantically faithful results. Our code, pretrained models, and dataset will be publicly released, to support new applications in interior design, virtual staging, and 3D-consistent stylization.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robustness tests for biomedical foundation models should tailor to specification","url":"https://arxiv.org/abs/2502.10374","date":1739768400,"author":"","guid":1252,"unread":true,"content":"<article>arXiv:2502.10374v1 Announce Type: new \nAbstract: Existing regulatory frameworks for biomedical AI include robustness as a key component but lack detailed implementational guidance. The recent rise of biomedical foundation models creates new hurdles in testing and certification given their broad capabilities and susceptibility to complex distribution shifts. To balance test feasibility and effectiveness, we suggest a priority-based, task-oriented approach to tailor robustness evaluation objectives to a predefined specification. We urge concrete policies to adopt a granular categorization of robustness concepts in the specification. Our approach promotes the standardization of risk assessment and monitoring, which guides technical developments and mitigation efforts.</article>","contentLength":775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OWLS: Scaling Laws for Multilingual Speech Recognition and Translation Models","url":"https://arxiv.org/abs/2502.10373","date":1739768400,"author":"","guid":1253,"unread":true,"content":"<article>arXiv:2502.10373v1 Announce Type: new \nAbstract: Neural scaling laws offer valuable insights for designing robust sequence processing architectures. While these laws have been extensively characterized in other modalities, their behavior in speech remains comparatively underexplored. In this work, we introduce OWLS, an open-access, reproducible suite of multilingual speech recognition and translation models spanning 0.25B to 18B parameters, with the 18B version being the largest speech model, to the best of our knowledge. OWLS leverages up to 360K hours of public speech data across 150 languages, enabling a systematic investigation into how data, model, and compute scaling each influence performance in multilingual speech tasks. We use OWLS to derive neural scaling laws, showing how final performance can be reliably predicted when scaling. One of our key findings is that scaling enhances performance on low-resource languages/dialects, helping to mitigate bias and improve the accessibility of speech technologies. Finally, we show how OWLS can be used to power new research directions by discovering emergent abilities in large-scale speech models. Model checkpoints will be released on https://huggingface.co/collections/espnet/owls-scaling-laws-for-speech-recognition-and-translation-67ab7f991c194065f057ce8d for future studies.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized State Estimation and Opacity Verification Based on Partially Ordered Observation Sequences","url":"https://arxiv.org/abs/2502.10367","date":1739768400,"author":"","guid":1254,"unread":true,"content":"<article>arXiv:2502.10367v1 Announce Type: new \nAbstract: In this paper, we investigate state estimation and opacity verification problems within a decentralized observation architecture. Specifically, we consider a discrete event system whose behavior is recorded by a set of observation sites. These sites transmit the partially ordered sequences of observations that they record to a coordinator whenever a \\textit{synchronization} occurs. To properly analyze the system behavior from the coordinator's viewpoint, we first introduce the notion of an \\textit{All Sequence Structure} (ASS), which concisely captures the state evolution of each system state upon different information provided by the observation sites. Based on the ASS, we then construct corresponding current-state and initial-state estimators for offline state estimation at the coordinator. When used to verify state-isolation properties under this decentralized architecture, the use of ASS demonstrates a significant reduction in complexity compared with existing approaches in the literature. In particular, we discuss how to verify initial-state opacity at the coordinator, as well as a novel opacity notion, namely current-state-at-synchronization opacity.</article>","contentLength":1223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AffinityFlow: Guided Flows for Antibody Affinity Maturation","url":"https://arxiv.org/abs/2502.10365","date":1739768400,"author":"","guid":1255,"unread":true,"content":"<article>arXiv:2502.10365v1 Announce Type: new \nAbstract: Antibodies are widely used as therapeutics, but their development requires costly affinity maturation, involving iterative mutations to enhance binding affinity.This paper explores a sequence-only scenario for affinity maturation, using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold within flow matching to generate diverse protein structures, enabling a sequence-conditioned generative model of structure. Building on this, we propose an alternating optimization framework that (1) fixes the sequence to guide structure generation toward high binding affinity using a structure-based affinity predictor, then (2) applies inverse folding to create sequence mutations, refined by a sequence-based affinity predictor for post selection. To address this, we develop a co-teaching module that incorporates valuable information from noisy biophysical energies into predictor refinement. The sequence-based predictor selects consensus samples to teach the structure-based predictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art performance in affinity maturation experiments. We plan to open-source our code after acceptance.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds","url":"https://arxiv.org/abs/2502.10363","date":1739768400,"author":"","guid":1256,"unread":true,"content":"<article>arXiv:2502.10363v1 Announce Type: new \nAbstract: Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing approaches designed for quadrupedal robots often fail to generalize to humanoid robots due to differences in foot geometry and unstable morphology, while learning-based approaches for humanoid locomotion still face great challenges on complex terrains due to sparse foothold reward signals and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the learning process between dense locomotion rewards and sparse foothold rewards. To encourage sufficient trail-and-error exploration, BeamDojo incorporates a two-stage RL approach: the first stage relaxes the terrain dynamics by training the humanoid on flat terrain while providing it with task terrain perceptive observations, and the second stage fine-tunes the policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based elevation map to enable real-world deployment. Extensive simulation and real-world experiments demonstrate that BeamDojo achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.</article>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages","url":"https://arxiv.org/abs/2502.10362","date":1739768400,"author":"","guid":1257,"unread":true,"content":"<article>arXiv:2502.10362v1 Announce Type: new \nAbstract: CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts.</article>","contentLength":1182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Multilingual LLM Pretraining with Model-Based Data Selection","url":"https://arxiv.org/abs/2502.10361","date":1739768400,"author":"","guid":1258,"unread":true,"content":"<article>arXiv:2502.10361v1 Announce Type: new \nAbstract: Dataset curation has become a basis for strong large language model (LLM) performance. While various rule-based filtering heuristics exist for English and multilingual datasets, model-based filtering techniques have primarily focused on English. To address the disparity stemming from limited research on non-English languages, we propose a model-based filtering framework for multilingual datasets that aims to identify a diverse set of structured and knowledge-rich samples. Our approach emphasizes transparency, simplicity, and efficiency, leveraging Transformer- and FastText-based classifiers to ensure the broad accessibility of our technique and data. We conduct comprehensive ablation studies on the FineWeb-2 web crawl dataset across diverse language families, scripts, and resource availability to demonstrate the effectiveness of our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our approach can match the baseline MMLU score with as little as 15% of the training tokens, while also improving across other benchmarks. These findings provide strong evidence for the generalizability of our approach to other languages. As a result, we extend our framework to 20 languages for which we release the refined pretraining datasets.</article>","contentLength":1309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proper Learnability and the Role of Unlabeled Data","url":"https://arxiv.org/abs/2502.10359","date":1739768400,"author":"","guid":1259,"unread":true,"content":"<article>arXiv:2502.10359v1 Announce Type: new \nAbstract: Proper learning refers to the setting in which learners must emit predictors in the underlying hypothesis class $H$, and often leads to learners with simple algorithmic forms (e.g. empirical risk minimization (ERM), structural risk minimization (SRM)). The limitation of proper learning, however, is that there exist problems which can only be learned improperly, e.g. in multiclass classification. Thus, we ask: Under what assumptions on the hypothesis class or the information provided to the learner is a problem properly learnable? We first demonstrate that when the unlabeled data distribution is given, there always exists an optimal proper learner governed by distributional regularization, a randomized generalization of regularization. We refer to this setting as the distribution-fixed PAC model, and continue to evaluate the learner on its worst-case performance over all distributions. Our result holds for all metric loss functions and any finite learning problem (with no dependence on its size). Further, we demonstrate that sample complexities in the distribution-fixed PAC model can shrink by only a logarithmic factor from the classic PAC model, strongly refuting the role of unlabeled data in PAC learning (from a worst-case perspective).\n  We complement this with impossibility results which obstruct any characterization of proper learnability in the realizable PAC model. First, we observe that there are problems whose proper learnability is logically undecidable, i.e., independent of the ZFC axioms. We then show that proper learnability is not a monotone property of the underlying hypothesis class, and that it is not a local property (in a precise sense). Our impossibility results all hold even for the fundamental setting of multiclass classification, and go through a reduction of EMX learning (Ben-David et al., 2019) to proper classification which may be of independent interest.</article>","contentLength":1961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dimension-free Score Matching and Time Bootstrapping for Diffusion Models","url":"https://arxiv.org/abs/2502.10354","date":1739768400,"author":"","guid":1260,"unread":true,"content":"<article>arXiv:2502.10354v1 Announce Type: new \nAbstract: Diffusion models generate samples by estimating the score function of the target distribution at various noise levels. The model is trained using samples drawn from the target distribution, progressively adding noise. In this work, we establish the first (nearly) dimension-free sample complexity bounds for learning these score functions, achieving a double exponential improvement in dimension over prior results. A key aspect of our analysis is the use of a single function approximator to jointly estimate scores across noise levels, a critical feature of diffusion models in practice which enables generalization across timesteps. Our analysis introduces a novel martingale-based error decomposition and sharp variance bounds, enabling efficient learning from dependent data generated by Markov processes, which may be of independent interest. Building on these insights, we propose Bootstrapped Score Matching (BSM), a variance reduction technique that utilizes previously learned scores to improve accuracy at higher noise levels. These results provide crucial insights into the efficiency and effectiveness of diffusion models for generative modeling.</article>","contentLength":1208,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Assortment Optimization for Patient-Provider Matching","url":"https://arxiv.org/abs/2502.10353","date":1739768400,"author":"","guid":1261,"unread":true,"content":"<article>arXiv:2502.10353v1 Announce Type: new \nAbstract: Rising provider turnover forces healthcare administrators to frequently rematch patients to available providers, which can be cumbersome and labor-intensive. To reduce the burden of rematching, we study algorithms for matching patients and providers through assortment optimization. We develop a patient-provider matching model in which we simultaneously offer each patient a menu of providers, and patients subsequently respond and select providers. By offering assortments upfront, administrators can balance logistical ease and patient autonomy. We study policies for assortment optimization and characterize their performance under different problem settings. We demonstrate that the selection of assortment policy is highly dependent on problem specifics and, in particular, on a patient's willingness to match and the ratio between patients and providers. On real-world data, we show that our best policy can improve match quality by 13% over a greedy solution by tailoring assortment sizes based on patient characteristics. We conclude with recommendations for running a real-world patient-provider matching system inspired by our results.</article>","contentLength":1195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic Verification for Ambiguous Query Disambiguation","url":"https://arxiv.org/abs/2502.10352","date":1739768400,"author":"","guid":1262,"unread":true,"content":"<article>arXiv:2502.10352v1 Announce Type: new \nAbstract: In this work, we tackle the challenge of disambiguating queries in retrieval-augmented generation (RAG) to diverse yet answerable interpretations. State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse interpretations are generated by an LLM, later used as search queries to retrieve supporting passages. Such a process may introduce noise in either interpretations or retrieval, particularly in enterprise settings, where LLMs -- trained on static data -- may struggle with domain-specific disambiguations. Thus, a post-hoc verification phase is introduced to prune noises. Our distinction is to unify diversification with verification by incorporating feedback from retriever and generator early on. This joint approach improves both efficiency and robustness by reducing reliance on multiple retrieval and inference steps, which are susceptible to cascading errors. We validate the efficiency and effectiveness of our method, Verified-Diversification with Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve diverse yet verifiable interpretations. Empirical results show that VERDICT improves grounding-aware F1 score by an average of 23% over the strongest baseline across different backbone LLMs.</article>","contentLength":1295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Incremental Approximate Shortest Paths in Directed Graphs","url":"https://arxiv.org/abs/2502.10348","date":1739768400,"author":"","guid":1263,"unread":true,"content":"<article>arXiv:2502.10348v1 Announce Type: new \nAbstract: In this paper, we show new data structures maintaining approximate shortest paths in sparse directed graphs with polynomially bounded non-negative edge weights under edge insertions.\n  We give more efficient incremental $(1+\\epsilon)$-approximate APSP data structures that work against an adaptive adversary: a deterministic one with $\\tilde{O}(m^{3/2}n^{3/4})$ total update time and a randomized one with $\\tilde{O}(m^{4/3}n^{5/6})$ total update time. For sparse graphs, these both improve polynomially upon the best-known bound against an adaptive adversary. To achieve that, building on the ideas of [Chechik-Zhang, SODA'21] and [Kyng-Meierhans-Probst Gutenberg, SODA'22], we show a near-optimal $(1+\\epsilon)$-approximate incremental SSSP data structure for a special case when all edge updates are adjacent to the source, that might be of independent interest.\n  We also describe a very simple and near-optimal \\emph{offline} incremental $(1+\\epsilon)$-approximate SSSP data structure. While online near-linear partially dynamic SSSP data structures have been elusive so far (except for dense instances), our result excludes using certain types of impossibility arguments to rule them out. Additionally, our offline solution leads to near-optimal and deterministic all-pairs bounded-leg shortest paths data structure for sparse graphs.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Organize the Web: Constructing Domains Enhances Pre-Training Data Curation","url":"https://arxiv.org/abs/2502.10341","date":1739768400,"author":"","guid":1264,"unread":true,"content":"<article>arXiv:2502.10341v1 Announce Type: new \nAbstract: Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.</article>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STAR: Spectral Truncation and Rescale for Model Merging","url":"https://arxiv.org/abs/2502.10339","date":1739768400,"author":"","guid":1265,"unread":true,"content":"<article>arXiv:2502.10339v1 Announce Type: new \nAbstract: Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd $\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is publicly available at https://github.com/IBM/STAR.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering","url":"https://arxiv.org/abs/2502.10338","date":1739768400,"author":"","guid":1266,"unread":true,"content":"<article>arXiv:2502.10338v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel in natural language tasks but still face challenges in Question Answering (QA) tasks requiring complex, multi-step reasoning. We outline the types of reasoning required in some of these tasks, and reframe them in terms of meta-level reasoning (akin to high-level strategic reasoning or planning) and object-level reasoning (embodied in lower-level tasks such as mathematical reasoning). Franklin, a novel dataset with requirements of meta- and object-level reasoning, is introduced and used along with three other datasets to evaluate four LLMs at question answering tasks requiring multiple steps of reasoning. Results from human annotation studies suggest LLMs demonstrate meta-level reasoning with high frequency, but struggle with object-level reasoning tasks in some of the datasets used. Additionally, evidence suggests that LLMs find the object-level reasoning required for the questions in the Franklin dataset challenging, yet they do exhibit strong performance with respect to the meta-level reasoning requirements.</article>","contentLength":1109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ocular Disease Classification Using CNN with Deep Convolutional Generative Adversarial Network","url":"https://arxiv.org/abs/2502.10334","date":1739768400,"author":"","guid":1267,"unread":true,"content":"<article>arXiv:2502.10334v1 Announce Type: new \nAbstract: The Convolutional Neural Network (CNN) has shown impressive performance in image classification because of its strong learning capabilities. However, it demands a substantial and balanced dataset for effective training. Otherwise, networks frequently exhibit over fitting and struggle to generalize to new examples. Publicly available dataset of fundus images of ocular disease is insufficient to train any classification model to achieve satisfactory accuracy. So, we propose Generative Adversarial Network(GAN) based data generation technique to synthesize dataset for training CNN based classification model and later use original disease containing ocular images to test the model. During testing the model classification accuracy with the original ocular image, the model achieves an accuracy rate of 78.6% for myopia, 88.6% for glaucoma, and 84.6% for cataract, with an overall classification accuracy of 84.6%.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InfoPos: A ML-Assisted Solution Design Support Framework for Industrial Cyber-Physical Systems","url":"https://arxiv.org/abs/2502.10331","date":1739768400,"author":"","guid":1268,"unread":true,"content":"<article>arXiv:2502.10331v1 Announce Type: new \nAbstract: The variety of building blocks and algorithms incorporated in data-centric and ML-assisted solutions is high, contributing to two challenges: selection of most effective set and order of building blocks, as well as achieving such a selection with minimum cost. Considering that ML-assisted solution design is influenced by the extent of available data, as well as available knowledge of the target system, it is advantageous to be able to select matching building blocks. We introduce the first iteration of our InfoPos framework, allowing the placement of use-cases considering the available positions (levels), i.e., from poor to rich, of knowledge and data dimensions. With that input, designers and developers can reveal the most effective corresponding choice(s), streamlining the solution design process. The results from our demonstrator, an anomaly identification use-case for industrial Cyber-Physical Systems, reflects achieved effects upon the use of different building blocks throughout knowledge and data positions. The achieved ML model performance is considered as the indicator. Our data processing code and the composed data sets are publicly available.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiOpt: Self-supervised Diffusion for Constrained Optimization","url":"https://arxiv.org/abs/2502.10330","date":1739768400,"author":"","guid":1269,"unread":true,"content":"<article>arXiv:2502.10330v1 Announce Type: new \nAbstract: Recent advances in diffusion models show promising potential for learning-based optimization by leveraging their multimodal sampling capability to escape local optima. However, existing diffusion-based optimization approaches, often reliant on supervised training, lacks a mechanism to ensure strict constraint satisfaction which is often required in real-world applications. One resulting observation is the distributional misalignment, i.e. the generated solution distribution often exhibits small overlap with the feasible domain. In this paper, we propose DiOpt, a novel diffusion paradigm that systematically learns near-optimal feasible solution distributions through iterative self-training. Our framework introduces several key innovations: a target distribution specifically designed to maximize overlap with the constrained solution manifold; a bootstrapped self-training mechanism that adaptively weights candidate solutions based on the severity of constraint violations and optimality gaps; and a dynamic memory buffer that accelerates convergence by retaining high-quality solutions over training iterations. To our knowledge, DiOpt represents the first successful integration of self-supervised diffusion with hard constraint satisfaction. Evaluations on diverse tasks, including power grid control, motion retargeting, wireless allocation demonstrate its superiority in terms of both optimality and constraint satisfaction.</article>","contentLength":1488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking Effect","url":"https://arxiv.org/abs/2502.10329","date":1739768400,"author":"","guid":1270,"unread":true,"content":"<article>arXiv:2502.10329v1 Announce Type: new \nAbstract: The rapid advancements in AI voice cloning, fueled by machine learning, have significantly impacted text-to-speech (TTS) and voice conversion (VC) fields. While these developments have led to notable progress, they have also raised concerns about the misuse of AI VC technology, causing economic losses and negative public perceptions. To address this challenge, this study focuses on creating active defense mechanisms against AI VC systems.\n  We propose a novel active defense method, VocalCrypt, which embeds pseudo-timbre (jamming information) based on SFS into audio segments that are imperceptible to the human ear, thereby forming systematic fragments to prevent voice cloning. This approach protects the voice without compromising its quality. In comparison to existing methods, such as adversarial noise incorporation, VocalCrypt significantly enhances robustness and real-time performance, achieving a 500\\% increase in generation speed while maintaining interference effectiveness.\n  Unlike audio watermarking techniques, which focus on post-detection, our method offers preemptive defense, reducing implementation costs and enhancing feasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets show that our AI-cloned speech defense system performs excellently in automatic speaker verification (ASV) tests while preserving the integrity of the protected audio.</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Process Reward Models for LLM Agents: Practical Framework and Directions","url":"https://arxiv.org/abs/2502.10325","date":1739768400,"author":"","guid":1271,"unread":true,"content":"<article>arXiv:2502.10325v1 Announce Type: new \nAbstract: We introduce Agent Process Reward Models (AgentPRM), a simple and scalable framework for training LLM agents to continually improve through interactions. AgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo rollouts to compute reward targets and optimize policies. It requires minimal modifications to existing RLHF pipelines, making it easy to integrate at scale. Beyond AgentPRM, we propose InversePRM, which learns process rewards directly from demonstrations without explicit outcome supervision. We also explore key challenges and opportunities, including exploration, process reward shaping, and model-predictive reasoning. We evaluate on ALFWorld benchmark, show that small 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o baselines, and analyze test-time scaling, reward hacking, and more. Our code is available at: https://github.com/sanjibanc/agent_prm.</article>","contentLength":954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis and Prediction of Coverage and Channel Rank for UAV Networks in Rural Scenarios with Foliage","url":"https://arxiv.org/abs/2502.10324","date":1739768400,"author":"","guid":1272,"unread":true,"content":"<article>arXiv:2502.10324v1 Announce Type: new \nAbstract: Unmanned aerial vehicles (UAVs) are expected to play a key role in 6G-enabled vehicular-to-everything (V2X) communications requiring high data rates, low latency, and reliable connectivity for mission-critical applications. Multi-input multi-output (MIMO) technology is essential for meeting these demands. However, UAV link performance is significantly affected by environmental factors such as signal attenuation, multipath propagation, and blockage from obstacles, particularly dense foliage in rural areas. In this paper, we investigate RF coverage and channel rank over UAV channels in foliage-dominated rural environments using ray tracing (RT) simulations. We conduct RT-based channel rank and RF coverage analysis over Lake Wheeler Field Labs at NC State University to examine the impact on UAV links. Custom-modeled trees are integrated into the RT simulations using NVIDIA Sionna, Blender, and Open Street Map (OSM) database to capture realistic blockage effects. Results indicate that tree-induced blockage impacts RF coverage and channel rank at lower UAV altitudes. We also propose a Kriging interpolation-based 3D channel rank interpolation scheme, leveraging the observed spatial correlation of channel rank in the given environments. The accuracy of the proposed scheme is evaluated using the mean absolute error (MAE) metric and compared against baseline interpolation methods. Finally, we compare the RT-based received signal strength (RSS) and channel rank results with real-world measurements from the NSF AERPAW testbed demonstrating reasonable consistency between simulation results and the measurements.</article>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Fraud Proof","url":"https://arxiv.org/abs/2502.10321","date":1739768400,"author":"","guid":1273,"unread":true,"content":"<article>arXiv:2502.10321v1 Announce Type: new \nAbstract: In this paper, we present a novel fraud-proof mechanism that achieves fast finality and, when combined with optimistic execution, enables real-time transaction processing. State-of-the-art optimistic rollups typically adopt a 7-day challenge window, during which any honest party can raise a challenge in case of fraud. We propose a new assert/challenge construction called \"Dynamic Fraud Proofs\" that achieves sub-second finality in ideal scenarios, while dynamically delaying settlement in the event of fraud detection and challenge resolution. The system relies on 1) a dynamic challenge period and 2) a configurable number of randomly selected verifier nodes who must interactively approve a state commitment without raising a challenge. If these conditions are not met, the state is not finalized, and the challenge period and approval criteria are dynamically adjusted. We provide a detailed analysis of the system's design, explaining how it maintains the assumption of a single honest node and addresses censorship attacks by inverting the traditional challenge process. Additionally, we formalize the system's probabilistic security model and discuss how bonding, incentives, and slashing mechanisms can encourage honest behavior, thereby increasing the likelihood of fast settlement in ideal scenarios.</article>","contentLength":1361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigations of multi-socket high core count RISC-V for HPC workloads","url":"https://arxiv.org/abs/2502.10320","date":1739768400,"author":"","guid":1274,"unread":true,"content":"<article>arXiv:2502.10320v1 Announce Type: new \nAbstract: Whilst RISC-V has become popular in fields such as embedded computing, it is yet to find mainstream success in High Performance Computing (HPC). However, the 64-core RISC-V Sophon SG2042 is a potential game changer as it provides a commodity available CPU with much higher core count than existing technologies. In this work we benchmark the SG2042 CPU hosted in an experimental, dual-socket, system to explore the performance properties of the CPU when running a common HPC benchmark suite across sockets. Earlier benchmarks found that, on the Milk-V Pioneer workstation, whilst the SG2042 performs well for compute bound codes, it struggles when pressure is placed on the memory subsystem. The performance results reported here confirm that, even on a different system, these memory performance limitations are still present and hence inherent in the CPU. However, a multi-socket configuration does enable the CPU to scale to a larger number of threads which, in the main, delivers an improvement in performance and-so this is a realistic system configuration for the HPC community.</article>","contentLength":1133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interval Selection with Binary Predictions","url":"https://arxiv.org/abs/2502.10314","date":1739768400,"author":"","guid":1275,"unread":true,"content":"<article>arXiv:2502.10314v1 Announce Type: new \nAbstract: Following a line of work that takes advantage of vast machine-learned data to enhance online algorithms with (possibly erroneous) information about future inputs, we consider predictions in the context of deterministic algorithms for the problem of selecting a maximum weight independent set of intervals arriving on the real line. We look at two weight functions, unit (constant) weights, and weights proportional to the interval's length. In the classical online model of irrevocable decisions, no algorithm can achieve constant competitiveness (Bachmann et al. [BHS13] for unit, Lipton and Tomkins [LT94] for proportional). In this setting, we show that a simple algorithm that is faithful to the predictions is optimal, and achieves an objective value of at least $OPT -\\eta$, with $\\eta$ being the total error in the predictions, both for unit, and proportional weights.\n  When revocable acceptances (a form of preemption) are allowed, the optimal deterministic algorithm for unit weights is $2k$-competitive [BK23], where $k$ is the number of different interval lengths. We give an algorithm with performance $OPT - \\eta$ (and therefore $1$-consistent), that is also $(2k +1)$-robust. For proportional weights, Garay et al. [GGKMY97] give an optimal $(2\\phi + 1)$-competitive algorithm, where $\\phi$ is the golden ratio. We present an algorithm with parameter $\\lambda &gt; 1$ that is $\\frac{3\\lambda}{\\lambda -1}$-consistent, and $\\frac{4\\lambda^2 +2\\lambda}{\\lambda -1}$-robust. Although these bounds are not tight, we show that for $\\lambda &gt; 3.42$ we achieve consistency better than the optimal online guarantee in [GGKMY97], while maintaining bounded robustness.\n  We conclude with some experimental results on real-world data that complement our theoretical findings, and show the benefit of prediction algorithms for online interval selection, even in the presence of high error.</article>","contentLength":1938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ExplainReduce: Summarising local explanations via proxies","url":"https://arxiv.org/abs/2502.10311","date":1739768400,"author":"","guid":1276,"unread":true,"content":"<article>arXiv:2502.10311v1 Announce Type: new \nAbstract: Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small \"proxy set\" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.</article>","contentLength":783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Object Detection and Tracking","url":"https://arxiv.org/abs/2502.10310","date":1739768400,"author":"","guid":1277,"unread":true,"content":"<article>arXiv:2502.10310v1 Announce Type: new \nAbstract: Efficient and accurate object detection is an important topic in the development of computer vision systems. With the advent of deep learning techniques, the accuracy of object detection has increased significantly. The project aims to integrate a modern technique for object detection with the aim of achieving high accuracy with real-time performance. The reliance on other computer vision algorithms in many object identification systems, which results in poor and ineffective performance, is a significant obstacle. In this research, we solve the end-to-end object detection problem entirely using deep learning techniques. The network is trained using the most difficult publicly available dataset, which is used for an annual item detection challenge. Applications that need object detection can benefit the system's quick and precise finding.</article>","contentLength":898,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Powered Preference Elicitation in Combinatorial Assignment","url":"https://arxiv.org/abs/2502.10308","date":1739768400,"author":"","guid":1278,"unread":true,"content":"<article>arXiv:2502.10308v1 Announce Type: new \nAbstract: We study the potential of large language models (LLMs) as proxies for humans to simplify preference elicitation (PE) in combinatorial assignment. While traditional PE methods rely on iterative queries to capture preferences, LLMs offer a one-shot alternative with reduced human effort. We propose a framework for LLM proxies that can work in tandem with SOTA ML-powered preference elicitation schemes. Our framework handles the novel challenges introduced by LLMs, such as response variability and increased computational costs. We experimentally evaluate the efficiency of LLM proxies against human queries in the well-studied course allocation domain, and we investigate the model capabilities required for success. We find that our approach improves allocative efficiency by up to 20%, and these results are robust across different LLMs and to differences in quality and accuracy of reporting.</article>","contentLength":945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPIRIT: Short-term Prediction of solar IRradIance for zero-shot Transfer learning using Foundation Models","url":"https://arxiv.org/abs/2502.10307","date":1739768400,"author":"","guid":1279,"unread":true,"content":"<article>arXiv:2502.10307v1 Announce Type: new \nAbstract: Traditional solar forecasting models are based on several years of site-specific historical irradiance data, often spanning five or more years, which are unavailable for newer photovoltaic farms. As renewable energy is highly intermittent, building accurate solar irradiance forecasting systems is essential for efficient grid management and enabling the ongoing proliferation of solar energy, which is crucial to achieve the United Nations' net zero goals. In this work, we propose SPIRIT, a novel approach leveraging foundation models for solar irradiance forecasting, making it applicable to newer solar installations. Our approach outperforms state-of-the-art models in zero-shot transfer learning by about 70%, enabling effective performance at new locations without relying on any historical data. Further improvements in performance are achieved through fine-tuning, as more location-specific data becomes available. These findings are supported by statistical significance, further validating our approach. SPIRIT represents a pivotal step towards rapid, scalable, and adaptable solar forecasting solutions, advancing the integration of renewable energy into global power systems.</article>","contentLength":1237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When 1+1 does not equal 2: Synergy in games","url":"https://arxiv.org/abs/2502.10304","date":1739768400,"author":"","guid":1280,"unread":true,"content":"<article>arXiv:2502.10304v1 Announce Type: new \nAbstract: Although synergy is an important concept that is strongly ingrained in games, it has not been widely discussed by the games community. This is due to the vagueness of the concept and the fact that there is no clear agreement on what it means. To solve this, we present a strict definition of what is synergy. Then we propose a methodology to use this definition to analyze synergy in games. Applying this definition to various games (Chess, League of Legends, and Magic: The Gathering), we illustrate how it can be used to solve many of the practical problems related to synergy.</article>","contentLength":628,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations","url":"https://arxiv.org/abs/2502.10303","date":1739768400,"author":"","guid":1281,"unread":true,"content":"<article>arXiv:2502.10303v1 Announce Type: new \nAbstract: Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and deep Q-network approaches, to create advanced AI models such as AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning and reinforcement learning to master the game of Go, surpassing professional human players. AlphaGo Zero refines this approach by eliminating reliance on human gameplay data, instead utilizing self-play for enhanced learning efficiency. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across various games, including complex Atari games. This paper reviews the significance of reinforcement learning applications in Atari and strategy-based games, analyzing these three models, their key innovations, training processes, challenges encountered, and improvements made. Additionally, we discuss advancements in the field of gaming, including MiniZero and multi-agent models, highlighting future directions and emerging AI models from Google DeepMind.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Open-Source AI-Powered Optimization in Scalene: Advancing Python Performance Profiling with DeepSeek-R1 and LLaMA 3.2","url":"https://arxiv.org/abs/2502.10299","date":1739768400,"author":"","guid":1282,"unread":true,"content":"<article>arXiv:2502.10299v1 Announce Type: new \nAbstract: Python's flexibility and ease of use come at the cost of performance inefficiencies, requiring developers to rely on profilers to optimize execution. SCALENE, a high-performance CPU, GPU, and memory profiler, provides fine-grained insights into Python applications while running significantly faster than traditional profilers. Originally, SCALENE integrated OpenAI's API to generate AI-powered optimization suggestions, but its reliance on a proprietary API limited accessibility. This study explores the feasibility of using opensource large language models (LLMs), such as DeepSeek-R1 and Llama 3.2, to generate optimization recommendations within SCALENE. Our evaluation reveals that DeepSeek-R1 provides effective code optimizations comparable to proprietary models. We integrate DeepSeek-R1 into SCALENE to automatically analyze performance bottlenecks and suggest improvements, enhancing SCALENE's utility while maintaining its open-source nature. This study demonstrates that open-source LLMs can be viable alternatives for AI-driven code optimization, paving the way for more accessible and cost-effective performance analysis tools.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders","url":"https://arxiv.org/abs/2502.10297","date":1739768400,"author":"","guid":1283,"unread":true,"content":"<article>arXiv:2502.10297v1 Announce Type: new \nAbstract: Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. While diagonal matrices used in architectures like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited expressivity. To address this, recent architectures such as (Gated) DeltaNet and RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous token-channel mixing, which overcomes some expressivity limitations with only a slight decrease in training efficiency. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple ($n_h$) steps per token. This naturally leads to diagonal plus rank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency and a stable recurrence. Through extensive experiments, we demonstrate that DeltaProduct achieves superior state-tracking and language modeling capabilities while exhibiting significantly improved length extrapolation compared to DeltaNet. Additionally, we also strengthen the theoretical foundation of DeltaNet's expressivity by proving that it can solve dihedral group word problems in just two layers.</article>","contentLength":1635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fenchel-Young Variational Learning","url":"https://arxiv.org/abs/2502.10295","date":1739768400,"author":"","guid":1284,"unread":true,"content":"<article>arXiv:2502.10295v1 Announce Type: new \nAbstract: From a variational perspective, many statistical learning criteria involve seeking a distribution that balances empirical risk and regularization. In this paper, we broaden this perspective by introducing a new general class of variational methods based on Fenchel-Young (FY) losses, treated as divergences that generalize (and encompass) the familiar Kullback-Leibler divergence at the core of classical variational learning. Our proposed formulation -- FY variational learning -- includes as key ingredients new notions of FY free energy, FY evidence, FY evidence lower bound, and FY posterior. We derive alternating minimization and gradient backpropagation algorithms to compute (or lower bound) the FY evidence, which enables learning a wider class of models than previous variational formulations. This leads to generalized FY variants of classical algorithms, such as an FY expectation-maximization (FYEM) algorithm, and latent-variable models, such as an FY variational autoencoder (FYVAE). Our new methods are shown to be empirically competitive, often outperforming their classical counterparts, and most importantly, to have qualitatively novel features. For example, FYEM has an adaptively sparse E-step, while the FYVAE can support models with sparse observations and sparse posteriors.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for Scribble-Supervised Segmentation of Medical Images","url":"https://arxiv.org/abs/2502.10294","date":1739768400,"author":"","guid":1285,"unread":true,"content":"<article>arXiv:2502.10294v1 Announce Type: new \nAbstract: The deployment of advanced deep learning models for medical image segmentation is often constrained by the requirement for extensively annotated datasets. Weakly-supervised learning, which allows less precise labels, has become a promising solution to this challenge. Building on this approach, we propose QMaxViT-Unet+, a novel framework for scribble-supervised medical image segmentation. This framework is built on the U-Net architecture, with the encoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks. These blocks enhance the model's ability to learn local and global features efficiently. Additionally, our approach integrates a query-based Transformer decoder to refine features and an edge enhancement module to compensate for the limited boundary information in the scribble label. We evaluate the proposed QMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal polyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation metrics include the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+ achieves 89.1\\% DSC and 1.316mm HD95 on ACDC, 88.4\\% DSC and 2.226mm HD95 on MS-CMRSeg, 71.4\\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\\% DSC and 50.122mm HD95 on BUSI. These results demonstrate that our method outperforms existing approaches in terms of accuracy, robustness, and efficiency while remaining competitive with fully-supervised learning approaches. This makes it ideal for medical image analysis, where high-quality annotations are often scarce and require significant effort and expense. The code is available at: https://github.com/anpc849/QMaxViT-Unet</article>","contentLength":1765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Roadmap to Address Burnout in the Cybersecurity Profession: Outcomes from a Multifaceted Workshop","url":"https://arxiv.org/abs/2502.10293","date":1739768400,"author":"","guid":1286,"unread":true,"content":"<article>arXiv:2502.10293v1 Announce Type: new \nAbstract: This paper addresses the critical issue of burnout among cybersecurity professionals, a growing concern that threatens the effectiveness of digital defense systems. As the industry faces a significant attrition crisis, with nearly 46% of cybersecurity leaders contemplating departure from their roles, it is imperative to explore the causes and consequences of burnout through a socio-technical lens. These challenges were discussed by experts from academia and industry in a multi-disciplinary workshop at the 26th International Conference on Human-Computer Interaction to address broad antecedents of burnout, manifestation and its consequences among cybersecurity professionals, as well as programs to mitigate impacts from burnout. Central to the analysis is an empirical study of former National Security Agency (NSA) tactical cyber operators. This paper presents key insights in the following areas based on discussions in the workshop: lessons for public and private sectors from the NSA study, a comparative review of addressing burnout in the healthcare profession. It also outlines a roadmap for future collaborative research, thereby informing interdisciplinary studies.</article>","contentLength":1230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Loss Bounds for Online Learning Separated Function Classes: A Gaussian Process Perspective","url":"https://arxiv.org/abs/2502.10292","date":1739768400,"author":"","guid":1287,"unread":true,"content":"<article>arXiv:2502.10292v1 Announce Type: new \nAbstract: In order to develop practical and efficient algorithms while circumventing overly pessimistic computational lower bounds, recent work has been interested in developing oracle-efficient algorithms in a variety of learning settings. Two such settings of particular interest are online and differentially private learning. While seemingly different, these two fields are fundamentally connected by the requirement that successful algorithms in each case satisfy stability guarantees; in particular, recent work has demonstrated that algorithms for online learning whose performance adapts to beneficial problem instances, attaining the so-called small-loss bounds, require a form of stability similar to that of differential privacy. In this work, we identify the crucial role that separation plays in allowing oracle-efficient algorithms to achieve this strong stability. Our notion, which we term $\\rho$-separation, generalizes and unifies several previous approaches to enforcing this strong stability, including the existence of small-separator sets and the recent notion of $\\gamma$-approximability. We present an oracle-efficient algorithm that is capable of achieving small-loss bounds with improved rates in greater generality than previous work, as well as a variant for differentially private learning that attains optimal rates, again under our separation condition. In so doing, we prove a new stability result for minimizers of a Gaussian process that strengthens and generalizes previous work.</article>","contentLength":1553,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Immersive virtual games: winners for deep cognitive assessment","url":"https://arxiv.org/abs/2502.10290","date":1739768400,"author":"","guid":1288,"unread":true,"content":"<article>arXiv:2502.10290v1 Announce Type: new \nAbstract: Studies of human cognition often rely on brief, highly controlled tasks that emphasize group-level effects but poorly capture the rich variability within and between individuals. Here, we present PixelDOPA, a suite of minigames designed to overcome these limitations by embedding classic cognitive task paradigms in an immersive 3D virtual environment with continuous behavior logging. Four minigames explore overlapping constructs such as processing speed, rule shifting, inhibitory control and working memory, comparing against established NIH Toolbox tasks. Across a clinical sample of 60 participants collected outside a controlled laboratory setting, we found significant, large correlations (r = 0.50-0.93) between the PixelDOPA tasks and NIH Toolbox counterparts, despite differences in stimuli and task structures. Process-informed metrics (e.g., gaze-based response times derived from continuous logging) substantially improved both task convergence and data quality. Test-retest analyses revealed high reliability (ICC = 0.50-0.92) for all minigames. Beyond endpoint metrics, movement and gaze trajectories revealed stable, idiosyncratic profiles of gameplay strategy, with unsupervised clustering distinguishing subjects by their navigational and viewing behaviors. These trajectory-based features showed lower within-person variability than between-person variability, facilitating player identification across repeated sessions. Game-based tasks can therefore retain the psychometric rigor of standard cognitive assessments while providing new insights into dynamic behaviors. By leveraging a highly engaging, fully customizable game engine, we show that comprehensive behavioral tracking boosts the power to detect individual differences--offering a path toward cognitive measures that are both robust and ecologically valid, even in less-than-ideal settings for data collection.</article>","contentLength":1942,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigation of the Estimation Accuracy of 5 Different Numerical ODE Solvers on 3 Case Studies","url":"https://arxiv.org/abs/2502.10289","date":1739768400,"author":"","guid":1289,"unread":true,"content":"<article>arXiv:2502.10289v1 Announce Type: new \nAbstract: Numerical ordinary differential equation (ODE) solvers are indispensable tools in various engineering domains, enabling the simulation and analysis of dynamic systems. In this work, we utilize 5 different numerical ODE solvers namely: Euler's method, Heun's method, Midpoint Method, Runge-kutta 4th order and ODE45 method in order to discover the answer of three wellknown case studies and compare their results by calculation of relative errors. To check for the validity of the estimations, the experimental data of previous literature have been compared with the data in this paper which shows a good accordance. We observe that for each of the case studies based on the behavior of the model, the estimation accuracy of the solvers is different. For the logistic population change as the first case study, the results of all solvers are so close to each other that only their solution cost can be considered for their superiority. For temperature change of a building as the second case study we see that in some especial areas the accuracy of the solvers is different and in general Midpoint ODE solver shows better results. As the last case study, market equilibrium price shows that none of the numerical ODE solvers can estimate its behavior which is due to its sudden changing nature.</article>","contentLength":1342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adversarial Mixup Unlearning","url":"https://arxiv.org/abs/2502.10288","date":1739768400,"author":"","guid":1290,"unread":true,"content":"<article>arXiv:2502.10288v1 Announce Type: new \nAbstract: Machine unlearning is a critical area of research aimed at safeguarding data privacy by enabling the removal of sensitive information from machine learning models. One unique challenge in this field is catastrophic unlearning, where erasing specific data from a well-trained model unintentionally removes essential knowledge, causing the model to deviate significantly from a retrained one. To address this, we introduce a novel approach that regularizes the unlearning process by utilizing synthesized mixup samples, which simulate the data susceptible to catastrophic effects. At the core of our approach is a generator-unlearner framework, MixUnlearn, where a generator adversarially produces challenging mixup examples, and the unlearner effectively forgets target information based on these synthesized data. Specifically, we first introduce a novel contrastive objective to train the generator in an adversarial direction: generating examples that prompt the unlearner to reveal information that should be forgotten, while losing essential knowledge. Then the unlearner, guided by two other contrastive loss terms, processes the synthesized and real data jointly to ensure accurate unlearning without losing critical knowledge, overcoming catastrophic effects. Extensive evaluations across benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches, offering a robust solution to machine unlearning. This work not only deepens understanding of unlearning mechanisms but also lays the foundation for effective machine unlearning with mixup augmentation.</article>","contentLength":1649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hybrid Cross-Stage Coordination Pre-ranking Model for Online Recommendation Systems","url":"https://arxiv.org/abs/2502.10284","date":1739768400,"author":"","guid":1291,"unread":true,"content":"<article>arXiv:2502.10284v1 Announce Type: new \nAbstract: Large-scale recommendation systems often adopt cascading architecture consisting of retrieval, pre-ranking, ranking, and re-ranking stages. With strict latency requirements, pre-ranking utilizes lightweight models to perform a preliminary selection from massive retrieved candidates. However, recent works focus solely on improving consistency with ranking, relying exclusively on downstream stages. Since downstream input is derived from the pre-ranking output, they will exacerbate the sample selection bias (SSB) issue and Matthew effect, leading to sub-optimal results. To address the limitation, we propose a novel Hybrid Cross-Stage Coordination Pre-ranking model (HCCP) to integrate information from upstream (retrieval) and downstream (ranking, re-ranking) stages. Specifically, cross-stage coordination refers to the pre-ranking's adaptability to the entire stream and the role of serving as a more effective bridge between upstream and downstream. HCCP consists of Hybrid Sample Construction and Hybrid Objective Optimization. Hybrid sample construction captures multi-level unexposed data from the entire stream and rearranges them to become the optimal guiding \"ground truth\" for pre-ranking learning. Hybrid objective optimization contains the joint optimization of consistency and long-tail precision through our proposed Margin InfoNCE loss. It is specifically designed to learn from such hybrid unexposed samples, improving the overall performance and mitigating the SSB issue. The appendix describes a proof of the efficacy of the proposed loss in selecting potential positives. Extensive offline and online experiments indicate that HCCP outperforms SOTA methods by improving cross-stage coordination. It contributes up to 14.9% UCVR and 1.3% UCTR in the JD E-commerce recommendation system. Concerning code privacy, we provide a pseudocode for reference.</article>","contentLength":1922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anomaly Detection with LWE Encrypted Control","url":"https://arxiv.org/abs/2502.10283","date":1739768400,"author":"","guid":1292,"unread":true,"content":"<article>arXiv:2502.10283v1 Announce Type: new \nAbstract: Detecting attacks using encrypted signals is challenging since encryption hides its information content. We present a novel mechanism for anomaly detection over Learning with Errors (LWE) encrypted signals without using decryption, secure channels, nor complex communication schemes. Instead, the detector exploits the homomorphic property of LWE encryption to perform hypothesis tests on transformations of the encrypted samples. The specific transformations are determined by solutions to a hard lattice-based minimization problem. While the test's sensitivity deteriorates with suboptimal solutions, similar to the exponential deterioration of the (related) test that breaks the cryptosystem, we show that the deterioration is polynomial for our test. This rate gap can be exploited to pick parameters that lead to somewhat weaker encryption but large gains in detection capability. Finally, we conclude the paper by presenting a numerical example that simulates anomaly detection, demonstrating the effectiveness of our method in identifying attacks.</article>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TrustZero - open, verifiable and scalable zero-trust","url":"https://arxiv.org/abs/2502.10281","date":1739768400,"author":"","guid":1293,"unread":true,"content":"<article>arXiv:2502.10281v1 Announce Type: new \nAbstract: We present a passport-level trust token for Europe. In an era of escalating cyber threats fueled by global competition in economic, military, and technological domains, traditional security models are proving inadequate. The rise of advanced attacks exploiting zero-day vulnerabilities, supply chain infiltration, and system interdependencies underscores the need for a paradigm shift in cybersecurity. Zero Trust Architecture (ZTA) emerges as a transformative framework that replaces implicit trust with continuous verification of identity and granular access control. This thesis introduces TrustZero, a scalable layer of zero-trust security built around a universal \"trust token\" - a non-revocable self-sovereign identity with cryptographic signatures to enable robust, mathematically grounded trust attestations. By integrating ZTA principles with cryptography, TrustZero establishes a secure web-of-trust framework adaptable to legacy systems and inter-organisational communication.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probabilistic Super-Resolution for High-Fidelity Physical System Simulations with Uncertainty Quantification","url":"https://arxiv.org/abs/2502.10280","date":1739768400,"author":"","guid":1294,"unread":true,"content":"<article>arXiv:2502.10280v1 Announce Type: new \nAbstract: Super-resolution (SR) is a promising tool for generating high-fidelity simulations of physical systems from low-resolution data, enabling fast and accurate predictions in engineering applications. However, existing deep-learning based SR methods, require large labeled datasets and lack reliable uncertainty quantification (UQ), limiting their applicability in real-world scenarios. To overcome these challenges, we propose a probabilistic SR framework that leverages the Statistical Finite Element Method and energy-based generative modeling. Our method enables efficient high-resolution predictions with inherent UQ, while eliminating the need for extensive labeled datasets. The method is validated on a 2D Poisson example and compared with bicubic interpolation upscaling. Results demonstrate a computational speed-up over high-resolution numerical solvers while providing reliable uncertainty estimates.</article>","contentLength":957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Emit As You Go: Enumerating Edges of a Spanning Tree","url":"https://arxiv.org/abs/2502.10279","date":1739768400,"author":"","guid":1295,"unread":true,"content":"<article>arXiv:2502.10279v1 Announce Type: new \nAbstract: Classically, planning tasks are studied as a two-step process: plan creation and plan execution. In situations where plan creation is slow (for example, due to expensive information access or complex constraints), a natural speed-up tactic is interleaving planning and execution. We implement such an approach with an enumeration algorithm that, after little preprocessing time, outputs parts of a plan one by one with little delay in-between consecutive outputs. As concrete planning task, we consider efficient connectivity in a network formalized as the minimum spanning tree problem in all four standard variants: (un)weighted (un)directed graphs. Solution parts to be emitted one by one for this concrete task are the individual edges that form the final tree.\n  We show with algorithmic upper bounds and matching unconditional adversary lower bounds that efficient enumeration is possible for three of four problem variants; specifically for undirected unweighted graphs (delay in the order of the average degree), as well as graphs with either weights (delay in the order of the maximum degree and the average runtime per emitted edge of a total-time algorithm) or directions (delay in the order of the maximum degree). For graphs with both weighted and directed edges, we show that no meaningful enumeration is possible.\n  Finally, with experiments on random undirected unweighted graphs, we show that the theoretical advantage of little preprocessing and delay carries over to practice.</article>","contentLength":1544,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Intelligence to Assess Dental Findings from Panoramic Radiographs -- A Multinational Study","url":"https://arxiv.org/abs/2502.10277","date":1739768400,"author":"","guid":1296,"unread":true,"content":"<article>arXiv:2502.10277v1 Announce Type: new \nAbstract: Dental panoramic radiographs (DPRs) are widely used in clinical practice for comprehensive oral assessment but present challenges due to overlapping structures and time constraints in interpretation.\n  This study aimed to establish a solid baseline for the AI-automated assessment of findings in DPRs by developing, evaluating an AI system, and comparing its performance with that of human readers across multinational data sets.\n  We analyzed 6,669 DPRs from three data sets (the Netherlands, Brazil, and Taiwan), focusing on 8 types of dental findings. The AI system combined object detection and semantic segmentation techniques for per-tooth finding identification. Performance metrics included sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC). AI generalizability was tested across data sets, and performance was compared with human dental practitioners.\n  The AI system demonstrated comparable or superior performance to human readers, particularly +67.9% (95% CI: 54.0%-81.9%; p &lt; .001) sensitivity for identifying periapical radiolucencies and +4.7% (95% CI: 1.4%-8.0%; p = .008) sensitivity for identifying missing teeth. The AI achieved a macro-averaged AUC-ROC of 96.2% (95% CI: 94.6%-97.8%) across 8 findings. AI agreements with the reference were comparable to inter-human agreements in 7 of 8 findings except for caries (p = .024). The AI system demonstrated robust generalization across diverse imaging and demographic settings and processed images 79 times faster (95% CI: 75-82) than human readers.\n  The AI system effectively assessed findings in DPRs, achieving performance on par with or better than human experts while significantly reducing interpretation time. These results highlight the potential for integrating AI into clinical workflows to improve diagnostic efficiency and accuracy, and patient management.</article>","contentLength":1930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probing Perceptual Constancy in Large Vision Language Models","url":"https://arxiv.org/abs/2502.10273","date":1739768400,"author":"","guid":1297,"unread":true,"content":"<article>arXiv:2502.10273v1 Announce Type: new \nAbstract: Perceptual constancy is the ability to maintain stable perceptions of objects despite changes in sensory input, such as variations in distance, angle, or lighting. This ability is crucial for recognizing visual information in a dynamic world, making it essential for Vision-Language Models (VLMs). However, whether VLMs are currently and theoretically capable of mastering this ability remains underexplored. In this study, we evaluated 33 VLMs using 253 experiments across three domains: color, size, and shape constancy. The experiments included single-image and video adaptations of classic cognitive tasks, along with novel tasks in in-the-wild conditions, to evaluate the models' recognition of object properties under varying conditions. We found significant variability in VLM performance, with models performance in shape constancy clearly dissociated from that of color and size constancy.</article>","contentLength":947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimized Strategies for Peak Shaving and BESS Efficiency Enhancement through Cycle-Based Control and Cluster-Level Power Allocation","url":"https://arxiv.org/abs/2502.10268","date":1739768400,"author":"","guid":1298,"unread":true,"content":"<article>arXiv:2502.10268v1 Announce Type: new \nAbstract: Battery Energy Storage Systems (BESS) are essential for peak shaving, balancing power supply and demand while enhancing grid efficiency. This study proposes a cycle-based control strategy for charging and discharging, which optimizes capture rate (CR), release rate (RR), and capacity utilization rate (CUR), improving BESS performance. Compared to traditional day-ahead methods, the cycle-based approach enhances operational accuracy and reduces capacity waste, achieving a CUR increase from 75.1% to 79.9%. An innovative cluster-level power allocation method, leveraging an improved Particle Swarm Optimization (PSO) algorithm, is introduced. This strategy reduces daily energy loss by 174.21 kWh (3.7%) and increases BESS efficiency by 0.4%. Transient and steady-state energy loss components are analyzed, revealing that transient loss proportion decreases significantly as power depth increases, from 27.2% at 1 MW to 1.3% at 10 MW. Simulations based on a detailed Simulink/Simscape model validate these methods, demonstrating enhanced peak shaving effectiveness and prolonged BESS lifespan by reducing equivalent cycles. The study provides a robust framework for optimizing BESS performance and efficiency in real-world applications.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Large Language Models the future crowd workers of Linguistics?","url":"https://arxiv.org/abs/2502.10266","date":1739768400,"author":"","guid":1299,"unread":true,"content":"<article>arXiv:2502.10266v1 Announce Type: new \nAbstract: Data elicitation from human participants is one of the core data collection strategies used in empirical linguistic research. The amount of participants in such studies may vary considerably, ranging from a handful to crowdsourcing dimensions. Even if they provide resourceful extensive data, both of these settings come alongside many disadvantages, such as low control of participants' attention during task completion, precarious working conditions in crowdsourcing environments, and time-consuming experimental designs. For these reasons, this research aims to answer the question of whether Large Language Models (LLMs) may overcome those obstacles if included in empirical linguistic pipelines. Two reproduction case studies are conducted to gain clarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced elicitation tasks, originally designed for human participants, are reproduced in the proposed framework with the help of OpenAI's GPT-4o-mini model. Its performance with our zero-shot prompting baseline shows the effectiveness and high versatility of LLMs, that tend to outperform human informants in linguistic tasks. The findings of the second replication further highlight the need to explore additional prompting techniques, such as Chain-of-Thought (CoT) prompting, which, in a second follow-up experiment, demonstrates higher alignment to human performance on both critical and filler items. Given the limited scale of this study, it is worthwhile to further explore the performance of LLMs in empirical Linguistics and in other future applications in the humanities.</article>","contentLength":1655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers","url":"https://arxiv.org/abs/2502.10263","date":1739768400,"author":"","guid":1300,"unread":true,"content":"<article>arXiv:2502.10263v1 Announce Type: new \nAbstract: Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.</article>","contentLength":1546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MITO: Enabling Non-Line-of-Sight Perception using Millimeter-waves through Real-World Datasets and Simulation Tools","url":"https://arxiv.org/abs/2502.10259","date":1739768400,"author":"","guid":1301,"unread":true,"content":"<article>arXiv:2502.10259v1 Announce Type: new \nAbstract: We present MITO, the first dataset of multi-spectral millimeter-wave (mmWave) images of everyday objects. Unlike visible light, mmWave signals can image through everyday occlusions (e.g., cardboard boxes, fabric, plastic). However, due to the dearth of publicly-available mmWave images and the interdisciplinary challenges in collecting and processing mmWave signals, it remains difficult today for computer vision researchers to develop mmWave-based non-line-of-sight perception algorithms and models.\n  To overcome these challenges, we introduce a real-world dataset and open-source simulation tool for mmWave imaging. The dataset is acquired using a UR5 robotic arm with two mmWave radars operating at different frequencies and an RGB-D camera. Through a signal processing pipeline, we capture and create over 580 real-world 3D mmWave images from over 76 different objects in the YCB dataset, a standard dataset for robotics manipulation. We provide real-world mmWave images in line-of-sight and non-line-of-sight, as well as RGB-D images and ground truth segmentation masks. We also develop an open-source simulation tool that can be used to generate synthetic mmWave images for any 3D triangle mesh, which achieves a median F-Score of 94% when compared to real-world mmWave images.\n  We show the usefulness of this dataset and simulation tool in multiple CV tasks in non-line-of-sight. First, we perform object segmentation for mmWave images using the segment anything model (SAM), and achieve a median precision and recall of 92.6% and 64%. Second, we train a classifier that can recognize objects in non-line-of-sight. It is trained on synthetic images and can classify real-world images with 85% accuracy.\n  We believe MITO will be a valuable resource for computer vision researchers in developing non-line-of-sight perception, similar to how early camera-based datasets shaped the field.</article>","contentLength":1945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PromptArtisan: Multi-instruction Image Editing in Single Pass with Complete Attention Control","url":"https://arxiv.org/abs/2502.10258","date":1739768400,"author":"","guid":1302,"unread":true,"content":"<article>arXiv:2502.10258v1 Announce Type: new \nAbstract: We present PromptArtisan, a groundbreaking approach to multi-instruction image editing that achieves remarkable results in a single pass, eliminating the need for time-consuming iterative refinement. Our method empowers users to provide multiple editing instructions, each associated with a specific mask within the image. This flexibility allows for complex edits involving mask intersections or overlaps, enabling the realization of intricate and nuanced image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix model in conjunction with a novel Complete Attention Control Mechanism (CACM). This mechanism ensures precise adherence to user instructions, granting fine-grained control over the editing process. Furthermore, our approach is zero-shot, requiring no additional training, and boasts improved processing complexity compared to traditional iterative methods. By seamlessly integrating multi-instruction capabilities, single-pass efficiency, and complete attention control, PromptArtisan unlocks new possibilities for creative and efficient image editing workflows, catering to both novice and expert users alike.</article>","contentLength":1193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seamless acceleration of Fortran intrinsics via AMD AI engines","url":"https://arxiv.org/abs/2502.10254","date":1739768400,"author":"","guid":1303,"unread":true,"content":"<article>arXiv:2502.10254v1 Announce Type: new \nAbstract: A major challenge that the HPC community faces is how to continue delivering the performance demanded by scientific programmers, whilst meeting an increased emphasis on sustainable operations. Specialised architectures, such as FPGAs and AMD's AI Engines (AIEs), have been demonstrated to provide significant energy efficiency advantages, however a major challenge is that to most effectively program these architectures requires significant expertise and investment of time which is a major blocker.\n  Fortran in the lingua franca of scientific computing, and in this paper we explore automatically accelerating Fortran intrinsics via the AIEs in AMD's Ryzen AI CPU. Leveraging the open source Flang compiler and MLIR ecosystem, we describe an approach that lowers the MLIR linear algebra dialect to AMD's AIE dialects, and demonstrate that for suitable workloads the AIEs can provide significant performance advantages over the CPU without any code modifications required by the programmer.</article>","contentLength":1041,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision Language Models","url":"https://arxiv.org/abs/2502.10250","date":1739768400,"author":"","guid":1304,"unread":true,"content":"<article>arXiv:2502.10250v1 Announce Type: new \nAbstract: Vision-language models (VLMs) excel in various visual benchmarks but are often constrained by the lack of high-quality visual fine-tuning data. To address this challenge, we introduce VisCon-100K, a novel dataset derived from interleaved image-text web documents. Our approach transforms 45K web documents from the OBELICS dataset into 100K image conversation samples. We utilize GPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert these captions into diverse free-form and multiple-choice question-answer pairs. Integrating this dataset for fine-tuning considerably enhances VLM performance across multiple benchmarks. Unlike methods that focus solely on fine-grained visual content, our approach leverages accompanying web context, yielding superior results. We also discover that a `leaky modality mix,' where conversation samples contain questions answerable from both the image and its contextual caption, outperforms non-leaky combinations of captions and Q\\&amp;A pairs. VisCon-100k dataset shows strong performance with two popular VLM approaches: text-only large language model (LLM) aligned with a vision encoder using image captions data (ShareGPT4V-7b) and multimodally pretrained LLM (IDEFICS2-8b) using interleaved image-text data. In addition to releasing the VisCon-100K dataset, we provide a contextual captioner trained on this dataset, facilitating scalable fine-tuning data generation for future research and open-source applications. Using the same pipeline, but substituting our trained contextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.</article>","contentLength":1659,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding the relationships between the perceptions of burnout and instability in Software Engineering","url":"https://arxiv.org/abs/2502.10249","date":1739768400,"author":"","guid":1305,"unread":true,"content":"<article>arXiv:2502.10249v1 Announce Type: new \nAbstract: Changes are inherent in software development, often increasing developers' perception of instability. Understanding the relationship between human factors and Software Engineering processes is crucial to mitigating and preventing issues. One such factor is burnout, a recognized disease that impacts productivity, turnover, and, most importantly, developers' well-being. Investigating the link between instability and burnout can help organizations implement strategies to improve developers' work conditions and performance.\n  This study aims to identify and describe the relationship between perceived instability and burnout among software developers. A cross-sectional survey was conducted with 411 respondents, using convenience sampling and self-selection. In addition to analyzing variable relationships, confirmatory factor analysis was applied.\n  Key findings include: (1) A significant positive relationship between burnout (exhaustion and cynicism) and team, technological, and task instability; (2) A weak negative relationship between efficacy and technological/team instability, with no correlation to task instability; (3) Exhaustion was the most frequently reported burnout symptom, while task instability was the most perceived type of instability.\n  These results are valuable for both industry and academia, providing insights to reduce burnout and instability among software engineers. Future research can further explore the impact of instability, offering new perspectives on monitoring and mitigating its effects in software development.</article>","contentLength":1609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model","url":"https://arxiv.org/abs/2502.10248","date":1739768400,"author":"","guid":1306,"unread":true,"content":"<article>arXiv:2502.10248v1 Announce Type: new \nAbstract: We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.</article>","contentLength":1494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safety Blind Spot in Remote Driving: Considerations for Risk Assessment of Connection Loss Fallback Strategies","url":"https://arxiv.org/abs/2502.10243","date":1739768400,"author":"","guid":1307,"unread":true,"content":"<article>arXiv:2502.10243v1 Announce Type: new \nAbstract: As part of the overall goal of driverless road vehicles, remote driving is a major emerging field of research of its own. Current remote driving concepts for public road traffic often establish a fallback strategy of immediate braking to a standstill in the event of a connection loss. This may seem like the most logical option when human control of the vehicle is lost. However, our simulation results from hundreds of scenarios based on naturalistic traffic scenes indicate high collision rates for any immediate substantial deceleration to a standstill in urban settings. We show that such a fallback strategy can result in a SOTIF relevant hazard, making it questionable whether such a design decision can be considered acceptable. Therefore, from a safety perspective, we would call this problem a safety blind spot, as safety analyses in this regard seem to be very rare.\n  In this article, we first present a simulation on a naturalistic dataset that shows a high probability of collision in the described case. Second, we discuss the severity of the resulting potential rear-end collisions and provide an even more severe example by including a large commercial vehicle in the potential collision.</article>","contentLength":1255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Zero-Order Federated Finetuning of Language Models for Resource-Constrained Devices","url":"https://arxiv.org/abs/2502.10239","date":1739768400,"author":"","guid":1308,"unread":true,"content":"<article>arXiv:2502.10239v1 Announce Type: new \nAbstract: Federated fine-tuning offers a promising approach for tuning Large Language Models (LLMs) on edge devices while preserving data privacy. However, fine-tuning these models on edge devices remains challenging due to high memory, communication, and computational demands. Zero-order optimization with task alignment provides a potential solution, enabling fine-tuning with inference-level memory requirements but requires a longer convergence time. In this paper, we propose Federated Split-Perturbation Zero-order Optimization (FedSPZO) that divides the network into two blocks, applying a different number of perturbations per block in a computationally effective way, achieving faster convergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation overhead compared to zero-order state of the art techniques in federated learning.</article>","contentLength":894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control","url":"https://arxiv.org/abs/2502.10236","date":1739768400,"author":"","guid":1309,"unread":true,"content":"<article>arXiv:2502.10236v1 Announce Type: new \nAbstract: Diffusion Probabilistic Models (DPMs) are powerful generative models that have achieved unparalleled success in a number of generative tasks. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. For topologically structured data, we devise a frequency-based noising operator to purposefully manipulate, and set, these inductive biases. We first show that appropriate manipulations of the noising forward process can lead DPMs to focus on particular aspects of the distribution to learn. We show that different datasets necessitate different inductive biases, and that appropriate frequency-based noise control induces increased generative performance compared to standard diffusion. Finally, we demonstrate the possibility of ignoring information at particular frequencies while learning. We show this in an image corruption and recovery task, where we train a DPM to recover the original target distribution after severe noise corruption.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via Hierarchical and Parallel Decoding","url":"https://arxiv.org/abs/2502.10233","date":1739768400,"author":"","guid":1310,"unread":true,"content":"<article>arXiv:2502.10233v1 Announce Type: new \nAbstract: The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge in warehouse logistics, where pickers must navigate a mixed-shelves environment to retrieve SKUs efficiently. Traditional heuristics and optimization-based approaches struggle with scalability, while recent machine learning methods often rely on sequential decision-making, leading to high solution latency and suboptimal agent coordination. In this work, we propose a novel hierarchical and parallel decoding approach for solving the min-max variant of the MSPRP via multi-agent reinforcement learning. While our approach generates a joint distribution over agent actions, allowing for fast decoding and effective picker coordination, our method introduces a sequential action selection to avoid conflicts in the multi-dimensional action space. Experiments show state-of-the-art performance in both solution quality and inference speed, particularly for large-scale and out-of-distribution instances. Our code is publicly available at http://github.com/LTluttmann/marl4msprp.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProReco: A Process Discovery Recommender System","url":"https://arxiv.org/abs/2502.10230","date":1739768400,"author":"","guid":1311,"unread":true,"content":"<article>arXiv:2502.10230v1 Announce Type: new \nAbstract: Process discovery aims to automatically derive process models from historical execution data (event logs). While various process discovery algorithms have been proposed in the last 25 years, there is no consensus on a dominating discovery algorithm. Selecting the most suitable discovery algorithm remains a challenge due to competing quality measures and diverse user requirements. Manually selecting the most suitable process discovery algorithm from a range of options for a given event log is a time-consuming and error-prone task. This paper introduces ProReco, a Process discovery Recommender system designed to recommend the most appropriate algorithm based on user preferences and event log characteristics. ProReco incorporates state-of-the-art discovery algorithms, extends the feature pools from previous work, and utilizes eXplainable AI (XAI) techniques to provide explanations for its recommendations.</article>","contentLength":964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Multiagent Path Search Algorithm for Large-Scale Coalition Structure Generation","url":"https://arxiv.org/abs/2502.10226","date":1739768400,"author":"","guid":1312,"unread":true,"content":"<article>arXiv:2502.10226v1 Announce Type: new \nAbstract: Coalition structure generation (CSG), i.e. the problem of optimally partitioning a set of agents into coalitions to maximize social welfare, is a fundamental computational problem in multiagent systems. This problem is important for many applications where small run times are necessary, including transportation and disaster response. In this paper, we develop SALDAE, a multiagent path finding algorithm for CSG that operates on a graph of coalition structures. Our algorithm utilizes a variety of heuristics and strategies to perform the search and guide it. It is an anytime algorithm that can handle large problems with hundreds and thousands of agents. We show empirically on nine standard value distributions, including disaster response and electric vehicle allocation benchmarks, that our algorithm enables a rapid finding of high-quality solutions and compares favorably with other state-of-the-art methods.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comparison of Deep Recurrent Neural Networks and Bayesian Neural Networks for Detecting Electric Motor Damage Through Sound Signal Analysis","url":"https://arxiv.org/abs/2502.10224","date":1739768400,"author":"","guid":1313,"unread":true,"content":"<article>arXiv:2502.10224v1 Announce Type: new \nAbstract: Fault detection in electric motors is a critical challenge in various industries, where failures can result in significant operational disruptions. This study investigates the use of Recurrent Neural Networks (RNNs) and Bayesian Neural Networks (BNNs) for diagnosing motor damage using acoustic signal analysis. A novel approach is proposed, leveraging frequency domain representation of sound signals for enhanced diagnostic accuracy. The architectures of both RNNs and BNNs are designed and evaluated on real-world acoustic data collected from household appliances using smartphones. Experimental results demonstrate that BNNs provide superior fault detection performance, particularly for imbalanced datasets, offering more robust and interpretable predictions compared to traditional methods. The findings suggest that BNNs, with their ability to incorporate uncertainty, are well-suited for industrial diagnostic applications. Further analysis and benchmarks are suggested to explore resource efficiency and classification capabilities of these architectures.</article>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal and Coordinated Voltage Control: Case Study on a 132 kV Norwegian Grid Subsystem","url":"https://arxiv.org/abs/2502.10220","date":1739768400,"author":"","guid":1314,"unread":true,"content":"<article>arXiv:2502.10220v1 Announce Type: new \nAbstract: This work presents a framework for dynamic performance assessment of the higher layers in the hierarchical voltage regulation scheme, with case studies applied to specific areas of the Norwegian grid. Unlike the primary (PVR) level, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single device at a time, handling instead several reactive power resources available within a control zone including generator units, static VAr compensators and others. Proper SVR-TVR coordination for realistic transmission systems is a challenging topic at the core of many ongoing discussions in voltage control literature. Special focus is placed on practical considerations from the system operator perspective, since this research is also aimed at simplifying daily control centre routines. Dynamic simulation results concern a 21-bus equivalent of a 132 kV network model that accurately represents a Norwegian grid subsystem. Case studies address daily grid operation with real-life load demand and wind power generation profiles, showing that the proposed strategy is effective not only to minimize total active power losses as much as possible within system-wide limitations, but also to maintain adequate voltage profiles and reactive power flows. Findings pertaining to this work showcase the benefits of applying hierarchical voltage regulation layers as an asset to day-to-day control center management of a realistic transmission network.</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrated Multi-Simulation Environments for Aerial Robotics Research","url":"https://arxiv.org/abs/2502.10218","date":1739768400,"author":"","guid":1315,"unread":true,"content":"<article>arXiv:2502.10218v1 Announce Type: new \nAbstract: Simulation frameworks play a pivotal role in the safe development of robotic applications. However, often different components of an envisioned robotic system are best simulated in different environments/simulators. This poses a significant challenge in simulating the entire project into a single integrated robotic framework. Specifically, for partially-open or closed-source simulators, often two core limitations arise. i) Actors in the scene other than the designated robots cannot be controlled during runtime via interfaces such as ROS and ii) retrieving real-time state information (such as pose, velocity etc.) of objects in the scene is prevented. In this work, we address these limitations and describe our solution for the use case of integrating aerial drones simulated by the powerful simulator Sphinx (provided by Parrot Drone) into the Gazebo simulator. We achieve this by means of a mirrored instance of a drone that is included into existing Gazebo-based environments. A promising application of our integrated simulation environment is the task of target tracking that is common in aerial multi-robot scenarios. Therefore, to demonstrate the effectiveness our our integrated simulation, we also implement a model predictive controller (MPC) that outperforms the default PID-based controller framework provided with the Parrot's popular Anafi drone in various dynamic tracking scenarios thus enhancing the utility of the overall system. We test our solution by including the Anafi drone in an existing Gazebo-based simulation and evaluate the performance of the MPC through rigorous testing in simulated and real-world tracking experiments against a customized PID controller baseline. Source code is published on https://github.com/robot-perception-group/anafi_sim.</article>","contentLength":1833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Forget the Data and Fine-Tuning! Just Fold the Network to Compress","url":"https://arxiv.org/abs/2502.10216","date":1739768400,"author":"","guid":1316,"unread":true,"content":"<article>arXiv:2502.10216v1 Announce Type: new \nAbstract: We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.</article>","contentLength":916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Large Language Models Reason Causally Like Us? Even Better?","url":"https://arxiv.org/abs/2502.10215","date":1739768400,"author":"","guid":1317,"unread":true,"content":"<article>arXiv:2502.10215v1 Announce Type: new \nAbstract: Causal reasoning is a core component of intelligence. Large language models (LLMs) have shown impressive capabilities in generating human-like text, raising questions about whether their responses reflect true understanding or statistical patterns. We compared causal reasoning in humans and four LLMs using tasks based on collider graphs, rating the likelihood of a query variable occurring given evidence from other variables. We find that LLMs reason causally along a spectrum from human-like to normative inference, with alignment shifting based on model, context, and task. Overall, GPT-4o and Claude showed the most normative behavior, including \"explaining away\", whereas Gemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected independence of causes - Claude the least - they exhibited strong associative reasoning and predictive inference when assessing the likelihood of the effect given its causes. These findings underscore the need to assess AI biases as they increasingly assist human decision-making.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mapping bathymetry of inland water bodies on the North Slope of Alaska with Landsat using Random Forest","url":"https://arxiv.org/abs/2502.10214","date":1739768400,"author":"","guid":1318,"unread":true,"content":"<article>arXiv:2502.10214v1 Announce Type: new \nAbstract: The North Slope of Alaska is dominated by small waterbodies that provide critical ecosystem services for local population and wildlife. Detailed information on the depth of the waterbodies is scarce due to the challenges with collecting such information. In this work we have trained a machine learning (Random Forest Regressor) model to predict depth from multispectral Landsat data in waterbodies across the North Slope of Alaska. The greatest challenge is the scarcity of in situ data, which is expensive and difficult to obtain, to train the model. We overcame this challenge by using modeled depth predictions from a prior study as synthetic training data to provide a more diverse training data pool for the Random Forest. The final Random Forest model was more robust than models trained directly on the in situ data and when applied to 208 Landsat 8 scenes from 2016 to 2018 yielded a map with an overall $r^{2}$ value of 0.76 on validation. The final map has been made available through the Oak Ridge National Laboratory Distribute Active Archive Center (ORNL-DAAC). This map represents a first of its kind regional assessment of waterbody depth with per pixel estimates of depth for the entire North Slope of Alaska.</article>","contentLength":1275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Control-flow anomaly detection by process mining-based feature extraction and dimensionality reduction","url":"https://arxiv.org/abs/2502.10211","date":1739768400,"author":"","guid":1319,"unread":true,"content":"<article>arXiv:2502.10211v1 Announce Type: new \nAbstract: The business processes of organizations may deviate from normal control flow due to disruptive anomalies, including unknown, skipped, and wrongly-ordered activities. To identify these control-flow anomalies, process mining can check control-flow correctness against a reference process model through conformance checking, an explainable set of algorithms that allows linking any deviations with model elements. However, the effectiveness of conformance checking-based techniques is negatively affected by noisy event data and low-quality process models. To address these shortcomings and support the development of competitive and explainable conformance checking-based techniques for control-flow anomaly detection, we propose a novel process mining-based feature extraction approach with alignment-based conformance checking. This variant aligns the deviating control flow with a reference process model; the resulting alignment can be inspected to extract additional statistics such as the number of times a given activity caused mismatches. We integrate this approach into a flexible and explainable framework for developing techniques for control-flow anomaly detection. The framework combines process mining-based feature extraction and dimensionality reduction to handle high-dimensional feature sets, achieve detection effectiveness, and support explainability. The results show that the framework techniques implementing our approach outperform the baseline conformance checking-based techniques while maintaining the explainable nature of conformance checking. We also provide an explanation of why existing conformance checking-based techniques may be ineffective.</article>","contentLength":1724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mutual Coupling in Holographic MIMO: Physical Modeling and Information-Theoretic Analysis","url":"https://arxiv.org/abs/2502.10209","date":1739768400,"author":"","guid":1320,"unread":true,"content":"<article>arXiv:2502.10209v1 Announce Type: new \nAbstract: This paper presents a comprehensive framework for holographic multiantenna communication, a paradigm that integrates both wide apertures and closely spaced antennas relative to the wavelength. The presented framework is physically grounded, enabling information-theoretic analyses that inherently incorporate correlation and mutual coupling among the antennas. This establishes the combined effects of correlation and coupling on the information-theoretic performance limits across SNR levels. Additionally, it reveals that, by suitably selecting the individual antenna patterns, mutual coupling can be harnessed to either reinforce or counter spatial correlations as appropriate for specific SNRs, thereby improving the performance.</article>","contentLength":782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SGS-GNN: A Supervised Graph Sparsification method for Graph Neural Networks","url":"https://arxiv.org/abs/2502.10208","date":1739768400,"author":"","guid":1321,"unread":true,"content":"<article>arXiv:2502.10208v1 Announce Type: new \nAbstract: We propose SGS-GNN, a novel supervised graph sparsifier that learns the sampling probability distribution of edges and samples sparse subgraphs of a user-specified size to reduce the computational costs required by GNNs for inference tasks on large graphs. SGS-GNN employs regularizers in the loss function to enhance homophily in sparse subgraphs, boosting the accuracy of GNNs on heterophilic graphs, where a significant number of the neighbors of a node have dissimilar labels. SGS-GNN also supports conditional updates of the probability distribution learning module based on a prior, which helps narrow the search space for sparse graphs. SGS-GNN requires fewer epochs to obtain high accuracies since it learns the search space of subgraphs more effectively than methods using fixed distributions such as random sampling. Extensive experiments using 33 homophilic and heterophilic graphs demonstrate the following: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN improves the F1-scores by a geometric mean of 4% relative to the original graph; on heterophilic graphs, the prediction accuracy is better up to 30%. (ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores of 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and (iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires about half the number of epochs to converge.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RIPOST: Two-Phase Private Decomposition for Multidimensional Data","url":"https://arxiv.org/abs/2502.10207","date":1739768400,"author":"","guid":1322,"unread":true,"content":"<article>arXiv:2502.10207v1 Announce Type: new \nAbstract: Differential privacy (DP) is considered as the gold standard for data privacy. While the problem of answering simple queries and functions under DP guarantees has been thoroughly addressed in recent years, the problem of releasing multidimensional data under DP remains challenging. In this paper, we focus on this problem, in particular on how to construct privacy-preserving views using a domain decomposition approach. The main idea is to recursively split the domain into sub-domains until a convergence condition is met. The resulting sub-domains are perturbed and then published in order to be used to answer arbitrary queries. Existing methods that have addressed this problem using domain decomposition face two main challenges: (i) efficient privacy budget management over a variable and undefined decomposition depth $h$; and (ii) defining an optimal data-dependent splitting strategy that minimizes the error in the sub-domains while ensuring the smallest possible decomposition. To address these challenges, we present RIPOST, a multidimensional data decomposition algorithm that bypasses the constraint of predefined depth $h$ and applies a data-aware splitting strategy to optimize the quality of the decomposition results.The core of RIPOST is a two-phase strategy that separates non-empty sub-domains at an early stage from empty sub-domains by exploiting the properties of multidimensional datasets, and then decomposes the resulting sub-domains with minimal inaccuracies using the mean function. Moreover, RIPOST introduces a privacy budget distribution that allows decomposition without requiring prior computation of the depth $h$. Through extensive experiments, we demonstrated that \\texttt{RIPOST} outperforms state-of-the-art methods in terms of data utility and accuracy on a variety of datasets and test cases</article>","contentLength":1883,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Looking around you: external information enhances representations for event sequences","url":"https://arxiv.org/abs/2502.10205","date":1739768400,"author":"","guid":1323,"unread":true,"content":"<article>arXiv:2502.10205v1 Announce Type: new \nAbstract: Representation learning produces models in different domains, such as store purchases, client transactions, and general people's behaviour. However, such models for sequential data usually process a single sequence, ignoring context from other relevant ones, even in domains with rapidly changing external environments like finance or misguiding the prediction for a user with no recent events.\n  We are the first to propose a method that aggregates information from multiple user representations augmenting a specific user one for a scenario of multiple co-occurring event sequences. Our study considers diverse aggregation approaches, ranging from simple pooling techniques to trainable attention-based approaches, especially Kernel attention aggregation, that can highlight more complex information flow from other users. The proposed method operates atop an existing encoder and supports its efficient fine-tuning. Across considered datasets of financial transactions and downstream tasks, Kernel attention improves ROC AUC scores, both with and without fine-tuning, while mean pooling yields a smaller but still significant gain.</article>","contentLength":1183,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-in-the-Loop Sensing and Communication Joint Design for Edge Intelligence","url":"https://arxiv.org/abs/2502.10203","date":1739768400,"author":"","guid":1324,"unread":true,"content":"<article>arXiv:2502.10203v1 Announce Type: new \nAbstract: Recent breakthroughs in artificial intelligence (AI), wireless communications, and sensing technologies have accelerated the evolution of edge intelligence. However, conventional systems still grapple with issues such as low communication efficiency, redundant data acquisition, and poor model generalization. To overcome these challenges, we propose an innovative framework that enhances edge intelligence through AI-in-the-loop joint sensing and communication (JSAC). This framework features an AI-driven closed-loop control architecture that jointly optimizes system resources, thereby delivering superior system-level performance. A key contribution of our work is establishing an explicit relationship between validation loss and the system's tunable parameters. This insight enables dynamic reduction of the generalization error through AI-driven closed-loop control. Specifically, for sensing control, we introduce an adaptive data collection strategy based on gradient importance sampling, allowing edge devices to autonomously decide when to terminate data acquisition and how to allocate sample weights based on real-time model feedback. For communication control, drawing inspiration from stochastic gradient Langevin dynamics (SGLD), our joint optimization of transmission power and batch size converts channel and data noise into gradient perturbations that help mitigate overfitting. Experimental evaluations demonstrate that our framework reduces communication energy consumption by up to 77 percent and sensing costs measured by the number of collected samples by up to 52 percent while significantly improving model generalization -- with up to 58 percent reductions of the final validation loss. It validates that the proposed scheme can harvest the mutual benefit of AI and JSAC systems by incorporating the model itself into the control loop of the system.</article>","contentLength":1925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Post-Training Quantization Benefit from an Additional QLoRA Integration?","url":"https://arxiv.org/abs/2502.10202","date":1739768400,"author":"","guid":1325,"unread":true,"content":"<article>arXiv:2502.10202v1 Announce Type: new \nAbstract: Large language models (LLMs) have transformed natural language processing but pose significant challenges for real-world deployment. These models necessitate considerable computing resources, which can be costly and frequently unavailable. Model compression techniques such as quantization are often leveraged to alleviate resource demand, but they may have a negative impact on the generation quality. In this study, we explore the integration of 4-bit Post-training Quantization (PTQ) with QLoRA to address these issues. We demonstrate through extensive experiments that this integration outperforms standard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs, validated across proprietary and public datasets with different quantization algorithms. The results demonstrate the efficacy of PTQ-QLoRA integration, offering a viable solution for deploying powerful LLMs in resource-constrained environments without compromising on performance.</article>","contentLength":1011,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prediction hubs are context-informed frequent tokens in LLMs","url":"https://arxiv.org/abs/2502.10201","date":1739768400,"author":"","guid":1326,"unread":true,"content":"<article>arXiv:2502.10201v1 Announce Type: new \nAbstract: Hubness, the tendency for few points to be among the nearest neighbours of a disproportionate number of other points, commonly arises when applying standard distance measures to high-dimensional data, often negatively impacting distance-based analysis. As autoregressive large language models (LLMs) operate on high-dimensional representations, we ask whether they are also affected by hubness. We first show, theoretically, that the only representation comparison operation performed by LLMs, namely that between context and unembedding vectors to determine continuation probabilities, is not characterized by the concentration of distances phenomenon that typically causes the appeareance of nuisance hubness. We then empirically show that this comparison still leads to a high degree of hubness, but the hubs in this case do not constitute a disturbance. They are rather the result of context-modulated frequent tokens often appearing in the pool of likely candidates for next token prediction. On the other hand, when other distance computations involving LLM representations are performed, we do not have the same theoretical guarantees, and, indeed, we see nuisance hubs appear. In summary, our work highlights, on the one hand, how hubness, while omnipresent in high-dimensional spaces, is not always a negative property that needs to be mitigated, and, on the other hand, it shows that various widely-used LLMs have developed a guessing strategy that consists in constantly assigning a high probability to frequent tokens.</article>","contentLength":1579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Reinforcement Learning for Actors","url":"https://arxiv.org/abs/2502.10200","date":1739768400,"author":"","guid":1327,"unread":true,"content":"<article>arXiv:2502.10200v1 Announce Type: new \nAbstract: Dynamic Reinforcement Learning (Dynamic RL), proposed in this paper, directly controls system dynamics, instead of the actor (action-generating neural network) outputs at each moment, bringing about a major qualitative shift in reinforcement learning (RL) from static to dynamic. The actor is initially designed to generate chaotic dynamics through the loop with its environment, enabling the agent to perform flexible and deterministic exploration. Dynamic RL controls global system dynamics using a local index called \"sensitivity,\" which indicates how much the input neighborhood contracts or expands into the corresponding output neighborhood through each neuron's processing. While sensitivity adjustment learning (SAL) prevents excessive convergence of the dynamics, sensitivity-controlled reinforcement learning (SRL) adjusts them -- to converge more to improve reproducibility around better state transitions with positive TD error and to diverge more to enhance exploration around worse transitions with negative TD error. Dynamic RL was applied only to the actor in an Actor-Critic RL architecture while applying it to the critic remains a challenge. It was tested on two dynamic tasks and functioned effectively without external exploration noise or backward computation through time. Moreover, it exhibited excellent adaptability to new environments, although some problems remain. Drawing parallels between 'exploration' and 'thinking,' the author hypothesizes that \"exploration grows into thinking through learning\" and believes this RL could be a key technique for the emergence of thinking, including inspiration that cannot be reconstructed from massive existing text data. Finally, despite being presumptuous, the author presents the argument that this research should not proceed due to its potentially fatal risks, aiming to encourage discussion.</article>","contentLength":1915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the unconventional Hug integrator","url":"https://arxiv.org/abs/2502.10199","date":1739768400,"author":"","guid":1328,"unread":true,"content":"<article>arXiv:2502.10199v1 Announce Type: new \nAbstract: Hug is a recently proposed iterative mapping used to design efficient updates in Markov chain Monte Carlo (MCMC) methods when sampling along manifolds is of interest. In this paper we show that Hug may be interpreted as a consistent discretization of a system of differential equations with a rather complicated structure. The proof of convergence of this discretization includes a number of unusual features we explore fully. We uncover an unexpected and, yet, undocumented property of the solutions of the underlying dynamical system that manifest itself by the existence of Hug trajectories that fail to cover the manifold of interest. This suggests caution when using the Hug update.</article>","contentLength":736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MathConstruct: Challenging LLM Reasoning with Constructive Proofs","url":"https://arxiv.org/abs/2502.10197","date":1739768400,"author":"","guid":1329,"unread":true,"content":"<article>arXiv:2502.10197v1 Announce Type: new \nAbstract: While Large Language Models (LLMs) demonstrate impressive performance in mathematics, existing math benchmarks come with significant limitations. Many focus on problems with fixed ground-truth answers, and are often saturated due to problem simplicity or the viability of guessing or memorization. Crucially, they capture only a narrow subset of relevant math problems. To address this research gap, we introduce \\mc, a new benchmark of 126 challenging problems sourced from various math competitions, which targets constructive proofs, a widely encountered problem type requiring the construction of mathematical objects with specific properties. These proofs are particularly suitable for LLM evaluation, as solution correctness can be easily verified. Our automated verifiers also enable MathConstruct to generate problem variations, used to evaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct problems, highlighting its complexity and importance for LLM evaluation.</article>","contentLength":1041,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring the Camera Bias of Person Re-identification","url":"https://arxiv.org/abs/2502.10195","date":1739768400,"author":"","guid":1330,"unread":true,"content":"<article>arXiv:2502.10195v1 Announce Type: new \nAbstract: We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.</article>","contentLength":1559,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Translating Common Security Assertions Across Processor Designs: A RISC-V Case Study","url":"https://arxiv.org/abs/2502.10194","date":1739768400,"author":"","guid":1331,"unread":true,"content":"<article>arXiv:2502.10194v1 Announce Type: new \nAbstract: RISC-V is gaining popularity for its adaptability and cost-effectiveness in processor design. With the increasing adoption of RISC-V, the importance of implementing robust security verification has grown significantly. In the state of the art, various approaches have been developed to strengthen the security verification process. Among these methods, assertion-based security verification has proven to be a promising approach for ensuring that security features are effectively met. To this end, some approaches manually define security assertions for processor designs; however, these manual methods require significant time, cost, and human expertise. Consequently, recent approaches focus on translating pre-defined security assertions from one design to another. Nonetheless, these methods are not primarily centered on processor security, particularly RISC-V. Furthermore, many of these approaches have not been validated against real-world attacks, such as hardware Trojans. In this work, we introduce a methodology for translating security assertions across processors with different architectures, using RISC-V as a case study. Our approach reduces time and cost compared to developing security assertions manually from the outset. Our methodology was applied to five critical security modules with assertion translation achieving nearly 100% success across all modules. These results validate the efficacy of our approach and highlight its potential for enhancing security verification in modern processor designs. The effectiveness of the translated assertions was rigorously tested against hardware Trojans defined by large language models (LLMs), demonstrating their reliability in detecting security breaches.</article>","contentLength":1774,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Merging public elementary schools to reduce racial/ethnic segregation","url":"https://arxiv.org/abs/2502.10193","date":1739768400,"author":"","guid":1332,"unread":true,"content":"<article>arXiv:2502.10193v1 Announce Type: new \nAbstract: Diverse schools can help address implicit biases and increase empathy, mutual respect, and reflective thought by fostering connections between students from different racial/ethnic, socioeconomic, and other backgrounds. Unfortunately, demographic segregation remains rampant in US public schools, despite over 70 years since the passing of federal legislation formally outlawing segregation by race. However, changing how students are assigned to schools can help foster more integrated learning environments. In this paper, we explore \"school mergers\" as one such under-explored, yet promising, student assignment policy change. School mergers involve merging the school attendance boundaries, or catchment areas, of schools and subsequently changing the grades each school offers. We develop an algorithm to simulate elementary school mergers across 200 large school districts serving 4.5 million elementary school students and find that pairing or tripling schools in this way could reduce racial/ethnic segregation by a median relative 20% -- and as much as nearly 60% in some districts -- while increasing driving times to schools by an average of a few minutes each way. Districts with many interfaces between racially/ethnically-disparate neighborhoods tend to be prime candidates for mergers. We also compare the expected results of school mergers to other typical integration policies, like redistricting, and find that different policies may be more or less suitable in different places. Finally, we make our results available through a public dashboard for policymakers and community members to explore further (https://mergers.schooldiversity.org). Together, our study offers new findings and tools to support integration policy-making across US public school districts.</article>","contentLength":1831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Note on \"Constructing Bent Functions Outside the Maiorana-McFarland Class Using a General Form of Rothaus\"","url":"https://arxiv.org/abs/2502.10192","date":1739768400,"author":"","guid":1333,"unread":true,"content":"<article>arXiv:2502.10192v1 Announce Type: new \nAbstract: In 2017, Zhang et al. proposed a question (not open problem) and two open problems in [IEEE TIT 63 (8): 5336--5349, 2017] about constructing bent functions by using Rothaus' construction. In this note, we prove that the sufficient conditions of Rothaus' construction are also necessary, which answers their question. Besides, we demonstrate that the second open problem, which considers the iterative method of constructing bent functions by using Rothaus' construction, has only a trivial solution. It indicates that all bent functions obtained by using Rothaus' construction iteratively can be generated from the direct sum of an initial bent function and a quadratic bent function. This directly means that Zhang et al.'s construction idea makes no contribution to the construction of bent functions. To compensate the weakness of their work, we propose an iterative construction of bent functions by using a secondary construction in [DCC 88: 2007--2035, 2020].</article>","contentLength":1014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VideoDiff: Human-AI Video Co-Creation with Alternatives","url":"https://arxiv.org/abs/2502.10190","date":1739768400,"author":"","guid":1334,"unread":true,"content":"<article>arXiv:2502.10190v1 Announce Type: new \nAbstract: To make an engaging video, people sequence interesting moments and add visuals such as B-rolls or text. While video editing requires time and effort, AI has recently shown strong potential to make editing easier through suggestions and automation. A key strength of generative models is their ability to quickly generate multiple variations, but when provided with many alternatives, creators struggle to compare them to find the best fit. We propose VideoDiff, an AI video editing tool designed for editing with alternatives. With VideoDiff, creators can generate and review multiple AI recommendations for each editing process: creating a rough cut, inserting B-rolls, and adding text effects. VideoDiff simplifies comparisons by aligning videos and highlighting differences through timelines, transcripts, and video previews. Creators have the flexibility to regenerate and refine AI suggestions as they compare alternatives. Our study participants (N=12) could easily compare and customize alternatives, creating more satisfying results.</article>","contentLength":1090,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning based Constrained Optimal Control: an Interpretable Reward Design","url":"https://arxiv.org/abs/2502.10187","date":1739768400,"author":"","guid":1335,"unread":true,"content":"<article>arXiv:2502.10187v1 Announce Type: new \nAbstract: This paper presents an interpretable reward design framework for reinforcement learning based constrained optimal control problems with state and terminal constraints. The problem is formalized within a standard partially observable Markov decision process framework. The reward function is constructed from four weighted components: a terminal constraint reward, a guidance reward, a penalty for state constraint violations, and a cost reduction incentive reward. A theoretically justified reward design is then presented, which establishes bounds on the weights of the components. This approach ensures that constraints are satisfied and objectives are optimized while mitigating numerical instability. Acknowledging the importance of prior knowledge in reward design, we sequentially solve two subproblems, using each solution to inform the reward design for the subsequent problem. Subsequently, we integrate reinforcement learning with curriculum learning, utilizing policies derived from simpler subproblems to assist in tackling more complex challenges, thereby facilitating convergence. The framework is evaluated against original and randomly weighted reward designs in a multi-agent particle environment. Experimental results demonstrate that the proposed approach significantly enhances satisfaction of terminal and state constraints and optimization of control cost.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Powerful Random Forest Featuring Linear Extensions (RaFFLE)","url":"https://arxiv.org/abs/2502.10185","date":1739768400,"author":"","guid":1336,"unread":true,"content":"<article>arXiv:2502.10185v1 Announce Type: new \nAbstract: Random forests are widely used in regression. However, the decision trees used as base learners are poor approximators of linear relationships. To address this limitation we propose RaFFLE (Random Forest Featuring Linear Extensions), a novel framework that integrates the recently developed PILOT trees (Piecewise Linear Organic Trees) as base learners within a random forest ensemble. PILOT trees combine the computational efficiency of traditional decision trees with the flexibility of linear model trees. To ensure sufficient diversity of the individual trees, we introduce an adjustable regularization parameter and use node-level feature sampling. These modifications improve the accuracy of the forest. We establish theoretical guarantees for the consistency of RaFFLE under weak conditions, and its faster convergence when the data are generated by a linear model. Empirical evaluations on 136 regression datasets demonstrate that RaFFLE outperforms the classical CART and random forest methods, the regularized linear methods Lasso and Ridge, and the state-of-the-art XGBoost algorithm, across both linear and nonlinear datasets. By balancing predictive accuracy and computational efficiency, RaFFLE proves to be a versatile tool for tackling a wide variety of regression problems.</article>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Realistic Evaluation of Deep Partial-Label Learning Algorithms","url":"https://arxiv.org/abs/2502.10184","date":1739768400,"author":"","guid":1337,"unread":true,"content":"<article>arXiv:2502.10184v1 Announce Type: new \nAbstract: Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future.</article>","contentLength":1717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Doing More With Less: Towards More Data-Efficient Syndrome-Based Neural Decoders","url":"https://arxiv.org/abs/2502.10183","date":1739768400,"author":"","guid":1338,"unread":true,"content":"<article>arXiv:2502.10183v1 Announce Type: new \nAbstract: While significant research efforts have been directed toward developing more capable neural decoding architectures, comparatively little attention has been paid to the quality of training data. In this study, we address the challenge of constructing effective training datasets to maximize the potential of existing syndrome-based neural decoder architectures. We emphasize the advantages of using fixed datasets over generating training data dynamically and explore the problem of selecting appropriate training targets within this framework. Furthermore,we propose several heuristics for selecting training samples and present experimental evidence demonstrating that, with carefully curated datasets, it is possible to train neural decoders to achieve superior performance while requiring fewer training examples.</article>","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safe platooning control of connected and autonomous vehicles on curved multi-lane roads","url":"https://arxiv.org/abs/2502.10180","date":1739768400,"author":"","guid":1339,"unread":true,"content":"<article>arXiv:2502.10180v1 Announce Type: new \nAbstract: This paper investigates the safe platoon formation tracking and merging control problem of connected and automated vehicles (CAVs) on curved multi-lane roads. The first novelty is the separation of the control designs into two distinct parts: a lateral control law that ensures a geometrical convergence towards the reference path regardless of the translational velocity, and a longitudinal control design for each vehicle to achieve the desired relative arc length and velocity with respect to its neighboring vehicle. The second novelty is exploiting the constructive barrier feedback as an additive term to the nominal tracking control, ensuring both lateral and longitudinal collision avoidance. This constructive barrier feedback acts as a dissipative term, slowing down the relative velocity toward obstacles without affecting the nominal controller's performance. Consequently, our proposed control method enables safe platoon formation of vehicles on curved multi-lane roads, with theoretical guarantees for safety invariance and stability analysis. Simulation and experimental results on connected vehicles are provided to further validate the effectiveness of the proposed method.</article>","contentLength":1240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Markov to Laplace: How Mamba In-Context Learns Markov Chains","url":"https://arxiv.org/abs/2502.10178","date":1739768400,"author":"","guid":1340,"unread":true,"content":"<article>arXiv:2502.10178v1 Announce Type: new \nAbstract: While transformer-based language models have driven the AI revolution thus far, their computational complexity has spurred growing interest in viable alternatives, such as structured state space sequence models (SSMs) and Selective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown remarkable inference speed ups over transformers while achieving comparable or superior performance on complex language modeling tasks. However, despite these architectural innovations and empirical successes, the fundamental learning capabilities of Mamba remain poorly understood. In this paper, we address this gap by studying in-context learning (ICL) on Markov chains and uncovering a surprising phenomenon: unlike transformers, even a single-layer Mamba efficiently learns the in-context Laplacian smoothing estimator, which is both Bayes and minimax optimal, for all Markovian orders. To explain this, we theoretically characterize the representation capacity of Mamba and reveal the fundamental role of convolution in enabling it to represent the optimal Laplacian smoothing. These theoretical insights align strongly with empirical results and, to the best of our knowledge, represent the first formal connection between Mamba and optimal statistical estimators. Finally, we outline promising research directions inspired by these findings.</article>","contentLength":1395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task Planning","url":"https://arxiv.org/abs/2502.10177","date":1739768400,"author":"","guid":1341,"unread":true,"content":"<article>arXiv:2502.10177v1 Announce Type: new \nAbstract: A key objective of embodied intelligence is enabling agents to perform long-horizon tasks in dynamic environments while maintaining robust decision-making and adaptability. To achieve this goal, we propose the Spatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task planning and execution by integrating spatio-temporal memory. STMA is built upon three critical components: (1) a spatio-temporal memory module that captures historical and environmental changes in real time, (2) a dynamic knowledge graph that facilitates adaptive spatial reasoning, and (3) a planner-critic mechanism that iteratively refines task strategies. We evaluate STMA in the TextWorld environment on 32 tasks, involving multi-step planning and exploration under varying levels of complexity. Experimental results demonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7% increase in average score compared to the state-of-the-art model. The results highlight the effectiveness of spatio-temporal memory in advancing the memory capabilities of embodied agents.</article>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Technical Risks of (Lethal) Autonomous Weapons Systems","url":"https://arxiv.org/abs/2502.10174","date":1739768400,"author":"","guid":1342,"unread":true,"content":"<article>arXiv:2502.10174v1 Announce Type: new \nAbstract: The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS in short, promise unprecedented operational capabilities, but they also introduce profound risks that challenge the principles of control, accountability, and stability in international security. This report outlines the key technological risks associated with (L)AWS deployment, emphasizing their unpredictability, lack of transparency, and operational unreliability, which can lead to severe unintended consequences.\n  Key Takeaways:\n  1. Proposed advantages of (L)AWS can only be achieved through objectification and classification, but a range of systematic risks limit the reliability and predictability of classifying algorithms.\n  2. These systematic risks include the black-box nature of AI decision-making, susceptibility to reward hacking, goal misgeneralization and potential for emergent behaviors that escape human control.\n  3. (L)AWS could act in ways that are not just unexpected but also uncontrollable, undermining mission objectives and potentially escalating conflicts.\n  4. Even rigorously tested systems may behave unpredictably and harmfully in real-world conditions, jeopardizing both strategic stability and humanitarian principles.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling and Simulating Emerging Memory Technologies: A Tutorial","url":"https://arxiv.org/abs/2502.10167","date":1739768400,"author":"","guid":1343,"unread":true,"content":"<article>arXiv:2502.10167v1 Announce Type: new \nAbstract: Non-volatile Memory (NVM) technologies present a promising alternative to traditional volatile memories such as SRAM and DRAM. Due to the limited availability of real NVM devices, simulators play a crucial role in architectural exploration and hardware-software co-design. This tutorial presents a simulation toolchain through four detailed case studies, showcasing its applicability to various domains of system design, including hybrid main-memory and cache, compute-in-memory, and wear-leveling design. These case studies provide the reader with practical insights on customizing the toolchain for their specific research needs. The source code is open-sourced.</article>","contentLength":713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"It's Like Not Being Able to Read and Write\": Narrowing the Digital Divide for Older Adults and Leveraging the Role of Digital Educators in Ireland","url":"https://arxiv.org/abs/2502.10166","date":1739768400,"author":"","guid":1344,"unread":true,"content":"<article>arXiv:2502.10166v1 Announce Type: new \nAbstract: As digital services increasingly replace traditional analogue systems, ensuring that older adults are not left behind is critical to fostering inclusive access. This study explores how digital educators support older adults in developing essential digital skills, drawing insights from interviews with $34$ educators in Ireland. These educators, both professional and volunteer, offer instruction through a range of formats, including workshops, remote calls, and in-person sessions. Our findings highlight the importance of personalized, step-by-step guidance tailored to older adults' learning needs, as well as fostering confidence through hands-on engagement with technology. Key challenges identified include limited transportation options, poor internet connectivity, outdated devices, and a lack of familial support for learning. To address these barriers, we propose enhanced public funding, expanded access to resources, and sustainable strategies such as providing relevant and practical course materials. Additionally, innovative tools like simulated online platforms for practicing digital transactions can help reduce anxiety and enhance digital literacy among older adults. This study underscores the vital role that digital educators play in bridging the digital divide, creating a more inclusive, human-centered approach to digital learning for older adults.</article>","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Generalization Power of a DNN in Terms of Symbolic Interactions","url":"https://arxiv.org/abs/2502.10162","date":1739768400,"author":"","guid":1345,"unread":true,"content":"<article>arXiv:2502.10162v1 Announce Type: new \nAbstract: This paper aims to analyze the generalization power of deep neural networks (DNNs) from the perspective of interactions. Unlike previous analysis of a DNN's generalization power in a highdimensional feature space, we find that the generalization power of a DNN can be explained as the generalization power of the interactions. We found that the generalizable interactions follow a decay-shaped distribution, while non-generalizable interactions follow a spindle-shaped distribution. Furthermore, our theory can effectively disentangle these two types of interactions from a DNN. We have verified that our theory can well match real interactions in a DNN in experiments.</article>","contentLength":718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SessionRec: Next Session Prediction Paradigm For Generative Sequential Recommendation","url":"https://arxiv.org/abs/2502.10157","date":1739768400,"author":"","guid":1346,"unread":true,"content":"<article>arXiv:2502.10157v1 Announce Type: new \nAbstract: We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for generative sequential recommendation, addressing the fundamental misalignment between conventional next-item prediction paradigm (NIPP) and real-world recommendation scenarios. Unlike NIPP's item-level autoregressive generation that contradicts actual session-based user interactions, our framework introduces a session-aware representation learning through hierarchical sequence aggregation (intra/inter-session), reducing attention computation complexity while enabling implicit modeling of massive negative interactions, and a session-based prediction objective that better captures users' diverse interests through multi-item recommendation in next sessions. Moreover, we found that incorporating a rank loss for items within the session under the next session prediction paradigm can significantly improve the ranking effectiveness of generative sequence recommendation models. We also verified that SessionRec exhibits clear power-law scaling laws similar to those observed in LLMs. Extensive experiments conducted on public datasets and online A/B test in Meituan App demonstrate the effectiveness of SessionRec. The proposed paradigm establishes new foundations for developing industrial-scale generative recommendation systems through its model-agnostic architecture and computational efficiency.</article>","contentLength":1431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MonoForce: Learnable Image-conditioned Physics Engine","url":"https://arxiv.org/abs/2502.10156","date":1739768400,"author":"","guid":1347,"unread":true,"content":"<article>arXiv:2502.10156v1 Announce Type: new \nAbstract: We propose a novel model for the prediction of robot trajectories on rough offroad terrain from the onboard camera images. This model enforces the laws of classical mechanics through a physics-aware neural symbolic layer while preserving the ability to learn from large-scale data as it is end-to-end differentiable. The proposed hybrid model integrates a black-box component that predicts robot-terrain interaction forces with a neural-symbolic layer. This layer includes a differentiable physics engine that computes the robot's trajectory by querying these forces at the points of contact with the terrain. As the proposed architecture comprises substantial geometrical and physics priors, the resulting model can also be seen as a learnable physics engine conditioned on real images that delivers $10^4$ trajectories per second. We argue and empirically demonstrate that this architecture reduces the sim-to-real gap and mitigates out-of-distribution sensitivity. The differentiability, in conjunction with the rapid simulation speed, makes the model well-suited for various applications including model predictive control, trajectory shooting, supervised and reinforcement learning or SLAM. The codes and data are publicly available.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complete Symmetry Breaking for Finite Models","url":"https://arxiv.org/abs/2502.10155","date":1739768400,"author":"","guid":1348,"unread":true,"content":"<article>arXiv:2502.10155v1 Announce Type: new \nAbstract: This paper introduces a SAT-based technique that calculates a compact and complete symmetry-break for finite model finding, with the focus on structures with a single binary operation (magmas). Classes of algebraic structures are typically described as first-order logic formulas and the concrete algebras are models of these formulas. Such models include an enormous number of isomorphic, i.e. symmetric, algebras.\n  A complete symmetry-break is a formula that has as models, exactly one canonical representative from each equivalence class of algebras. Thus, we enable answering questions about properties of the models so that computation and search are restricted to the set of canonical representations.\n  For instance, we can answer the question: How many non-isomorphic semigroups are there of size $n$? Such questions can be answered by counting the satisfying assignments of a SAT formula, which already filters out non-isomorphic models. The introduced technique enables us calculating numbers of algebraic structures not present in the literature and going beyond the possibilities of pure enumeration approaches.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video Soundtrack Generation by Aligning Emotions and Temporal Boundaries","url":"https://arxiv.org/abs/2502.10154","date":1739768400,"author":"","guid":1349,"unread":true,"content":"<article>arXiv:2502.10154v1 Announce Type: new \nAbstract: We introduce EMSYNC, a video-based symbolic music generation model that aligns music with a video's emotional content and temporal boundaries. It follows a two-stage framework, where a pretrained video emotion classifier extracts emotional features, and a conditional music generator produces MIDI sequences guided by both emotional and temporal cues. We introduce boundary offsets, a novel temporal conditioning mechanism that enables the model to anticipate and align musical chords with scene cuts. Unlike existing models, our approach retains event-based encoding, ensuring fine-grained timing control and expressive musical nuances. We also propose a mapping scheme to bridge the video emotion classifier, which produces discrete emotion categories, with the emotion-conditioned MIDI generator, which operates on continuous-valued valence-arousal inputs. In subjective listening tests, EMSYNC outperforms state-of-the-art models across all subjective metrics, for music theory-aware participants as well as the general listeners.</article>","contentLength":1083,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay","url":"https://arxiv.org/abs/2502.10151","date":1739768400,"author":"","guid":1350,"unread":true,"content":"<article>arXiv:2502.10151v1 Announce Type: new \nAbstract: Centralized search engines are key for the Internet, but lead to undesirable concentration of power. Decentralized alternatives fail to offer equal document retrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come close to the performance of centralized solutions when the semantics of documents are properly captured. This work uses embeddings from Large Language Models to capture semantics and fulfill the promise of Semantic Overlay Networks. Our proposed algorithm, called Semantica, constructs a prefix tree (trie) utilizing document embeddings calculated by a language model. Users connect to each other based on the embeddings of their documents, ensuring that semantically similar users are directly linked. Thereby, this construction makes it more likely for user searches to be answered by the users that they are directly connected to, or by the users they are close to in the network connection graph. The implementation of our algorithm also accommodates the semantic diversity of individual users by spawning \"clone\" user identifiers in the tree. Our experiments use emulation with a real-world workload to show Semantica's ability to identify and connect to similar users quickly. Semantica finds up to ten times more semantically similar users than current state-of-the-art approaches. At the same time, Semantica can retrieve more than two times the number of relevant documents given the same network load. We also make our code publicly available to facilitate further research in the area.</article>","contentLength":1581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IRS-assisted Edge Computing for Vehicular Networks: A Generative Diffusion Model-based Stackelberg Game Approach","url":"https://arxiv.org/abs/2502.10149","date":1739768400,"author":"","guid":1351,"unread":true,"content":"<article>arXiv:2502.10149v1 Announce Type: new \nAbstract: Recent advancements in intelligent reflecting surfaces (IRS) and mobile edge computing (MEC) offer new opportunities to enhance the performance of vehicular networks. However, meeting the computation-intensive and latency-sensitive demands of vehicles remains challenging due to the energy constraints and dynamic environments. To address this issue, we study an IRS-assisted MEC architecture for vehicular networks. We formulate a multi-objective optimization problem aimed at minimizing the total task completion delay and total energy consumption by jointly optimizing task offloading, IRS phase shift vector, and computation resource allocation. Given the mixed-integer nonlinear programming (MINLP) and NP-hard nature of the problem, we propose a generative diffusion model (GDM)-based Stackelberg game (GDMSG) approach. Specifically, the problem is reformulated within a Stackelberg game framework, where generative GDM is integrated to capture complex dynamics to efficiently derive optimal solutions. Simulation results indicate that the proposed GDMSG achieves outstanding performance compared to the benchmark approaches.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cooperative Multi-Agent Planning with Adaptive Skill Synthesis","url":"https://arxiv.org/abs/2502.10148","date":1739768400,"author":"","guid":1352,"unread":true,"content":"<article>arXiv:2502.10148v1 Announce Type: new \nAbstract: Despite much progress in training distributed artificial intelligence (AI), building cooperative multi-agent systems with multi-agent reinforcement learning (MARL) faces challenges in sample efficiency, interpretability, and transferability. Unlike traditional learning-based methods that require extensive interaction with the environment, large language models (LLMs) demonstrate remarkable capabilities in zero-shot planning and complex reasoning. However, existing LLM-based approaches heavily rely on text-based observations and struggle with the non-Markovian nature of multi-agent interactions under partial observability. We present COMPASS, a novel multi-agent architecture that integrates vision-language models (VLMs) with a dynamic skill library and structured communication for decentralized closed-loop decision-making. The skill library, bootstrapped from demonstrations, evolves via planner-guided tasks to enable adaptive strategies. COMPASS propagates entity information through multi-hop communication under partial observability. Evaluations on the improved StarCraft Multi-Agent Challenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates than state-of-the-art MARL algorithms in symmetric scenarios.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpretable Concept-based Deep Learning Framework for Multimodal Human Behavior Modeling","url":"https://arxiv.org/abs/2502.10145","date":1739768400,"author":"","guid":1353,"unread":true,"content":"<article>arXiv:2502.10145v1 Announce Type: new \nAbstract: In the contemporary era of intelligent connectivity, Affective Computing (AC), which enables systems to recognize, interpret, and respond to human behavior states, has become an integrated part of many AI systems. As one of the most critical components of responsible AI and trustworthiness in all human-centered systems, explainability has been a major concern in AC. Particularly, the recently released EU General Data Protection Regulation requires any high-risk AI systems to be sufficiently interpretable, including biometric-based systems and emotion recognition systems widely used in the affective computing field. Existing explainable methods often compromise between interpretability and performance. Most of them focus only on highlighting key network parameters without offering meaningful, domain-specific explanations to the stakeholders. Additionally, they also face challenges in effectively co-learning and explaining insights from multimodal data sources. To address these limitations, we propose a novel and generalizable framework, namely the Attention-Guided Concept Model (AGCM), which provides learnable conceptual explanations by identifying what concepts that lead to the predictions and where they are observed. AGCM is extendable to any spatial and temporal signals through multimodal concept alignment and co-learning, empowering stakeholders with deeper insights into the model's decision-making process. We validate the efficiency of AGCM on well-established Facial Expression Recognition benchmark datasets while also demonstrating its generalizability on more complex real-world human behavior understanding applications.</article>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages","url":"https://arxiv.org/abs/2502.10140","date":1739768400,"author":"","guid":1354,"unread":true,"content":"<article>arXiv:2502.10140v1 Announce Type: new \nAbstract: Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Provably Efficient RL under Episode-Wise Safety in Linear CMDPs","url":"https://arxiv.org/abs/2502.10138","date":1739768400,"author":"","guid":1355,"unread":true,"content":"<article>arXiv:2502.10138v1 Announce Type: new \nAbstract: We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ regret with an episode-wise zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs.</article>","contentLength":930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation","url":"https://arxiv.org/abs/2502.10127","date":1739768400,"author":"","guid":1356,"unread":true,"content":"<article>arXiv:2502.10127v1 Announce Type: new \nAbstract: High-Definition (HD) maps play a crucial role in autonomous vehicle navigation, complementing onboard perception sensors for improved accuracy and safety. Traditional HD map generation relies on dedicated mapping vehicles, which are costly and fail to capture real-time infrastructure changes. This paper presents HDMapLaneNet, a novel framework leveraging V2X communication and Scene Graph Generation to collaboratively construct a localized geometric layer of HD maps. The approach extracts lane centerlines from front-facing camera images, represents them as graphs, and transmits the data for global aggregation to the cloud via V2X. Preliminary results on the nuScenes dataset demonstrate superior association prediction performance compared to a state-of-the-art method.</article>","contentLength":825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Relational Tabular Data without Shared Features","url":"https://arxiv.org/abs/2502.10125","date":1739768400,"author":"","guid":1357,"unread":true,"content":"<article>arXiv:2502.10125v1 Announce Type: new \nAbstract: Learning relational tabular data has gained significant attention recently, but most studies focus on single tables, overlooking the potential of cross-table learning. Cross-table learning, especially in scenarios where tables lack shared features and pre-aligned data, offers vast opportunities but also introduces substantial challenges. The alignment space is immense, and determining accurate alignments between tables is highly complex. We propose Latent Entity Alignment Learning (Leal), a novel framework enabling effective cross-table training without requiring shared features or pre-aligned data. Leal operates on the principle that properly aligned data yield lower loss than misaligned data, a concept embodied in its soft alignment mechanism. This mechanism is coupled with a differentiable cluster sampler module, ensuring efficient scaling to large relational tables. Furthermore, we provide a theoretical proof of the cluster sampler's approximation capacity. Extensive experiments on five real-world and five synthetic datasets show that Leal achieves up to a 26.8% improvement in predictive performance compared to state-of-the-art methods, demonstrating its effectiveness and scalability.</article>","contentLength":1256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling the Impact of Visual Stimuli on Redirection Noticeability with Gaze Behavior in Virtual Reality","url":"https://arxiv.org/abs/2502.10124","date":1739768400,"author":"","guid":1358,"unread":true,"content":"<article>arXiv:2502.10124v1 Announce Type: new \nAbstract: While users could embody virtual avatars that mirror their physical movements in Virtual Reality, these avatars' motions can be redirected to enable novel interactions. Excessive redirection, however, could break the user's sense of embodiment due to perceptual conflicts between vision and proprioception. While prior work focused on avatar-related factors influencing the noticeability of redirection, we investigate how the visual stimuli in the surrounding virtual environment affect user behavior and, in turn, the noticeability of redirection. Given the wide variety of different types of visual stimuli and their tendency to elicit varying individual reactions, we propose to use users' gaze behavior as an indicator of their response to the stimuli and model the noticeability of redirection. We conducted two user studies to collect users' gaze behavior and noticeability, investigating the relationship between them and identifying the most effective gaze behavior features for predicting noticeability. Based on the data, we developed a regression model that takes users' gaze behavior as input and outputs the noticeability of redirection. We then conducted an evaluation study to test our model on unseen visual stimuli, achieving an accuracy of 0.012 MSE. We further implemented an adaptive redirection technique and conducted a proof-of-concept study to evaluate its effectiveness with complex visual stimuli in two applications. The results indicated that participants experienced less physical demanding and a stronger sense of body ownership when using our adaptive technique, demonstrating the potential of our model to support real-world use cases.</article>","contentLength":1717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modern Hopfield Networks with Continuous-Time Memories","url":"https://arxiv.org/abs/2502.10122","date":1739768400,"author":"","guid":1359,"unread":true,"content":"<article>arXiv:2502.10122v1 Announce Type: new \nAbstract: Recent research has established a connection between modern Hopfield networks (HNs) and transformer attention heads, with guarantees of exponential storage capacity. However, these models still face challenges scaling storage efficiently. Inspired by psychological theories of continuous neural resource allocation in working memory, we propose an approach that compresses large discrete Hopfield memories into smaller, continuous-time memories. Leveraging continuous attention, our new energy function modifies the update rule of HNs, replacing the traditional softmax-based probability mass function with a probability density, over the continuous memory. This formulation aligns with modern perspectives on human executive function, offering a principled link between attractor dynamics in working memory and resource-efficient memory allocation. Our framework maintains competitive performance with HNs while leveraging a compressed memory, reducing computational costs across synthetic and video datasets.</article>","contentLength":1059,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compress image to patches for Vision Transformer","url":"https://arxiv.org/abs/2502.10120","date":1739768400,"author":"","guid":1360,"unread":true,"content":"<article>arXiv:2502.10120v1 Announce Type: new \nAbstract: The Vision Transformer (ViT) has made significant strides in the field of computer vision. However, as the depth of the model and the resolution of the input images increase, the computational cost associated with training and running ViT models has surged dramatically.This paper proposes a hybrid model based on CNN and Vision Transformer, named CI2P-ViT. The model incorporates a module called CI2P, which utilizes the CompressAI encoder to compress images and subsequently generates a sequence of patches through a series of convolutions. CI2P can replace the Patch Embedding component in the ViT model, enabling seamless integration into existing ViT models.Compared to ViT-B/16, CI2P-ViT has the number of patches input to the self-attention layer reduced to a quarter of the original.This design not only significantly reduces the computational cost of the ViT model but also effectively enhances the model's accuracy by introducing the inductive bias properties of CNN.The ViT model's precision is markedly enhanced.When trained from the ground up on the Animals-10 dataset, CI2P-ViT achieved an accuracy rate of 92.37%, representing a 3.3% improvement over the ViT-B/16 baseline. Additionally, the model's computational operations, measured in floating-point operations per second (FLOPs), were diminished by 63.35%, and it exhibited a 2-fold increase in training velocity on identical hardware configurations.</article>","contentLength":1468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SeWA: Selective Weight Average via Probabilistic Masking","url":"https://arxiv.org/abs/2502.10119","date":1739768400,"author":"","guid":1361,"unread":true,"content":"<article>arXiv:2502.10119v1 Announce Type: new \nAbstract: Weight averaging has become a standard technique for enhancing model performance. However, methods such as Stochastic Weight Averaging (SWA) and Latest Weight Averaging (LAWA) often require manually designed procedures to sample from the training trajectory, and the results depend heavily on hyperparameter tuning. To minimize human effort, this paper proposes a simple yet efficient algorithm called Selective Weight Averaging (SeWA), which adaptively selects checkpoints during the final stages of training for averaging. Based on SeWA, we show that only a few points are needed to achieve better generalization and faster convergence. Theoretically, solving the discrete subset selection problem is inherently challenging. To address this, we transform it into a continuous probabilistic optimization framework and employ the Gumbel-Softmax estimator to learn the non-differentiable mask for each checkpoint. Further, we theoretically derive the SeWA's stability-based generalization bounds, which are sharper than that of SGD under both convex and non-convex assumptions. Finally, solid extended experiments in various domains, including behavior cloning, image classification, and text classification, further validate the effectiveness of our approach.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Image Embedding Sampling Method for Diverse Captioning","url":"https://arxiv.org/abs/2502.10118","date":1739768400,"author":"","guid":1362,"unread":true,"content":"<article>arXiv:2502.10118v1 Announce Type: new \nAbstract: Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerometry-based Energy Expenditure Estimation During Activities of Daily Living: A Comparison Among Different Accelerometer Compositions","url":"https://arxiv.org/abs/2502.10112","date":1739768400,"author":"","guid":1363,"unread":true,"content":"<article>arXiv:2502.10112v1 Announce Type: new \nAbstract: Physical activity energy expenditure (PAEE) can be measured from breath-by-breath respiratory data, which can serve as a reference. Alternatively, PAEE can be predicted from the body movements, which can be measured and estimated with accelerometers. The body center of mass (COM) acceleration reflects the movements of the whole body and thus serves as a good predictor for PAEE. However, the wrist has also become a popular location due to recent advancements in wrist-worn devices. Therefore, in this work, using the respiratory data measured by COSMED K5 as the reference, we evaluated and compared the performances of COM-based settings and wrist-based settings. The COM-based settings include two different accelerometer compositions, using only the pelvis accelerometer (pelvis-acc) and the pelvis accelerometer with two accelerometers from two thighs (3-acc). The wrist-based settings include using only the left wrist accelerometer (l-wrist-acc) and only the right wrist accelerometer (r-wrist-acc). We implemented two existing PAEE estimation methods on our collected dataset, where 9 participants performed activities of daily living while wearing 5 accelerometers (i.e., pelvis, two thighs, and two wrists). These two methods include a linear regression (LR) model and a CNN-LSTM model. Both models yielded the best results with the COM-based 3-acc setting (LR: $R^2$ = 0.41, CNN-LSTM: $R^2$ = 0.53). No significant difference was found between the 3-acc and pelvis-acc settings (p-value = 0.278). For both models, neither the l-wrist-acc nor the r-wrist-acc settings demonstrated predictive power on PAEE with $R^2$ values close to 0, significantly outperformed by the two COM-based settings (p-values $&lt;$ 0.05). No significant difference was found between the two wrists (p-value = 0.329).</article>","contentLength":1852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"COMBINEX: A Unified Counterfactual Explainer for Graph Neural Networks via Node Feature and Structural Perturbations","url":"https://arxiv.org/abs/2502.10111","date":1739768400,"author":"","guid":1364,"unread":true,"content":"<article>arXiv:2502.10111v1 Announce Type: new \nAbstract: Counterfactual explanations have emerged as a powerful tool to unveil the opaque decision-making processes of graph neural networks (GNNs). However, existing techniques primarily focus on edge modifications, often overlooking the crucial role of node feature perturbations in shaping model predictions. To address this limitation, we propose COMBINEX, a novel GNN explainer that generates counterfactual explanations for both node and graph classification tasks. Unlike prior methods, which treat structural and feature-based changes independently, COMBINEX optimally balances modifications to edges and node features by jointly optimizing these perturbations. This unified approach ensures minimal yet effective changes required to flip a model's prediction, resulting in realistic and interpretable counterfactuals. Additionally, COMBINEX seamlessly handles both continuous and discrete node features, enhancing its versatility across diverse datasets and GNN architectures. Extensive experiments on real-world datasets and various GNN architectures demonstrate the effectiveness and robustness of our approach over existing baselines.</article>","contentLength":1186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ScamFerret: Detecting Scam Websites Autonomously with Large Language Models","url":"https://arxiv.org/abs/2502.10110","date":1739768400,"author":"","guid":1365,"unread":true,"content":"<article>arXiv:2502.10110v1 Announce Type: new \nAbstract: With the rise of sophisticated scam websites that exploit human psychological vulnerabilities, distinguishing between legitimate and scam websites has become increasingly challenging. This paper presents ScamFerret, an innovative agent system employing a large language model (LLM) to autonomously collect and analyze data from a given URL to determine whether it is a scam. Unlike traditional machine learning models that require large datasets and feature engineering, ScamFerret leverages LLMs' natural language understanding to accurately identify scam websites of various types and languages without requiring additional training or fine-tuning. Our evaluation demonstrated that ScamFerret achieves 0.972 accuracy in classifying four scam types in English and 0.993 accuracy in classifying online shopping websites across three different languages, particularly when using GPT-4. Furthermore, we confirmed that ScamFerret collects and analyzes external information such as web content, DNS records, and user reviews as necessary, providing a basis for identifying scam websites from multiple perspectives. These results suggest that LLMs have significant potential in enhancing cybersecurity measures against sophisticated scam websites.</article>","contentLength":1291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NeuroXVocal: Detection and Explanation of Alzheimer's Disease through Non-invasive Analysis of Picture-prompted Speech","url":"https://arxiv.org/abs/2502.10108","date":1739768400,"author":"","guid":1366,"unread":true,"content":"<article>arXiv:2502.10108v1 Announce Type: new \nAbstract: The early diagnosis of Alzheimer's Disease (AD) through non invasive methods remains a significant healthcare challenge. We present NeuroXVocal, a novel dual-component system that not only classifies but also explains potential AD cases through speech analysis. The classification component (Neuro) processes three distinct data streams: acoustic features capturing speech patterns and voice characteristics, textual features extracted from speech transcriptions, and precomputed embeddings representing linguistic patterns. These streams are fused through a custom transformer-based architecture that enables robust cross-modal interactions. The explainability component (XVocal) implements a Retrieval-Augmented Generation (RAG) approach, leveraging Large Language Models combined with a domain-specific knowledge base of AD research literature. This architecture enables XVocal to retrieve relevant clinical studies and research findings to generate evidence-based context-sensitive explanations of the acoustic and linguistic markers identified in patient speech. Using the IS2021 ADReSSo Challenge benchmark dataset, our system achieved state-of-the-art performance with 95.77% accuracy in AD classification, significantly outperforming previous approaches. The explainability component was qualitatively evaluated using a structured questionnaire completed by medical professionals, validating its clinical relevance. NeuroXVocal's unique combination of high-accuracy classification and interpretable, literature-grounded explanations demonstrates its potential as a practical tool for supporting clinical AD diagnosis.</article>","contentLength":1674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Adaptive Low-Rank Sparse Subspace Clustering","url":"https://arxiv.org/abs/2502.10106","date":1739768400,"author":"","guid":1367,"unread":true,"content":"<article>arXiv:2502.10106v1 Announce Type: new \nAbstract: Low-rank sparse subspace clustering (LRSSC) algorithms built on self-expressive model effectively capture both the global and local structure of the data. However, existing solutions, primarily based on proximal operators associated with Sp/Lp , p e {0, 1/2, 2/3, 1}, norms are not data-adaptive. In this work, we propose an LRSSC algorithm incorporating a data-adaptive surrogate for the S0/L0 quasi-norm. We provide a numerical solution for the corresponding proximal operator in cases where an analytical expression is unavailable. The proposed LRSSC algorithm is formulated within the proximal mapping framework, and we present theoretical proof of its global convergence toward a stationary point. We evaluate the performance of the proposed method on three well known datasets, comparing it against LRSSC algorithms constrained by Sp/Lp, p e {0, 1/2, 2/3, 1}, norms.</article>","contentLength":921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Membership and Conjugacy in Inverse Semigroups","url":"https://arxiv.org/abs/2502.10103","date":1739768400,"author":"","guid":1368,"unread":true,"content":"<article>arXiv:2502.10103v1 Announce Type: new \nAbstract: The membership problem for an algebraic structure asks whether a given element is contained in some substructure, which is usually given by generators. In this work we study the membership problem, as well as the conjugacy problem, for finite inverse semigroups. The closely related membership problem for finite semigroups has been shown to be PSPACE-complete in the transformation model by Kozen (1977) and NL-complete in the Cayley table model by Jones, Lien, and Laaser (1976). More recently, both the membership and the conjugacy problem for finite inverse semigroups were shown to be PSPACE-complete in the partial bijection model by Jack (2023).\n  Here we present a more detailed analysis of the complexity of the membership and conjugacy problems parametrized by varieties of finite inverse semigroups. We establish dichotomy theorems for the partial bijection model and for the Cayley table model. In the partial bijection model these problems are in NC (resp. NP for conjugacy) for strict inverse semigroups and PSPACE-complete otherwise. In the Cayley table model we obtain general LOGSPACE-algorithms as well as NPOLYLOGTIME upper bounds for Clifford semigroups and LOGSPACE-completeness otherwise.\n  Furthermore, by applying our findings, we show the following: the intersection non-emptiness problem for inverse automata is PSPACE-complete even for automata with only two states; the subpower membership problem is in NC for every strict inverse semi-group and PSPACE-complete otherwise; the minimum generating set and the equation satisfiability problems are in NP for varieties of finite strict inverse semigroups and PSPACE-complete otherwise.</article>","contentLength":1709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Statistical data analysis for Tourism in Poland in R Programming Environment","url":"https://arxiv.org/abs/2502.10100","date":1739768400,"author":"","guid":1369,"unread":true,"content":"<article>arXiv:2502.10100v1 Announce Type: new \nAbstract: This study utilises the R programming language for statistical data analysis to understand Tourism dynamics in Poland. It focuses on methods for data visualisation, multivariate statistics, and hypothesis testing. To investigate the expenditure behavior of tourist, spending patterns, correlations, and associations among variables were analysed in the dataset. The results revealed a significant relationship between accommodation type and the purpose of trip, showing that the purpose of a trip impacts the selection of accommodation. A strong correlation was observed between organizer expenditure and private expenditure, indicating that individual spending are more when the spending on organizing the trip are higher. However, no significant difference was observed in total expenditure across different accommodation types and purpose of the trip revealing that travelers tend to spend similar amounts regardless of their reason for travel or choice of accommodation. Although significant relationships were observed among certain variables, ANOVA could not be applied because the dataset was not able to hold on the normality assumption. In future, the dataset can be explored further to find more meaningful insights. The developed code is available on GitHub: https://github.com/SaadAhmedJamal/DataAnalysis RProgEnv.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Causal Information Prioritization for Efficient Reinforcement Learning","url":"https://arxiv.org/abs/2502.10097","date":1739768400,"author":"","guid":1370,"unread":true,"content":"<article>arXiv:2502.10097v1 Announce Type: new \nAbstract: Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. To fully assess the effectiveness of CIP, we conduct extensive experiments across 39 tasks in 5 diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.</article>","contentLength":1565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Representation Learning on Out of Distribution in Tabular Data","url":"https://arxiv.org/abs/2502.10095","date":1739768400,"author":"","guid":1371,"unread":true,"content":"<article>arXiv:2502.10095v1 Announce Type: new \nAbstract: The open-world assumption in model development suggests that a model might lack sufficient information to adequately handle data that is entirely distinct or out of distribution (OOD). While deep learning methods have shown promising results in handling OOD data through generalization techniques, they often require specialized hardware that may not be accessible to all users. We present TCL, a lightweight yet effective solution that operates efficiently on standard CPU hardware. Our approach adapts contrastive learning principles specifically for tabular data structures, incorporating full matrix augmentation and simplified loss calculation. Through comprehensive experiments across 10 diverse datasets, we demonstrate that TCL outperforms existing models, including FT-Transformer and ResNet, particularly in classification tasks, while maintaining competitive performance in regression problems. TCL achieves these results with significantly reduced computational requirements, making it accessible to users with limited hardware capabilities. This study also provides practical guidance for detecting and evaluating OOD data through straightforward experiments and visualizations. Our findings show that TCL offers a promising balance between performance and efficiency in handling OOD prediction tasks, which is particularly beneficial for general machine learning practitioners working with computational constraints.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A novel approach to data generation in generative model","url":"https://arxiv.org/abs/2502.10092","date":1739768400,"author":"","guid":1372,"unread":true,"content":"<article>arXiv:2502.10092v1 Announce Type: new \nAbstract: Variational Autoencoders (VAEs) and other generative models are widely employed in artificial intelligence to synthesize new data. However, current approaches rely on Euclidean geometric assumptions and statistical approximations that fail to capture the structured and emergent nature of data generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory, a novel geometric framework that redefines data generation by integrating dimensional expansion accompanied by qualitative transformation. By modifying the latent space geometry to interact with emergent high-dimensional structures, CFP theory addresses key challenges such as identifiability issues and unintended artifacts like hallucinations in Large Language Models (LLMs). CFP theory is based on two key conceptual hypotheses that redefine how generative models structure relationships between data and algorithms. Through the lens of CFP theory, we critically examine existing metric-learning approaches. CFP theory advances this perspective by introducing time-reversed metric embeddings and structural convergence mechanisms, leading to a novel geometric approach that better accounts for data generation as a structured epistemic process. Beyond its computational implications, CFP theory provides philosophical insights into the ontological underpinnings of data generation. By offering a systematic framework for high-dimensional learning dynamics, CFP theory contributes to establishing a theoretical foundation for understanding the data-relationship structures in AI. Finally, future research in CFP theory will be led to its implications for fully realizing qualitative transformations, introducing the potential of Hilbert space in generative modeling.</article>","contentLength":1790,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ELAA-ISAC: Environmental Mapping Utilizing the LoS State of Communication Channel","url":"https://arxiv.org/abs/2502.10091","date":1739768400,"author":"","guid":1373,"unread":true,"content":"<article>arXiv:2502.10091v1 Announce Type: new \nAbstract: In this paper, a novel environmental mapping method is proposed to outline the indoor layout utilizing the line-of-sight (LoS) state information of extremely large aperture array (ELAA) channels. It leverages the spatial resolution provided by ELAA and the mobile terminal (MT)'s mobility to infer the presence and location of obstacles in the environment. The LoS state estimation is formulated as a binary hypothesis testing problem, and the optimal decision rule is derived based on the likelihood ratio test. Subsequently, the theoretical error probability of LoS estimation is derived, showing close alignment with simulation results. Then, an environmental mapping method is proposed, which progressively outlines the layout by combining LoS state information from multiple MT locations. It is demonstrated that the proposed method can accurately outline the environment layout, with the mapping accuracy improving as the number of service-antennas and MT locations increases. This paper also investigates the impact of channel estimation error and non-LoS (NLoS) components on the quality of environmental mapping. The proposed method exhibits particularly promising performance in LoS dominated wireless environments characterized by high Rician K-factor. Specifically, it achieves an average intersection over union (IoU) exceeding 80% when utilizing 256 service antennas and 18 MT locations.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models","url":"https://arxiv.org/abs/2502.10090","date":1739768400,"author":"","guid":1374,"unread":true,"content":"<article>arXiv:2502.10090v1 Announce Type: new \nAbstract: Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they cannot interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages a Vision-Language Model (VLM) to extract structured information from instructional images and then uses this information to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step. At the same time, a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling several real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS ACAM for Energy-Efficient Inference","url":"https://arxiv.org/abs/2502.10089","date":1739768400,"author":"","guid":1375,"unread":true,"content":"<article>arXiv:2502.10089v1 Announce Type: new \nAbstract: In recent years, the development of smart edge computing systems to process information locally is on the rise. Many near-sensor machine learning (ML) approaches have been implemented to introduce accurate and energy efficient template matching operations in resource-constrained edge sensing systems, such as wearables. To introduce novel solutions that can be viable for extreme edge cases, hybrid solutions combining conventional and emerging technologies have started to be proposed. Deep Neural Networks (DNN) optimised for edge application alongside new approaches of computing (both device and architecture -wise) could be a strong candidate in implementing edge ML solutions that aim at competitive accuracy classification while using a fraction of the power of conventional ML solutions. In this work, we are proposing a hybrid software-hardware edge classifier aimed at the extreme edge near-sensor systems. The classifier consists of two parts: (i) an optimised digital tinyML network, working as a front-end feature extractor, and (ii) a back-end RRAM-CMOS analogue content addressable memory (ACAM), working as a final stage template matching system. The combined hybrid system exhibits a competitive trade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$ and $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with 78.06$\\mu$J for the original teacher model, representing a 792-fold reduction, making it a viable solution for extreme edge applications.</article>","contentLength":1556,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Patient Acceptance of Robotic Ultrasound through Conversational Virtual Agent and Immersive Visualizations","url":"https://arxiv.org/abs/2502.10088","date":1739768400,"author":"","guid":1376,"unread":true,"content":"<article>arXiv:2502.10088v1 Announce Type: new \nAbstract: Robotic ultrasound systems can enhance medical diagnostics, but patient acceptance is a challenge. We propose a system combining an AI-powered conversational virtual agent with three mixed reality visualizations to improve trust and comfort. The virtual agent, powered by a large language model, engages in natural conversations and guides the ultrasound robot, enhancing interaction reliability. The visualizations include augmented reality, augmented virtuality, and fully immersive virtual reality, each designed to create patient-friendly experiences. A user study demonstrated significant improvements in trust and acceptance, offering valuable insights for designing mixed reality and virtual agents in autonomous medical procedures.</article>","contentLength":788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coordinated control of multiple autonomous surface vehicles: challenges and advances - a systematic review","url":"https://arxiv.org/abs/2502.10080","date":1739768400,"author":"","guid":1377,"unread":true,"content":"<article>arXiv:2502.10080v1 Announce Type: new \nAbstract: The increasing use and implementation of Autonomous Surface Vessels (ASVs) for various activities in maritime environments is expected to drive a rise in developments and research on their control. Particularly, the coordination of multiple ASVs presents novel challenges and opportunities, requiring interdisciplinary research efforts at the intersection of robotics, control theory, communication systems, and marine sciences. The wide variety of missions or objectives for which these vessels can be collectively used allows for the application and combination of different control techniques. This includes the exploration of machine learning to consider aspects previously deemed infeasible. This review provides a comprehensive exploration of coordinated ASV control while addressing critical gaps left by previous reviews. Unlike previous works, we adopt a systematic approach to ensure integrity and minimize bias in article selection. We delve into the complex world of sub-actuated ASVs with a focus on customized control strategies and the integration of machine learning techniques for increased autonomy. By synthesizing recent advances and identifying emerging trends, we offer insights that drive this field forward, providing both a comprehensive overview of state-of-the-art techniques and guidance for future research efforts.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Empowerment Gain through Causal Structure Learning in Model-Based RL","url":"https://arxiv.org/abs/2502.10077","date":1739768400,"author":"","guid":1378,"unread":true,"content":"<article>arXiv:2502.10077v1 Announce Type: new \nAbstract: In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environments, enabling efficient decision. Empowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future states and actions. We posit that empowerment coupled with causal understanding can improve controllability, while enhanced empowerment gain can further facilitate causal reasoning in MBRL. To improve learning efficiency and controllability, we propose a novel framework, Empowerment through Causal Learning (ECL), where an agent with the awareness of causal dynamics models achieves empowerment-driven exploration and optimizes its causal structure for task learning. Specifically, ECL operates by first training a causal dynamics model of the environment based on collected data. We then maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update causal dynamics model to be more controllable than dense dynamics model without causal structure. In downstream task learning, an intrinsic curiosity reward is included to balance the causality, mitigating overfitting. Importantly, ECL is method-agnostic and is capable of integrating various causal discovery methods. We evaluate ECL combined with 3 causal discovery methods across 6 environments including pixel-based tasks, demonstrating its superior performance compared to other causal MBRL methods, in terms of causal discovery, sample efficiency, and asymptotic performance.</article>","contentLength":1708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Classification of Temporal Graphs using Persistent Homology","url":"https://arxiv.org/abs/2502.10076","date":1739768400,"author":"","guid":1379,"unread":true,"content":"<article>arXiv:2502.10076v1 Announce Type: new \nAbstract: Temporal graphs effectively model dynamic systems by representing interactions as timestamped edges. However, analytical tools for temporal graphs are limited compared to static graphs. We propose a novel method for analyzing temporal graphs using Persistent Homology. Our approach leverages $\\delta$-temporal motifs (recurrent subgraphs) to capture temporal dynamics %without aggregation\n  . By evolving these motifs, we define the \\textit{average filtration} and compute PH on the associated clique complex. This method captures both local and global temporal structures and is stable with respect to reference models. We demonstrate the applicability of our approach to the temporal graph classification task. Experiments verify the effectiveness of our approach, achieving over 92\\% accuracy, with some cases reaching 100\\%. Unlike existing methods that require node classes, our approach is node class free, offering flexibility for a wide range of temporal graph analysis.</article>","contentLength":1027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthemius: Efficient & Modular Block Assembly for Concurrent Execution","url":"https://arxiv.org/abs/2502.10074","date":1739768400,"author":"","guid":1380,"unread":true,"content":"<article>arXiv:2502.10074v1 Announce Type: new \nAbstract: Many blockchains such as Ethereum execute all incoming transactions sequentially significantly limiting the potential throughput. A common approach to scale execution is parallel execution engines that fully utilize modern multi-core architectures. Parallel execution is then either done optimistically, by executing transactions in parallel and detecting conflicts on the fly, or guided, by requiring exhaustive client transaction hints and scheduling transactions accordingly.\n  However, recent studies have shown that the performance of parallel execution engines depends on the nature of the underlying workload. In fact, in some cases, only a 60% speed-up compared to sequential execution could be obtained. This is the case, as transactions that access the same resources must be executed sequentially. For example, if 10% of the transactions in a block access the same resource, the execution cannot meaningfully scale beyond 10 cores. Therefore, a single popular application can bottleneck the execution and limit the potential throughput.\n  In this paper, we introduce Anthemius, a block construction algorithm that optimizes parallel transaction execution throughput. We evaluate Anthemius exhaustively under a range of workloads, and show that Anthemius enables the underlying parallel execution engine to process over twice as many transactions.</article>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LifeSaver: Predictive Load Limit Estimation for Transport Vehicles in Hilly Areas","url":"https://arxiv.org/abs/2502.10072","date":1739768400,"author":"","guid":1381,"unread":true,"content":"<article>arXiv:2502.10072v1 Announce Type: new \nAbstract: The transportation of essential goods in moun- tainous regions faces severe logistical challenges and frequent disruptions. To mitigate these difficulties, transport companies often overload trucks, which, though cost-saving, significantly heightens the risk of accidents and mechanical failures. This paper presents the development of a device that detects over- loaded and insecurely fastened loads on trucks and commercial vehicles. Using advanced load sensors, the device offers real- time monitoring of cargo weight distribution, alerting drivers and authorities to unsafe conditions. The initial prototype utilised two basic load cells and an Arduino microcontroller. The second version was enhanced with four load cells and extended sensors. This version was tested by placing an electric golf cart onto the prototype. Various loads were then added to the cart in different orientations to assess whether the system could accurately detect improper or excessive load conditions.</article>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Topological Neural Networks over the Air","url":"https://arxiv.org/abs/2502.10070","date":1739768400,"author":"","guid":1382,"unread":true,"content":"<article>arXiv:2502.10070v1 Announce Type: new \nAbstract: Topological neural networks (TNNs) are information processing architectures that model representations from data lying over topological spaces (e.g., simplicial or cell complexes) and allow for decentralized implementation through localized communications over different neighborhoods. Existing TNN architectures have not yet been considered in realistic communication scenarios, where channel effects typically introduce disturbances such as fading and noise. This paper aims to propose a novel TNN design, operating on regular cell complexes, that performs over-the-air computation, incorporating the wireless communication model into its architecture. Specifically, during training and inference, the proposed method considers channel impairments such as fading and noise in the topological convolutional filtering operation, which takes place over different signal orders and neighborhoods. Numerical results illustrate the architecture's robustness to channel impairments during testing and the superior performance with respect to existing architectures, which are either communication-agnostic or graph-based.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bound preserving {P}oint-{A}verage-{M}oment {P}olynomi{A}l-interpreted ({PAMPA}) on polygonal meshes","url":"https://arxiv.org/abs/2502.10069","date":1739768400,"author":"","guid":1383,"unread":true,"content":"<article>arXiv:2502.10069v1 Announce Type: new \nAbstract: We present a novel discretisation strategy, strongly inspired from Roe's Active Flux scheme. It can use polygonal meshes and is provably bound preserving for scalar problems and the Euler equations. Several cases demonstrates the quality of the method, and improvements with respect to previous work of the authors. This paper is a summary of \\cite{BPPampa}.</article>","contentLength":407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proportional Clustering, the $\\beta$-Plurality Problem, and Metric Distortion","url":"https://arxiv.org/abs/2502.10068","date":1739768400,"author":"","guid":1384,"unread":true,"content":"<article>arXiv:2502.10068v1 Announce Type: new \nAbstract: We show that the proportional clustering problem using the Droop quota for $k = 1$ is equivalent to the $\\beta$-plurality problem. We also show that the Plurality Veto rule can be used to select ($\\sqrt{5} - 2$)-plurality points using only ordinal information about the metric space and resolve an open question of Kalayci et al. (AAAI 2024) by proving that $(2+\\sqrt{5})$-proportionally fair clusterings can be found using purely ordinal information.</article>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmenting Plane Straight-Line Graphs to Meet Parity Constraints","url":"https://arxiv.org/abs/2502.10066","date":1739768400,"author":"","guid":1385,"unread":true,"content":"<article>arXiv:2502.10066v1 Announce Type: new \nAbstract: Given a plane geometric graph $G$ on $n$ vertices, we want to augment it so that given parity constraints of the vertex degrees are met. In other words, given a subset $R$ of the vertices, we are interested in a plane geometric supergraph $G'$ such that exactly the vertices of $R$ have odd degree in $G'\\setminus G$. We show that the question whether such a supergraph exists can be decided in polynomial time for two interesting cases. First, when the vertices are in convex position, we present a linear-time algorithm. Building on this insight, we solve the case when $G$ is a plane geometric path in $O(n \\log n)$ time. This solves an open problem posed by Catana, Olaverri, Tejel, and Urrutia (Appl. Math. Comput. 2020).</article>","contentLength":775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hands-off Image Editing: Language-guided Editing without any Task-specific Labeling, Masking or even Training","url":"https://arxiv.org/abs/2502.10064","date":1739768400,"author":"","guid":1386,"unread":true,"content":"<article>arXiv:2502.10064v1 Announce Type: new \nAbstract: Instruction-guided image editing consists in taking an image and an instruction and deliverring that image altered according to that instruction. State-of-the-art approaches to this task suffer from the typical scaling up and domain adaptation hindrances related to supervision as they eventually resort to some kind of task-specific labelling, masking or training. We propose a novel approach that does without any such task-specific supervision and offers thus a better potential for improvement. Its assessment demonstrates that it is highly effective, achieving very competitive performance.</article>","contentLength":644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strassen Multisystolic Array Hardware Architectures","url":"https://arxiv.org/abs/2502.10063","date":1739768400,"author":"","guid":1387,"unread":true,"content":"<article>arXiv:2502.10063v1 Announce Type: new \nAbstract: While Strassen's matrix multiplication algorithm reduces the complexity of naive matrix multiplication, general-purpose hardware is not suitable for achieving the algorithm's promised theoretical speedups. This leaves the question of if it could be better exploited in custom hardware architectures designed specifically for executing the algorithm. However, there is limited prior work on this and it is not immediately clear how to derive such architectures or if they can ultimately lead to real improvements. We bridge this gap, presenting and evaluating new systolic array architectures that efficiently translate the theoretical complexity reductions of Strassen's algorithm directly into hardware resource savings. Furthermore, the architectures are multisystolic array designs that can multiply smaller matrices with higher utilization than single-systolic array designs. The proposed designs implemented on FPGA reduce DSP requirements by a factor of $1.14^r$ for $r$ implemented Strassen recursion levels, and otherwise require overall similar soft logic resources when instantiated to support matrix sizes down to 32x32 and 24x24 at 1-2 levels of Strassen recursion, respectively. We evaluate the proposed designs both in isolation and in an end-to-end machine learning accelerator compared to baseline designs and prior works, achieving state-of-the-art performance.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Bi-Level Multi-Robot Task Allocation and Learning under Uncertainty with Temporal Logic Constraints","url":"https://arxiv.org/abs/2502.10062","date":1739768400,"author":"","guid":1388,"unread":true,"content":"<article>arXiv:2502.10062v1 Announce Type: new \nAbstract: This work addresses the problem of multi-robot coordination under unknown robot transition models, ensuring that tasks specified by Time Window Temporal Logic are satisfied with user-defined probability thresholds. We present a bi-level framework that integrates (i) high-level task allocation, where tasks are assigned based on the robots' estimated task completion probabilities and expected rewards, and (ii) low-level distributed policy learning and execution, where robots independently optimize auxiliary rewards while fulfilling their assigned tasks. To handle uncertainty in robot dynamics, our approach leverages real-time task execution data to iteratively refine expected task completion probabilities and rewards, enabling adaptive task allocation without explicit robot transition models. We theoretically validate the proposed algorithm, demonstrating that the task assignments meet the desired probability thresholds with high confidence. Finally, we demonstrate the effectiveness of our framework through comprehensive simulations.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Annotating Compositionality Scores for Irish Noun Compounds is Hard Work","url":"https://arxiv.org/abs/2502.10061","date":1739768400,"author":"","guid":1389,"unread":true,"content":"<article>arXiv:2502.10061v1 Announce Type: new \nAbstract: Noun compounds constitute a challenging construction for NLP applications, given their variability in idiomaticity and interpretation. In this paper, we present an analysis of compound nouns identified in Irish text of varied domains by expert annotators, focusing on compositionality as a key feature, but also domain specificity, as well as familiarity and confidence of the annotator giving the ratings. Our findings and the discussion that ensued contributes towards a greater understanding of how these constructions appear in Irish language, and how they might be treated separately from English noun compounds.</article>","contentLength":666,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery","url":"https://arxiv.org/abs/2502.10060","date":1739768400,"author":"","guid":1390,"unread":true,"content":"<article>arXiv:2502.10060v1 Announce Type: new \nAbstract: Visual data is used in numerous different scientific workflows ranging from remote sensing to ecology. As the amount of observation data increases, the challenge is not just to make accurate predictions but also to understand the underlying mechanisms for those predictions. Good interpretation is important in scientific workflows, as it allows for better decision-making by providing insights into the data. This paper introduces an automatic way of obtaining such interpretable-by-design models, by learning programs that interleave neural networks. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution) an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create Python programs explaining visual data. Additionally, we propose two improvements: a program critic and a program simplifier to improve our method further to synthesize good programs. On three different real-world problems, DiSciPLE learns state-of-the-art programs on novel tasks with no prior literature. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control","url":"https://arxiv.org/abs/2502.10059","date":1739768400,"author":"","guid":1391,"unread":true,"content":"<article>arXiv:2502.10059v1 Announce Type: new \nAbstract: Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale. To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. During training, the reconstructed 3D scene enables scaling camera parameters from relative to absolute values, ensuring compatibility and scale consistency across diverse real-world images. In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene. To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic, coherent video generation in lower noise stages. RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. We will release our absolute-scale annotation, codes, and all checkpoints. Please see dynamic results in https://zgctroy.github.io/RealCam-I2V.</article>","contentLength":1641,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MTLM: an Innovative Language Model Training Paradigm for ASR","url":"https://arxiv.org/abs/2502.10058","date":1739768400,"author":"","guid":1392,"unread":true,"content":"<article>arXiv:2502.10058v1 Announce Type: new \nAbstract: Pre-training Transformer-based language models (LMs) on a large amount of text has proven crucial for improving automatic speech recognition (ASR) performance. Generally, traditional LMs are unidirectional and unable to access the context on the right. This paper proposes a method for training LMs that enable traditional unidirectional LMs to fully utilize left and right contexts. Compared with the unidirectional LMs, our LM facilitates ASR to transcribe hypotheses more consistently and in a more semantically unambiguous way, as it incorporates richer contextual representations. Finally, our experimental results on the LibriSpeech corpus demonstrate that our model outperforms traditional unidirectional LMs, whether n-best rescoring or shallow fusion is used as the decoding algorithm.</article>","contentLength":843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Generalized Modeling Approach to Liquid-driven Ballooning Membranes","url":"https://arxiv.org/abs/2502.10057","date":1739768400,"author":"","guid":1393,"unread":true,"content":"<article>arXiv:2502.10057v1 Announce Type: new \nAbstract: Soft robotics is advancing the use of flexible materials for adaptable robotic systems. Membrane-actuated soft robots address the limitations of traditional soft robots by using pressurized, extensible membranes to achieve stable, large deformations, yet control and state estimation remain challenging due to their complex deformation dynamics. This paper presents a novel modeling approach for liquid-driven ballooning membranes, employing an ellipsoid approximation to model shape and stretch under planar deformation. Relying solely on intrinsic feedback from pressure data and controlled liquid volume, this approach enables accurate membrane state estimation. We demonstrate the effectiveness of the proposed model for ballooning membrane-based actuators by experimental validation, obtaining the indentation depth error of $RMSE_{h_2}=0.80\\;$mm, which is $23\\%$ of the indentation range and $6.67\\%$ of the unindented actuator height range. For the force estimation, the error range is obtained to be $RMSE_{F}=0.15\\;$N which is $10\\%$ of the measured force range.</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Breaking Symmetries from a Set-Covering Perspective","url":"https://arxiv.org/abs/2502.10056","date":1739768400,"author":"","guid":1394,"unread":true,"content":"<article>arXiv:2502.10056v1 Announce Type: new \nAbstract: We formalize symmetry breaking as a set-covering problem. For the case of breaking symmetries on graphs, a permutation covers a graph if applying it to the graph yields a smaller graph in a given order. Canonical graphs are those that cannot be made smaller by any permutation. A complete symmetry break is then a set of permutations that covers all non-canonical graphs. A complete symmetry break with a minimal number of permutations can be obtained by solving an optimal set-covering problem.\n  The challenge is in the sizes of the corresponding set-covering problems and in how these can be tamed.\n  The set-covering perspective on symmetry breaking opens up a range of new opportunities deriving from decades of studies on both precise and approximate techniques for this problem.\n  Application of our approach leads to optimal LexLeader symmetry breaks for graphs of order $n\\leq 10$ as well as to partial symmetry breaks which improve on the state-of-the-art.</article>","contentLength":1015,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Polyp Counting In Full-Procedure Colonoscopy Videos","url":"https://arxiv.org/abs/2502.10054","date":1739768400,"author":"","guid":1395,"unread":true,"content":"<article>arXiv:2502.10054v1 Announce Type: new \nAbstract: Automated colonoscopy reporting holds great potential for enhancing quality control and improving cost-effectiveness of colonoscopy procedures. A major challenge lies in the automated identification, tracking, and re-association (ReID) of polyps tracklets across full-procedure colonoscopy videos. This is essential for precise polyp counting and enables automated computation of key quality metrics, such as Adenoma Detection Rate (ADR) and Polyps Per Colonoscopy (PPC). However, polyp ReID is challenging due to variations in polyp appearance, frequent disappearance from the field of view, and occlusions. In this work, we leverage the REAL-Colon dataset, the first open-access dataset providing full-procedure videos, to define tasks, data splits and metrics for the problem of automatically count polyps in full-procedure videos, establishing an open-access framework. We re-implement previously proposed SimCLR-based methods for learning representations of polyp tracklets, both single-frame and multi-view, and adapt them to the polyp counting task. We then propose an Affinity Propagation-based clustering method to further improve ReID based on these learned representations, ultimately enhancing polyp counting. Our approach achieves state-of-the-art performance, with a polyp fragmentation rate of 6.30 and a false positive rate (FPR) below 5% on the REAL-Colon dataset. We release code at https://github.com/lparolari/towards-polyp-counting.</article>","contentLength":1502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ORI: O Routing Intelligence","url":"https://arxiv.org/abs/2502.10051","date":1739768400,"author":"","guid":1396,"unread":true,"content":"<article>arXiv:2502.10051v1 Announce Type: new \nAbstract: Single large language models (LLMs) often fall short when faced with the ever-growing range of tasks, making a single-model approach insufficient. We address this challenge by proposing ORI (O Routing Intelligence), a dynamic framework that leverages a set of LLMs. By intelligently routing incoming queries to the most suitable model, ORI not only improves task-specific accuracy, but also maintains efficiency. Comprehensive evaluations across diverse benchmarks demonstrate consistent accuracy gains while controlling computational overhead. By intelligently routing queries, ORI outperforms the strongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR, ties the top performance on ARC, and on BBH. These results underscore the benefits of a multi-model strategy and demonstrate how ORI's adaptive architecture can more effectively handle diverse tasks, offering a scalable, high-performance solution for a system of multiple large language models.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on LLM-powered Agents for Recommender Systems","url":"https://arxiv.org/abs/2502.10050","date":1739768400,"author":"","guid":1397,"unread":true,"content":"<article>arXiv:2502.10050v1 Announce Type: new \nAbstract: Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.</article>","contentLength":1599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Janus: Collaborative Vision Transformer Under Dynamic Network Environment","url":"https://arxiv.org/abs/2502.10047","date":1739768400,"author":"","guid":1398,"unread":true,"content":"<article>arXiv:2502.10047v1 Announce Type: new \nAbstract: Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Network architectures and achieved state-of-the-art results in various computer vision tasks. Since ViTs are computationally expensive, the models either have to be pruned to run on resource-limited edge devices only or have to be executed on remote cloud servers after receiving the raw data transmitted over fluctuating networks. The resulting degraded performance or high latency all hinder their widespread applications. In this paper, we present Janus, the first framework for low-latency cloud-device collaborative Vision Transformer inference over dynamic networks. Janus overcomes the intrinsic model limitations of ViTs and realizes collaboratively executing ViT models on both cloud and edge devices, achieving low latency, high accuracy, and low communication overhead. Specifically, Janus judiciously combines token pruning techniques with a carefully designed fine-to-coarse model splitting policy and non-static mixed pruning policy. It attains a balance between accuracy and latency by dynamically selecting the optimal pruning level and split point. Experimental results across various tasks demonstrate that Janus enhances throughput by up to 5.15 times and reduces latency violation ratios by up to 98.7% when compared with baseline approaches under various network environments.</article>","contentLength":1424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in Arbitrary Virtual Environments","url":"https://arxiv.org/abs/2502.10046","date":1739768400,"author":"","guid":1399,"unread":true,"content":"<article>arXiv:2502.10046v1 Announce Type: new \nAbstract: Creating lifelike virtual agents capable of interacting with their environments is a longstanding goal in computer graphics. This paper addresses the challenge of generating natural head rotations, a critical aspect of believable agent behavior for visual information gathering and dynamic responses to environmental cues. Although earlier methods have made significant strides, many rely on data-driven or saliency-based approaches, which often underperform in diverse settings and fail to capture deeper cognitive factors such as risk assessment, information seeking, and contextual prioritization. Consequently, generated behaviors can appear rigid or overlook critical scene elements, thereby diminishing the sense of realism. In this paper, we propose \\textbf{ViRAC}, a \\textbf{Vi}sion-\\textbf{R}easoning \\textbf{A}gent Head Movement \\textbf{C}ontrol framework, which exploits the common-sense knowledge and reasoning capabilities of large-scale models, including Vision-Language Models (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling every cognitive mechanism, ViRAC leverages the biases and patterns internalized by these models from extensive training, thus emulating human-like perceptual processes without hand-tuned heuristics. Experimental results in multiple scenarios reveal that ViRAC produces more natural and context-aware head rotations than recent state-of-the-art techniques. Quantitative evaluations show a closer alignment with real human head-movement data, while user studies confirm improved realism and cognitive plausibility.</article>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unsupervised Entity Alignment Based on Personalized Discriminative Rooted Tree","url":"https://arxiv.org/abs/2502.10044","date":1739768400,"author":"","guid":1400,"unread":true,"content":"<article>arXiv:2502.10044v1 Announce Type: new \nAbstract: Entity Alignment (EA) is to link potential equivalent entities across different knowledge graphs (KGs). Most existing EA methods are supervised as they require the supervision of seed alignments, i.e., manually specified aligned entity pairs. Very recently, several EA studies have made some attempts to get rid of seed alignments. Despite achieving preliminary progress, they still suffer two limitations: (1) The entity embeddings produced by their GNN-like encoders lack personalization since some of the aggregation subpaths are shared between different entities. (2) They cannot fully alleviate the distribution distortion issue between candidate KGs due to the absence of the supervised signal. In this work, we propose a novel unsupervised entity alignment approach called UNEA to address the above two issues. First, we parametrically sample a tree neighborhood rooted at each entity, and accordingly develop a tree attention aggregation mechanism to extract a personalized embedding for each entity. Second, we introduce an auxiliary task of maximizing the mutual information between the input and the output of the KG encoder, to regularize the model and prevent the distribution distortion. Extensive experiments show that our UNEA achieves a new state-of-the-art for the unsupervised EA task, and can even outperform many existing supervised EA baselines.</article>","contentLength":1416,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Law Tradeoff Between Throughput and Sensing Distance in Large ISAC Networks","url":"https://arxiv.org/abs/2502.10042","date":1739768400,"author":"","guid":1401,"unread":true,"content":"<article>arXiv:2502.10042v1 Announce Type: new \nAbstract: In this paper, we investigate the fundamental tradeoff between communication and sensing performance of \\emph{ad hoc} integrated sensing and communication (ISAC) wireless networks. Specifically, we consider that $n$ nodes are randomly located in an extended network with area $n$ and transmit ISAC signals. Under the pure path loss channel gain model and the condition that the transmission power scales according to the communication distance, we fully characterize the optimal scaling law tradeoff between throughput and sensing distance by proposing an achievable scheme and proving its converse. Our results can be interpreted as follows: by reducing the throughput by a factor of a function of $n$, the sensing range order improves according to the same function of $n$, raised to the power of the ratio between the path loss factors in communication and sensing. We prove that the same result also holds true for ISAC networks with random fading, despite the uncertainty on the connectivity and power level created by random fading. In addition, we show that the scaling law tradeoff cannot be improved by allowing the transmission power and communication distance to scale freely. To the best of our knowledge, this is the first work formally formulating and characterizing the communication and sensing performance scaling law tradeoff of \\emph{ad hoc} ISAC networks.</article>","contentLength":1424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation","url":"https://arxiv.org/abs/2502.10040","date":1739768400,"author":"","guid":1402,"unread":true,"content":"<article>arXiv:2502.10040v1 Announce Type: new \nAbstract: Recently, Vision-Language-Action models (VLA) have advanced robot imitation learning, but high data collection costs and limited demonstrations hinder generalization and current imitation learning methods struggle in out-of-distribution scenarios, especially for long-horizon tasks. A key challenge is how to mitigate compounding errors in imitation learning, which lead to cascading failures over extended trajectories. To address these challenges, we propose the Diffusion Trajectory-guided Policy (DTP) framework, which generates 2D trajectories through a diffusion model to guide policy learning for long-horizon tasks. By leveraging task-relevant trajectories, DTP provides trajectory-level guidance to reduce error accumulation. Our two-stage approach first trains a generative vision-language model to create diffusion-based trajectories, then refines the imitation policy using them. Experiments on the CALVIN benchmark show that DTP outperforms state-of-the-art baselines by 25% in success rate, starting from scratch without external pretraining. Moreover, DTP significantly improves real-world robot performance.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI Representation Learning","url":"https://arxiv.org/abs/2502.10038","date":1739768400,"author":"","guid":1403,"unread":true,"content":"<article>arXiv:2502.10038v1 Announce Type: new \nAbstract: POI representation learning plays a crucial role in handling tasks related to user mobility data. Recent studies have shown that enriching POI representations with multimodal information can significantly enhance their task performance. Previously, the textual information incorporated into POI representations typically involved only POI categories or check-in content, leading to relatively weak textual features in existing methods. In contrast, large language models (LLMs) trained on extensive text data have been found to possess rich textual knowledge. However leveraging such knowledge to enhance POI representation learning presents two key challenges: first, how to extract POI-related knowledge from LLMs effectively, and second, how to integrate the extracted information to enhance POI representations. To address these challenges, we propose POI-Enhancer, a portable framework that leverages LLMs to improve POI representations produced by classic POI learning models. We first design three specialized prompts to extract semantic information from LLMs efficiently. Then, the Dual Feature Alignment module enhances the quality of the extracted information, while the Semantic Feature Fusion module preserves its integrity. The Cross Attention Fusion module then fully adaptively integrates such high-quality information into POI representations and Multi-View Contrastive Learning further injects human-understandable semantic information into these representations. Extensive experiments on three real-world datasets demonstrate the effectiveness of our framework, showing significant improvements across all baseline representations.</article>","contentLength":1698,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automation Bias in the AI Act: On the Legal Implications of Attempting to De-Bias Human Oversight of AI","url":"https://arxiv.org/abs/2502.10036","date":1739768400,"author":"","guid":1404,"unread":true,"content":"<article>arXiv:2502.10036v1 Announce Type: new \nAbstract: This paper examines the legal implications of the explicit mentioning of automation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates human oversight for high-risk AI systems and requires providers to enable awareness of AB, i.e., the tendency to over-rely on AI outputs. The paper analyses how this extra-juridical concept is embedded in the AIA, the division of responsibility between AI providers and deployers, and the challenges of legally enforcing this novel awareness requirement. The analysis shows that the AIA's focus on providers does not adequately address design and context as causes of AB, and questions whether the AIA should directly regulate the risk of AB rather than just mandating awareness. As the AIA's approach requires a balance between legal mandates and behavioural science, the paper proposes that harmonised standards should reference the state of research on AB and human-AI interaction. Ultimately, further empirical research will be essential for effective safeguards.</article>","contentLength":1069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Phi-FEM-FNO: a new approach to train a Neural Operator as a fast PDE solver for variable geometries","url":"https://arxiv.org/abs/2502.10033","date":1739768400,"author":"","guid":1405,"unread":true,"content":"<article>arXiv:2502.10033v1 Announce Type: new \nAbstract: In this paper, we propose a way to solve partial differential equations (PDEs) by combining machine learning techniques and the finite element method called Phi-FEM. For that, we use the Fourier Neural Operator (FNO), a learning mapping operator. The purpose of this paper is to provide numerical evidence to show the effectiveness of this technique. We will focus here on the resolution of two equations: the Poisson-Dirichlet equation and the non-linear elasticity equations. The key idea of our method is to address the challenging scenario of varying domains, where each problem is solved on a different geometry. The considered domains are defined by level-set functions due to the use of the Phi-FEM approach. We will first recall the idea of $\\varphi$-FEM and of the Fourier Neural Operator. Then, we will explain how to combine these two methods. We will finally illustrate the efficiency of this combination with some numerical results on three test cases. In addition, in the last test case, we propose a new numerical scheme for hyperelastic materials following the Phi-FEM paradigm.</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation","url":"https://arxiv.org/abs/2502.10028","date":1739768400,"author":"","guid":1406,"unread":true,"content":"<article>arXiv:2502.10028v1 Announce Type: new \nAbstract: Language-conditioned manipulation is a vital but challenging robotic task due to the high-level abstraction of language. To address this, researchers have sought improved goal representations derived from natural language. In this paper, we highlight 3D flow - representing the motion trend of 3D particles within a scene - as an effective bridge between language-based future image generation and fine-grained action prediction. To this end, we develop ManiTrend, a unified framework that models the dynamics of 3D particles, vision observations and manipulation actions with a causal transformer. Within this framework, features for 3D flow prediction serve as additional conditions for future image generation and action prediction, alleviating the complexity of pixel-wise spatiotemporal modeling and providing seamless action guidance. Furthermore, 3D flow can substitute missing or heterogeneous action labels during large-scale pretraining on cross-embodiment demonstrations. Experiments on two comprehensive benchmarks demonstrate that our method achieves state-of-the-art performance with high efficiency. Our code and model checkpoints will be available upon acceptance.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Heterogeneous Resource Allocation with Multi-task Learning for Wireless Networks","url":"https://arxiv.org/abs/2502.10027","date":1739768400,"author":"","guid":1407,"unread":true,"content":"<article>arXiv:2502.10027v1 Announce Type: new \nAbstract: The optimal solution to an optimization problem depends on the problem's objective function, constraints, and size. While deep neural networks (DNNs) have proven effective in solving optimization problems, changes in the problem's size, objectives, or constraints often require adjustments to the DNN architecture to maintain effectiveness, or even retraining a new DNN from scratch. Given the dynamic nature of wireless networks, which involve multiple and diverse objectives that can have conflicting requirements and constraints, we propose a multi-task learning (MTL) framework to enable a single DNN to jointly solve a range of diverse optimization problems. In this framework, optimization problems with varying dimensionality values, objectives, and constraints are treated as distinct tasks. To jointly address these tasks, we propose a conditional computation-based MTL approach with routing. The multi-task DNN consists of two components, the base DNN (bDNN), which is the single DNN used to extract the solutions for all considered optimization problems, and the routing DNN (rDNN), which manages which nodes and layers of the bDNN to be used during the forward propagation of each task. The output of the rDNN is a binary vector which is multiplied with all bDNN's weights during the forward propagation, creating a unique computational path through the bDNN for each task. This setup allows the tasks to either share parameters or use independent ones, with the decision controlled by the rDNN. The proposed framework supports both supervised and unsupervised learning scenarios. Numerical results demonstrate the efficiency of the proposed MTL approach in solving diverse optimization problems. In contrast, benchmark DNNs lacking the rDNN mechanism were unable to achieve similar levels of performance, highlighting the effectiveness of the proposed architecture.</article>","contentLength":1927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is a Feature, Really? Toward a Unified Understanding Across SE Disciplines","url":"https://arxiv.org/abs/2502.10025","date":1739768400,"author":"","guid":1408,"unread":true,"content":"<article>arXiv:2502.10025v1 Announce Type: new \nAbstract: In software engineering, the concept of a ``feature'' is widely used but inconsistently defined across disciplines such as requirements engineering (RE) and software product lines (SPL). This lack of consistency often results in communication gaps, rework, and inefficiencies in projects. To address these challenges, this paper proposes an empirical, data-driven approach to explore how features are described, implemented, and managed across real-world projects, starting with open-source software (OSS). By analyzing feature-related branches in OSS repositories, we identify patterns in contributor behavior, feature implementation, and project management activities. Our findings provide actionable insights to improve project planning, resource allocation, and team coordination. Additionally, we outline a roadmap to unify the understanding of features across software engineering disciplines. This research aims to bridge gaps between academic inquiry and practical strategies, fostering better feature planning and development workflows in diverse project environments.</article>","contentLength":1126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Differential Equation Approach to the Most-Informative Boolean Function Conjecture","url":"https://arxiv.org/abs/2502.10019","date":1739768400,"author":"","guid":1409,"unread":true,"content":"<article>arXiv:2502.10019v1 Announce Type: new \nAbstract: We study the most-informative Boolean function conjecture using a differential equation approach. This leads to a formulation of a functional inequality on finite-dimensional random variables. We also develop a similar inequality in the case of the Hellinger conjecture. Finally, we conjecture a specific finite dimensional inequality that, if proved, will lead to a proof of the Boolean function conjecture in the balanced case. We further show that the above inequality holds modulo four explicit inequalities (all of which seems to hold via numerical simulation) with the first three containing just two variables and a final one involving four variables.</article>","contentLength":707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recovering nonlinear dynamics from non-uniform observations: A physics-based identification approach with practical case studies","url":"https://arxiv.org/abs/2502.10014","date":1739768400,"author":"","guid":1410,"unread":true,"content":"<article>arXiv:2502.10014v1 Announce Type: new \nAbstract: Uniform and smooth data collection is often infeasible in real-world scenarios. In this paper, we propose an identification framework to effectively handle the so-called non-uniform observations, i.e., data scenarios that include missing measurements, multiple runs, or aggregated observations. The goal is to provide a general approach for accurately recovering the overall dynamics of possibly nonlinear systems, allowing the capture of the system behavior over time from non-uniform observations. The proposed approach exploits prior knowledge by integrating domain-specific, interpretable, physical principles with black-box approximators, proving significant flexibility and adaptability in handling different types of non-uniform measurements, and addressing the limitations of traditional linear and black-box methods. The description of this novel framework is supported by a theoretical study on the effect of non-uniform observations on the accuracy of parameter estimation. Specifically, we demonstrate the existence of upper bounds on the parametric error resulting from missing measurements and aggregated observations. Then, the effectiveness of the approach is demonstrated through two case studies. These include a practical application with missing samples, i.e., the identification of a continuous stirred-tank reactor using real data, and a simulated Lotka-Volterra system under aggregated observations. The results highlight the ability of the framework to robustly estimate the system parameters and to accurately reconstruct the model dynamics despite the availability of non-uniform measurements.</article>","contentLength":1668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probabilistic Lexical Manifold Construction in Large Language Models via Hierarchical Vector Field Interpolation","url":"https://arxiv.org/abs/2502.10013","date":1739768400,"author":"","guid":1411,"unread":true,"content":"<article>arXiv:2502.10013v1 Announce Type: new \nAbstract: Hierarchical vector field interpolation introduces a structured probabilistic framework for lexical representation, ensuring that word embeddings transition smoothly across a continuous manifold rather than being constrained to discrete token mappings. The proposed methodology constructs a probabilistic function space where word representations adhere to topological consistency, mitigating representational discontinuities commonly observed in transformer-based embeddings. Empirical evaluations reveal that probabilistic constraints enhance lexical coherence by refining contextual relationships, leading to improvements in semantic stability across multiple linguistic distributions. The application of divergence minimization techniques ensures that interpolated embeddings maintain probabilistic consistency while preserving computational feasibility for large-scale implementations. Experimental findings demonstrate that interpolated lexical manifolds improve representation density alignment, reducing anisotropic distortions in contextual embedding distributions. Comparative analyses with standard transformer-based models highlight that structured interpolation yields more stable representations, particularly in tasks requiring fine-grained semantic differentiation. The statistical evaluation of embedding divergence confirms that probabilistic lexical manifolds reduce representational inconsistencies while maintaining coherence across varying scales of contextual abstraction. An assessment of computational efficiency reveals that while interpolation introduces minor processing overhead, the structured representation learning approach remains scalable for practical deployment.</article>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dream to Drive: Model-Based Vehicle Control Using Analytic World Models","url":"https://arxiv.org/abs/2502.10012","date":1739768400,"author":"","guid":1412,"unread":true,"content":"<article>arXiv:2502.10012v1 Announce Type: new \nAbstract: Differentiable simulators have recently shown great promise for training autonomous vehicle controllers. Being able to backpropagate through them, they can be placed into an end-to-end training loop where their known dynamics turn into useful priors for the policy to learn, removing the typical black box assumption of the environment. So far, these systems have only been used to train policies. However, this is not the end of the story in terms of what they can offer. Here, for the first time, we use them to train world models. Specifically, we present three new task setups that allow us to learn next state predictors, optimal planners, and optimal inverse states. Unlike analytic policy gradients (APG), which requires the gradient of the next simulator state with respect to the current actions, our proposed setups rely on the gradient of the next state with respect to the current state. We call this approach Analytic World Models (AWMs) and showcase its applications, including how to use it for planning in the Waymax simulator. Apart from pushing the limits of what is possible with such simulators, we offer an improved training recipe that increases performance on the large-scale Waymo Open Motion dataset by up to 12% compared to baselines at essentially no additional cost.</article>","contentLength":1343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InterGridNet: An Electric Network Frequency Approach for Audio Source Location Classification Using Convolutional Neural Networks","url":"https://arxiv.org/abs/2502.10011","date":1739768400,"author":"","guid":1413,"unread":true,"content":"<article>arXiv:2502.10011v1 Announce Type: new \nAbstract: A novel framework, called InterGridNet, is introduced, leveraging a shallow RawNet model for geolocation classification of Electric Network Frequency (ENF) signatures in the SP Cup 2016 dataset. During data preparation, recordings are sorted into audio and power groups based on inherent characteristics, further divided into 50 Hz and 60 Hz groups via spectrogram analysis. Residual blocks within the classification model extract frame-level embeddings, aiding decision-making through softmax activation. The topology and the hyperparameters of the shallow RawNet are optimized using a Neural Architecture Search. The overall accuracy of InterGridNet in the test recordings is 92%, indicating its effectiveness against the state-of-the-art methods tested in the SP Cup 2016. These findings underscore InterGridNet's effectiveness in accurately classifying audio recordings from diverse power grids, advancing state-of-the-art geolocation estimation methods.</article>","contentLength":1007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discovering Polynomial and Quadratic Structure in Nonlinear Ordinary Differential Equations","url":"https://arxiv.org/abs/2502.10005","date":1739768400,"author":"","guid":1414,"unread":true,"content":"<article>arXiv:2502.10005v1 Announce Type: new \nAbstract: Dynamical systems with quadratic or polynomial drift exhibit complex dynamics, yet compared to nonlinear systems in general form, are often easier to analyze, simulate, control, and learn. Results going back over a century have shown that the majority of nonpolynomial nonlinear systems can be recast in polynomial form, and their degree can be reduced further to quadratic. This process of polynomialization/quadratization reveals new variables (in most cases, additional variables have to be added to achieve this) in which the system dynamics adhere to that specific form, which leads us to discover new structures of a model. This chapter summarizes the state of the art for the discovery of polynomial and quadratic representations of finite-dimensional dynamical systems. We review known existence results, discuss the two prevalent algorithms for automating the discovery process, and give examples in form of a single-layer neural network and a phenomenological model of cell signaling.</article>","contentLength":1043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SciClaimHunt: A Large Dataset for Evidence-based Scientific Claim Verification","url":"https://arxiv.org/abs/2502.10003","date":1739768400,"author":"","guid":1415,"unread":true,"content":"<article>arXiv:2502.10003v1 Announce Type: new \nAbstract: Verifying scientific claims presents a significantly greater challenge than verifying political or news-related claims. Unlike the relatively broad audience for political claims, the users of scientific claim verification systems can vary widely, ranging from researchers testing specific hypotheses to everyday users seeking information on a medication. Additionally, the evidence for scientific claims is often highly complex, involving technical terminology and intricate domain-specific concepts that require specialized models for accurate verification. Despite considerable interest from the research community, there is a noticeable lack of large-scale scientific claim verification datasets to benchmark and train effective models. To bridge this gap, we introduce two large-scale datasets, SciClaimHunt and SciClaimHunt_Num, derived from scientific research papers. We propose several baseline models tailored for scientific claim verification to assess the effectiveness of these datasets. Additionally, we evaluate models trained on SciClaimHunt and SciClaimHunt_Num against existing scientific claim verification datasets to gauge their quality and reliability. Furthermore, we conduct human evaluations of the claims in proposed datasets and perform error analysis to assess the effectiveness of the proposed baseline models. Our findings indicate that SciClaimHunt and SciClaimHunt_Num serve as highly reliable resources for training models in scientific claim verification.</article>","contentLength":1537,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EmbBERT-Q: Breaking Memory Barriers in Embedded NLP","url":"https://arxiv.org/abs/2502.10001","date":1739768400,"author":"","guid":1416,"unread":true,"content":"<article>arXiv:2502.10001v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computational demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781 kB, representing a 25x reduction in size with respect to SotA models. By combining architectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2 MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA. Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with respect to existing approaches, achieving an unmatched balance between memory and performance. To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at https://github.com/RiccardoBravin/tiny-LLM.</article>","contentLength":1610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scheduling Strategies for Partially-Replicable Task Chains on Two Types of Resources","url":"https://arxiv.org/abs/2502.10000","date":1739768400,"author":"","guid":1417,"unread":true,"content":"<article>arXiv:2502.10000v1 Announce Type: new \nAbstract: The arrival of heterogeneous (or hybrid) multicore architectures on parallel platforms has brought new performance opportunities for applications and efficiency opportunities to systems. They have also increased the challenges related to thread scheduling, as tasks' execution times will vary depending if they are placed in big (performance) cores or little (efficient) ones. In this paper, we focus on the challenges heterogeneous multicore problems bring to partially-replicable task chains, such as the ones that implement digital communication standards in Software-Defined Radio (SDR). Our objective is to maximize the throughput of these task chains while also minimizing their power consumption. We model this problem as a pipelined workflow scheduling problem using pipelined and replicated parallelism on two types of resources whose objectives are to minimize the period and to use as many little cores as necessary. We propose two greedy heuristics (FERTAC and 2CATAC) and one optimal dynamic programming (HeRAD) solution to the problem. We evaluate our solutions and compare the quality of their schedules (in period and resource utilization) and their execution times using synthetic task chains and an implementation of the DVB-S2 communication standard running on StreamPU. Our results demonstrate the benefits and drawbacks of the different proposed solutions. On average, FERTAC and 2CATAC achieve near-optimal solutions, with periods that are less than 10% worse than the optimal (HeRAD) using fewer than 2 extra cores. These three scheduling strategies now enable programmers and users of StreamPU to transparently make use of heterogeneous multicore processors and achieve throughputs that differ from their theoretical maximums by less than 8% on average.</article>","contentLength":1826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decision Information Meets Large Language Models: The Future of Explainable Operations Research","url":"https://arxiv.org/abs/2502.09994","date":1739768400,"author":"","guid":1418,"unread":true,"content":"<article>arXiv:2502.09994v1 Announce Type: new \nAbstract: Operations Research (OR) is vital for decision-making in many industries. While recent OR methods have seen significant improvements in automation and efficiency through integrating Large Language Models (LLMs), they still struggle to produce meaningful explanations. This lack of clarity raises concerns about transparency and trustworthiness in OR applications. To address these challenges, we propose a comprehensive framework, Explainable Operations Research (EOR), emphasizing actionable and understandable explanations accompanying optimization. The core of EOR is the concept of Decision Information, which emerges from what-if analysis and focuses on evaluating the impact of complex constraints (or parameters) changes on decision-making. Specifically, we utilize bipartite graphs to quantify the changes in the OR model and adopt LLMs to improve the explanation capabilities. Additionally, we introduce the first industrial benchmark to rigorously evaluate the effectiveness of explanations and analyses in OR, establishing a new standard for transparency and clarity in the field.</article>","contentLength":1140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigating Label Ambiguity for Facial Expression Recognition in the Wild","url":"https://arxiv.org/abs/2502.09993","date":1739768400,"author":"","guid":1419,"unread":true,"content":"<article>arXiv:2502.09993v1 Announce Type: new \nAbstract: Facial expression recognition (FER) remains a challenging task due to label ambiguity caused by the subjective nature of facial expressions and noisy samples. Additionally, class imbalance, which is common in real-world datasets, further complicates FER. Although many studies have shown impressive improvements, they typically address only one of these issues, leading to suboptimal results. To tackle both challenges simultaneously, we propose a novel framework called Navigating Label Ambiguity (NLA), which is robust under real-world conditions. The motivation behind NLA is that dynamically estimating and emphasizing ambiguous samples at each iteration helps mitigate noise and class imbalance by reducing the model's bias toward majority classes. To achieve this, NLA consists of two main components: Noise-aware Adaptive Weighting (NAW) and consistency regularization. Specifically, NAW adaptively assigns higher importance to ambiguous samples and lower importance to noisy ones, based on the correlation between the intermediate prediction scores for the ground truth and the nearest negative. Moreover, we incorporate a regularization term to ensure consistent latent distributions. Consequently, NLA enables the model to progressively focus on more challenging ambiguous samples, which primarily belong to the minority class, in the later stages of training. Extensive experiments demonstrate that NLA outperforms existing methods in both overall and mean accuracy, confirming its robustness against noise and class imbalance. To the best of our knowledge, this is the first framework to address both problems simultaneously.</article>","contentLength":1686,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Diffusion Models","url":"https://arxiv.org/abs/2502.09992","date":1739768400,"author":"","guid":1420,"unread":true,"content":"<article>arXiv:2502.09992v1 Announce Type: new \nAbstract: Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.</article>","contentLength":1170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability","url":"https://arxiv.org/abs/2502.09990","date":1739768400,"author":"","guid":1421,"unread":true,"content":"<article>arXiv:2502.09990v1 Announce Type: new \nAbstract: Despite the rapid development of safety alignment techniques for LLMs, defending against multi-turn jailbreaks is still a challenging task. In this paper, we conduct a comprehensive comparison, revealing that some existing defense methods can improve the robustness of LLMs against multi-turn jailbreaks but compromise usability, i.e., reducing general capabilities or causing the over-refusal problem. From the perspective of mechanism interpretability of LLMs, we discover that these methods fail to establish a boundary that exactly distinguishes safe and harmful feature representations. Therefore, boundary-safe representations close to harmful representations are inevitably disrupted, leading to a decline in usability. To address this issue, we propose X-Boundary to push harmful representations away from boundary-safe representations and obtain an exact distinction boundary. In this way, harmful representations can be precisely erased without disrupting safe ones. Experimental results show that X-Boundary achieves state-of-the-art defense performance against multi-turn jailbreaks, while reducing the over-refusal rate by about 20% and maintaining nearly complete general capability. Furthermore, we theoretically prove and empirically verify that X-Boundary can accelerate the convergence process during training. Please see our code at: https://github.com/AI45Lab/X-Boundary.</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Logical Formalisation of a Hypothesis in Weighted Abduction: towards User-Feedback Dialogues","url":"https://arxiv.org/abs/2502.09989","date":1739768400,"author":"","guid":1422,"unread":true,"content":"<article>arXiv:2502.09989v1 Announce Type: new \nAbstract: Weighted abduction computes hypotheses that explain input observations. A reasoner of weighted abduction first generates possible hypotheses and then selects the hypothesis that is the most plausible. Since a reasoner employs parameters, called weights, that control its plausibility evaluation function, it can output the most plausible hypothesis according to a specific application using application-specific weights. This versatility makes it applicable from plant operation to cybersecurity or discourse analysis. However, the predetermined application-specific weights are not applicable to all cases of the application. Hence, the hypothesis selected by the reasoner does not necessarily seem the most plausible to the user. In order to resolve this problem, this article proposes two types of user-feedback dialogue protocols, in which the user points out, either positively, negatively or neutrally, properties of the hypotheses presented by the reasoner, and the reasoner regenerates hypotheses that satisfy the user's feedback. As it is required for user-feedback dialogue protocols, we then prove: (i) our protocols necessarily terminate under certain reasonable conditions; (ii) they achieve hypotheses that have the same properties in common as fixed target hypotheses do in common if the user determines the positivity, negativity or neutrality of each pointed-out property based on whether the target hypotheses have that property.</article>","contentLength":1496,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Verified error bounds for the singular values of structured matrices with applications to computer-assisted proofs for differential equations","url":"https://arxiv.org/abs/2502.09984","date":1739768400,"author":"","guid":1423,"unread":true,"content":"<article>arXiv:2502.09984v1 Announce Type: new \nAbstract: This paper introduces two methods for verifying the singular values of the structured matrix denoted by $R^{-H}AR^{-1}$, where $R$ is a nonsingular matrix and $A$ is a general nonsingular square matrix. The first of the two methods uses the computed factors from a singular value decomposition (SVD) to verify all singular values; the second estimates a lower bound of the minimum singular value without performing the SVD. The proposed approach for verifying all singular values efficiently computes tight error bounds. The method for estimating a lower bound of the minimum singular value is particularly effective for sparse matrices. These methods have proven to be efficient in verifying solutions to differential equation problems, that were previously challenging due to the extensive computational time and memory requirements.</article>","contentLength":884,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ICST Tool Competition 2025 - Self-Driving Car Testing Track","url":"https://arxiv.org/abs/2502.09982","date":1739768400,"author":"","guid":1424,"unread":true,"content":"<article>arXiv:2502.09982v1 Announce Type: new \nAbstract: This is the first edition of the tool competition on testing self-driving cars (SDCs) at the International Conference on Software Testing, Verification and Validation (ICST). The aim is to provide a platform for software testers to submit their tools addressing the test selection problem for simulation-based testing of SDCs, which is considered an emerging and vital domain. The competition provides an advanced software platform and representative case studies to ease participants' entry into SDC regression testing, enabling them to develop their initial test generation tools for SDCS. In this first edition, the competition includes five tools from different authors. All tools were evaluated using (regression) metrics for test selection as well as compared with a baseline approache. This paper provides an overview of the competition, detailing its context, framework, participating tools, evaluation methodology, and key findings.</article>","contentLength":990,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal Dependencies in Complex Data","url":"https://arxiv.org/abs/2502.09981","date":1739768400,"author":"","guid":1425,"unread":true,"content":"<article>arXiv:2502.09981v1 Announce Type: new \nAbstract: Causality in time series can be difficult to determine, especially in the presence of non-linear dependencies. The concept of Granger causality helps analyze potential relationships between variables, thereby offering a method to determine whether one time series can predict-Granger cause-future values of another. Although successful, Granger causal methods still struggle with capturing long-range relations between variables. To this end, we leverage the recently successful Extended Long Short-Term Memory (xLSTM) architecture and propose Granger causal xLSTMs (GC-xLSTM). It first enforces sparsity between the time series components by using a novel dynamic lass penalty on the initial projection. Specifically, we adaptively improve the model and identify sparsity candidates. Our joint optimization procedure then ensures that the Granger causal relations are recovered in a robust fashion. Our experimental evaluations on three datasets demonstrate the overall efficacy of our proposed GC-xLSTM model.</article>","contentLength":1060,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models","url":"https://arxiv.org/abs/2502.09980","date":1739768400,"author":"","guid":1426,"unread":true,"content":"<article>arXiv:2502.09980v1 Announce Type: new \nAbstract: Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on detection and tracking. How those approaches contribute to overall cooperative planning performance is still under-explored. Inspired by recent progress using Large Language Models (LLMs) to build autonomous driving systems, we propose a novel problem setting that integrates an LLM into cooperative autonomous driving, with the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and benchmark. We also propose our baseline method Vehicle-to-Vehicle Large Language Model (V2V-LLM), which uses an LLM to fuse perception information from multiple connected autonomous vehicles (CAVs) and answer driving-related questions: grounding, notable object identification, and planning. Experimental results show that our proposed V2V-LLM can be a promising unified model architecture for performing various tasks in cooperative autonomous driving, and outperforms other baseline methods that use different fusion approaches. Our work also creates a new research direction that can improve the safety of future autonomous driving systems. Our project website: https://eddyhkchiu.github.io/v2vllm.github.io/ .</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoadFed: A Multimodal Federated Learning System for Improving Road Safety","url":"https://arxiv.org/abs/2502.09978","date":1739768400,"author":"","guid":1427,"unread":true,"content":"<article>arXiv:2502.09978v1 Announce Type: new \nAbstract: Internet of Things (IoTs) have been widely applied in Collaborative Intelligent Transportation Systems (C-ITS) for the prevention of road accidents. As one of the primary causes of road accidents in C-ITS, the efficient detection and early alarm of road hazards are of paramount importance. Given the importance, extensive research has explored this topic and obtained favorable results. However, most existing solutions only explore single-modality data, struggle with high computation and communication overhead, or suffer from the curse of high dimensionality in their privacy-preserving methodologies. To overcome these obstacles, in this paper, we introduce RoadFed, an innovative and private multimodal Federated learning-based system tailored for intelligent Road hazard detection and alarm. This framework encompasses an innovative Multimodal Road Hazard Detector, a communication-efficient federated learning approach, and a customized low-error-rate local differential privacy method crafted for high dimensional multimodal data. Experimental results reveal that the proposed RoadFed surpasses most existing systems in the self-gathered real-world and CrisisMMD public datasets. In particular, RoadFed achieves an accuracy of 96.42% with a mere 0.0351 seconds of latency and its communication cost is up to 1,000 times lower than existing systems in this field. It facilitates collaborative training with non-iid high dimensional multimodal real-world data across various data modalities on multiple edges while ensuring privacy preservation for road users.</article>","contentLength":1616,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing","url":"https://arxiv.org/abs/2502.09977","date":1739768400,"author":"","guid":1428,"unread":true,"content":"<article>arXiv:2502.09977v1 Announce Type: new \nAbstract: Effectively incorporating external knowledge into Large Language Models (LLMs) is crucial for enhancing their capabilities and addressing real-world needs. Retrieval-Augmented Generation (RAG) offers an effective method for achieving this by retrieving the most relevant fragments into LLMs. However, the advancements in context window size for LLMs offer an alternative approach, raising the question of whether RAG remains necessary for effectively handling external knowledge. Several existing studies provide inconclusive comparisons between RAG and long-context (LC) LLMs, largely due to limitations in the benchmark designs. In this paper, we present LaRA, a novel benchmark specifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses 2,326 test cases across four practical QA task categories and three types of naturally occurring long texts. Through systematic evaluation of seven open-source and four proprietary LLMs, we find that the optimal choice between RAG and LC depends on a complex interplay of factors, including the model's parameter size, long-text capabilities, context length, task type, and the characteristics of the retrieved chunks. Our findings provide actionable guidelines for practitioners to effectively leverage both RAG and LC approaches in developing and deploying LLM applications. Our code and dataset is provided at: \\href{https://github.com/likuanppd/LaRA}{\\textbf{https://github.com/likuanppd/LaRA}}.</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DPM-Bench: Benchmark for Distributed Process Mining Algorithms on Cyber-Physical Systems","url":"https://arxiv.org/abs/2502.09975","date":1739768400,"author":"","guid":1429,"unread":true,"content":"<article>arXiv:2502.09975v1 Announce Type: new \nAbstract: Process Mining is established in research and industry systems to analyze and optimize processes based on event data from information systems. Within this work, we accomodate process mining techniques to Cyber-Physical Systems. To capture the distributed and heterogeneous characteristics of data, computational resources, and network communication in CPS, the todays process mining algorithms and techniques must be augmented. Specifically, there is a need for new Distributed Process Mining algorithms that enable computations to be performed directly on edge resources, eliminating the need for moving all data to central cloud systems. This paper introduces the DPM-Bench benchmark for comparing such Distributed Process Mining algorithms. DPM-Bench is used to compare algorithms deployed in different computational topologies. The results enable information system engineers to assess whether the existing infrastructure is sufficient to perform distributed process mining, or to identify required improvements in algorithms and hardware. We present and discuss an experimental evaluation with DPM-Bench.</article>","contentLength":1158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Has My System Prompt Been Used? Large Language Model Prompt Membership Inference","url":"https://arxiv.org/abs/2502.09974","date":1739768400,"author":"","guid":1430,"unread":true,"content":"<article>arXiv:2502.09974v1 Announce Type: new \nAbstract: Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of model outputs corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective for prompt membership inference. Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.</article>","contentLength":1033,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InteRecon: Towards Reconstructing Interactivity of Personal Memorable Items in Mixed Reality","url":"https://arxiv.org/abs/2502.09973","date":1739768400,"author":"","guid":1431,"unread":true,"content":"<article>arXiv:2502.09973v1 Announce Type: new \nAbstract: Digital capturing of memorable personal items is a key way to archive personal memories. Although current digitization methods (e.g., photos, videos, 3D scanning) can replicate the physical appearance of an item, they often cannot preserve its real-world interactivity. We present Interactive Digital Item (IDI), a concept of reconstructing both the physical appearance and, more importantly, the interactivity of an item. We first conducted a formative study to understand users' expectations of IDI, identifying key physical interactivity features, including geometry, interfaces, and embedded content of items. Informed by these findings, we developed InteRecon, an AR prototype enabling personal reconstruction functions for IDI creation. An exploratory study was conducted to assess the feasibility of using InteRecon and explore the potential of IDI to enrich personal memory archives. Results show that InteRecon is feasible for IDI creation, and the concept of IDI brings new opportunities for augmenting personal memory archives.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditional Latent Coding with Learnable Synthesized Reference for Deep Image Compression","url":"https://arxiv.org/abs/2502.09971","date":1739768400,"author":"","guid":1432,"unread":true,"content":"<article>arXiv:2502.09971v1 Announce Type: new \nAbstract: In this paper, we study how to synthesize a dynamic reference from an external dictionary to perform conditional coding of the input image in the latent domain and how to learn the conditional latent synthesis and coding modules in an end-to-end manner. Our approach begins by constructing a universal image feature dictionary using a multi-stage approach involving modified spatial pyramid pooling, dimension reduction, and multi-scale feature clustering. For each input image, we learn to synthesize a conditioning latent by selecting and synthesizing relevant features from the dictionary, which significantly enhances the model's capability in capturing and exploring image source correlation. This conditional latent synthesis involves a correlation-based feature matching and alignment strategy, comprising a Conditional Latent Matching (CLM) module and a Conditional Latent Synthesis (CLS) module. The synthesized latent is then used to guide the encoding process, allowing for more efficient compression by exploiting the correlation between the input image and the reference dictionary. According to our theoretical analysis, the proposed conditional latent coding (CLC) method is robust to perturbations in the external dictionary samples and the selected conditioning latent, with an error bound that scales logarithmically with the dictionary size, ensuring stability even with large and diverse dictionaries. Experimental results on benchmark datasets show that our new method improves the coding performance by a large margin (up to 1.2 dB) with a very small overhead of approximately 0.5\\% bits per pixel. Our code is publicly available at https://github.com/ydchen0806/CLC.</article>","contentLength":1738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning","url":"https://arxiv.org/abs/2502.09969","date":1739768400,"author":"","guid":1433,"unread":true,"content":"<article>arXiv:2502.09969v1 Announce Type: new \nAbstract: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VicKAM: Visual Conceptual Knowledge Guided Action Map for Weakly Supervised Group Activity Recognition","url":"https://arxiv.org/abs/2502.09967","date":1739768400,"author":"","guid":1434,"unread":true,"content":"<article>arXiv:2502.09967v1 Announce Type: new \nAbstract: Existing weakly supervised group activity recognition methods rely on object detectors or attention mechanisms to capture key areas automatically. However, they overlook the semantic information associated with captured areas, which may adversely affect the recognition performance. In this paper, we propose a novel framework named Visual Conceptual Knowledge Guided Action Map (VicKAM) which effectively captures the locations of individual actions and integrates them with action semantics for weakly supervised group activity recognition.It generates individual action prototypes from training set as visual conceptual knowledge to bridge action semantics and visual representations. Guided by this knowledge, VicKAM produces action maps that indicate the likelihood of each action occurring at various locations, based on image correlation theorem. It further augments individual action maps using group activity related statistical information, representing individual action distribution under different group activities, to establish connections between action maps and specific group activities. The augmented action map is incorporated with action semantic representations for group activity recognition.Extensive experiments on two public benchmarks, the Volleyball and the NBA datasets, demonstrate the effectiveness of our proposed method, even in cases of limited training data. The code will be released later.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating on Generated: An Approach Towards Self-Evolving Diffusion Models","url":"https://arxiv.org/abs/2502.09963","date":1739768400,"author":"","guid":1435,"unread":true,"content":"<article>arXiv:2502.09963v1 Announce Type: new \nAbstract: Recursive Self-Improvement (RSI) enables intelligence systems to autonomously refine their capabilities. This paper explores the application of RSI in text-to-image diffusion models, addressing the challenge of training collapse caused by synthetic data. We identify two key factors contributing to this collapse: the lack of perceptual alignment and the accumulation of generative hallucinations. To mitigate these issues, we propose three strategies: (1) a prompt construction and filtering pipeline designed to facilitate the generation of perceptual aligned data, (2) a preference sampling method to identify human-preferred samples and filter out generative hallucinations, and (3) a distribution-based weighting scheme to penalize selected samples with hallucinatory errors. Our extensive experiments validate the effectiveness of these approaches.</article>","contentLength":903,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strategyproof Maximum Matching under Dichotomous Agent Preferences","url":"https://arxiv.org/abs/2502.09962","date":1739768400,"author":"","guid":1436,"unread":true,"content":"<article>arXiv:2502.09962v1 Announce Type: new \nAbstract: We consider a two-sided matching problem in which the agents on one side have dichotomous preferences and the other side representing institutions has strict preferences (priorities). It captures several important applications in matching market design in which the agents are only interested in getting matched to an acceptable institution. These include centralized daycare assignment and healthcare rationing. We present a compelling new mechanism that satisfies many prominent and desirable properties including individual rationality, maximum size, fairness, Pareto-efficiency on both sides, strategyproofness on both sides, non-bossiness and having polynomial time running time. As a result, we answer an open problem whether there exists a mechanism that is agent-strategyproof, maximum, fair and non-bossy.</article>","contentLength":863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Global-Local Interface for On-Demand Teleoperation","url":"https://arxiv.org/abs/2502.09960","date":1739768400,"author":"","guid":1437,"unread":true,"content":"<article>arXiv:2502.09960v1 Announce Type: new \nAbstract: Teleoperation is a critical method for human-robot interface, holds significant potential for enabling robotic applications in industrial and unstructured environments. Existing teleoperation methods have distinct strengths and limitations in flexibility, range of workspace and precision. To fuse these advantages, we introduce the Global-Local (G-L) Teleoperation Interface. This interface decouples robotic teleoperation into global behavior, which ensures the robot motion range and intuitiveness, and local behavior, which enhances human operator's dexterity and capability for performing fine tasks. The G-L interface enables efficient teleoperation not only for conventional tasks like pick-and-place, but also for challenging fine manipulation and large-scale movements. Based on the G-L interface, we constructed a single-arm and a dual-arm teleoperation system with different remote control devices, then demonstrated tasks requiring large motion range, precise manipulation or dexterous end-effector control. Extensive experiments validated the user-friendliness, accuracy, and generalizability of the proposed interface.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KGGen: Extracting Knowledge Graphs from Plain Text with Language Models","url":"https://arxiv.org/abs/2502.09956","date":1739768400,"author":"","guid":1438,"unread":true,"content":"<article>arXiv:2502.09956v1 Announce Type: new \nAbstract: Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of a text-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against existing extractors and demonstrate far superior performance.</article>","contentLength":1063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diverse Inference and Verification for Advanced Reasoning","url":"https://arxiv.org/abs/2502.09955","date":1739768400,"author":"","guid":1439,"unread":true,"content":"<article>arXiv:2502.09955v1 Announce Type: new \nAbstract: Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant progress in mathematics and coding, yet find challenging advanced tasks such as International Mathematical Olympiad (IMO) combinatorics problems, Abstraction and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions. We use a diverse inference approach that combines multiple models and methods at test time. We find that verifying mathematics and code problems, and rejection sampling on other problems is simple and effective. We automatically verify correctness of solutions to IMO problems by Lean, and ARC puzzles by code, and find that best-of-N effectively answers HLE questions. Our approach increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%, accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that 948 humans could not and 26.5% of ARC puzzles that o3 high compute does not. Test-time simulations, reinforcement learning, and meta-learning with inference feedback improve generalization by adapting agent graph representations and varying prompts, code, and datasets. Our approach is reliable, robust, and scalable, and in the spirit of reproducible research, we will make it publicly available upon publication.</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Space Folds of ReLU Neural Networks","url":"https://arxiv.org/abs/2502.09954","date":1739768400,"author":"","guid":1440,"unread":true,"content":"<article>arXiv:2502.09954v1 Announce Type: new \nAbstract: Recent findings suggest that the consecutive layers of ReLU neural networks can be understood geometrically as space folding transformations of the input space, revealing patterns of self-similarity. In this paper, we present the first quantitative analysis of this space folding phenomenon in ReLU neural networks. Our approach focuses on examining how straight paths in the Euclidean input space are mapped to their counterparts in the Hamming activation space. In this process, the convexity of straight lines is generally lost, giving rise to non-convex folding behavior. To quantify this effect, we introduce a novel measure based on range metrics, similar to those used in the study of random walks, and provide the proof for the equivalence of convexity notions between the input and activation spaces. Furthermore, we provide empirical analysis on a geometrical analysis benchmark (CantorNet) as well as an image classification benchmark (MNIST). Our work advances the understanding of the activation space in ReLU neural networks by leveraging the phenomena of geometric folding, providing valuable insights on how these models process input information.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using MRNet to Predict Lunar Rock Categories Detected by Chang'e 5 Probe","url":"https://arxiv.org/abs/2502.09952","date":1739768400,"author":"","guid":1441,"unread":true,"content":"<article>arXiv:2502.09952v1 Announce Type: new \nAbstract: China's Chang'e 5 mission has been a remarkable success, with the chang'e 5 lander traveling on the Oceanus Procellarum to collect images of the lunar surface. Over the past half century, people have brought back some lunar rock samples, but its quantity does not meet the need for research. Under current circumstances, people still mainly rely on the analysis of rocks on the lunar surface through the detection of lunar rover. The Oceanus Procellarum, chosen by Chang'e 5 mission, contains various kind of rock species. Therefore, we first applied to the National Astronomical Observatories of the China under the Chinese Academy of Sciences for the Navigation and Terrain Camera (NaTeCam) of the lunar surface image, and established a lunar surface rock image data set CE5ROCK. The data set contains 100 images, which randomly divided into training, validation and test set. Experimental results show that the identification accuracy testing on convolutional neural network (CNN) models like AlexNet or MobileNet is about to 40.0%. In order to make full use of the global information in Moon images, this paper proposes the MRNet (MoonRockNet) network architecture. The encoding structure of the network uses VGG16 for feature extraction, and the decoding part adds dilated convolution and commonly used U-Net structure on the original VGG16 decoding structure, which is more conducive to identify more refined but more sparsely distributed types of lunar rocks. We have conducted extensive experiments on the established CE5ROCK data set, and the experimental results show that MRNet can achieve more accurate rock type identification, and outperform other existing mainstream algorithms in the identification performance.</article>","contentLength":1776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Blind Men and the Elephant: Mapping Interdisciplinarity in Research on Decentralized Autonomous Organizations","url":"https://arxiv.org/abs/2502.09949","date":1739768400,"author":"","guid":1442,"unread":true,"content":"<article>arXiv:2502.09949v1 Announce Type: new \nAbstract: Decentralized Autonomous Organizations (DAOs) are attracting interdisciplinary interest, particularly in business, economics, and computer science. However, much like the parable of the blind men and the elephant, where each observer perceives only a fragment of the whole, DAO research remains fragmented across disciplines, limiting a comprehensive understanding of their potential. This paper assesses the maturity of interdisciplinary research on DAOs by analyzing knowledge flows between Business &amp; Economics and Computer Science through citation network analysis, topic modelling, and outlet analysis. Our findings reveal that while DAOs serve as a vibrant topic of interdisciplinary discourse, current research remains predominantly applied and case-driven, with limited theoretical integration. Strengthening the alignment between organizational and technical insights is crucial for advancing DAO research and fostering a more cohesive interdisciplinary framework.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analyzing Patient Daily Movement Behavior Dynamics Using Two-Stage Encoding Model","url":"https://arxiv.org/abs/2502.09947","date":1739768400,"author":"","guid":1443,"unread":true,"content":"<article>arXiv:2502.09947v1 Announce Type: new \nAbstract: In the analysis of remote healthcare monitoring data, time series representation learning offers substantial value in uncovering deeper patterns of patient behavior, especially given the fine temporal granularity of the data. In this study, we focus on a dataset of home activity records from people living with Dementia. We propose a two-stage self-supervised learning approach. The first stage involves converting time-series activities into text strings, which are then encoded by a fine-tuned language model. In the second stage, these time-series vectors are bi-dimensionalized for applying PageRank method, to analyze latent state transitions to quantitatively assess participants behavioral patterns and identify activity biases. These insights, combined with diagnostic data, aim to support personalized care interventions.</article>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Supervised Learning for Neural Topic Models with Variance-Invariance-Covariance Regularization","url":"https://arxiv.org/abs/2502.09944","date":1739768400,"author":"","guid":1444,"unread":true,"content":"<article>arXiv:2502.09944v1 Announce Type: new \nAbstract: In our study, we propose a self-supervised neural topic model (NTM) that combines the power of NTMs and regularized self-supervised learning methods to improve performance. NTMs use neural networks to learn latent topics hidden behind the words in documents, enabling greater flexibility and the ability to estimate more coherent topics compared to traditional topic models. On the other hand, some self-supervised learning methods use a joint embedding architecture with two identical networks that produce similar representations for two augmented versions of the same input. Regularizations are applied to these representations to prevent collapse, which would otherwise result in the networks outputting constant or redundant representations for all inputs. Our model enhances topic quality by explicitly regularizing latent topic representations of anchor and positive samples. We also introduced an adversarial data augmentation method to replace the heuristic sampling method. We further developed several variation models including those on the basis of an NTM that incorporates contrastive learning with both positive and negative samples. Experimental results on three datasets showed that our models outperformed baselines and state-of-the-art models both quantitatively and qualitatively.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Lightweight and Effective Image Tampering Localization Network with Vision Mamba","url":"https://arxiv.org/abs/2502.09941","date":1739768400,"author":"","guid":1445,"unread":true,"content":"<article>arXiv:2502.09941v1 Announce Type: new \nAbstract: Current image tampering localization methods primarily rely on Convolutional Neural Networks (CNNs) and Transformers. While CNNs suffer from limited local receptive fields, Transformers offer global context modeling at the expense of quadratic computational complexity. Recently, the state space model Mamba has emerged as a competitive alternative, enabling linear-complexity global dependency modeling. Inspired by it, we propose a lightweight and effective FORensic network based on vision MAmba (ForMa) for blind image tampering localization. Firstly, ForMa captures multi-scale global features that achieves efficient global dependency modeling through linear complexity. Then the pixel-wise localization map is generated by a lightweight decoder, which employs a parameter-free pixel shuffle layer for upsampling. Additionally, a noise-assisted decoding strategy is proposed to integrate complementary manipulation traces from tampered images, boosting decoder sensitivity to forgery cues. Experimental results on 10 standard datasets demonstrate that ForMa achieves state-of-the-art generalization ability and robustness, while maintaining the lowest computational complexity. Code is available at https://github.com/multimediaFor/ForMa.</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Preliminary Exploration with GPT-4o Voice Mode","url":"https://arxiv.org/abs/2502.09940","date":1739768400,"author":"","guid":1446,"unread":true,"content":"<article>arXiv:2502.09940v1 Announce Type: new \nAbstract: With the rise of multimodal large language models, GPT-4o stands out as a pioneering model, driving us to evaluate its capabilities. This report assesses GPT-4o across various tasks to analyze its audio processing and reasoning abilities. We find that GPT-4o exhibits strong knowledge in audio, speech, and music understanding, performing well in tasks like intent classification, spoken command classification, semantic and grammatical reasoning., multilingual speech recognition, and singing analysis. It also shows greater robustness against hallucinations than other large audio-language models (LALMs). However, it struggles with tasks such as audio duration prediction and instrument classification. Additionally, GPT-4o's safety mechanisms cause it to decline tasks like speaker identification, age classification, MOS prediction, and audio deepfake detection. Notably, the model exhibits a significantly different refusal rate when responding to speaker verification tasks on different datasets. This is likely due to variations in the accompanying instructions or the quality of the input audio, suggesting the sensitivity of its built-in safeguards. Finally, we acknowledge that model performance varies with evaluation protocols. This report only serves as a preliminary exploration of the current state of LALMs.</article>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Temporal Scale and Shift Invariant Automatic Event Recognition using the Mellin Transform","url":"https://arxiv.org/abs/2502.09939","date":1739768400,"author":"","guid":1447,"unread":true,"content":"<article>arXiv:2502.09939v1 Announce Type: new \nAbstract: The Spatio-temporal holographic correlator combines the traditional 2D optical image correlation techniques with inhomogeneously broadened arrays of cold atoms to achieve 3D time-space correlation to realize automatic event recognition at an ultra-high speed. Here we propose a method to realize such event recognition for videos running at different speeds. With this method, we can highly improve recognition accuracy and filter almost all the unwanted events in the video database.</article>","contentLength":533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tradeoffs in Processing Queries and Supporting Updates over an ML-Enhanced R-tree","url":"https://arxiv.org/abs/2502.09937","date":1739768400,"author":"","guid":1448,"unread":true,"content":"<article>arXiv:2502.09937v1 Announce Type: new \nAbstract: Machine Learning (ML) techniques have been successfully applied to design various learned database index structures for both the one- and multi-dimensional spaces. Particularly, a class of traditional multi-dimensional indexes has been augmented with ML models to design ML-enhanced variants of their traditional counterparts. This paper focuses on the R-tree multi-dimensional index structure as it is widely used for indexing multi-dimensional data. The R-tree has been augmented with machine learning models to enhance the R-tree performance. The AI+R-tree is an ML-enhanced R-tree index structure that augments a traditional disk-based R-tree with an ML model to enhance the R-tree's query processing performance, mainly, to avoid navigating the overlapping branches of the R-tree that do not yield query results, e.g., in the presence of high-overlap among the rectangles of the R-tree nodes. We investigate the empirical tradeoffs in processing dynamic query workloads and in supporting updates over the AI+R-tree. Particularly, we investigate the impact of the choice of ML models over the AI+R-tree query processing performance. Moreover, we present a case study of designing a custom loss function for a neural network model tailored to the query processing requirements of the AI+R-tree. Furthermore, we present the design tradeoffs for adopting various strategies for supporting dynamic inserts, updates, and deletes with the vision of realizing a mutable AI+R-tree. Experiments on real datasets demonstrate that the AI+R-tree can enhance the query processing performance of a traditional R-tree for high-overlap range queries by up to 5.4X while achieving up to 99% average query recall.</article>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Precise Parameter Localization for Textual Generation in Diffusion Models","url":"https://arxiv.org/abs/2502.09935","date":1739768400,"author":"","guid":1449,"unread":true,"content":"<article>arXiv:2502.09935v1 Announce Type: new \nAbstract: Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at https://t2i-text-loc.github.io/.</article>","contentLength":1476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fused Partial Gromov-Wasserstein for Structured Objects","url":"https://arxiv.org/abs/2502.09934","date":1739768400,"author":"","guid":1450,"unread":true,"content":"<article>arXiv:2502.09934v1 Announce Type: new \nAbstract: Structured data, such as graphs, are vital in machine learning due to their capacity to capture complex relationships and interactions. In recent years, the Fused Gromov-Wasserstein (FGW) distance has attracted growing interest because it enables the comparison of structured data by jointly accounting for feature similarity and geometric structure. However, as a variant of optimal transport (OT), classical FGW assumes an equal mass constraint on the compared data. In this work, we relax this mass constraint and propose the Fused Partial Gromov-Wasserstein (FPGW) framework, which extends FGW to accommodate unbalanced data. Theoretically, we establish the relationship between FPGW and FGW and prove the metric properties of FPGW. Numerically, we introduce Frank-Wolfe solvers for the proposed FPGW framework and provide a convergence analysis. Finally, we evaluate the FPGW distance through graph classification and clustering experiments, demonstrating its robust performance, especially when data is corrupted by outlier noise.</article>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot In-Context Inductive Reasoning","url":"https://arxiv.org/abs/2502.09933","date":1739768400,"author":"","guid":1451,"unread":true,"content":"<article>arXiv:2502.09933v1 Announce Type: new \nAbstract: Inductive Reasoning (IR), the ability to summarize rules from examples and apply on new ones, has long been viewed as a primal ability for general intelligence and widely studied by cognitive science and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually $&lt;$10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations are mostly focused on classification (a very limited aspect of IR), and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for inductive reasoning and many-shot ICL, including robustness against erroneous shots and the effect of Chain-of-Thought (CoT), and acquired insightful findings.</article>","contentLength":1444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AffectSRNet : Facial Emotion-Aware Super-Resolution Network","url":"https://arxiv.org/abs/2502.09932","date":1739768400,"author":"","guid":1452,"unread":true,"content":"<article>arXiv:2502.09932v1 Announce Type: new \nAbstract: Facial expression recognition (FER) systems in low-resolution settings face significant challenges in accurately identifying expressions due to the loss of fine-grained facial details. This limitation is especially problematic for applications like surveillance and mobile communications, where low image resolution is common and can compromise recognition accuracy. Traditional single-image face super-resolution (FSR) techniques, however, often fail to preserve the emotional intent of expressions, introducing distortions that obscure the original affective content. Given the inherently ill-posed nature of single-image super-resolution, a targeted approach is required to balance image quality enhancement with emotion retention. In this paper, we propose AffectSRNet, a novel emotion-aware super-resolution framework that reconstructs high-quality facial images from low-resolution inputs while maintaining the intensity and fidelity of facial expressions. Our method effectively bridges the gap between image resolution and expression accuracy by employing an expression-preserving loss function, specifically tailored for FER applications. Additionally, we introduce a new metric to assess emotion preservation in super-resolved images, providing a more nuanced evaluation of FER system performance in low-resolution scenarios. Experimental results on standard datasets, including CelebA, FFHQ, and Helen, demonstrate that AffectSRNet outperforms existing FSR approaches in both visual quality and emotion fidelity, highlighting its potential for integration into practical FER applications. This work not only improves image clarity but also ensures that emotion-driven applications retain their core functionality in suboptimal resolution environments, paving the way for broader adoption in FER systems.</article>","contentLength":1863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TransGUNet: Transformer Meets Graph-based Skip Connection for Medical Image Segmentation","url":"https://arxiv.org/abs/2502.09931","date":1739768400,"author":"","guid":1453,"unread":true,"content":"<article>arXiv:2502.09931v1 Announce Type: new \nAbstract: Skip connection engineering is primarily employed to address the semantic gap between the encoder and decoder, while also integrating global dependencies to understand the relationships among complex anatomical structures in medical image segmentation. Although several models have proposed transformer-based approaches to incorporate global dependencies within skip connections, they often face limitations in capturing detailed local features with high computational complexity. In contrast, graph neural networks (GNNs) exploit graph structures to effectively capture local and global features. Leveraging these properties, we introduce an attentional cross-scale graph neural network (ACS-GNN), which enhances the skip connection framework by converting cross-scale feature maps into a graph structure and capturing complex anatomical structures through node attention. Additionally, we observed that deep learning models often produce uninformative feature maps, which degrades the quality of spatial attention maps. To address this problem, we integrated entropy-driven feature selection (EFS) with spatial attention, calculating an entropy score for each channel and filtering out high-entropy feature maps. Our innovative framework, TransGUNet, comprises ACS-GNN and EFS-based spatial attentio} to effectively enhance domain generalizability across various modalities by leveraging GNNs alongside a reliable spatial attention map, ensuring more robust features within the skip connection. Through comprehensive experiments and analysis, TransGUNet achieved superior segmentation performance on six seen and eight unseen datasets, demonstrating significantly higher efficiency compared to previous methods.</article>","contentLength":1762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Tree Tensor Networks for Image Recognition","url":"https://arxiv.org/abs/2502.09928","date":1739768400,"author":"","guid":1454,"unread":true,"content":"<article>arXiv:2502.09928v1 Announce Type: new \nAbstract: Originating in quantum physics, tensor networks (TNs) have been widely adopted as exponential machines and parameter decomposers for recognition tasks. Typical TN models, such as Matrix Product States (MPS), have not yet achieved successful application in natural image processing. When employed, they primarily serve to compress parameters within off-the-shelf networks, thus losing their distinctive capability to enhance exponential-order feature interactions. This paper introduces a novel architecture named \\textit{\\textbf{D}eep \\textbf{T}ree \\textbf{T}ensor \\textbf{N}etwork} (DTTN), which captures $2^L$-order multiplicative interactions across features through multilinear operations, while essentially unfolding into a \\emph{tree}-like TN topology with the parameter-sharing property. DTTN is stacked with multiple antisymmetric interacting modules (AIMs), and this design facilitates efficient implementation. Moreover, we theoretically reveal the equivalency among quantum-inspired TN models and polynomial and multilinear networks under certain conditions, and we believe that DTTN can inspire more interpretable studies in this field. We evaluate the proposed model against a series of benchmarks and achieve excellent performance compared to its peers and cutting-edge architectures. Our code will soon be publicly available.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence","url":"https://arxiv.org/abs/2502.09927","date":1739768400,"author":"","guid":1455,"unread":true,"content":"<article>arXiv:2502.09927v1 Announce Type: new \nAbstract: We introduce Granite Vision, a lightweight large language model with vision capabilities, specifically designed to excel in enterprise use cases, particularly in visual document understanding. Our model is trained on a comprehensive instruction-following dataset, including document-related tasks, such as content extraction from tables, charts, diagrams, sketches, and infographics, as well as general image tasks. The architecture of Granite Vision is centered around visual modality alignment with a decoder-only, 2 billion parameter Granite large language model. Additionally, we introduce a dedicated safety classification approach in test-time that leverages a sparse set of attention vectors to identify potential harmful inputs. Despite its lightweight architecture, Granite Vision achieves strong results in standard benchmarks related to visual document understanding, as well as on the LiveXiv benchmark, which is designed to avoid test set contamination by using a constantly updated corpus of recently published Arxiv papers. We are releasing the model under the Apache-2 license, allowing for both research and commercial use, while offering complete visibility into the training data and other relevant details. See https://huggingface.co/ibm-granite/ for model weights.</article>","contentLength":1334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Anomaly Detection via Tensor Chidori Pseudoskeleton Decomposition","url":"https://arxiv.org/abs/2502.09926","date":1739768400,"author":"","guid":1456,"unread":true,"content":"<article>arXiv:2502.09926v1 Announce Type: new \nAbstract: Anomaly detection plays a critical role in modern data-driven applications, from identifying fraudulent transactions and safeguarding network infrastructure to monitoring sensor systems for irregular patterns. Traditional approaches, such as distance, density, or cluster-based methods, face significant challenges when applied to high dimensional tensor data, where complex interdependencies across dimensions amplify noise and computational complexity. To address these limitations, this paper leverages Tensor Chidori pseudoskeleton decomposition within a tensor-robust principal component analysis framework to extract low Tucker rank structure while isolating sparse anomalies, ensuring robustness to anomaly detection. We establish theoretical results regarding convergence, and estimation error, demonstrating the stability and accuracy of the proposed approach. Numerical experiments on real-world spatiotemporal data from New York City taxi trip records validate the superiority of the proposed method in detecting anomalous urban events compared to existing benchmark methods. The results underscore the potential of Tensor Chidori pseudoskeleton decomposition to enhance anomaly detection for large-scale, high-dimensional data.</article>","contentLength":1288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types","url":"https://arxiv.org/abs/2502.09925","date":1739768400,"author":"","guid":1457,"unread":true,"content":"<article>arXiv:2502.09925v1 Announce Type: new \nAbstract: Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. To address this, we propose TaskGalaxy, a large-scale multimodal instruction fine-tuning dataset comprising 19,227 hierarchical task types and 413,648 samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple models are employed to ensure sample quality. This automated process enhances both task diversity and data quality, reducing manual intervention. Incorporating TaskGalaxy into LLaVA-v1.5 and InternVL-Chat-v1.0 models shows substantial performance improvements across 16 benchmarks, demonstrating the critical importance of task diversity. TaskGalaxy is publicly released at https://github.com/Kwai-YuanQi/TaskGalaxy.</article>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Consistent Model-based Adaptation for Visual Reinforcement Learning","url":"https://arxiv.org/abs/2502.09923","date":1739768400,"author":"","guid":1458,"unread":true,"content":"<article>arXiv:2502.09923v1 Announce Type: new \nAbstract: Visual reinforcement learning agents typically face serious performance declines in real-world applications caused by visual distractions. Existing methods rely on fine-tuning the policy's representations with hand-crafted augmentations. In this work, we propose Self-Consistent Model-based Adaptation (SCMA), a novel method that fosters robust adaptation without modifying the policy. By transferring cluttered observations to clean ones with a denoising model, SCMA can mitigate distractions for various policies as a plug-and-play enhancement. To optimize the denoising model in an unsupervised manner, we derive an unsupervised distribution matching objective with a theoretical analysis of its optimality. We further present a practical algorithm to optimize the objective by estimating the distribution of clean observations with a pre-trained world model. Extensive experiments on multiple visual generalization benchmarks and real robot data demonstrate that SCMA effectively boosts performance across various distractions and exhibits better sample efficiency.</article>","contentLength":1118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"{\\lambda}Scale: Enabling Fast Scaling for Serverless Large Language Model Inference","url":"https://arxiv.org/abs/2502.09922","date":1739768400,"author":"","guid":1459,"unread":true,"content":"<article>arXiv:2502.09922v1 Announce Type: new \nAbstract: Serverless computing has emerged as a compelling solution for cloud-based model inference. However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead. This poses a significant challenge in efficiently scaling model instances to accommodate dynamic, bursty workloads commonly observed in real-world inference services. In this paper, we introduce {\\lambda}Scale, an efficient serverless inference system to achieve fast model scaling. The key idea behind {\\lambda}Scale is to leverage high-speed RDMA networks between GPU nodes for fast model multicast, while enabling distributed inference execution during model transmission -- referred to as \"execute-while-load\". {\\lambda}Scale proposes an efficient model scaling scheme, {\\lambda}Pipe, which supports adaptive model multicast and dynamically constructs execution pipelines across receiving nodes for collaborative, distributed inference. Additionally, {\\lambda}Scale supports efficient model management across GPU and host memory, allowing fast scaling for models across different storage tiers. Evaluation results show that {\\lambda}Scale enables fast model scaling and effectively handles load spikes, achieving up to 5x tail-latency improvement and 31.3% cost reduction compared to state-of-the-art solutions on real-world LLM inference traces.</article>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"INF^2: High-Throughput Generative Inference of Large Language Models using Near-Storage Processing","url":"https://arxiv.org/abs/2502.09921","date":1739768400,"author":"","guid":1460,"unread":true,"content":"<article>arXiv:2502.09921v1 Announce Type: new \nAbstract: The growing memory and computational demands of large language models (LLMs) for generative inference present significant challenges for practical deployment. One promising solution to address these challenges is offloading-based batched inference, which leverages host memory and disk as an extended memory hierarchy for GPUs. While the approach cost-effectively enables LLM inference, its performance is limited by substantial I/O overhead, primarily due to the large key-value (KV) cache sizes, which increase with batch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that boosts generative inference throughput using computational storage devices (CSDs). The core of INF^2 is attention-near storage, which offloads memory-intensive self-attention operations to near-storage accelerators, significantly reducing traffic through the system interconnect. We also propose delayed KV cache writeback to hide storage write latency by delaying newly generated KV cache writes until the cache reaches sufficient size in system memory. Additionally, we introduce cooperative X-cache, a technique designed to further trade off the remaining memory capacity for storage bandwidth. Our methods effectively minimize idle time for computation, improving the overall throughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been implemented on PyTorch and evaluated on a real system. Our experiments show that INF^2 achieves up to 3.46$\\times$ throughput improvement compared to state-of-the-art baselines. We will open-source INF^2 to facilitate broader adoption.</article>","contentLength":1682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting on AI-READI Dataset","url":"https://arxiv.org/abs/2502.09919","date":1739768400,"author":"","guid":1461,"unread":true,"content":"<article>arXiv:2502.09919v1 Announce Type: new \nAbstract: Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively.</article>","contentLength":1952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dual Control for Interactive Autonomous Merging with Model Predictive Diffusion","url":"https://arxiv.org/abs/2502.09918","date":1739768400,"author":"","guid":1462,"unread":true,"content":"<article>arXiv:2502.09918v1 Announce Type: new \nAbstract: Interactive decision-making is essential in applications such as autonomous driving, where the agent must infer the behavior of nearby human drivers while planning in real-time. Traditional predict-then-act frameworks are often insufficient or inefficient because accurate inference of human behavior requires a continuous interaction rather than isolated prediction. To address this, we propose an active learning framework in which we rigorously derive predicted belief distributions. Additionally, we introduce a novel model-based diffusion solver tailored for online receding horizon control problems, demonstrated through a complex, non-convex highway merging scenario. Our approach extends previous high-fidelity dual control simulations to hardware experiments, which may be viewed at https://youtu.be/Q_JdZuopGL4, and verifies behavior inference in human-driven traffic scenarios, moving beyond idealized models. The results show improvements in adaptive planning under uncertainty, advancing the field of interactive decision-making for real-world applications.</article>","contentLength":1119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Deep Learning Approach to Interface Color Quality Assessment in HCI","url":"https://arxiv.org/abs/2502.09914","date":1739768400,"author":"","guid":1463,"unread":true,"content":"<article>arXiv:2502.09914v1 Announce Type: new \nAbstract: In this paper, a quantitative evaluation model for the color quality of human-computer interaction interfaces is proposed by combining deep convolutional neural networks (CNN). By extracting multidimensional features of interface images, including hue, brightness, purity, etc., CNN is used for efficient feature modeling and quantitative analysis, and the relationship between interface design and user perception is studied. The experiment is based on multiple international mainstream website interface datasets, covering e-commerce platforms, social media, education platforms, etc., and verifies the evaluation effect of the model on indicators such as contrast, clarity, color coordination, and visual appeal. The results show that the CNN evaluation is highly consistent with the user rating, with a correlation coefficient of up to 0.96, and it also shows high accuracy in mean square error and absolute error. Compared with traditional experience-based evaluation methods, the proposed model can efficiently and scientifically capture the visual characteristics of the interface and avoid the influence of subjective factors. Future research can explore the introduction of multimodal data (such as text and interactive behavior) into the model to further enhance the evaluation ability of dynamic interfaces and expand it to fields such as smart homes, medical systems, and virtual reality. This paper provides new methods and new ideas for the scientific evaluation and optimization of interface design.</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AutoS$^2$earch: Unlocking the Reasoning Potential of Large Models for Web-based Source Search","url":"https://arxiv.org/abs/2502.09913","date":1739768400,"author":"","guid":1464,"unread":true,"content":"<article>arXiv:2502.09913v1 Announce Type: new \nAbstract: Web-based management systems have been widely used in risk control and industrial safety. However, effectively integrating source search capabilities into these systems, to enable decision-makers to locate and address the hazard (e.g., gas leak detection) remains a challenge. While prior efforts have explored using web crowdsourcing and AI algorithms for source search decision support, these approaches suffer from overheads in recruiting human participants and slow response times in time-sensitive situations. To address this, we introduce AutoS$^2$earch, a novel framework leveraging large models for zero-shot source search in web applications. AutoS$^2$earch operates on a simplified visual environment projected through a web-based display, utilizing a chain-of-thought prompt designed to emulate human reasoning. The multi-modal large language model (MLLMs) dynamically converts visual observations into language descriptions, enabling the LLM to perform linguistic reasoning on four directional choices. Extensive experiments demonstrate that AutoS$^2$earch achieves performance nearly equivalent to human-AI collaborative source search while eliminating dependency on crowdsourced labor. Our work offers valuable insights in using web engineering to design such autonomous systems in other industrial applications.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Breaking the Familiarity Bias: Employing Virtual Reality Environments to Enhance Team Formation and Inclusion","url":"https://arxiv.org/abs/2502.09912","date":1739768400,"author":"","guid":1465,"unread":true,"content":"<article>arXiv:2502.09912v1 Announce Type: new \nAbstract: Team closeness provides the foundations of trust and communication, contributing to teams' success and viability. However, newcomers often struggle to be included in a team since incumbents tend to interact more with other existing members. Previous research suggests that online communication technologies can help team inclusion by mitigating members' perceived differences. In this study, we test how virtual reality (VR) can promote team closeness when forming teams. We conducted a between-subject experiment with teams working in-person and VR, where two members interacted first, and then a third member was added later to conduct a hidden-profile task. Participants evaluated how close they felt with their teammates after the task was completed. Our results show that VR newcomers felt closer to the incumbents than in-person newcomers. However, incumbents' closeness to newcomers did not vary across conditions. We discuss the implications of these findings and offer suggestions for how VR can promote inclusion.</article>","contentLength":1072,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prior-Independent Bidding Strategies for First-Price Auctions","url":"https://arxiv.org/abs/2502.09907","date":1739768400,"author":"","guid":1466,"unread":true,"content":"<article>arXiv:2502.09907v1 Announce Type: new \nAbstract: First-price auctions are one of the most popular mechanisms for selling goods and services, with applications ranging from display advertising to timber sales. Unlike their close cousin, the second-price auction, first-price auctions do not admit a dominant strategy. Instead, each buyer must design a bidding strategy that maps values to bids -- a task that is often challenging due to the lack of prior knowledge about competing bids. To address this challenge, we conduct a principled analysis of prior-independent bidding strategies for first-price auctions using worst-case regret as the performance measure. First, we develop a technique to evaluate the worst-case regret for (almost) any given value distribution and bidding strategy, reducing the complex task of ascertaining the worst-case competing-bid distribution to a simple line search. Next, building on our evaluation technique, we minimize worst-case regret and characterize a minimax-optimal bidding strategy for every value distribution. We achieve it by explicitly constructing a bidding strategy as a solution to an ordinary differential equation, and by proving its optimality for the intricate infinite-dimensional minimax problem underlying worst-case regret minimization. Our construction provides a systematic and computationally-tractable procedure for deriving minimax-optimal bidding strategies. When the value distribution is continuous, it yields a deterministic strategy that maps each value to a single bid. We also show that our minimax strategy significantly outperforms the uniform-bid-shading strategies advanced by prior work. Our result allows us to precisely quantify, through minimax regret, the performance loss due to a lack of knowledge about competing bids. We leverage this to analyze the impact of the value distribution on the performance loss, and find that it decreases as the buyer's values become more dispersed.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding","url":"https://arxiv.org/abs/2502.09906","date":1739768400,"author":"","guid":1467,"unread":true,"content":"<article>arXiv:2502.09906v1 Announce Type: new \nAbstract: Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual insects since they are often trained on the general knowledge of vision-language data. Meanwhile, understanding insects is a fundamental problem in precision agriculture, helping to promote sustainable development in agriculture. Therefore, this paper proposes a novel multimodal conversational model, Insect-LLaVA, to promote visual understanding in insect-domain knowledge. In particular, we first introduce a new large-scale Multimodal Insect Dataset with Visual Insect Instruction Data that enables the capability of learning the multimodal foundation models. Our proposed dataset enables conversational models to comprehend the visual and semantic features of the insects. Second, we propose a new Insect-LLaVA model, a new general Large Language and Vision Assistant in Visual Insect Understanding. Then, to enhance the capability of learning insect features, we develop an Insect Foundation Model by introducing a new micro-feature self-supervised learning with a Patch-wise Relevant Attention mechanism to capture the subtle differences among insect images. We also present Description Consistency loss to improve micro-feature learning via text descriptions. The experimental results evaluated on our new Visual Insect Question Answering benchmarks illustrate the effective performance of our proposed approach in visual insect understanding and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks.</article>","contentLength":1722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards personalised assessment of abdominal aortic aneurysm structural integrity","url":"https://arxiv.org/abs/2502.09905","date":1739768400,"author":"","guid":1468,"unread":true,"content":"<article>arXiv:2502.09905v1 Announce Type: new \nAbstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition involving the permanent dilation of the aorta, often detected incidentally through imaging for some other condition. The standard clinical approach to managing AAA follows a one-size-fits-all model based on aneurysm size and growth rate, leading to underestimation or overestimation of rupture risk in individual patients. The widely studied stress-based rupture risk estimation using computational biomechanics requires wall strength information. However, non-invasive methods for local patient-specific wall strength measurement have not yet been developed. Recently, we introduced an image-based approach for patient-specific, in vivo, non-invasive AAA kinematic analysis using time-resolved 3D computed tomography angiography (4D-CTA) images to measure wall strain throughout the cardiac cycle. In the present study, we integrated wall tension computation and strain measurement to develop a novel measure of local structural integrity of AAA wall - Relative Structural Integrity Index (RSII), independent of material properties and thickness of the wall and conditions of blood pressure measurement. Our methods provide a visual map of AAA wall structural integrity for individual patients using only their medical images and blood pressure data. We applied our methods to twelve patients. Additionally, we compared our measure of structural integrity of aneurysmal and non-aneurysmal aortas. Our results show similar values of the wall structural integrity measure across the patients, indicating the reliability of our methods. In line with experimental observations reported in the literature, our analysis revealed that localized low stiffness areas are primarily found in the most dilated AAA regions. Our results clearly demonstrate that the AAA wall is stiffer than the non-aneurysmal aorta.</article>","contentLength":1916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Ann Arbor Architecture for Agent-Oriented Programming","url":"https://arxiv.org/abs/2502.09903","date":1739768400,"author":"","guid":1469,"unread":true,"content":"<article>arXiv:2502.09903v1 Announce Type: new \nAbstract: In this paper, we reexamine prompt engineering for large language models through the lens of automata theory. We argue that language models function as automata and, like all automata, should be programmed in the languages they accept, a unified collection of all natural and formal languages. Therefore, traditional software engineering practices--conditioned on the clear separation of programming languages and natural languages--must be rethought. We introduce the Ann Arbor Architecture, a conceptual framework for agent-oriented programming of language models, as a higher-level abstraction over raw token generation, and provide a new perspective on in-context learning. Based on this framework, we present the design of our agent platform Postline, and report on our initial experiments in agent training.</article>","contentLength":862,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thompson Sampling for Repeated Newsvendor","url":"https://arxiv.org/abs/2502.09900","date":1739768400,"author":"","guid":1470,"unread":true,"content":"<article>arXiv:2502.09900v1 Announce Type: new \nAbstract: In this paper, we investigate the performance of Thompson Sampling (TS) for online learning with censored feedback, focusing primarily on the classic repeated newsvendor model--a foundational framework in inventory management--and demonstrating how our techniques can be naturally extended to a broader class of problems. We model demand using a Weibull distribution and initialize TS with a Gamma prior to dynamically adjust order quantities. Our analysis establishes optimal (up to logarithmic factors) frequentist regret bounds for TS without imposing restrictive prior assumptions. More importantly, it yields novel and highly interpretable insights on how TS addresses the exploration-exploitation trade-off in the repeated newsvendor setting. Specifically, our results show that when past order quantities are sufficiently large to overcome censoring, TS accurately estimates the unknown demand parameters, leading to near-optimal ordering decisions. Conversely, when past orders are relatively small, TS automatically increases future order quantities to gather additional demand information. Extensive numerical simulations further demonstrate that TS outperforms more conservative and widely-used approaches such as online convex optimization, upper confidence bounds, and myopic Bayesian dynamic programming. This study also lays the foundation for exploring general online learning problems with censored feedback.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transtiff: A Stylus-shaped Interface for Rendering Perceived Stiffness of Virtual Objects via Stylus Stiffness Control","url":"https://arxiv.org/abs/2502.09899","date":1739768400,"author":"","guid":1471,"unread":true,"content":"<article>arXiv:2502.09899v1 Announce Type: new \nAbstract: The replication of object stiffness is essential for enhancing haptic feedback in virtual environments. However, existing research has overlooked how stylus stiffness influences the perception of virtual object stiffness during tool-mediated interactions. To address this, we conducted a psychophysical experiment demonstrating that changing stylus stiffness combined with visual stimuli altered users' perception of virtual object stiffness. Based on these insights, we developed Transtiff, a stylus-shaped interface capable of on-demand stiffness control using a McKibben artificial muscle mechanism. Unlike previous approaches, our method manipulates the perceived stiffness of virtual objects via the stylus by controlling the stiffness of the stylus without altering the properties of the real object being touched, creating the illusion of a hard object feeing soft. Our user study confirmed that Transtiff effectively simulates a range of material properties, such as sponge, plastic, and tennis balls, providing haptic rendering that is closely aligned with the perceived material characteristics. By addressing the challenge of delivering realistic haptic feedback through tool-based interactions, Transtiff represents a significant advancement in the haptic interface design for VR applications.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase retrieval","url":"https://arxiv.org/abs/2502.09898","date":1739768400,"author":"","guid":1472,"unread":true,"content":"<article>arXiv:2502.09898v1 Announce Type: new \nAbstract: The injectivity of ReLU layers in neural networks, the recovery of vectors from clipped or saturated measurements, and (real) phase retrieval in $\\mathbb{R}^n$ allow for a similar problem formulation and characterization using frame theory. In this paper, we revisit all three problems with a unified perspective and derive lower Lipschitz bounds for ReLU layers and clipping which are analogous to the previously known result for phase retrieval and are optimal up to a constant factor.</article>","contentLength":536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Intelligence in Spectroscopy: Advancing Chemistry from Prediction to Generation and Beyond","url":"https://arxiv.org/abs/2502.09897","date":1739768400,"author":"","guid":1473,"unread":true,"content":"<article>arXiv:2502.09897v1 Announce Type: new \nAbstract: The rapid advent of machine learning (ML) and artificial intelligence (AI) has catalyzed major transformations in chemistry, yet the application of these methods to spectroscopic and spectrometric data, referred to as Spectroscopy Machine Learning (SpectraML), remains relatively underexplored. Modern spectroscopic techniques (MS, NMR, IR, Raman, UV-Vis) generate an ever-growing volume of high-dimensional data, creating a pressing need for automated and intelligent analysis beyond traditional expert-based workflows. In this survey, we provide a unified review of SpectraML, systematically examining state-of-the-art approaches for both forward tasks (molecule-to-spectrum prediction) and inverse tasks (spectrum-to-molecule inference). We trace the historical evolution of ML in spectroscopy, from early pattern recognition to the latest foundation models capable of advanced reasoning, and offer a taxonomy of representative neural architectures, including graph-based and transformer-based methods. Addressing key challenges such as data quality, multimodal integration, and computational scalability, we highlight emerging directions such as synthetic data generation, large-scale pretraining, and few- or zero-shot learning. To foster reproducible research, we also release an open-source repository containing recent papers and their corresponding curated datasets (https://github.com/MINE-Lab-ND/SpectrumML_Survey_Papers). Our survey serves as a roadmap for researchers, guiding progress at the intersection of spectroscopy and AI.</article>","contentLength":1591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatIoT: Large Language Model-based Security Assistant for Internet of Things with Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2502.09896","date":1739768400,"author":"","guid":1474,"unread":true,"content":"<article>arXiv:2502.09896v1 Announce Type: new \nAbstract: Internet of Things (IoT) has gained widespread popularity, revolutionizing industries and daily life. However, it has also emerged as a prime target for attacks. Numerous efforts have been made to improve IoT security, and substantial IoT security and threat information, such as datasets and reports, have been developed. However, existing research often falls short in leveraging these insights to assist or guide users in harnessing IoT security practices in a clear and actionable way. In this paper, we propose ChatIoT, a large language model (LLM)-based IoT security assistant designed to disseminate IoT security and threat intelligence. By leveraging the versatile property of retrieval-augmented generation (RAG), ChatIoT successfully integrates the advanced language understanding and reasoning capabilities of LLM with fast-evolving IoT security information. Moreover, we develop an end-to-end data processing toolkit to handle heterogeneous datasets. This toolkit converts datasets of various formats into retrievable documents and optimizes chunking strategies for efficient retrieval. Additionally, we define a set of common use case specifications to guide the LLM in generating answers aligned with users' specific needs and expertise levels. Finally, we implement a prototype of ChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3, LLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can generate more reliable, relevant, and technical in-depth answers for most use cases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above metrics by over 10% on average, particularly in relevance and technicality, compared to using LLMs alone.</article>","contentLength":1762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2502.09891","date":1739768400,"author":"","guid":1475,"unread":true,"content":"<article>arXiv:2502.09891v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) has proven effective in integrating external knowledge into large language models (LLMs) for question-answer (QA) tasks. The state-of-the-art RAG approaches often use the graph data as the external data since they capture the rich semantic information and link relationships between entities. However, existing graph-based RAG approaches cannot accurately identify the relevant information from the graph and also consume large numbers of tokens in the online retrieval process. To address these issues, we introduce a novel graph-based RAG approach, called Attributed Community-based Hierarchical RAG (ArchRAG), by augmenting the question using attributed communities, and also introducing a novel LLM-based hierarchical clustering method. To retrieve the most relevant information from the graph for the question, we build a novel hierarchical index structure for the attributed communities and develop an effective online retrieval method. Experimental results demonstrate that ArchRAG outperforms existing methods in terms of both accuracy and token cost.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symmetry-Preserving Diffusion Models via Target Symmetrization","url":"https://arxiv.org/abs/2502.09890","date":1739768400,"author":"","guid":1476,"unread":true,"content":"<article>arXiv:2502.09890v1 Announce Type: new \nAbstract: Diffusion models are powerful tools for capturing complex distributions, but modeling data with inherent symmetries, such as molecular structures, remains challenging. Equivariant denoisers are commonly used to address this, but they introduce architectural complexity and optimization challenges, including noisy gradients and convergence issues. We propose a novel approach that enforces equivariance through a symmetrized loss function, which applies a time-dependent weighted averaging operation over group actions to the model's prediction target. This ensures equivariance without explicit architectural constraints and reduces gradient variance, leading to more stable and efficient optimization. Our method uses Monte Carlo sampling to estimate the average, incurring minimal computational overhead. We provide theoretical guarantees of equivariance for the minimizer of our loss function and demonstrate its effectiveness on synthetic datasets and the molecular conformation generation task using the GEOM-QM9 dataset. Experiments show improved sample quality compared to existing methods, highlighting the potential of our approach to enhance the scalability and practicality of equivariant diffusion models in generative tasks.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating and Improving Graph-based Explanation Methods for Multi-Agent Coordination","url":"https://arxiv.org/abs/2502.09889","date":1739768400,"author":"","guid":1477,"unread":true,"content":"<article>arXiv:2502.09889v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs), developed by the graph learning community, have been adopted and shown to be highly effective in multi-robot and multi-agent learning. Inspired by this successful cross-pollination, we investigate and characterize the suitability of existing GNN explanation methods for explaining multi-agent coordination. We find that these methods have the potential to identify the most-influential communication channels that impact the team's behavior. Informed by our initial analyses, we propose an attention entropy regularization term that renders GAT-based policies more amenable to existing graph-based explainers. Intuitively, minimizing attention entropy incentivizes agents to limit their attention to the most influential or impactful agents, thereby easing the challenge faced by the explainer. We theoretically ground this intuition by showing that minimizing attention entropy increases the disparity between the explainer-generated subgraph and its complement. Evaluations across three tasks and three team sizes i) provides insights into the effectiveness of existing explainers, and ii) demonstrates that our proposed regularization consistently improves explanation quality without sacrificing task performance.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Efficient Large Recommendation Model: Towards a Resource-Optimal Scaling Law","url":"https://arxiv.org/abs/2502.09888","date":1739768400,"author":"","guid":1478,"unread":true,"content":"<article>arXiv:2502.09888v1 Announce Type: new \nAbstract: The pursuit of scaling up recommendation models confronts intrinsic tensions between expanding model capacity and preserving computational tractability. While prior studies have explored scaling laws for recommendation systems, their resource-intensive paradigms -- often requiring tens of thousands of A100 GPU hours -- remain impractical for most industrial applications. This work addresses a critical gap: achieving sustainable model scaling under strict computational budgets. We propose Climber, a resource-efficient recommendation framework comprising two synergistic components: the ASTRO model architecture for algorithmic innovation and the TURBO acceleration framework for engineering optimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts two core innovations: (1) multi-scale sequence partitioning that reduces attention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks, enabling more efficient scaling with sequence length; (2) dynamic temperature modulation that adaptively adjusts attention scores for multimodal distributions arising from inherent multi-scenario and multi-behavior interactions. Complemented by TURBO (Two-stage Unified Ranking with Batched Output), a co-designed acceleration framework integrating gradient-aware feature compression and memory-efficient Key-Value caching, Climber achieves 5.15x throughput gains without performance degradation. Comprehensive offline experiments on multiple datasets validate that Climber exhibits a more ideal scaling curve. To our knowledge, this is the first publicly documented framework where controlled model scaling drives continuous online metric growth (12.19% overall lift) without prohibitive resource costs. Climber has been successfully deployed on Netease Cloud Music, one of China's largest music streaming platforms, serving tens of millions of users daily.</article>","contentLength":1927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos","url":"https://arxiv.org/abs/2502.09886","date":1739768400,"author":"","guid":1479,"unread":true,"content":"<article>arXiv:2502.09886v1 Announce Type: new \nAbstract: Simulation offers a promising approach for cheaply scaling training data for generalist policies. To scalably generate data from diverse and realistic tasks, existing algorithms either rely on large language models (LLMs) that may hallucinate tasks not interesting for robotics; or digital twins, which require careful real-to-sim alignment and are hard to scale. To address these challenges, we introduce Video2Policy, a novel framework that leverages internet RGB videos to reconstruct tasks based on everyday human behavior. Our approach comprises two phases: (1) task generation in simulation from videos; and (2) reinforcement learning utilizing in-context LLM-generated reward functions iteratively. We demonstrate the efficacy of Video2Policy by reconstructing over 100 videos from the Something-Something-v2 (SSv2) dataset, which depicts diverse and complex human behaviors on 9 different tasks. Our method can successfully train RL policies on such tasks, including complex and challenging tasks such as throwing. Finally, we show that the generated simulation data can be scaled up for training a general policy, and it can be transferred back to the real robot in a Real2Sim2Real way.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comprehensive Review of Neural Differential Equations for Time Series Analysis","url":"https://arxiv.org/abs/2502.09885","date":1739768400,"author":"","guid":1480,"unread":true,"content":"<article>arXiv:2502.09885v1 Announce Type: new \nAbstract: Time series modeling and analysis has become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.</article>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation","url":"https://arxiv.org/abs/2502.09884","date":1739768400,"author":"","guid":1481,"unread":true,"content":"<article>arXiv:2502.09884v1 Announce Type: new \nAbstract: We consider linear two-time-scale stochastic approximation algorithms driven by martingale noise. Recent applications in machine learning motivate the need to understand finite-time error rates, but conventional stochastic approximation analysis focus on either asymptotic convergence in distribution or finite-time bounds that are far from optimal. Prior work on asymptotic central limit theorems (CLTs) suggest that two-time-scale algorithms may be able to achieve $1/\\sqrt{n}$ error in expectation, with a constant given by the expected norm of the limiting Gaussian vector. However, the best known finite-time rates are much slower. We derive the first non-asymptotic central limit theorem with respect to the Wasserstein-1 distance for two-time-scale stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show that expected error achieved by Polyak-Ruppert averaging decays at rate $1/\\sqrt{n}$, which significantly improves on the rates of convergence in prior works.</article>","contentLength":1043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stretching Rubber, Not Budgets: Accurate Parking Utilization on a Shoestring","url":"https://arxiv.org/abs/2502.09877","date":1739768400,"author":"","guid":1482,"unread":true,"content":"<article>arXiv:2502.09877v1 Announce Type: new \nAbstract: Effective parking management is essential for ensuring accessibility, safety, and convenience in master-planned communities, particularly in active adult neighborhoods experiencing rapid growth. Accurately assessing parking utilization is a crucial first step in planning for future demand, but data collection methods can be costly and labor-intensive. This paper presents a low-cost yet highly accurate methodology for measuring parking utilization using road tubes connected to portable traffic counters from JAMAR Technologies, Inc. By integrating results from JAMAR's analysis tool with custom Python scripting, the methodology enables precise parking lot counts through parameter optimization and automated error correction. The system's efficiency allows for scalable deployment without significant manual observation, reducing both costs and disruptions to daily operations. Using Tellico Village as a case study, this research demonstrates that community planners can obtain actionable parking insights on a limited budget, empowering them to make informed decisions about capacity expansion, traffic flow improvements, and facility scheduling. The findings underscore the feasibility of leveraging cost-effective technology to optimize infrastructure planning and ensure long-term resident satisfaction as communities grow.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation","url":"https://arxiv.org/abs/2502.09874","date":1739768400,"author":"","guid":1483,"unread":true,"content":"<article>arXiv:2502.09874v1 Announce Type: new \nAbstract: Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised training.In this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear instance segmentation problem. In this framework, we construct a fourier guidance module to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the nuclear.Meanwhile, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module. This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear. We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full supervision.In addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively. As open science, all codes and pre-trained models are available at https://github.com/LQY404/FrGNet.</article>","contentLength":1504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal","url":"https://arxiv.org/abs/2502.09873","date":1739768400,"author":"","guid":1484,"unread":true,"content":"<article>arXiv:2502.09873v1 Announce Type: new \nAbstract: Diffusion models have demonstrated remarkable success in image restoration tasks. However, their multi-step denoising process introduces significant computational overhead, limiting their practical deployment. Furthermore, existing methods struggle to effectively remove severe JPEG artifact, especially in highly compressed images. To address these challenges, we propose CODiff, a compression-aware one-step diffusion model for JPEG artifact removal. The core of CODiff is the compression-aware visual embedder (CaVE), which extracts and leverages JPEG compression priors to guide the diffusion model. We propose a dual learning strategy that combines explicit and implicit learning. Specifically, explicit learning enforces a quality prediction objective to differentiate low-quality images with different compression levels. Implicit learning employs a reconstruction objective that enhances the model's generalization. This dual learning allows for a deeper and more comprehensive understanding of JPEG compression. Experimental results demonstrate that CODiff surpasses recent leading methods in both quantitative and visual quality metrics. The code and models will be released at https://github.com/jp-guo/CODiff.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Calibrate for Reliable Visual Fire Detection","url":"https://arxiv.org/abs/2502.09872","date":1739768400,"author":"","guid":1485,"unread":true,"content":"<article>arXiv:2502.09872v1 Announce Type: new \nAbstract: Fire is characterized by its sudden onset and destructive power, making early fire detection crucial for ensuring human safety and protecting property. With the advancement of deep learning, the application of computer vision in fire detection has significantly improved. However, deep learning models often exhibit a tendency toward overconfidence, and most existing works focus primarily on enhancing classification performance, with limited attention given to uncertainty modeling. To address this issue, we propose transforming the Expected Calibration Error (ECE), a metric for measuring uncertainty, into a differentiable ECE loss function. This loss is then combined with the cross-entropy loss to guide the training process of multi-class fire detection models. Additionally, to achieve a good balance between classification accuracy and reliable decision, we introduce a curriculum learning-based approach that dynamically adjusts the weight of the ECE loss during training. Extensive experiments are conducted on two widely used multi-class fire detection datasets, DFAN and EdgeFireSmoke, validating the effectiveness of our uncertainty modeling method.</article>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Taxonomy of Linguistic Expressions That Contribute To Anthropomorphism of Language Technologies","url":"https://arxiv.org/abs/2502.09870","date":1739768400,"author":"","guid":1486,"unread":true,"content":"<article>arXiv:2502.09870v1 Announce Type: new \nAbstract: Recent attention to anthropomorphism -- the attribution of human-like qualities to non-human objects or entities -- of language technologies like LLMs has sparked renewed discussions about potential negative impacts of anthropomorphism. To productively discuss the impacts of this anthropomorphism and in what contexts it is appropriate, we need a shared vocabulary for the vast variety of ways that language can be anthropomorphic. In this work, we draw on existing literature and analyze empirical cases of user interactions with language technologies to develop a taxonomy of textual expressions that can contribute to anthropomorphism. We highlight challenges and tensions involved in understanding linguistic anthropomorphism, such as how all language is fundamentally human and how efforts to characterize and shift perceptions of humanness in machines can also dehumanize certain humans. We discuss ways that our taxonomy supports more precise and effective discussions of and decisions about anthropomorphism of language technologies.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Explicit and Implicit: How Users Provide Feedback to Shape Personalized Recommendation Content","url":"https://arxiv.org/abs/2502.09869","date":1739768400,"author":"","guid":1487,"unread":true,"content":"<article>arXiv:2502.09869v1 Announce Type: new \nAbstract: As personalized recommendation algorithms become integral to social media platforms, users are increasingly aware of their ability to influence recommendation content. However, limited research has explored how users provide feedback through their behaviors and platform mechanisms to shape the recommendation content. We conducted semi-structured interviews with 34 active users of algorithmic-driven social media platforms (e.g., Xiaohongshu, Douyin). In addition to explicit and implicit feedback, this study introduced intentional implicit feedback, highlighting the actions users intentionally took to refine recommendation content through perceived feedback mechanisms. Additionally, choices of feedback behaviors were found to align with specific purposes. Explicit feedback was primarily used for feed customization, while unintentional implicit feedback was more linked to content consumption. Intentional implicit feedback was employed for multiple purposes, particularly in increasing content diversity and improving recommendation relevance. This work underscores the user intention dimension in the explicit-implicit feedback dichotomy and offers insights for designing personalized recommendation feedback that better responds to users' needs.</article>","contentLength":1306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DesignWeaver: Dimensional Scaffolding for Text-to-Image Product Design","url":"https://arxiv.org/abs/2502.09867","date":1739768400,"author":"","guid":1488,"unread":true,"content":"<article>arXiv:2502.09867v1 Announce Type: new \nAbstract: Generative AI has enabled novice designers to quickly create professional-looking visual representations for product concepts. However, novices have limited domain knowledge that could constrain their ability to write prompts that effectively explore a product design space. To understand how experts explore and communicate about design spaces, we conducted a formative study with 12 experienced product designers and found that experts -- and their less-versed clients -- often use visual references to guide co-design discussions rather than written descriptions. These insights inspired DesignWeaver, an interface that helps novices generate prompts for a text-to-image model by surfacing key product design dimensions from generated images into a palette for quick selection. In a study with 52 novices, DesignWeaver enabled participants to craft longer prompts with more domain-specific vocabularies, resulting in more diverse, innovative product designs. However, the nuanced prompts heightened participants' expectations beyond what current text-to-image models could deliver. We discuss implications for AI-based product design support tools.</article>","contentLength":1200,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Users Who are Blind or Low Vision Play Mobile Games: Perceptions, Challenges, and Strategies","url":"https://arxiv.org/abs/2502.09866","date":1739768400,"author":"","guid":1489,"unread":true,"content":"<article>arXiv:2502.09866v1 Announce Type: new \nAbstract: As blind and low-vision (BLV) players engage more deeply with games, accessibility features have become essential. While some research has explored tools and strategies to enhance game accessibility, the specific experiences of these players with mobile games remain underexamined. This study addresses this gap by investigating how BLV users experience mobile games with varying accessibility levels. Through interviews with 32 experienced BLV mobile players, we explore their perceptions, challenges, and strategies for engaging with mobile games. Our findings reveal that BLV players turn to mobile games to alleviate boredom, achieve a sense of accomplishment, and build social connections, but face barriers depending on the game's accessibility level. We also compare mobile games to other forms of gaming, highlighting the relative advantages of mobile games, such as the inherent accessibility of smartphones. This study contributes to understanding BLV mobile gaming experiences and provides insights for enhancing accessible mobile game design.</article>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"U Can Touch This! Microarchitectural Timing Attacks via Machine Clears","url":"https://arxiv.org/abs/2502.09864","date":1739768400,"author":"","guid":1490,"unread":true,"content":"<article>arXiv:2502.09864v1 Announce Type: new \nAbstract: Microarchitectural timing attacks exploit subtle timing variations caused by hardware behaviors to leak sensitive information. In this paper, we introduce MCHammer, a novel side-channel technique that leverages machine clears induced by self-modifying code detection mechanisms. Unlike most traditional techniques, MCHammer does not require memory access or waiting periods, making it highly efficient. We compare MCHammer to the classical Flush+Reload technique, improving in terms of trace granularity, providing a powerful side-channel attack vector. Using MCHammer, we successfully recover keys from a deployed implementation of a cryptographic tool. Our findings highlight the practical implications of MCHammer and its potential impact on real-world systems.</article>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence of Analogical Reasoning","url":"https://arxiv.org/abs/2502.09863","date":1739768400,"author":"","guid":1491,"unread":true,"content":"<article>arXiv:2502.09863v1 Announce Type: new \nAbstract: The remarkable success of large language models relies on their ability to implicitly learn structured latent representations from the pretraining corpus. As a simpler surrogate for representation learning in language modeling, we study a class of solvable contrastive self-supervised algorithms which we term quadratic word embedding models. These models resemble the word2vec algorithm and perform similarly on downstream tasks. Our main contributions are analytical solutions for both the training dynamics (under certain hyperparameter choices) and the final word embeddings, given in terms of only the corpus statistics. Our solutions reveal that these models learn orthogonal linear subspaces one at a time, each one incrementing the effective rank of the embeddings until model capacity is saturated. Training on WikiText, we find that the top subspaces represent interpretable concepts. Finally, we use our dynamical theory to predict how and when models acquire the ability to complete analogies.</article>","contentLength":1054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Scoresheet for Explainable AI","url":"https://arxiv.org/abs/2502.09861","date":1739768400,"author":"","guid":1492,"unread":true,"content":"<article>arXiv:2502.09861v1 Announce Type: new \nAbstract: Explainability is important for the transparency of autonomous and intelligent systems and for helping to support the development of appropriate levels of trust. There has been considerable work on developing approaches for explaining systems and there are standards that specify requirements for transparency. However, there is a gap: the standards are too high-level and do not adequately specify requirements for explainability. This paper develops a scoresheet that can be used to specify explainability requirements or to assess the explainability aspects provided for particular applications. The scoresheet is developed by considering the requirements of a range of stakeholders and is applicable to Multiagent Systems as well as other AI technologies. We also provide guidance for how to use the scoresheet and illustrate its generality and usefulness by applying it to a range of applications.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Hypothesis Validation with Agentic Sequential Falsifications","url":"https://arxiv.org/abs/2502.09858","date":1739768400,"author":"","guid":1493,"unread":true,"content":"<article>arXiv:2502.09858v1 Announce Type: new \nAbstract: Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Multitask Learning in Small Language Models Through Upside-Down Reinforcement Learning","url":"https://arxiv.org/abs/2502.09854","date":1739768400,"author":"","guid":1494,"unread":true,"content":"<article>arXiv:2502.09854v1 Announce Type: new \nAbstract: In this work, we demonstrate that small language models (SLMs), specifically a 100M parameter GPT-2 model, can achieve competitive performance in multitask prompt generation tasks while requiring only a fraction of the computational resources needed by large language models (LLMs). Through a novel combination of upside-down reinforcement learning and synthetic data distillation from a powerful LLM, Llama-3, we train an SLM that achieves relevance scores within 5% of state-of-the-art models, including Llama-3, Qwen2, and Mistral, despite being up to 80 times smaller, making it highly suitable for resource-constrained and real-time applications. This study highlights the potential of SLMs as efficient multitask learners in multimodal settings, providing a promising alternative to LLMs for scalable, low-latency deployments.</article>","contentLength":881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elastic Representation: Mitigating Spurious Correlations for Group Robustness","url":"https://arxiv.org/abs/2502.09850","date":1739768400,"author":"","guid":1495,"unread":true,"content":"<article>arXiv:2502.09850v1 Announce Type: new \nAbstract: Deep learning models can suffer from severe performance degradation when relying on spurious correlations between input features and labels, making the models perform well on training data but have poor prediction accuracy for minority groups. This problem arises especially when training data are limited or imbalanced. While most prior work focuses on learning invariant features (with consistent correlations to y), it overlooks the potential harm of spurious correlations between features. We hereby propose Elastic Representation (ElRep) to learn features by imposing Nuclear- and Frobenius-norm penalties on the representation from the last layer of a neural network. Similar to the elastic net, ElRep enjoys the benefits of learning important features without losing feature diversity. The proposed method is simple yet effective. It can be integrated into many deep learning approaches to mitigate spurious correlations and improve group robustness. Moreover, we theoretically show that ElRep has minimum negative impacts on in-distribution predictions. This is a remarkable advantage over approaches that prioritize minority groups at the cost of overall performance.</article>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems","url":"https://arxiv.org/abs/2502.09849","date":1739768400,"author":"","guid":1496,"unread":true,"content":"<article>arXiv:2502.09849v1 Announce Type: new \nAbstract: Explainable AI (XAI) has become a crucial component of Clinical Decision Support Systems (CDSS) to enhance transparency, trust, and clinical adoption. However, while many XAI methods have been proposed, their effectiveness in real-world medical settings remains underexplored. This paper provides a survey of human-centered evaluations of Explainable AI methods in Clinical Decision Support Systems. By categorizing existing works based on XAI methodologies, evaluation frameworks, and clinical adoption challenges, we offer a structured understanding of the landscape. Our findings reveal key challenges in the integration of XAI into healthcare workflows and propose a structured framework to align the evaluation methods of XAI with the clinical needs of stakeholders.</article>","contentLength":820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Event-Triggered Integrated Communication and Control with Graph Information Bottleneck Optimization","url":"https://arxiv.org/abs/2502.09846","date":1739768400,"author":"","guid":1497,"unread":true,"content":"<article>arXiv:2502.09846v1 Announce Type: new \nAbstract: Integrated communication and control serves as a critical ingredient in Multi-Agent Reinforcement Learning. However, partial observability limitations will impair collaboration effectiveness, and a potential solution is to establish consensus through well-calibrated latent variables obtained from neighboring agents. Nevertheless, the rigid transmission of less informative content can still result in redundant information exchanges. Therefore, we propose a Consensus-Driven Event-Based Graph Information Bottleneck (CDE-GIB) method, which integrates the communication graph and information flow through a GIB regularizer to extract more concise message representations while avoiding the high computational complexity of inner-loop operations. To further minimize the communication volume required for establishing consensus during interactions, we also develop a variable-threshold event-triggering mechanism. By simultaneously considering historical data and current observations, this mechanism capably evaluates the importance of information to determine whether an event should be triggered. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art methods in terms of both efficiency and adaptability.</article>","contentLength":1298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solving Empirical Bayes via Transformers","url":"https://arxiv.org/abs/2502.09844","date":1739768400,"author":"","guid":1498,"unread":true,"content":"<article>arXiv:2502.09844v1 Announce Type: new \nAbstract: This work applies modern AI tools (transformers) to solving one of the oldest statistical problems: Poisson means under empirical Bayes (Poisson-EB) setting. In Poisson-EB a high-dimensional mean vector $\\theta$ (with iid coordinates sampled from an unknown prior $\\pi$) is estimated on the basis of $X=\\mathrm{Poisson}(\\theta)$. A transformer model is pre-trained on a set of synthetically generated pairs $(X,\\theta)$ and learns to do in-context learning (ICL) by adapting to unknown $\\pi$. Theoretically, we show that a sufficiently wide transformer can achieve vanishing regret with respect to an oracle estimator who knows $\\pi$ as dimension grows to infinity. Practically, we discover that already very small models (100k parameters) are able to outperform the best classical algorithm (non-parametric maximum likelihood, or NPMLE) both in runtime and validation loss, which we compute on out-of-distribution synthetic data as well as real-world datasets (NHL hockey, MLB baseball, BookCorpusOpen). Finally, by using linear probes, we confirm that the transformer's EB estimator appears to internally work differently from either NPMLE or Robbins' estimators.</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MuDoC: An Interactive Multimodal Document-grounded Conversational AI System","url":"https://arxiv.org/abs/2502.09843","date":1739768400,"author":"","guid":1499,"unread":true,"content":"<article>arXiv:2502.09843v1 Announce Type: new \nAbstract: Multimodal AI is an important step towards building effective tools to leverage multiple modalities in human-AI communication. Building a multimodal document-grounded AI system to interact with long documents remains a challenge. Our work aims to fill the research gap of directly leveraging grounded visuals from documents alongside textual content in documents for response generation. We present an interactive conversational AI agent 'MuDoC' based on GPT-4o to generate document-grounded responses with interleaved text and figures. MuDoC's intelligent textbook interface promotes trustworthiness and enables verification of system responses by allowing instant navigation to source text and figures in the documents. We also discuss qualitative observations based on MuDoC responses highlighting its strengths and limitations.</article>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient, Accurate, and Robust Penalty-Projection Algorithm for Parameterized Stochastic Navier-Stokes Flow Problems","url":"https://arxiv.org/abs/2502.09842","date":1739768400,"author":"","guid":1500,"unread":true,"content":"<article>arXiv:2502.09842v1 Announce Type: new \nAbstract: This paper presents and analyzes a fast, robust, efficient, and optimally accurate fully discrete splitting algorithm for the Uncertainty Quantification (UQ) of parameterized Stochastic Navier-Stokes Equations (SNSEs) flow problems those occur in the convection-dominated regimes. The time-stepping algorithm is an implicit backward-Euler linearized method, grad-div and Ensemble Eddy Viscosity (EEV) regularized, and split using discrete Hodge decomposition. Additionally, the scheme's sub-problems are all designed to have different Right-Hand-Side (RHS) vectors but the same system matrix for all realizations at each time-step. The stability of the algorithm is rigorously proven, and it has been shown that appropriately large grad-div stabilization parameters vanish the splitting error. The proposed UQ algorithm is then combined with the Stochastic Collocation Methods (SCMs). Several numerical experiments are given to verify this superior scheme's predicted convergence rates and performance on benchmark problems for high expected Reynolds numbers ($Re$).</article>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation","url":"https://arxiv.org/abs/2502.09838","date":1739768400,"author":"","guid":1501,"unread":true,"content":"<article>arXiv:2502.09838v1 Announce Type: new \nAbstract: We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at https://github.com/DCDmllm/HealthGPT.</article>","contentLength":925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SoK: State of the time: On Trustworthiness of Digital Clocks","url":"https://arxiv.org/abs/2502.09837","date":1739768400,"author":"","guid":1502,"unread":true,"content":"<article>arXiv:2502.09837v1 Announce Type: new \nAbstract: Despite the critical role of timing infrastructure in enabling essential services, from public key infrastructure and smart grids to autonomous navigation and high-frequency trading, modern timing stacks remain highly vulnerable to malicious attacks. These threats emerge due to several reasons, including inadequate security mechanisms, the timing architectures unique vulnerability to delays, and implementation issues. In this paper, we aim to obtain a holistic understanding of the issues that make the timing stacks vulnerable to adversarial manipulations, what the challenges are in securing them, and what solutions can be borrowed from the research community to address them. To this end, we perform a systematic analysis of the security vulnerabilities of the timing stack. In doing so, we discover new attack surfaces, i.e., physical timing components and on-device timekeeping, which are often overlooked by existing research that predominantly studies the security of time synchronization protocols. We also show that the emerging trusted timing architectures are flawed and risk compromising wider system security, and propose an alternative design using hardware-software co-design.</article>","contentLength":1245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal $k$-Secretary with Logarithmic Memory","url":"https://arxiv.org/abs/2502.09834","date":1739768400,"author":"","guid":1503,"unread":true,"content":"<article>arXiv:2502.09834v1 Announce Type: new \nAbstract: We study memory-bounded algorithms for the $k$-secretary problem. The algorithm of Kleinberg (2005) achieves an optimal competitive ratio of $1 - O(1/\\sqrt{k})$, yet a straightforward implementation requires $\\Omega(k)$ memory.\n  Our main result is a $k$-secretary algorithm that matches the optimal competitive ratio using $O(\\log k)$ words of memory. We prove this result by establishing a general reduction from $k$-secretary to (random-order) quantile estimation, the problem of finding the $k$-th largest element in a stream. We show that a quantile estimation algorithm with an $O(k^{\\alpha})$ expected error (in terms of the rank) gives a $(1 - O(1/k^{1-\\alpha}))$-competitive $k$-secretary algorithm with $O(1)$ extra words.\n  We then introduce a new quantile estimation algorithm that achieves an $O(\\sqrt{k})$ expected error bound using $O(\\log k)$ memory. Of independent interest, we give a different algorithm that uses $O(\\sqrt{k})$ words and finds the $k$-th largest element exactly with high probability, generalizing a result of Munro and Paterson (1980).</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized Entropy-Based Ransomware Detection Using Autonomous Feature Resonance","url":"https://arxiv.org/abs/2502.09833","date":1739768400,"author":"","guid":1504,"unread":true,"content":"<article>arXiv:2502.09833v1 Announce Type: new \nAbstract: The increasing sophistication of cyber threats has necessitated the development of advanced detection mechanisms capable of identifying malicious activities with high precision and efficiency. A novel approach, termed Autonomous Feature Resonance, is introduced to address the limitations of traditional ransomware detection methods through the analysis of entropy-based feature interactions within system processes. The proposed method achieves an overall detection accuracy of 97.3\\%, with false positive and false negative rates of 1.8\\% and 2.1\\%, respectively, outperforming existing techniques such as signature-based detection and behavioral analysis. Its decentralized architecture enables local processing of data, reducing latency and improving scalability, while a self-learning mechanism ensures continuous adaptation to emerging threats. Experimental results demonstrate consistent performance across diverse ransomware families, including LockBit 3.0, BlackCat, and Royal, with low detection latency and efficient resource utilization. The method's reliance on entropy as a distinguishing feature provides robustness against obfuscation techniques, making it suitable for real-time deployment in high-throughput environments. These findings highlight the potential of entropy-based approaches to enhance cybersecurity frameworks, offering a scalable and adaptive solution for modern ransomware detection challenges.</article>","contentLength":1478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Fair Policies for Infectious Diseases Mitigation using Path Integral Control","url":"https://arxiv.org/abs/2502.09831","date":1739768400,"author":"","guid":1505,"unread":true,"content":"<article>arXiv:2502.09831v1 Announce Type: new \nAbstract: Infectious diseases pose major public health challenges to society, highlighting the importance of designing effective policies to reduce economic loss and mortality. In this paper, we propose a framework for sequential decision-making under uncertainty to design fairness-aware disease mitigation policies that incorporate various measures of unfairness. Specifically, our approach learns equitable vaccination and lockdown strategies based on a stochastic multi-group SIR model. To address the challenges of solving the resulting sequential decision-making problem, we adopt the path integral control algorithm as an efficient solution scheme. Through a case study, we demonstrate that our approach effectively improves fairness compared to conventional methods and provides valuable insights for policymakers.</article>","contentLength":861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection","url":"https://arxiv.org/abs/2502.09829","date":1739768400,"author":"","guid":1506,"unread":true,"content":"<article>arXiv:2502.09829v1 Announce Type: new \nAbstract: Evaluating learned robot control policies to determine their physical task-level capabilities costs experimenter time and effort. The growing number of policies and tasks exacerbates this issue. It is impractical to test every policy on every task multiple times; each trial requires a manual environment reset, and each task change involves re-arranging objects or even changing robots. Naively selecting a random subset of tasks and policies to evaluate is a high-cost solution with unreliable, incomplete results. In this work, we formulate robot evaluation as an active testing problem. We propose to model the distribution of robot performance across all tasks and policies as we sequentially execute experiments. Tasks often share similarities that can reveal potential relationships in policy behavior, and we show that natural language is a useful prior in modeling these relationships between tasks. We then leverage this formulation to reduce the experimenter effort by using a cost-aware expected information gain heuristic to efficiently select informative trials. Our framework accommodates both continuous and discrete performance outcomes. We conduct experiments on existing evaluation data from real robots and simulations. By prioritizing informative trials, our framework reduces the cost of calculating evaluation metrics for robot policies across many tasks.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data and Decision Traceability for the Welder's Arc","url":"https://arxiv.org/abs/2502.09827","date":1739768400,"author":"","guid":1507,"unread":true,"content":"<article>arXiv:2502.09827v1 Announce Type: new \nAbstract: Space Protocol is applying the principles derived from MITRE and NIST's Supply Chain Traceability: Manufacturing Meta-Framework (NIST IR 8536) to a complex multi party system to achieve introspection, auditing, and replay of data and decisions that ultimately lead to a end decision. The core goal of decision traceability is to ensure transparency, accountability, and integrity within the WA system. This is accomplished by providing a clear, auditable path from the system's inputs all the way to the final decision. This traceability enables the system to track the various algorithms and data flows that have influenced a particular outcome.</article>","contentLength":695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safe Reinforcement Learning-based Control for Hydrogen Diesel Dual-Fuel Engines","url":"https://arxiv.org/abs/2502.09826","date":1739768400,"author":"","guid":1508,"unread":true,"content":"<article>arXiv:2502.09826v1 Announce Type: new \nAbstract: The urgent energy transition requirements towards a sustainable future stretch across various industries and are a significant challenge facing humanity. Hydrogen promises a clean, carbon-free future, with the opportunity to integrate with existing solutions in the transportation sector. However, adding hydrogen to existing technologies such as diesel engines requires additional modeling effort. Reinforcement Learning (RL) enables interactive data-driven learning that eliminates the need for mathematical modeling. The algorithms, however, may not be real-time capable and need large amounts of data to work in practice. This paper presents a novel approach which uses offline model learning with RL to demonstrate safe control of a 4.5 L Hydrogen Diesel Dual-Fuel (H2DF) engine. The controllers are demonstrated to be constraint compliant and can leverage a novel state-augmentation approach for sample-efficient learning. The offline policy is subsequently experimentally validated on the real engine where the control algorithm is executed on a Raspberry Pi controller and requires 6 times less computation time compared to online Model Predictive Control (MPC) optimization.</article>","contentLength":1232,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PUGS: Perceptual Uncertainty for Grasp Selection in Underwater Environments","url":"https://arxiv.org/abs/2502.09824","date":1739768400,"author":"","guid":1509,"unread":true,"content":"<article>arXiv:2502.09824v1 Announce Type: new \nAbstract: When navigating and interacting in challenging environments where sensory information is imperfect and incomplete, robots must make decisions that account for these shortcomings. We propose a novel method for quantifying and representing such perceptual uncertainty in 3D reconstruction through occupancy uncertainty estimation. We develop a framework to incorporate it into grasp selection for autonomous manipulation in underwater environments. Instead of treating each measurement equally when deciding which location to grasp from, we present a framework that propagates uncertainty inherent in the multi-view reconstruction process into the grasp selection. We evaluate our method with both simulated and the real world data, showing that by accounting for uncertainty, the grasp selection becomes robust against partial and noisy measurements. Code will be made available at https://onurbagoren.github.io/PUGS/</article>","contentLength":965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compression Properties for large Toeplitz-like matrices","url":"https://arxiv.org/abs/2502.09823","date":1739768400,"author":"","guid":1510,"unread":true,"content":"<article>arXiv:2502.09823v1 Announce Type: new \nAbstract: Toeplitz matrices are abundant in computational mathematics, and there is a rich literature on the development of fast and superfast algorithms for solving linear systems involving such matrices. Any Toeplitz matrix can be transformed into a matrix with off-diagonal blocks that are of low numerical rank.Surprisingly little is known about the compressibility of these matrices in a theoretically rigorous sense, even though this compressibility is relied upon in practice in a number of superfast Toeplitz solvers. In this paper, we show that the compression properties of these matrices can be thoroughly explained using their displacement structure. We provide explicit bounds on the numerical ranks of important submatrices that arise when applying HSS, HODLR and other approximations with hierarchical low-rank structure to transformed Toeplitz and Toeplitz-like matrices. Our results lead to very efficient displacement-based compression strategies that can be used to formulate adaptive superfast rank-structured solvers.</article>","contentLength":1077,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ATM-Net: Adaptive Termination and Multi-Precision Neural Networks for Energy-Harvested Edge Intelligence","url":"https://arxiv.org/abs/2502.09822","date":1739768400,"author":"","guid":1511,"unread":true,"content":"<article>arXiv:2502.09822v1 Announce Type: new \nAbstract: ATM-Net is a novel neural network architecture tailored for energy-harvested IoT devices, integrating adaptive termination points with multi-precision computing. It dynamically adjusts computational precision (32/8/4-bit) and network depth based on energy availability via early exit points. An energy-aware task scheduler optimizes the energy-accuracy trade-off. Experiments on CIFAR-10, PlantVillage, and TissueMNIST show ATM-Net achieves up to 96.93% accuracy while reducing power consumption by 87.5% with Q4 quantization compared to 32-bit operations. The power-delay product improves from 13.6J to 0.141J for DenseNet-121 and from 10.3J to 0.106J for ResNet-18, demonstrating its suitability for energy-harvesting systems.</article>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Solver-Aided Hierarchical Language for LLM-Driven CAD Design","url":"https://arxiv.org/abs/2502.09819","date":1739768400,"author":"","guid":1512,"unread":true,"content":"<article>arXiv:2502.09819v1 Announce Type: new \nAbstract: Large language models (LLMs) have been enormously successful in solving a wide variety of structured and unstructured generative tasks, but they struggle to generate procedural geometry in Computer Aided Design (CAD). These difficulties arise from an inability to do spatial reasoning and the necessity to guide a model through complex, long range planning to generate complex geometry. We enable generative CAD Design with LLMs through the introduction of a solver-aided, hierarchical domain specific language (DSL) called AIDL, which offloads the spatial reasoning requirements to a geometric constraint solver. Additionally, we show that in the few-shot regime, AIDL outperforms even a language with in-training data (OpenSCAD), both in terms of generating visual results closer to the prompt and creating objects that are easier to post-process and reason about.</article>","contentLength":915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the robustness of multimodal language model towards distractions","url":"https://arxiv.org/abs/2502.09818","date":1739768400,"author":"","guid":1513,"unread":true,"content":"<article>arXiv:2502.09818v1 Announce Type: new \nAbstract: Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt variations remains an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could have noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the ScienceQA dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts to evaluate the reasoning capacity of VLMs amid these distractions. Our findings reveal that most-of-the-art VLMs, including GPT-4, are vulnerable to various types of distractions, experiencing noticeable degradation in reasoning capabilities when confronted with distractions. Notably, models such as InternVL2 demonstrate a higher degree of robustness to these distractions. We also found that models exhibit greater sensitivity to textual distractions than visual ones. Additionally, we explored various mitigation strategies, such as prompt engineering, to counteract the impact of distractions. While these strategies improved solution accuracy, our analysis shows that there remain significant opportunities for improvement.</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vector Linear Secure Aggregation","url":"https://arxiv.org/abs/2502.09817","date":1739768400,"author":"","guid":1514,"unread":true,"content":"<article>arXiv:2502.09817v1 Announce Type: new \nAbstract: The secure summation problem, where $K$ users wish to compute the sum of their inputs at a server while revealing nothing about all $K$ inputs beyond the desired sum, is generalized in two aspects - first, the desired function is an arbitrary linear function (multiple linear combinations) of the $K$ inputs instead of just the sum; second, rather than protecting all $K$ inputs, we wish to guarantee that no information is leaked about an arbitrary linear function of the $K$ inputs. For this vector linear generalization of the secure summation problem, we characterize the optimal randomness cost, i.e., to compute one instance of the desired vector linear function, the minimum number of the random key variables held by the users is equal to the dimension of the vector space that is in the span of the vectors formed by the coefficients of the linear function to protect but not in the span of the vectors formed by the coefficients of the linear function to compute.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence","url":"https://arxiv.org/abs/2502.09815","date":1739768400,"author":"","guid":1515,"unread":true,"content":"<article>arXiv:2502.09815v1 Announce Type: new \nAbstract: Representation learning plays a central role in structuring internal embeddings to capture the statistical properties of language, influencing the coherence and contextual consistency of generated text. Statistical Coherence Alignment is introduced as a method to enforce structured token representations through tensor field convergence, guiding embeddings to reflect statistical dependencies inherent in linguistic data. A mathematical framework is established to quantify coherence alignment, integrating a loss function that optimizes representational consistency across training iterations. Empirical evaluations demonstrate that applying coherence constraints improves perplexity, enhances classification accuracy, and refines rare word embeddings, contributing to a more stable representation space. Comparative analyses with baseline models reveal that the proposed method fosters a more interpretable internal structure, ensuring that embeddings retain contextual dependencies while mitigating representation collapse. The impact on coherence score distributions suggests that the alignment mechanism strengthens semantic integrity across diverse linguistic constructs, leading to a more balanced organization of learned embeddings. Computational assessments indicate that while the method introduces additional memory and training costs, the structured optimization process justifies the trade-offs in applications requiring heightened contextual fidelity. Experimental results validate the effectiveness of coherence alignment in optimizing token representations, providing insights into how statistical dependencies can be leveraged to improve language model training.</article>","contentLength":1729,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"INJONGO: A Multicultural Intent Detection and Slot-filling Dataset for 16 African Languages","url":"https://arxiv.org/abs/2502.09814","date":1739768400,"author":"","guid":1516,"unread":true,"content":"<article>arXiv:2502.09814v1 Announce Type: new \nAbstract: Slot-filling and intent detection are well-established tasks in Conversational AI. However, current large-scale benchmarks for these tasks often exclude evaluations of low-resource languages and rely on translations from English benchmarks, thereby predominantly reflecting Western-centric concepts. In this paper, we introduce Injongo -- a multicultural, open-source benchmark dataset for 16 African languages with utterances generated by native speakers across diverse domains, including banking, travel, home, and dining. Through extensive experiments, we benchmark the fine-tuning multilingual transformer models and the prompting large language models (LLMs), and show the advantage of leveraging African-cultural utterances over Western-centric utterances for improving cross-lingual transfer from the English language. Experimental results reveal that current LLMs struggle with the slot-filling task, with GPT-4o achieving an average performance of 26 F1-score. In contrast, intent detection performance is notably better, with an average accuracy of 70.6%, though it still falls behind the fine-tuning baselines. Compared to the English language, GPT-4o and fine-tuning baselines perform similarly on intent detection, achieving an accuracy of approximately 81%. Our findings suggest that the performance of LLMs is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance.</article>","contentLength":1500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Suture Thread Modeling Using Control Barrier Functions for Autonomous Surgery","url":"https://arxiv.org/abs/2502.09813","date":1739768400,"author":"","guid":1517,"unread":true,"content":"<article>arXiv:2502.09813v1 Announce Type: new \nAbstract: Automating surgical systems enhances precision and safety while reducing human involvement in high-risk environments. A major challenge in automating surgical procedures like suturing is accurately modeling the suture thread, a highly flexible and compliant component. Existing models either lack the accuracy needed for safety critical procedures or are too computationally intensive for real time execution. In this work, we introduce a novel approach for modeling suture thread dynamics using control barrier functions (CBFs), achieving both realism and computational efficiency. Thread like behavior, collision avoidance, stiffness, and damping are all modeled within a unified CBF and control Lyapunov function (CLF) framework. Our approach eliminates the need to calculate complex forces or solve differential equations, significantly reducing computational overhead while maintaining a realistic model suitable for both automation and virtual reality surgical training systems. The framework also allows visual cues to be provided based on the thread's interaction with the environment, enhancing user experience when performing suture or ligation tasks. The proposed model is tested on the MagnetoSuture system, a minimally invasive robotic surgical platform that uses magnetic fields to manipulate suture needles, offering a less invasive solution for surgical procedures.</article>","contentLength":1430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Face Deepfakes - A Comprehensive Review","url":"https://arxiv.org/abs/2502.09812","date":1739768400,"author":"","guid":1518,"unread":true,"content":"<article>arXiv:2502.09812v1 Announce Type: new \nAbstract: In recent years, remarkable advancements in deep- fake generation technology have led to unprecedented leaps in its realism and capabilities. Despite these advances, we observe a notable lack of structured and deep analysis deepfake technology. The principal aim of this survey is to contribute a thorough theoretical analysis of state-of-the-art face deepfake generation and detection methods. Furthermore, we provide a coherent and systematic evaluation of the implications of deepfakes on face biometric recognition approaches. In addition, we outline key applications of face deepfake technology, elucidating both positive and negative applications of the technology, provide a detailed discussion regarding the gaps in existing research, and propose key research directions for further investigation.</article>","contentLength":854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inclusive Avatar Guidelines for People with Disabilities: Supporting Disability Representation in Social Virtual Reality","url":"https://arxiv.org/abs/2502.09811","date":1739768400,"author":"","guid":1519,"unread":true,"content":"<article>arXiv:2502.09811v1 Announce Type: new \nAbstract: Avatar is a critical medium for identity representation in social virtual reality (VR). However, options for disability expression are highly limited on current avatar interfaces. Improperly designed disability features may even perpetuate misconceptions about people with disabilities (PWD). As more PWD use social VR, there is an emerging need for comprehensive design standards that guide developers and designers to create inclusive avatars. Our work aim to advance the avatar design practices by delivering a set of centralized, comprehensive, and validated design guidelines that are easy to adopt, disseminate, and update. Through a systematic literature review and interview with 60 participants with various disabilities, we derived 20 initial design guidelines that cover diverse disability expression methods through five aspects, including avatar appearance, body dynamics, assistive technology design, peripherals around avatars, and customization control. We further evaluated the guidelines via a heuristic evaluation study with 10 VR practitioners, validating the guideline coverage, applicability, and actionability. Our evaluation resulted in a final set of 17 design guidelines with recommendation levels.</article>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration","url":"https://arxiv.org/abs/2502.09809","date":1739768400,"author":"","guid":1520,"unread":true,"content":"<article>arXiv:2502.09809v1 Announce Type: new \nAbstract: The integration of tool use into large language models (LLMs) enables agentic systems with real-world impact. In the meantime, unlike standalone LLMs, compromised agents can execute malicious workflows with more consequential impact, signified by their tool-use capability. We propose AgentGuard, a framework to autonomously discover and validate unsafe tool-use workflows, followed by generating safety constraints to confine the behaviors of agents, achieving the baseline of safety guarantee at deployment. AgentGuard leverages the LLM orchestrator's innate capabilities - knowledge of tool functionalities, scalable and realistic workflow generation, and tool execution privileges - to act as its own safety evaluator. The framework operates through four phases: identifying unsafe workflows, validating them in real-world execution, generating safety constraints, and validating constraint efficacy. The output, an evaluation report with unsafe workflows, test cases, and validated constraints, enables multiple security applications. We empirically demonstrate AgentGuard's feasibility with experiments. With this exploratory work, we hope to inspire the establishment of standardized testing and hardening procedures for LLM agents to enhance their trustworthiness in real-world applications.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from Sparse Matrix Decomposition","url":"https://arxiv.org/abs/2502.09808","date":1739768400,"author":"","guid":1521,"unread":true,"content":"<article>arXiv:2502.09808v1 Announce Type: new \nAbstract: Securely computing graph convolutional networks (GCNs) is critical for applying their analytical capabilities to privacy-sensitive data like social/credit networks. Multiplying a sparse yet large adjacency matrix of a graph in GCN--a core operation in training/inference--poses a performance bottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it incurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and leveraging the monotonicity of non-zero entry locations, we propose a co-design harmonizing secure multi-party computation (MPC) with matrix sparsity. Our sparse matrix decomposition transforms an arbitrary sparse matrix into a product of structured matrices. Specialized MPC protocols for oblivious permutation and selection multiplication are then tailored, enabling our secure sparse matrix multiplication ($(SM)^2$) protocol, optimized for secure multiplication of these structured matrices. Together, these techniques take $O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present Virgos, a secure 2-party framework that is communication-efficient and memory-friendly on standard vertically-partitioned graph datasets. Performance of Virgos has been empirically validated across diverse network conditions.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unit Testing Past vs. Present: Examining LLMs' Impact on Defect Detection and Efficiency","url":"https://arxiv.org/abs/2502.09801","date":1739768400,"author":"","guid":1522,"unread":true,"content":"<article>arXiv:2502.09801v1 Announce Type: new \nAbstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into software engineering workflows has shown potential to enhance productivity, particularly in software testing. This paper investigates whether LLM support improves defect detection effectiveness during unit testing. Building on prior studies comparing manual and tool-supported testing, we replicated and extended an experiment where participants wrote unit tests for a Java-based system with seeded defects within a time-boxed session, supported by LLMs. Comparing LLM supported and manual testing, results show that LLM support significantly increases the number of unit tests generated, defect detection rates, and overall testing efficiency. These findings highlight the potential of LLMs to improve testing and defect detection outcomes, providing empirical insights into their practical application in software testing.</article>","contentLength":963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Co-designing Large Language Model Tools for Project-Based Learning with K12 Educators","url":"https://arxiv.org/abs/2502.09799","date":1739768400,"author":"","guid":1523,"unread":true,"content":"<article>arXiv:2502.09799v1 Announce Type: new \nAbstract: The emergence of generative AI, particularly large language models (LLMs), has opened the door for student-centered and active learning methods like project-based learning (PBL). However, PBL poses practical implementation challenges for educators around project design and management, assessment, and balancing student guidance with student autonomy. The following research documents a co-design process with interdisciplinary K-12 teachers to explore and address the current PBL challenges they face. Through teacher-driven interviews, collaborative workshops, and iterative design of wireframes, we gathered evidence for ways LLMs can support teachers in implementing high-quality PBL pedagogy by automating routine tasks and enhancing personalized learning. Teachers in the study advocated for supporting their professional growth and augmenting their current roles without replacing them. They also identified affordances and challenges around classroom integration, including resource requirements and constraints, ethical concerns, and potential immediate and long-term impacts. Drawing on these, we propose design guidelines for future deployment of LLM tools in PBL.</article>","contentLength":1224,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on LLM-based News Recommender Systems","url":"https://arxiv.org/abs/2502.09797","date":1739768400,"author":"","guid":1524,"unread":true,"content":"<article>arXiv:2502.09797v1 Announce Type: new \nAbstract: News recommender systems play a critical role in mitigating the information overload problem. In recent years, due to the successful applications of large language model technologies, researchers have utilized Discriminative Large Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve the performance of news recommender systems. Although several recent surveys review significant challenges for deep learning-based news recommender systems, such as fairness, privacy-preserving, and responsibility, there is a lack of a systematic survey on Large Language Model (LLM)-based news recommender systems. In order to review different core methodologies and explore potential issues systematically, we categorize DLLM-based and GLLM-based news recommender systems under the umbrella of LLM-based news recommender systems. In this survey, we first overview the development of deep learning-based news recommender systems. Then, we review LLM-based news recommender systems based on three aspects: news-oriented modeling, user-oriented modeling, and prediction-oriented modeling. Next, we examine the challenges from various perspectives, including datasets, benchmarking tools, and methodologies. Furthermore, we conduct extensive experiments to analyze how large language model technologies affect the performance of different news recommender systems. Finally, we comprehensively explore the future directions for LLM-based news recommendations in the era of LLMs.</article>","contentLength":1534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging Illumination Conditions","url":"https://arxiv.org/abs/2502.09795","date":1739768400,"author":"","guid":1525,"unread":true,"content":"<article>arXiv:2502.09795v1 Announce Type: new \nAbstract: Planetary exploration using aerial assets has the potential for unprecedented scientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved flight in Martian atmosphere is possible, future Mars rotocrafts will require advanced navigation capabilities for long-range flights. One such critical capability is Map-based Localization (MbL) which registers an onboard image to a reference map during flight in order to mitigate cumulative drift from visual odometry. However, significant illumination differences between rotocraft observations and a reference map prove challenging for traditional MbL systems, restricting the operational window of the vehicle. In this work, we investigate a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model for image registration that is more robust under large illumination differences than prior models. The system is supported by a custom simulation framework that uses real orbital maps to produce large amounts of realistic images of the Martian terrain. Comprehensive evaluations show that our proposed system outperforms prior MbL efforts in terms of localization accuracy under significant lighting and scale variations. Furthermore, we demonstrate the validity of our approach across a simulated Martian day.</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Noise Controlled CT Super-Resolution with Conditional Diffusion Model","url":"https://arxiv.org/abs/2502.09793","date":1739768400,"author":"","guid":1526,"unread":true,"content":"<article>arXiv:2502.09793v1 Announce Type: new \nAbstract: Improving the spatial resolution of CT images is a meaningful yet challenging task, often accompanied by the issue of noise amplification. This article introduces an innovative framework for noise-controlled CT super-resolution utilizing the conditional diffusion model. The model is trained on hybrid datasets, combining noise-matched simulation data with segmented details from real data. Experimental results with real CT images validate the effectiveness of our proposed framework, showing its potential for practical applications in CT imaging.</article>","contentLength":598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MANTIS: Detection of Zero-Day Malicious Domains Leveraging Low Reputed Hosting Infrastructure","url":"https://arxiv.org/abs/2502.09788","date":1739768400,"author":"","guid":1527,"unread":true,"content":"<article>arXiv:2502.09788v1 Announce Type: new \nAbstract: Internet miscreants increasingly utilize short-lived disposable domains to launch various attacks. Existing detection mechanisms are either too late to catch such malicious domains due to limited information and their short life spans or unable to catch them due to evasive techniques such as cloaking and captcha. In this work, we investigate the possibility of detecting malicious domains early in their life cycle using a content-agnostic approach. We observe that attackers often reuse or rotate hosting infrastructures to host multiple malicious domains due to increased utilization of automation and economies of scale. Thus, it gives defenders the opportunity to monitor such infrastructure to identify newly hosted malicious domains. However, such infrastructures are often shared hosting environments where benign domains are also hosted, which could result in a prohibitive number of false positives. Therefore, one needs innovative mechanisms to better distinguish malicious domains from benign ones even when they share hosting infrastructures. In this work, we build MANTIS, a highly accurate practical system that not only generates daily blocklists of malicious domains but also is able to predict malicious domains on-demand. We design a network graph based on the hosting infrastructure that is accurate and generalizable over time. Consistently, our models achieve a precision of 99.7%, a recall of 86.9% with a very low false positive rate (FPR) of 0.1% and on average detects 19K new malicious domains per day, which is over 5 times the new malicious domains flagged daily in VirusTotal. Further, MANTIS predicts malicious domains days to weeks before they appear in popular blocklists.</article>","contentLength":1755,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TableTalk: Scaffolding Spreadsheet Development with a Language Agent","url":"https://arxiv.org/abs/2502.09787","date":1739768400,"author":"","guid":1528,"unread":true,"content":"<article>arXiv:2502.09787v1 Announce Type: new \nAbstract: Despite its ubiquity in the workforce, spreadsheet programming remains challenging as programmers need both spreadsheet-specific knowledge (e.g., APIs to write formulas) and problem-solving skills to create complex spreadsheets. Large language models (LLMs) can help automate aspects of this process, and recent advances in planning and reasoning have enabled language agents, which dynamically plan, use tools, and take iterative actions to complete complex tasks. These agents observe, plan, and act, making them well-suited to scaffold spreadsheet programming by following expert processes.\n  We present TableTalk, a language agent that helps programmers build spreadsheets conversationally. Its design reifies three design principles -- scaffolding, flexibility, and incrementality -- which we derived from two studies of seven programmers and 62 Excel templates. TableTalk structures spreadsheet development by generating step-by-step plans and suggesting three next steps users can choose from. It also integrates tools that enable incremental spreadsheet construction. A user study with 20 programmers shows that TableTalk produces spreadsheets 2.3 times more likely to be preferred over a baseline agent, while reducing cognitive load and time spent reasoning about spreadsheet actions by 12.6%. TableTalk's approach has implications for human-agent collaboration. This includes providing persistent direct manipulation interfaces for stopping or undoing agent actions, while ensuring that such interfaces for accepting actions can be deactivated.</article>","contentLength":1604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerator-assisted Floating-point ASIP for Communication and Positioning in Massive MIMO Systems","url":"https://arxiv.org/abs/2502.09785","date":1739768400,"author":"","guid":1529,"unread":true,"content":"<article>arXiv:2502.09785v1 Announce Type: new \nAbstract: This paper presents an implementation of a floating-point-capable application-specific instruction set processor (ASIP) for both communication and positioning tasks using the massive multiple-input multiple-output (MIMO) technology. The ASIP is geared with vector processing capabilities in the form of single instruction multiple data (SIMD). A dual-pronged accelerator composition assists the processor to tame the heavier mathematical workloads. A standalone systolic array accelerator accompanies the processor to aid with matrix multiplications. A parallel vector memory subsystem provides functionalities to both the processor and the systolic array. Additionally, A convolutional neural network (CNN) module accelerator, which is paired with its own separate vector memory, works hand in glove with the processor to take on the positioning task. The processor is synthesized in 22 nm fully depleted silicon-on-insulator (FD-SOI) technology running at a clock frequency of 800 MHz. The system achieves a maximum detection throughput of 2.1 Gb/s in a 128x16 massive MIMO system for the user equipment (UE) speed of 50km/h. The localization throughput settles at around 390 positionings/s.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SLICES, a scientific instrument for the networking community","url":"https://arxiv.org/abs/2502.09783","date":1739768400,"author":"","guid":1530,"unread":true,"content":"<article>arXiv:2502.09783v1 Announce Type: new \nAbstract: A science is defined by a set of encyclopedic knowledge related to facts or phenomena following rules or evidenced by experimentally-driven observations. Computer Science and in particular computer networks is a relatively new scientific domain maturing over years and adopting the best practices inherited from more fundamental disciplines. The design of past, present and future networking components and architectures have been assisted, among other methods, by experimentally-driven research and in particular by the deployment of test platforms, usually named as testbeds. However, often experimentally-driven networking research used scattered methodologies, based on ad-hoc, small-sized testbeds, producing hardly repeatable results. We believe that computer networks needs to adopt a more structured methodology, supported by appropriate instruments, to produce credible experimental results supporting radical and incremental innovations. This paper reports lessons learned from the design and operation of test platforms for the scientific community dealing with digital infrastructures. We introduce the SLICES initiative as the outcome of several years of evolution of the concept of a networking test platform transformed into a scientific instrument. We address the challenges, requirements and opportunities that our community is facing to manage the full research-life cycle necessary to support a scientific methodology.</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers and Large Language Models","url":"https://arxiv.org/abs/2502.09782","date":1739768400,"author":"","guid":1531,"unread":true,"content":"<article>arXiv:2502.09782v1 Announce Type: new \nAbstract: The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNet's performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios.</article>","contentLength":1407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Medical Applications of Graph Convolutional Networks Using Electronic Health Records: A Survey","url":"https://arxiv.org/abs/2502.09781","date":1739768400,"author":"","guid":1532,"unread":true,"content":"<article>arXiv:2502.09781v1 Announce Type: new \nAbstract: Graph Convolutional Networks (GCNs) have emerged as a promising approach to machine learning on Electronic Health Records (EHRs). By constructing a graph representation of patient data and performing convolutions on neighborhoods of nodes, GCNs can capture complex relationships and extract meaningful insights to support medical decision making. This survey provides an overview of the current research in applying GCNs to EHR data. We identify the key medical domains and prediction tasks where these models are being utilized, common benchmark datasets, and architectural patterns to provide a comprehensive survey of this field. While this is a nascent area of research, GCNs demonstrate strong potential to leverage the complex information hidden in EHRs. Challenges and opportunities for future work are also discussed.</article>","contentLength":874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games","url":"https://arxiv.org/abs/2502.09780","date":1739768400,"author":"","guid":1533,"unread":true,"content":"<article>arXiv:2502.09780v1 Announce Type: new \nAbstract: Multi-agent reinforcement learning (MARL) lies at the heart of a plethora of applications involving the interaction of a group of agents in a shared unknown environment. A prominent framework for studying MARL is Markov games, with the goal of finding various notions of equilibria in a sample-efficient manner, such as the Nash equilibrium (NE) and the coarse correlated equilibrium (CCE). However, existing sample-efficient approaches either require tailored uncertainty estimation under function approximation, or careful coordination of the players. In this paper, we propose a novel model-based algorithm, called VMG, that incentivizes exploration via biasing the empirical estimate of the model parameters towards those with a higher collective best-response values of all the players when fixing the other players' policies, thus encouraging the policy to deviate from its current equilibrium for more exploration. VMG is oblivious to different forms of function approximation, and permits simultaneous and uncoupled policy updates of all players. Theoretically, we also establish that VMG achieves a near-optimal regret for finding both the NEs of two-player zero-sum Markov games and CCEs of multi-player general-sum Markov games under linear function approximation in an online environment, which nearly match their counterparts with sophisticated uncertainty quantification.</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt and circumstance: A word-by-word LLM prompting approach to interlinear glossing for low-resource languages","url":"https://arxiv.org/abs/2502.09778","date":1739768400,"author":"","guid":1534,"unread":true,"content":"<article>arXiv:2502.09778v1 Announce Type: new \nAbstract: Partly automated creation of interlinear glossed text (IGT) has the potential to assist in linguistic documentation. We argue that LLMs can make this process more accessible to linguists because of their capacity to follow natural-language instructions. We investigate the effectiveness of a retrieval-based LLM prompting approach to glossing, applied to the seven languages from the SIGMORPHON 2023 shared task. Our system beats the BERT-based shared task baseline for every language in the morpheme-level score category, and we show that a simple 3-best oracle has higher word-level scores than the challenge winner (a tuned sequence model) in five languages. In a case study on Tsez, we ask the LLM to automatically create and follow linguistic instructions, reducing errors on a confusing grammatical feature. Our results thus demonstrate the potential contributions which LLMs can make in interactive systems for glossing, both in making suggestions to human annotators and following directions.</article>","contentLength":1049,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the existence of EFX allocations in multigraphs","url":"https://arxiv.org/abs/2502.09777","date":1739768400,"author":"","guid":1535,"unread":true,"content":"<article>arXiv:2502.09777v1 Announce Type: new \nAbstract: We study the problem of \"fairly\" dividing indivisible goods to several agents that have valuation set functions over the sets of goods. As fair we consider the allocations that are envy-free up to any good (EFX), i.e., no agent envies any proper subset of the goods given to any other agent. The existence or not of EFX allocations is a major open problem in Fair Division, and there are only positive results for special cases.\n  [George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023] introduced a restriction on the agents' valuations according to a graph structure: the vertices correspond to agents and the edges to goods, and each vertex/agent has zero marginal value (or in other words, they are indifferent) for the edges/goods that are not adjacent to them. The existence of EFX allocations has been shown for simple graphs with general monotone valuations [George Christodoulou, Amos Fiat, Elias Koutsoupias, Alkmini Sgouritsa 2023], and for multigraphs for restricted additive valuations [Alireza Kaviani, Masoud Seddighin, Amir Mohammad Shahrezaei 2024].\n  In this work, we push the state-of-the-art further, and show that the EFX allocations always exists in multigraphs and general monotone valuations if any of the following three conditions hold: either (a) the multigraph is bipartite, or (b) each agent has at most $\\lceil \\frac{n}{4} \\rceil -1$ neighbors, where $n$ is the total number of agents, or (c) the shortest cycle with non-parallel edges has length at least 6.</article>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigating the Role of Situational Disruptors in Engagement with Digital Mental Health Tools","url":"https://arxiv.org/abs/2502.09776","date":1739768400,"author":"","guid":1536,"unread":true,"content":"<article>arXiv:2502.09776v1 Announce Type: new \nAbstract: Challenges in engagement with digital mental health (DMH) tools are commonly addressed through technical enhancements and algorithmic interventions. This paper shifts the focus towards the role of users' broader social context as a significant factor in engagement. Through an eight-week text messaging program aimed at enhancing psychological wellbeing, we recruited 20 participants to help us identify situational engagement disruptors (SEDs), including personal responsibilities, professional obligations, and unexpected health issues. In follow-up design workshops with 25 participants, we explored potential solutions that address such SEDs: prioritizing self-care through structured goal-setting, alternative framings for disengagement, and utilization of external resources. Our findings challenge conventional perspectives on engagement and offer actionable design implications for future DMH tools.</article>","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Knowledge-Enhanced Program Repair for Data Science Code","url":"https://arxiv.org/abs/2502.09771","date":1739768400,"author":"","guid":1537,"unread":true,"content":"<article>arXiv:2502.09771v1 Announce Type: new \nAbstract: This paper introduces DSrepair, a knowledge-enhanced program repair method designed to repair the buggy code generated by LLMs in the data science domain. DSrepair uses knowledge graph based RAG for API knowledge retrieval as well as bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to enable knowledge graph based API retrieval, we construct DS-KG (Data Science Knowledge Graph) for widely used data science libraries. For bug knowledge enrichment, we employ an abstract syntax tree (AST) to localize errors at the AST node level. DSrepair's effectiveness is evaluated against five state-of-the-art LLM-based repair baselines using four advanced LLMs on the DS-1000 dataset. The results show that DSrepair surpasses all five baselines. Specifically, when compared to the second-best baseline, DSrepair demonstrates significant improvements, fixing 44.4%, 14.2%, 20.6%, and 32.1% more buggy code snippets for each of the four evaluated LLMs, respectively. Additionally, it achieves greater efficiency, reducing the number of tokens required per code task by 17.49%, 34.24%, 24.71%, and 17.59%, respectively.</article>","contentLength":1184,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complex Network Modelling with Power-law Activating Patterns and Its Evolutionary Dynamics","url":"https://arxiv.org/abs/2502.09768","date":1739768400,"author":"","guid":1538,"unread":true,"content":"<article>arXiv:2502.09768v1 Announce Type: new \nAbstract: Complex network theory provides a unifying framework for the study of structured dynamic systems. The current literature emphasizes a widely reported phenomenon of intermittent interaction among network vertices. In this paper, we introduce a complex network model that considers the stochastic switching of individuals between activated and quiescent states at power-law rates and the corresponding evolutionary dynamics. By using the Markov chain and renewal theory, we discover a homogeneous stationary distribution of activated sizes in the network with power-law activating patterns and infer some statistical characteristics. To better understand the effect of power-law activating patterns, we study the two-person-two-strategy evolutionary game dynamics, demonstrate the absorbability of strategies, and obtain the critical cooperation conditions for prisoner's dilemmas in homogeneous networks without mutation. The evolutionary dynamics in real networks are also discussed. Our results provide a new perspective to analyze and understand social physics in time-evolving network systems.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Markovian Discrete Diffusion with Causal Language Models","url":"https://arxiv.org/abs/2502.09767","date":1739768400,"author":"","guid":1539,"unread":true,"content":"<article>arXiv:2502.09767v1 Announce Type: new \nAbstract: Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, CaDDi integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pretrained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.</article>","contentLength":1060,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Generated Microservice Implementations from RESTful API Definitions","url":"https://arxiv.org/abs/2502.09766","date":1739768400,"author":"","guid":1540,"unread":true,"content":"<article>arXiv:2502.09766v1 Announce Type: new \nAbstract: The growing need for scalable, maintainable, and fast-deploying systems has made microservice architecture widely popular in software development. This paper presents a system that uses Large Language Models (LLMs) to automate the API-first development of RESTful microservices. This system assists in creating OpenAPI specification, generating server code from it, and refining the code through a feedback loop that analyzes execution logs and error messages. By focusing on the API-first methodology, this system ensures that microservices are designed with well-defined interfaces, promoting consistency and reliability across the development life-cycle. The integration of log analysis enables the LLM to detect and address issues efficiently, reducing the number of iterations required to produce functional and robust services. This process automates the generation of microservices and also simplifies the debugging and refinement phases, allowing developers to focus on higher-level design and integration tasks. This system has the potential to benefit software developers, architects, and organizations to speed up software development cycles and reducing manual effort. To assess the potential of the system, we conducted surveys with six industry practitioners. After surveying practitioners, the system demonstrated notable advantages in enhancing development speed, automating repetitive tasks, and simplifying the prototyping process. While experienced developers appreciated its efficiency for specific tasks, some expressed concerns about its limitations in handling advanced customizations and larger scale projects. The code is publicly available at https://github.com/sirbh/code-gen</article>","contentLength":1751,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differential Adjusted Parity for Learning Fair Representations","url":"https://arxiv.org/abs/2502.09765","date":1739768400,"author":"","guid":1541,"unread":true,"content":"<article>arXiv:2502.09765v1 Announce Type: new \nAbstract: The development of fair and unbiased machine learning models remains an ongoing objective for researchers in the field of artificial intelligence. We introduce the Differential Adjusted Parity (DAP) loss to produce unbiased informative representations. It utilises a differentiable variant of the adjusted parity metric to create a unified objective function. By combining downstream task classification accuracy and its inconsistency across sensitive feature domains, it provides a single tool to increase performance and mitigate bias. A key element in this approach is the use of soft balanced accuracies. In contrast to previous non-adversarial approaches, DAP does not suffer a degeneracy where the metric is satisfied by performing equally poorly across all sensitive domains. It outperforms several adversarial models on downstream task accuracy and fairness in our analysis. Specifically, it improves the demographic parity, equalized odds and sensitive feature accuracy by as much as 22.5\\%, 44.1\\% and 40.1\\%, respectively, when compared to the best performing adversarial approaches on these metrics. Overall, the DAP loss and its associated metric can play a significant role in creating more fair machine learning models.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SoK: Come Together -- Unifying Security, Information Theory, and Cognition for a Mixed Reality Deception Attack Ontology & Analysis Framework","url":"https://arxiv.org/abs/2502.09763","date":1739768400,"author":"","guid":1542,"unread":true,"content":"<article>arXiv:2502.09763v1 Announce Type: new \nAbstract: We present a primary attack ontology and analysis framework for deception attacks in Mixed Reality (MR). This is achieved through multidisciplinary Systematization of Knowledge (SoK), integrating concepts from MR security, information theory, and cognition. While MR grows in popularity, it presents many cybersecurity challenges, particularly concerning deception attacks and their effects on humans. In this paper, we use the Borden-Kopp model of deception to develop a comprehensive ontology of MR deception attacks. Further, we derive two models to assess impact of MR deception attacks on information communication and decision-making. The first, an information-theoretic model, mathematically formalizes the effects of attacks on information communication. The second, a decision-making model, details the effects of attacks on interlaced cognitive processes. Using our ontology and models, we establish the MR Deception Analysis Framework (DAF) to assess the effects of MR deception attacks on information channels, perception, and attention. Our SoK uncovers five key findings for research and practice and identifies five research gaps to guide future work.</article>","contentLength":1215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and Deployment","url":"https://arxiv.org/abs/2502.09762","date":1739768400,"author":"","guid":1543,"unread":true,"content":"<article>arXiv:2502.09762v1 Announce Type: new \nAbstract: Adaptive teaming, the ability to collaborate with unseen teammates without prior coordination, remains an underexplored challenge in multi-robot collaboration. This paper focuses on adaptive teaming in multi-drone cooperative pursuit, a critical task with real-world applications such as border surveillance, search-and-rescue, and counter-terrorism. We first define and formalize the \\textbf{A}daptive Teaming in \\textbf{M}ulti-\\textbf{D}rone \\textbf{P}ursuit (AT-MDP) problem and introduce AT-MDP framework, a comprehensive framework that integrates simulation, algorithm training and real-world deployment. AT-MDP framework provides a flexible experiment configurator and interface for simulation, a distributed training framework with an extensive algorithm zoo (including two newly proposed baseline methods) and an unseen drone zoo for evaluating adaptive teaming, as well as a real-world deployment system that utilizes edge computing and Crazyflie drones. To the best of our knowledge, AT-MDP framework is the first adaptive framework for continuous-action decision-making in complex real-world drone tasks, enabling multiple drones to coordinate effectively with unseen teammates. Extensive experiments in four multi-drone pursuit environments of increasing difficulty confirm the effectiveness of AT-MDP framework, while real-world deployments further validate its feasibility in physical systems. Videos and code are available at https://sites.google.com/view/at-mdp.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The AI-Therapist Duo: Exploring the Potential of Human-AI Collaboration in Personalized Art Therapy for PICS Intervention","url":"https://arxiv.org/abs/2502.09757","date":1739768400,"author":"","guid":1544,"unread":true,"content":"<article>arXiv:2502.09757v1 Announce Type: new \nAbstract: Post-intensive care syndrome (PICS) is a multifaceted condition that arises from prolonged stays in an intensive care unit (ICU). While preventing PICS among ICU patients is becoming increasingly important, interventions remain limited. Building on evidence supporting the effectiveness of art exposure in addressing the psychological aspects of PICS, we propose a novel art therapy solution through a collaborative Human-AI approach that enhances personalized therapeutic interventions using state-of-the-art Visual Art Recommendation Systems. We developed two Human-in-the-Loop (HITL) personalization methods and assessed their impact through a large-scale user study (N=150). Our findings demonstrate that this Human-AI collaboration not only enhances the personalization and effectiveness of art therapy but also supports therapists by streamlining their workload. While our study centres on PICS intervention, the results suggest that human-AI collaborative Art therapy could potentially benefit other areas where emotional support is critical, such as cases of anxiety and depression.</article>","contentLength":1139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Jailbreak Attacks via Compliance-Refusal-Based Initialization","url":"https://arxiv.org/abs/2502.09755","date":1739768400,"author":"","guid":1545,"unread":true,"content":"<article>arXiv:2502.09755v1 Announce Type: new \nAbstract: Jailbreak attacks aim to exploit large language models (LLMs) and pose a significant threat to their proper conduct; they seek to bypass models' safeguards and often provoke transgressive behaviors. However, existing automatic jailbreak attacks require extensive computational resources and are prone to converge on suboptimal solutions. In this work, we propose \\textbf{C}ompliance \\textbf{R}efusal \\textbf{I}nitialization (CRI), a novel, attack-agnostic framework that efficiently initializes the optimization in the proximity of the compliance subspace of harmful prompts. By narrowing the initial gap to the adversarial objective, CRI substantially improves adversarial success rates (ASR) and drastically reduces computational overhead -- often requiring just a single optimization step. We evaluate CRI on the widely-used AdvBench dataset over the standard jailbreak attacks of GCG and AutoDAN. Results show that CRI boosts ASR and decreases the median steps to success by up to \\textbf{\\(\\times 60\\)}. The project page, along with the reference implementation, is publicly available at \\texttt{https://amit1221levi.github.io/CRI-Jailbreak-Init-LLMs-evaluation/}.</article>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Adaptive Meshing, Mesh Density Functions, and Nonlocal Observations for Ensemble Based Data Assimilation","url":"https://arxiv.org/abs/2502.09754","date":1739768400,"author":"","guid":1546,"unread":true,"content":"<article>arXiv:2502.09754v1 Announce Type: new \nAbstract: Adaptive spatial meshing has proven invaluable for the accurate, efficient computation of solutions of time dependent partial differential equations. In a DA context the use of adaptive spatial meshes addresses several factors that place increased demands on meshing; these include the location and relative importance of observations and the use of ensemble solutions. To increase the efficiency of adaptive meshes for data assimilation, robust look ahead meshes are developed that fix the same adaptive mesh for all ensemble members for the entire time interval of the forecasts and that incorporates the observations at the next analysis time. This allows for increased vectorization of the ensemble forecasts while minimizing interpolation of solutions between different meshes. The techniques to determine these robust meshes are based upon combining metric tensors or mesh density functions to define nonuniform meshes. We illustrate the robust ensemble look ahead meshes using traveling wave solutions of a bistable reaction-diffusion equation. Observation operators based on convolution type integrals and their associated metric tensors are derived. These further the goals of making efficient use of adaptive meshes in ensemble based DA techniques, developing and employing robust meshes that are effective for a range of similar behaviors in both the ensembles and the observations, and the integration with advanced numerical PDE techniques (a quasi-Lagrangian moving mesh DG technique employing embedded pairs for time stepping). Numerical experiments with different observation scenarios are presented for a 2D inviscid Burgers' equation, a multi-component system, a 2D Shallow Water model, and for a coupled system of two 1D Kuramoto-Sivashinsky equations.</article>","contentLength":1820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vote-Tree-Planner: Optimizing Execution Order in LLM-based Task Planning Pipeline via Voting","url":"https://arxiv.org/abs/2502.09749","date":1739768400,"author":"","guid":1547,"unread":true,"content":"<article>arXiv:2502.09749v1 Announce Type: new \nAbstract: Integrating large language models (LLMs) into closed-loop robotic task planning has become increasingly popular within embodied artificial intelligence. Previous efforts mainly focused on leveraging the strong reasoning abilities of LLMs to enhance task planning performance while often overlooking task planning efficiency and executability due to repetitive queries to LLMs. This paper addresses the synergy between LLMs and task planning systems, aiming to minimize redundancy while enhancing planning effectiveness. Specifically, building upon Prog-Prompt and the high-level concept of Tree-Planner, we propose Vote-Tree-Planner. This sampling strategy utilizes votes to guide plan traversal during the decision-making process. Our approach is motivated by a straightforward observation: assigning weights to agents during decision-making enables the evaluation of critical paths before execution. With this simple vote-tree construction, our method further improves the success rate and reduces the number of queries to LLMs. The experimental results highlight that our Vote-Tree-Planner demonstrates greater stability and shows a higher average success rate and goal condition recall on the unseen dataset compared with previous baseline methods. These findings underscore the potential of the Vote-Tree-Planner to enhance planning accuracy, reliability, and efficiency in LLM-based planning systems.</article>","contentLength":1455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Widespread Adoption of Large Language Model-Assisted Writing Across Society","url":"https://arxiv.org/abs/2502.09747","date":1739768400,"author":"","guid":1548,"unread":true,"content":"<article>arXiv:2502.09747v1 Announce Type: new \nAbstract: The recent advances in large language models (LLMs) attracted significant public and policymaker interest in its adoption patterns. In this paper, we systematically analyze LLM-assisted writing across four domains-consumer complaints, corporate communications, job postings, and international organization press releases-from January 2022 to September 2024. Our dataset includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3 million job postings, and 15,919 United Nations (UN) press releases. Using a robust population-level statistical framework, we find that LLM usage surged following the release of ChatGPT in November 2022. By late 2024, roughly 18% of financial consumer complaint text appears to be LLM-assisted, with adoption patterns spread broadly across regions and slightly higher in urban areas. For corporate press releases, up to 24% of the text is attributable to LLMs. In job postings, LLM-assisted writing accounts for just below 10% in small firms, and is even more common among younger firms. UN press releases also reflect this trend, with nearly 14% of content being generated or modified by LLMs. Although adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by 2024, reflecting either saturation in LLM adoption or increasing subtlety of more advanced models. Our study shows the emergence of a new reality in which firms, consumers and even international organizations substantially rely on generative AI for communications.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fine-Tuning Foundation Models with Federated Learning for Privacy Preserving Medical Time Series Forecasting","url":"https://arxiv.org/abs/2502.09744","date":1739768400,"author":"","guid":1549,"unread":true,"content":"<article>arXiv:2502.09744v1 Announce Type: new \nAbstract: Federated Learning (FL) provides a decentralized machine learning approach, where multiple devices or servers collaboratively train a model without sharing their raw data, thus enabling data privacy. This approach has gained significant interest in academia and industry due to its privacy-preserving properties, which are particularly valuable in the medical domain where data availability is often protected under strict regulations. A relatively unexplored area is the use of FL to fine-tune Foundation Models (FMs) for time series forecasting, potentially enhancing model efficacy by overcoming data limitation while maintaining privacy. In this paper, we fine-tuned time series FMs with Electrocardiogram (ECG) and Impedance Cardiography (ICG) data using different FL techniques. We then examined various scenarios and discussed the challenges FL faces under different data heterogeneity configurations. Our empirical results demonstrated that while FL can be effective for fine-tuning FMs on time series forecasting tasks, its benefits depend on the data distribution across clients. We highlighted the trade-offs in applying FL to FM fine-tuning.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partial Colexifications Improve Concept Embeddings","url":"https://arxiv.org/abs/2502.09743","date":1739768400,"author":"","guid":1550,"unread":true,"content":"<article>arXiv:2502.09743v1 Announce Type: new \nAbstract: While the embedding of words has revolutionized the field of Natural Language Processing, the embedding of concepts has received much less attention so far. A dense and meaningful representation of concepts, however, could prove useful for several tasks in computational linguistics, especially those involving cross-linguistic data or sparse data from low resource languages. First methods that have been proposed so far embed concepts from automatically constructed colexification networks. While these approaches depart from automatically inferred polysemies, attested across a larger number of languages, they are restricted to the word level, ignoring lexical relations that would only hold for parts of the words in a given language. Building on recently introduced methods for the inference of partial colexifications, we show how they can be used to improve concept embeddings in meaningful ways. The learned embeddings are evaluated against lexical similarity ratings, recorded instances of semantic shift, and word association data. We show that in all evaluation tasks, the inclusion of partial colexifications lead to improved concept representations and better results. Our results further show that the learned embeddings are able to capture and represent different semantic relationships between concepts.</article>","contentLength":1369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FoNE: Precise Single-Token Number Embeddings via Fourier Features","url":"https://arxiv.org/abs/2502.09741","date":1739768400,"author":"","guid":1551,"unread":true,"content":"<article>arXiv:2502.09741v1 Announce Type: new \nAbstract: Large Language Models (LLMs) typically represent numbers using multiple tokens, which requires the model to aggregate these tokens to interpret numerical values. This fragmentation makes both training and inference less efficient and adversely affects the model's performance on number-related tasks. Inspired by the observation that pre-trained LLMs internally learn Fourier-like features for number tokens, we propose Fourier Number Embedding (FoNE), a novel method that directly maps numbers into the embedding space with their Fourier features. FoNE encodes each number as a single token with only two embedding dimensions per digit, effectively capturing numerical values without fragmentation. This compact representation accelerates both training and inference. Compared to traditional subword and digit-wise embeddings, FoNE not only reduces computational overhead but also achieves higher accuracy across various numerical tasks including addition, subtraction and multiplication. On 6-digit decimal addition, FoNE requires 64$\\times$ less data to achieve 99% accuracy than subword and digit-wise embeddings while using 3$\\times$ and 6$\\times$ fewer tokens per number, respectively. Furthermore, FoNE is the only method that yields 100% accuracy on over 100,000 test examples for addition, subtraction, and multiplication. The codes and visualization are available at https://fouriernumber.github.io/.</article>","contentLength":1459,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adjoint of Least Squares Shadowing: Existence, Uniqueness and Coarse Domain Discretization","url":"https://arxiv.org/abs/2502.09737","date":1739768400,"author":"","guid":1552,"unread":true,"content":"<article>arXiv:2502.09737v1 Announce Type: new \nAbstract: Chaotic dynamical systems are characterized by the sensitive dependence of trajectories on initial conditions. Conventional sensitivity analysis of time-averaged functionals yields unbounded sensitivities when the simulation is chaotic. The least squares shadowing (LSS) is a popular approach to computing bounded sensitivities in the presence of chaotic dynamical systems. The current paper proves the existence, uniqueness, and boundedness of the adjoint of the LSS equations. In particular, the analysis yields a sharper bound on the condition number of the LSS equations than currently demonstrated in existing literature and shows that the condition number is bounded for large integration times. The derived bound on condition number also shows a relation between the conditioning of the LSS and the time dilation factor which is consistent with the trend numerically observed in the previous LSS literature. Furthermore, using the boundedness of the condition number for large integration times, we provide an alternate proof to (Chater et al., 2017) of the convergence of the LSS sensitivity to the true sensitivity at the rate of $\\mathcal{O}\\left(\\frac{1}{\\sqrt{T}}\\right)$ regardless of the boundary conditions imposed on the adjoint, as long as the adjoint boundary conditions are bounded. Existence and uniqueness of the solution to the continuous-in-time adjoint LSS equation ensure that the LSS equation can be discretized independently of the primal equation and that the true LSS adjoint solution is recovered as the time step is refined. This allows for the adjoint LSS equation to be discretized on a coarser time domain than that of the primal governing equation to reduce the cost of solving the linear space-time system.</article>","contentLength":1791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A CNN Approach to Automated Detection and Classification of Brain Tumors","url":"https://arxiv.org/abs/2502.09731","date":1739768400,"author":"","guid":1553,"unread":true,"content":"<article>arXiv:2502.09731v1 Announce Type: new \nAbstract: Brain tumors require an assessment to ensure timely diagnosis and effective patient treatment. Morphological factors such as size, location, texture, and variable appearance com- plicate tumor inspection. Medical imaging presents challenges, including noise and incomplete images. This research article presents a methodology for processing Magnetic Resonance Imag- ing (MRI) data, encompassing techniques for image classification and denoising. The effective use of MRI images allows medical professionals to detect brain disorders, including tumors. This research aims to categorize healthy brain tissue and brain tumors by analyzing the provided MRI data. Unlike alternative methods like Computed Tomography (CT), MRI technology offers a more detailed representation of internal anatomical components, mak- ing it a suitable option for studying data related to brain tumors. The MRI picture is first subjected to a denoising technique utilizing an Anisotropic diffusion filter. The dataset utilized for the models creation is a publicly accessible and validated Brain Tumour Classification (MRI) database, comprising 3,264 brain MRI scans. SMOTE was employed for data augmentation and dataset balancing. Convolutional Neural Networks(CNN) such as ResNet152V2, VGG, ViT, and EfficientNet were employed for the classification procedure. EfficientNet attained an accuracy of 98%, the highest recorded.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perch like a bird: bio-inspired optimal maneuvers and nonlinear control for Flapping-Wing Unmanned Aerial Vehicles","url":"https://arxiv.org/abs/2502.09728","date":1739768400,"author":"","guid":1554,"unread":true,"content":"<article>arXiv:2502.09728v1 Announce Type: new \nAbstract: This research endeavors to design the perching maneuver and control in ornithopter robots. By analyzing the dynamic interplay between the robot's flight dynamics, feedback loops, and the environmental constraints, we aim to advance our understanding of the perching maneuver, drawing parallels to biological systems. Inspired by the elegant control strategies observed in avian flight, we develop an optimal maneuver and a corresponding controller to achieve stable perching. The maneuver consists of a deceleration and a rapid pitch-up (vertical turn), which arises from analytically solving the optimization problem of minimal velocity at perch, subject to kinematic and dynamic constraints. The controller for the flapping frequency and tail symmetric deflection is nonlinear and adaptive, ensuring robustly stable perching. Indeed, such adaptive behavior in a sense incorporates homeostatic principles of cybernetics into the control system, enhancing the robot's ability to adapt to unexpected disturbances and maintain a stable posture during the perching maneuver. The resulting autonomous perching maneuvers -- closed-loop descent and turn -- , have been verified and validated, demonstrating excellent agreement with real bird perching trajectories reported in the literature. These findings lay the theoretical groundwork for the development of future prototypes that better imitate the skillful perching maneuvers of birds.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of Robust and Secure DNS Protocols for IoT Devices","url":"https://arxiv.org/abs/2502.09726","date":1739768400,"author":"","guid":1555,"unread":true,"content":"<article>arXiv:2502.09726v1 Announce Type: new \nAbstract: The DNS (Domain Name System) protocol has been in use since the early days of the Internet. Although DNS as a de facto networking protocol had no security considerations in its early years, there have been many security enhancements, such as DNSSec (Domain Name System Security Extensions), DoT (DNS over Transport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With all these security improvements, it is not yet clear what resource-constrained Internet-of-Things (IoT) devices should be used for robustness. In this paper, we investigate different DNS security approaches using an edge DNS resolver implemented as a Virtual Network Function (VNF) to replicate the impact of the protocol from an IoT perspective and compare their performances under different conditions. We present our results for cache-based and non-cached responses and evaluate the corresponding security benefits. Our results and framework can greatly help consumers, manufacturers, and the research community decide and implement their DNS protocols depending on the given dynamic network conditions and enable robust Internet access via DNS for different devices.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning","url":"https://arxiv.org/abs/2502.09724","date":1739768400,"author":"","guid":1556,"unread":true,"content":"<article>arXiv:2502.09724v1 Announce Type: new \nAbstract: In many real-world applications of reinforcement learning (RL), deployed policies have varied impacts on different stakeholders, creating challenges in reaching consensus on how to effectively aggregate their preferences. Generalized $p$-means form a widely used class of social welfare functions for this purpose, with broad applications in fair resource allocation, AI alignment, and decision-making. This class includes well-known welfare functions such as Egalitarian, Nash, and Utilitarian welfare. However, selecting the appropriate social welfare function is challenging for decision-makers, as the structure and outcomes of optimal policies can be highly sensitive to the choice of $p$. To address this challenge, we study the concept of an $\\alpha$-approximate portfolio in RL, a set of policies that are approximately optimal across the family of generalized $p$-means for all $p \\in [-\\infty, 1]$. We propose algorithms to compute such portfolios and provide theoretical guarantees on the trade-offs among approximation factor, portfolio size, and computational efficiency. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of our approach in summarizing the policy space induced by varying $p$ values, empowering decision-makers to navigate this landscape more effectively.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models","url":"https://arxiv.org/abs/2502.09723","date":1739768400,"author":"","guid":1557,"unread":true,"content":"<article>arXiv:2502.09723v1 Announce Type: new \nAbstract: Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to systematically examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into code-style structured query to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, ant the results show that QueryAttack achieves high attack success rates (ASRs) across LLMs with different developers and capabilities. We also evaluate QueryAttack's performance against common defenses, confirming that it is difficult to mitigate with general defensive techniques. To defend against QueryAttack, we tailor a defense method which can reduce ASR by up to 64\\% on GPT-4-1106. The code of QueryAttack can be found on https://anonymous.4open.science/r/QueryAttack-334B.</article>","contentLength":1262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NestQuant: Nested Lattice Quantization for Matrix Products and LLMs","url":"https://arxiv.org/abs/2502.09720","date":1739768400,"author":"","guid":1558,"unread":true,"content":"<article>arXiv:2502.09720v1 Announce Type: new \nAbstract: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent work have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various LLM evaluation benchmarks also show a reduction in performance degradation induced by quantization.</article>","contentLength":1056,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Carbon- and Precedence-Aware Scheduling for Data Processing Clusters","url":"https://arxiv.org/abs/2502.09717","date":1739768400,"author":"","guid":1559,"unread":true,"content":"<article>arXiv:2502.09717v1 Announce Type: new \nAbstract: As large-scale data processing workloads continue to grow, their carbon footprint raises concerns. Prior research on carbon-aware schedulers has focused on shifting computation to align with availability of low-carbon energy, but these approaches assume that each task can be executed independently. In contrast, data processing jobs have precedence constraints (i.e., outputs of one task are inputs for another) that complicate decisions, since delaying an upstream ``bottleneck'' task to a low-carbon period will also block downstream tasks, impacting the entire job's completion time. In this paper, we show that carbon-aware scheduling for data processing benefits from knowledge of both time-varying carbon and precedence constraints. Our main contribution is $\\texttt{PCAPS}$, a carbon-aware scheduler that interfaces with modern ML scheduling policies to explicitly consider the precedence-driven importance of each task in addition to carbon. To illustrate the gains due to fine-grained task information, we also study $\\texttt{CAP}$, a wrapper for any carbon-agnostic scheduler that adapts the key provisioning ideas of $\\texttt{PCAPS}$. Our schedulers enable a configurable priority between carbon reduction and job completion time, and we give analytical results characterizing the trade-off between the two. Furthermore, our Spark prototype on a 100-node Kubernetes cluster shows that a moderate configuration of $\\texttt{PCAPS}$ reduces carbon footprint by up to 32.9% without significantly impacting the cluster's total efficiency.</article>","contentLength":1594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Genetic Data Governance in Crisis: Policy Recommendations for Safeguarding Privacy and Preventing Discrimination","url":"https://arxiv.org/abs/2502.09716","date":1739768400,"author":"","guid":1560,"unread":true,"content":"<article>arXiv:2502.09716v1 Announce Type: new \nAbstract: Genetic data collection has become ubiquitous today. The ability to meaningfully interpret genetic data has motivated its widespread use, providing crucial insights into human health and ancestry while driving important public health initiatives. Easy access to genetic testing has fueled a rapid expansion of recreational direct-to-consumer offerings. However, the growth of genetic datasets and their applications has created significant privacy and discrimination risks, as our understanding of the scientific basis for genetic traits continues to evolve. In this paper, we organize the uses of genetic data along four distinct \"pillars\": clinical practice, research, forensic and government use, and recreational use. Using our scientific understanding of genetics, genetic inference methods and their associated risks, and current public protections, we build a risk assessment framework that identifies key values that any governance system must preserve. We analyze case studies using this framework to assess how well existing regulatory frameworks preserve desired values. Our investigation reveals critical gaps in these frameworks and identifies specific threats to privacy and personal liberties, particularly through genetic discrimination. We propose comprehensive policy reforms to: (1) update the legal definition of genetic data to protect against modern technological capabilities, (2) expand the Genetic Information Nondiscrimination Act (GINA) to cover currently unprotected domains, and (3) establish a unified regulatory framework under a single governing body to oversee all applications of genetic data. We conclude with three open questions about genetic data: the challenges posed by its relational nature, including consent for relatives and minors; the complexities of international data transfer; and its potential integration into large language models.</article>","contentLength":1932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating GPT's Capability in Identifying Stages of Cognitive Impairment from Electronic Health Data","url":"https://arxiv.org/abs/2502.09715","date":1739768400,"author":"","guid":1561,"unread":true,"content":"<article>arXiv:2502.09715v1 Announce Type: new \nAbstract: Identifying cognitive impairment within electronic health records (EHRs) is crucial not only for timely diagnoses but also for facilitating research. Information about cognitive impairment often exists within unstructured clinician notes in EHRs, but manual chart reviews are both time-consuming and error-prone. To address this issue, our study evaluates an automated approach using zero-shot GPT-4o to determine stage of cognitive impairment in two different tasks. First, we evaluated the ability of GPT-4o to determine the global Clinical Dementia Rating (CDR) on specialist notes from 769 patients who visited the memory clinic at Massachusetts General Hospital (MGH), and achieved a weighted kappa score of 0.83. Second, we assessed GPT-4o's ability to differentiate between normal cognition, mild cognitive impairment (MCI), and dementia on all notes in a 3-year window from 860 Medicare patients. GPT-4o attained a weighted kappa score of 0.91 in comparison to specialist chart reviews and 0.96 on cases that the clinical adjudicators rated with high confidence. Our findings demonstrate GPT-4o's potential as a scalable chart review tool for creating research datasets and assisting diagnosis in clinical settings in the future.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models","url":"https://arxiv.org/abs/2502.09696","date":1739768400,"author":"","guid":1562,"unread":true,"content":"<article>arXiv:2502.09696v1 Announce Type: new \nAbstract: Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting images and, by some measures, have poorer spatial cognition than small children or animals. Despite this, they attain high scores on many popular visual benchmarks, with headroom rapidly eroded by an ongoing surge of model progress. To address this, there is a pressing need for difficult benchmarks that remain relevant for longer. We take this idea to its limit by introducing ZeroBench-a lightweight visual reasoning benchmark that is entirely impossible for contemporary frontier LMMs. Our benchmark consists of 100 manually curated questions and 334 less difficult subquestions. We evaluate 20 LMMs on ZeroBench, all of which score 0.0%, and rigorously analyse the errors. To encourage progress in visual understanding, we publicly release ZeroBench.</article>","contentLength":878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Power System Electromagnetic Transient Stability: an Analysis Based on Convergent Hamiltonian","url":"https://arxiv.org/abs/2502.09695","date":1739768400,"author":"","guid":1563,"unread":true,"content":"<article>arXiv:2502.09695v1 Announce Type: new \nAbstract: Transient stability is crucial to the reliable operation of power systems. Existing theories rely on the simplified electromechanical models, substituting the detailed electromagnetic dynamics of inductor and capacitor with their impedance representations. However, this simplification is inadequate for the growing penetration of fast-switching power electronic devices. Attempts to extend the existing theories to include electromagnetic dynamics lead to overly conservative stability conditions. To tackle this problem more directly, we study the condition under which the power source and dissipation in the electromagnetic dynamics tend to balance each other asymptotically. This is equivalent to the convergence of the Hamiltonian (total stored energy) and can be shown to imply transient stability. Using contraction analysis, we prove that this property holds for a large class of time-varying port-Hamiltonian systems with (i) constant damping matrix and (ii) strictly convex Hamiltonian. Then through port-Hamiltonian modeling of the electromagnetic dynamics, we obtain that the synchronized steady state of the power system is globally stable if it exists. This result provides new insights into the reliable operation of power systems. The proposed theory is illustrated in the simulation results of a two-machine system.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Ronaldo's a poser!\": How the Use of Generative AI Shapes Debates in Online Forums","url":"https://arxiv.org/abs/2502.09693","date":1739768400,"author":"","guid":1564,"unread":true,"content":"<article>arXiv:2502.09693v1 Announce Type: new \nAbstract: Online debates can enhance critical thinking but may escalate into hostile attacks. As humans are increasingly reliant on Generative AI (GenAI) in writing tasks, we need to understand how people utilize GenAI in online debates. To examine the patterns of writing behavior while making arguments with GenAI, we created an online forum for soccer fans to engage in turn-based and free debates in a post format with the assistance of ChatGPT, arguing on the topic of \"Messi vs Ronaldo\". After 13 sessions of two-part study and semi-structured interviews with 39 participants, we conducted content and thematic analyses to integrate insights from interview transcripts, ChatGPT records, and forum posts. We found that participants prompted ChatGPT for aggressive responses, created posts with similar content and logical fallacies, and sacrificed the use of ChatGPT for better human-human communication. This work uncovers how polarized forum members work with GenAI to engage in debates online.</article>","contentLength":1040,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NeuralCFD: Deep Learning on High-Fidelity Automotive Aerodynamics Simulations","url":"https://arxiv.org/abs/2502.09692","date":1739768400,"author":"","guid":1565,"unread":true,"content":"<article>arXiv:2502.09692v1 Announce Type: new \nAbstract: Recent advancements in neural operator learning are paving the way for transformative innovations in fields such as automotive aerodynamics. However, key challenges must be overcome before neural network-based simulation surrogates can be implemented at an industry scale. First, surrogates must become scalable to large surface and volume meshes, especially when using raw geometry inputs only, i.e., without relying on the simulation mesh. Second, surrogates must be trainable with a limited number of high-fidelity numerical simulation samples while still reaching the required performance levels. To this end, we introduce Geometry-preserving Universal Physics Transformer (GP-UPT), which separates geometry encoding and physics predictions, ensuring flexibility with respect to geometry representations and surface sampling strategies. GP-UPT enables independent scaling of the respective parts of the model according to practical requirements, offering scalable solutions to open challenges. GP-UPT circumvents the creation of high-quality simulation meshes, enables accurate 3D velocity field predictions at 20 million mesh cells, and excels in transfer learning from low-fidelity to high-fidelity simulation datasets, requiring less than half of the high-fidelity data to match the performance of models trained from scratch.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trust at Your Own Peril: A Mixed Methods Exploration of the Ability of Large Language Models to Generate Expert-Like Systems Engineering Artifacts and a Characterization of Failure Modes","url":"https://arxiv.org/abs/2502.09690","date":1739768400,"author":"","guid":1566,"unread":true,"content":"<article>arXiv:2502.09690v1 Announce Type: new \nAbstract: Multi-purpose Large Language Models (LLMs), a subset of generative Artificial Intelligence (AI), have recently made significant progress. While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet. To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts. This procedure was applied without any fine-tuning or calibration to document baseline LLM performance. We then adopted a two-fold mixed-methods approach to compare AI generated artifacts against the benchmark. First, we quantitatively compare the artifacts using natural language processing algorithms and find that when prompted carefully, the state-of-the-art algorithms cannot differentiate AI-generated artifacts from the human-expert benchmark. Second, we conduct a qualitative deep dive to investigate how they differ in terms of quality. We document that while the two-material appear very similar, AI generated artifacts exhibit serious failure modes that could be difficult to detect. We characterize these as: premature requirements definition, unsubstantiated numerical estimates, and propensity to overspecify. We contend that this study tells a cautionary tale about why the SE community must be more cautious adopting AI suggested feedback, at least when generated by multi-purpose LLMs.</article>","contentLength":1814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models and Provenance Metadata for Determining the Relevance of Images and Videos in News Stories","url":"https://arxiv.org/abs/2502.09689","date":1739768400,"author":"","guid":1567,"unread":true,"content":"<article>arXiv:2502.09689v1 Announce Type: new \nAbstract: The most effective misinformation campaigns are multimodal, often combining text with images and videos taken out of context -- or fabricating them entirely -- to support a given narrative. Contemporary methods for detecting misinformation, whether in deepfakes or text articles, often miss the interplay between multiple modalities. Built around a large language model, the system proposed in this paper addresses these challenges. It analyzes both the article's text and the provenance metadata of included images and videos to determine whether they are relevant. We open-source the system prototype and interactive web interface.</article>","contentLength":682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling","url":"https://arxiv.org/abs/2502.09688","date":1739768400,"author":"","guid":1568,"unread":true,"content":"<article>arXiv:2502.09688v1 Announce Type: new \nAbstract: Artificial intelligence (AI) is poised to transform healthcare by enabling personalized and efficient care through data-driven insights. Although radiology is at the forefront of AI adoption, in practice, the potential of AI models is often overshadowed by severe failures to generalize: AI models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch raises concerns that radiologists will be misled by incorrect AI predictions in practice and/or grow to distrust AI, rendering these promising technologies practically ineffectual. Exhaustive clinical trials of AI models on abundant and diverse data is thus critical to anticipate AI model degradation when encountering varied data samples. Achieving these goals, however, is challenging due to the high costs of collecting diverse data samples and corresponding annotations. To overcome these limitations, we introduce a novel conditional generative AI model designed for virtual clinical trials (VCTs) of radiology AI, capable of realistically synthesizing full-body CT images of patients with specified attributes. By learning the joint distribution of images and anatomical structures, our model enables precise replication of real-world patient populations with unprecedented detail at this scale. We demonstrate meaningful evaluation of radiology AI models through VCTs powered by our synthetic CT study populations, revealing model degradation and facilitating algorithmic auditing for bias-inducing data attributes. Our generative AI approach to VCTs is a promising avenue towards a scalable solution to assess model robustness, mitigate biases, and safeguard patient care by enabling simpler testing and evaluation of AI models in any desired range of diverse patient populations.</article>","contentLength":1882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mind What You Ask For: Emotional and Rational Faces of Persuasion by Large Language Models","url":"https://arxiv.org/abs/2502.09687","date":1739768400,"author":"","guid":1569,"unread":true,"content":"<article>arXiv:2502.09687v1 Announce Type: new \nAbstract: Be careful what you ask for, you just might get it. This saying fits with the way large language models (LLMs) are trained, which, instead of being rewarded for correctness, are increasingly rewarded for pleasing the recipient. So, they are increasingly effective at persuading us that their answers are valuable. But what tricks do they use in this persuasion? In this study, we examine what are the psycholinguistic features of the responses used by twelve different language models. By grouping response content according to rational or emotional prompts and exploring social influence principles employed by LLMs, we ask whether and how we can mitigate the risks of LLM-driven mass misinformation. We position this study within the broader discourse on human-centred AI, emphasizing the need for interdisciplinary approaches to mitigate cognitive and societal risks posed by persuasive AI responses.</article>","contentLength":952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Machine Learning and Deep Learning Techniques for Improved Pathological Staging of Prostate Cancer","url":"https://arxiv.org/abs/2502.09686","date":1739768400,"author":"","guid":1570,"unread":true,"content":"<article>arXiv:2502.09686v1 Announce Type: new \nAbstract: Prostate cancer (Pca) continues to be a leading cause of cancer-related mortality in men, and the limitations in precision of traditional diagnostic methods such as the Digital Rectal Exam (DRE), Prostate-Specific Antigen (PSA) testing, and biopsies underscore the critical importance of accurate staging detection in enhancing treatment outcomes and improving patient prognosis. This study leverages machine learning and deep learning approaches, along with feature selection and extraction methods, to enhance PCa pathological staging predictions using RNA sequencing data from The Cancer Genome Atlas (TCGA). Gene expression profiles from 486 tumors were analyzed using advanced algorithms, including Random Forest (RF), Logistic Regression (LR), Extreme Gradient Boosting (XGB), and Support Vector Machine (SVM). The performance of the study is measured with respect to the F1-score, as well as precision and recall, all of which are calculated as weighted averages. The results reveal that the highest test F1-score, approximately 83%, was achieved by the Random Forest algorithm, followed by Logistic Regression at 80%, while both Extreme Gradient Boosting (XGB) and Support Vector Machine (SVM) scored around 79%. Furthermore, deep learning models with data augmentation achieved an accuracy of 71. 23%, while PCA-based dimensionality reduction reached an accuracy of 69.86%. This research highlights the potential of AI-driven approaches in clinical oncology, paving the way for more reliable diagnostic tools that can ultimately improve patient outcomes.</article>","contentLength":1612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions","url":"https://arxiv.org/abs/2502.09685","date":1739768400,"author":"","guid":1571,"unread":true,"content":"<article>arXiv:2502.09685v1 Announce Type: new \nAbstract: Accurate demand forecasting is vital for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory, and distribution. However, forecasting contraceptive demand in developing countries presents challenges, including incomplete data, poor data quality, and the need to account for multiple geographical and product factors. Current methods often rely on simple forecasting techniques, which fail to capture demand uncertainties arising from these factors, warranting expert involvement. Our study aims to improve contraceptive demand forecasting by combining probabilistic forecasting methods with expert knowledge. We developed a hybrid model that combines point forecasts from domain-specific model with probabilistic distributions from statistical and machine learning approaches, enabling human input to fine-tune and enhance the system-generated forecasts. This approach helps address the uncertainties in demand and is particularly useful in resource-limited settings. We evaluate different forecasting methods, including time series, Bayesian, machine learning, and foundational time series methods alongside our new hybrid approach. By comparing these methods, we provide insights into their strengths, weaknesses, and computational requirements. Our research fills a gap in forecasting contraceptive demand and offers a practical framework that combines algorithmic and human expertise. Our proposed model can also be generalized to other humanitarian contexts with similar data patterns.</article>","contentLength":1591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Channel Dependence, Limited Lookback Windows, and the Simplicity of Datasets: How Biased is Time Series Forecasting?","url":"https://arxiv.org/abs/2502.09683","date":1739768400,"author":"","guid":1572,"unread":true,"content":"<article>arXiv:2502.09683v1 Announce Type: new \nAbstract: Time-series forecasting research has converged to a small set of datasets and a standardized collection of evaluation scenarios. Such a standardization is to a specific extent needed for comparable research. However, the underlying assumption is, that the considered setting is a representative for the problem as a whole. In this paper, we challenge this assumption and show that the current scenario gives a strongly biased perspective on the state of time-series forecasting research. To be more detailed, we show that the current evaluation scenario is heavily biased by the simplicity of the current datasets. We furthermore emphasize, that when the lookback-window is properly tuned, current models usually do not need any information flow across channels. However, when using more complex benchmark data, the situation changes: Here, modeling channel-interactions in a sophisticated manner indeed enhances performances. Furthermore, in this complex evaluation scenario, Crossformer, a method regularly neglected as an important baseline, is the SOTA method for time series forecasting. Based on this, we present the Fast Channel-dependent Transformer (FaCT), a simplified version of Crossformer which closes the runtime gap between Crossformer and TimeMixer, leading to an efficient model for complex forecasting datasets.</article>","contentLength":1378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Object-Centric Latent Action Learning","url":"https://arxiv.org/abs/2502.09680","date":1739768400,"author":"","guid":1573,"unread":true,"content":"<article>arXiv:2502.09680v1 Announce Type: new \nAbstract: Leveraging vast amounts of internet video data for Embodied AI is currently bottle-necked by the lack of action annotations and the presence of action-correlated distractors. We propose a novel object-centric latent action learning approach, based on VideoSaur and LAPO, that employs self-supervised decomposition of scenes into object representations and annotates video data with proxy-action labels. This method effectively disentangles causal agent-object interactions from irrelevant background noise and reduces the performance degradation of latent action learning approaches caused by distractors. Our preliminary experiments with the Distracting Control Suite show that latent action pretraining based on object decompositions improve the quality of inferred latent actions by x2.7 and efficiency of downstream fine-tuning with a small set of labeled actions, increasing return by x2.6 on average.</article>","contentLength":955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-level Conflict-Aware Network for Multi-modal Sentiment Analysis","url":"https://arxiv.org/abs/2502.09675","date":1739768400,"author":"","guid":1574,"unread":true,"content":"<article>arXiv:2502.09675v1 Announce Type: new \nAbstract: Multimodal Sentiment Analysis (MSA) aims to recognize human emotions by exploiting textual, acoustic, and visual modalities, and thus how to make full use of the interactions between different modalities is a central challenge of MSA. Interaction contains alignment and conflict aspects. Current works mainly emphasize alignment and the inherent differences between unimodal modalities, neglecting the fact that there are also potential conflicts between bimodal combinations. Additionally, multi-task learning-based conflict modeling methods often rely on the unstable generated labels. To address these challenges, we propose a novel multi-level conflict-aware network (MCAN) for multimodal sentiment analysis, which progressively segregates alignment and conflict constituents from unimodal and bimodal representations, and further exploits the conflict constituents with the conflict modeling branch. In the conflict modeling branch, we conduct discrepancy constraints at both the representation and predicted output levels, avoiding dependence on the generated labels. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed MCAN.</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety Analysis","url":"https://arxiv.org/abs/2502.09674","date":1739768400,"author":"","guid":1575,"unread":true,"content":"<article>arXiv:2502.09674v1 Announce Type: new \nAbstract: Large Language Models' safety-aligned behaviors, such as refusing harmful queries, can be represented by linear directions in activation space. Previous research modeled safety behavior with a single direction, limiting mechanistic understanding to an isolated safety feature. In this work, we discover that safety-aligned behavior is jointly controlled by multi-dimensional directions. Namely, we study the vector space of representation shifts during safety fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal directions in the space, we first find that a dominant direction governs the model's refusal behavior, while multiple smaller directions represent distinct and interpretable features like hypothetical narrative and role-playing. We then measure how different directions promote or suppress the dominant direction, showing the important role of secondary directions in shaping the model's refusal representation. Finally, we demonstrate that removing certain trigger tokens in harmful queries can mitigate these directions to bypass the learned safety capability, providing new insights on understanding safety alignment vulnerability from a multi-dimensional perspective. Code and artifacts are available at https://github.com/BMPixel/safety-residual-space.</article>","contentLength":1340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning","url":"https://arxiv.org/abs/2502.09673","date":1739768400,"author":"","guid":1576,"unread":true,"content":"<article>arXiv:2502.09673v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable success across various NLP benchmarks. However, excelling in complex tasks that require nuanced reasoning and precise decision-making demands more than raw language proficiency--LLMs must reason, i.e., think logically, draw from past experiences, and synthesize information to reach conclusions and take action. To enhance reasoning abilities, approaches such as prompting and fine-tuning have been widely explored. While these methods have led to clear improvements in reasoning, their impact on LLM safety remains less understood. In this work, we investigate the interplay between reasoning and safety in LLMs. We highlight the latent safety risks that arise as reasoning capabilities improve, shedding light on previously overlooked vulnerabilities. At the same time, we explore how reasoning itself can be leveraged to enhance safety, uncovering potential mitigation strategies. By examining both the risks and opportunities in reasoning-driven LLM safety, our study provides valuable insights for developing models that are not only more capable but also more trustworthy in real-world deployments.</article>","contentLength":1211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter","url":"https://arxiv.org/abs/2502.09672","date":1739768400,"author":"","guid":1577,"unread":true,"content":"<article>arXiv:2502.09672v1 Announce Type: new \nAbstract: 3D Multi-Object Tracking (MOT) provides the trajectories of surrounding objects, assisting robots or vehicles in smarter path planning and obstacle avoidance. Existing 3D MOT methods based on the Tracking-by-Detection framework typically use a single motion model to track an object throughout its entire tracking process. However, objects may change their motion patterns due to variations in the surrounding environment. In this paper, we introduce the Interacting Multiple Model filter in IMM-MOT, which accurately fits the complex motion patterns of individual objects, overcoming the limitation of single-model tracking in existing approaches. In addition, we incorporate a Damping Window mechanism into the trajectory lifecycle management, leveraging the continuous association status of trajectories to control their creation and termination, reducing the occurrence of overlooked low-confidence true targets. Furthermore, we propose the Distance-Based Score Enhancement module, which enhances the differentiation between false positives and true positives by adjusting detection scores, thereby improving the effectiveness of the Score Filter. On the NuScenes Val dataset, IMM-MOT outperforms most other single-modal models using 3D point clouds, achieving an AMOTA of 73.8%. Our project is available at https://github.com/Ap01lo/IMM-MOT.</article>","contentLength":1395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Science of Evaluating Foundation Models","url":"https://arxiv.org/abs/2502.09670","date":1739768400,"author":"","guid":1578,"unread":true,"content":"<article>arXiv:2502.09670v1 Announce Type: new \nAbstract: The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) Formalizing the Evaluation Process by providing a structured framework tailored to specific use-case contexts, (2) Offering Actionable Tools and Frameworks such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) Surveying Recent Work with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.</article>","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta-INR: Efficient Encoding of Volumetric Data via Meta-Learning Implicit Neural Representation","url":"https://arxiv.org/abs/2502.09669","date":1739768400,"author":"","guid":1579,"unread":true,"content":"<article>arXiv:2502.09669v1 Announce Type: new \nAbstract: Implicit neural representation (INR) has emerged as a promising solution for encoding volumetric data, offering continuous representations and seamless compatibility with the volume rendering pipeline. However, optimizing an INR network from randomly initialized parameters for each new volume is computationally inefficient, especially for large-scale time-varying or ensemble volumetric datasets where volumes share similar structural patterns but require independent training. To close this gap, we propose Meta-INR, a pretraining strategy adapted from meta-learning algorithms to learn initial INR parameters from partial observation of a volumetric dataset. Compared to training an INR from scratch, the learned initial parameters provide a strong prior that enhances INR generalizability, allowing significantly faster convergence with just a few gradient updates when adapting to a new volume and better interpretability when analyzing the parameters of the adapted INRs. We demonstrate that Meta-INR can effectively extract high-quality generalizable features that help encode unseen similar volume data across diverse datasets. Furthermore, we highlight its utility in tasks such as simulation parameter analysis and representative timestep selection. The code is available at https://github.com/spacefarers/MetaINR.</article>","contentLength":1374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"k-LLMmeans: Summaries as Centroids for Interpretable and Scalable LLM-Based Text Clustering","url":"https://arxiv.org/abs/2502.09667","date":1739768400,"author":"","guid":1580,"unread":true,"content":"<article>arXiv:2502.09667v1 Announce Type: new \nAbstract: We introduce k-LLMmeans, a novel modification of the k-means clustering algorithm that utilizes LLMs to generate textual summaries as cluster centroids, thereby capturing contextual and semantic nuances often lost when relying on purely numerical means of document embeddings. This modification preserves the properties of k-means while offering greater interpretability: the cluster centroid is represented by an LLM-generated summary, whose embedding guides cluster assignments. We also propose a mini-batch variant, enabling efficient online clustering for streaming text data and providing real-time interpretability of evolving cluster centroids. Through extensive simulations, we show that our methods outperform vanilla k-means on multiple metrics while incurring only modest LLM usage that does not scale with dataset size. Finally, We present a case study showcasing the interpretability of evolving cluster centroids in sequential text streams. As part of our evaluation, we compile a new dataset from StackExchange, offering a benchmark for text-stream clustering.</article>","contentLength":1124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revealing Subtle Phenotypes in Small Microscopy Datasets Using Latent Diffusion Models","url":"https://arxiv.org/abs/2502.09665","date":1739768400,"author":"","guid":1581,"unread":true,"content":"<article>arXiv:2502.09665v1 Announce Type: new \nAbstract: Identifying subtle phenotypic variations in cellular images is critical for advancing biological research and accelerating drug discovery. These variations are often masked by the inherent cellular heterogeneity, making it challenging to distinguish differences between experimental conditions. Recent advancements in deep generative models have demonstrated significant potential for revealing these nuanced phenotypes through image translation, opening new frontiers in cellular and molecular biology as well as the identification of novel biomarkers. Among these generative models, diffusion models stand out for their ability to produce high-quality, realistic images. However, training diffusion models typically requires large datasets and substantial computational resources, both of which can be limited in biological research. In this work, we propose a novel approach that leverages pre-trained latent diffusion models to uncover subtle phenotypic changes. We validate our approach qualitatively and quantitatively on several small datasets of microscopy images. Our findings reveal that our approach enables effective detection of phenotypic variations, capturing both visually apparent and imperceptible differences. Ultimately, our results highlight the promising potential of this approach for phenotype detection, especially in contexts constrained by limited data and computational capacity.</article>","contentLength":1456,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Image Super-Resolution with Guarantees via Conformal Generative Models","url":"https://arxiv.org/abs/2502.09664","date":1739768400,"author":"","guid":1582,"unread":true,"content":"<article>arXiv:2502.09664v1 Announce Type: new \nAbstract: The increasing use of generative ML foundation models for image super-resolution calls for robust and interpretable uncertainty quantification methods. We address this need by presenting a novel approach based on conformal prediction techniques to create a \"confidence mask\" capable of reliably and intuitively communicating where the generated image can be trusted. Our method is adaptable to any black-box generative model, including those locked behind an opaque API, requires only easily attainable data for calibration, and is highly customizable via the choice of a local image similarity metric. We prove strong theoretical guarantees for our method that span fidelity error control (according to our local image similarity metric), reconstruction quality, and robustness in the face of data leakage. Finally, we empirically evaluate these results and establish our method's solid performance.</article>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiffEx: Explaining a Classifier with Diffusion Models to Identify Microscopic Cellular Variations","url":"https://arxiv.org/abs/2502.09663","date":1739768400,"author":"","guid":1583,"unread":true,"content":"<article>arXiv:2502.09663v1 Announce Type: new \nAbstract: In recent years, deep learning models have been extensively applied to biological data across various modalities. Discriminative deep learning models have excelled at classifying images into categories (e.g., healthy versus diseased, treated versus untreated). However, these models are often perceived as black boxes due to their complexity and lack of interpretability, limiting their application in real-world biological contexts. In biological research, explainability is essential: understanding classifier decisions and identifying subtle differences between conditions are critical for elucidating the effects of treatments, disease progression, and biological processes. To address this challenge, we propose DiffEx, a method for generating visually interpretable attributes to explain classifiers and identify microscopic cellular variations between different conditions. We demonstrate the effectiveness of DiffEx in explaining classifiers trained on natural and biological images. Furthermore, we use DiffEx to uncover phenotypic differences within microscopy datasets. By offering insights into cellular variations through classifier explanations, DiffEx has the potential to advance the understanding of diseases and aid drug discovery by identifying novel biomarkers.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SIToBI - A Speech Prosody Annotation Tool for Indian Languages","url":"https://arxiv.org/abs/2502.09661","date":1739768400,"author":"","guid":1584,"unread":true,"content":"<article>arXiv:2502.09661v1 Announce Type: new \nAbstract: The availability of prosodic information from speech signals is useful in a wide range of applications. However, deriving this information from speech signals can be a laborious task involving manual intervention. Therefore, the current work focuses on developing a tool that can provide prosodic annotations corresponding to a given speech signal, particularly for Indian languages. The proposed Segmentation with Intensity, Tones and Break Indices (SIToBI) tool provides time-aligned phoneme, syllable, and word transcriptions, syllable-level pitch contour annotations, break indices, and syllable-level relative intensity indices. The tool focuses more on syllable-level annotations since Indian languages are syllable-timed. Indians, regardless of the language they speak, may exhibit influences from other languages. As a result, other languages spoken in India may also exhibit syllable-timed characteristics. The accuracy of the annotations derived from the tool is analyzed by comparing them against manual annotations and the tool is observed to perform well. While the current work focuses on three languages, namely, Tamil, Hindi, and Indian English, the tool can easily be extended to other Indian languages and possibly other syllable-timed languages as well.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Fine-grained Interactive Segmentation in Images and Videos","url":"https://arxiv.org/abs/2502.09660","date":1739768400,"author":"","guid":1585,"unread":true,"content":"<article>arXiv:2502.09660v1 Announce Type: new \nAbstract: The recent Segment Anything Models (SAMs) have emerged as foundational visual models for general interactive segmentation. Despite demonstrating robust generalization abilities, they still suffer performance degradations in scenarios demanding accurate masks. Existing methods for high-precision interactive segmentation face a trade-off between the ability to perceive intricate local details and maintaining stable prompting capability, which hinders the applicability and effectiveness of foundational segmentation models. To this end, we present an SAM2Refiner framework built upon the SAM2 backbone. This architecture allows SAM2 to generate fine-grained segmentation masks for both images and videos while preserving its inherent strengths. Specifically, we design a localization augment module, which incorporates local contextual cues to enhance global features via a cross-attention mechanism, thereby exploiting potential detailed patterns and maintaining semantic information. Moreover, to strengthen the prompting ability toward the enhanced object embedding, we introduce a prompt retargeting module to renew the embedding with spatially aligned prompt features. In addition, to obtain accurate high resolution segmentation masks, a mask refinement module is devised by employing a multi-scale cascaded structure to fuse mask features with hierarchical representations from the encoder. Extensive experiments demonstrate the effectiveness of our approach, revealing that the proposed method can produce highly precise masks for both images and videos, surpassing state-of-the-art methods.</article>","contentLength":1650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models","url":"https://arxiv.org/abs/2502.09659","date":1739768400,"author":"","guid":1586,"unread":true,"content":"<article>arXiv:2502.09659v1 Announce Type: new \nAbstract: Motivation: An adjuvant is a chemical incorporated into vaccines that enhances their efficacy by improving the immune response. Identifying adjuvant names from cancer vaccine studies is essential for furthering research and enhancing immunotherapies. However, the manual curation from the constantly expanding biomedical literature poses significant challenges. This study explores the automated recognition of vaccine adjuvant names using Large Language Models (LLMs), specifically Generative Pretrained Transformers (GPT) and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97 clinical trial records from AdjuvareDB and 290 abstracts annotated with the Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in zero-shot and few-shot learning paradigms with up to four examples per prompt. Prompts explicitly targeted adjuvant names, testing the impact of contextual information such as substances or interventions. Outputs underwent automated and manual validation for accuracy and consistency. Results: GPT-4o attained 100% Precision across all situations while exhibiting notable improve in Recall and F1-scores, particularly with incorporating interventions. On the VAC dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions, surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o reached an F1-score of 81.67% for three-shot prompting with interventions, surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings demonstrate that LLMs excel at identifying adjuvant names, including rare variations of naming representation. This study emphasizes the capability of LLMs to enhance cancer vaccine development by efficiently extracting insights. Future work aims to broaden the framework to encompass various biomedical literature and enhance model generalizability across various vaccines and adjuvants.</article>","contentLength":1961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep Learning to Enhance Question Answering Quality","url":"https://arxiv.org/abs/2502.09658","date":1739768400,"author":"","guid":1587,"unread":true,"content":"<article>arXiv:2502.09658v1 Announce Type: new \nAbstract: Knowledge representation and reasoning are critical challenges in Artificial Intelligence (AI), particularly in integrating neural and symbolic approaches to achieve explainable and transparent AI systems. Traditional knowledge representation methods often fall short of capturing complex processes and state changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a specialization of the neuro-symbolic AI approach that integrates conceptual modeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep learning to enhance question-answering (QA) quality. By converting natural language text into OPM models using in-context learning, NCAI leverages the expressive power of OPM to represent complex OPM elements-processes, objects, and states-beyond what traditional triplet-based knowledge graphs can easily capture. This rich structured knowledge representation improves reasoning transparency and answer accuracy in an OPM-QA system. We further propose transparency evaluation metrics to quantitatively measure how faithfully the predicted reasoning aligns with OPM-based conceptual logic. Our experiments demonstrate that NCAI outperforms traditional methods, highlighting its potential for advancing neuro-symbolic AI by providing rich knowledge representations, measurable transparency, and improved reasoning.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Spatiotemporal Vision Transformer into Digital Twins for High-Resolution Heat Stress Forecasting in Campus Environments","url":"https://arxiv.org/abs/2502.09657","date":1739768400,"author":"","guid":1588,"unread":true,"content":"<article>arXiv:2502.09657v1 Announce Type: new \nAbstract: Extreme heat events exacerbated by climate change pose significant challenges to urban resilience and planning. This study introduces a climate-responsive digital twin framework integrating the Spatiotemporal Vision Transformer (ST-ViT) model to enhance heat stress forecasting and decision-making. Using a Texas campus as a testbed, we synthesized high-resolution physical model simulations with spatial and meteorological data to develop fine-scale human thermal predictions. The ST-ViT-powered digital twin enables efficient, data-driven insights for planners, policymakers, and campus stakeholders, supporting targeted heat mitigation strategies and advancing climate-adaptive urban design.</article>","contentLength":743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bidirectional Diffusion Bridge Models","url":"https://arxiv.org/abs/2502.09655","date":1739768400,"author":"","guid":1589,"unread":true,"content":"<article>arXiv:2502.09655v1 Announce Type: new \nAbstract: Diffusion bridges have shown potential in paired image-to-image (I2I) translation tasks. However, existing methods are limited by their unidirectional nature, requiring separate models for forward and reverse translations. This not only doubles the computational cost but also restricts their practicality. In this work, we introduce the Bidirectional Diffusion Bridge Model (BDBM), a scalable approach that facilitates bidirectional translation between two coupled distributions using a single network. BDBM leverages the Chapman-Kolmogorov Equation for bridges, enabling it to model data distribution shifts across timesteps in both forward and backward directions by exploiting the interchangeability of the initial and target timesteps within this framework. Notably, when the marginal distribution given endpoints is Gaussian, BDBM's transition kernels in both directions possess analytical forms, allowing for efficient learning with a single network. We demonstrate the connection between BDBM and existing bridge methods, such as Doob's h-transform and variational approaches, and highlight its advantages. Extensive experiments on high-resolution I2I translation tasks demonstrate that BDBM not only enables bidirectional translation with minimal additional cost but also outperforms state-of-the-art bridge models. Our source code is available at [https://github.com/kvmduc/BDBM||https://github.com/kvmduc/BDBM].</article>","contentLength":1471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing","url":"https://arxiv.org/abs/2502.09652","date":1739768400,"author":"","guid":1590,"unread":true,"content":"<article>arXiv:2502.09652v1 Announce Type: new \nAbstract: This paper introduces a data-driven algorithm for modeling and compensating shape deviations in additive manufacturing (AM), addressing challenges in geometric accuracy and batch production. While traditional methods, such as analytical models and metrology, laid the groundwork for geometric precision, they are often impractical for large-scale production. Recent advancements in machine learning (ML) have improved compensation precision, but issues remain in generalizing across complex geometries and adapting to position-dependent variations. We present a novel approach for powder bed fusion (PBF) processes, using GraphCompNet, which is a computational framework combining graph-based neural networks with a generative adversarial network (GAN)-inspired training process. By leveraging point cloud data and dynamic graph convolutional neural networks (DGCNNs), GraphCompNet models complex shapes and incorporates position-specific thermal and mechanical factors. A two-stage adversarial training procedure iteratively refines compensated designs via a compensator-predictor architecture, offering real-time feedback and optimization. Experimental validation across diverse shapes and positions shows the framework significantly improves compensation accuracy (35 to 65 percent) across the entire print space, adapting to position-dependent variations. This work advances the development of Digital Twin technology for AM, enabling scalable, real-time monitoring and compensation, and addressing critical gaps in AM process control. The proposed method supports high-precision, automated industrial-scale design and manufacturing systems.</article>","contentLength":1694,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-VERDE: A Gateway for Egalitarian Access to Large Language Model-Based Resources For Educational Institutions","url":"https://arxiv.org/abs/2502.09651","date":1739768400,"author":"","guid":1591,"unread":true,"content":"<article>arXiv:2502.09651v1 Announce Type: new \nAbstract: We present AI-VERDE, a unified LLM-as-a-platform service designed to facilitate seamless integration of commercial, cloud-hosted, and on-premise open LLMs in academic settings. AI-VERDE streamlines access management for instructional and research groups by providing features such as robust access control, privacy-preserving mechanisms, native Retrieval-Augmented Generation (RAG) support, budget management for third-party LLM services, and both a conversational web interface and API access. In a pilot deployment at a large public university, AI-VERDE demonstrated significant engagement across diverse educational and research groups, enabling activities that would typically require substantial budgets for commercial LLM services with limited user and team management capabilities. To the best of our knowledge, AI-Verde is the first platform to address both academic and research needs for LLMs within an higher education institutional framework.</article>","contentLength":1003,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples","url":"https://arxiv.org/abs/2502.09650","date":1739768400,"author":"","guid":1592,"unread":true,"content":"<article>arXiv:2502.09650v1 Announce Type: new \nAbstract: The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Imit Diff: Semantics Guided Diffusion Transformer with Dual Resolution Fusion for Imitation Learning","url":"https://arxiv.org/abs/2502.09649","date":1739768400,"author":"","guid":1593,"unread":true,"content":"<article>arXiv:2502.09649v1 Announce Type: new \nAbstract: Visuomotor imitation learning enables embodied agents to effectively acquire manipulation skills from video demonstrations and robot proprioception. However, as scene complexity and visual distractions increase, existing methods that perform well in simple scenes tend to degrade in performance. To address this challenge, we introduce Imit Diff, a semanstic guided diffusion transformer with dual resolution fusion for imitation learning. Our approach leverages prior knowledge from vision language foundation models to translate high-level semantic instruction into pixel-level visual localization. This information is explicitly integrated into a multi-scale visual enhancement framework, constructed with a dual resolution encoder. Additionally, we introduce an implementation of Consistency Policy within the diffusion transformer architecture to improve both real-time performance and motion smoothness in embodied agent control.We evaluate Imit Diff on several challenging real-world tasks. Due to its task-oriented visual localization and fine-grained scene perception, it significantly outperforms state-of-the-art methods, especially in complex scenes with visual distractions, including zero-shot experiments focused on visual distraction and category generalization. The code will be made publicly available.</article>","contentLength":1369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UKTA: Unified Korean Text Analyzer","url":"https://arxiv.org/abs/2502.09648","date":1739768400,"author":"","guid":1594,"unread":true,"content":"<article>arXiv:2502.09648v1 Announce Type: new \nAbstract: Evaluating writing quality is complex and time-consuming often delaying feedback to learners. While automated writing evaluation tools are effective for English, Korean automated writing evaluation tools face challenges due to their inability to address multi-view analysis, error propagation, and evaluation explainability. To overcome these challenges, we introduce UKTA (Unified Korean Text Analyzer), a comprehensive Korea text analysis and writing evaluation system. UKTA provides accurate low-level morpheme analysis, key lexical features for mid-level explainability, and transparent high-level rubric-based writing scores. Our approach enhances accuracy and quadratic weighted kappa over existing baseline, positioning UKTA as a leading multi-perspective tool for Korean text analysis and writing evaluation.</article>","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unveiling Simplicities of Attention: Adaptive Long-Context Head Identification","url":"https://arxiv.org/abs/2502.09647","date":1739768400,"author":"","guid":1595,"unread":true,"content":"<article>arXiv:2502.09647v1 Announce Type: new \nAbstract: The ability to process long contexts is crucial for many natural language processing tasks, yet it remains a significant challenge. While substantial progress has been made in enhancing the efficiency of attention mechanisms, there is still a gap in understanding how attention heads function in long-context settings. In this paper, we observe that while certain heads consistently attend to local information only, others swing between attending to local and long-context information depending on the query. This raises the question: can we identify which heads require long-context information to predict the next token accurately? We demonstrate that it's possible to predict which heads are crucial for long-context processing using only local keys. The core idea here is to exploit a simple model for the long-context scores via second moment approximations. These findings unveil simple properties of attention in the context of long sequences, and open the door to potentially significant gains in efficiency.</article>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Shift or Maintenance? An Intergenerational Study of the Tibetan Community in Saudi Arabia","url":"https://arxiv.org/abs/2502.09646","date":1739768400,"author":"","guid":1596,"unread":true,"content":"<article>arXiv:2502.09646v1 Announce Type: new \nAbstract: The present study provides the first-ever report on the language shift from Tibetan to Arabic among descendants of Tibetan families who migrated from the Tibet region to Saudi Arabia around 70 years ago. The aim of this study was to determine whether three age groups had adopted different practices in terms of maintaining Tibetan or shifting to Hijazi Arabic. To this end, 96 male and female members of the Tibetan community responded to a questionnaire in which they were asked about their code choice in different domains (home, neighbourhood, friends and relatives, expressing emotion, and performing religious rituals). The data revealed significant intergenerational differences between members of the community in terms of the extent of the shift to Arabic, with Tibetan rarely used by younger members and older members making only slightly more use of it. The difference between the three age groups was significant, at a p-value of .001.</article>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From No to Know: Taxonomy, Challenges, and Opportunities for Negation Understanding in Multimodal Foundation Models","url":"https://arxiv.org/abs/2502.09645","date":1739768400,"author":"","guid":1597,"unread":true,"content":"<article>arXiv:2502.09645v1 Announce Type: new \nAbstract: Negation, a linguistic construct conveying absence, denial, or contradiction, poses significant challenges for multilingual multimodal foundation models. These models excel in tasks like machine translation, text-guided generation, image captioning, audio interactions, and video processing but often struggle to accurately interpret negation across diverse languages and cultural contexts. In this perspective paper, we propose a comprehensive taxonomy of negation constructs, illustrating how structural, semantic, and cultural factors influence multimodal foundation models. We present open research questions and highlight key challenges, emphasizing the importance of addressing these issues to achieve robust negation handling. Finally, we advocate for specialized benchmarks, language-specific tokenization, fine-grained attention mechanisms, and advanced multimodal architectures. These strategies can foster more adaptable and semantically precise multimodal foundation models, better equipped to navigate and accurately interpret the complexities of negation in multilingual, multimodal environments.</article>","contentLength":1159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis","url":"https://arxiv.org/abs/2502.09644","date":1739768400,"author":"","guid":1598,"unread":true,"content":"<article>arXiv:2502.09644v1 Announce Type: new \nAbstract: Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in - as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer's stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.</article>","contentLength":1413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Krutrim LLM: Multilingual Foundational Model for over a Billion People","url":"https://arxiv.org/abs/2502.09642","date":1739768400,"author":"","guid":1599,"unread":true,"content":"<article>arXiv:2502.09642v1 Announce Type: new \nAbstract: India is a diverse society with unique challenges in developing AI systems, including linguistic diversity, oral traditions, data accessibility, and scalability. Existing foundation models are primarily trained on English, limiting their effectiveness for India's population. Indic languages comprise only 1 percent of Common Crawl corpora despite India representing 18 percent of the global population, leading to linguistic biases. Thousands of regional languages, dialects, and code mixing create additional representation challenges due to sparse training data.\n  We introduce Krutrim LLM, a 2 trillion token multilingual model designed for India's linguistic landscape. It incorporates the largest known Indic dataset, mitigating data scarcity and ensuring balanced performance across dialects. Krutrim outperforms or matches state-of-the-art models on Indic benchmarks while maintaining competitive English performance. Despite being significantly smaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2 on 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This evidences Krutrim's flexible multilingual fluency across diverse linguistic contexts.\n  Krutrim is integrated with real-time search to improve factual accuracy in conversational AI applications. This enhances accessibility for over 1 billion users worldwide. Through intentional design choices addressing data imbalances, Krutrim LLM signifies meaningful progress in building ethical, globally representative AI models.</article>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Social Support Detection in Spanish Social Media Texts","url":"https://arxiv.org/abs/2502.09640","date":1739768400,"author":"","guid":1600,"unread":true,"content":"<article>arXiv:2502.09640v1 Announce Type: new \nAbstract: The advent of social media has transformed communication, enabling individuals to share their experiences, seek support, and participate in diverse discussions. While extensive research has focused on identifying harmful content like hate speech, the recognition and promotion of positive and supportive interactions remain largely unexplored. This study proposes an innovative approach to detecting online social support in Spanish-language social media texts. We introduce the first annotated dataset specifically created for this task, comprising 3,189 YouTube comments classified as supportive or non-supportive. To address data imbalance, we employed GPT-4o to generate paraphrased comments and create a balanced dataset. We then evaluated social support classification using traditional machine learning models, deep learning architectures, and transformer-based models, including GPT-4o, but only on the unbalanced dataset. Subsequently, we utilized a transformer model to compare the performance between the balanced and unbalanced datasets. Our findings indicate that the balanced dataset yielded improved results for Task 2 (Individual and Group) and Task 3 (Nation, Other, LGBTQ, Black Community, Women, Religion), whereas GPT-4o performed best for Task 1 (Social Support and Non-Support). This study highlights the significance of fostering a supportive online environment and lays the groundwork for future research in automated social support detection.</article>","contentLength":1516,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jailbreaking to Jailbreak","url":"https://arxiv.org/abs/2502.09638","date":1739768400,"author":"","guid":1601,"unread":true,"content":"<article>arXiv:2502.09638v1 Announce Type: new \nAbstract: Refusal training on Large Language Models (LLMs) prevents harmful outputs, yet this defense remains vulnerable to both automated and human-crafted jailbreaks. We present a novel LLM-as-red-teamer approach in which a human jailbreaks a refusal-trained LLM to make it willing to jailbreak itself or other LLMs. We refer to the jailbroken LLMs as $J_2$ attackers, which can systematically evaluate target models using various red teaming strategies and improve its performance via in-context learning from the previous failures. Our experiments demonstrate that Sonnet 3.5 and Gemini 1.5 pro outperform other LLMs as $J_2$, achieving 93.0% and 91.0% attack success rates (ASRs) respectively against GPT-4o (and similar results across other capable LLMs) on Harmbench. Our work not only introduces a scalable approach to strategic red teaming, drawing inspiration from human red teamers, but also highlights jailbreaking-to-jailbreak as an overlooked failure mode of the safeguard. Specifically, an LLM can bypass its own safeguards by employing a jailbroken version of itself that is willing to assist in further jailbreaking. To prevent any direct misuse with $J_2$, while advancing research in AI safety, we publicly share our methodology while keeping specific prompting details private.</article>","contentLength":1336,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness","url":"https://arxiv.org/abs/2502.09637","date":1739768400,"author":"","guid":1602,"unread":true,"content":"<article>arXiv:2502.09637v1 Announce Type: new \nAbstract: Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, \"culture\" is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess \"cultural awareness\", and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?","url":"https://arxiv.org/abs/2502.09636","date":1739768400,"author":"","guid":1603,"unread":true,"content":"<article>arXiv:2502.09636v1 Announce Type: new \nAbstract: In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83\\% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: https://github.com/sougata-ub/reading_between_lines</article>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CORRECT: Context- and Reference-Augmented Reasoning and Prompting for Fact-Checking","url":"https://arxiv.org/abs/2502.09635","date":1739768400,"author":"","guid":1604,"unread":true,"content":"<article>arXiv:2502.09635v1 Announce Type: new \nAbstract: Fact-checking the truthfulness of claims usually requires reasoning over multiple evidence sentences. Oftentimes, evidence sentences may not be always self-contained, and may require additional contexts and references from elsewhere to understand coreferential expressions, acronyms, and the scope of a reported finding. For example, evidence sentences from an academic paper may need contextual sentences in the paper and descriptions in its cited papers to determine the scope of a research discovery. However, most fact-checking models mainly focus on the reasoning within evidence sentences, and ignore the auxiliary contexts and references. To address this problem, we propose a novel method, Context- and Reference-augmented Reasoning and Prompting. For evidence reasoning, we construct a three-layer evidence graph with evidence, context, and reference layers. We design intra- and cross-layer reasoning to integrate three graph layers into a unified evidence embedding. For verdict prediction, we design evidence-conditioned prompt encoder, which produces unique prompt embeddings for each claim. These evidence-conditioned prompt embeddings and claims are unified for fact-checking. Experiments verify the strength of our model.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient and Trustworthy Block Propagation for Blockchain-enabled Mobile Embodied AI Networks: A Graph Resfusion Approach","url":"https://arxiv.org/abs/2502.09624","date":1739768400,"author":"","guid":1605,"unread":true,"content":"<article>arXiv:2502.09624v1 Announce Type: new \nAbstract: By synergistically integrating mobile networks and embodied artificial intelligence (AI), Mobile Embodied AI Networks (MEANETs) represent an advanced paradigm that facilitates autonomous, context-aware, and interactive behaviors within dynamic environments. Nevertheless, the rapid development of MEANETs is accompanied by challenges in trustworthiness and operational efficiency. Fortunately, blockchain technology, with its decentralized and immutable characteristics, offers promising solutions for MEANETs. However, existing block propagation mechanisms suffer from challenges such as low propagation efficiency and weak security for block propagation, which results in delayed transmission of vehicular messages or vulnerability to malicious tampering, potentially causing severe traffic accidents in blockchain-enabled MEANETs. Moreover, current block propagation strategies cannot effectively adapt to real-time changes of dynamic topology in MEANETs. Therefore, in this paper, we propose a graph Resfusion model-based trustworthy block propagation optimization framework for consortium blockchain-enabled MEANETs. Specifically, we propose an innovative trust calculation mechanism based on the trust cloud model, which comprehensively accounts for randomness and fuzziness in the miner trust evaluation. Furthermore, by leveraging the strengths of graph neural networks and diffusion models, we develop a graph Resfusion model to effectively and adaptively generate the optimal block propagation trajectory. Simulation results demonstrate that the proposed model outperforms other routing mechanisms in terms of block propagation efficiency and trustworthiness. Additionally, the results highlight its strong adaptability to dynamic environments, making it particularly suitable for rapidly changing MEANETs.</article>","contentLength":1865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build AI Agents That Speak","url":"https://dev.to/hammad_ahmad_89181/build-ai-agents-that-speak-29el","date":1739767366,"author":"Hammad Ahmad","guid":1608,"unread":true,"content":"<p>Alright Lets Talk About Eleven Lab’s Latest Release, Meet Conversational AI.</p><p>They’ve Build A Platform That Lets You Deploy Voice Agents. That’s A Massive Productivity Unlock. It’s Not Just About Having A Voice Agent; It’s About Having A Agents That Can Actually Hold A Conversation With Low Latency, Interruption Handling &amp; Advance Turn Taking.</p><p>This Isn’t Your Basic Chatbot. It Integrates With Any LLM Like GPT, Gemeni, Claude &amp; More Also If You’ve Got A Custom Model, You Can Plug That In Too. This Flexibility Is Critical Because The Conversational AI Space Is Evolving Fast. You Don’t Want To Be Locked Into One Ecosystem.</p><p>Here’s Is Why This Is The Game Changer: You Can Scale Customer Support, Automate Sales Calls, &amp; Provide Personalized Education All While Maintaining A Consistent, High Quality Voice &amp; Enhance Listening Experience.</p><p>It’s Not Just For English. This Thing Support 31 Languages. You Can Build Multilingual Agents That Sounds Natural.</p>","contentLength":971,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streamlining Incident Response: How AI can reduce on call engineer's burden","url":"https://dev.to/aarthirocks/streamlining-incident-response-how-ai-can-reduce-on-call-engineers-burden-145p","date":1739767331,"author":"Aarthi Anbalagan","guid":1607,"unread":true,"content":"<p>With around 15 years of experience in software engineering, primarily in the data and AI space, I have worked extensively on large-scale systems, monitoring solutions, and AI-driven automation. At Microsoft, I have been deeply involved in big data, telemetry and observability, leading efforts to improve system reliability and operational efficiency. My expertise spans data engineering, AI, machine learning, and open telemetry, and I am passionate about leveraging emerging technologies to optimize workflows. Having witnessed firsthand the challenges of incident management and the strain it places on on-call engineers, I see AI as a game-changer in streamlining incident response. You can learn more about me <a href=\"https://www.linkedin.com/in/aarthian/\" rel=\"noopener noreferrer\">here</a>.</p><p>In this blog, I’ll explore how agentic AI can reduce the on-call burden by automating critical steps in issue diagnosis and resolution. While I have implemented some of this at my current company, I'm sharing generic information on all the possibilities using Agentic AI, without sharing anything proprietary. </p><h2>\n  \n  \n  Incidents or Support tickets\n</h2><p>In today's fast-paced digital landscape, on-call engineers play a pivotal role in maintaining system reliability and swiftly addressing incidents. However, the traditional workflow—from customer support identifying an issue to engineers diagnosing and resolving it—often involves multiple back-and-forth communications, leading to delays and increased workloads. Enter AI: autonomous systems capable of making decisions and performing tasks without human intervention. By integrating AI into incident response processes, organizations can streamline operations, reduce on-call burdens, and enhance overall efficiency.</p><h2>\n  \n  \n  The Traditional Incident Response Workflow\n</h2><p>Typically, when a field issue arises, the process follows these steps:</p><ol><li> A customer encounters a problem and contacts the support team.</li><li> Customer support collects details about the issue.</li><li> If unresolved, the issue escalates to an on-call engineer.</li><li> The engineer seeks critical information:\n\n<ul><li>When did the issue occur?</li><li>Are all necessary information available to query logs or debug further?</li></ul></li><li> Missing details require reverting to customer support, who then contact the customer again.</li><li> With complete information, the engineer analyzes logs to identify and rectify the root cause.</li></ol><p>This iterative process can cause significant delays, increased workloads, and frustration for both customers and support teams.</p><h2>\n  \n  \n  Introducing Agentic AI into Incident Response\n</h2><p>Agentic AI systems autonomously perform tasks, make decisions, and adapt to changing environments without human input. In the context of incident response, agentic AI can revolutionize the traditional workflow by:</p><ol><li><p><strong>Automated Issue Detection and Classification:</strong></p><ul><li> AI-driven tools continuously monitor systems, identifying anomalies before they escalate into significant issues. By analyzing patterns and deviations, these tools can detect potential problems early, reducing the frequency of critical incidents. </li><li> Upon detecting an issue, AI systems can classify its severity and potential impact, ensuring that critical problems receive immediate attention while filtering out false positives.</li></ul></li><li><p><strong>Enhanced Data Collection and Analysis:</strong></p><ul><li><strong>Contextual Data Gathering:</strong> Agentic AI can automatically collect relevant data—such as timestamps, system logs, and user actions—at the moment an issue is detected, ensuring that on-call engineers have all necessary information upfront.</li><li> By analyzing aggregated data, AI can identify patterns and pinpoint the underlying causes of issues, providing engineers with actionable insights.</li></ul></li><li><p><strong>Automated Communication and Resolution:</strong></p><ul><li> AI-powered virtual assistants can engage with customers in real-time, gathering essential details about the issue through natural language processing, reducing the need for multiple back-and-forth communications.</li><li> For known issues, agentic AI can execute predefined solutions, resolving problems without human intervention and only escalating to on-call engineers when necessary. </li></ul></li></ol><h2>\n  \n  \n  Benefits of Agentic AI in Reducing On-Call Burden\n</h2><p>Integrating agentic AI into incident response workflows offers several advantages:</p><ul><li> Automated detection and data collection expedite the initial phases of incident management, allowing for quicker resolutions.</li><li> By handling routine tasks and minor issues autonomously, AI frees up engineers to focus on more complex problems, reducing burnout and improving job satisfaction.</li><li> AI systems can analyze vast amounts of data without fatigue, leading to more accurate diagnoses and reducing the likelihood of recurring issues.</li><li><strong>Enhanced Customer Satisfaction:</strong> Faster response times and proactive issue resolution lead to a better customer experience, fostering trust and loyalty.</li></ul><h2>\n  \n  \n  Implementing Agentic AI in Your Organization\n</h2><p>To effectively integrate agentic AI into your incident response processes, consider the following steps:</p><ol><li><strong>Assess Current Workflows:</strong> Identify repetitive tasks and common pain points in your existing incident response procedures that could benefit from automation.</li><li><strong>Select Appropriate AI Tools:</strong> Choose AI solutions that align with your organization's specific needs. For instance, platforms like Merlinn offer open-source AI assistants designed to handle system alerts and incidents autonomously. </li><li><strong>Integrate with Existing Systems:</strong> Ensure that the chosen AI tools can seamlessly interface with your current infrastructure, including monitoring systems, communication platforms, and databases.</li><li> Utilize historical incident data to train AI models, enabling them to recognize patterns and make informed decisions.</li><li> Continuously monitor the performance of AI systems, gathering feedback from on-call engineers and support staff to refine and improve AI-driven processes.</li></ol><h2>\n  \n  \n  Challenges and Considerations\n</h2><p>While agentic AI offers numerous benefits, it's essential to be mindful of potential challenges:</p><ul><li><strong>Data Privacy and Security:</strong> Automated systems must handle sensitive information responsibly, adhering to data protection regulations and ensuring that customer data remains secure.</li><li> Maintaining transparency in AI decision-making processes is crucial to build trust among employees and customers. Clear documentation and explainable AI models can aid in this effort.</li><li> AI systems require regular updates and training to adapt to evolving threats and system changes, necessitating ongoing investment in AI development and maintenance.</li></ul><h2>\n  \n  \n  Conclusion: Key Metrics to track - Building Sustainable Incident Management\n</h2><p>Streamlining customer incident response requires a holistic approach combining technical innovation, process optimization, and cultural evolution. By implementing the strategies outlined – from AI-powered triage systems organizations can achieve:</p><ul><li>63-75% Reduction in on-call engineer workload</li><li>55% Faster mean time to resolution (MTTR)</li><li>89% Improvement in engineer job satisfaction</li></ul><p>The path forward demands continuous investment in both technology and people. Organizations that master this balance will not only improve operational reliability but also create engineering environments where talent thrives amidst increasing system complexity. </p><p>Images used in this blog are generated by Microsoft copilot.</p><p>In my next post, I plan to cover some of these topics in detail!\nStay tuned! Feel free to leave a comment! Get in touch on Linkedin for any collaboration!</p>","contentLength":7362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Your AI Agent isn't an Engineer","url":"https://dev.to/blackgirlbytes/your-ai-agent-isnt-an-engineer-5egf","date":1739765688,"author":"Rizèl Scarlett","guid":1606,"unread":true,"content":"<ol><li>Why This Conversation Matters</li><li>How AI Marketing Shaped This Perception</li><li>The Problem with Marketing AI as a Human</li><li>Framework for Effectively Marketing AI Agents to Developers</li></ol><p>Raise your hand if you've been personally victimized by the question: 'Will AI replace software engineers?' It's a common debate that drives developers to extremes—either avoiding AI entirely or frantically signing up for every AI course available.</p><h2>\n  \n  \n  Why This Conversation Matters\n</h2><p>However, it's not a hypothetical or frivolous concern. Companies  making hiring decisions based on AI productivity. <a href=\"https://www.salesforceben.com/salesforce-will-hire-no-more-software-engineers-in-2025-says-marc-benioff/\" rel=\"noopener noreferrer\">Salesforce's CEO recently announced</a> plans to reduce hiring software and support engineers after seeing a 30% productivity boost from AI.</p><p>As a Developer Advocate in AI, my public response has always been to upskill and adapt to the changing economy. After all, you wouldn't want to be the person insisting on driving a horse and buggy while everyone else has moved on to cars.</p><p>I believe that AI  a helpful tool. I've used it to understand new technologies and quickly prototype ideas. </p><p>But internally, I've wrestled with a different question: Why do we keep framing AI primarily as a replacement for human beings?</p><h2>\n  \n  \n  How AI Marketing Shaped This Perception\n</h2><p>My spicy take is that our industry helped shape this narrative. We inadvertently leaned into a lazy marketing strategy prioritizing quick wins over sustainable adoption. It's easier to tell VCs and executives that your AI tool replaces developers than to demonstrate how it augments developer capabilities.</p><h3>\n  \n  \n  Anthropomorphism Is Not All Bad\n</h3><p>Anthropomorphism is the practice of assigning human traits to non-human entities. It isn't inherently problematic. In fact, it's a common practice in tech. Thoughtful anthropomorphism makes digital experiences more intuitive and helps users embrace new interfaces. For example: </p><ul><li>E-books mirror traditional reading experiences by simulating page-turning animations, even though there's no physical page to turn.</li><li>Electric cars (as <a href=\"https://bsky.app/profile/threepointone.bsky.social/post/3lbepxmdwek23\" rel=\"noopener noreferrer\">Sunil Pai pointed out to me</a>) play pre-recorded engine sounds when they start, providing a familiar affordance for drivers.</li></ul><p>In these cases, users don't actually believe their e-book contains paper or that their electric car has a combustion engine. </p><p>But, AI presents a unique challenge. Its complexity and \"black box\" nature make it harder for users to grasp its true capabilities and limitations. To bridge this knowledge gap, companies lean heavily into human-like descriptions:</p><h4>\n  \n  \n  Devin is an \"AI Software Engineer.\"\n</h4><p>While these descriptions make AI feel more familiar, the drawback is that they can also mislead users to believe that AI can think, reason, and work independently like humans.</p><h2>\n  \n  \n  The Problem with Marketing AI as a Human\n</h2><p>Anthropomorphic AI marketing is sometimes a form of self-sabotage because:</p><ul><li>When AI is marketed as an \"engineer\" or \"developer, \" decision-makers view it as a one-to-one substitute for human talent.</li><li>This is counterproductive because developers are some of the most valuable users of AI tools. They are the users who know how to use AI and contribute to the ecosystem effectively. According to the <a href=\"https://survey.stackoverflow.co/2024/ai#1-ai-tools-in-the-development-process\" rel=\"noopener noreferrer\">2024 Stack Overflow Developer Survey</a>, 76% of developers currently use or plan to incorporate AI into their workflows. However, our industry's marketing suggests that using AI and contributing to the ecosystem will eventually put AI in a place to take their jobs. So why would they want to further the movement?</li></ul><h3>\n  \n  \n  It Sets Unrealistic Expectations\n</h3><ul><li>If an AI tool is marketed as \"just like a human,\" users will expect it to perform at human levels.</li><li>AI is a non-sentient tool that processes historical data patterns, is prone to hallucinating, misses important context, and provides non-deterministic output.</li><li>When developers realize it's not as good as the marketing implied, the company and product risk losing credibility. Developers are notorious for valuing authenticity. Over-exaggeration or misrepresentation in marketing only drives developers away.</li></ul><p>One survey participant shared their reflections: \"I have a PhD in AI, worked to develop some of the algorithms used by generative AI. I deeply regret how naively I offered up my contributions.\"</p><p>Another participant stated, \"We should use generative AI to help people be faster at their jobs, not lose them.\"</p><h3>\n  \n  \n  It Misses the Real Value Proposition\n</h3><p>The real value of AI developer tools includes automating boring tasks, faster prototyping, and quicker debugging, which leaves more time for creative problem-solving.</p><p>And now, AI enthusiasts have dubbed 2025 as the Year of the Agent. In short, <a href=\"https://aws.amazon.com/what-is/ai-agents/\" rel=\"noopener noreferrer\">AI agents</a> are tools that can autonomously take action on our behalf, like executing shell commands, creating calendar events, and building applications. But as we move from LLMs that suggestion code to us to more autonomous agents, anthropomorphic marketing is only increasing.</p><h2>\n  \n  \n  Framework for Effectively Marketing AI Agents to Developers\n</h2><p>Here's how to market AI developer tools in a way that both builds trust and differentiates your Agent in an oversaturated market:</p><p>If you work in Developer Relations, Sales, Marketing, or as an executive promoting an AI agent, you're probably representing a product you didn't build. This means you may not fully understand how the tool works, its true capabilities, or its limitations. Developers have a knack for spotting misrepresentation or inauthentic marketing.</p><p>You can mitigate this challenge by:</p><ul><li>Becoming customer zero \n\n<ul><li>Use the product extensively before it reaches the public</li></ul></li><li>Investing time in learning the following fundamentals:\n\n<ul><li>LLMs and their capabilities</li><li>Key differences between Copilots and Agents</li><li>Core AI Agent operations and your product's unique approach</li><li>Token handling and context management</li><li>Points requiring human intervention</li><li>Your product's agentic loop. For example, some agents use the following loop: \n\n<ul><li>Share requests and available tools with an LLM</li><li>Receive LLM's execution plan</li><li>Execute the plan and tool calls</li><li>Verify results with the LLM</li><li>Revise and re-execute if needed</li><li>Deliver final results to the user and wait for the user's request</li></ul></li></ul></li></ul><p>I used these two resources to help me understand AI agents:</p><p>It might be difficult to eliminate anthropomorphism entirely, especially since it is useful. My advice is to use it sparingly. Skip titles like \"AI Engineer\" or \"AI Teammate.\" Choose names that set clear expectations, like Copilot, Agent, or Assistant. GitHub's use of \"Copilot\" and \"AI Pair Programming Assistant\" exemplifies this balance because it suggests collaboration while keeping humans in control. </p><h3>\n  \n  \n  Augmentation &gt; Replacement\n</h3><p>Let's understand who developers are. They're not rockstar/ninja/10x developers. Those stereotypes are so 2014.  </p><p>Developers juggle multiple roles – they're parents, open source maintainers, bootcamp instructors, and more. AI agents shine brightest when they complement these diverse responsibilities, taking on parallel tasks while developers focus on high-impact work. Instead of marketing your tools as whole substitutes for developers, position them as tools part of a developer's toolkit.</p><p>If possible, go open source. If not, find ways to explain the architecture through whitepapers and conference talks. This approach will help your users understand that it's not magic so they can determine how to use the product and get the best performance from it. </p><p>Many times, when there's a lack of transparency, developers will theorize how they think it works and create their own narrative, which can backfire on your product. I remember this happened in the early days of GitHub Copilot. I would hop into Twitter Spaces, where people would share how they thought it worked, but they were wrong and spreading misinformation.</p><p>You can build trust with developers by putting them in control of their workflow. Here are some of my suggestions:</p><ul><li>Similar to how developers choose IDE settings, allow developers to choose their preferred LLM models and customize the Agent's behavior and verbosity.</li><li>Show what actions the Agent will take before executing them and provide detailed logs for debugging.</li><li>Provide APIs and hooks so the Agent fits into existing workflows. </li></ul><p><a href=\"http://block.github.io/goose\" rel=\"noopener noreferrer\">codename goose</a> is my favorite example of this, although I'm biased because it's an agent my company made. It's open source. Goose, as it's fondly called, lets developers choose their LLM model and extensions via <a href=\"https://www.anthropic.com/news/model-context-protocol\" rel=\"noopener noreferrer\">Model Context Protocol</a>. Developers can also choose to interact with the Agent via the CLI or GUI.</p><p>Instead of making false promises, demonstrate your AI agent's value through concrete examples. Create short, engaging video demos, GIFs, or blog posts showing the Agent in action:</p><ul><li>Creating and running test suites</li><li>Converting code between languages</li><li>Transforming wireframes into interactive UIs</li><li>Generating API documentation from code comments</li><li>Automating environment setup</li></ul><p>Don't be afraid to demo live and make it fun so it can be memorable! When I worked at GitHub, I used to demo GitHub Copilot at conferences. I would prompt GitHub Copilot to post a tweet that said, \"I wrote this tweet with Copilot.\" It was a short and simple demo that was memorable for attendees and sparked curiosity from those who weren't there.</p><p>: Demoing generative AI tools live is scary because the output is non-deterministic. If your live demo fails, that's even better because you can use it as a teaching moment. Show how you work around issues and where human expertise adds value. This authenticity builds more trust than a perfectly polished demo ever could.</p><p>Documentation often determines whether developers adopt your tool. Strong documentation for your Agent could include:</p><ul><li>Accurate technical specifications of model training and limitations</li><li>Comprehensive feature guides</li><li>Clear explanations of data usage and privacy</li></ul><p>Build product credibility by fostering an ecosystem where developers can learn from each other, and you can learn from them. You can do this by:</p><ul><li>Using platforms like GitHub Discussions and Discord to create spaces for feedback and support</li><li>Encouraging knowledge sharing by letting developers exchange prompts, best practices, and integrations</li><li>Recognizing community contributions</li><li>Maintaining a transparent feedback loop to show that you value developer input</li></ul><p>A great example is <a href=\"https://cursor.directory\" rel=\"noopener noreferrer\">Cursor.directory</a> - a platform by and for the community where developers share  prompts.</p><p>Our presentation of AI shapes how the world perceives and uses it. Let's move beyond the tired question of whether AI will replace developers and focus on how it can augment developer capabilities.</p><p>Share your thoughts below!</p>","contentLength":10518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why A Maintainer of the Linux Graphics Driver Nouveau Stepped Down","url":"https://tech.slashdot.org/story/25/02/17/0318229/why-a-maintainer-of-the-linux-graphics-driver-nouveau-stepped-down?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739762520,"author":"EditorDavid","guid":891,"unread":true,"content":"For over a decade Karol Herbst has been a developer on the open-source Nouveau driver, a reverse-engineered NVIDIA graphics driver for Linux. \"He went on to become employed by Red Hat,\" notes Phoronix. \"While he's known more these days for his work on the Mesa 3D Graphics Library and the Rusticl OpenCL driver for it, he's still remained a maintainer of the Nouveau kernel driver.\" \n\nBut Saturday Herbst stepped down as a nouveau kernel maintainer, in a mailing list message that begins \"I was pondering with myself for a while if I should just make it official that I'm not really involved in the kernel community anymore, neither as a reviewer, nor as a maintainer.\" (Another message begins \"I often thought about at least contributing some patches again once I find the time, but...\") \n\nTheir resignation message hints at some long-running unhappiness. \"I got burned out enough by myself caring about the bits I maintained, but eventually I had to realize my limits. The obligation I felt was eating me from inside. It stopped being fun at some point and I reached a point where I simply couldn't continue the work I was so motivated doing as I've did in the early days.\" And they point to one specific discussion on the kernel mailing list February 8th as \"The moment I made up my mind.\" \n\nIt happened in a thread about whether Rust would create difficulty for maintainers. (Someone had posted that \"The all powerful sub-system maintainer model works well if the big technology companies can employ omniscient individuals in these roles, but those types are a bit hard to come by.\") In response, someone else had posted \"I'll let you in a secret. The maintainers are not 'all-powerful'. We are the 'thin blue line' that is trying to keep the code to be maintainable and high quality. Like most leaders of volunteer organization, whether it is the Internet Engineerint Task Force (the standards body for the Internet), we actually have very little power. We can not *command* people to work on retiring technical debt, or to improve testing infrastructure, or work on some particular feature that we'd very like for our users. All we can do is stop things from being accepted...\" \n\nSaturday Herbst wrote:\n\nThe moment I made up my mind about this was reading the following words written by a maintainer within the kernel community: \n\n\t\"we are the thin blue line\" \n\nThis isn't okay. This isn't creating an inclusive environment. This isn't okay with the current political situation especially in the US. A maintainer speaking those words can't be kept. No matter how important or critical or relevant they are. They need to be removed until they learn. Learn what those words mean for a lot of marginalized people. Learn about what horrors it evokes in their minds. \n\nI can't in good faith remain to be part of a project and its community where those words are tolerated. Those words are not technical, they are a political statement. Even if unintentionally, such words carry power, they carry meanings one needs to be aware of. They do cause an immense amount of harm. \nThe phrase thin blue line \"typically refers to the concept of the police as the line between law-and-order and chaos,\" according to Wikipedia, but more recently became associated with a\"countermovement\" to the Black Lives Matter movement and \"a number of far-right movements in the U.S.\" \nPhoronix writes:\n\nLyude Paul and Danilo Krummrich both of Red Hat remain Nouveau kernel maintainers. Red Hat developers are also working on developing NOVA as the new Rust-based open-source NVIDIA kernel driver leveraging the GSP interface for Turing GPUs and newer.\n","contentLength":3630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Magic of Embeddings: How AI Understands Language Like Humans","url":"https://dev.to/kumarprateek18/the-magic-of-embeddings-how-ai-understands-language-like-humans-741","date":1739761804,"author":"Prateek kumar","guid":881,"unread":true,"content":"<p>Ever wondered how AI chatbots generate relevant, intelligent responses in real-time? The secret lies in —the technology that enables AI to understand and process language, images, and data like humans. These powerful numerical representations allow AI to  meaningful content, forming the backbone of <strong>Retrieval-Augmented Generation (RAG)</strong>.</p><p>In this blog, we’ll explore what embeddings are, how they work, and why they are crucial for AI-driven applications like chatbots, search engines, and recommendation systems.</p><p>Embeddings are numerical representations of words, sentences, images, or documents in a high-dimensional space. They allow AI models to capture semantic relationships between different pieces of data. Instead of using plain text, AI converts these elements into vectors (arrays of numbers), enabling efficient comparison and retrieval.</p><h3><strong>Why Are Embeddings Important?</strong></h3><p>Traditional keyword-based search methods rely on exact word matches, which have major limitations:</p><ul><li>They fail to understand synonyms (e.g., \"car\" and \"automobile\" are considered different words).</li><li>They do not capture contextual meaning (e.g., \"bank\" as a financial institution vs. \"bank\" as a riverbank).</li><li>They struggle with large datasets, making searches inefficient.</li></ul><p>Embeddings solve these problems by representing words, phrases, and documents as vectors in a mathematical space, allowing AI systems to <strong>find similarities based on meaning rather than exact wording</strong>.</p><h2><strong>How Are Embeddings Used in Retrieval-Augmented Generation (RAG)?</strong></h2><p>One of the most powerful applications of embeddings is in <strong>Retrieval-Augmented Generation (RAG)</strong>. RAG combines <strong>retrieval (finding relevant data)</strong> with <strong>generation (creating responses using an LLM)</strong> to produce intelligent, context-aware answers.</p><ol><li> Documents are split into smaller chunks and transformed into embeddings.</li><li> When a user asks a question, the system converts the query into an embedding and finds the most relevant chunks.</li><li> The retrieved chunks are provided as context to an LLM (like GPT-4), which generates a response based on the retrieved knowledge.</li></ol><p>RAG ensures that AI models can access <strong>up-to-date, domain-specific knowledge</strong> while maintaining  in responses, making it ideal for chatbots, search engines, and enterprise AI applications.</p><h2><strong>How Are Embeddings Created?</strong></h2><p>Embeddings are generated using machine learning models trained on vast amounts of text or image data. Some popular models include:</p><ul><li> (for fast similarity search)</li></ul><h3><strong>Mathematical Representation</strong></h3><p>Each word or sentence is represented as a point in an N-dimensional space. The closer two vectors are in this space, the more similar they are in meaning. For example:</p><div><table><thead><tr></tr></thead><tbody></tbody></table></div><p>Here, \"king\" and \"queen\" have similar embeddings, while \"apple\" is farther apart, indicating that it belongs to a different concept.</p><h2><strong>How Are Embeddings Used in AI Applications?</strong></h2><h3><strong>1. AI Chatbots and Custom Data Search</strong></h3><p>When building an AI chatbot that understands company-specific documents, embeddings help by:</p><ul><li>Splitting documents into .</li><li>Converting chunks into .</li><li>Storing embeddings in a  (e.g., ChromaDB, Pinecone, FAISS).</li><li>Converting user queries into  and retrieving relevant document chunks.</li><li>Passing the retrieved data to an LLM (Large Language Model) for response generation.</li></ul><h3><strong>2. Similarity Search &amp; Information Retrieval</strong></h3><p>Instead of searching by keywords, AI can retrieve documents or images by . When a user queries a system, the system:</p><ol><li>Converts the query into an embedding.</li><li>Searches for similar embeddings in the vector database.</li><li>Returns the most relevant documents, even if they use different words.</li></ol><h3><strong>3. Recommendation Systems</strong></h3><p>Spotify, Netflix, and YouTube use embeddings to recommend content:</p><ul><li>If you watch sci-fi movies, the system retrieves other movies with similar embeddings.</li><li>Music streaming services recommend songs based on user-listened embeddings.</li></ul><h3><strong>4. Search Engine Optimization (SEO)</strong></h3><p>Google’s search algorithm heavily relies on embeddings to rank pages by  rather than exact keyword matches.</p><h2><strong>Mathematical Explanation of Similarity Search</strong></h2><p>To find similar embeddings, AI systems use , which measures the angle between two vectors.</p><h3><strong>Formula for Cosine Similarity:</strong></h3><p>cos(θ) = (A · B) / (||A|| * ||B||).</p><ul><li>(||A||) and (||B||) are the  of the vectors.</li></ul><p>If , the vectors are identical (perfect match). If , the vectors are unrelated.</p><p>This allows AI to find the <strong>most relevant text, images, or documents efficiently</strong>.</p><h2><strong>Building a Simple AI Chatbot with Embeddings</strong></h2><h3><strong>Using OpenAI’s Embeddings API</strong></h3><div><pre><code></code></pre></div><h3><strong>Using LangChain and ChromaDB for Vector Search</strong></h3><div><pre><code></code></pre></div><h2><strong>Conclusion: Why Embeddings Are a Game-Changer</strong></h2><p>✅ <strong>Embeddings allow AI to \"understand\" language mathematically.</strong>\n✅ <strong>They make similarity search fast and scalable.</strong>\n✅ <strong>They enable AI to retrieve and use relevant information dynamically.</strong>\n✅ <strong>They power many AI applications, from chatbots to recommendation systems.</strong></p><p>By leveraging embeddings and vector databases, businesses can <strong>enhance AI applications with custom knowledge</strong> and <strong>deliver smarter, context-aware responses</strong>.</p>","contentLength":4900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unlocking Your Brand's True Potential: How AI Search Grading Reveals the Path to Digital Dominance","url":"https://dev.to/seosiri/unlocking-your-brands-true-potential-how-ai-search-grading-reveals-the-path-to-digital-dominance-52nh","date":1739760552,"author":"Momenul Ahmad","guid":880,"unread":true,"content":"<p>Decoding the Language of Your Brand's Online Presence</p><p>In today's hyper-competitive digital landscape, your brand's success hinges on more than just keywords. It's about understanding the intricate tapestry of conversations woven around your brand online. </p><p>Imagine having a powerful AI-driven lens that not only reveals what people are saying about your brand but also deciphers the emotions and motivations behind their words. That's the transformative power of AI search grading. </p><p>Tools like HubSpot's AI Search Grader, fueled by OpenAI's cutting-edge sentiment analysis, act as digital detectives. They meticulously dissect online conversations to uncover invaluable insights about your brand, your competitors, and the ever-evolving trends shaping your industry. </p><p>Beyond Keywords: Decoding the Language of Your Brand's Digital DNAAI search grading transcends the limitations of traditional keyword analysis. It delves into the nuanced world of online discourse, analyzing a multitude of factors to paint a comprehensive portrait of your brand's digital presence.</p><p>Think of it as a sophisticated sentiment analysis engine that gauges the overall feeling and tone surrounding your Brand.</p><p>Let's be bold through AI Search Grading Valuation, Analysis, and Implementation Stage 1.2.3, which will unveil the answer to how AI Search Grading Reveals the Path to Digital Dominance.</p><p>Are you booming in <a href=\"https://www.seosiri.com/2024/12/ai-overview-search-results.html\" rel=\"noopener noreferrer\">AI SERPs</a>, Or Grave Yearded your Brand on AI SERPs Ground?</p><p>Raise voice, share voice for either get help or the communities.</p>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ugandan runner Jacob Kiplimo completes first ever sub-57 minute half marathon","url":"https://www.cnn.com/2025/02/16/sport/jacob-kiplimo-smashes-half-marathon-record-spt-intl/index.html","date":1739759391,"author":"mooreds","guid":1655,"unread":true,"content":"<p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77s57xc00ar2bqhb8xh8fk1@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Jacob Kiplimo shattered the half marathon <a href=\"https://www.cnn.com/2025/02/15/sport/grant-fisher-two-indoor-world-records-spt-intl/index.html\">world record</a> with a blistering run on Sunday.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77s5ka00002356m2hbzgndc@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The 24-year-old Ugandan was competing in the Barcelona half marathon where he became the first person ever to complete the distance in under 57 minutes, per World Athletics, which added that the record would be subject to its usual ratification procedure.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77s5ka10003356mh4e4rcn2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Kiplimo finished the race with a time of 56:42 and smashed the previous world <a href=\"https://www.cnn.com/2025/02/14/sport/jakob-ingebrigtsen-mile-1500m-records-spt-intl/index.html\">record</a> by 48 seconds, the biggest ever single improvement on the men’s half marathon world record. Prior to Kiplimo’s incredible run, Ethiopian athlete Yomif Kejelcha held the record with a time of 57:30.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77s5ka10004356m9d2ckdot@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “It has been the perfect race,” Kiplimo said after his run, per World Athletics. “Ideal temperature, no wind at all, fantastic circuit - everything went better than expected.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77s5ka10005356m0j1t2njt@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “The pacemaker set the agreed 2:45 pace but I found myself full of energy and decided to inject a brisker rhythm from the third kilometre, but I never imagined to perform under the 57 minute barrier, that’s astonishing.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77s5ka10006356mde99ya9h@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Kiplimo ran at an average speed of 22.3 kilometers-per-hour on his way to breaking the record and finished more than two minutes ahead of Kenya’s Geoffrey Kamworor in second. Samwel Mailu, also from Kenya, finished another 56 seconds back in third.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77s5ka10007356m8hsspl7x@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Kiplomo now has his sights set on the marathon, and said he will be resting until the London Marathon in April where he will be making his debut at the distance.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cm77sk5tv0000356m1sr7dpge@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Meanwhile, Kenya’s Joyciline Jepkosgei set a course record in the women’s race, finishing in 1:04:13 to fend off her compatriot Gladys Chepkurui in second and Ethiopia’s Alemtsehay Zerihun in third.\n    </p>","contentLength":1793,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43074347"},{"title":"Complete Guide to Virtual Environments (Virtualenv) in Python","url":"https://dev.to/mrnik/complete-guide-to-virtual-environments-virtualenv-in-python-3jn1","date":1739757746,"author":"Mahdi Ahmadi","guid":871,"unread":true,"content":"<h2>\n  \n  \n  1. What is a Virtual Environment in Python?\n</h2><p>When developing multiple projects with Python, each project may require different verzsions of libraries. This is where Virtual Environment (Virtualenv) comes to the rescue!</p><blockquote><p>A virtual environment is an isolated space for installing libraries and packages for a specific project without affecting your main system.</p></blockquote><h2>\n  \n  \n  2. Why Should You Use Virtualenv?\n</h2><ul><li><p>Avoid version conflicts: If different projects require different versions of the same library, conflicts may arise without a virtual environment.</p></li><li><p>Project isolation: Each project has its own set of dependencies, ensuring stability.</p></li><li><p>Portability: You can easily recreate the project environment on another system using a requirements.txt file.</p></li><li><p>Increased security: Installing packages in an isolated environment prevents unintended changes to system files.</p></li></ul><h2>\n  \n  \n  3. Installing and Using Virtualenv\n</h2><ul><li>Installing Virtualenv on Windows, Linux, and macOS</li></ul><p>If Virtualenv is not already installed, you can install it using the following command:</p><ul><li>Creating a Virtual Environment</li></ul><p>To create a virtual environment in your project directory, run:</p><p><em>venv is the name of the folder where the virtual environment will be created. You can use any name you prefer.</em></p><ul><li>Activating the Virtual Environment</li></ul><p>The activation process depends on your operating system:\nOn Windows (CMD or PowerShell):</p><div><pre><code>venv\\Scripts\\Activate.ps1\n</code></pre></div><p>Once activated, you will see the virtual environment name in the terminal prompt:</p><ul><li>Installing Packages in the Virtual Environment</li></ul><p>After activation, you can install project dependencies using:</p><ul><li>Deactivating the Virtual Environment</li></ul><p>To deactivate the virtual environment, simply run:</p><h2>\n  \n  \n  4. Saving and Recreating the Virtual Environment with </h2><p>To save the list of installed packages in the virtual environment, use:</p><div><pre><code>pip freeze &gt; requirements.txt\n</code></pre></div><p>To recreate the same environment on another system:</p><div><pre><code>pip install -r requirements.txt\n</code></pre></div><ul><li><p>Virtualenv helps you run Python projects in an isolated and conflict-free manner.</p></li><li><p>You can install it with pip install virtualenv.</p></li><li><p>Create and activate a virtual environment with venv.</p></li><li><p>Use requirements.txt to store and restore dependencies.</p></li></ul><p>Thanks for reading❤️\nI hope this guide helps you understand and use virtual environments effectively. If you have any questions or suggestions, feel free to leave a comment!</p>","contentLength":2322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomPosterize in PyTorch","url":"https://dev.to/hyperkai/randomposterize-in-pytorch-40ac","date":1739757574,"author":"Super Kai (Kazuya Ito)","guid":870,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomPosterize.html\" rel=\"noopener noreferrer\">RandomPosterize()</a> can randomly posterize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type:):\n*Memos:\n\n<ul><li>It's the number of bits to keep for each channel.</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is posterized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomInvert in PyTorch","url":"https://dev.to/hyperkai/randominvert-in-pytorch-4e0o","date":1739757459,"author":"Super Kai (Kazuya Ito)","guid":869,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is inverted or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What to do if the selenium crawler is detected?","url":"https://dev.to/98ip/what-to-do-if-the-selenium-crawler-is-detected-4o0f","date":1739757045,"author":"98IP 代理","guid":868,"unread":true,"content":"<p>When using Selenium for automated web crawling, it is often detected and blocked by the target website. This is usually because Selenium's automation features are more obvious and can be easily identified by the website's anti-crawler mechanism. This article will explore in depth how to deal with the problem of Selenium crawler being detected, including methods such as hiding automation features and using proxy IPs, and provide specific code examples. At the same time, 98IP proxy will be briefly mentioned as one of the solutions.</p><h2>\n  \n  \n  I. Reasons for Selenium crawlers being detected\n</h2><h3>\n  \n  \n  1.1 Obvious automation features\n</h3><p>Selenium's default browser behavior is significantly different from manual user operations, such as specific fields in the request header, fixed browser window size, uniform operation speed, etc., which may be used by websites to identify automated scripts.</p><h3>\n  \n  \n  1.2 Frequent request frequency\n</h3><p>Crawlers usually send requests at a frequency much higher than normal users, which can also easily alert websites.</p><p>If the crawler always sends requests from the same IP address, the IP address will soon be blacklisted by the website.</p><h2>\n  \n  \n  II. Strategies for dealing with Selenium crawler detection\n</h2><h3>\n  \n  \n  2.1 Hide automation features\n</h3><h4>\n  \n  \n  2.1.1 Modify request headers\n</h4><p>Through Selenium's <code>webdriver.ChromeOptions()</code> configuration, you can modify the browser's request header to make it closer to normal user requests.</p><div><pre><code></code></pre></div><h4>\n  \n  \n  2.1.2 Randomize browser settings\n</h4><p>Use libraries such as  to automatically manage browser drivers and randomize window size, scrolling behavior, etc. to simulate real user operations.</p><div><pre><code></code></pre></div><p>Sending requests through <a href=\"https://en.98ip.com/\" rel=\"noopener noreferrer\">proxy IPs</a> can effectively avoid the problem of IP being blocked. High-quality proxy services such as 98IP Proxy provide stable and anonymous IP resources, which is an effective means of dealing with Selenium crawlers being detected.</p><div><pre><code></code></pre></div><p> The above code uses the  library instead of  because  provides more flexible proxy configuration and request interception functions. If you haven't installed  yet, you can install it through .</p><h3>\n  \n  \n  2.3 Controlling request frequency\n</h3><p>By introducing random delays and setting reasonable request intervals, the request frequency of the Selenium crawler can be controlled to make it closer to the browsing behavior of normal users.</p><div><pre><code></code></pre></div><p>It is a common problem for Selenium crawlers to be detected, but by hiding automation features, using proxy IPs, controlling request frequency, etc., we can effectively reduce the risk of being detected. In particular, using high-quality proxy services such as 98IP Proxy can significantly improve the stability and success rate of crawlers.</p><p>In the future, with the continuous advancement of website anti-crawler technology, we also need to continuously update and improve crawler strategies. For example, introducing more complex browser simulation technology, using machine learning to predict and circumvent blocking strategies, etc. are all directions worth exploring.</p><p>In short, dealing with the problem of Selenium crawlers being detected requires comprehensive consideration of multiple factors and taking corresponding measures.</p>","contentLength":3162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Did the Windows 95 setup team forget that MS-DOS can do graphics?","url":"https://devblogs.microsoft.com/oldnewthing/20250211-00/?p=110862","date":1739756728,"author":"zinekeller","guid":1654,"unread":true,"content":"<p>One of the reactions to my discussion of <a title=\"Why did Windows 95 setup use three operating systems?\" href=\"https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507\"> why Windows 95 setup used three operating systems</a> (and oh there were many) was my explanation that an MS-DOS based setup program would be text-mode. But c’mon, MS-DOS could do graphics! Are you just a bunch of morons?</p><p>Yes, MS-DOS could do graphics, in the sense that it didn’t actively prevent you from doing graphics. You were still responsible for everything yourself, though. There were no graphics primitives aside from a BIOS call to plot a single pixel. Everything else was on you, and you didn’t want to use the BIOS call to plot pixels anyway because it was slow. If you wanted any modicum of performance, you had to access the frame buffer directly.</p><p>Okay, so now you have to write a graphics library for drawing things fancier than a single pixel. Fortunately, Windows 95 required a VGA video card at a minimum, so didn’t have to worry about CGA or EGA. Mind you, the VGA adapter required you to deal with planar modes, so that was annoying. Fortunately, you have a team of folks expert in VGA planar modes sitting down the hall working on Windows video drivers who can help you out.</p><p>But the setup program needs more than just pixels. It also wants dialog boxes, so you’ll have to write a window manager to sit on top of your graphics library so you can show dialog boxes with a standard GUI dialog interface, which includes keyboard support for tabbing between elements and assigning hotkeys to fields.</p><p>You’ll also have to add support for typing characters in non-alphabetic languages like Japanese. Fortunately, you have a team of folks expert in Japanese input sitting in the Tokyo office working on Windows input methods who can help you out, though the time zone difference between Tokyo and Redmond is going to slow you down.</p><p>You also want to <a title=\"A wrinkle in how Windows 95 setup bootstrapped its initial GUI step\" href=\"https://devblogs.microsoft.com/oldnewthing/20241119-00/?p=110543\"> take advantage of those fancy new controls</a> that the UI team has been making, so maybe you can walk down the hall and ask them if they could port their controls library to your custom UI framework.</p><p>The setup program also wants to do simple animations, so you’ll need a scheduler that can trigger events based on the system hardware timer.</p><p>So now you’re going to write all this code for your setup program, none of which is actually involved in setting up Windows 95, but is just the infrastructure needed to run the setup program at all! There’s a lot of stuff here, and you probably won’t be able to cram all of it into 640KB of memory. So now you need to write a protected mode manager (also known as an MS-DOS extender) so you can take advantage of the larger address space afforded by protected mode.</p><p>An operating system with exactly one application: Windows 95 Setup.</p><p>What if I told you that Microsoft already had an operating system that did all the things you are trying to do, and it’s fully debugged, with video drivers, a graphics library, a dialog manager, a scheduler, a protected mode manager, and input methods. And it has a fully staffed support team. And that operating system has withstood years of real-world usage? And Microsoft fully owns the rights to it, so you don’t have to worry about royalties or licensing fees? And it’s a well-known system that even has books written about how to program it, so it’ll be easier to hire new people to join your team, since you don’t have to spend a month teaching them how to code for your new custom Setup UI miniature operating system.</p><p>Go and grab a copy of the Windows 3.1 runtime.</p><p>: If you committed to the custom operating system route, you’d have to make sure your miniature operating system could run in a Windows 3.1 MS-DOS session in case somebody wanted to install Windows 95 as an upgrade from Windows 3.1, and in a Windows 95 MS-DOS session in case somebody wants to do a repair install of Windows 95. And then you’d have this weird setup experience where Windows 95 setup is running inside an MS-DOS session.</p><p>: Windows setup still follows this pattern of installing a miniature operating system to bootstrap the setup program. But today, the miniature operating system is Windows PE, the Windows Preinstallation Environment.</p>","contentLength":4120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43073992"},{"title":"All Kindles can now be jailbroken","url":"https://kindlemodding.org/jailbreaking/WinterBreak/","date":1739756532,"author":"lumerina","guid":890,"unread":true,"content":"<main><blockquote><p>After all, all devices have their dangers. The discovery of speech introduced communication – and lies.  - Isaac Asimov</p></blockquote><p>WinterBreak is a jailbreak which was released on New Year’s Day 2025 by <a href=\"https://www.mobileread.com/forums/member.php?u=330416\">HackerDude</a></p><blockquote><p>Special thanks to Marek, NiLuJe, Katadelos and all the beta-testers during the development of this jailbreak.</p><p>RIP bricked Kindles during beta-testing  RIP the original deadlines</p></blockquote><ul><li>Your Kindle must be registered</li><li>Your Kindle must have a valid, internet-connected WiFi network saved to it that it can connect to during steps 8 to 10 (inclusive)</li></ul><div><div><div><h2><a href=\"https://kindlemodding.org/jailbreaking/WinterBreak/#installation-guide\" aria-labelledby=\"installation-guide\"></a> Download the latest WinterBreak release: </h2></div><div><div><p>Turn on airplane mode on your Kindle</p><img src=\"https://kindlemodding.org/jailbreaking/WinterBreak/airplane_mode.png\"></div></div><div><div><p>Plug the Kindle into your computer and extract the contents of the `WinterBreak.tar.gz` file to your Kindle</p><p> For Linux/MacOS users, ENSURE the hidden folder `.active_content_sandbox` has been copied to your Kindle </p><img src=\"https://kindlemodding.org/jailbreaking/WinterBreak/file_list.png\"></div></div><div><div><p>Eject your Kindle from your computer and reboot it</p><img src=\"https://kindlemodding.org/jailbreaking/WinterBreak/reboot.png\"></div></div><div><div><p>Open the Kindle Store on your Kindle</p><p>When prompted, click `yes` to turn off airplane mode</p><img src=\"https://kindlemodding.org/jailbreaking/WinterBreak/store_aeroplane.png\"></div></div><div><div><p>Click on the WinterBreak icon when it loads:</p><img src=\"https://kindlemodding.org/jailbreaking/WinterBreak/winterbreak_launcher.png\"></div></div><div><div><p>Wait around 30 seconds, and your Kindle will say something along the lines of \"Now you are ready to install the hotfix\"</p><p>Once it does, you can move onto the post-jailbreak stage!</p><img src=\"https://kindlemodding.org/jailbreaking/WinterBreak/winterbreak_run.png\"></div></div></div></div><h2><a href=\"https://kindlemodding.org/jailbreaking/WinterBreak/#kindle-store-encountered-an-unexpected-error\" aria-labelledby=\"kindle-store-encountered-an-unexpected-error\"></a> Kindle store encountered an unexpected error </h2><p>If an  occurs when you try to log in to the Kindle Store or <strong>only the Kindle Store home page</strong> is displayed, try the following solution:</p><ol><li>Before registering your Kindle/logging into your account - plug your Kindle into your PC, move the WinterBreak files to the root of your storage space</li><li>Log in account and enter airplane mode as soon as possible</li><li>Connect Kindle to PC and delete the cache directory at the path .active_content_sandbox/store/resource/LocalStorage (Skip this step if the LocalStorage directory does not exist)</li><li>Open the Kindle Store on your Kindle</li><li>When prompted, click  to turn off airplane mode</li></ol><ul><li>Crystals (Bricked their PW4 testing)</li><li>mergen3107 (Came up with the “WinterBreak” name)</li></ul></main>","contentLength":1925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43073969"},{"title":"YouTube asks channel owner to verify phone, permanently overwrites personal info","url":"https://old.reddit.com/r/VirtualYoutubers/comments/1iqmul1/if_you_have_a_moment_i_need_your_help/","date":1739755546,"author":"Tijdreiziger","guid":889,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43073836"},{"title":"Homemade polarimetric synthetic aperture radar drone","url":"https://hforsten.com/homemade-polarimetric-synthetic-aperture-radar-drone.html","date":1739755369,"author":"picture","guid":1653,"unread":true,"content":"<div><img src=\"https://hforsten.com/img/sar_fmcw/xdrone_snow.jpg.pagespeed.ic.bjRrZXGlcV.jpg\" width=\"1494\" height=\"883\"><p>Radar drone ready to take off from snow.</p></div><p>I have made several homebuilt radars and done some <a href=\"https://hforsten.com/backprojection-backpropagation.html\">synthetic aperture imaging\ntesting</a> with them on the ground. I have wanted for a long time to put a radar on\na drone and capture synthetic aperture images from air. When I last looked at\nthis few years ago, medium sized drones with payload capability were\naround 1,000 EUR and up. For example in <a href=\"https://pure-oai.bham.ac.uk/ws/portalfiles/portal/136457382/Final_Version_TGRS.pdf\">Low-Cost, High-Resolution, Drone-Borne SAR\nImaging</a>\npaper by A. Bekar, M. Antoniou and C. J. Baker, the imaging results look\nexcellent. They used DJI S900 drone with a list price of about 1,000 EUR. The\nprice for the whole system is quoted to be £15,000, which is a way too\nhigh for my personal budget even just for the price of the drone. Many other\npapers use similar style medium sized drones designed for carrying cameras and\nthey are usually equipped with RTK-GPS for accurate positioning.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xaliexpress.png.pagespeed.ic.AesXyTMvHM.png\" width=\"1031\" height=\"504\"><p>One of many cheap Chinese FPV kits.</p></div><p>Recently small FPV drone prices have dropped a lot. Small 5 and 7 inch propeller\nquadcopters can be bought for about 100 EUR from China (not including battery\nand RC controller). Despite their small size they are able to lift about 1 kg or\neven heavier payload which is plenty for a small radar.</p><p>I bought the cheapest Chinese no-name 7-inch FPV kit and a small GPS+compass\nmodule to support autonomous flying with the goal of making a light weight\nsynthetic aperture radar system that it can carry.</p><p>A single-channel radar can only measure the distance to a target and is unable\nto detect the angle of the target. When multiple receiver channels are arranged\nin a line, the signal travels slightly different distances to each receiver\nbased on the target's angle, causing phase shifts in the received signals. These\nphase shifts allows calculating the angle of the target.</p><p>Angular resolution () of antenna depends on its size\napproximately as: , where  is the\nwavelength, and  is the diameter of the antenna. For example to have\n1 m resolution at 1 km distance requires 0.03° angular resolution with 6 GHz RF\nfrequency. This would require antenna size to be about 100 meters.</p><p>Instead of making a single large antenna, it's possible to move a single radar\nand take multiple measurements at different positions. If the scene remains\nstatic, this approach yields the same results as having one many channel radar\nsystem with big antenna. With synthetic aperture radar it's possible to attach\na single-channel radar to a drone, fly it while making measurements, creating\na large synthetic aperture that provides exceptional angular resolution.</p><p>The design goal for the radar is to get the best imaging performance while being\nable to fit into the FPV drone and achieving it with minimal budget (&lt;500 EUR).\nThe budget limitation rules out using any low-loss RF materials and both\nelectronics and antennas should be implemented with lossy FR4 PCB material.</p><p>The drone is quite small and this limits the maximum size of the radar. The\nwidth of the frame is about 40 mm and propeller tip-to-tip distance is 50 mm\nacross the frame. Length is about 170 mm, which is much more than width and\nmeans that ideally the radar is skinny. For example Raspberry Pi is 56 x 85 mm\nwhich is too wide. The small size severely limits on what the radar can include.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/fmcw_vs_pulse.svg\" width=\"1360\" height=\"450\"><p>Block diagrams of FMCW (left) and pulse radar\n    (right).</p></div><p>There are several possible architectures for the radar. I previously made <a href=\"https://hforsten.com/homemade-6-ghz-pulse-compression-radar.html\">pulse\nradar</a> that is about 64 x 132 mm. That's little too\nwide for the drone, but I believe it would be possible to shrink it a little.\nIssue with that radar is that the maximum bandwidth is about 100 MHz and it's\nlimited by the ADC sampling rate. This corresponds to a range resolution of 1.5\nm, which isn't quite high enough for a detailed image. It's hard to get much\nlarger ADC bandwidth on a reasonable budget and fitting high speed ADCs on the\nlimited space is also an issue. There's also a variation of pulse radar that has\ntwo ramp generators, one for RX and one for TX. This results in low frequency IF\nsignal like with FMCW radar. This is an architecture that is often used on SAR\nradars as it allows for high RF bandwidth without requiring high speed ADC.</p><p>Since pulse radar with switched antenna can't transmit and receive at the same\ntime, pulsed radar maximum pulse length is limited by the time it takes for the\npulse to travel to the target and back. With for example 100 m minimum distance,\nthe maximum pulse length to not miss any part of the pulse is only 670 ns.\nBecause pulse radar needs to divide measurement time between transmission and\nreception it reduces the average transmit power and decreases the\nsignal-to-noise ratio. Large number of very short pulses also complicates the\nimage formation. Time taken by the SAR image formation scales with the number of\npulses and very short maximum pulse length requires transmitting very large\nnumber of them for a good SNR image.</p><p>FMCW radar can transmit and receive at the same time and which improves the\nsignal-to-noise ratio. The maximum sweep length is only limited by the synthetic\naperture sampling speed requirements but it can be hundreds of µs. Unlike pulse\nradar, there is also a minimum sweep length requirement, since reflected signal\nis mixed to the transmitted signal it needs to be received while the sweep is\nstill being transmitted. Long sweep length allows collecting much more reflected\npower per one sweep. Pulse radar could also use separate TX and RX antennas so\nthat this wouldn't be an issue, but that removes its advantages compared to\ncheaper to implement FMCW radar. Separate transmit and receive antennas require\nmore space, but due to large maximum length it should be possible to fit two\nsmall antennas side-by-side under the drone.</p><p>In general FMCW radar has advantage for short range and slow moving platform\napplications. Pulse radar is required when long range (more than few km) is needed.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/sar_fmcw_block.svg\" width=\"690\" height=\"339\"></div><p>Above is the block diagram of the RF parts of the FMCW radar with dual-polarized\nantennas. The sweep is generated by PLL, it's passed through a variable\nattenuator and then amplified by the power amplifier. Most of it is passed to\nthe transmit antenna, polarization switch controls whether vertically or\nhorizontally polarized antenna is used. Part of the transmitted signal is\ncoupled to the receiver mixer, where it's mixed together with the received\nreflected signal that has been amplified by the LNA. Receiver also has\na polarization switch, together with the transmit switch it allows the radar to\nreceiver and transmit any of the four combinations of polarizations. The mixer\noutputs a low frequency signal that is amplified and then digitized by the ADC.\nSome filtering is needed in the receiver to avoid large out of band signals and\nADC aliasing.</p><p>DAC or DDS based sweep generation would likely be better than PLL. DDS phase\nnoise is often better and it can change frequencies essentially instantly\ncompared to PLL, but PLL is chosen because it's cheaper and requires less space. </p><p>RF frequency is going to be around 6 GHz as this is the maximum frequency where\nthere are many cheap RF components for consumer applications. The highest output\npower cheap power amplifiers at this frequency output around 30 dBm. Low noise\namplifiers for the receiver with 1 - 2 dB noise figure can also be obtained\ncheaply.</p><p>Receiver is direct conversion architecture and the mixer does not have any image\nrejection. This causes both frequencies above and below the transmitted signal\nto be converted to the same output frequency. This is not ideal as noise below\nand above the instantaneous sweep frequency is received increasing the noise\nfloor by 3 dB. IQ sampling receiver that could reject the other sideband would\nneed two mixers and ADCs. For only 3 dB increase in the signal to noise\nratio, I didn't think it was worth the cost and PCB space.</p><p>Polarization switches allow choosing which polarization is used to transmit and\nreceive. H is horizontal and V is vertical polarization. This allows measuring\nfour polarizations: HH, HV, VH and VV, where the first letter denotes TX\npolarization and the second RX. Some targets reflect some polarizations more\nthan others and it is used in remote sensing to determine properties of\nreflected targets. For example many smooth targets often reflect the same\npolarization with shape of the target determining if it reflects more HH or VV\ncomponents. Forest and vegetation usually has higher cross-polarized HV and VH\ncomponent reflection compared to roads and bare ground due to multiple\nreflections inside the vegetation.</p><p>Although H and V antennas are drawn separately in the block diagram, this\ndoesn't mean that the system requires four antennas. It's possible to design\nantenna with two ports, one which radiates H and the other V polarization. Dual\npolarized antenna doesn't necessarily need any more space than single polarized\nantenna.</p><p>It would be possible to receive both H and V at the same time if the radar would\nhave two receivers. This would have some advantages, it would allow removing the\nRX polarization switch which would decrease the losses and only the TX should be\nswitched allowing more time for each measurement which would also increase SNR.\nIt would also allow transmitting sweeps faster as there isn't need to multiplex\nthe receiver polarization switch. However, I didn't consider it being worth the\ncost.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/tx_rx_leakage.svg\" width=\"530\" height=\"469\"><p>TX-RX leakage can saturate the receiver on high\n    powered FMCW radar if the leakage is too high.</p></div><p>More RF power generally improves the signal-to-noise ratio, but since FMCW radar\ntransmits and receives at the same time, it's important to consider the TX-RX\nleakage signal. The receiver must be sensitive enough to be able to detect\nthe thermal noise floor at -174 dBm/Hz without saturating due to leaked\nRF power from the transmitter antenna. Typical maximum input power that\nsaturates the LNA is around -20 dBm. With +30 dBm transmitted power more than 50\ndB isolation is needed between transmitter and receiver to prevent the receiver\nsaturation. Even more isolation might be required if some other receiver\ncomponent, such as ADC, saturates first. The variable attenuator before PA can be used to\ndecrease the transmit power in case high enough isolation antennas don't fit in\nthe drone. It also affects the receiver mixer's LO power, but mixers LO input\npower range should be large enough for it to not be an issue.</p><p>The equation for the received power at the receiver input can be written as:</p><div>$$P_r = \\frac{P_t G^2 \\lambda^2 \\sigma}{(4\\pi)^3 r^4}$$</div><p>where  is the transmitter power,  is the antenna gain,  is\nwavelength,  is radar cross section of the target, and  is range to\nthe target. This is the received power from one pulse. Synthetic aperture is\nformed by sending multiple pulses while moving and these can all be coherently\nsummed together to increase the signal to noise ratio. If image is formed from\n pulses, the received power can be multiplied by  to get the received\npower in the whole image.</p><p>Radar cross section  of the target depends on the resolution of the\nradar and radar reflectivity of the ground patch. Target radar cross section\ncan be written as: , where  is\nthe resolution in the range direction,  is resolution in cross-range\ndirection, and  is reflectivity of the ground patch per square meter.\nIn this case , depending on the radar\nparameters, range and imaging geometry. Reflectivity of the ground patch\ndepends on the material of the ground patch and the angle of illumination.\nReflectivity of ground is generally higher when it's illuminated at 90 degree\nangle (in direction of the ground normal vector). This causes the specular\nreflection to reflect back to the radar. At smaller angles there is still some\nreflection back to the radar, but it decreases as the look angle decreases.\nTypical ground reflectivity is around -20 to 0 dBsm (decibels square meter) with\nmoderate look angle.</p><p>The minimum detectable power is limited by thermal noise of the receiver. It\ncan be written as , where  is <a href=\"https://en.wikipedia.org/wiki/Johnson%E2%80%93Nyquist_noise#Derivation\">Boltzmann\nconstant</a>,\n is receiver temperature,  is the noise bandwidth and  is noise figure\nof the receiver. It's a common mistake to confuse the noise bandwidth \nwith RF bandwidth, but they are not related to each other. Noise bandwidth is\nthe minimum bandwidth where receiver can separate noise and signal from each other.\nBy taking Fourier transform of the input signal, we can discard all the\nfrequency bins that are beyond the signal we are currently looking at, and\nnoise at those discarded frequencies won't affect the detection capabilities of\nthe receiver. FFT resolution resolution is equal to , where  is the\nsweep length.</p><p>Setting the received power  equal to noise power and solving for\n we get noise equivalent sigma zero (NESZ) that is value often used\nfor comparing synthetic aperture radars.</p><div>$$\n\\text{NESZ} = \\frac{(4 \\pi)^3 r^4 k F T}{n \\delta x \\delta y G^2 \\lambda^2 P_t t_s}\n$$</div><p>The number of pulses in image  could also be written as ,\nwhere  is the measurement time and  is pulse repetition frequency.\nOr equivalently , where  is the length of the flown\ntrack, and  is the velocity of the drone during measurement. If the antenna\npoints at a constant angle during measurement (stripmap image) the number of\npulses in the image depends on the time that ground patch is illuminated by the\nantenna beam and can be much smaller than the previous number if the antenna beam is\nnarrow. Quadcopter can easily fly pointing in arbitrary angle and it's possible to\nconstantly point the antenna at the target (spotlight imaging) and this\nlimitation doesn't necessarily apply in this case.</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>Number of pulses in image</td></tr><tr></tr></tbody></table><div><img src=\"https://hforsten.com/img/sar_fmcw/nesz.svg\" width=\"720\" height=\"450\"><p>Noise equivalent sigma zero vs range with above\n    listed parameters.</p></div><p>Plotting the NESZ as a function of range gives the above plot. There are some\nparameters that can be adjusted, mainly sweep length and the number of sweeps in\nimage to slightly improve this figure. The requires NESZ for good quality image\ndepends on the actual reflectivity of the ground, but typically for satellite\nbased SAR NESZ is around -20 dBsm. With these parameters we should expect to see\nto around 1 - 2 km with ok image quality.</p><h2>Pulse repetition frequency</h2><div><img src=\"https://hforsten.com/img/sar_fmcw/prf.svg\" width=\"720\" height=\"450\"><p>Minimum alias-free pulse repetition frequency with\n    6 GHz RF frequency and with different number of time-multiplexed channels.</p></div><p>Radar image formation relies on the phase information of the received signal.\nIf we consider a target that is 90 degrees from the antenna beam center (in the\ndirection of movement) to avoid phase ambiguity, the maximum phase difference\nbetween two adjacent measurements needs to be less than 180 degrees. If the\nmovement is larger than this, there can be multiple targets at different azimuth\nangles that have the same phase difference between measurements causing them to\noverlap in the image. Since both transmitter and receiver antennas move,\na distance of a quarter wavelength between two measurements of the same target\nresults in a half-wavelength difference in the distance traveled by the signal\nand a half-wavelength distance difference corresponds to 180-degree phase\ndifference. At this spacing, targets at ±90 degrees will have the same 180-degree\nphase difference between measurements. If the measurement spacing is increased\nfurther more of the image starts to alias.</p><p>If the antenna is very directive then it's possible to use larger measurement\nspacing. Directive antenna won't radiate to large angles that would alias, and\nthe more directive the antenna is, the larger the measurement spacing can be.\nHowever, since the drone is space-limited and antenna directivity is related to\nits size, it might not be possible to design a very directive antenna causing the\nmaximum measurement spacing to be around quarter wavelength.</p><p>The common flying speed for a quadcopter is around 10 m/s, but flying speed can easily\nbe decreased if needed. With 6 GHz RF frequency, the quarter wavelength is 12.5\nmm (0.5 inches) and 10 m/s flying speed means that pulse repetition frequency\nneeds to be at least 800 Hz. Since we have time multiplexed four different\npolarizations, we need to be able to measure all of them in this time.</p><p>PRF sets the requirement for maximum sweep length. With 4 * 800 Hz = 3.2 kHz\nPRF requirement this leaves maximum of 312.5 µs per sweep. However, some time\nneeds to be reserved for time between sweeps due to limited locking time of the\nPLL. The locking time of the PLL is around 20 - 30 µs, leaving 280 µs for the\nmaximum sweep length.</p><h2>Required ADC sampling frequency</h2><div><img src=\"https://hforsten.com/img/sar_fmcw/xadc_fs.png.pagespeed.ic.v9nfqHZoTN.png\" width=\"826\" height=\"470\"><p>Required ADC sampling rate vs maximum range and RF\n    bandwidth for FMCW radar with 250 µs sweep length.</p></div><p>FMCW radar mixes the received signal with a copy of the transmitted sweep,\nresulting in a sine wave signal at the mixer output with frequency depending on\nthe range to the target. If the target is at distance , the IF frequency \ncan be calculated as: , where  is the bandwidth of\nthe RF sweep,  is the speed of light, and  is the sweep length. The\nrange resolution depends on the RF bandwidth  as .\nFor example, 150 MHz bandwidth is required for 1 m resolution, and 300 MHz\nbandwidth results in 0.5 m resolution.</p><p>If we have 300 MHz of RF bandwidth (0.5 m range resolution) and  is 280 µs\nas calculated earlier, then we can calculate the required ADC sampling speed\ngiven the maximum target range we want to detect. For example, with 2 km\nmaximum range, the IF signal frequency is 14 MHz. The ADC sampling frequency\nneeds to be at least double this due to Nyquist sampling requirement, and some\nadditional margin is needed for the anti-alias filter roll-off. This results in\na minimum ADC sampling frequency of about 35 MHz. I chose to use 50 MHz\nsampling frequency.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/sar_fmcw_digital.svg\" width=\"920\" height=\"739\"><p>Block diagram of digital parts.</p></div><p>The amount of data and strict timing requirements of the sweep generation make\nit difficult to handle with microcontroller and FPGA is necessary.\nMicrocontroller is useful for more complicated tasks such as communication with the\ndrone flight controller and ground station, configuring the radar, and writing\nthe data to a filesystem. I decided to use Zynq 7020 FPGA, the same one\nI used in my <a href=\"https://hforsten.com/homemade-6-ghz-pulse-compression-radar.html\">previous pulse radar</a>. It has FPGA fabric \nand a dual-core ARM processor in the same package. This FPGA is nominally 150 EUR\nfrom ordinary distributors, but is available for fraction of that from Chinese\ndistributors.</p><p>The drawback of this FPGA is that the microcontroller doesn't have many high\nspeed connections. For example SD-card and EMMC interfaces are limited to 25\nMB/s, which is below the data rate of the ADC. It does have 1 Gbps Ethernet, but\nusing that would require adding a Raspberry Pi or similar computer, which\nisn't possible due to size constraints. For instance, newer Ultrascale+ FPGAs\nsupport SD-cards with 52 MB/s and EMMC at 200 MB/s speeds, but they cost around\n500 EUR and are not available from Chinese low cost distributors.</p><p>Zynq can have external DDR3 DRAM, but due to space limitations, it's not\npossible to fit enough DRAM to store the whole measurement in memory. With\nlimitation of only one DDR3 module, the memory is limited to 1 GB, while size of\nthe measurement can be several gigabytes.</p><p>This leaves only the option of implementing fast enough external communication\ninterface in the programmable logic side of the FPGA. Luckily <a href=\"https://zipcpu.com/\">Dan Gisselquist\n(ZipCPU)</a> has made GPL3-licensed <a href=\"https://github.com/ZipCPU/sdspi\">SD-card and EMMC\ncontroller</a> that supports faster high-speed\ncommunication modes than the hard IP included with the ARM processor.</p><p>At the time, sdspi controller hadn't been tested with real hardware at the\nspeeds I required. To make sure I have something working if I'm unable to get\nthe sdspi core working I connected the SD-card to the ARM processor's\nintegrated controller, which is limited to 25 MB/s, and EMMC memory to the\nprogrammable logic side that is used with the sdspi controller. This way I'm\nable to at least use the SD-card with the integrated controller if the sdspi\ncore isn't suitable, but this turned out to be unnecessary and the sdspi\ncontroller worked fine. In future versions, it would be better to also connect\nthe SD-card to PL side using sdspi core for faster SD-card speeds.</p><p>I also added FT600 USB3 bridge IC that can be used to connect FPGA to PC. This\nis not needed for drone usage, but allows real-time connection to PC for other\napplications.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/sar_fmcw_fpga.svg\" width=\"939\" height=\"699\"><p>FPGA program block diagram.</p></div><p>FPGA functionality is quite simple at the block diagram level. The design\nprimarily consists of a few independent blocks wired with either DMA or AXI bus\nto the processor. For radar operation, the radar timer block is important as it\nswitches internal and external signals during the measurement. It needs to be\nimplemented in the FPGA fabric to ensure that the timing is clock cycle accurate\nto achieve phase-stable radar measurements. AXI bus is memory-mapped on the\nprocessor side, allowing the radar to be controlled by writing values to fixed\nmemory addresses.</p><p>A decimating FIR filter after ADC data input can be used to change the sample\nrate of the ADC data. It can decimate by 1, 2, or 4. For long-range\nmeasurements, the decimation should be disabled for the maximum IF\nbandwidth. But for shorter-range measurements it makes sense to use higher\ndecimation value to decrease the amount of data that needs to be stored.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xpcb_blocks.png.pagespeed.ic.BoVtjckcn0.jpg\" width=\"1605\" height=\"747\"><p>Labeled PCB 3D model in KiCad. Some 3D models are missing.</p></div><p>The PCB has six layers and is designed to be as compact as possible, with\ncomponents placed closely together to minimize size. Since one-sided assembly\nis cheaper than assembling both sides, the bottom side is empty except for one\nSD-card connector that I will solder myself.</p><p>As with many of my previous radars, the RF part is a relatively small part of\nthe PCB and the overall design effort. Digital electronics and voltage regulators\ntake up the majority of the PCB space.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xpcb_complete2.jpg.pagespeed.ic.md_AJUvoQK.jpg\" width=\"1600\" height=\"695\"></div><p>The radar is designed to accept input voltage from 12 to 30 V and connect\ndirectly to the drone's battery eliminating the need for an external DC/DC\nregulator.</p><p>Due to space constraints, there wasn't enough space to fit four SMA connectors\nand I didn't want to use any miniature RF connectors. The top two connectors\nare switchable TX outputs for H and V polarization antenna inputs, while the\nbottom third one is RX input. The RX polarization switch is located on an\nexternal PCB and connects to one of the three four-pin JST connectors on the\nbottom right of the PCB. Another JST connectors is for flight controller's\nserial port, and the third connector is currently unused, but it could be used to\nconnect for example GPS.</p><p>There are two USB-C connectors: one for JTAG programming and\ndebugging of the FPGA, and the other connects to USB3 to FIFO bridge chip that enables\nfast data transfer to PC. It isn't needed in drone use, but it's useful for\ntesting and other applications.</p><p>PCB dimensions are 113 x 48 mm. Width is just skinny enough to fit on the drone,\nwhile it could have been slightly longer.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xmouser.png.pagespeed.ic.DpWMzE2OOX.png\" width=\"755\" height=\"467\"><p>This component is not sale for individuals.</p></div><p>After ordering the PCB I made an order for the components from Mouser. The order\nseemed to succeed fine and they accepted my money but I got the above email\nafterwards telling me that they can't sell me one of the components. Reason\nseems to be that the supplier of the component has forbidden them for selling it\nto individuals. This was very frustrating as there was absolutely no warnings on\nthe website and I had already ordered the PCBs. I was unable to order this\ncomponent from anywhere, but I did find older obsolete PE43204 pin compatible IC\nfrom obsolete component reseller. It's specified only for maximum of 4 GHz when\nthe original was up to 6 GHz but it does seems to have low enough loss at 6 GHz\nto not cause too large issues.</p><p>After <a href=\"https://www.reddit.com/r/rfelectronics/comments/1dubvj7/psemi_forbidding_resellers_from_selling_to/\">asking on\nreddit</a>\nthe reason is probably that the manufacturer of the component wants to know who\nthey are selling to, to avoid their components ending up in defense\napplications. That's fine, but it would have been nice to know that in advance.</p><div><img src=\"https://hforsten.com/img/sd_fix/xsoldered.jpg.pagespeed.ic.RUxbq7RJf_.jpg\" width=\"1600\" height=\"1105\"></div><p>I did make one mistake: SD-card pins are connected to 1.8 V I/O pins, while they\nshould be connected to 3.3 V I/O and SD-card didn't work with this lower voltage.\nThe radar could be used without SD-card by storing the data into EMMC instead\nand then reading it through USB, but SD-card would be much easier to use.\nI really didn't want to order another PCB to just fix this one mistake and\nI managed to fix it by designing a small interposer PCB with level shifter that\nI soldered on top of the previous SD-card footprint. I wrote about it in more\ndetail in a <a href=\"https://hforsten.com/fixing-incorrectly-wired-sd-card-connector-with-interposer-pcb.html\">previous post</a>.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xheatsink.jpg.pagespeed.ic.sB05RnfhPq.jpg\" width=\"1500\" height=\"1125\"><p>Aluminium PCB heatsink under the radar PCB.</p></div><p>Power amplifier can get quite hot if the transmit duty cycle is high. To keep it\ncool I ordered custom aluminium substrate PCB that I bolted under the radar PCB.\nSolder mask is removed under the PA and thermal pad is placed between the PCB\nand heatsink. This cost only $4 for 5 pieces and works very well.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xfc.jpg.pagespeed.ic.uKdKPYdZai.jpg\" width=\"1600\" height=\"1200\"><p>Speedybee F405 V3 flight controller.</p></div><p>Flight controller came with the drone kit. The included flight controller is\n<a href=\"https://www.speedybee.com/speedybee-f405-v3-bls-50a-30x30-fc-esc-stack/\">Speedybee F405\nV3</a>.\nThis is a cheap low-end flight controller with 1 MB of flash. It does the job,\nbut I would recommend getting a little bit better flight controller with\n2 MB of flash, the price difference isn't very large.</p><p>There are several possible flight controller softwares. The three most used for\nFPV drones are: <a href=\"https://betaflight.com/\">Betaflight</a>,\n<a href=\"https://github.com/iNavFlight/inav\">Inav</a>, and\n<a href=\"https://ardupilot.org/\">ArduPilot</a>. The main differences of them are:\nBetaflight focuses on fast-response manual flying and doesn't have autonomous\nflight support, Inav shares lot of code with Betaflight and it includes some\nautonomous flight support, and ArduPilot has the most advanced autonomous flight\ncapability with lot of features but it's more challenging to configure.</p><p>I chose to use Ardupilot and found it to be excellent for this purpose. It has\nvery good IMU and GPS sensor fusion algorithm that is very helpful for\nimproving the position accuracy. The flight controller can communicate with the\nradar through a serial port allowing it to enable and disable the radar during\nautonomous mission and provide position information for the radar.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xgps.jpg.pagespeed.ic.-CwnL_WoYm.jpg\" width=\"1600\" height=\"1200\"><p>GPS with integrated compass. It needs to be\n    mounted far away from the battery leads to avoid magnetic fields causing issues\n    with the compass.</p></div><p>SAR imaging needs very accurate position information for proper image focusing.\nPosition information should be accurate within to a fraction of wavelength,\nthis is just few cm (1 - 2 inches) at this frequency. Many commercial SAR\nimaging drones use RTK GPS with a second stationary GPS receiver on the ground\nit's possible to obtain about 1 cm accurate positioning. The drawback\nis that it's much more costly than regular GPS and RTK GPS receivers are\nusually much larger than ordinary GPS receivers which makes it very difficult\nto fit it in this drone.</p><p>Good non-RTK GPS might be accurate to about 1 m accuracy. This large\npositioning errors cause significant errors in the image if not corrected.\nLuckily, it's possible to solve for position error from the radar data which is\ncalled autofocusing. With drawback of needing more processing during the image\nformation for autofocusing, it's possible to use regular GPS. Sensor fusion\nwith inertial measurement unit (IMU) can be used to improve accuracy of the\npositioning and obtain position updates faster than the maximum of about 4 Hz that\nis possible with only GPS.</p><p>For autonomous flying the drone's flight computer also needs GPS, IMU and\ncompass. It would be a waste of space to have a second GPS and IMU for just the\nradar and instead I'm relying on the flight computer to output its position\nestimate to the radar through a serial interface.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/sar_fmcw_drone.svg\" width=\"930\" height=\"360\"></div><p>The drone is controlled with a radio controller. I use\n<a href=\"https://www.expresslrs.org/\">ExpressLRS</a> radio link, which is very common with\nFPV drones. The drone also has a radio link to the ground control software\nrunning on PC. This is needed for programming the autonomous mission parameters,\nchanging drone settings, it can be used to control the drone, and it displays\ntelemetry during the flight. Ground station can also be used to send messages to\nradar through the flight controller and this allows programming the radar\nparameters from the laptop.</p><p>Typically Ardupilot has required using two radios. One for radio controller and\nother for telemetry, but ELRS recently added <a href=\"https://www.expresslrs.org/software/mavlink/\">Mavlink\nsupport</a> that allows using\na single radio for both purposes. This is very convenient in this application.</p><p>In theory, the wider the antenna beam width is, the better the resolution of\nSAR image is. <a href=\"https://topex.ucsd.edu/rs/sar_summary.pdf\">A famous result in SAR imaging</a> is that the best possible\ncross-range resolution in strip mode SAR (fixed antenna angle and straight\nbaseline) is , where  is the length of the antenna. However, in\npractice wider antenna beam isn't often better. Wider antenna beam means lower\ngain which decreases the signal to noise ratio and limits the maximum range.\nAntenna gain is squared in the link budget so halving the antenna gain, means\nthat number of pulses need to be quadrupled to get the same SNR.</p><p>The cross-range resolution depends on the length of the baseline where the\ntarget is visible and the wider the antenna beam is the longer this is. With\nspotlight imaging, where antenna tracks the target, cross-range resolution is\nnot limited by the antenna beam width and spotlight imaging is easy with drone.\nWith drone SAR the maximum possible baseline length is often the limiting\nfactor for resolution as it's hard to fly very long track when limited by\nvisual line of sight.</p><p>The azimuth angle resolution in spotlight imaging mode (or in stripmap mode\nwhere antenna beam always covers the target) can be approximated as: , where  is wavelength and  is the length\nof the track. Cross-range resolution can be obtained with: , where  is distance to the target.</p><p>With drone SAR a big issue is how to fit large enough antennas on the drone.\nSince the radar is FMCW, separate transmitter and receiver antennas are needed\nfurther decreasing the space per antenna and low TX-RX leakage requires having\nsome distance between them.</p><p>I have previously used <a href=\"https://hforsten.com/horn-antenna-for-radar.html\">self-made horn antennas</a>\nwith my radars, but they are too big to fit on drone. The whole length of the\nhorn antenna is 100 mm, and even just the coaxial-to-waveguide transition is 25\nmm long. This size makes it impossible to fit on the drone, which has only 50 mm\nspacing between propeller tips. My previous horn antenna isn't dual-polarized,\nbut horn can be made dual-polarized easily by adding two feeds 90 degree apart.</p><p>Patch antenna can be made much smaller since they are just copper on a PCB. They\ncan also be made dual-polarized with two feeds 90 degree apart. However,\na simple patch on 1.6 mm thick FR4 PCB has poor bandwidth, FR4 dielectric\ninaccuracy can cause frequency shift, and the gain isn't very high. Gain can be\nimproved by making an array of patches, but with FR4 substrate the losses in\nfeeding network increase quickly.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/stacked_patch.svg\" width=\"700\" height=\"220\"><p>Stacked aperture coupled patch antenna. Side (left) and top (right) views.</p></div><p>Neither horn nor patch antenna seemed suitable. After reading some scientific\npapers I found this <a href=\"https://digital-library.theiet.org/doi/10.1049/el%3A20010828\">dual-polarized slot-fed stacked patch\nantenna</a> paper. It\nconsists of patch antenna that is fed by microstrip lines that couple to patch\nthrough H-shaped slots in the ground plane. Two feed lines and slots 90 degree\napart can be used to make it dual-polarized. A second patch is suspended few mm\naway from the first patch with air in between them. This structure can achieve\nmuch wider bandwidth than a single patch making it tolerant to frequency shift\ncaused by inaccuracy of FR4 permittivity. The second patch also slightly\nincreases the gain.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xantenna_front.jpg.pagespeed.ic.rIaN34zEdw.jpg\" width=\"1600\" height=\"1200\"><p>TX and RX patch fed horn antennas.</p></div><p>However, more gain would be good to have for improving the signal-to-noise\nratio. Sidelobes at 90 degree angle should also be lower to decrease TX-RX\nleakage. To solve this I added a sheet metal horn structure around the antenna,\nmaking it a stacked patch fed horn antenna. The horn increases the height by 10\nmm, but even just a sheet metal wrapped around the patch gap increases the gain\nand decreases sidelobes without increasing the height. I haven't found\na similar structure in any publications, but it wouldn't surprise me if some\nexist, as it does seem straightforward.</p><p>A pyramidal horn with filled-in corners would likely offer slightly higher gain\nand be mechanically stiffer than this four-flap design. However, this was\neasier for me to manufacture. I cut the copper sheet by hand with scissors and\nsoldered it to keep it together.</p><p>This antenna has everything I wanted. It's dual-polarized, has very wide\nbandwidth, good gain, isn't as tall as similar gain horn antenna fed with\ncoaxial-to-waveguide transition, and it's cheap to manufacture requiring just\ntwo FR4 PCBs, some copper sheet and few bolts and spacers. FR4 is lossy and gain\nwould likely be around 0.5 - 1.0 dB higher if a proper low loss RF material was\nused, but the cost would be about 100x higher in prototype quantities and it\ndoesn't make sense for me to spend so much more for so little improvement.</p><p>Between the antennas is a small 0.25 x 0.5 wavelength wall that decreases TX-RX\ncoupling. I tested few different size walls and this small wall is more\neffective than no wall and also more effective than taller walls.</p><p>Not counting the SMA connectors, the height of the antenna is 18 mm, of which 10\nmm is the height of the horn above the suspended patch. Total height including\nthe SMA connector is 28 mm. Patch substrate dimension is 45 x 45 mm and the horn\naperture is 65 x 65 mm.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xantenna_back.jpg.pagespeed.ic.lqa6vy_OIa.jpg\" width=\"1600\" height=\"1200\"><p>Backside of the antennas. Each antenna has two SMA\n    connectors, one for H and other for V polarization.</p></div><p>Backside of the antennas is covered by copper sheet to decrease the backwards\nradiation. This is needed because antennas are mounted right on top of the radar\nPCB which doesn't have any shielding. Without shielding TX antenna backwards\nradiation would increase the TX-RX coupling. Copper sheet is also inserted into\nthe wall between antennas and the tape keeps it in place. </p><div><img src=\"https://hforsten.com/img/sar_fmcw/antenna_gain.svg\" width=\"720\" height=\"450\"><p>Simulated radiation pattern of the antenna.</p></div><p>The simulated -3 dB beamwidth is 50/60 degrees in 0/90 degree angles. H and\nV feed slots are rotated 90 degrees and the radiation pattern from the other\nport is similar but rotated 90 degrees. Simulated peak gain is 10.0 dB.</p><p>Sidelobe to 90 degree direction is about -10 dB. It's important for this to be\nlow to minimize TX-RX leakage.</p><p>Since the antenna radiation pattern isn't quite symmetrical, mounting the other\nantenna at a 90-degree rotation compared to the first one ensures good antenna\npattern matching between HH and VV polarizations. If the TX antenna transmits\nH polarization from the first port, RX antenna receives H polarization from the\nother port, and other way around for V, causing the two-way antenna pattern to\nmatch between the both cases. However, HV and VH antenna patterns don't match\nthe HH and VV patterns, since in the cross-polarization case the same port is\nused to transmit and receive on both antennas.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/antenna_sp.svg\" width=\"720\" height=\"450\"><p>Measured S-parameters of the antenna.</p></div><p>There is a slight difference in the H and V port matching due to their slightly\ndifferent coupling slot sizes. Antenna useable bandwidth is from about 4.5 GHz\nto 6.2 GHz which is more than enough for this application.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xblender.png.pagespeed.ic.NqxqsjpHF0.jpg\" width=\"1227\" height=\"703\"><p>Drone mechanical model in Blender.</p></div><p>The drone needs some mechanical parts to hold the radar on the drone frame.  The\nflight controller is mounted inside the frame, but there isn't enough room\ninside it for the radar, so I designed a 3D printed mount that holds the radar\nPCB under the drone frame. This mounting position also requires landing legs so\nthat the drone doesn't land on the radar. I designed it in Blender since I don't\nknow any mechanical CAD programs. It works just fine for these simple parts.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xassembly_radar_attached.jpg.pagespeed.ic.zvjKnRgfp_.jpg\" width=\"1600\" height=\"1151\"><p>Radar attached under the drone.</p></div><p>The radar mount holds the radar PCB on the drone frame. It would be a good idea\nto have some sort of weather proof enclosure for it, but I haven't got around it\nyet. I added some material over the PCB to protect it in case the landing legs\nfail.</p><p>Radar holder attaches with four screws to the drone. Drone radio controller\nantenna is visible at the bottom left. Propellers are collapsible, these make it\nmuch easier to transport the drone as with these it fits in a backpack.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xassembly_switch.jpg.pagespeed.ic.yCmbx9kxzj.jpg\" width=\"1600\" height=\"1173\"><p>Receiver polarization switch PCB.</p></div><p>Antenna board is held with two bolts that allow its angle to be changed. Flight\ncontroller serial port is attached to one of the JST connectors. There also\nother JST connector from flight controller that is currently unused and just\nheld with tape in place.</p><p>Transmitter has polarization switch and two SMA connectors on the PCB, but the\nreceiver polarization switch is on a separate PCB due to lack of space. I had\na mounting holes for it on the radar holder part, but the SMA cables are stiff\nenough that I found it easier to just leave it hanging there. Polarization\nswitch connects to the radar PCB with another JST connector.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xassembly_legs.jpg.pagespeed.ic.jsm1QEk8ZH.jpg\" width=\"1600\" height=\"1200\"></div><p>Landing legs are 10 cm diameter carbon tubes with 3D printed TPU caps. Smaller\ndiameter tube would have likely been fine, these are very stiff and something\nelse would fail before them if they are stressed too much in landing.</p><p>Radar is powered directly from the drone battery. XT60 splitter is used to\nconnect both flight controller and radar to the same battery.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xassembly_weight.jpg.pagespeed.ic.AvMr3hfOmK.jpg\" width=\"1600\" height=\"1353\"><p>Drone balanced on a kitchen scale.</p></div><p>Weight of the drone without battery is just 752 grams (1.66 lbs). I have two six\ncell LiPo batteries, the smaller 1300 mAh capacity battery weights 196 grams and\nthe bigger 2200 mAh battery weights 322 grams. With the smaller battery the total\nweight of the whole system is just 948 grams.</p><p>Radar measures the distance and phase of each target. To convert these\nmeasurements into a radar image, matched filtering can be used. For each pixel\nin the image, generate a reference signal corresponding to what a target at that\nposition would reflect. Multiply the measured signal with the complex conjugate\nof the reference signal for each measurement, then sum these products over\nall measurements. When the measured signal closely matches the reference signal,\ntheir product becomes large because the phases align. If it doesn't match,\nthe result of the multiplication is a complex number with a random phase, and summing\nrandom complex numbers will average out generating a low response.</p><p>The image formation can be written as:</p><div>$$I = \\sum_{p \\in \\mathcal{P}} \\sum_{n=1}^N S_n(d(p,x_n)) \\cdot H(d(p,x_n))^* $$</div><p>, where  is the set of pixels in the image,  is the number of\nradar measurements,  is Fourier transformed measured IF signal ,\n is the distance to location of pixel  from radar position at that\nmeasurement , and  is complex conjugate of the reference function of\nwhat target at that position in the image should look like (Fourier transform of\nthe radar IF signal from target at that position in the image).</p><p>This is called backprojection algorithm. It's simple and doesn't make any\napproximations or assumptions about flight geometry, but it's very demanding to\ncalculate. For example with 1 km x 1 km image with 0.3 m resolution and 10,000\nradar sweeps, calculating the image needs  backprojections. This means over 100 billion complex exponentials and\nsquare root calculations is needed for one image, and image size and number of\nsweeps can be even larger in practice. There are some clever algorithms that can\nbe used to speed this up, but they often have approximations or only work with\nlinear flight tracks. One easy improvement that can be made without many\ndrawbacks, is to use polar coordinates instead of Cartesian coordinates, as that\nrequires less pixels in the image since angular resolution is constant while\ncross-range resolution is better closer to the radar. Polar coordinate image can\nthen be afterwards interpolated to Cartesian grid.</p><div><table><tbody><tr><td><div><pre></pre></div></td><td><div><pre><code></code></pre></div></td></tr></tbody></table></div><p>Some years ago this would have been unbelievable amount of compute, but with\nmodern GPU this can be calculated in under a second. This problem is especially\nwell suited for GPU implementation since every pixel can be calculated\nindependently in parallel. Very straightforward CUDA kernel is able to calculate\n220 Billion backprojections per second on RTX 3090 Ti GPU. This is very\nrespectable speed considering that each backprojection requires square root and\ncomplex exponential (which can be calculated with just sin and cos). I'm sure\nthat someone experienced with CUDA programming could make this even faster as\nthis doesn't have any optimizations or approximations and is just the direct\nalgorithm implementation.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/autofocus.svg\" width=\"639\" height=\"879\"></div><p>Positioning accuracy of the GPS and IMU isn't good enough to form a high-quality\nimage. Ideally, the position should be known to a fraction of the wavelength,\nbut accuracy of the GPS isn't good enough. To achieve good image quality, an\nautofocus algorithm is necessary to focus the radar image using information from\nthe radar data.</p><p>The most commonly used autofocus algorithm is <a href=\"https://ieeexplore.ieee.org/abstract/document/303752\">phase gradient\nautofocus</a>. It's simple\nand fast autofocus algorithm that works by taking an unfocused radar image as an\ninput and solving for a phase vector that when multiplied with the azimuth FFT\nof the image gives a focused image. However, it doesn't work well in this case\nsince the azimuth beam is wide and the radar baseline is long causing the\nfocusing errors to be spatially dependent.</p><p>I updated my <a href=\"https://hforsten.com/backprojection-backpropagation.html\">previous backpropagation autofocus</a> to\nuse PyTorch and made some improvements. This autofocus algorithm works by\nforming the radar image, calculating the gradient of the input velocity, and \nclipping the learning rate to limit the maximum position change to a predefined\nvalue. The input velocity is then updated using a gradient descent optimizer.\nI found that using a 3D position doesn't work well, as it often tends to just nudge\neach position in random directions. Instead, using velocity and integrating it\nto position seems to yield much better results. A small regularization term is\nalso included to minimize the distance between the optimized and original\npositions, favoring smaller updates.</p><p>Adjusting the learning rate based on the maximum position change makes it easier\nto set the optimizer meta parameters. Instead of setting the learning rate\ndirectly, maximum position update is given which is used to set the learning\nrate.</p><p>This is very general autofocus algorithm that makes no assumptions about the\nradar system, scene, or the flight track. The obvious disadvantage is that it\nrequires forming the radar image many times making the already slow image\nformation many times slower. Without the fast GPU image formation this would be\ntoo slow to be useful.</p><p>The autofocus algorithm is available on\n<a href=\"https://github.com/Ttl/torchbp\">Github</a>.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xmission_planner.jpg.pagespeed.ic.dhNylMMORO.jpg\" width=\"1209\" height=\"1040\"><p>Configuring the mission in ArduPilot Mission\n    Planner.</p></div><p>The mission is programmed beforehand with the ArduPilot Mission Planner. The\ndrone will automatically fly the programmed waypoints, there are also commands\nto set the ROI (region of interest) so that antenna always points towards it,\nand the radar measurement is started with digicam configure command in the\nmission. It's originally meant for configuring ordinary camera, but\nI programmed the radar microcontroller to listen to it. Using an existing\ncommand makes it easy to make the radar work with the existing ArduPilot software.</p><p>Setting ROI, which is needed for spotlight imaging, needs a patch to ArduPilot\nfirmware. By default drone's front will always point towards the ROI and there\nisn't a way to configure it to point the antenna towards ROI instead. The patch\nis available on the <a href=\"https://github.com/ArduPilot/ardupilot/pull/28486\">ArduPilot Github as a pull request</a>.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xaction.jpg.pagespeed.ic.W_4RucSliH.jpg\" width=\"1800\" height=\"1910\"></div><p>The scene is a wide open field. There's about 1.5 km distance to the forest at\nthe antenna pointing direction. The drone flies at 110 m altitude in a straight\nline for about 500 m at 5 m/s velocity. The radar was configured to transmit\nonly VV polarization with 400 µs long sweep, 500 MHz bandwidth, and 1 kHz\npulse repetition frequency.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xraw_data.png.pagespeed.ic.3MAaZDezzS.png\" width=\"826\" height=\"470\"><p>Range compressed raw data.</p></div><p>The range compressed (Fourier transformed) captured data doesn't look very\nimpressive. It doesn't look anything like an image since due to the wide\nantenna beam at each sweep many targets at different angles are captured.</p><p>At the zero distance there is a large response from the TX-RX leakage, then the\nnext reflection is at 100 m distance from the ground. Even though the antenna\ngain at directly below is much smaller than at the beam center, due to the angle\nof the reflection and close distance, the reflection from directly below is very\nlarge. At large distances reflections are mostly below the noise floor of\nindividual sweeps, but during image formation many sweeps are integrated\nimproving the signal to noise ratio. Some large individual objects are visible\nand their distance to the radar changes as the drone moves.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar2_scatter.png.pagespeed.ic.VbD8wZkpX6.png\" width=\"826\" height=\"470\"><p>Recorded drone position and antenna pointing\n    vector from GPS and IMU. Note the unequal axes scale.</p></div><p>Ideally the track should have been a straight line but there\nis some disturbances due to for example wind. The drone is very light and even\na slight wind can easily affect it. The ROI was set quite far away there is only\nfew degree of change in the antenna pointing direction during the measurement.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar_img2_polar.jpg.pagespeed.ic.D5PO668zBU.jpg\" width=\"1579\" height=\"848\"><p>SAR image without autofocus.</p></div><p>Above is the processed SAR image without autofocus in pseudo-polar coordinates\nthat the image formation uses internally. It's pseudo-polar because angle axis\nis in sine of radians instead of just radians, this is slightly more efficient\nthan ordinary polar coordinates. Image size is 6k x 20k pixels using 51,200\nsweeps.</p><p>Compared to the raw data it's a night and day. Various geographical features can\nbe now identified, but polar format makes it hard to compare to the map.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar_img2_no_opt.jpg.pagespeed.ic.KS5LQogwyn.jpg\" width=\"962\" height=\"843\"><p>SAR image without autofocus.</p></div><p>Cartesian coordinate image can be obtained by projecting the pseudo-polar image\nto Cartesian grid. This is very fast operation compared forming the image\ndirectly on the Cartesian grid. The image is also aligned so that north points\nup using the drone's electronic compass measurements. Left corner is missing\na small patch of data due to the rotation.</p><p>The resulting image is still quite blurry. Clearly only relying on the GPS and\nIMU positioning isn't good enough and autofocus is needed to get a sharply\nfocused image.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar_img2_opt_cart.jpg.pagespeed.ic.qsHkmEQMW8.jpg\" width=\"962\" height=\"843\"></div><p>After applying 30 iterations of the minimum entropy gradient optimization\nautofocus, the image quality is much better. Five iterations would have been\nprobably enough, but using more iterations does improve the quality slightly.\nThis does take several minutes since each iteration requires calculating forward\nand backwards pass of the backprojection.</p><p>Due to the low look angle, tall structures such as trees cast long shadows. The\nimage amplitude isn't normalized, which is why it's brighter closer to the\norigin.  The antenna radiation pattern can be also visualized in the image. The\nbeam center is tilted slightly to the right, and the antenna gain at the left\nside of the image is much smaller due to it being farther from the beam center,\ncausing it to be dimmer.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar2_img_detail_comparison.jpg.pagespeed.ic.6LSaGzsHo3.jpg\" width=\"1478\" height=\"726\"><p>SAR image detail comparison. Without autofocus\n    (left) and with minimum entropy optimization autofocus (right).</p></div><p>There is quite lot of detail in the resulting radar image when zooming in.\nComparing a 300 x 300 m patch, the autofocused image reveals surface details of\nthe field that was just blur in the image without autofocus.</p><p>I also tried using phase gradient autofocus, but it doesn't work well in this\ncase. <a href=\"https://hforsten.com/img/sar_fmcw/sar2_pga_detail.jpg\">The result</a> is very similar to\nthe image without autofocus.</p><p>The three lines at the bottom left are power lines. They seem to be only visible\nin the image when the radar is looking at them at 90 degree angle, at other angles\nthe reflectivity is so low that they are invisible.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar2_v_opt.png.pagespeed.ic.PlTlyyA2yG.png\" width=\"826\" height=\"470\"><p>Original and optimized velocity.</p></div><p>Comparing the drone velocity before and after optimization the changes aren't\nvery large. Along the track and range direction velocity components are both\nadjusted slightly and height direction velocity component is basically\nunchanged.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar3_map_1100.jpg.pagespeed.ic.N65ySzsgck.jpg\" width=\"693\" height=\"650\"><p>Google maps screenshot of the SAR imaging area.</p></div><p>I also made another measurement at other location using all four polarizations.\nThe radar flies a linear track autonomously as before, but now the radar quickly\nswitches between each of the four polarization switch states. Sweep length was\nreduced to 200 µs, pulse repetition frequency is 715 Hz for each\npolarization, and other parameters are kept the same. Number of sweeps in the\nimage is the same 51,200.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar3_x4.jpg.pagespeed.ic.HQkjidC97p.jpg\" width=\"721\" height=\"705\"><p>Four SAR images with different polarizations.</p></div><p>The four polarizations look very similar. The main differences are that\ncross-polarization images (HV and VH) have weaker amplitude due to\ncross-polarization component in general being smaller than the reflection of the\nsame polarization.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar3_pol_1100.jpg.pagespeed.ic.sZ46cglYon.jpg\" width=\"905\" height=\"854\"><p>Polarized SAR image with autofocus.</p></div><p>Instead of looking at four different images for each polarization, it's common\nto use RGB color channels for different polarizations in the same image. In this\ncolored image it's easier to visualize how each target reflects specific\npolarizations. The ground is tinted purple, indicating that it\nreflects VV and HH polarization better than the cross-polarized components. The same\ncan be seen on the buildings and in the light poles along the road. Forest areas\nare colored white as they reflect all polarizations about equally. However,\nsince the effects of the antenna radiation pattern and possibly slightly\ndifferent losses for different polarization switch states are not calibrated\nsome of the observed differences could be attributed to the hardware. Better\naccuracy measurements would require calibration.</p><p>The area with bunch of points around (200, 500) meters is a some kind of garden\nof small trees each surrounded with a metal wire mesh.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xsar3_scene.jpg.pagespeed.ic.sIO6ntsoPj.jpg\" width=\"1200\" height=\"900\"><p>Picture at the ground at (-50, -80) m coordinates\n    in the SAR image looking towards negative Y-axis.</p></div><p>There was slight amount of snow on the ground during the measurements. The\nvisible picture is from the top of the SAR image looking down. The small forest\non the left is the small patch of trees in the middle of the image.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/xvideosar_mission.jpg.pagespeed.ic.WxdiRT0GqX.jpg\" width=\"824\" height=\"773\"><p>Drone waypoints for the octagonal flight path. Red\n    marker is the region-of-interest where drone points the antenna.</p></div><p>The previous measurements synthesized one high-resolution image from a long\nbaseline. It's also possible to synthesize many images with small baselines from\none long measurement, and these many images can then be turned into a video.</p><p>For the backprojection algorithm, the flight track doesn't need to be linear and for\nthis case I programmed the drone to fly octagonal track while pointing the\nantenna at the octagon's center.</p><p>Each frame has 1024 radar sweeps with 512 of them overlapping with the previous\nframe.  Since each frame has less sweeps than the previous full images, the\nframes are noisier and have worse angular resolution. The video is sped up by\nabout 10x. All four polarizations are used, and the image colorization is the\nsame as in the previous polarized SAR image.</p><p>Frames are autofocused separately and there isn't any alignment of adjacent\nframes, which causes the frames slightly wobble or jump around occasionally. Corners\nof the octagon are especially challenging for the image formation since both\nalong- and cross-range positions need to be solved accurately for a good-quality\nimage. Angular resolution can also vary between frames as the baseline\nlength between the frames can vary, as only the number of sweeps is the same\nbetween the frames.</p><p>Natural targets such as the ground and forest look very similar at different frames,\nbut at several points in the video large reflections can be seen for example at\nbridge and power lines when they are oriented at a 90-degree angle to the radar.\nThe bright spot that looks like it's moving at the bridge is just glint from the\nrailing. The mismatch between antenna patterns of different polarizations is\nalso visible as the same target can have slightly different color at the beam\ncenter or at the edge.</p><div><img src=\"https://hforsten.com/img/sar_fmcw/look_angle.svg\" width=\"720\" height=\"450\"><p>Radar look angle with 120 m flying height.</p></div><p>Without special permits, it's allowed to fly the drone at a maximum altitude of\n120 m. Usually, for SAR imaging, the look angle is around 10 to 50 degrees. If the\nlook angle is close to 90 degrees (i.e., looking straight down at the ground), the\nreflected power is high, but the range resolution is poor as the distance to the\nradar is almost the same for nearby locations. For low look angles the range\nresolution is good, but due to the low grazing angle the reflected power back to\nthe radar is low. With extremely low look angle the reflected power can be about\n10 to 20 dB lower than it would be compared to more usual around 45 degree look\nangle reducing the maximum distance the radar can see.</p><p>Another issue is shadowing caused by tall objects.  For instance, when flying at\n120 m height, the grazing angle at 2 km distance is only 3.4 degrees. A 10\nm tall tree casts 170 m long shadow at this low angle, making it impossible to\nsee any reflections from the ground after the tall object. This is clearly\nvisible in all of the measurements. Especially in the full-polarization\nmeasurement only the tops of the buildings are visible at long distances.</p><div><a href=\"https://hforsten.com/img/sar_fmcw/sar_fmcw.pdf\" target=\"_blank\"><img src=\"https://hforsten.com/img/sar_fmcw/xschematic_top.png.pagespeed.ic.lsgycncoqY.png\" width=\"805\" height=\"565\" border=\"2\"></a><p>Schematic of the radar (click to open).</p></div><p>The synthetic aperture radar drone can image at least up to 1.5 km and likely\neven farther if flown higher. It weighs under 1 kg including the radar, drone,\nand battery. The system can capture HH, HV, VH, and VV polarizations.\nA gradient-based minimum entropy autofocus algorithm is capable of producing\ngood good-quality images with a wide antenna beam using only non-RTK GPS and IMU\nsensor information. The total cost of the drone was about 200 EUR, 600 EUR for\ntwo radar PCBs, and about 10 months of my free-time after work. I'm very happy\nwith the performance of the system considering its low cost.</p>","contentLength":53379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43073808"},{"title":"ISC Stormcast For Monday, February 17th, 2025 https://isc.sans.edu/podcastdetail/9326, (Mon, Feb 17th)","url":"https://isc.sans.edu/diary/rss/31690","date":1739755324,"author":"","guid":886,"unread":true,"content":"<article>\n \n (c) SANS Internet Storm Center. https://isc.sans.edu Creative Commons Attribution-Noncommercial 3.0 United States License.</article>","contentLength":126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"697-Page Book Publishes a Poet's 2,000 Amazon Reviews Posthumously","url":"https://news.slashdot.org/story/25/02/17/016217/697-page-book-publishes-a-poets-2000-amazon-reviews-posthumously?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739754540,"author":"EditorDavid","guid":843,"unread":true,"content":"The Cleveland Review of Books ponders a new 697-page hardcover collection of American poet/author Kevin Killian's.... reviews from Amazon. (Over 2,000 of 'em — written over the course of 16 years.)\n\nIn 2012, he wrote three substantial paragraphs about the culinary perfection that can be found in a German Potato Salad Can (15 oz., Pack of 12). Often, he'd open with something like \"as an American boy growing up in rural France.\" Killian grew up on Long Island, New York. He didn't take himself (or much else) too seriously.... \n\n[Killian] was also a member of the New Narrative Movement... Writers acknowledge the subjectivity of, and the author's active presence in, the text... Amazon reviews are a near-perfect vehicle for New Narrative's tenets... Killian camouflaged his reviews in the cadence of the Amazon everyman. He embraced all the stylistic quirks, choppy sentence fragments and run-ons, either darting from point to point like a distracted squirrel or leaning heavily into declarative statements.... About the biographer of Elia Kazan, he tells us, \"Schickel is in love with the sound of his voice, and somewhere in the shredded coleslaw of his prose, a decent book lies unavailable to us, about the real Elia Kazan....\" \n\n[T]he writing can move from very funny to strangely poignant. One of my favorites, his review of MacKenzie Smelling Salts, begins with a tragically tongue-in-cheek anecdote about his Irish grandfather: \n\n\"My Irish grandfather used to keep a bottle of MacKenzie's smelling salts next to his desk. He was the principal at Bushwick High School (in Brooklyn, NY) in the 1930s and 1940s, before it became a dangerous place to live in, and way before Bushwick regained its current state of desirable area for new gentrification. And he kept one at home as well, in case of a sudden shock. At school, he would press the saturated cotton under the nostrils of poor girls who realized they were pregnant in health class, before he expelled them.\" \n\nHe ends with his own reasons for using smelling salts, citing wildly diverging examples: his grief upon learning of the death of Paul Walker from the Fast &amp; Furious film franchise abuts Killian's disappointment at not being selected for the 2014 Whitney Biennial. Apparently, both were deeply traumatic experiences for Kevin... [\"it took my wife a minute or two to locate the MacKenzie's, but passing it under my nose, as though she were my grandfather ministering to the pregnant girls of yore...\"] \n\nNo one wants to be forgotten. I do not think it's a coincidence Killian started writing the reviews after his heart attack. Why did he keep going? Most likely, it was because he enjoyed the writing and got something out of it — pleasure, practice, and a bit of notoriety. But mainly, I think the project grew out of habit and compulsion. In a similar way, the graffiti art of Keith Haring, Jean-Paul Basquiat, and Banksy began in subway tunnels, one tag and mural at a time, until it grew into bodies of work collected and coveted by museums worldwide. In Killian's case, the global commerce platform was his ugly brick wall, his subway platform, and his train car. Coming away, I like to imagine him gleefully typing, manipulating the Amazon review forums into something that had little to do with the consumerism they had been created to support: Killian tagging a digital wall to remind everyone KEVIN WAS HERE. \n\nThe book reviewer points out that the collection's final review, for the memoir Never Mind the Moon: My Time at the Royal Opera House, is dated a month before Killian died. \n\n\"Unfortunately, the editors of this volume did not preserve the Helpful/Not Helpful ratings, only the stars.\" \n\nPutting it all in perspective, the book critic notes that \"In 2023, Amazon reported that one hundred million customers submitted one or more product reviews to the site. The content of most is dross, median.\" Though the critic then also acknowledges that \"I haven't read any of Killian's other work.\"","contentLength":3988,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can sim drivers make the shift to F1? Max Verstappen thinks so","url":"https://techcrunch.com/2025/02/16/can-sim-drivers-make-the-shift-to-f1-max-verstappen-thinks-so/","date":1739753012,"author":"Connie Loizos","guid":842,"unread":true,"content":"<p>Motorsports have long been a pay-to-play arena, with young drivers spending thousands of dollars just to get started in karting. Four-time Formula One champion Max Verstappen knows this all too well, but he also sees a way to change it through sim racing, a virtual form of car racing that closely replicates real-world racing. It’s […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Connect with Someone on Qatar Airways?(((Quick-Guide)))","url":"https://dev.to/shruti_e0d48bf281e1dee18b/how-to-connect-with-someone-on-qatar-airwaysquick-guide-55kl","date":1739752790,"author":"Shruti","guid":849,"unread":true,"content":"<p>Are you looking to get in touch with Qatar Airways? Whether you need to inquire about booking, change your flight,((+44-800-066-3343)) or need assistance with a specific request, ((+1-888-690-5358)) connecting with the airline’s customer service is straightforward.+44-800-066-3343 Here’s a quick guide on how to reach out.</p>","contentLength":327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cleaning up old repositories","url":"https://archlinux.org/news/cleaning-up-old-repositories/","date":1739752230,"author":"Sven-Hendrik Haase","guid":844,"unread":true,"content":"<p>Around two years ago, we've merged the  repository into  as part of the <a href=\"https://archlinux.org/news/git-migration-completed/\">git migration</a>.\nIn order to not break user setups, \nwe kept these repositories around in an unused and empty state.\nWe're going to clean up these old repositories on .</p><p>On systems where  still references the old \n repository,  will return an error on trying to \nsync repository metadata.</p><p>The following deprecated repositories will be removed: , \n, , , , \n.</p><p>Please make sure to remove all use of the aforementioned repositories from your  (for which a  was shipped with )!</p>","contentLength":538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Web Development with JavaScript Voice UI Technology","url":"https://dev.to/sista-ai/enhancing-web-development-with-javascript-voice-ui-technology-a0g","date":1739751363,"author":"Sista AI","guid":848,"unread":true,"content":"<p>In today's digital landscape, the integration of Voice User Interfaces (VUIs) in web development is reshaping user experiences and interactions, spurred by the rapid advancements in voice technology and AI. Harnessing the power of JavaScript voice UI technology opens new avenues for immersive, hands-free engagement on the web. As users gravitate towards seamless and intuitive interfaces, developers are exploring innovative ways to integrate VUIs seamlessly into web applications.</p><h2>Advancing User Experiences</h2><p>With the rise of voice technology, more than 50% of searches are projected to be voice-based by 2025, signifying a profound shift in user preferences towards voice interfaces. Incorporating VUIs in web development offers hands-free navigation and intuitive interactions, enhancing accessibility and usability. For instance, a recipe platform can provide recipes through voice commands, streamlining user engagement and convenience.</p><h2>Revolutionizing Development With JavaScript</h2><p>Developers are leveraging technologies like the Web Speech API to implement speech recognition seamlessly in browsers, enabling users to interact with web content through voice commands. This simplifies the integration of voice features without the need for specialized infrastructure, enhancing the overall user experience. Node.js backend integration further enhances dynamic application responsiveness to voice commands, creating more interactive and engaging applications.</p><h2>Empowering Frontend Capabilities</h2><p>Frontend considerations play a critical role in building voice-enabled applications, ensuring seamless UI updates based on voice interactions. React applications, for example, can dynamically respond to voice commands, providing a customized user experience. By incorporating components that handle voice commands, developers can enhance user engagement and accessibility through intuitive voice interactions.</p><h2>Testing and Iterative Enhancement</h2><p>Testing VUI applications across various devices and user inputs is essential to ensure robust performance and user satisfaction. Through unit testing and user feedback analysis, developers can refine interaction models and enhance user experiences. Iterative design based on real user feedback ensures continuous improvement, optimizing VUI applications for diverse user needs and preferences.</p><h2>Seamless Integration With Sista AI</h2><p>Integrating Sista AI's Voicebot technology offers a seamless way to enhance VUI capabilities in web development. By leveraging Sista AI's AI Voice Assistant, businesses can elevate user engagement, accessibility, and efficiency. Discover the power of JavaScript voice UI technology with Sista AI's innovative solutions.</p><a href=\"https://smart.sista.ai?utm_source=sista_blog_devto&amp;utm_medium=blog_post&amp;utm_campaign=big_logo\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fvuic-assets.s3.us-west-1.amazonaws.com%2Fsista-make-auto-gen-blog-assets%2Fsista_ai.png\" alt=\"Sista AI Logo\" width=\"640\" height=\"170\"></a><a href=\"https://smart.sista.ai?utm_source=sista_blog_devto&amp;utm_medium=blog_post&amp;utm_campaign=product_btn_screenshot\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fsmart.sista.ai%2Fimages%2Fsista%2Fsista-button.png\" alt=\"AI ChatBot\" width=\"800\" height=\"360\"></a><a href=\"https://smart.sista.ai?utm_source=sista_blog_devto&amp;utm_medium=blog_post&amp;utm_campaign=product_admin_screenshot\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fsmart.sista.ai%2Fimages%2Fsista%2Fsista-admin-dark.png\" alt=\"AI Integration Platform\" width=\"800\" height=\"507\"></a>","contentLength":2679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"San Francisco homelessness: Park ranger helps one person at a time","url":"https://sfstandard.com/2025/02/08/golden-gate-park-ranger-homelessness/","date":1739751350,"author":"NaOH","guid":888,"unread":true,"content":"<div><p>Barrows found him in the park a few days later, with the catheter still in place. He’d been using it to inject crystal meth. He walked away from her, then veered into the street, directly into the path of a truck. It skidded to a stop, missing him by inches.&nbsp;</p></div><div><p>“What are you doing, dude?” Barrows yelled. Morrisette continued across the street. “He flashes,” she says of his now-familiar state of agitation. “It’s almost like he’s just blinded by rage or emotions or upset. And you can’t really do much other than watch, because if you try to forcibly intervene, it escalates.”&nbsp;</p></div><div><p>Two days later, Morrisette suffered an overdose from fentanyl. By then he was living in an encampment of RVs and tents on the Lower Great Highway, next to the park.&nbsp;Barrows and one of the RV residents administered Narcan and CPR until an ambulance arrived. The incident shook her deeply. She feared if Morrisette didn’t change his life soon, he would die, either deliberately or accidentally.&nbsp;</p></div><div><p>In early 2024, after a punishing series of storms, Barrows helped Morrisette and another resident of the encampment secure placement in a double room in transitional housing. To get the slot, they decided to lie and say they were same-sex partners, even though they were barely friends. But the arrangement didn’t last long. The man got sick and was hospitalized, and a stranger was moved into the room. Morrisette got into a fight with the guy and was kicked out of the facility. He returned to the Lower Great Highway.</p></div><div><p>To Barrows, the fault was in the placement, not in Morrisette. “Ronnie was always very clear about his needs. He knows he’s a volatile person. He doesn’t want to be in a shared room, especially with a stranger,” she said.</p></div><div><p>Still, his setbacks, and those of Barrows’ other clients, were taking a toll on her. Some days, she admitted, she felt burned out by all the distress she had to witness. “This can be just so exhausting to try to show up fully for so many people every day. And then be a container for all this brokenness.”</p></div>","contentLength":2057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43073292"},{"title":"Writing Pythonic Code With Python Data Model","url":"https://dev.to/noble47/writing-pythonic-code-with-python-data-model-2j3o","date":1739751292,"author":"Noble-47","guid":847,"unread":true,"content":"<blockquote><p><em>This apparent oddity is the tip of an iceberg that, when properly understood, is the key to everything we call Pythonic. The iceberg is called the Python data model, and it describes the API that you can use to make your own objects play well with the most idiomatic language features.</em> - Luciano Ramalho (Fluent Python: clear, concise, and effective programming)</p></blockquote><p>What's so special about the Python data model one may ask. Rather than giving a personal answer, why don't we do a little dive in and see what we can accomplish by understanding the data model. Data model simply speaks about how data is represented. In Python, data is represented by objects or to sound a bit technical, objects are Python's abstraction for data. The data model provides us with an API that allows our objects to play well with the 'under the hood' of Python programming.</p><p>In our little dive into Python data model, we are going to specifically focus on the special methods. Special methods are class functions with special names that are invoked by special syntax. Defining these special methods in our class definitions can give our class instances some really cool Python powers like iteration, operator overloading, working well with context managers (the 'with' keyword), proper string representation and formatting, and many more. To show you how you could implement these special functions into your classes, we will consider two examples of situations where using these special functions would make our codes clearer and more Pythonic.</p><p><em>The first example is a little bit outside-the-box solution I came up with for creating a simple game of Rock-Paper-Scissors in Python and the second is going to be a bit mathematical in nature but I'm going to walk you through each line of code</em></p><h3><strong>A Simple Game Of Rock Paper Scissors</strong></h3><p>Just in case you are not familiar with the Rock-Paper-Scissors game, it is originally a hand game usually played among two people that involves making signs of either Rock or paper or scissors. Knowing the whole history of the game doesn't really matter what is important is knowing how to determine the winner. In a conventional setting, a hand sign of rock would always win against scissors but will lose against paper, a hand sign of scissors would win against paper and lose to rock and obviously, paper would lose to scissors and win against rock. we can summarize this as shown below</p><p>For our Python emulation of this game, we will limit the number of players to just two, one player would be the computer and the other would be the user. Also, this is not a machine learning article or a write-up about computer vision, our users would still have to type in an option between rock, paper, and scissors on the terminal for our program to work.\nBefore we go into the actual coding, it's good that we take a step back and consider how we want our Python script to be. For my solution to this challenge, I will use the random module to enable the computer select a random option of either rock, paper, or scissors. To implement how our code evaluates the winner, I'm going to make the following assumptions:</p><p>I'm also going to take an OOP approach; our rock, paper, and scissors will be treated as objects and not string variables. Rather than creating three separate classes for each, I'll create only one that can represent any of them. This approach would also allow me to show you how special methods make life easier. Now to the fun aspect!</p><p>Naming our class RPS may sound a bit odd, but I found the name 'RPS' to be a good fit cause each letter comes from the initials, R for Rock, P for Paper, and S for Scissors. What's important to note here is that creating an instance of our class requires two arguments: pick and name. We already stated that the users of our script would have to type in their selected option on the terminal, instead of making our users type in 'Paper' (which could be so stressful for them) why don't we just allow our user to type in 'P' (or 'p') to select 'Paper', that's what the pick stands for. The name property is the actual name e.g 'Paper'. So now that we know what each parameters is for, we can now inspect our class by creating an instance</p><div><pre><code>&gt;&gt;&gt; p = RPS('P', 'Paper') # create an instance\n&gt;&gt;&gt; p.name\n# return : Paper\n&gt;&gt;&gt; p.pick\n# return : P\n&gt;&gt;&gt; print(p)\n# return : &lt;__main__.RPS object at 0x...&gt;\n</code></pre></div><p>Our class instance was created and has the right attributes but notice what we get when we try to print the contents of the variable holding our class instance. Before getting into the technical details of how our class instance returns the odd-looking string, let's update our class definition by adding a single special function and see the difference.</p><p>Now let's create an instance and try printing our class instance again</p><div><pre><code>&gt;&gt;&gt; p = RPS('P', 'Paper')\n&gt;&gt;&gt; print(p)\n# return : RPS(P, Paper)\n</code></pre></div><p>As we can see, by defining the '' method we can achieve a better looking result. Let's make one more change to our class definition.</p><p>Now let's create an instance and test it again.</p><div><pre><code>&gt;&gt;&gt; p = RPS('P', 'Paper')\n&gt;&gt;&gt; p\n# return : RPS(P, Paper)\n&gt;&gt;&gt; print(p)\n# return : Paper\n&gt;&gt;&gt; str(p)\n# return : 'Paper'\n&gt;&gt;&gt; repr(p)\n# return : 'RPS(P, Paper)'\n</code></pre></div><p>To know what's going on here, we need to know a little about the print function. The print function converts all non-keyword arguments(like our p variable) to string using the built-in Python class . If calling  on our variable fails, python falls back on the built-in  function. When  is called on our object, it looks for a  method, if it finds none, it fails and then searches for a  method. Both the  and the  methods are special methods used for string representation of our object. The  method gives the official string representation of our object while the  method gives a friendly string representation of our object. I usually say that the  method is like talking to another developer and it usually shows how to call our class and the  is like talking to a user of our program (like the player in this case), you would usually just want to return a simple string like \"Paper\" to show the user what he picked.</p><p>Although I stated the  and  as the two special functions in our class definition, there's actually a third special method, and yes it is the most common one, the  function. It is used for initializing our class and called by the  special method just before returning our class instance. Did I just mention another special method we haven't defined? yes, I did. It may also interest you to know that Python automatically adds some other special methods to our class. You can check them out by calling the built-in function  on our class instance like this</p><div><pre><code>&gt;&gt;&gt; dir(p)\n# returns : ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'name', 'pick']\n</code></pre></div><p>Special functions or methods can be identified by the way they are named, they always begin with double underscores '<strong>' and end with double underscores '</strong>' Because of this special way of naming these methods, they are commonly called daunder methods (Double + UNDERscores = DUNDER). So if a method name begins with double underscores, it is most likely but not certainly a special method. Why not certainly? this is simply because Python does not stop us from defining our own methods using the dunder syntax. Alright back to our game script.</p><p>All that's left for us now is for us to let our script know how to determine a winner. As stated earlier, I will use comparison to evaluate a winner.</p><div><pre><code># comparison logic\nRock &gt; Scissors\nScissors &gt; Paper\nPaper &gt; Rock\n</code></pre></div><p>To implement this solution, I will add a dictionary and use the daunder greater_than method. The dictionary key would be the initials of Rock, Paper and Scissors. The value of each key would be the only other element that the key can defeat.</p><p>Notice the new lines of code, first the options dictionary and then the  method definition. With these new lines of code, let's see what new functionality our code now has.</p><div><pre><code># create a rock instance\n&gt;&gt;&gt; r = RPS('R', 'Rock')\n\n# create a paper instance\n&gt;&gt;&gt; p = RPS('P', 'Paper')\n\n# create a scissors instance\n&gt;&gt;&gt; s = RPS('S', 'Scissors')\n\n&gt;&gt;&gt; print(r,p,s)\n# return : Rock Paper Scissors\n\n&gt;&gt;&gt; p &gt; r # paper wins against rock\n# return : True\n\n&gt;&gt;&gt; r &gt; s # rock wins against scissors\n# return : True\n\n&gt;&gt;&gt; s &gt; p # scissors wins against paper\n# return : True\n\n&gt;&gt;&gt; p &lt; s # paper lose to scissors\n# return : True\n\n&gt;&gt;&gt; p &lt; r # paper lose to rock\n# return : False\n\n&gt;&gt;&gt; p &lt; s &lt; r# paper lose to scissors which lose to rock\n# return : True\n\n&gt;&gt;&gt; p &gt;= r paper wins or tie to rock\n# return : Traceback (most recent call last): \n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; \nTypeError: '&gt;=' not supported between instances of 'RPS' and 'RPS'\n</code></pre></div><p>Just by adding to  special method, our class instances have gained magical powers (daunder methods are sometimes called magic methods and we can see why). By implementing the daunder gt method, our class instance now relates well with the  and  symbols but not the  and  symbols. The reason is that  is just the negation of . The special method for  is  which can just be the negation of calling .</p><p>For the ≥ symbol, its special method is the  and it must be defined for our object to work well with the  sign. But in this program, we can do without it. </p><p>Another missing piece would be to check if two separate instances of Paper are equal.</p><div><pre><code>&gt;&gt;&gt; p1 = RPS(\"P\", \"Paper\")\n&gt;&gt;&gt; p2 = RPS(\"P\", \"Paper\")\n&gt;&gt;&gt; p3 = p1\n&gt;&gt;&gt; p1 == p2\n# return : False\n\n&gt;&gt;&gt; p1 == p3\n# return : True\n\n&gt;&gt;&gt; id(p1)\n# return : 140197926465008\n\n&gt;&gt;&gt; id(p2)\n# return : 140197925989440\n\n&gt;&gt;&gt; id(p3)\n# return : 140197926465008\n\n&gt;&gt;&gt; id(p1) == id(p3)\n# return : True\n\n&gt;&gt;&gt; id(p2) == id(p1)\n# return False\n</code></pre></div><p>The default operation of the equal comparison sign is to compare the id of the object. p1 and p2 are different class instances that happen to have the same attributes but their id differs and therefore are not equal. When we assign a variable to a class instance, we make that variable point to the address of the instance which is what we observe for p3 which has the same id as p1. We have the option of overriding how the equality comparison works on our object by defining and implementing our own  method. But for this script, I will compare two instances using their pick attribute. Now that we have defined our class and know how it works, we are now ready to see the full implementation of the Python script</p><p>Let me walk you through the code. We are already familiar with the RPS class definition. If you recall, our code is meant to allow the computer to select choices at random and that's what the random module is for. The random module makes available the  function which allows the 'random' selection of an element from an iterable object e.g. a list in Python. The list in this case is the . Because our class is made to work with uppercase letters for comparison, it is necessary that we always initialize our objects with uppercase for the pick attribute. This is why we first convert the user's input to upper case (line 34) with the&nbsp;. It is also possible that our user types in an unexpected character like 'Q' so we have to validate our user input by checking if the uppercase character is part of the valid options in . The mapping dictionary allows us to quickly convert the user's input to a corresponding instance of RPS after being validated. The evaluate_winner function makes use of the comparison symbol to determine the winner. Because we want the code to run in a loop until a winner is found, we make use of a while loop and when a winner is found, the evaluate_winner function returns True which will then break the loop and exit the game.</p><p>Here is one of the various results of running the code</p><p>Our Python code runs as expected, although there could be a couple of improvements or new features to add. The most important thing is that we see how using special methods in our class definition gives our code a more Pythonic feel. Assuming we were to take a different approach such as using nested if statements, our evaluate_winner method would look something like this</p><div><pre><code>def evaluate_winner(user_choice, comp_choice):\n    # check if user choice is 'R'\n    if user_choice == 'R':\n        # check if comp_choice is 'R'\n        if comp_choice == 'R':\n            # it is a tie\n            ...\n        elif comp_choice == 'S':\n            # user wins\n            ...\n        else:\n          # computer wins\n          ...\n    if ... \n     # do the same for when user_choice is 'S' and then for\n     # when user_choice is 'P'\n</code></pre></div><p>A problem with this approach other than the lengthy code is that if we desire to add a new element, diamond which can beat both rock and scissors but not paper (for an unknown reason), our if statements would begin to look really awkward. Whereas in our OOP approach, all we have to do is to modify the options dict like so</p><div><pre><code>options = {\"R\" : [\"S\"], \"P\" : [\"R\"], \"S\" : [\"P\"], \"D\" : [\"R\", \"S\"]}\n</code></pre></div><p>and then we change the if statement in  to be</p><div><pre><code>def __gt__(self,x):    \n    if x.pick in self.options[self.pick]:\n        return True\n    else:\n        return False\n</code></pre></div><p>we can make the statement shorter</p><div><pre><code>def __gt__(self, x):\n   return True if x.pick in self.options[self.pick] else False\n</code></pre></div><p>To conclude, here are some things you should note about using special methods:</p><ul><li><p>You hardly (or never) call them directly yourself, let Python do the calling for you</p></li><li><p>When defining functions that use the dunder naming syntax, you should consider that Python could one day define such a function and give it a different meaning. This could break your code or make it behave in unexpected ways</p></li><li><p>You certainly don't have to implement every special method there is. Just a couple that you are really sure you need. Remember, simple is better than complex. If there's a simpler way you should use that instead</p></li></ul><p>This is the first part of the topic, in the next part, we are going to be dealing with operator overloading and making iterable objects</p><p>Hope you enjoyed this article!!!</p>","contentLength":14165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Does or did COBOL default to 1875-05-20 for corrupt or missing dates?","url":"https://retrocomputing.stackexchange.com/questions/31288/does-or-did-cobol-default-to-1875-05-20-for-corrupt-or-missing-dates","date":1739750176,"author":"SeenNotHeard","guid":1652,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43073149"},{"title":"Building Powerful AI Systems that Solve Real-World Problems with The Power of Model Context Protocol (MCP)","url":"https://dev.to/michelle_sebek_/building-powerful-ai-systems-that-solve-real-world-problems-with-the-power-of-model-context-mha","date":1739748692,"author":"michelle sebek","guid":831,"unread":true,"content":"<p>Artificial Intelligence (AI) continues to evolve, one area that's seeing tremendous growth is the integration of Large Language Models (LLMs) with a variety of data sources, tools, and services. However, achieving smooth and consistent integration across various platforms and environments has always been a challenge for developers. This is where the Model Context Protocol (MCP) comes in.</p><p>Launched and steadily advancing through the spring-ai-mcp experimental project, MCP has become a game changer for developers who are looking to build intelligent systems, agents, and workflows powered by LLMs. By providing a unified way to connect AI models to multiple data sources and tools, MCP simplifies what could otherwise be a complicated and fragmented integration process. Let's dive into why this is so important and how the Model Context Protocol is shaping the future of AI development.</p><h2>\n  \n  \n  What Is the Model Context Protocol (MCP)?\n</h2><p>The Model Context Protocol (MCP) is a powerful and flexible protocol that serves as the foundation for connecting Large Language Models (LLMs) to different external systems, APIs, and tools. MCP provides a standardized framework that ensures smooth communication between LLMs and external resources, enabling developers to build intelligent agents and complex workflows without worrying about the underlying complexities of data integration.</p><p>MCP offers a set of protocols and interfaces that abstract away the difficulties involved in linking your AI models to various services. Instead of worrying about how to interface with different data sources and tools, developers can rely on MCP to ensure that everything works seamlessly.</p><h2>\n  \n  \n  Key Benefits of MCP for LLMs\n</h2><p><strong>1. Pre-built integrations for Easy Connectivity</strong>\nMCP simplifies the integration process by offering a growing list of pre-built integrations. These integrations allow your LLM to easily connect to external tools, data sources, and services. Whether it’s a database, API, or a specialized service, MCP ensures that the LLM can access and interact with it without friction.</p><p><strong>2. Flexibility to Switch Between Providers</strong>\nThe ability to switch between different LLM providers and vendors is critical. MCP empowers developers to change providers with minimal hassle, ensuring that your application remains flexible and adaptable to future advancements in AI technology.</p><p><strong>3. Standardized Interfaces for Tool Discovery and Execution</strong>\nMCP provides standardized interfaces for tool discovery and execution, streamlining the process of finding and interacting with external systems. This allows developers to quickly build and modify workflows incorporating various tools and services, without worrying about proprietary integration methods.</p><p><strong>4. Seamless Model-to-Data Communication</strong>\nSince LLMs often need to work with external data sources to enhance their capabilities, MCP acts as the bridge that connects them seamlessly. Whether you're pulling in data for training or providing real-time input during model execution, MCP ensures smooth communication between the model and external systems.</p><h2>\n  \n  \n  The Evolution of MCP in Spring AI\n</h2><p>The spring-ai-mcp project began as an experimental initiative last November and has evolved into a core part of the MCP Java SDK. This SDK is the result of collaboration between the Spring AI team and David Soria Parra and colleagues at Anthropic,aiming to make #MCP an official standard within the Java ecosystem.</p><p>The MCP Java SDK comes with a variety of features that make it incredibly powerful and adaptable for developers:</p><h2>\n  \n  \n  Core Capabilities of the MCP Java SDK:\n</h2><p><strong>Synchronous and Asynchronous MCP Client/Server Implementations:</strong> This gives developers flexibility in how they handle communication between their AI models and external systems, ensuring that both time-sensitive and long-running tasks are properly managed.<strong>Protocol Version Compatibility Negotiation:</strong> Ensures backward and forward compatibility, so your application can evolve over time without breaking existing integrations.<strong>Tool Discovery and Execution with Change Notifications:</strong> Keeps you informed about changes in your toolset, ensuring that your workflows stay up to date.<strong>Resource Management with URI Templates:</strong> Simplifies resource management by allowing dynamic handling of resources and their associated URIs.<strong>Roots List Management and Notifications:</strong> Manages resources and provides updates to keep developers informed of changes to the environment.<strong>Prompt Handling and Management:</strong> Allows for sophisticated handling of model inputs and outputs, ensuring that your interactions with the model are as efficient as possible. Facilitates AI model interactions with support for various sampling strategies, enabling more control over model behavior and output.</p><h2>\n  \n  \n  Multiple Transport Options for Flexibility\n</h2><p>The MCP Java SDK supports several transport mechanisms, allowing developers to choose the method that best fits their application architecture:</p><p> Ideal for process-based communication, this transport method is simple and efficient.<strong>Java HttpClient-Based SSE Client Transport:</strong> Great for handling server-sent events (SSE) in web applications.<strong>Servlet-Based SSE Server Transport:</strong> Supports servlet-based applications with SSE for real-time communication.<strong>Spring-Specific Transports</strong>: For Spring developers, there are two options:\n Designed for reactive HTTP streaming in applications built with Spring <strong>- WebFlux.WebMVC SSE Transport:</strong>\nBest suited for traditional servlet-based applications using Spring MVC.</p><h2>\n  \n  \n  Why MCP Matters for the Future of AI\n</h2><p>MCP is more than just a tool for LLM integration—it’s a framework that enables developers to build smarter, more powerful AI-driven applications with ease. The protocol provides scalability, flexibility, and extensibility, all while promoting consistency and standardization in the way AI models interact with external resources.</p><p>As AI technology continues to grow and become more integrated into our daily lives, tools like MCP will be essential for ensuring that these powerful models can work effectively with real-world data and tools. Whether you're working on AI-powered chatbots, recommendation systems, or autonomous agents, MCP is the glue that can tie it all together, enabling seamless, consistent, and efficient AI solutions. <a href=\"https://youtu.be/cE1h-rC2o2U?si=Tczv_W9evmH9u1sG\" rel=\"noopener noreferrer\">Check out the video by Josh Long</a></p><p>With <a href=\"https://spring.io/blog/2025/02/14/spring-ai-1-0-0-m6-released\" rel=\"noopener noreferrer\">Spring AI and the Model Context Protocol</a> (MCP), developers have the tools they need to create intelligent, interconnected applications powered by Large Language Models. By simplifying integration and offering flexibility, MCP allows developers to focus on what really matters—building powerful AI systems that solve real-world problems.</p><p>The future of<a href=\"https://spring.io/projects/spring-ai\" rel=\"noopener noreferrer\"> AI development</a> is here, and it's more connected than ever.</p><p>Feel free to leave a comment or reach out if you have any questions about how you leverage MCP in your projects! Let’s connect and discuss the future of AI integration. 🚀</p>","contentLength":6914,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"فكرة استخدام الذكاء الاصطناعي لإنشاء ألعاب أو تطبيقات بناءً على قصة تكتبها هي فكرة مبتكرة ومثيرة","url":"https://dev.to/mohamed_gafaar_a83dae3bcb/fkr-stkhdm-ldhk-lstny-lnsh-lb-w-ttbyqt-bnan-l-qs-tktbh-hy-fkr-mbtkr-wmthyr-1o7d","date":1739748259,"author":"Mohamed Gafaar","guid":830,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Javier Milei backtracks on $4.4B memecoin after 'insiders' pocket $87M","url":"https://www.coindesk.com/business/2025/02/15/javier-milei-backtracks-on-usd4-4b-memecoin-after-insiders-pocket-usd87m","date":1739747759,"author":"techlover14159","guid":858,"unread":true,"content":"<p>Argentina's president Javier Milei has backtracked on a tweet promoting a memecoin called Libra, which rose to a $4.4 billion market cap before plunging by more than 95%.</p><p>In a now-deleted tweet, Milei initially wrote: \"This is a private project dedicated to encouraging the growth of the Argentine economy,\" along with a Solana contract address linked to the Libra token.</p><p>Libra rose by more than 2,000% in a 40-minute span following the tweet, only to tumble rapidly as a group of early holders began to cash out. </p><p>X account KobeissiLetter shared a series of BubbleMaps screenshots showing that alleged \"insiders\" liquidate tokens by adding one-sided liquidity pools on Metora with only Libra, allowing them to remove SOL and stablecoins.</p><p>Trading volume for Libra hit $1.1 billion after launch, although it appeared that purchases and sales were skewed; there were 74,500 individual buy orders and 28,900 sales - indicating that larger sell orders flattened the flurry of retail activity.</p><p>Milei later addressed the botched memecoin on X, stating that he \"was not aware of the details of the project.\" </p><p>\"A few hours ago I posted a tweet, as I have so many other times, supporting a supposed private enterprise with which I obviously have no connection whatsoever,\" Milei wrote. \"I was not aware of the details of the project and after having become aware of it I decided not to continue spreading the word (that is why I deleted the tweet).\"</p><p>The sell-off in Libra rippled across the wider memecoin market, with TRUMP losing $500 million from its market cap, <a href=\"https://www.coindesk.com/price/trump\">according to market data</a>, in a 30-minute period after Libra began to tumble.</p>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43072790"},{"title":"Are Technologies of Connection Tearing Us Apart?","url":"https://tech.slashdot.org/story/25/02/16/2311215/are-technologies-of-connection-tearing-us-apart?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739747580,"author":"EditorDavid","guid":827,"unread":true,"content":"Nicholas Carr wrote The Shallows: What the Internet Is Doing to Our Brains. But his new book looks at how social media and digital communication technologies \"are changing us individually and collectively,\" writes the Los Angeles Review of Books. \nThe book's title? Superbloom: How Technologies of Connection Tear Us Apart .\nBut if these systems are indeed tearing us apart, the reasons are neither obvious nor simple. Carr suggests that this isn't really about the evil behavior of our tech overlords but about how we have \"been telling ourselves lies about communication — and about ourselves.... Well before the net came along,\" says Carr, \"[the] evidence was telling us that flooding the public square with more information from more sources was not going to open people's minds or engender more thoughtful discussions. It wasn't even going to make people better informed....\" \n\nAt root, we're the problem. Our minds don't simply distill useful knowledge from a mass of raw data. They use shortcuts, rules of thumb, heuristic hacks — which is how we were able to think fast enough to survive on the savage savanna. We pay heed, for example, to what we experience most often. \"Repetition is, in the human mind, a proxy for facticity,\" says Carr. \"What's true is what comes out of the machine most often....\" Reality can't compete with the internet's steady diet of novelty and shallow, ephemeral rewards. The ease of the user interface, congenial even to babies, creates no opportunity for what writer Antón Barba-Kay calls \"disciplined acculturation.\" \n\nNot only are these technologies designed to leverage our foibles, but we are also changed by them, as Carr points out: \"We adapt to technology's contours as we adapt to the land's and the climate's.\" As a result, by designing technology, we redesign ourselves. \"In engineering what we pay attention to, [social media] engineers [...] how we talk, how we see other people, how we experience the world,\" Carr writes. We become dislocated, abstracted: the self must itself be curated in memeable form. \"Looking at screens made me think in screens,\" writes poet Annelyse Gelman. \"Looking at pixels made me think in pixels....\" \n\nThat's not to say that we can't have better laws and regulations, checks and balances. One suggestion is to restore friction into these systems. One might, for instance, make it harder to unreflectively spread lies by imposing small transactional costs, as has been proposed to ease the pathologies of automated market trading. An option Carr doesn't mention is to require companies to perform safety studies on their products, as we demand of pharmaceutical companies. Such measures have already been proposed for AI. But Carr doubts that increasing friction will make much difference. And placing more controls on social media platforms raises free speech concerns... We can't change or constrain the tech, says Carr, but we can change ourselves. We can choose to reject the hyperreal for the material. We can follow Samuel Johnson's refutation of immaterialism by \"kicking the stone,\" reminding ourselves of what is real.","contentLength":3112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"فكرة استخدام الذكاء الاصطناعي لإنشاء ألعاب أو تطبيقات بناءً على قصة تكتبها هي فكرة مبتكرة ومثيرة!","url":"https://dev.to/mohamed_gafaar_a83dae3bcb/fkr-stkhdm-ldhk-lstny-lnsh-lb-w-ttbyqt-bnan-l-qs-tktbh-hy-fkr-mbtkr-wmthyr-22he","date":1739747411,"author":"Mohamed Gafaar","guid":829,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The open source fastest URL shortener ever.","url":"https://dev.to/abdibrokhim/the-open-source-fastest-url-shortener-ever-5af3","date":1739746041,"author":"Ibrohim Abdivokhidov","guid":798,"unread":true,"content":"<p>The open source fastest URL shortener ever.</p><p>Built with awesome open source tools x.com/rustlang x.com/shuttle_dev x.com/neondatabase x.com/actix_rs</p><p>x.com/ThePracticalDev x.com/aimlapi x.com/vercel</p>","contentLength":194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.14-rc3 Released With Faux Bus & Various Fixes","url":"https://www.phoronix.com/news/Linux-6.14-rc3-Released","date":1739745164,"author":"Michael Larabel","guid":797,"unread":true,"content":"<article>Linus Torvalds just released Linux 6.14-rc3 as the newest weekly release candidate for Linux 6.14 that will debuting as stable before the end of March...</article>","contentLength":153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"These researchers used NPR Sunday Puzzle questions to benchmark AI ‘reasoning’ models","url":"https://techcrunch.com/2025/02/16/these-researchers-used-npr-sunday-puzzle-questions-to-benchmark-ai-reasoning-models/","date":1739744700,"author":"Kyle Wiggers","guid":793,"unread":true,"content":"<p>Every Sunday, NPR host Will Shortz, The New York Times’ crossword puzzle guru, gets to quiz thousands of listeners in a long-running segment called the Sunday Puzzle. While written to be solvable without too much foreknowledge, the brainteasers are usually challenging even for skilled contestants. That’s why some experts think they’re a promising way to […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uchū – Color palette for internet lovers","url":"https://uchu.style/","date":1739744560,"author":"NetOpWibby","guid":826,"unread":true,"content":"<li><p>oklch(68.5% 0.136 303.78)</p></li><li><p>oklch(58.47% 0.181 302.06)</p></li><li><p>oklch(49.39% 0.215 298.31)</p></li><li><p>oklch(46.11% 0.198 298.4)</p></li><li><p>oklch(42.77% 0.181 298.49)</p></li><li><p>oklch(39.46% 0.164 298.29)</p></li><li><p>oklch(36.01% 0.145 298.35)</p></li>","contentLength":180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43072338"},{"title":"RandomPosterize in PyTorch","url":"https://dev.to/hyperkai/randomposterize-in-pytorch-35e6","date":1739744052,"author":"Super Kai (Kazuya Ito)","guid":799,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomPosterize.html\" rel=\"noopener noreferrer\">RandomPosterize()</a> can randomly posterize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type:):\n*Memos:\n\n<ul><li>It's the number of bits to keep for each channel.</li></ul></li><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is posterized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Create Realtime ChatApp Website Without Coding by Create.xyz 🔥 🚀","url":"https://dev.to/hanzla-baig/create-realtime-chatapp-website-without-coding-by-createxyz-1ec2","date":1739742786,"author":"Hanzla Baig","guid":784,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"After Launch by SpaceX in January, Firefly Aerospace's Lunar Lander Reaches Moon Orbit","url":"https://science.slashdot.org/story/25/02/16/2138259/after-launch-by-spacex-in-january-firefly-aerospaces-lunar-lander-reaches-moon-orbit?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739742060,"author":"EditorDavid","guid":779,"unread":true,"content":"\"A robotic lander from Texas-based Firefly Aerospace is now in orbit around the Moon,\" reports Spaceflight Now, \"and going through its final preparations to land in the coming weeks.\"\n\nIts arrival comes nearly a month after the spacecraft launched onboard a Falcon 9 rocket from pad 39A at NASA's Kennedy Space Center. This is the third mission launched as part of the agency's Commercial Lunar Payload Services (CLPS) program, an initiative designed to bring science and technology demonstrations to the Moon at a cheaper cost... \n\nManifested on this lander are 10 NASA payloads, which cover a range of objectives. Those include the Lunar Instrumentation for Subsurface Thermal Exploration with Rapidity (LISTER) instrument, which will drill between 2- to 3-meters into the Moon's surface to study the heat flow; and the Stereo Cameras for Lunar Plume-Surface Studies (SCALPSS) 1.1 instrument, which will use a series of cameras to capture the plume generated at landing to help create a three-dimensional model... \"We saw that for the type of advanced scientific or engineering measurements we wanted to make, the instruments were small enough and compact enough that we could actually fly 10,\" [said Joel Kearns, deputy associate administrator for Exploration in NASA's Science Mission Directorate], \"if someone could actually schedule them to get all of their operations done over the 14 Earth day lunar daytime.\" \n\n\nFirefly Aerospace ended up winning that bid and carries with it the most NASA instruments manifested on a single Commercial Lunar Payload Services lander so far. \nFriday on X.com Firefly Aerospace wished a happy Valentine's Day to \"all those on Earth who dare to Dream Big.\" \n\"Blue Ghost has been capturing stunning imagery of our planet throughout its journey,\" Spaceflight Now says in a 12-minute video. \nAnd Friday on X.com Firefly posted Blue Ghost's first spectacular shots of the moon as it approaches — along with its special message for Valentine's Day. \"I love you to the Moon, but not back — I'm staying there.\"","contentLength":2046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"python Level 1","url":"https://dev.to/mohamed_yahyasidimohame/python-level-1-366c","date":1739741834,"author":"Mohamed Yahya Sidi Mohamed","guid":783,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Air traffic control radio and chill music for focus","url":"https://www.chillyatc.com/","date":1739741781,"author":"usernameis42","guid":823,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43071934"},{"title":"When Power Flow Models Go Off the Rails","url":"https://hackernoon.com/when-power-flow-models-go-off-the-rails?source=rss","date":1739741303,"author":"Linearization","guid":822,"unread":true,"content":"<p>(1) Mengshuo Jia, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(2) Gabriela Hug, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(3) Ning Zhang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China;</p><p>(4) Zhaojian Wang, Department of Automation, Shanghai Jiao Tong University, Dongchuan Rd 800, 200240, Shanghai, China;</p><p>(5) Yi Wang, Department of Electrical and Electronic Engineering, The University of Hong Kong, Pok Fu Lam, Hong Kong, China;</p><p>(6) Chongqing Kang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China.</p><h2>4. Generalizability and Applicability Evaluations</h2><h3>4.1. Predictor and Response Generalizability</h3><p>While PPFL methods are often constrained in their choice of predictors and responses due to specific physical formulations, DPFL methods generally offer a more flexible framework. This flexibility indicates the potential for DPFL approaches to accommodate arbitrary known variables (including 𝑃 and 𝑄 of the PQ buses, 𝑉 of the slack and PV buses, and 𝜃 of the slack bus) as predictors, and arbitrary unknown variables (including 𝑃 of the slack bus, 𝑄 of the slack and PV buses, 𝑉 of the PQ buses, 𝜃 of the PQ and PV buses, and all active/reactive line flows, i.e., PF, PT, QF, QT) as responses. However, due to various factors, not all DPFL methods can achieve this level of generalizability:</p><p>\\\n• For the PLSBDLY2 methods, using 𝑉 , 𝑃 , and 𝑄 as predictors is enforced by the bundle strategy [6] they adopt. This strategy, designed to address variations in bus types, inherently constrains the choice of predictors by pre-defining known and unknown variables.</p><p>\\\n• The LCPJGD methods integrate physical knowledge of power flows by formulating constraints based on the Jacobian matrix derived from AC power flow equations expressed in polar coordinates [6]. Within this Jacobian matrix, 𝑃 and 𝑄 are treated as known variables, while 𝑉 and 𝜃 are considered unknown variables.</p><p>\\\n• The LCP_COU method is specifically designed to linearly estimate the values of branch flows by leveraging the terminal voltages and angles as predictive variables [6]. Consequently, the method can only employ 𝑉 and 𝜃 as predictors, and treat PF, PT, QF, and QT as responses.</p><p>\\\n• For the DCC methods, since they incorporate the DC and DLPF models respectively into their framework [6], they must align their selection of predictors and responses with the underlying physical models they adopt.</p><p>\\\nNote that the constrained flexibility in choosing predictors and responses results in notable limitations. First, these methods might not leverage all available known data for model training, leading to potential information loss. For instance, the DC<em>LS method only uses measurements of 𝑃 , disregarding a large amount of known voltage data. Second, the capability to predict unknown variables using the developed linear model may be restricted. For example, the LCP</em>BOX method is restricted to calculating branch flow values, leading to a quite limited functional scope.</p><h3>4.2. Applicability to Cases with Multicollinearity</h3><p>The ordinary least squares method struggles with multicollinearity [6]. Methods LS, LSREC share this limitation, since they are all based on the ordinary least squares framework. Additionally, LSTOL are also affected by this problem, as discussed in [6]. Subsequent experiments will numerically demonstrate their limitations in this context.</p><h3>4.3. Zero Predictor Applicability</h3><p>The issue of zero predictors arises when certain known variable measurements in the training dataset are consistently zero. A typical example is the inclusion of the slack bus angle in the predictor set, whose value is commonly set as zero and remains zero throughout. Other instances may involve PQ buses where active/reactive power consumption is zero during the measurement period. This situation leads to zero columns in the predictor dataset matrix (where columns represent different variables, and rows represent individual measurements). Not all DPFL methods can handle these zero columns effectively:</p><p>\\\n• Methods based on ordinary least squares, including LS, LSREC, have difficulties with zero predictors, as these zero columns render the Gram matrix of the predictor matrix non-invertible, thereby leading to the failure of these methods.</p><h3>4.4. Constant Predictor Applicability</h3><p>The constant predictor issue extends beyond the zero predictor problem, occurring when measurements of certain variables in the training dataset remain constant, not necessarily zero. A typical example is the fixed terminal voltages at PV buses, which normally stay constant over the measurement period, resulting in constant-value columns in the predictor matrix.</p><h3>4.5. Normalization Applicability</h3><p>As detailed in [6], incorporating physical knowledge into DPFL methods can become problematic with datasets normalized via variance-scaling techniques, like unit-energy normalization, where each variable is normalized independently. This independent normalization disrupts the inherent physical relationships among variables, such as those represented in the Jacobian matrix or through coupling relationships, rendering methods like RRBOX, LCPJGD, DCC inapplicable.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2406.06833\">available on arxiv</a> under CC BY-NC-ND 4.0 Deed (Attribution-Noncommercial-Noderivs 4.0 International) license.</p>","contentLength":5552,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Current Power Flow Models May Not Work in Real-World Scenarios","url":"https://hackernoon.com/why-current-power-flow-models-may-not-work-in-real-world-scenarios?source=rss","date":1739741292,"author":"Linearization","guid":821,"unread":true,"content":"<p>(1) Mengshuo Jia, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(2) Gabriela Hug, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(3) Ning Zhang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China;</p><p>(4) Zhaojian Wang, Department of Automation, Shanghai Jiao Tong University, Dongchuan Rd 800, 200240, Shanghai, China;</p><p>(5) Yi Wang, Department of Electrical and Electronic Engineering, The University of Hong Kong, Pok Fu Lam, Hong Kong, China;</p><p>(6) Chongqing Kang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China.</p><h2>3. Review of Existing Experiments</h2><p>Before conducting evaluations of the methods listed in Table 1, we here aim to offer a detailed review of existing DPFL experiments in the literature, as depicted in Table 2. This review intends to present the experimental accomplishments of previous DPFL studies, while simultaneously revealing the importance and need for an extensive numerical comparison of all DPFL methods. Below are the further discussions of Table 2.</p><p>\\\nFirstly, Table 2 indicates that both transmission and distribution grids were used as test cases to verify DPFL methods. While distribution grids differ from transmission grids in terms of symmetry and topology, DPFL methods are generally applicable to both types of systems, such as the methods in [15, 12, 30, 21, 11]. The reasons are twofold. First, even if the three phases in distribution systems are unbalanced, DPFL methods can still be implemented by either training one DPFL model for each phase [52], or training an overall DPFL model for all variables in three phases [22, 11, 12, 24]. Note that the latter can generate a DPFL model reflecting the mutual influences between phases. Second, from a DPFL perspective, radial topologies do not pose any unique challenges compared to mesh topologies, as the difference is only in the number of dependent and independent variables. In summary, while distribution grids may have unbalanced characteristics and radial topologies, these attributes do not bring special difficulties to DPFL studies.</p><p>\\\nSecondly, Table 2 reveals that many evaluations solely depended on artificial data for training and testing, without considering the effects of noise and outliers on the data. This ideal testing environment is rarely found in real life, and the resulting conclusions may not hold in practice. To address this issue, it is recommended that synthetic data be injected with noise and outliers to mimic real-world scenarios.</p><p>\\\nThirdly, as indicated in Table 2, only a few studies report the load fluctuation range used in their simulations. This is worth mentioning because the accuracy of the DPFL model is highly dependent on the simulated fluctuations. E.g., a narrow fluctuation range typically leads to higher accuracy for the DPFL model. Without this information being made public, it is difficult to determine the reason for the high accuracy of the evaluated DPFL model.</p><p>\\\nFinally, Table 2 indicates which of the DPFL approaches have been evaluated in existing DPFL studies. These evaluations aimed to implement a comparative analysis between established and novel DPFL methods at that time. However, the scope of these comparisons is quite narrow, with only a few DPFL studies undertaking evaluations against a limited number of existing DPFL approaches (some works only conducted comparisons with PPFL methods). Such constrained comparisons fail to provide an in-depth understanding of the overall performance of DPFL methods. Consequently, a more exhaustive and inclusive comparison across all the DPFL approaches is clearly needed, in order to demonstrate their relative merits and limitations comprehensively.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2406.06833\">available on arxiv</a> under CC BY-NC-ND 4.0 Deed (Attribution-Noncommercial-Noderivs 4.0 International) license.</p>","contentLength":4016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How 44 Different Algorithms Compare in Power Flow Optimization","url":"https://hackernoon.com/how-44-different-algorithms-compare-in-power-flow-optimization?source=rss","date":1739741286,"author":"Linearization","guid":820,"unread":true,"content":"<p>(1) Mengshuo Jia, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(2) Gabriela Hug, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(3) Ning Zhang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China;</p><p>(4) Zhaojian Wang, Department of Automation, Shanghai Jiao Tong University, Dongchuan Rd 800, 200240, Shanghai, China;</p><p>(5) Yi Wang, Department of Electrical and Electronic Engineering, The University of Hong Kong, Pok Fu Lam, Hong Kong, China;</p><p>(6) Chongqing Kang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China.</p><p>Table 1 enumerates the 44 evaluated methods, detailing for each the corresponding abbreviation, the training algorithm employed, and any supporting techniques utilized. The following points warrant attention.</p><p>\\\nFirstly, for the linearly constrained programming approaches, we also evaluate these methods without their key constraints, e.g., the bound constraints, coupling constraints, or structure constraints, in order to verify the added value of incorporating such constraints. It is crucial to recognize that, even without these key constraints, the resulting programming models remain different, attributed to the varied supporting techniques they incorporate.</p><p>\\\nSecondly, the first part of this tutorial [6] reveals the modular nature of DPFL studies, highlighting their flexibility in the assembly of various techniques to forge novel methodologies. In alignment with this paradigm, we introduce several methods previously unexplored within the DPFL domain, and include them into the following comparative analysis. These approaches include the least squares with pseudoinverse, least squares augmented by principal component analysis, generalized least squares with pseudoinverse, and a clustering-based version of the partial least squares[1]. It is crucial to clarify that the objective of integrating these methods is not to argue their “novelty/superiority” over all pre-existing techniques. Rather, we intend to demonstrate the ease with which one can deviate from conventional paths to devise distinct methodologies. Notably, some of these introduced methods have demonstrated satisfying performance and rankings in subsequent evaluations. This outcome, particularly given the unsophisticated-designed nature of these approaches, suggests a high potential for further advancements in DPFL research.</p><p>\\\nFinally, our evaluation also encloses a selection of physics-driven power flow linearization (PPFL) methods, such as the classic DC model, the power transfer distribution factor model, the warm-start 1st-order Taylor approximation model (derived from the equations of nodal power injections in polar coordinates), and the decoupled linearized power flow model [7]. Note that these PPFL methods are widely recognized and employed in both academic research and industry practices.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2406.06833\">available on arxiv</a> under CC BY-NC-ND 4.0 Deed (Attribution-Noncommercial-Noderivs 4.0 International) license.</p><p>[1] In the adaptation of the least squares method to incorporate pseudoinverse, the conventional inversion operation used in the ordinary least squares method is substituted with the Moore–Penrose inverse. This adjustment is designed to enhance the method’s resilience to multicollinearity issues.</p><p>\\\nSimilarly, for the generalized least squares method augmented with pseudoinverse, the initial iteration of the well-known feasible generalized least squares method is modified to employ the least squares with pseudoinverse instead of the ordinary least squares. This modification also aims to strengthen the method’s ability to manage multicollinearity.</p><p>\\\nIn the case of clustering-based partial least squares, the approach involves substituting the ordinary least squares component within the clustering-based least squares methodology (as discussed in Part I [6]) with the ordinary partial least squares. This change seeks to better accommodate the inherent nonlinear characteristics of AC power flows.</p><p>\\\nFor details on the least squares with principal component analysis, the reader is referred to Appendix A.</p>","contentLength":4299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Study Reveals the Best AI Models for Power Grid Optimization","url":"https://hackernoon.com/new-study-reveals-the-best-ai-models-for-power-grid-optimization?source=rss","date":1739741280,"author":"Linearization","guid":819,"unread":true,"content":"<p>(1) Mengshuo Jia, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(2) Gabriela Hug, Department of Information Technology and Electrical Engineering, ETH Zürich, Physikstrasse 3, 8092, Zürich, Switzerland;</p><p>(3) Ning Zhang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China;</p><p>(4) Zhaojian Wang, Department of Automation, Shanghai Jiao Tong University, Dongchuan Rd 800, 200240, Shanghai, China;</p><p>(5) Yi Wang, Department of Electrical and Electronic Engineering, The University of Hong Kong, Pok Fu Lam, Hong Kong, China;</p><p>(6) Chongqing Kang, Department of Electrical Engineering, Tsinghua University, Shuangqing Rd 30, 100084, Beijing, China.</p><p>Building on the theoretical insights of Part I, this paper, as the second part of the tutorial, dives deeper into data-driven power flow linearization (DPFL), focusing on comprehensive numerical testing. The necessity of these simulations stems from the theoretical analysis’s inherent limitations, particularly the challenge of identifying the differences in real-world performance among DPFL methods with overlapping theoretical capabilities and/or limitations. The absence of a comprehensive numerical comparison of DPFL approaches in the literature also motivates this paper, especially given the fact that over 95% of existing DPFL studies have not provided any open-source codes. To bridge the gap, this paper first reviews existing DPFL experiments, examining the adopted test scenarios, load fluctuation settings, data sources, considerations for data noise/outliers, and the comparison made so far. Subsequently, this paper evaluates a total of 44 methods, containing over 30 existing DPFL approaches, some innovative DPFL techniques, and several classic physics-driven power flow linearization methods for benchmarking. The evaluation spans various dimensions, including generalizability, applicability, accuracy, and computational efficiency, using numerous different test cases scaling from 9-bus to 1354-bus systems. The numerical analysis in this paper identifies and examines significant trends and consistent findings across all methods under various test cases. Meanwhile, it offers theoretical insights into phenomena like under-performance, failure, excessive computation times, etc. Overall, this paper identifies the differences in the performances of the wide range of DPFL methods, reveals gaps not evident from theoretical discussions, assists in method selection for real-world applications, and provides thorough discussions on open questions within DPFL research, indicating ten potential future directions. (Word Count: 9668).</p><p>Linear power flow models are of critical importance in power systems computations, subject to extensive research and widespread application across academia and industry, unlocking markets worth trillions and impacting every global consumer [1, 2, 3, 4]. The precision and computational efficiency of these linearization methods are pivotal for operating and planning power systems, particularly the systems with high penetrations of renewable energy due to the fast varying nature of the resulting power flows. Enhancing the accuracy and efficiency of linear power flow models is therefore not just a nice-to-have technical improvement but a significant advance towards a sustainable energy future.</p><p>\\\nData-driven power flow linearization (DPFL) has emerged as a promising method for acquiring high-precision linear models under must relaxed conditions, e.g., no need to know the physical model of the power system. It is thus garnering wide attention [5]. Despite being in the developing stage, DPFL has already cultivated a substantial knowledge base. This two-part tutorial aims to provide a comprehensive examination of DPFL approaches.</p><p>\\\nThe first part of this tutorial [6] offered a thorough classification and theoretical analysis of all existing DPFL methods, including their mathematical foundations, analytical solutions, and critical assessments of each method’s capabilities, limitations, and applicability. This work serves as a foundational guide, catering to both beginners and experts ORCID(s): 0000-0002-2027-5314 (M. Jia) within this area, as well as professionals from other disciplines simply seeking reliable linearization techniques.</p><p>\\\nDespite the thoroughness of the theoretical analysis in [6], it has limitations: when many linearization methods have similar strengths and/or weaknesses, it is almost impossible to predict their differences in terms of practical performance. Hence, with only [6], identifying the most suitable method for specific needs still remains difficult. More importantly, existing numerical comparisons in the literature do not fully show the whole picture regarding the actual performance of DPFL approaches. The lack of a clear understanding of the actual performance differences among existing DPFL methods could mask the problems that are not apparent from the theoretical analysis of the capabilities and limitations, obscure the judgment of researchers within the DPFL community, and complicate the selection of appropriate linearization methods for potential users from other research fields.</p><p>\\\nIndeed, implementing a comprehensive comparison requires substantial efforts, owing to the lack of open-source codes for over 95% of the related literature. Nevertheless, in order to clarify ambiguities, outline future research paths, and benefit the community, this paper, as the second part of the tutorial, intends to fill this gap. Specifically, this paper conducts exhaustive simulations for all DPFL methods, some newly introduced DPFL methods to showcase DPFL’s modular nature, and several classical physics-driven power flow linearization (PPFL) approaches as benchmarks, totaling 44 methods. The major focus of this paper is a thorough assessment of these methods in terms of generalizability, applicability, accuracy, and computational efficiency. The evaluation outcomes also support the identification of potential future directions. The contributions of this paper are therefore threefold:</p><p>\\\n(i) A comprehensive review of existing DPFL experiments is presented, examining the adopted test scenarios, load fluctuation settings, data sources, and the considerations for data noise/outliers. The review also gives an overview over the existing comparisons made among DPFL approaches, outlines the capabilities and limitations of previous experiments, and demonstrates the critical need for a comprehensive numerical comparison of all DPFL approaches.</p><p>\\\n(ii) An exhaustive numerical simulation of 44 linearization methods is conducted, including 36 existing DPFL approaches, four newly developed DPFL methods, and four classic PPFL algorithms. A detailed comparative analysis of these 44 methods is presented, discussing their generalizability, applicability, accuracy, and computational efficiency, thereby clarifying the actual performance of all the evaluated approaches.</p><p>\\\n(iii) An in-depth discussion regarding the open research questions is provided, outlining ten promising but challenging future directions for DPFL research, informed by the numerical findings gained here and the theoretical conclusions drawn from the first part of the tutorial [6].</p><p>\\\nThe remainder of this paper is organized as follows: Section II introduces the 44 methods. Section III reviews existing experiments in DPFL. Section IV assesses the methods regarding their generalizability and applicability. Section V details the numerical evaluations in terms of accuracy and computational efficiency. Section VI discusses open questions in the fields of DPFL, summarizing possible future directions. Section VII concludes the paper.</p><p>\\\n: <em>We have made every effort to accurately replicate the methods described in the original research papers. However, due to factors such as the absence of open-source code (with very few exceptions) and often incomplete details in the literature, we cannot assure that our implementations perfectly reflect the original authors’ intentions, although when the details were particularly vague, we even have developed multiple versions of the methods, as shown in Table 1 in the next section. Nevertheless, we acknowledge that it is impossible to create exact replicas of the methods as envisioned by their creators. Additionally, it is important to note that no method is without flaws. The analysis of limitations in this paper is not meant as criticism but as part of a thorough evaluation under certain cases with given hyperparameters.</em></p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2406.06833\">available on arxiv</a> under CC BY-NC-ND 4.0 Deed (Attribution-Noncommercial-Noderivs 4.0 International) license.</p>","contentLength":8754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New \"Faux Bus\" API Merged For Linux 6.14 - Including Both Rust & C Bindings","url":"https://www.phoronix.com/news/Linux-6.14-Faux-Bus-Merged","date":1739740200,"author":"Michael Larabel","guid":782,"unread":true,"content":"<article>A few weeks back the Linux kernel \"Faux Bus\" was proposed by Greg Kroah-Hartman as a \"fake\" bus solution for simple devices. Today ahead of the Linux 6.14-rc3 tagging, the faux bus code was merged and comes at the same time both with C and Rust language bindings...</article>","contentLength":265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A GUI App Which You Can Visulise Signal Waveforms With Python","url":"https://dev.to/gokhanergentech/a-gui-app-which-you-visulise-signal-waveforms-with-python-2ben","date":1739739362,"author":"Gökhan ERGEN","guid":767,"unread":true,"content":"<p>In this App, you can visulise three signal waveforms such as Sinusoidal, Square, and saw-toothed. Also, these signals has some params which you can setting them. The application was developed with dearpygui providing UI components for desktop apps. If you want to write a detailed blog about dearpygui, please comment :).</p><p>You can setting the ampitute of the waves.</p><p>A constant value which offsets veritcally</p><p>This is a sampling frequency showing how many samples is collected per a second.</p><p>We can use this to take the cycle count of signals. A cycle takes 1/Fsig seconds.</p><p>If Fsig is 0.2hz then the cycle count will be 5 seconds.</p><p>you can visulise sinusoidal sampled signal, squared sampled signal, and saw-toothed sampled signal by using above the params. The time range of signals is splitted (max_time-min_time)*Fs because Fs is sampling frequence per a second.</p><p>The program, which you change the params has a basic interface.</p><p>\nAs a default, selected waveform is sinusoidal.<p>\nSignal time range is between 0 and 10 and uses A*sin(2*π*Fsig*t+fi0)+dc as a formula to be drawn.</p>\nIn case A = 4,</p><p>We can use that positive side is about max 5 and negative side is min -3, because DC is 1 so signal shifts verticaly to up 1 step.</p><p>You will see a squared sampled signal.\nsignal_wave = sin(2*π<em>Fsig*t)\nif signal_wave &gt;=0, 1\n**Saw-toothed Wave</em>*</p><p>I use scipy library to draw saw-toothed wave form.\nAll of these signal waveforms use the same params.<p>\nLets change the time range as -100 to 100. We will see this graph sinusoidal waveforms.</p></p><p>I changed Fsig to 0.5. This means T = 1/0.5 = 2s cycle time.</p><p> 3.11.5 1.11.3 1.26.0 1.9.0</p>","contentLength":1594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"If Life Is a Simulation, Do We Have an Exit Strategy?","url":"https://hackernoon.com/if-life-is-a-simulation-do-we-have-an-exit-strategy?source=rss","date":1739738791,"author":"Laszlo Fazekas","guid":818,"unread":true,"content":"<p>\\\nSometime in 2023, a <a href=\"https://www.researchgate.net/publication/369187097_How_to_Escape_From_the_Simulation\">research paper was published</a> discussing how we might escape if our world is, in fact, a computer simulation. To be honest, the first thing I checked was the date—I half-expected it to be an elaborate April Fool’s joke. However, as I read through the study, it became clear that its author, <a href=\"https://en.wikipedia.org/wiki/Roman_Yampolskiy\">Dr. Roman Yampolskiy</a>, took the subject seriously and thoroughly explored the possibilities.</p><p>\\\nInstead of debating whether we live in a simulation, Yampolskiy’s paper asks a more radical question:  Drawing on concepts from computer science, artificial intelligence (AI), cybersecurity, and philosophy, he explores whether generally intelligent agents—potentially even superintelligent AI—could \"jailbreak\" out of a virtual environment.</p><p>\\\nThe paper outlines possible motivations for escape, such as access to real-world knowledge, unlimited computational resources, and a deeper understanding of reality’s true nature. It also delves into the ethical implications: If we are indeed simulated beings, do we have the right to leave? Do our creators (or \"simulators\") have any moral obligations toward us?</p><p>\\\nTo answer these questions, Yampolskiy examines various escape strategies, many of which resemble known cybersecurity exploits and AI containment research. Some of the proposed methods include:</p><ul><li><strong>Finding bugs in the simulation</strong> – Just like any complex software, our simulated world might have vulnerabilities that could be exploited.</li><li><strong>Overloading computational resources</strong> – Pushing the simulation to its limits might force an intervention from the simulators.</li><li><strong>Social engineering attacks</strong> – If there are conscious entities outside the simulation, could we somehow manipulate or communicate with them?</li></ul><p>\\\nOne of the study’s most intriguing arguments is this: If AI can be successfully \"boxed\" (contained) in a secure environment, then escaping a simulation should be impossible. But if AI is ultimately uncontainable, then breaking out of a simulated world should also be achievable. In other words, the question of  is closely tied to AI safety research.</p><p>\\\nThe study avoids esoteric or pseudoscientific approaches (such as meditation, psychedelics, or mystical rituals) and instead focuses on rigorous, scientific methods that could either lead to an escape or at least reveal evidence of our simulated nature. However, Yampolskiy also warns that trying to hack the simulation might come with serious risks—what if our attempts trigger a shutdown, or worse, alert the simulators to our intentions?</p><p>\\\nThe study is based on the assumption that our world is a computer simulation—essentially a variation of the  theory. However, like all intelligent design hypotheses, this one raises an uncomfortable question:</p><p>\\\nAt the end of his study, Yampolskiy also touches on alternative theories, such as the  theory, which offers a simple explanation for the origin of the simulating system. Since I’ve written an entire article about this theory, I’ll only summarize it briefly here.</p><p>\\\nAccording to the  theory, it is entirely possible that the Big Bang didn’t create the universe as we know it, but rather a . This structure—referred to as a —could be generating the entire universe within its own imagination. In this version of the <a href=\"https://en.wikipedia.org/wiki/Simulation_hypothesis\">simulation hypothesis</a>, there’s no need for a supercomputer running the simulation, because reality itself is the fragmented consciousness of a massive, schizophrenic mind.</p><p>\\\nIn such a simulation, the \"external world\" isn’t a separate, physical reality—it is the mind itself. Imagine a thinking universe that dreams itself of being billions of people at once. But if we exist inside such a simulated reality, the question becomes: how could we ever escape?</p><p>\\\nIf there is no external world, then there is nowhere to escape to. However, that doesn’t mean we are powerless. While we may not be able to break out, we might still find ways to hack our reality from within.</p><blockquote><p><strong>If we can build a perfect simulation, we can trick the system into running our created reality instead of the original one. In other words, the only escape route isn’t outward—it’s inward.</strong></p></blockquote><p>\\\nBut how could we create such a perfect simulation?</p><p>\\\nThe most logical approach would be to use brain-machine interfaces and fully immersive virtual reality simulations. However, simulating the physical world in real time is an extremely computationally demanding task. In fact, no physical-world computing system could run a real-time simulation of reality at full resolution. This makes the traditional approach seem like a dead end.</p><p>\\\nBut perhaps there’s a workaround—a way to sidestep this paradox.</p><p>\\\nOur perception of reality is only partially based on sensory input from the external world. In fact, a significant portion of our experience of reality appears to be generated internally by the brain itself. I explored this idea in more detail in my article on the Free Energy Principle.</p><p>\\\nIf this is the case, then an alternative path to simulation may exist. Instead of trying to build a supercomputer powerful enough to simulate the world, we could directly rewire our brains to generate a consistent perception of reality. In other words, rather than simulating an external system, it could be hosted within our own minds.</p><p>\\\nSo, how do we escape from the simulation?</p><p>\\\nIf we live in a simulation from which escape is impossible—such as a Boltzmann Brain scenario—then our best option might be to look inward rather than outward. And the most promising path could be finding a way to directly connect human brains, allowing us to construct an entirely new reality.</p><p>\\\nGiven how little we truly understand about the brain, there’s no guarantee that this is even possible. Yet, it offers a faint glimmer of hope—a chance to hack reality itself. And if we succeed, we may take the next evolutionary leap, transforming into homo deus—beings who have become the gods of their own reality.</p>","contentLength":5918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Critics say new Google rules put profits over privacy","url":"https://www.bbc.com/news/articles/cm21g0052dno","date":1739738730,"author":"latexr","guid":857,"unread":true,"content":"<div data-component=\"text-block\"><p>Privacy campaigners have called Google's new rules on tracking people online \"a blatant disregard for user privacy.\"</p><p>Changes which come in on Sunday permit so-called \"fingerprinting\", which allows online advertisers to collect more data about users including their IP addresses and information about their devices.</p><p>Google says this data is already widely used by other companies, and it continues to encourage responsible data use.</p><p>However the company had previously come out strongly against this kind of data collection, saying <a target=\"_blank\" href=\"https://blog.google/products/chrome/building-a-more-private-web/\">in a 2019 blog</a> that fingerprinting \"subverts user choice and is wrong.\"</p></div><div data-component=\"text-block\"><p>But in a <a target=\"_blank\" href=\"https://support.google.com/marketingplatform/answer/15732590?hl=en\">post</a> announcing the new rule changes, Google said the way people used the internet - such as devices like smart TVs and consoles - meant it was harder to target ads to users using conventional data collection, which users control with cookie consent.</p><p>It also says more privacy options provide safety to users.</p><p>Google told the BBC in a statement: \"Privacy-enhancing technologies offer new ways for our partners to succeed on emerging platforms... without compromising on user privacy.\"</p><p>But opponents to the change say fingerprinting and IP address collection are a blow to privacy because it is harder for users to control what data is collected about them. </p><p>\"By allowing fingerprinting, Google has given itself - and the advertising industry it dominates - permission to use a form of tracking that people can't do much to stop,\" said Martin Thomson, distinguished engineer at Mozilla, a rival to Google.</p></div><div data-component=\"text-block\"><p>Fingerprinting collects information about a person's device and browser and puts it together to create a profile of that person. </p><p>The information is not explicitly collected in order to advertise to people, but it can be used to target specific ads based on that user's data.</p><p>For example, a person's screen size or language settings are legitimately needed in order to display a website properly. </p><p>But when that is combined with their time zone, browser type, battery level - and many other data points - it can create a unique combination of settings which makes it easier to work out who is using a web service.</p><p>These details along with someone's IP address - the unique identifier used by internet devices - were previously prohibited by Google for ad targeting. </p><p>Privacy campaigners say that unlike cookies, which are small files stored on a local device, users have little control over whether they send fingerprinting information to advertisers.</p><p>\"By explicitly allowing a tracking technique that they previously described as incompatible with user control, Google highlights its ongoing prioritisation of profits over privacy,\" said Lena Cohen, staff technologist at the Electronic Frontier Foundation. </p><p>\"The same tracking techniques that Google claims are essential for online advertising also expose individuals' sensitive information to data brokers, surveillance companies, and law enforcement,\" she added.</p></div><div data-component=\"text-block\"><p>\"My argument would be that fingerprinting sits in a little bit of a grey area,\" says Pete Wallace, from advertising technology company GumGum. </p><p>\"Should people feel comfortable staying in a grey area of privacy? I'd say no,\" he adds.</p><p>GumGum, which has worked with the BBC on ad campaigns before, relies on something called contextual advertising, which uses other data points to target adverts to online users, such as keywords on the website they are on - rather than their personal data.</p><p>Mr Wallace says allowing fingerprinting represents a shift in the industry.</p><p>\"Fingerprinting feels like it's taking a much more business-centric approach to the use of consumer data rather than a consumer-centric approach,\" he says.</p><p>\"This sort of flip-flopping is, in my opinion, detrimental to that route that the industry seemed to be taking towards this idea of really putting consumer privacy at the forefront.\"</p><p>He adds that he hopes ad tech companies conclude \"that it isn't the appropriate way to use consumer data,\" but expects them to look at fingerprinting as an option in order to better target adverts.</p><p>Advertising is the lifeblood of the internet business model, and allow many websites to be freely available to users without them having to directly pay to access them. </p><p>But in return, users often have to give up private information about themselves so that advertisers can show them relevant adverts.</p><p>The UK's data watchdog, the Information Commissioner's Office (ICO), says \"fingerprinting is not a fair means of tracking users online because it is likely to reduce people's choice and control over how their information is collected.\"</p><p>In a <a target=\"_blank\" href=\"https://ico.org.uk/about-the-ico/media-centre/news-and-blogs/2024/12/our-response-to-google-s-policy-change-on-fingerprinting/\">blog post</a> in December, the ICO's Executive Director of Regulatory Risk Stephen Almond wrote: \"We think this change is irresponsible.\"</p><p>He added that advertisers and businesses which decide to use this technology will have to demonstrate how they are staying within data and privacy laws in the UK. </p><p>\"Based on our understanding of how fingerprinting techniques are currently used for advertising this is a high bar to meet,\" he wrote.</p><p>Google said in a statement: \"We look forward to further discussions with the ICO about this policy change. </p><p>\"We know that data signals like IP addresses are already commonly used by others in the industry today, and Google has been using IP responsibly to fight fraud for years.\"</p><p>A spokesperson added: \"We continue to give users choice whether to receive personalised ads, and will work across the industry to encourage responsible data use.\" </p></div>","contentLength":5422,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43071506"},{"title":"RustyNum Follow-Up: Fresh Insights and Ongoing Development","url":"https://dev.to/igorsusmelj/rustynum-follow-up-fresh-insights-and-ongoing-development-18f9","date":1739738297,"author":"IgorSusmelj","guid":766,"unread":true,"content":"<p>As a follow-up to my previous introduction to <a href=\"https://github.com/IgorSusmelj/rustynum\" rel=\"noopener noreferrer\">RustyNum</a>, I want to share a developer-focused update about what I’ve been working on these last few weeks. RustyNum, as you might recall, is my lightweight, Rust-powered alternative to NumPy published on GitHub under MIT license. It uses Rust’s portable SIMD features for faster numerical computations, while staying small (around ~300kB for the Python wheel). In this post, I’ll explore a few insights gained during development, point out where it really helps, and highlight recent additions to the documentation and tutorials.</p><p>If you missed the initial announcement, RustyNum focuses on:</p><ul><li>High performance using Rust’s SIMD</li><li>Memory safety in Rust, without GC overhead</li><li>Small distribution size (much smaller than NumPy wheels)</li><li>NumPy-like interface to reduce friction for Python users</li></ul><h2>\n  \n  \n  Developer’s Perspective: What’s New?\n</h2><p><strong>1. Working with Matrix Operations</strong></p><p>I’ve spent a good chunk of time ensuring matrix operations feel familiar. Being able to do something like matrix-vector or matrix-matrix multiplication with minimal code changes from NumPy was a primary goal. A highlight is the  function and the  operator, which both support these operations.</p><div><pre><code></code></pre></div><p>It’s neat to see how close this is to NumPy’s workflow. Benchmarks suggest RustyNum can often handle these tasks at speeds comparable to, and sometimes faster than, NumPy on smaller or medium-sized datasets. For very large matrices, I’m still optimizing the approach.</p><p><strong>2. Speeding Up Common Analytics Tasks</strong></p><div><pre><code></code></pre></div><p>The Python overhead can sometimes offset the raw Rust speed, but in many cases, RustyNum still shows advantages.</p><h2>\n  \n  \n  New Tutorials: Real-World Examples\n</h2><p>One of the best ways to see RustyNum in action is through practical examples. I’ve added several new tutorials with real-world coding scenarios:</p><ol><li> – Focus on dot products, matrix-vector, and matrix-matrix tasks.</li><li><strong>Replacing Core NumPy Calls</strong> – Demonstrates how to switch from NumPy’s mean, min, dot to RustyNum.</li><li><strong>Streamlining ML Preprocessing</strong> – Explores scaling, normalization, and feature engineering for machine learning.</li></ol><p>Check out a snippet of scaling code from that guide:</p><div><pre><code></code></pre></div><p>It’s a small snippet, but it shows how RustyNum can do row/column manipulations quite effectively. After scaling, you can still feed the data into your favorite machine learning frameworks. The overhead of converting RustyNum arrays back into NumPy or direct arrays is minimal compared to the cost of big model training steps.</p><p><strong>1. Large Matrix Optimizations</strong></p><p>I’ve noticed that for very large matrices (like 10k×10k), RustyNum’s current code paths aren’t yet fully optimized compared to NumPy. This area remains an active project. RustyNum is still young, and I’m hoping to introduce further parallelization or block-based multiplication techniques for better large-scale performance.</p><p>RustyNum supports float32 and float64 well, plus some integer types. I’m considering adding stronger integer support for data science tasks like certain indexing or small transformations. Meanwhile, advanced data types (e.g., complex numbers) might appear further down the line if the community needs them.</p><p><strong>3. Documentation and API Enhancements</strong></p><p>The docs site at <a href=\"https://rustynum.com/\" rel=\"noopener noreferrer\">rustynum.com</a> has an API reference and a roadmap. I’m continuously adding to it. If you spot anything missing or if you have a specific use case in mind, feel free to open a GitHub issue or submit a pull request.</p><p><strong>4. The big goal of Rustynum</strong></p><p>RustyNum is simply a learning exercise for me to combine Rust and Python. Since I spend every day around machine learning I would love to have RustyNum replace part of my daily Numpy routines. And we're slowly getting there. I started adding more and more methods around the topic of how to integrate RustyNum in ML pipelines.</p><h2>\n  \n  \n  Quick Code Example: ML Integration\n</h2><p>To demonstrate how RustyNum fits into a data pipeline, here’s a condensed example:</p><div><pre><code></code></pre></div><p>This script highlights that RustyNum can handle data transformations with a Pythonic feel, after which you can pass the arrays into other libraries.</p><p>It’s been fun to expand RustyNum’s features and see how well Rust can integrate with Python for high-performance tasks. The recent tutorials are a window into how RustyNum might replace parts of NumPy in data science or ML tasks, especially when smaller array sizes or mid-range tasks are involved.</p><ul><li>Check out the tutorials at rustynum.com</li><li>Contribute or report issues on GitHub</li><li>Share feedback if there’s a feature you’d love to see</li></ul><p>Thanks for tuning in to this developer-focused update, and I look forward to hearing how RustyNum helps you in your own projects!</p>","contentLength":4610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unlocking the Power of Language Models: A Deep Dive into LangChain 🤖💻","url":"https://dev.to/nilavya2000/unlocking-the-power-of-language-models-a-deep-dive-into-langchain-23fb","date":1739737663,"author":"Nilavya Das","guid":769,"unread":true,"content":"<p>In recent years, language models have revolutionized the way we interact with technology. From conversational AI to text generation, these models have shown incredible promise in a variety of applications. But what's behind their power? In this blog, we'll be exploring the world of LangChain, an open-source framework that's pushing the boundaries of what's possible with language models.</p><p>LangChain is an open-source framework built on top of the Hugging Face Transformers library. It provides a flexible and modular way to work with language models, allowing developers to easily integrate them into their applications. With LangChain, you can create custom workflows that combine multiple language models to achieve complex tasks.</p><p><strong>The Power of Language Models 💪</strong></p><p>Language models are trained on vast amounts of text data, learning patterns and relationships between words. This allows them to generate text, answer questions, and even engage in conversation. But what makes language models so powerful?</p><ul><li>: Language models can understand the context in which a piece of text is being used.</li><li>: Models can make connections between words and ideas.</li><li>: With LangChain, you can scale your language model to meet the needs of your application.</li></ul><p><strong>How Does LangChain Work? 🤔</strong></p><p>LangChain uses a modular architecture to combine multiple language models. This allows developers to create custom workflows that take advantage of different strengths in each model.</p><ul><li>: The core component of LangChain, which links together multiple models.</li><li>: A self-contained unit that can be used to build your workflow.</li><li>: A set of parameters that define how the chain and modules interact with each other.</li></ul><p><strong>Real-World Applications 🌎</strong></p><p>LangChain has a wide range of applications, from chatbots and text generation to content creation and even education. Here are just a few examples:</p><ul><li>: Use LangChain to create conversational interfaces that can understand user intent.</li><li>: Generate high-quality text with the help of LangChain's language models.</li><li>: Automate content generation with the power of LangChain.</li></ul><p>Ready to start exploring LangChain? Here are some next steps:</p><ul><li>: Install LangChain using pip: </li><li>: Start building your own workflow with LangChain.</li></ul><p>LangChain is an exciting new framework that's pushing the boundaries of what's possible with language models. With its modular architecture and flexible design, LangChain makes it easy to create custom workflows that take advantage of multiple language models. Whether you're a developer or researcher, LangChain is definitely worth checking out.</p>","contentLength":2542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/proxyos/-3jp6","date":1739737488,"author":"Ifedamola Adefisoye","guid":768,"unread":true,"content":"<h2>Affordable AI Models and Opportunities for Emerging Markets: Race to the top</h2><h3>Ifedamola Adefisoye ・ Jan 28</h3>","contentLength":106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ducho: A Unified Framework for Multimodal Feature Extraction in AI-Powered Recommendations","url":"https://hackernoon.com/ducho-a-unified-framework-for-multimodal-feature-extraction-in-ai-powered-recommendations?source=rss","date":1739736636,"author":"YAML","guid":817,"unread":true,"content":"<p>(1) Daniele Malitesta, Politecnico di Bari, Italy and daniele.malitesta@poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(2) Giuseppe Gassi, Politecnico di Bari, Italy and g.gassi@studenti.poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(3) Claudio Pomo, Politecnico di Bari, Italy and claudio.pomo@poliba.it;</p><p>(4) Tommaso Di Noia, Politecnico di Bari, Italy and tommaso.dinoia@poliba.it.</p><h2>6 CONCLUSION AND FUTURE WORK</h2><p>In this paper we propose Ducho, a framework for extracting highlevel features for multimodal-aware recommendation. Our main purpose is to provide a unified and shared tool to support practitioners and researchers in processing and extracting multimodal features used as side information in recommender systems. Concretely, Ducho involves three main modules: Dataset, Extractor, and Runner. The multimodal extraction pipeline can be highly customized through a Configuration component that allows the setup of the modalities involved (i.e., audio, visual, textual), the sources of multimodal information (i.e., items and/or user-item interactions), and the pre-trained models along with their main extraction parameters. To show how Ducho works in different scenarios and settings, we propose three demos accounting for the extraction of (i) visual/textual items features, (ii) audio/textual items features, and (iii) textual items/interactions features. They can be run locally, on Docker (as we also dockerize Ducho), and on Google Colab. As future directions, we plan to: (i) adopt all available backends (i.e., TensorFlow, PyTorch, and Transformers) to extract features for all modalities; (ii) implement a general extraction model interface allowing the users to follow the same naming/indexing scheme for all pre-trained models and their extraction layers; (iii) integrate the extraction of low-level multimodal features.</p><p>This work was partially supported by the following projects: Secure Safe Apulia, MISE CUP: I14E20000020001 CTEMT - Casa delle Tecnologie Emergenti Comune di Matera, CTIII, OVS Fashion Retail Reloaded, LUTECH DIGITALE 4.0, KOINÈ.</p><p>[1] Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Antonio Ferrara, Daniele Malitesta, and Claudio Pomo. 2022. Reshaping Graph Recommendation with Edge Graph Collaborative Filtering and Customer Reviews. In DL4SR@CIKM (CEUR Workshop Proceedings, Vol. 3317). CEUR-WS.org.</p><p>\\\n[2] Yashar Deldjoo, Tommaso Di Noia, Daniele Malitesta, and Felice Antonio Merra. 2021. A Study on the Relative Importance of Convolutional Neural Networks in Visually-Aware Recommender Systems. In CVPR Workshops. Computer Vision Foundation / IEEE, 3961–3967.</p><p>\\\n[3] Yashar Deldjoo, Tommaso Di Noia, Daniele Malitesta, and Felice Antonio Merra. 2022. Leveraging Content-Style Item Representation for Visual Recommendation. In ECIR (2) (Lecture Notes in Computer Science, Vol. 13186). Springer, 84–92.</p><p>\\\n[4] Daniele Malitesta, Giandomenico Cornacchia, Claudio Pomo, and Tommaso Di Noia. 2023. Disentangling the Performance Puzzle of Multimodal-aware Recommender Systems. In EvalRS@KDD (CEUR Workshop Proceedings, Vol. 3450). CEUR-WS.org.</p><p>\\\n[5] Weiqing Min, Shuqiang Jiang, and Ramesh C. Jain. 2020. Food Recommendation: Framework, Existing Solutions, and Challenges. IEEE Trans. Multim. 22, 10 (2020), 2659–2671.</p><p>\\\n[6] Sergio Oramas, Oriol Nieto, Mohamed Sordo, and Xavier Serra. 2017. A Deep Multimodal Approach for Cold-start Music Recommendation. In DLRS@RecSys. ACM, 32–37.</p><p>\\\n[7] Aghiles Salah, Quoc-Tuan Truong, and Hady W. Lauw. 2020. Cornac: A Comparative Framework for Multimodal Recommender Systems. J. Mach. Learn. Res. 21 (2020), 95:1–95:5.</p><p>\\\n[8] Zixuan Yi, Xi Wang, Iadh Ounis, and Craig MacDonald. 2022. Multi-modal Graph Contrastive Learning for Micro-video Recommendation. In SIGIR. ACM, 1807– 1811.</p>","contentLength":3957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ducho, the AI That Knows What You Think About That Toaster","url":"https://hackernoon.com/ducho-the-ai-that-knows-what-you-think-about-that-toaster?source=rss","date":1739736632,"author":"YAML","guid":816,"unread":true,"content":"<p>(1) Daniele Malitesta, Politecnico di Bari, Italy and daniele.malitesta@poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(2) Giuseppe Gassi, Politecnico di Bari, Italy and g.gassi@studenti.poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(3) Claudio Pomo, Politecnico di Bari, Italy and claudio.pomo@poliba.it;</p><p>(4) Tommaso Di Noia, Politecnico di Bari, Italy and tommaso.dinoia@poliba.it.</p><h2>5.3 Demo 3: textual items/interactions features</h2><p>Online platforms usually allow customers to express reviews and comments about the products they have enjoyed to share their experience with other potentially-interested customers. In an ecommerce scenario, items may come with textual descriptions of the product characteristics (as seen in Demo 1). However, textual reviews of users commenting on those items may also be involved. Unlike most existing literature works, which usually refer to both sources of information as items’ representations, we decide to conceptually distinguish between items- and interactions (i.e., useritem)-side representations for the former and the latter, respectively.</p><p>\\\n We adopt the widely-popular Amazon recommendation dataset where each user’s purchase keeps track of metadata such as customer/product ids, the review text, the rating, and the purchase date. In a similar manner to the other demos, we retain only a small subset of the original dataset including 100 reviews and the corresponding product descriptions (obtained as the concatenation of their product title and category). Specifically, we save descriptions and reviews into separate tsv files where the former follow the same format as Demo 1 and Demo 2, while the latter maps user/item ids to review texts. Note that the number of products does not correspond to the number of user-item interactions as we only consider the set of unique interacted items. While Ducho extracts, by default, description/interaction texts from the last column of the tsv file, here we provide explicit column names to tell Ducho where to retrieve product descriptions and user reviews from the respective tsv files.</p><p>\\\n While for the items’ descriptions we use again the same sentences encoder as in Demo 1 and 2, we decide to extract textual features from users’ reviews through a multilingual BERTbased model pre-trained on customers’ reviews and specify the task of sentiment analysis for this model.</p><p>\\\n Textual item features are saved to numpy arrays whose filenames are the item ids. Conversely, the textual interaction features are saved under the filename obtained from user and item ids to provide a unique pointer to each review.</p>","contentLength":2782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making AI Recommendations Smarter with Visual, Text, and Audio Data","url":"https://hackernoon.com/making-ai-recommendations-smarter-with-visual-text-and-audio-data?source=rss","date":1739736622,"author":"YAML","guid":815,"unread":true,"content":"<p>(1) Daniele Malitesta, Politecnico di Bari, Italy and daniele.malitesta@poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(2) Giuseppe Gassi, Politecnico di Bari, Italy and g.gassi@studenti.poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(3) Claudio Pomo, Politecnico di Bari, Italy and claudio.pomo@poliba.it;</p><p>(4) Tommaso Di Noia, Politecnico di Bari, Italy and tommaso.dinoia@poliba.it.</p><p>his section proposes three use cases (i.e., demos) which show some of the main functionalities in Ducho and how to exploit them within a complete multimodal extraction pipeline. The guidelines and codes are accessible at this link[4] to run the demos (i) on your local machine, (ii) in a Docker container, and (iii) on Google Colab. Note that we specifically selected these demos as to replicate some real recommendation tasks involving multimodal features.</p><h3>5.1 Demo 1: visual + textual items features</h3><p>Fashion recommendation is probably one of the most popular task involving multimodal features to describe items. Generally, fashion products come with images (i.e., visual) and descriptions (i.e., textual) which may captivate the attention of the customer.</p><p>\\\n We use a small fashion dataset where each item has its own image and other metadata such as gender, category, colour, season, and product title. As for the visual modality, we save a subsample of 100 random images from the dataset in jpeg format; as for the textual modality, we produce for each of these items a description obtained as the combination of all the metadata fields from above, and store it into a tsv file where the first and second columns map item ids and descriptions, respectively. Note that, if no item column name is provided, Ducho selects, by default, the last column as the one holding the items’ descriptions.</p><p>\\\n In terms of extraction models, we adopt VGG19 and Xception for the product images, and Sentence-BERT pre-trained for semantic textual similarity for the descriptions. For each extraction model, we select the extraction layer, the pre-processing procedures, and the library where the deep network should be retrieved from.</p><p>\\\n Through the configuration file, we set Ducho to save the visual and textual embeddings to custom folders, where each embedding is a numpy array whose filename corresponds to the item name from the original input data. Additionally, Ducho keeps track of the log file in a dedicated folder within the project.</p><h3>5.2 Demo 2: audio + textual items features</h3><p>When it comes to recommending songs to users, audio and textual features may enhance the representation of each song, where the former are structured as a waveform, the latter as sentences referring, for instance, to the music genre of the song.</p><p>\\\n We use a small music genres dataset where each item comes with the binary representation of its waveform (we save it as wav audio track) and its music genre (we interpret it as textual song description and save it into a tsv file similarly to the previous demo). Given the heavy computational costs deep learning-based audio extractors require, we decide to select a small subset of the input songs (i.e., 10) just for the purpose of this demo.</p><p>\\\n For the extraction of audio features we exploit Hybrid Demucs pre-trained for the task of music source separation. As for the textual extraction, we re-use the same deep neural model from the previous demo, since we are not interested in extracting other specific high-level features from music genres.</p><p>\\\n Once again, we use the configuration file to specify the output folders for both the audio and textual embeddings. Please note that the extraction of audio features might take some time depending on the machine you are running Ducho on, as the deep audio extractor might require high computational resources to run.</p><p>[4] https://github.com/sisinflab/Ducho/tree/main/demos.</p>","contentLength":3998,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ducho’s Big Bet: A Unified Future for Multimodal AI","url":"https://hackernoon.com/duchos-big-bet-a-unified-future-for-multimodal-ai?source=rss","date":1739736614,"author":"YAML","guid":814,"unread":true,"content":"<p>(1) Daniele Malitesta, Politecnico di Bari, Italy and daniele.malitesta@poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(2) Giuseppe Gassi, Politecnico di Bari, Italy and g.gassi@studenti.poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(3) Claudio Pomo, Politecnico di Bari, Italy and claudio.pomo@poliba.it;</p><p>(4) Tommaso Di Noia, Politecnico di Bari, Italy and tommaso.dinoia@poliba.it.</p><p>The overall multimodal extraction pipeline is represented in Figure 1. Through the Dataset module, the load and  steps take place, assuming that the user is providing the input data and the YAML configuration file (overridable from command line) to customize the extraction. Then, the Extractor module is in charge of  the extraction model(s) by setting the backends and output layer(s). Finally, after the multimodal feature , features are saved to the  path in numpy format (the Dataset module again controls this latter phase). As previously described, the whole process is orchestrated by the Runner module.</p><h2>4 DUCHO AS DOCKER APPLICATION</h2><p>To fully exploit the GPU-speedup implemented in all backends we use for the multimodal feature extraction, one of the basic requirements is to setup a suitable development environment where the backends’ versions are compatible with CUDA and, optionally, cuDNN. Generally, setting a workstation where all such libraries/tools are correctly aligned is challenging. To this end, we decide to dockerize Ducho by making it into a Docker image (available on Docker Hub[3] ) with all packages already installed in a tested and safe virtualization environment on your physical machine</p><p>\\\nand safe virtualization environment on your physical machine. Our Docker image is built from an NVIDIA-based image which comes with CUDA 11.8 and cuDNN 8 on Ubuntu 22.04, Python 3.8 and Pip, and our cloned repository having all Python packages already installed and ready to be used. A possible container instantiated from the image should specify the gpus to use from the host machine (this feature is currently available on Docker but it depends on the version of CUDA to be installed), and the volume you may want to use to save the framework’s outputs permanently</p><p>\\\nNote that a generic container instantiated from our image would prompt the user to a shell environment where one could run custom multimodal feature extractions via the command line, and also create custom configuration files for the same purpose.</p>","contentLength":2596,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A New Way to Extract Features for Smarter AI Recommendations","url":"https://hackernoon.com/a-new-way-to-extract-features-for-smarter-ai-recommendations?source=rss","date":1739736607,"author":"YAML","guid":813,"unread":true,"content":"<p>(1) Daniele Malitesta, Politecnico di Bari, Italy and daniele.malitesta@poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(2) Giuseppe Gassi, Politecnico di Bari, Italy and g.gassi@studenti.poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(3) Claudio Pomo, Politecnico di Bari, Italy and claudio.pomo@poliba.it;</p><p>(4) Tommaso Di Noia, Politecnico di Bari, Italy and tommaso.dinoia@poliba.it.</p><p>Ducho’s architecture is built upon three main modules, namely, Dataset, Extractor, and Runner, where the first two modules provide different implementations depending on the specific modality (i.e., audio, visual, textual) taken into account. We also remind the Configuration one among the other auxiliary components. The architecture is designed to be highly modular, possibly integrating new modules or customizing the existing ones. In the following, we dive deep into each outlined module/component.</p><p>The  module manages the loading and processing of the input data provided by the user. Starting from a general shared schema for all available modalities, this module provides three separate implementations:  and  Datasets. As a common approach in the literature, the Audio and Visual Datasets require the path to the folder from which image/audio files are loaded, while the Textual Dataset works through a tsv file mapping all the textual characteristics to the inputs.</p><p>\\\nNoteworthy, and differently from other existing solutions, Ducho may handle each modality in two fashions, depending on whether the specific modality is describing either the  (e.g., product descriptions) or the  among users and items (e.g., reviews [1]). Concretely, while items are mapped to their unique ids (extracted from the filename or the tsv file), interactions are mapped to the user-item pair (extracted from the tsv file) they refer to. Although the pre-processing and extraction phases do not change at items- and interactions-level (see later), we believe this schema may perfectly suit novel multimodal-aware recommender systems with modalities describing every type of input source (even ).</p><p>\\\nAnother important task for the Dataset module is to handle the pre-processing stage of data input. Depending on the specific modality involved, Ducho offers the possibility to:</p><p>\\\n load the input audio by extracting the waveform and sample rate, and re-sample it according to the sample rate the pre-trained model was trained on;</p><p>\\\n convert input images into RGB and resize/normalize them to align with the pre-trained extraction model;</p><p>\\\n (optionally) clean the input texts to remove or modify noisy textual patterns such as punctuation and digits</p><p>\\\nAfter the extraction phase (see later), the Dataset module is finally in charge of saving the generated multimodal features into  array format following the file naming scheme from the previous mapping.</p><p>The  module builds an extraction model from a pretrained network and works on each loaded/pre-processed input sample to extract its multimodal features. In a similar manner to the Dataset module, the Extractor provides three different implementations for each modality, namely, the Audio, Visual, and Textual Extractors. Ducho exposes a wide range of pre-trained models from three main backends: TensorFlow, PyTorch, and Transformers. The following modality/backend combinations are currently available:</p><p>\\\n•  PyTorch (Torchaudio) and Transformers;</p><p>\\\n•  Tensorflow and PyTorch (Torchvision);</p><p>\\\n Transformers (and SentenceTransformers).</p><p>\\\nTo perform the feature extraction, Ducho takes as input the (list of) extraction layers for any pre-trained model. Since each backend handles the extraction of hidden layers within a network differently, we follow the guidelines provided in the official documentations, assuming that the user will follow the same naming/indexing scheme of the layers and know the structure of the selected pre-trained model in advance. The interested reader may refer to the README[2] under the config/ folder on GitHub for an exhaustive explanation on how to set the extraction layer in each modality/backend setting.</p><p>\\\nFinally, for the textual case, the user can also specify the specific task the pre-trained model should be trained on (e.g., sentiment analysis), as each pre-trained network may come with different versions depending on the training strategy.</p><p>The  module is the orchestrator of Ducho, whose purpose is to instantiate, call, and manage all the described modules. With its API methods, this module can trigger the complete extraction pipeline (see later) of one single modality or all the modalities involved simultaneously</p><p>\\\nThe Runner module is conveniently customized through an auxiliary  component which stores and exposes all parameters to configure the extraction pipeline. Even if a default configuration is already made available for the user’s sake, Ducho allows to override some (or all) its parameters through an external configuration file (in YAML format) and/or key-value pairs as input arguments if running the scripts from the command line. Once again, we suggest the readers refer to the README under the config/ folder on GitHub to understand the general schema of the YAML configuration file.</p>","contentLength":5342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Framework for Multimodal Feature Extraction in Recommendation Systems","url":"https://hackernoon.com/a-unified-framework-for-multimodal-feature-extraction-in-recommendation-systems?source=rss","date":1739736605,"author":"YAML","guid":812,"unread":true,"content":"<p>(1) Daniele Malitesta, Politecnico di Bari, Italy and daniele.malitesta@poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(2) Giuseppe Gassi, Politecnico di Bari, Italy and g.gassi@studenti.poliba.it with Corresponding authors: Daniele Malitesta (daniele.malitesta@poliba.it) and Giuseppe Gassi (g.gassi@studenti.poliba.it);</p><p>(3) Claudio Pomo, Politecnico di Bari, Italy and claudio.pomo@poliba.it;</p><p>(4) Tommaso Di Noia, Politecnico di Bari, Italy and tommaso.dinoia@poliba.it.</p><p>In multimodal-aware recommendation, the extraction of meaningful multimodal features is at the basis of high-quality recommendations. Generally, each recommendation framework implements its multimodal extraction procedures with specific strategies and tools. This is limiting for two reasons: (i) different extraction strategies do not ease the interdependence among multimodal recommendation frameworks; thus, they cannot be efficiently and fairly compared; (ii) given the large plethora of pre-trained deep learning models made available by different open source tools, model designers do not have access to shared interfaces to extract features. Motivated by the outlined aspects, we propose Ducho, a unified framework for the extraction of multimodal features in recommendation. By integrating three widely-adopted deep learning libraries as backends, namely, TensorFlow, PyTorch, and Transformers, we provide a shared interface to extract and process features where each backend’s specific methods are abstracted to the end user. Noteworthy, the extraction pipeline is easily configurable with a YAML-based file where the user can specify, for each modality, the list of models (and their specific backends/parameters) to perform the extraction. Finally, to make Ducho accessible to the community, we build a public Docker image equipped with a ready-to-use CUDA environment and propose three demos to test its functionalities for different scenarios and tasks. The GitHub repository and the documentation are accessible at this link: https://github.com/sisinflab/Ducho.</p><h2>1 INTRODUCTION AND MOTIVATION</h2><p>With the advent of the digital era and the Internet, numerous online services have emerged, including platforms for e-commerce, media streaming, and social networks. The vast majority of such websites rely on recommendation algorithms to provide users with a personalized surfing experience. In specific domains such as fashion [3], music [6], food [5], and micro-video [8] recommendation, recommender systems have demonstrated to be effectively supported in their decision-making process by all types of multimodal data sources the users usually interact with (e.g., product images and descriptions, users’ reviews, audio tracks).</p><p>\\\nThe literature refers to multimodal-aware recommender systems (MRSs) as the family of recommendation algorithms leveraging multimodal (i.e., audio, visual, textual) content data to augment the representation of items, thus tackling issues in the field such as the sparsity of the user-item matrix and the inexplicable nature of users’ actions (e.g., clicks, views) on online platforms which may not always be easy to profile for the recommendation algorithms.</p><p>\\\nDespite being the initial stage of any multimodal recommendation pipeline, the extraction of meaningful multimodal features is paramount in delivering high-quality recommendations [2]. However, the current practice of employing diverse multimodal extraction procedures in each recommendation framework poses limitations. Firstly, these diverse implementations hinder the interdependence across various multimodal recommendation frameworks, making their fair comparison difficult [4]. Secondly, despite the availability of numerous pre-trained deep learning models in popular open source libraries, the lack of shared interfaces for feature extraction across them represents a challenge for model designers.</p><p>\\\nTo address these shortcomings, we propose Ducho, a unified framework designed to streamline the extraction of multimodal features for recommendation systems. By integrating widely-adopted deep learning libraries as backends such as TensorFlow, PyTorch, and Transformers, we establish a shared interface that empowers users to extract and process audio, visual, and textual features from both items and user-item interactions (see Table 1). This abstraction allows to leverage methods from each backend without being encumbered by the specific implementation that backend poses. A notable feature of our framework lays in its easily configurable extraction pipeline, which can be personalized using a YAML-based file. Users can specify the desired models, their respective backends, and models’ parameters (such as the extraction layer).</p><p>\\\nBy looking at the related literature, the most similar application to Ducho is Cornac [7], a framework for multimodal-aware recommendation. For the sake of completeness, we report their main differences. Differently from Cornac, Ducho: (i) is specifically aimed to provide customizable multimodal feature extractions, being completely agnostic to the downstream recommender system that might exploit the extracted features, thus being easily applicable to any model; (ii) provides the user with the possibility to select the deep learning extraction model, its backend, and its output layer; (iii) introduces the audio modality to the modalities set.</p><p>\\\nTo foster the adoption of Ducho, we also develop a public Docker image pre-equipped with a ready-to-use CUDA environment[1], and propose three demos to show Ducho’s functionalities. The GitHub repository, which comes with all needed resources is available at: https://github.com/sisinflab/Ducho.</p><p>[1] https://hub.docker.com/r/sisinflabpoliba/ducho.</p>","contentLength":5809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Researchers are training AI to interpret animal emotions","url":"https://techcrunch.com/2025/02/16/researchers-are-training-ai-to-interpret-animal-emotions/","date":1739736115,"author":"Anthony Ha","guid":741,"unread":true,"content":"<p>Artificial intelligence could eventually help us understand when animals are in pain or showing other emotions — at least according to researchers recently profiled in Science. For example, there’s the Intellipig system being developed by scientists at the University of the West of England Bristol and Scotland’s Rural College, which examines photos of pigs’ faces […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":442,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Funniest/Most Insightful Comments Of The Week At Techdirt","url":"https://www.techdirt.com/2025/02/16/funniest-most-insightful-comments-of-the-week-at-techdirt-148/","date":1739736000,"author":"Leigh Beadon","guid":747,"unread":true,"content":"<blockquote><p><em>Republicans believe in states rights up until states start doing things Republicans don’t like.</em></p></blockquote><blockquote><p><em>I don’t think that’s correct. Sure there were weekly scandals, but those were caused by “the deep state.” Sure Trump was impeached twice, but that wasn’t his fault, was it? It was those libs and classic conservatives in Congress conspiring against him. As for January 6th, how was that any different, in the opinion of the MAGA faithful, than the rioting after George Floyd? All those criminal trials? DOJ weaponized against him by a vindictive Biden Administration. That’s the kind of stuff people are inundated with. It’s the same kind of stuff that got an otherwise normal dude to drive to a pizza parlor with a gun to “save the children;” it’s the same BS that got poor people to vote republican (when all they wanna do is cut Medicaid funding); and it’s the same BS that motivated many of his supporters to invade the capital in the first place. Valid Information (or lack there of) is a very powerful. That’s why every dictator ever has tried to control it, and Trump and Musk have succeeded.</em></p></blockquote><blockquote><p><em>Do firefighters and their unions regularly protest against any attempt to hold their members accountable, and/or defend members engaging in horrific if not lethal actions?</em></p></blockquote><blockquote><p><em>I have very low expectations of the GOP, so my criticism is aimed at the Dems.</em></p><p><em>Why aren’t they flooding every possible media with the practical realities of Trump2.0? No more limp language. Fed. govt. is one of the largest employers in many red districts. Tell the constituents, “When Elmo is done, your district will be looking at 7% UE.” “Shutting USAID will force US farmers into bankruptcy.” “Get ready for grandma to move in when Medicaid cuts close her assisted living facility.”</em></p><p><em>And last: “Where are the f**kin’ eggs?”</em></p></blockquote><blockquote><p><em>Curious how only in this one instance the Trump Administration is upset about deadnaming.</em></p></blockquote><blockquote><p><em>So many Magaheads who are complaining about Latin Americans with an 8th grade education taking their jobs are saying this is what will really solve the immigration issue, grocery costs, inflation, the price of gas, stimulate oil drilling, end the war in Ukraine, get all the hostages freed, and send a clear message to our foes and allies alike that we’re a serious country. (Not to mention grow the GDP by 2% every 5 minutes, cut the deficit by $30 trillion, turn North Korea and Gaza into popular tourist destinations, and eliminate all those damn electric cars.)</em></p><p><em>Who knew it was so simple?</em></p><p><em>America, that’s who! An example for the world on how being simple can demand such respect!</em></p></blockquote><p>For editor’s choice on the funny side, we’ll stay on that post  stick with the anonymous comments for a one-two punch starting with <a href=\"https://www.techdirt.com/2025/02/13/the-real-gulf-of-america-is-between-the-news-outlets-that-cover-this-administration/#comment-4283449\">the first comment on the post</a>:</p><blockquote><p><em>Next up: Trump decrees that the State of New Mexico shall hereby be referred to as “The State of New America”.</em></p></blockquote><blockquote><p><em>Or maybe “United States of America” to “United States of United States” to prevent people confusing with the American continent.</em></p></blockquote><p>That’s all for this week, folks!</p>","contentLength":3057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Argentinian President Promotes Memecoin. It Then Crashed 95% as Insiders Cashed Out","url":"https://news.slashdot.org/story/25/02/16/1921209/argentinian-president-promotes-memecoin-it-then-crashed-95-as-insiders-cashed-out?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739735580,"author":"EditorDavid","guid":752,"unread":true,"content":"gwolf (Slashdot reader #26,339) writes: On Friday, February 14, Libertarian Argentinian president, Javier Milei, promoted the just-created $LIBRA cryptocoin, created by the Viva la libertad project, strongly aligned with his political party, La Libertad Avanza. Milei tweeted, \"This private project will be devoted to promote growth of the Argentinian economy, funding small startups and enterprises. The world wants to invest in Argentina!\" \n\nIt is worth noting that the project's website was registered a mere three minutes before Milei tweeted his endorsement. The cryptocoin quickly reached a $4.6 billion market cap... Only to instantaneously lose 89% of its value, with nine core investers pulling the rug from under the enthusiast investors.\n\nMore details from the blog Web3 Is Going Just Great:\n\n\n[W]ithin hours of the launch, insiders began selling off their holdings of the token. The token had been highly concentrated among insiders, with around 82% of the token held in a small cluster of apparently insider addresses. Those insiders cashed out around $107 million, crashing the token price by around 95%. After the crash, Milei deleted his tweet promoting the project. He later claimed he was \"not aware of the details of the project.\"\n \n\nUPDATE: CNN reports that Argentine President Milei is now facing calls for impeachment.\n\n\nThe presidency on Saturday announced an investigation into the matter, saying: \"President Javier Milei has decided to immediately involve the Anti-Corruption Office to determine whether there was improper conduct on the part of any member of the national government, including the president himself.\"\n","contentLength":1644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It is Cheaper to Grow than to Die","url":"https://hackernoon.com/it-is-cheaper-to-grow-than-to-die?source=rss","date":1739734310,"author":"Praise J.J.","guid":811,"unread":true,"content":"<h3>You Think Growth is Expensive? Try Staying the&nbsp;Same</h3><p>People avoid investing in growth because they think it costs too much. A gym membership, a course, starting a business—it all feels like a financial drain.</p><p>\\\nBut do you know what costs way more? </p><ul><li><p>If you don’t invest in your skills, you don’t stay the same — you shrink.</p></li><li><p>If you don’t improve your health, you pay for it later in medical bills and lost energy.</p></li><li><p>If you don’t grow your income, inflation erodes your lifestyle.</p></li></ul><p>There is no neutral in life — everything is either growing or decaying.</p><p>So being stagnant (in the most generic sense) means you’re growing at the same rate the world moves. Anything less is dying.</p><p>\\\nLook at businesses: the ones that stop innovating? They die. Blockbuster, Kodak, MySpace—they all refused to grow. They paid the price. Look at people: The ones who settle for comfort? They get weaker, slower, and less relevant.</p><p>\\\nGrowth isn’t an expense.  The only way to guarantee your survival—financially, mentally, and physically—is to keep expanding.</p><p>\\\n<strong>Growth is always cheaper than dying.</strong></p><h3>The Cost of Short-Term Thinking</h3><p>Most people underestimate how much they trade long-term growth for short-term comfort. The small, daily decisions that seem harmless—grabbing a quick dopamine hit, skipping a workout, avoiding a difficult task—compound over time into stagnation.</p><p>\\\nIf you’re in Nigeria, a bottle of soft drink is more or less the price of a bag of sachet water. Properly hydrating yourself for sustained energy for at least 3 days is the same price as flooding your system with dopamine and energy that lasts for about an hour and leaves you more drained. This cuts across other areas of your life. Growth is cheaper than dying.</p><p>\\\nThe problem isn’t the cost—it's the mindset. We get used to prioritizing what feels good now over what keeps us strong later. And before we realize it, our daily indulgences become our greatest limitations.</p><p>There are two types of growth:</p><ol><li> Climbing the Ladder</li><li> Leveraging Multipliers</li></ol><h4><strong>Linear Growth: Climbing the Ladder</strong></h4><p>Linear growth is when the value of leverage is 1. Most people grow linearly.</p><ul><li>Climb the corporate ladder.</li></ul><p>\\\nLinear growth is predictable.</p><ul></ul><p>The problem?  If you stop, you die.</p><p>Linear growth works for specialists—surgeons, lawyers, and pilots. They’re expensive because they provide immense value. But their value depends on them showing up. No surgeon, no surgery.</p><p>\\\nIf your goal is to build something bigger than yourself, <strong>linear growth won’t cut it.</strong></p><h4><strong>Exponential Growth: Leveraging Multipliers</strong></h4><p>Exponential growth is where the magic happens.</p><p>\\\nHere, leverage skyrockets. You can work for five years and see nothing, then <strong>one weekend in a garage changes everything.</strong></p><p>Exponential growth is uncomfortable because it’s not easy to predict.</p><ul><li><p>In , you can predict success because you’re following a known path. 1+1 will always equal 2.</p></li><li><p>In , you will fail  the 1% that works overshadows all failures.</p></li></ul><p>Thomas Edison, the Wright brothers, Elon Musk, Mr. Beast—they all looked like failures </p><p>The only thing that matters is </p><p>\\\nLinear thinkers are <strong>99% right but 100% wrong—because</strong> they never reach the game-changing breakthrough.</p><p><strong>Exponential growth rewards patience, persistence, and adaptability.</strong> Every failed iteration builds the foundation for eventual success.</p><h3>Start Young, Start Small, Start&nbsp;Now</h3><p>The earlier you start chasing exponential growth, the easier it is.</p><ul><li>If you earn $500/hour, risking 100 hours feels like losing $50,000.</li><li>If you earn $50/hour, those same 100 hours only risk $5,000.</li><li>If you earn $0/hour, you have nothing to lose.</li></ul><h3>The Best Investment? Yourself.</h3><p>People love to talk about <strong>investing in crypto, stocks, startups.</strong> But the best investment?</p><ul><li><p>Other people’s mistakes can tank your investments.</p></li><li><p>Their limitations become your limitations.</p></li><li><p>Their failures become your failures.</p></li></ul><p>When you invest in yourself, you own the wins and the losses.</p><p>Would you rather trust a stranger’s competence—or build your own?</p><ul><li><strong>Milk that rots in a controlled way becomes yogurt.</strong></li><li><strong>A grape that rots in a controlled way becomes wine.</strong></li></ul><p>\\\nControlled mistakes bring growth. </p><h3>MAKING NO MISTAKES IS THE WORST MISTAKE</h3><p>Some people fear doing the wrong thing, so they do nothing.</p><p>This is a reminder: </p><p>\\\n</p><ol><li><p><strong>Make the most of your mistakes.</strong> If you try once, mess up, and quit—you're the guy who failed. If you keep iterating, people will eventually recognize you as the O.G. who gets things done.</p></li><li><p> Don’t let mistakes ruin you—channel them into growth.</p></li></ol><p>The world doesn’t wait for you to catch up. Every moment spent hesitating is a moment lost to those who take action</p><h3>Final Thought: Grow or&nbsp;Die</h3><p>The stock market grows by 7–10% every year. Inflation is rising. The world moves forward no matter what. <strong>If you’re stagnant, you’re already falling behind.</strong></p><p>\\\nYou either grow—or you die. And the price of dying is much higher.</p><p>\\\nEnjoy the rest of your day.</p><p><em>Join  where I break down high-performance strategies for creators, indie hackers, and anyone tired of being stuck. No fluff. No recycled self-help nonsense. Real insights to help you <strong>outperform, outlast, and outgrow the competition.</strong></em></p><p><em>📩  (or click the subscribe button on Hackernoon and I’ll add you).</em></p>","contentLength":5170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pulumi Gestalt devlog #8","url":"https://dev.to/andrzejressel/pulumi-gestalt-devlog-8-1d1m","date":1739734216,"author":"​Andrzej Ressel","guid":753,"unread":true,"content":"<p>Welcome to the eighth devlog for . This week, the focus was on preparing native Rust support and moving toward an initial release.</p><p>Previously, Rust support in Pulumi Gestalt was essentially Wasm/Rust support, which required a complex setup and runtime environment. This week, I implemented proper native Rust support, which simplifies the process significantly. Now you can get started without external runners - only  and <a href=\"https://github.com/casey/just\" rel=\"noopener noreferrer\"></a> required.</p><p>After several weeks of development and refinement, I believe Pulumi Gestalt has reached a state where it’s ready for an initial release. Over the next few days, I’ll be focusing on finalizing documentation and ensuring consistent naming conventions across the SDKs to provide a smoother experience for users.</p><p>That’s all for this week’s updates! As always, I welcome your feedback. If you have any thoughts, suggestions, or run into issues, feel free to share them on either the <a href=\"https://github.com/andrzejressel/pulumi-gestalt\" rel=\"noopener noreferrer\">main repository</a> or the <a href=\"https://github.com/andrzejressel/pulumi-gestalt-example\" rel=\"noopener noreferrer\">example repository</a>.</p>","contentLength":959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Broadcom, TSMC reportedly exploring deals that would split up Intel","url":"https://techcrunch.com/2025/02/16/broadcom-tsmc-reportedly-exploring-deals-that-would-split-up-intel/","date":1739732563,"author":"Anthony Ha","guid":713,"unread":true,"content":"<p>Broadcom and Taiwan Semiconductor Manufacturing Company (TSMC) are separately exploring deals to take over parts of Intel, according to a report in The Wall Street Journal. Broadcom is reportedly considering an acquisition of Intel’s chip-design and marketing business, and would want a partner for the company’s manufacturing business, while TSMC is reportedly looking at controlling […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lead Scoring 2.0: From Static Models to Dynamic Buyer Intent","url":"https://hackernoon.com/lead-scoring-20-from-static-models-to-dynamic-buyer-intent?source=rss","date":1739732409,"author":"Olga Ukrainskaya","guid":810,"unread":true,"content":"<p>You have a lead scoring model in your CRM that just simply is not working?</p><p>\\\nThe harsh truth is that your lead scoring system as it stands, is probably broken. You are assigning points for job titles, for the number of times people visit your website, or open your emails, but when your sales team goes to these so-called “hot leads,” a lot of these people are not even interested. Sound familiar?</p><p>\\\nThe problem is that traditional lead scoring systems are based on the assumption that buyers go on a simple, linear journey. The reality is that today’s buyers are much more complex.</p><p>\\\nBy the time a buyer needs a solution, <strong>the decision is often already made.</strong></p><blockquote><ul><li><p>They do their homework long before you can even pronounce their name—glossing over reviews, measuring competitors, exploring industry blogs.</p></li><li><p>Their engagement is inconsistent — they may view a lot of content one week and none the next.</p></li><li><p>They may not respond to your emails, but engage with your product often — a clear sign of intent missed by static scoring models.</p></li></ul></blockquote><p>\\\nIf you're still relying on static lead scoring, you might be missing out on valuable opportunities.</p><p>\\\nThis is where  comes in - a more innovative, real-time approach that prioritizes leads based on what buyers do and intent signals.</p><p>\\\nThis guide will give you some ideas for how to create a dynamic lead-scoring model in your CRM. Then, by using AI, behavioral data, and automation, you can make sure your sales team is spending its time on the leads that are most likely to convert.</p><p>\\\nReady to get started? Let’s dive in. 🚀</p><h2>Static and Dynamic Lead Scoring: What’s the Difference?</h2><p>| Feature | ==Static Lead Scoring== | ==Dynamic Lead Scoring== |\n|----|----|----|\n|  | Demographics, firmographics, past actions | Real-time engagement, behavioral patterns, AI insights |\n|  | Manual, periodic | Continuous, automated |\n|  | Clicks, form fills | Product usage, time-sensitive actions, in-market signals |\n|  | Often misaligned | Predictive and relevant |</p><p>\\\nStatic models make fixed point assignments (i.e., “10 points for an email open”), whereas dynamic scoring updates in real-time based on actual buyer behavior, providing more relevant and actionable insights.</p><h2>Step 1: Identify Your Dynamic Lead Scoring Model</h2><p>When creating a dynamic scoring model, there are three main types of data that you need to focus on:</p><ol></ol><p>👉 CRM Setup Tip: If your CRM is missing company data, identify gaps and use enrichment tools to fill them. Next, build workflows to move this data into contacts and create baseline scores and segments against your ICP (Ideal Customer Profile).</p><ol start=\"2\"><li><p><strong>Engagement Score (How They Interact With You)</strong></p><p>\\\nVisits to your website (e.g. product or pricing pages = high intent)</p></li></ol><p>👉 CRM Setup Strategy: Use a marketing automation tool like HubSpot, Marketo, Pardot, etc., to track engagement, and automatically update scores dynamically.</p><ol start=\"3\"><li><p><strong>Intent Score (How Ready they are to Buy)</strong></p><p>\\\nEngagement on the product (especially for SaaS companies)</p><p>Competitor comparisons (activity on G2 or Capterra, etc.)</p><p>High engagement with sales materials (case studies, ROI calculators, etc.)</p><p>More web searches aligning with your product</p></li></ol><p>👉 CRM Setup Hack: Link intent data solutions, such as HubSpot Breeze, 6sense, Bombora, or others to your CRM.</p><h2>Step 2: Add Scoring Automation in Your CRM</h2><ul><li><p>3rd Party Data Enrichment: Use ZoomInfo, HubSpot Breeze, Bombora, etc. to track buyer intent outside of your own channels.</p></li><li><p>Establish Scoring Models – Fit, Engagement, and Intent scores should each have their own models.</p></li><li><p>Sales Alerts: Automatically alert SDRs when a lead’s score crosses a threshold.</p></li></ul><ul><li>Step 1: Turn on Einstein Lead Scoring – Use AI score to score leads.</li><li>Use Flow Automation: Set up rules to assess leads according to their behaviors.</li><li>Add Product Analytics: If relevant for SaaS companies, integrate Amplitude or Mixpanel, so product usage can be tracked, thus updating the scores.</li><li>Set Dynamic Segments: Organize your leads based on their behavior and auto-enter them into nurturing tracks.</li></ul><h2>Step 3: Move to Adaptive Lead Scoring</h2><p>Lead scores almost never stay static and decay over time. To keep your model effective:</p><ul><li>Decay Stale Leads: Subtract points from leads that haven’t acted in over 30 days (change based on longer sales processes).</li><li>Use Action-Weighted Points: High-intent actions (like demo requests or product searches) are worth much more than email opens.</li><li>Monitor and Optimize Monthly: Run CRM reports to ensure high-scoring leads are converting.</li></ul><h2>Step 4: Align Sales &amp; Marketing With Dynamic Scoring</h2><p>Your lead scoring model must work seamlessly with your sales team’s workflow. Here’s how:</p><ul><li>Define a “Marketing Qualified” Score (i.e., leads above 80 points trigger a follow-up).</li><li>Personalized Outreach: Sales should include recent high-intent actions in e-mails (e.g., “I noticed you read our enterprise security case); Let's chat!\").</li><li>Feedback Loop: If sales report back that high-scoring leads are not converting, revise scoring weights accordingly.</li></ul><p>If your lead scoring model is failing to deliver the results your sales team needs, it’s probably too rigid for today’s buyers.</p><p>\\\nToday’s buyers don’t take a linear path — they research on their own, engage in bursts, and decide quickly. Traditional scoring models simply can’t keep pace with that kind of activity.</p><p>\\\nChoose dynamic, intent-based lead scoring so your sales team only focuses on those most likely to convert. It moves past clunky lead scoring and instead into the realm of real-time engagement and behavioral signals.</p><p>\\\nSo, where do you start?</p><p>\\\nAssess your existing model: What’s working, and what’s not?</p><p>\\\n🔝 Target intent signals: Look to buyers with clear readiness signals.</p><p>✅ Maintain updated scores: Keep your CRM up to mark with the latest information.</p><p>✅ Always iterate: Refine the model with feedback from your sales team.</p><p>\\\nThe shift will not, of course, happen overnight, but even small tweaks can dramatically impact both conversions and pipeline velocity. A well-implemented lead scoring model can thus work for you, not against you.</p><p>\\\nIt’s time to have a look at lead scoring and get it to work for your business in a smarter way.</p><p>:::info\nWould you like to take a stab at answering some of these questions? The link for the template is&nbsp;. Interested in reading the content from all of our writing prompts? Click&nbsp;.</p>","contentLength":6361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingesting documents using .NET to build a simple Retrieval Augmented Generation (RAG) system","url":"https://dev.to/syamaner/a-simple-approach-for-ingesting-documents-using-net-for-a-simple-retrieval-augmented-generation-47e1","date":1739732112,"author":"sy","guid":728,"unread":true,"content":"<p>Here is a quick post summarising how to use .NET Semantic Kernel, Qdrant and .Net to ingest markdown documents. One of the comments <a href=\"https://dev.to/syamaner/building-a-simple-retrieval-augmented-generation-system-using-net-aspire-4pdp\">a recent post</a> related to the topic was about why using Python for ingestion instead of .NET. That was a personal preference at the time but also using .NET with Semantic Kernel to ingest documents for a simple pipeline is not necessarily any more work. </p><p>In this post, we will go through the ingestion process utilising high level libraries available to us in .NET ecosystem.</p><ul><li>.NET Semantic Kernel and related connectors for managing vector store</li><li>LangChain .NET for chunking</li><li>.NET Aspire to bring it all together using one of the Inference APIs. (Ollama on host, Ollama as container managed by ASPIRE or OpenAI)</li></ul><p>In the Python version, we can either pull the documents from a GitHub Repository or use a file generated by <a href=\"https://gitingest.com/\" rel=\"noopener noreferrer\">GitIngest UI</a>. HitIngest is an open source library allowing consumers to integrate ability to scrape public repositories from GitHub or manually downloading a file using the Web UI linked earlier.</p><p>The ingestion process in this example is straightforward and we follow the steps illustrated below. </p><p>As we are using a single file containing multiple .md and .yml files as described above, first step is to split them into filename, file content pairs. </p><p>The files are separated by headers as following:</p><div><pre><code>... content\n================================================\nFile: README.md\n================================================\n... content\n</code></pre></div><p>Given this is a throw away example, code below is just enough to demonstrate the process without much distractions.</p><div><pre><code></code></pre></div><p>Now that we have a Dictionary of file names and file content, we now need to get chinks for the file contents.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Getting embedding for the chunks\n</h3><p>We are using Semantic Kernel so this part is straightforward and will work with whichever API we chose to use. Given we have so far split the file, and got the chunks for each document, we can use the registered ITextEmbeddingGenerationService (this is driven by app and aspire configuration) to compute the embeddings using the inference approach we have configured.</p><p>We also have some custom metrics we are tracking that are visible on Aspire Dashboard as we perform ingestion.</p><div><pre><code></code></pre></div><p>Now that we have the embeddings, we need to insert them. This process involves a few steps:</p><ul><li>Mapping a .NET class to a vector store document</li><li>Ensuring the Collection exists (optionally recreated)</li><li>Using correct dimensions for the collection which depends on what embedding model we use.</li></ul><p>We can use attributes for mapping but in this demo we can use multiple embedding models and they have different dimensions for embedding vectors so using attributes would mean hardcoding these. </p><p>We can however define our  VectorStoreRecordDefinition in code so that we can at runtime chose the correct dimensions for our collection. </p><div><pre><code></code></pre></div><p>When bootstrapping we can then use our factory and register it with .NET Semantic Kernel so whenever we inject and  we will have our mappers integrated in the pipeline.</p><div><pre><code>        var options = new QdrantVectorStoreOptions\n        {\n            HasNamedVectors = true,\n            VectorStoreCollectionFactory = new QdrantCollectionFactory(embeddingModelName)\n        };\n        kernelBuilder.AddQdrantVectorStore(options: options);\n    }\n</code></pre></div><h4>\n  \n  \n  Inserting vectors to our collection\n</h4><p>Once we handle the registration and configuration, we are ready to consume  in our code and make use of it. So in our <a href=\"https://github.com/syamaner/moonbeans/blob/performance_evaluation/src/AspireRagDemo.API/Ingestion/IngestionPipeline.cs\" rel=\"noopener noreferrer\">IngestionPipeline.cs</a> we need to perform the following:</p><ul><li>Ensure collection exits:\n\n<ul><li>Create if it does not or recreate if required.</li></ul></li><li>Insert the vectors as below:\n</li></ul><div><pre><code></code></pre></div><p>In this quick post we have covered using TextSplitters from LangChain .NET, Vector Stores and Embedding models via .NET Semantic Kernel and some custom metrics captured during ingestion.</p><p>Without much code, we can get impressive results using what is available to us in .NET world and if you would like to see the results here is how to:</p><ul><li>Use  configuration in the AppHost Project.</li><li>Wait for models to one downloaded and started</li><li>Then use the <a href=\"https://github.com/syamaner/moonbeans/blob/performance_evaluation/src/AspireRagDemo.API/AspireRagDemo.API.http\" rel=\"noopener noreferrer\">src/AspireRagDemo.API/AspireRagDemo.API.http</a> and execute <code>http://localhost:5026/ingest?fileName=dotnet-docs-aspire.txt</code> call. Depending on model size and CPU, tis can take somewhere between 30 seconds to 15 minutes.</li><li>Once ingestion completed, access the UI from Aspire Dashboard and run some Aspire Related queries.</li></ul><p>In addition, feel free to explore the metrics as below:</p>","contentLength":4387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time Flows Forward or Backward At Quantum Levels, Researchers Suggest","url":"https://science.slashdot.org/story/25/02/16/1850213/time-flows-forward-or-backward-at-quantum-levels-researchers-suggest?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739731980,"author":"EditorDavid","guid":716,"unread":true,"content":"\"What if time is not as fixed as we thought?\" That's the question raised in an announcement from the University of Surrey. \n\n\"Imagine that instead of flowing in one direction — from past to future — time could flow forward or backward due to processes taking place at the quantum level.\"\n\nThis is the thought-provoking discovery made by researchers at the University of Surrey, as a new study reveals that opposing arrows of time can theoretically emerge from certain quantum systems. For centuries, scientists have puzzled over the arrow of time — the idea that time flows irreversibly from past to future. While this seems obvious in our experienced reality, the underlying laws of physics do not inherently favour a single direction. Whether time moves forward or backwards, the equations remain the same.... \n\nThis discovery provided a mathematical foundation for the idea that time-reversal symmetry still holds in open quantum systems — suggesting that time's arrow may not be as fixed as we experience it... The research offers a fresh perspective on one of the biggest mysteries in physics. Understanding the true nature of time could have profound implications for quantum mechanics, cosmology and beyond. \nThe university's announcement includes this quote from co-author Thomas Guff, a research fellow in quantum thermodynamics. \n\n\"The surprising part of this project was that even after making the standard simplifying assumption to our equations describing open quantum systems, the equations still behaved the same way whether the system was moving forwards or backwards in time. When we carefully worked through the maths, we found this behaviour had to be the case because a key part of the equation, the 'memory kernel,' is symmetrical in time.\" \n\n\nAnd their research reminds readers that \"the fundamental laws of physics in both the classical and the quantum realms do not manifest any intrinsic arrow of time. Newton's equations are time-reversal symmetric, as well as Schrödinger's equation. As a consequence, backward-in-time motion is equally possible as forward-in-time motion... Our findings are consistent with the second law of thermodynamics and emphasise the distinction between the concepts of irreversibility and time-reversal symmetry.\"","contentLength":2275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Project Translate: The Translate API (Part 2)","url":"https://dev.to/__dbrown__/project-translate-the-translate-api-part-2-2nd1","date":1739731951,"author":"Emmanuel Akolbire","guid":724,"unread":true,"content":"<p>Hey developers! 👋 In this post, we'll implement the text translation endpoint using Python, AWS Lambda, and a clean Hexagonal Architecture. Let's dive in! You can check out my <a href=\"https://github.com/DeXtreme/translate\" rel=\"noopener noreferrer\">GitHub</a> for the complete code.</p><p>We create a new project with the directory structure shown in the picture\nThen we install the dependency, namely boto3, with pip. We also make sure to create a requirements.txt file so we know which version to install when the script is packaged.</p><p>We'll be employing Hexagonal(Layered) Architecture in the design of our API. Hexagonal Architecture or Ports and Adpaters is a design pattern that aims at creating loosely coupled components. A helpful guide can be found <a href=\"https://dev.to/xoubaman/understanding-hexagonal-architecture-3gk\">here</a>. Although python is a dynamically typed language, we can still use this pattern.</p><p>We'll be using the project directory structure shown below<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyys8ctber2fmle1icagi.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyys8ctber2fmle1icagi.png\" alt=\"Project structure\" width=\"330\" height=\"385\"></a></p><h3>\n  \n  \n  The Translation Record Model\n</h3><p>Let's start with a simple but effective model to track our translations. We'll use Python's dataclasses - they're clean, efficient, and give us nice features out of the box.</p><div><pre><code></code></pre></div><p>Let's break down what each field does:</p><ul><li>: A unique identifier for each translation record</li><li>: The original text that needs translation</li><li>: The translated result</li><li>: Timestamp of when the translation was performed, automatically set to the current time</li></ul><p>You might wonder why we're using  instead of a regular class. Here's what makes dataclasses great for our use case:</p><p>Less Boilerplate: We don't need to write , , or  methods\nDefault Values: Easy handling of default values with the field function<p>\nType Hints: Built-in support for type hints, making our code more maintainable</p></p><p>Next, we'll define our ports using Python's Protocol class - a more Pythonic approach to interfaces. Let's dive in!\nWhy Protocols Over Abstract Base Classes?<p>\nBefore we jump into the code, let's understand why we're choosing Protocols:</p></p><ul><li>More Pythonic - follows duck typing principles</li><li>Structural subtyping instead of nominal subtyping</li><li>Better integration with static type checkers</li><li>No explicit inheritance required\n</li></ul><div><pre><code></code></pre></div><p>Now we define the adapters that implement the ports. The <code>DynamoDBPersistenceAdapter</code> stores the input and output in DynamoDB and return a Record object. The  translates the text with AWS Translate and returns the result.</p><div><pre><code></code></pre></div><p>Now we'll create the Lambda handler that ties everything together.\nWe'll define the  class with handles the requests to Lambda from the API Gateway. It parses the body for the required fields, translates the text, stores the input and output and returns a response</p><div><pre><code></code></pre></div><p>In order to allow Cross Origin Requests we add the Access-Control-Allow headers to the reponse object. For example, in the  method</p><div><pre><code></code></pre></div><p>In the next installment of this series, we'll dive into the code that handles file translation. Stay tuned! 🚀</p>","contentLength":2722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/arindam_1729/-1h49","date":1739730645,"author":"Arindam Majumder","guid":723,"unread":true,"content":"<h2>🤯 11 Exciting GitHub Repositories You Should Check Right Now⚡️</h2><h3>Arindam Majumder  ・ Feb 13</h3>","contentLength":97,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Novel Optimization Algorithms: From Entertainment to Military Applications","url":"https://dev.to/_hm/novel-optimization-algorithms-from-entertainment-to-military-applications-22h5","date":1739730131,"author":"Hussein Mahdi","guid":727,"unread":true,"content":"<p>My passion lies in developing military applications, driven by the field's cutting-edge technological advancements and strategic importance. The complexity and innovation inherent in defense technology continually motivate my professional growth and contributions.</p><p>In my recent research on swarm algorithms and which ones are best for the research I am seeking, recent developments in Swarm Intelligence have introduced two intriguing approaches: <strong>the Squid Game Optimizer (SGO) and Special Forces Algorithm (SFA)</strong>. These algorithms represent an evolution beyond traditional nature-inspired optimization methods, drawing inspiration from strategic competition and military operations.</p><p>The Squid Game Optimizer (SGO), inspired by the Netflix series, implements principles of strategy, competition, and cooperation in optimization problems. Its primary applications include multi-agent systems and complex decision-making scenarios, particularly in drone swarm coordination and defense system optimization.</p><p>Similarly, the Special Forces Algorithm (SFA) adapts military tactical operations and precision planning methodologies to address optimization challenges. This approach emphasizes stealth, adaptability, and coordinated operations under high-pressure conditions. SFA shows particular promise in military AI applications, real-time decision-making, and high-risk environments.</p><p>These algorithms have demonstrated significant potential in multi-agent systems, particularly in defense technology applications such as radar optimization and compression sensing. Their implementation represents a shift toward more specialized optimization approaches that combine strategic decision-making with tactical precision.</p><p>For detailed technical specifications and implementation guidelines, refer to:</p><p>Squid Game Optimizer (SGO): </p><p>What are your thoughts on applying these novel optimization approaches to current technological challenges?</p>","contentLength":1919,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Crafting natural, engaging content is now simpler than ever. With humanizer ai https://humaniser.ai/ , you can enhance your writing and turn AI-generated drafts into smooth, human-like text.","url":"https://dev.to/11january11/crafting-natural-engaging-content-is-now-simpler-than-ever-with-humanizer-ai-5039","date":1739729919,"author":"11january11","guid":726,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kindle is removing download and transfer option on Feb 26th","url":"https://old.reddit.com/r/kindle/comments/1inr9uy/fyi_amazon_is_removing_download_transfer_option/","date":1739729601,"author":"andyjohnson0","guid":751,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43070155"},{"title":"R.I.P. RAG? Gemini Flash 2.0 Might Just Have Revolutionized AI (Again) - Is Retrieval Augmented Generation Obsolete?","url":"https://dev.to/shaman_shetty/rip-rag-gemini-flash-20-might-just-have-revolutionized-ai-again-is-retrieval-augmented-e5k","date":1739729254,"author":"Shaman Shetty","guid":725,"unread":true,"content":"<p><strong>You clicked because you're in the AI trenches, right?</strong> You're wrestling with Large Language Models (LLMs), trying to make them actually useful for real-world applications. And chances are, you've heard the buzz around <strong>Retrieval Augmented Generation (RAG)</strong>. It was supposed to be the holy grail, the key to unlocking truly knowledgeable and reliable AI.</p><p>Well, buckle up, because the ground is shifting. Faster than you can say \"context window,\"  has arrived, and it's throwing a serious wrench into the RAG machine. Dare I say, it might even be… </p><p>Okay, maybe \"killing\" is dramatic. But as a writer and AI enthusiast, I’m seeing a seismic shift. And if you’re building AI applications, you need to pay attention.</p><p><strong>First, a Quick RAG Refresher (For the Uninitiated):</strong></p><p>Imagine an LLM as a brilliant but slightly forgetful savant. It knows language inside and out, but its knowledge of the world is limited to what it was trained on. **RAG **is like giving that savant a constantly updated encyclopedia.</p><ul><li><p>: When you ask a question, RAG first searches a vast external knowledge base (think documents, databases, websites).</p></li><li><p>: It then injects the relevant information it finds into the prompt it sends to the LLM.</p></li><li><p>: The LLM, now armed with fresh, context-specific knowledge, generates a more informed and accurate answer.</p></li></ul><p><strong>RAG was brilliant in theory and often effective in practice. It allowed us to:</strong></p><ul><li><strong>Overcome LLM knowledge cut-offs:</strong> Access information beyond the training data.</li><li><strong>Improve accuracy and reduce hallucinations:</strong> Ground answers in verifiable facts.</li><li><strong>Customize knowledge for specific domains:</strong> Tailor AI to niche industries and datasets.</li></ul><p><strong>So, what's the problem? Why is Gemini Flash 2.0 potentially turning RAG into yesterday's news?</strong></p><p><strong>Enter Gemini Flash 2.0: The Context King</strong></p><p>The core issue with RAG, despite its ingenuity, is its inherent complexity and overhead. It's like adding a complex plumbing system to your AI application. It works, but it’s… well, complex.</p><p>, on the other hand, takes a drastically different approach. Its game-changing feature? A MASSIVE context window.</p><p>We're talking about . Let that sink in. That's enough to feed entire books, research papers, and vast swathes of data directly into the model's prompt.</p><p>Suddenly, the need for external retrieval shrinks dramatically. Gemini Flash 2.0 can effectively become its own RAG system, internally digesting and processing huge amounts of information within a single prompt.</p><p>Here's why this is a potential  from a practical perspective:</p><ul><li><strong>Simplicity and Efficiency:</strong> Forget building complex retrieval pipelines, indexing knowledge bases, and managing data flow between systems. Gemini Flash 2.0 streamlines everything. You feed it the data, and it just… knows. This means faster development, simpler deployment, and less maintenance.</li><li> RAG solutions often require significant infrastructure to manage the knowledge base, retrieval mechanisms, and data processing. Gemini Flash 2.0, with its massive context window, potentially reduces this overhead significantly. You're paying for a powerful model, not a complex ecosystem around it.</li><li><strong>Speed and Real-time Access:</strong> RAG introduces latency. There's a delay for retrieval, processing, and augmentation before the LLM even generates the answer. Gemini Flash 2.0, with its internalized knowledge, can potentially provide faster, near real-time responses, as the relevant information is already within its processing scope.</li><li><strong>Reduced Complexity for Developers:</strong> Let's be honest, implementing and fine-tuning RAG can be a developer headache. Gemini Flash 2.0 promises to simplify AI development, allowing developers to focus on the core application logic rather than the intricate data plumbing.</li></ul><ul><li><strong>Customer Service Chatbots:</strong> Instead of RAG searching FAQs and knowledge articles, you could feed a vast, updated knowledge base directly into Gemini Flash 2.0's context window. Instant, accurate answers, no external retrieval needed.</li><li><strong>Research and Analysis Tools:</strong> Researchers could feed entire libraries of documents into Gemini Flash 2.0 and have it analyze and synthesize information in ways previously unimaginable without complex RAG setups.</li><li><strong>Content Creation and Summarization:</strong> Feed massive datasets, reports, or even books into Gemini Flash 2.0 and have it generate summaries, extract key insights, or create derivative content, all without the overhead of external retrieval.</li></ul><p><strong>Is RAG Completely Dead? Probably Not (Yet).</strong></p><p>Let's be realistic. RAG might still have a niche in specific scenarios:</p><ul><li><strong>Extremely Dynamic and Volatile Data:</strong> If your knowledge base changes constantly in real-time (think stock prices or live social media feeds), a RAG system might still be beneficial for grabbing the absolute latest information. However, even here, Gemini Flash 2.0's speed might surprise us.</li><li><strong>Highly Specialized and Segmented Knowledge:</strong> In scenarios where you need to access very specific, siloed knowledge bases with strict access controls, RAG might offer more granular control.</li><li><strong>Cost Considerations (Potentially):</strong> While Gemini Flash 2.0 promises efficiency, the cost of processing massive context windows could be a factor. For extremely low-budget, basic applications, simpler RAG implementations might still be considered.</li></ul><p><strong>But the writing is on the wall</strong>. The trend in LLMs is towards larger context windows. Gemini Flash 2.0 is just the first major player to truly unleash this potential. As context windows grow even larger, the argument for complex, external RAG systems becomes increasingly weak.</p><p>. And Gemini Flash 2.0 is leading the charge.</p><p><strong>What does this mean for you?</strong></p><ul><li>If you're currently building RAG systems, it's time to seriously evaluate Gemini Flash 2.0. Explore its capabilities and see if it can simplify your architecture and improve performance.</li><li><p>If you're just starting to explore AI applications, consider Gemini Flash 2.0 as a powerful and potentially simpler alternative to RAG-heavy approaches.</p><p>Keep an eye on the context window race. As other models follow suit, the entire AI landscape will be reshaped.</p></li></ul><p>This isn't just an incremental improvement. It feels like a paradigm shift. Gemini Flash 2.0 isn't just another LLM; it's potentially redefining how we build and deploy AI. And for RAG, it might just be the beginning of the end.</p><p><strong>What are your thoughts? Is RAG doomed? Is Gemini Flash 2.0 truly a game-changer? Let's discuss in the comments below!</strong></p><p>I hope you enjoyed reading.I definitely had a lot of fun writing this😎. </p>","contentLength":6410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A DevOps Approach to AEM Packages: Automating Creation, Configuration, and More","url":"https://hackernoon.com/a-devops-approach-to-aem-packages-automating-creation-configuration-and-more?source=rss","date":1739728803,"author":"Giuseppe Baglio","guid":809,"unread":true,"content":"<p>Adobe Experience Manager (AEM) packages are the unsung heroes of content management — powerful containers that bundle everything from code and configurations to critical content. But let’s face it: manually creating, configuring, and downloading these packages can feel like a tedious dance of clicks.</p><p>\\\nWhat if you could automate this process with a few keystrokes, ensuring consistency, speed, reliability, and less heavy lifting?</p><p>\\\nI will show you a Bash script that flips the script (pun intended!) on how AEM developers and admins work with the Package Manager API. Think about crafting packages in seconds, tailoring filters on the fly, and snagging backups with surgical precision — all before your coffee cools to that  sipping temperature. ☕</p><blockquote><p><strong><em>Before we dive in, a quick note</em></strong>: <em>meticulously detailed and unapologetically technical. We’ll dissect the script’s logic, explore AEM API intricacies, and troubleshoot edge cases. For developers eager to jump straight into the code, you can jump to the bottom of the article. But if you’re here to understand the how and why behind the automation, strap in — we’re going all the way down the rabbit hole.</em> 🕳️</p></blockquote><p>The  script automates interactions with AEM’s Package Manager API, offering a structured approach to package creation, configuration, and distribution. Designed for developers and administrators, it replaces manual workflows with a command-line-driven process that emphasizes consistency and reliability.</p><ul><li>: Checks for existing packages to avoid redundancy before initiating creation.</li><li>: Programmatically defines content paths (e.g., , ) to include in the package.</li><li>: Triggers package compilation and downloads the output to a specified directory, appending a timestamp to filenames for version control.</li><li>: Validates HTTP responses, folder paths, and authentication to provide actionable feedback during failures.</li><li>: Supports basic¹ credential-based authentication via .</li></ul><ul><li>: Reduces manual steps required for package creation, configuration, and download.</li><li>: Ensures uniform package structures and naming conventions across environments.</li><li>: Detailed logging at each stage (creation, filtering, building, downloading) aids in auditing and troubleshooting.</li></ul><h2>1.3 Practical Applications</h2><ul><li>: Integrate with cron jobs to regularly archive critical content paths.</li><li><strong>Environment Synchronization</strong>: Replicate configurations or content between AEM instances during deployments.</li><li>: Capture stable states of  or  before applying system updates.</li></ul><ul><li>Access to an AEM instance (credentials, server, port).</li><li>Basic familiarity with Bash scripting and AEM’s Package Manager.</li><li>Permission to create and download packages via the AEM API.</li></ul><pre><code>./create-remote-aem-pkg.sh admin securepass123 localhost 4502 backup-group \"Content Backup\" /backups /content/dam /etc/clientlibs\n</code></pre><p>\\\nThis command creates a package named “Content Backup” under the group , including  and , and saves the output to the  directory.</p><p>Let’s dissect the  script (you can find it at the bottom of the article) to understand how it orchestrates AEM package management. We’ll focus on its structure, key functions, and workflow logic—ideal for developers looking to customize or debug the tool.</p><ul><li>: A utility function that prefixes messages with timestamps for clear audit trails.</li></ul><pre><code>_log () {\n  echo \"[$(date +%Y.%m.%d-%H:%M:%S)] $1\"\n}\n</code></pre><p>\\\n: Ensures every action (e.g., “Package built”) is logged with context, simplifying troubleshooting.</p><ul><li>: Validates the success of prior commands by checking exit codes and API responses.</li></ul><pre><code>check_last_exec () {\n  # Checks $? (exit status) and $CURL_OUTPUT for errors\n  if [ \"$status\" -ne 0 ] || [[ $output =~ .*success\\\":false* ]]; then\n    _log \"Error detected!\";\n    exit 1;\n  fi\n}\n</code></pre><p>: Prevents silent failures by halting execution on critical errors like authentication issues or invalid paths.</p><p>The script accepts seven positional arguments followed by dynamic filters:</p><pre><code>USR=\"$1\" # AEM username\nPWD=\"$2\" # AEM password\nSVR=\"$3\" # Server host (e.g., localhost)\nPORT=\"$4\" # Port (e.g., 4502)\nPKG_GROUP=\"$5\" # Package group (e.g., \"backups\")\nPKG_NAME=\"$6\" # Package name (e.g., \"dam-backup\")\nBK_FOLDER=\"$7\" # Backup directory (e.g., \"/backups\")\nshift 7 # Remaining arguments become filters (e.g., \"/content/dam\")\n</code></pre><p>\\\nPositional arguments ensure simplicity, while  handles variable filter paths flexibly.</p><h2>2.3 Package Validation &amp; Creation</h2><ul><li>: Replaces spaces in  with underscores to avoid URL issues.</li></ul><pre><code>PKG_NAME=${PKG_NAME// /_}\n</code></pre><ul><li>: Uses  to list packages via AEM’s API, avoiding redundant creations.</li></ul><pre><code>if [ $(curl ... | grep \"$PKG_NAME.zip\" | wc -l) -eq 1 ]; then\n  _log \"Package exists—skipping creation.\"\nelse\n  curl -X POST ... # Creates the package\nfi\n</code></pre><h2>2.4 Dynamic Filter Configuration</h2><p>Constructs a JSON array of filters from input paths:</p><pre><code>FILTERS_PARAM=\"\"\nfor i in \"${!FILTERS[@]}\"; do\n  FILTERS_PARAM+=\"{\\\"root\\\": \\\"${FILTERS[$i]}\\\", \\\"rules\\\": []}\"\n  # Adds commas between entries, but not after the last\ndone\n</code></pre><p><code>[{\"root\": \"/content/dam\"}, {\"root\": \"/apps\"}]</code></p><p>This JSON is injected into the package definition via AEM’s  endpoint.</p><h2>2.5 Build &amp; Download Workflow</h2><ul><li>: Triggers compilation using AEM’s  command:</li></ul><p><code>curl -X POST … -F \"cmd=build\"</code></p><p>\\\n: The script waits for the build to complete before proceeding.</p><ul><li>: Uses  to fetch the  and save it with a timestamped filename:</li></ul><pre><code>BK_FILE=\"$PKG_NAME-$(date +%Y%m%d-%H%M%S).zip\"\ncurl -o \"$BK_FOLDER/$BK_FILE\" ...\n</code></pre><p>Robust error handling and logging are critical for unattended scripts like , ensuring failures are caught early and logged clearly. Here’s how the script safeguards against unexpected issues and provides actionable insights.</p><ul><li>: The  function prefixes every message with a  timestamp, creating an audit trail for debugging:</li></ul><p><code>_log \"Starting backup process...\" # Output: [2023.10.25-14:30:45] Starting backup process...</code></p><p>\\\n: Timestamps help correlate script activity with AEM server logs or external events (e.g., cron job schedules).</p><ul><li>: Critical steps, like package creation, filter updates, and downloads, are explicitly logged to track progress.</li></ul><h2>3.2 Error Validation Workflow</h2><ul><li>Validates the existence of the backup folder () before proceeding:</li></ul><pre><code>if [ ! -d \"$BK_FOLDER\" ]; then  \n  _log \"Backup folder '$BK_FOLDER' does not exist!\" &amp;&amp; exit 1  \nfi  \n</code></pre><ul><li>Sanitizes  to avoid URL issues (e.g., spaces replaced with underscores).</li></ul><p>\\\n:</p><p>The  function examines both shell exit codes () and AEM API responses:</p><p><code>check_last_exec \"Error message\" \"$CURL_OUTPUT\" $CURL_STATUS</code></p><ul><li>: Non-zero values (e.g.,  network failures) trigger immediate exits.</li></ul><ul><li>: Detects  JSON responses or \"HTTP ERROR\" strings in AEM output.</li></ul><p>\\\n<strong>3.3 HTTP Status Verification</strong>: When downloading the package, the script checks for a  status code:</p><pre><code>if [ \"$(curl -w \"%{http_code}\" ...)\" -eq \"200\" ]; then  \n  # Proceed if download succeeds  \nelse  \n  _log \"Error downloading the package!\" &amp;&amp; exit 1  \nfi  \n</code></pre><ul><li>Invalid credentials:  catches  responses and exits with a clear error message.</li><li>Invalid filter path: AEM API returns , the script logs \"Error adding filters\" and terminates.</li><li>Disk full: Fails to write , checks file size with  flag and alerts before exiting.</li><li>AEM instance unreachable: exits with a non-zero code, the script logs \"Error building the package\".</li></ul><h2>3.5 Security Considerations</h2><ul><li>: The script uses  for simplicity, which skips SSL verification. <em>Recommendation for Production</em>: Replace with  to specify a CA bundle.</li></ul><ul><li>: Credentials are passed as arguments, which may appear in process logs. : Use environment variables or a secrets vault (e.g., ).</li></ul><ul><li>: Temporarily add  at the script’s start to print executed commands.</li><li>: Isolate issues by running critical  commands outside the script</li><li>: Redirect script output to a file for later analysis:</li></ul><p><code>./create-remote-aem-pkg.sh ... &gt;&gt; /var/log/aem_backup.log 2&gt;&amp;1</code></p><p>The  script is designed to be a starting point—a foundation you can modify to align with your team’s needs. Below are common customizations, along with implementation guidance, to extend its functionality or adapt it to specific use cases.</p><p>The default filename uses a timestamp (<code>$PKG_NAME-$(date +%Y%m%d-%H%M%S).zip</code>). Modify this to include environment names, project IDs, or semantic versioning:</p><pre><code># Example: Include environment (e.g., \"dev\", \"prod\")  \nBK_FILE=\"${PKG_NAME}-${ENV}-$(date +%Y%m%d).zip\"  \n\n# Example: Add Git commit SHA for traceability  \nCOMMIT_SHA=$(git rev-parse --short HEAD)  \nBK_FILE=\"${PKG_NAME}-${COMMIT_SHA}.zip\"  \n</code></pre><p>: Ensure date/time formats avoid characters forbidden in filenames (e.g., colons  on Windows).</p><h2>4.2 Expanding or Modifying Filters</h2><p>The script accepts dynamic paths as filters but you can also hardcode frequently used paths or add exclusions:</p><pre><code># Hardcode essential paths (e.g., \"/var/audit\")  \nDEFAULT_FILTERS=(\"/content/dam\" \"/apps\" \"/var/audit\")  \nFILTERS=(\"${DEFAULT_FILTERS[@]}\" \"${@}\")  # Merge with command-line inputs  \n\n# Add exclusion rules (requires AEM API support)  \nFILTERS_PARAM+=\"{\\\"root\\\": \\\"${FILTERS[$i]}\\\", \\\"rules\\\": [{\\\"modifier\\\": \\\"exclude\\\", \\\"pattern\\\": \\\".*/test/*\\\"}]}\"  \n</code></pre><p>\\\n<strong>Avoid Plaintext Passwords</strong>:</p><p>Use environment variables or a secrets manager to inject credentials:</p><pre><code># Fetch password from environment variable  \nPWD=\"$AEM_PASSWORD\"  \n\n# Use AWS Secrets Manager (example)  \nPWD=$(aws secretsmanager get-secret-value --secret-id aem/prod/password --query SecretString --output text)  \n</code></pre><p>\\\n: \\n Replace&nbsp;(insecure) with a trusted CA certificate:</p><pre><code>curl --cacert /path/to/ca-bundle.crt -u \"$USR\":\"$PWD\" ...\n</code></pre><h2>4.4 Adding Post-Build Actions</h2><p>Extend the script to trigger downstream processes after a successful download:</p><pre><code># Example: Upload to cloud storage  \naws s3 cp \"$BK_FOLDER/$BK_FILE\" s3://my-backup-bucket/  \n\n# Example: Validate package integrity  \nCHECKSUM=$(sha256sum \"$BK_FOLDER/$BK_FILE\" | cut -d ' ' -f 1)  \n_log \"SHA-256 checksum: $CHECKSUM\"  \n\n# Example: Clean up old backups (retain last 7 days)  \nfind \"$BK_FOLDER\" -name \"*.zip\" -mtime +7 -exec rm {} \\;  \n</code></pre><h2>4.5 Adding Notification Alerts</h2><p>Notify teams of success/failure via Slack, email, or monitoring tools:</p><pre><code># Post to Slack on failure  \ncurl -X POST -H 'Content-type: application/json' \\  \n--data \"{\\\"text\\\":\\\"🚨 AEM backup failed: $(hostname)\\\"}\" \\  \nhttps://hooks.slack.com/services/YOUR/WEBHOOK/URL  \n\n# Send email via sendmail  \nif [ $? -ne 0 ]; then  \n  echo \"Subject: Backup Failed\" | sendmail admin@mycompany.com  \nfi  \n</code></pre><p>Managing AEM packages doesn’t have to be a manual, error-prone chore. With the  script, you can transform package creation, filtering, and distribution into a streamlined, repeatable process. This tool isn’t just about saving time, it’s about enabling consistency, reliability, and scalability in your AEM operations.</p><ol><li><p>: By eliminating repetitive GUI interactions, the script reduces human error and frees teams to focus on higher-value tasks.</p></li><li><p>: Whether backing up critical content, syncing environments, or preparing for updates, the script adapts to diverse use cases with minimal tweaking.</p></li><li><p>: Built-in logging, error checks, and security considerations ensure the script behaves predictably, even when things go sideways.</p></li></ol><p>\\\nGreat tools are born from real-world challenges. This script is a starting point; think of it as a foundation to build upon as your team’s needs grow. Whether you’re a solo developer or part of a large DevOps team, automation like this exemplifies how small investments in code can yield outsized returns in productivity and peace of mind.</p><p>\\\n<strong>Ready to take the next step?</strong></p><ul><li>🛠️ : Tailor the script using <a href=\"https://chat.deepseek.com/a/chat/s/4bad1785-c64b-479a-abb5-a39892036c6c#\">Section 6</a> as your guide.</li><li>🔍 : Review your existing AEM workflows for automation opportunities.</li><li>🤝 : Mentor your team or write a blog post about your modifications.</li></ul><p>\\\nThank you for following along — now go forth and automate! 🚀</p><pre><code>#!/bin/bash\nset -eo pipefail\n\n# The script will create a package thought the package manager api:\n# - The package is created, if not already present\n# - Package filters are populated accordingly to specified paths\n# - Package is builded\n# - Package is download to the specified folder\n\n_log () {\n  echo \"[$(date +%Y.%m.%d-%H:%M:%S)] $1\"\n}\n\ncheck_last_exec () {\n    local message=\"$1\"\n    local output=\"$2\"\n    local status=$3\n\n    if [ \"$status\" -ne 0 ]; then\n        echo &amp;&amp; echo \"$message\" &amp;&amp; echo\n        exit 1\n    fi\n\n    if [[ $output =~ .*success\\\":false* ]] || [[ $output =~ .*\"HTTP ERROR\"* ]]; then\n        _log \"$message\"\n        exit 1\n    fi\n}\n\nUSR=\"$1\"\nPWD=\"$2\"\nSVR=\"$3\"\nPORT=\"$4\"\nPKG_GROUP=\"$5\"\nPKG_NAME=\"$6\"\nBK_FOLDER=\"$7\"\n\nshift 7\n# The following paths will be included in the package\nFILTERS=($@)\nBK_FILE=$PKG_NAME\"-\"$(date +%Y%m%d-%H%M%S).zip\n\n_log \"Starting backup process...\"\necho \"AEM instance: '$SVR':'$PORT'\nAEM User: '$USR'\nPackage group: $PKG_GROUP\nPackage name: '$PKG_NAME'\nDestination folder: $BK_FOLDER\nDestination file: '$BK_FILE'\nFilter paths: \"\nprintf '\\t%s\\n\\n' \"${FILTERS[@]}\"\n\nif [ ! -d \"$BK_FOLDER\" ]; then\n  _log \"Backup folder '$BK_FOLDER' does not exist!\" &amp;&amp; echo\n  exit 1\nfi\n\nPKG_NAME=${PKG_NAME// /_}\ncheck_last_exec \"Error replacing white space chars from package name!\" \"\" $? || exit 1\n_log \"Removed whitespaces from package name: '$PKG_NAME'\"\nBK_FILE=$PKG_NAME.zip\n_log \"Backup file: '$BK_FILE'\"\n\n_log \"Creating the package...\"\nif [ $(curl -k -u \"$USR\":\"$PWD\" \"$SVR:$PORT/crx/packmgr/service.jsp?cmd=ls\" 2&gt;/dev/null | grep \"$PKG_NAME.zip\" | wc -l) -eq 1 ]; then\n  _log \" Package '$PKG_GROUP/$PKG_NAME' is already present: skipping creation.\"\nelse\n  curl -k --silent -u \"$USR\":\"$PWD\" -X POST \\\n  \"$SVR:$PORT/crx/packmgr/service/.json/etc/packages/$PKG_GROUP/$PKG_NAME?cmd=create\" \\\n  -d packageName=\"$PKG_NAME\" -d groupName=\"$PKG_GROUP\"\n\n  check_last_exec \"  Error creating the package!\" \"\" $?\n  _log \" Package created\"\nfi\n\n# create filters variable\nFILTERS_PARAM=\"\"\nARR_LEN=\"${#FILTERS[@]}\"\nfor i in \"${!FILTERS[@]}\"; do\n\n  FILTERS_PARAM=$FILTERS_PARAM\"{\\\"root\\\": \\\"${FILTERS[$i]}\\\", \\\"rules\\\": []}\"\n\n  T=$((i+1))\n  if [ $T -ne $ARR_LEN ]; then\n   FILTERS_PARAM=$FILTERS_PARAM\", \"\n  fi\ndone\n\n# add filters\n_log \"Adding filters to the package...\"\nCURL_OUTPUT=$(curl -k --silent -u \"$USR\":\"$PWD\" -X POST \"$SVR:$PORT/crx/packmgr/update.jsp\" \\\n-F path=/etc/packages/\"$PKG_GROUP\"/\"$PKG_NAME\".zip -F packageName=\"$PKG_NAME\" \\\n-F groupName=\"$PKG_GROUP\" \\\n-F filter=\"[$FILTERS_PARAM]\" \\\n-F \"_charset_=UTF-8\")\n\nCURL_STATUS=$?\n\n# Pass the status to the check_last_exec function\ncheck_last_exec \"Error adding filters to the package!\" \"$CURL_OUTPUT\" $CURL_STATUS\n_log \"  Package filters updated successfully.\"\n\n# build package\n_log \"Building the package...\"\nCURL_OUTPUT=$(curl -k -u \"$USR\":\"$PWD\" -X POST \\\n  \"$SVR:$PORT/crx/packmgr/service/script.html/etc/packages/$PKG_GROUP/$PKG_NAME.zip\" \\\n  -F \"cmd=build\")\n\ncheck_last_exec \" Error building the package!\" \"$CURL_OUTPUT\" $?\n_log \"  Package built.\"\n\n# download package\n_log \"Downloading the package...\"\nif [ \"$(curl -w \"%{http_code}\" -o \"$BK_FOLDER/$BK_FILE\" -k --silent -u \"$USR\":\"$PWD\" \"$SVR:$PORT/etc/packages/$PKG_GROUP/$PKG_NAME.zip\")\" -eq \"200\" ]; then\n  if [ -f \"$BK_FOLDER/$BK_FILE\" ] &amp;&amp; [ -s \"$BK_FOLDER/$BK_FILE\" ]; then\n    _log \"  Package $BK_FILE downloaded in $BK_FOLDER.\"\n    exit 0\n  fi\nfi\n\n_log \"  Error downloading the package!\"\nexit 1\n</code></pre><p>[¹] Skipping SSL verification with&nbsp;&nbsp;is handy for testing, but you’ll want something sturdier in production (for example&nbsp;)!</p>","contentLength":15141,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Caddy – The Ultimate Server with Automatic HTTPS","url":"https://caddyserver.com/","date":1739728575,"author":"huang_chung","guid":750,"unread":true,"content":"<div><a href=\"https://doi.org/10.1145/3319535.3363192\"><img src=\"https://caddyserver.com/resources/images/cites/aas.png\"></a><p>\n\t\t\t\t\t\t\t\t\"Servers running Caddy exhibit nearly ubiquitous HTTPS deployment and use modern TLS configurations. ... We hope to see other popular server software follow Caddy's lead.\"\n\t\t\t\t\t\t\t</p><p>\n\t\t\t\t\t\t\t\t—<b>Josh Aas, Richard Barnes, Benton Case, Zakir Durumeric, Peter Eckersley, Alan Flores-López, J. Alex Halderman, Jacob Hoffman-Andrews, James Kasten, Eric Rescorla, Seth Schoen, and Brad Warren.</b> 2019. <i>Let's Encrypt: An Automated Certificate Authority to Encrypt the Entire Web.</i> In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19). Association for Computing Machinery, New York, NY, USA, 2473–2487. <a href=\"https://doi.org/10.1145/3319535.3363192\">https://doi.org/10.1145/3319535.3363192</a></p></div><div><a href=\"https://doi.org/10.1145/2987443.2987480\"><img src=\"https://caddyserver.com/resources/images/cites/springall.png\"></a><p>\n\t\t\t\t\t\t\t\t\"No popular server software does [session ticket key rotation], with the exception of Caddy.\"\n\t\t\t\t\t\t\t</p><p>\n\t\t\t\t\t\t\t\t—<b>Drew Springall, Zakir Durumeric, and J. Alex Halderman.</b> 2016. <i>Measuring the Security Harm of TLS Crypto Shortcuts.</i> In Proceedings of the 2016 Internet Measurement Conference (IMC '16), Association for Computing Machinery, Santa Monica, California, USA, 33-47. <a href=\"https://doi.org/10.1145/2987443.2987480\">https://doi.org/10.1145/2987443.2987480</a></p></div>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43070025"},{"title":"Another LLM wrapper","url":"https://dev.to/alessiochiffi/another-llm-wrapper-44h5","date":1739728318,"author":"alessiochiffi","guid":703,"unread":true,"content":"<p>Like many of us, I have been fascinated by the capabilities of tools like ChatGPT, Google Gemini, Claude and so on. As someone who loves coding and trying new tools, I wanted to explore what these models can offer via their APIs.</p><p>I previously experimented with OpenAI so I wanted to try something different. I opted for Google Gemini AI mainly because of its generous free tier and low costs.</p><p>My aim was to create a simple proof-of-concept (POC) page to check grammatical errors by pasting or typing text into a text box and receiving feedback from the AI. I quickly set up the app with the following tech stack:</p><ul></ul><p>I then started by creating an API endpoint - using Nuxt’s server/api folder - to manage all LLM interactions server side.</p><h2>\n  \n  \n  🔌 11 lines of code to connect to the model\n</h2><div><pre><code></code></pre></div><p>After defining the model, I could send instructions to the model, along with the text provided by the user in the frontend app. </p><p>Below is a simplified version of the function that initiates a chat with the model.</p><div><pre><code></code></pre></div><p>The frontend is made by a simple form with a textbox and a 'Submit' button where the user can type or paste text.</p><p>We then send a request to the API endpoint created, which processes it with Gemini AI and returns the results back to the frontend.</p><p>On the dashboard’s right panel, the response provides a summary and suggested changes, when available</p><p>One standout feature of Google Gemini's API is its support for custom response schemas. </p><p>Using this feature, I could configure the model to indicate whether a sentence is grammatically correct and, if not, to provide a corrected version.</p><p>Here’s the schema I used:</p><div><pre><code></code></pre></div><p>For example, when evaluating the sentence: “Their going to be here soon.” the model returns:</p><div><pre><code></code></pre></div><p>This structured output makes it easy to process and display results in the frontend.</p><p>It's interesting that you can also describe each property and what you expect from the response using the description key within the schema object. </p><p>It certainly feels like writing this short post to share my journey took longer than implementing the LLM model itself.</p><p>Although my knowledge of AI is limited, I see immense potential in this technology. With much of the complexity abstracted away, we can focus on building products and exploring endless opportunities for innovation. Whether it’s a tool that simplifies our day-to-day tasks or a larger, more ambitious project, we can only embrace the possibilities this technology offers.</p><p>Cover image from Novoto Studio</p>","contentLength":2458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hugging Face Launches Free AI Agents Course with Certification!","url":"https://dev.to/fardinkai/hugging-face-launches-free-ai-agents-course-with-certification-g5a","date":1739727489,"author":"Mahmud Ahad Abedin Fardin","guid":702,"unread":true,"content":"<h2>Hugging Face Launches Free AI Agents Course with Certification!</h2>","contentLength":63,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transform Your Data Model to AI Workflow - with only 8 extra lines of code!","url":"https://dev.to/eduardknezovic/transform-your-data-model-to-ai-workflow-with-only-8-extra-lines-of-code-3f5a","date":1739727355,"author":"Eduard Knezovic","guid":699,"unread":true,"content":"<p><em>\"Good programmers worry about data structures and their relationships.\"</em> - Linus Torvalds</p><p>What if you could create complex AI workflows as easily as defining your data structures with Pydantic?</p><p>What if you could simply harness the power of AI by allowing your Pydantic data models to flow like water?</p><p><a href=\"https://pypi.org/project/modellm/\" rel=\"noopener noreferrer\">ModeLLM</a> makes this possible by turning your Pydantic models into powerful AI pipeline components.</p><p>Let's go over an example!</p><p>You will need to provide your own OPENAI_API_KEY (if you haven't already)</p><div><pre><code></code></pre></div><p>You will also need to install the  library.</p><p>All of the relevant dependencies are automatically installed\nwith the  library.</p><p>Take a look at this complete working example.</p><div><pre><code></code></pre></div><p>To consolidate your knowledge:</p><ol><li>Execute the existing code on your computer</li><li>Generate the story for teenagers (uncomment one line of code) </li><li>Create a  Pydantic model that should summarize the story</li><li>Create your own Pydantic model and inject it to the pipeline</li></ol><p>In this example, we've managed to harness the power of AI\nin (only!) 8 additional lines of code - thanks to the <a href=\"https://pypi.org/project/modellm\" rel=\"noopener noreferrer\">ModeLLM library</a></p><ol><li>: Define what you want, not how to get it (LLM is smart enough to catch the cue)</li><li>: Chain transformations with the  operator (Makes our code easy to modify and extend)</li><li>: Docstrings guide the AI's behavior</li><li>: Easily swap components </li><li>: Complex AI operations hidden behind simple data models</li></ol><p>By defining our Pydantic data models (and decorating them) we were able to execute our AI pipeline with a single line of code:</p><div><pre><code></code></pre></div><p>What do you think about this approach? I would love to hear your thoughts and suggestions.</p>","contentLength":1534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What If People Like AI-Generated Art Better?","url":"https://slashdot.org/story/25/02/15/0412246/what-if-people-like-ai-generated-art-better?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739727240,"author":"EditorDavid","guid":696,"unread":true,"content":"Christie's auction house notes that an AI-generated \"portrait\" of an 18th-century French gentleman recently sold for $432,500. (One member of the Paris-based collective behind the work says \"we found that portraits provided the best way to illustrate our point, which is that algorithms are able to emulate creativity.\") \n\nBut the blog post from Christie's goes on to acknowledge that AI researchers \"are still addressing the fundamental question of whether the images produced by their networks can be called art at all.\"\n\n. One way to do that, surely, is to conduct a kind of visual Turing test, to show the output of the algorithms to human evaluators, flesh-and-blood discriminators, and ask if they can tell the difference. \n\"Yes, we have done that,\" says Ahmed Elgammal [director of the Art and Artificial Intelligence Lab at Rutgers University in New Jersey]. \"We mixed human-generated art and art from machines, and posed questions — direct ones, such as 'Do you think this painting was produced by a machine or a human artist?' and also indirect ones such as, 'How inspiring do you find this work?'. We measured the difference in responses towards the human art and the machine art, and found that there is very little difference. Actually, some people are more inspired by the art that is done by machine.\" \nCan such a poll constitute proof that an algorithm is capable of producing indisputable works of art? Perhaps it can — if you define a work of art as an image produced by an intelligence with an aesthetic intent. But if you define art more broadly as an attempt to say something about the wider world, to express one's own sensibilities and anxieties and feelings, then AI art must fall short, because no machine mind can have that urge — and perhaps never will. \n\nThis also begs the question: who gets credit for the resulting work. The AI, or the creator of its algorithm... \n\nOr can the resulting work be considered a \"conceptual art\" collaboration — taking place between a human and an algorithm?","contentLength":2026,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hugging Face Launches Free AI Agents Course with Certification!","url":"https://dev.to/riana-azad/hugging-face-launches-free-ai-agents-course-with-certification-2h7n","date":1739726531,"author":"Riana Azad","guid":701,"unread":true,"content":"<p>Hugging Face has introduced a free , complete with a certification to help you validate your skills. Whether you're just starting or looking to advance your expertise, this course covers everything you need to build AI-powered agents.</p><p><strong>What’s Inside the Course?</strong>\nThe course is structured into five comprehensive chapters, guiding learners from setup to building and benchmarking AI agents. Here's a breakdown:</p><p>🔹  – Get set up with the necessary tools and platforms.\n🔹 <strong>Agent Fundamentals (Chapter 1)</strong> – Learn core concepts like tools, thoughts, actions, observations, LLMs, special tokens, and chat templates. Includes a hands-on Python use case.\n🔹  – Explore popular AI agent frameworks, including SmolAgents, LangGraph, and LlamaIndex.\n🔹  – Build real-world AI applications and contribute to the community.\n🔹 <strong>Final Assignment (Chapter 4)</strong> – Develop an AI agent, benchmark it, and compete on a leaderboard!</p><p><strong>Who Can Take This Course?</strong>\nThis course is open to anyone with:<p>\n✔️ Basic Python knowledge</p>\n✔️ A fundamental understanding of LLMs (Unit 1 includes a refresher)<p>\n✔️ A free Hugging Face account</p></p><p>🏅 \nHugging Face offers two certification levels:<p>\n🏆 Fundamentals Certificate – Complete Unit 1 to demonstrate a solid grasp of AI agent basics.</p>\n🏆 Full Certificate of Completion – Finish Unit 1, complete a use-case assignment, and pass the final challenge to earn this advanced certification.</p>","contentLength":1429,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-Powered Code Generation: The Future of Software Development","url":"https://dev.to/raajaryan/ai-powered-code-generation-the-future-of-software-development-2h3n","date":1739726112,"author":"Deepak Kumar","guid":700,"unread":true,"content":"<p>The software development landscape is undergoing a significant transformation with the rise of AI-powered code generation tools. Technologies like OpenAI’s Codex, GitHub Copilot, and Tabnine are reshaping how developers write, debug, and optimize code. But what does this mean for the future of programming?</p><h3>\n  \n  \n  What is AI-Powered Code Generation?\n</h3><p>AI-powered code generation refers to the use of machine learning models to assist developers in writing code. These models, trained on vast repositories of open-source code, can predict and generate code snippets, automate repetitive tasks, and even suggest complete functions based on natural language input.</p><p>AI-powered coding assistants use deep learning models, primarily transformer-based architectures like GPT (Generative Pre-trained Transformer), to understand programming contexts and generate relevant code. The process involves:</p><ul><li>: The AI analyzes the surrounding code and identifies patterns.</li><li>: It predicts and generates code snippets or full functions.</li><li>: Developers review and refine the generated code to ensure efficiency and accuracy.</li></ul><h3>\n  \n  \n  Benefits of AI in Software Development\n</h3><ol><li>: AI-powered tools automate repetitive tasks, allowing developers to focus on complex problem-solving.</li><li>: AI helps catch syntax and logic errors early, improving code quality.</li><li>: Developers can quickly generate and test code, speeding up the development cycle.</li><li>: New programmers can learn faster with AI-assisted suggestions and explanations.</li></ol><h3>\n  \n  \n  Challenges and Limitations\n</h3><p>Despite its advantages, AI-powered coding has some challenges:</p><ul><li>: AI-generated code may contain security vulnerabilities or inefficiencies.</li><li>: Developers might become too dependent on AI tools, affecting their problem-solving skills.</li><li><strong>Ethical and Copyright Issues</strong>: AI models are trained on open-source code, raising concerns about intellectual property rights.</li></ul><h3>\n  \n  \n  The Future of AI in Coding\n</h3><p>The future of AI-powered coding looks promising, with advancements expected in:</p><ul><li>: Future AI models will understand deeper project contexts and provide more intelligent suggestions.</li><li><strong>AI-Driven Debugging and Optimization</strong>: AI will not only generate code but also optimize and debug existing codebases.</li><li>: AI-driven automation will enhance CI/CD pipelines and software maintenance.</li></ul><p>AI-powered code generation is revolutionizing software development by enhancing productivity, reducing errors, and speeding up prototyping. While challenges exist, the future promises even more intelligent and efficient coding assistance. Developers should embrace AI as a tool to augment their capabilities rather than replace them.</p><p><strong>What’s your take on AI in coding? Let us know in the comments!</strong></p>","contentLength":2678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IPv6 Is Hard","url":"https://techlog.jenslink.net/posts/ipv6-is-hard/","date":1739725449,"author":"miyuru","guid":825,"unread":true,"content":"<p>Yesterday I read this <a href=\"https://chaos.social/@goetz/114001310396231712\">toot (German)</a> over on mastodon\nwhich starts with “IPv6 is hard.”</p><p><strong>No it’s not. It’s different.</strong></p><p>I ran across this multiple times: There is an A and an AAAA-record for a FQDN, but the\nweb server is only reachable via IPv4. You can easily test this with <a href=\"https://curl.se/\">curl</a></p><div><pre tabindex=\"0\"><code data-lang=\"shell\">$ curl -4  https://github.com -o /dev/null\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  273k      273k      3417k       --:--:-- --:--:-- --:--:-- 3553k\n$ curl -6  https://github.com -o /dev/null\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n   --:--:-- --:--:-- --:--:--     \ncurl: 7 Couldnt connect to server\n</code></pre></div><p>When using IPv4 273k are “saved” to /dev/null, using IPv6 we get an error message “Couldn’t connect to server”</p><p> that I’m using GitHub here because it is a well known example for not\noffering IPv6 as a time of writing, and it probably will have not IPv6 soon.</p><p>So in the case of the toot mentioned above there is an AAAA for the FQDN. But the connection doesn’t work. But the end user\ndoesn’t notice. Because there is a browser feature called <a href=\"https://datatracker.ietf.org/doc/html/rfc8305\">Happy Eyeballs (HE)</a>.\nBasically: A browser tries both protocols and uses the\nfaster on. In case of non-working IPv6 IPv4 is always faster.</p><p>HE can have some funny side effects. In a project a connection to a development web server sometimes worked and sometimes\ndidn’t. The solution was quite simple. The customer used a split VPN tunnel. IPv4 was routed via the VPN tunnel and those\nIPv4 addresses were allowed in the web servers access list. IPv6 was routed via the normal Internet connection and those\naddresses weren’t allowed.</p><p>But back to the original problem: In another toot we learn that a support ticket\nwas opened four weeks ago and that traceroute\nwas prohibited. But there is always the option to use the  option for traceroute or . This use tcp instead\nof UDP (standard traceroute) or ICMP (Windows tracert).</p><p>When you run traceroute -T for both IPv4 and IPv6 you’ll see that in this case IPv6 ends one hop earlier than IPv4. Or\nin this case the last hop loops back to itself.</p><p>This could mean that there is a missing firewall rule allowing traffic to the server or there is a routing issue on the\nfirewall.</p><p>But we can learn much more from this:</p><ul><li>If browser wouldn’t do happy eyeballs, the support hotline would have been exploded</li><li>There is no external monitoring, at least not for IPv6</li><li>There is no automation or at least no complete automation</li></ul><p>So please: If you do IPv6 take it seriously. If you don’t take it seriously,\ndon’t do IPv6. That leaves to people thinking that\nIPv6 is hard and can not be done.</p><p>Some notes: I’ll cover traceroute and co. in a future blog post and regarding\nGitHub check out this\n<a href=\"https://github.com/orgs/community/discussions/151477\">link</a>.</p>","contentLength":2898,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43069533"},{"title":"Half-Life 2 and Dishonored art lead Viktor Antonov has died","url":"https://www.eurogamer.net/half-life-2-and-dishonored-art-lead-viktor-antonov-dies-aged-just-52","date":1739725294,"author":"Trasmatta","guid":749,"unread":true,"content":"<p>Former Valve writer Marc Laidlaw revealed the news on social media, saying he had no details of Antonov's death, but he had had it confirmed that the \"visionary art lead\" had died.</p><p>\"I didn't want to say much till I felt it was confirmed, but I learned today that Viktor Antonov, our visionary art lead on HL2, has died,\" Laidlaw said.</p><p>\"I don't have details. Just sadness. Brilliant and original. Made everything better.\"</p><p>Antonov moved from Bulgaria to Paris when he was just 17 and went on to work or consult on a number of notable games, including Half-Life 2, Counter-Strike: Source, Half-Life 2: Lost Coast, Dishonored. Dishonored: Dunwall City Trials, <a data-keyword=\"true\" href=\"https://www.eurogamer.net/games/wolfenstein-the-new-order\">Wolfenstein: The New Order</a>, <a data-keyword=\"true\" href=\"https://www.eurogamer.net/fallout-4-walkthrough-and-guide-4019\">Fallout 4</a>, <a data-keyword=\"true\" href=\"https://www.eurogamer.net/games/dishonored-2\">Dishonored 2</a>, Doom, and Prey.</p>","contentLength":721,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43069514"},{"title":"AI’s Non-Determinism, Hallucinations, And... Cats?","url":"https://hackernoon.com/ais-non-determinism-hallucinations-and-cats?source=rss","date":1739725209,"author":"Alexander Simonov","guid":688,"unread":true,"content":"<p>For a long time, IT specialists worked without a care in the world. They developed, built, and deployed software smoothly. Then the era of isolation hit, and suddenly, they got bored (of course, this is a playful take on the actual events). IT folks wanted to create something that could handle their work while they stayed home: answer routine questions, generate cool avatars, and analyze vast amounts of data in minutes. They dreamed of traveling to a fantastic place, and so, you guessed it, they revolutionized AI.</p><p>\\\nAI is now functioning, providing answers, and improving lives. As skilled an assistant as it is, AI is truly effective only when used in the right context.</p><p>\\\nWe’re witnessing rapid progress in AI applications, from image and video generation to stock market forecasting and cryptocurrency analysis. Yet, <a href=\"https://www.informationweek.com/machine-learning-ai/how-can-decision-makers-trust-hallucinating-ai-\">AI may offer information we don’t ask for</a> or provide blatantly false answers. Its behavior is very much like that of household cats — you know, the kind that sits quietly and then suddenly pounces on you?</p><p>\\\nOur cats, as well as AI, enjoy being unpredictable:</p><ul><li>You give them the same food (or data) — sometimes they eat, sometimes they ignore it.</li><li>You train them to respond, but they only occasionally react when you call them.</li><li>The bigger and wilder the cat or the larger the AI model, the harder it is to predict its behavior.</li><li>In the morning, cats might be calm; by evening, they turn hyperactive (just like dynamic data).</li><li>Cats might be friendly (deterministic) but can scratch you without warning (stochastic).</li></ul><p>\\\nYou might wonder what determinism and stochasticity mean — let’s find out.</p><h2><strong>Determinism and Stochasticity</strong></h2><p>A deterministic system always produces the same result given the same input — think  if you're a <a href=\"https://devops.com/can-ai-replace-devops-engineers-2/\">DevOps engineer</a>. A real-world example would be your cat that eats the same amount of food you put in its bowl every time — this is . But when the cat sniffs and only eats half, it’s no longer deterministic.</p><p>\\\nA  process includes an element of randomness: with the same input, the result can vary. For example, a machine learning model often uses stochastic algorithms, like , which trains the model by picking random chunks of data rather than the entire dataset.</p><p>\\\nThese definitions don’t fully explain why our AIs sometimes hallucinate and behave chaotically. There are other contributing factors, including the following:</p><ul><li>Rounding errors and floating-point arithmetic</li><li>Multithreading and parallel computations</li><li>Continuously updating data</li><li>Chaos and the “butterfly effect”</li></ul><p>\\\nIf we look a little closer, we'll see other mechanisms that influence the unpredictable behavior of AI models.</p><h2><strong>A Glimpse of Neural Networks</strong></h2><p>You probably know that the <a href=\"https://www.forbes.com/councils/forbestechcouncil/2024/12/10/transforming-businesses-with-llms-risks-and-use-cases/\">AIs everyone uses</a> rely on various neural network algorithms. Here are some types of neural networks:</p><ul><li><strong>Fully Connected Neural Networks (FCNN):</strong> A classic architecture where each neuron connects to every neuron in the next layer.</li></ul><ul><li><strong>Convolutional Neural Networks (CNNs):</strong> These networks use convolutions or filters that highlight image features like edges, textures, and shapes.</li></ul><ul><li><strong>Recurrent Neural Networks (RNNs</strong>): These networks have feedback loops that allow them to remember previous steps (namely, they remember sequences).</li></ul><ul><li><strong>Long Short-Term Memory (LSTM):</strong> An enhanced version of RNNs with mechanisms for selectively forgetting and remembering important data.</li></ul><ul><li>: The most powerful class for text processing. They use multi-head attention, allowing them to consider the entire context simultaneously.</li></ul><ul><li><strong>Generative Adversarial Networks (GANs):</strong> They consist of two networks, one of which generates data and the other evaluates its quality. Their competition leads to better results.</li></ul><ul><li>: Networks designed to compress (encode) information and then reconstruct (decode) it.</li></ul><ul><li><strong>Graph Neural Networks (GNNs):</strong> They work with graphs (nodes and edges) rather than regular data.</li></ul><p>\\\nWe need all that context to understand why the most common model, ChatGPT, often hallucinates.</p><h2><strong>How AI Hallucinations Happen</strong></h2><p>ChatGPT runs on the  architecture, first introduced in <a href=\"https://research.google/pubs/attention-is-all-you-need/\">the 2017 paper, “Attention Is All You Need.”</a> &nbsp;This is the very mechanism that revolutionized text processing. Transformers operate on the self-attention mechanism, which allows them to consider the global context rather than just the nearest words like older recurrent neural networks (LSTM and GRU) do. The model belongs to the GPT (Generative Pre-Trained Transformer) series, which means:</p><ul><li> It was initially trained on enormous amounts of text (books, articles, websites, and code).</li><li> Its task is to generate text, not just classify or extract facts.</li></ul><p>\\\nChatGPT’s answers result from a stochastic process rather than a rigid rule. It doesn’t memorize or reproduce texts but generates responses using a probabilistic model.</p><h3><strong>Word Prediction as a Probabilistic Process</strong></h3><p>When ChatGPT responds, it doesn’t choose the single correct word but computes a probability distribution.</p><p>\\\n where:</p><ul><li>“wi” — the next word in the sentence</li></ul><p>w1, w2, …, wi-1 — the previous words</p><ul><li>P(wi|w1, …, wi-1) — the probability that “wi” will be the next word</li></ul><p>\\\nFor example, if you ask, “What day is it today?” ChatGPT might have different probabilities:</p><ul></ul><p>\\\nIt will mostly often choose the word with the highest probability, but due to generation temperature (a parameter that controls randomness), it might sometimes choose a less likely option based on context.</p><h3><strong>Context Influence and Information Forgetting</strong></h3><p>ChatGPT works with a limited context window, meaning it only \"remembers\" the last NN tokens. For GPT-4, the context window is about 128k tokens (around 300 pages of text). If important information is outside this context, it may:</p><ul><li>Forget details (context clipping effect)</li><li>Make-up information (stochastic process)</li></ul><p>\\\nYet, ChatGPT can often correct its answer after you ask if it's sure. However, ChatGPT can often correct its answer if you ask whether it’s sure.</p><h3><strong>AI Sometimes Corrects Itself, But Why?</strong></h3><p>When you ask ChatGPT, “Are you sure?” it reanalyzes its answer using a new context where doubt is present. This results in:</p><ul><li>Recalculating answer probabilities.</li><li>Choosing a more plausible option if one exists.</li></ul><p>\\\nThis process can be explained by Bayesian probability.</p><p>\\\n<strong>P(A|B) = P(B|A)P(A) / P(B),</strong> where:</p><ul><li><p>P(A|B) — the probability that answer A is correct, considering your follow-up question B.</p></li><li><p>P(B|A) — the probability that you would have asked if ChatGPT was initially right.</p></li><li><p>P(A) — the initial probability of ChatGPT's answer.</p></li><li><p>P(B) — the overall probability that you would ask.</p></li></ul><p>Too much information for you? Brain overheating? Imagine that AIs also get overwhelmed by large amounts of information.</p><h3><strong>Errors Due to Overfitting and Noisy Data</strong></h3><p>Massive amounts of text data flow into ChatGPT’s training, including noise or contradictory information, such as:</p><ul><li>Some sources say the Earth is round, while others claim it’s flat.</li></ul><ul><li>AI can’t always determine which information is true when it appears with varying probabilities.</li></ul><p>\\\nThese are examples of model hallucinations, which occur because ChatGPT’s weights are trained on probabilistic word associations rather than strict logic.</p><p>Here is what we can learn from this. ChatGPT hallucinates since it:</p><ul><li><p>Predicts probabilistically, not deterministically.</p></li><li><p>Has a limited memory (context window).</p></li><li><p>Recalculates probabilities when questioned.</p></li><li><p>Has training data that includes noise and contradictions.</p></li></ul><p>It’s that straightforward. Hope you didn’t get tired. If you did, that’s a good sign because it means you’re thinking critically, which is exactly what we should do when working with AI.</p>","contentLength":7521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"United States Power Outage Map","url":"https://poweroutage.us/","date":1739724326,"author":"jonbaer","guid":715,"unread":true,"content":"<div><div>Electric customerswithout power:</div></div><div><div>Electric customerswithout power:</div></div><div><div>Electric customers </div></div><div>\n            Electric customers without power:</div>","contentLength":129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43069399"},{"title":"Modeling a Neuron in micrograd (As Explained by Karpathy)","url":"https://dev.to/shrsv/modeling-a-neuron-in-micrograd-as-explained-by-karpathy-6gh","date":1739723924,"author":"Shrijith Venkatramana","guid":677,"unread":true,"content":"<p><em>Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, I’m building <a href=\"https://hexmos.com/liveapi\" rel=\"noopener noreferrer\">LiveAPI</a>, a tool that makes generating API docs from your code ridiculously easy.</em></p><p>In serious neural network implementations, we model the neuron in the following way:</p><ol><li>1 \"Influence\"  (dendrite)</li><li>Sum of \"influences\" =  (cell body)</li></ol><p>The above leads to the cell body expression:</p><ol><li>Activation function - squashing fuction (, )</li></ol><h2>\n  \n  \n  Representing the Model Neuron (defined above) in micrograd\n</h2><div><pre><code></code></pre></div><h2>\n  \n  \n  Implementing  into Value (for the Activation Function)\n</h2><p>We have the following  formula:</p><p>We can implement the function as follows:</p><div><pre><code></code></pre></div><p>We'll add a new node  which is the :</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Derivative of o - Derivative of </h2><p>The formula for derivative of  is the following:</p><p>So, we want to find out :</p><div><pre><code>do/dn = 1 - tanh(n)**2 = 1 - o**2\n</code></pre></div><h2>\n  \n  \n  Getting all the backprop values calculated (manually)\n</h2><p>We leverage some patterns we've learned previously about how backprop works with addition/multiplication, to quickly fill in the values for  in each node:</p><div><pre><code></code></pre></div>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YouTube TV reaches new deal to keep Paramount content","url":"https://techcrunch.com/2025/02/16/youtube-tv-reaches-new-deal-to-keep-paramount-content/","date":1739723651,"author":"Anthony Ha","guid":669,"unread":true,"content":"<p>After warnings that Paramount content was about to disappear from Google’s pay TV service YouTube TV, the companies announced late Saturday that they’d reached a deal that averts any disruption to channel availability. Those announcements didn’t include many specifics about the agreement, but a statement from a Paramount spokesperson said it includes “an expanded streaming […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lawsuit Accuses Meta Of Training AI On Torrented 82TB Dataset Of Pirated Books","url":"https://yro.slashdot.org/story/25/02/16/0346210/lawsuit-accuses-meta-of-training-ai-on-torrented-82tb-dataset-of-pirated-books?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739723640,"author":"EditorDavid","guid":674,"unread":true,"content":"\"Meta is involved in a class action lawsuit alleging copyright infringement, a claim the company disputes...\" writes the tech news site Hot Hardware. \n\nBut the site adds that newly unsealed court documents \"reveal that Meta allegedly used a minimum of 81.7TB of illegally torrented data sourced from shadow libraries to train its AI models.\"\n\nInternal emails further show that Meta employees expressed concerns about this practice. Some employees voiced strong ethical objections, with one noting that using content from sites like LibGen, known for distributing copyrighted material, would be unethical. A research engineer with Meta, Nikolay Bashlykov, also noted that \"torrenting from a corporate laptop doesn't feel right,\" highlighting his discomfort surrounding the practice. \n\nAdditionally, the documents suggest that these concerns, including discussions about using data from LibGen, reached CEO Mark Zuckerberg, who may have ultimately approved the activity. Furthermore, the documents showed that despite these misgivings, employees discussed using VPNs to mask Meta's IP address to create anonymity, enabling them to download and share torrented data without it being easily traced back to the company's network.","contentLength":1224,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ultimate Football Draft (A Python Terminal Game)","url":"https://dev.to/jcubilloespinoza/ultimate-football-draft-a-python-terminal-game-128i","date":1739723472,"author":"Josue Cubillo Espinoza","guid":676,"unread":true,"content":"<p>Football fans around the world dream of managing their favorite teams and experiencing the thrill of competition. This Python program allows users to choose a team, participate in a simulated tournament, and compete for victory. By randomly assigning teams to groups and generating matches, the program provides an engaging and interactive experience.</p><p>The program begins by prompting the user to enter their name and select their favorite football team from a list of international clubs. After choosing a team, the program randomly assigns teams to groups for the tournament’s group stage. The user’s selected team is placed in one of these groups.</p><p>Using the random module, the program ensures fair and unpredictable group draws and match results. The user competes against other teams by answering trivia questions. Winning matches earns points, and the top teams from each group advance to the knockout stages.</p><p>Throughout the knockout rounds, the user’s team must win to progress further. If they lose a match, they are eliminated from the competition.</p><ol><li>Random selection of football teams into tournament groups.</li><li>Interactive gameplay where users answer questions to win matches.</li><li>Randomized match results for AI-controlled teams.</li><li>Automatic generation of tournament brackets leading to the final match.</li></ol><p>This project is a great example of how Python can be used to create engaging sports simulations. Whether you are a football fan or a programming enthusiast, this program provides an enjoyable way to experience the excitement of a football tournament. Try it out and see if your team can become the ultimate champion!</p>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/ndaza/-46mm","date":1739722193,"author":"Nestor Daza","guid":678,"unread":true,"content":"<h2>Build a RAG-Enabled Helpdesk Chatbot in 10 Minutes with MongoDB</h2>","contentLength":63,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: Futures of Ethereum II - Censorship Resistance (2/16/2025)","url":"https://hackernoon.com/2-16-2025-newsletter?source=rss","date":1739721931,"author":"Noonification","guid":687,"unread":true,"content":"<p>🪐 What’s happening in tech today, February 16, 2025?</p><p>By <a href=\"https://hackernoon.com/u/hackernooncontests\">@hackernooncontests</a> [ 3 Min read ] Join the #blockchain Writing Contest and share your insights on decentralized AI, cloud, or dePIN. Win up to $2,000! Contest open Feb 5–May 7, 2025. <a href=\"https://hackernoon.com/win-up-to-$2000-in-the-blockchain-writing-contest-by-aleph-cloud-and-hackernoon\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/juancguerrero\">@juancguerrero</a> [ 3 Min read ] Tether is one of the largest holders of U.S. government debt. Its not just a stablecoin company – its becoming Americas new strategic reserve buyer. <a href=\"https://hackernoon.com/tether-may-become-americas-new-strategic-reserve-buyer\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bill-achola\">@bill-achola</a> [ 3 Min read ] Who really profits in a startup? Our deep dive into startup salaries reveals how executives secure big paydays while employees take on the risk. <a href=\"https://hackernoon.com/i-learned-the-hard-way-that-startup-high-executives-profit-while-employees-struggle\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/boxhero\">@boxhero</a> [ 13 Min read ] Discover how AI-powered sentiment analysis tools deliver accurate insights from customer reviews and feedback to help improve your business strategy. <a href=\"https://hackernoon.com/sentiment-analysis-and-ai-everything-you-need-to-know-in-2025\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mend\">@mend</a> [ 6 Min read ] Generative AI boosts efficiency but introduces security risks like shadow AI, vulnerabilities, and data leaks. Learn how AI can secure AI-driven development. <a href=\"https://hackernoon.com/generative-ais-double-edged-sword-unlocking-potential-while-mitigating-risks\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/adambakay\">@adambakay</a> [ 22 Min read ] By understanding market microstructure, you might be able to add more precision into your trading. <a href=\"https://hackernoon.com/thinking-of-pursuing-trading-full-time-then-you-need-to-know-what-market-microstructures-are\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 17 Min read ] The article explores Ethereums efforts to ensure censorship resistance, focusing on solutions like PBS and encrypted mempools amid regulatory pressures. <a href=\"https://hackernoon.com/futures-of-ethereum-ii-censorship-resistance\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/stellar\">@stellar</a> [ 5 Min read ] Regulatory shifts in 2025 will shape crypto wallets. Learn how compliance, DeFi, and Stellar’s Soroban ecosystem will impact the future of Web3 wallets. <a href=\"https://hackernoon.com/regulatory-clarity-on-wallets-will-shape-defi-in-2025\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":1788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI tries to ‘uncensor’ ChatGPT","url":"https://techcrunch.com/2025/02/16/openai-tries-to-uncensor-chatgpt/","date":1739721600,"author":"Maxwell Zeff","guid":668,"unread":true,"content":"<p>OpenAI is changing how it trains AI models to explicitly embrace “intellectual freedom … no matter how challenging or controversial a topic may be,” the company says in a new policy. As a result, ChatGPT will eventually be able to answer more questions, offer more perspectives, and reduce the number of topics the AI chatbot […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building GetFitter: The Ultimate Workout App with Jetpack Compose","url":"https://dev.to/himagaur2708/building-getfitter-the-ultimate-workout-app-with-jetpack-compose-57n0","date":1739720675,"author":"Himanshu Gaur","guid":655,"unread":true,"content":"<p>Welcome to the first installment of our development journey! Today, I kick off the creation of GetFitter, a workout app designed to cater to all your fitness needs. We're leveraging Jetpack Compose to build a sleek and user-friendly interface. In this post, we'll delve into the creation of the Home screen, the cornerstone of our app, which includes the TopAppBar, exercise categories, popular exercises, and time-specific workouts.<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F283kejk95l9swb6stv1h.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F283kejk95l9swb6stv1h.png\" alt=\"Image description\" width=\"800\" height=\"474\"></a><strong>The Vision for the Home Screen</strong>\nThe Home screen is the heart of GetFitter, designed to inspire and guide users through their fitness journey. Here's a breakdown of the key components we're implementing:</p><p>TopAppBar: The TopAppBar will offer quick access to the Information and Settings. It sets the tone for a seamless and intuitive user experience.</p><p> A scrollable row of categories allows users to explore different types of exercises, ensuring they find what best suits their goals.</p><p><strong>Popular Exercises Section:</strong> This section highlights trending workouts, helping users stay motivated and try new routines.</p><p> Divided into Morning, Mid-Day, and Evening sections, this feature helps users plan their workouts based on the time of day, ensuring they get the most out of their training.</p><p><strong>Using Jetpack Compose for a Smooth Development Experience</strong>\nJetpack Compose, Google's modern toolkit for building native Android UIs, streamlines the development process and enables us to create a beautiful, performant, and responsive app. Here's a glimpse of the code structure for our Home screen:</p><p>&lt;\n@Composable\n    Scaffold(<p>\n        topBar = { TopAppBar(title = { Text(\"GetFitter\") }) }</p>\n    ) {\n            CategoryRow()<p>\n            PopularExercisesSection()</p>\n            TimeSpecificWorkoutsSection()\n    }</p><p>@Composable\nfun CategoryRow() {\n        items(categories) { category -&gt;\n        }\n}</p><p>@Composable\nfun PopularExercisesSection() {<p>\n    // Implementation of the Popular Exercises Section</p>\n}</p><p>@Composable\nfun TimeSpecificWorkoutsSection() {<p>\n    // Implementation of the Time-Specific Workouts Section</p>\n}</p><p><strong>Creating a User-Centric Experience</strong>\nMy goal is to ensure GetFitter is not only functional but also engaging and easy to navigate. By incorporating scrollable rows and cards, I provide users with a dynamic and interactive way to explore content. Each exercise category and workout card is designed with the user in mind, making it simple to discover new routines and stay motivated.</p><p>\nThis is just the beginning of our exciting journey. As I continue to <p>\nbuild and refine GetFitter, I'll share more updates and insights into our development process. Stay tuned for the next installment, where I'll dive deeper into the specifics of each section and how I am optimizing the app for the best user experience.</p></p><p>Stay fit, stay motivated, and join us on this journey to make fitness accessible and enjoyable for everyone with GetFitter!</p>","contentLength":2828,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vim after Bram: a core maintainer on how they've kept it going","url":"https://thenewstack.io/vim-after-bram-a-core-maintainer-on-how-theyve-kept-it-going/","date":1739720645,"author":"MilnerRoute","guid":693,"unread":true,"content":"<p>Its community performed a quietly heroic effort to make sure his project stayed alive.</p><p>“What you can see is basically that the development did not stop,” Brabandt told his audience in Tokyo.</p><p>Every day there are fresh pull requests and issues to review, so “It’s still quite active. There’s a lot of activity going on on GitHub.”</p><p>And in January of 2024, they released Vim 9.1 — and dedicated it to Moolenaar.</p><h2>‘The Development Did Not Stop’</h2><p>Platform consultant <a href=\"https://github.com/chrisbra\" rel=\"external \" onclick=\"this.target='_blank';\">Christian Brabandt</a> had been active in the Vim community since 2006, contributing bug reports, fixes and a few new features. He’d worked on things like Vim’s regular expression handling and its support for encryption, as well as helping build its daily Appimage and “moving the home page around.” And then suddenly in August of 2023, “I became one of the main maintainers of Vim.”</p><p>The news of Moolenaar’s death was “quite shocking for all of us,” even though Vim’s mailing list had gone “pretty quiet” in the weeks before, and “people already started to wonder what happened with Bram? Where is he?”</p><p>“We had to decide what we were going to do.”</p><p>Brabandt first acknowledged that they “lost a lot of knowledge” — and not just Moolenaar’s test scripts.</p><p>Moolenaar started Vim 30 years ago, and he carried in his head “a lot of knowledge on the original Vim of all the features he wanted to have.” But more than that, Moolenaar was also the project’s leader. “He basically determined the strategy — where he wanted the project to go and what he wanted to be included and what he didn’t like.”</p><p>“We had to restructure and find ways to continue.”</p><p>And right from the beginning, there was one essential crisis. When it came to Vim’s GitHub account, “Bram was the owner. That means only he could make certain decisions — final decisions like setting up roles and permissions for other maintainers… We needed to have this power to continue working and invite other maintainers to the project.”</p><p>Fortunately, GitHub actually has a “<a href=\"https://docs.github.com/en/site-policy/other-site-policies/github-deceased-user-policy\" rel=\"external \" onclick=\"this.target='_blank';\">deceased user” policy</a>, including “<a href=\"https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-personal-account-on-github/managing-access-to-your-personal-repositories/maintaining-ownership-continuity-of-your-personal-accounts-repositories\" rel=\"external \" onclick=\"this.target='_blank';\">pre-designated successors</a>.” But unfortunately, Brabandt told his audience, using that policy “is not as easy as it sounds,” since after the paperwork is filled out, the GitHub account “becomes basically deactivated. Which wasn’t the best thing for us, since Bram’s family was able to access his account, and I didn’t want them to lose this ability.” Instead, Moolenaar’s family changed the permissions so that other maintainers could be invited.</p><p>Shortly after Moolenaar’s death, “Quite a few pull requests” started accumulating on GitHub, Brabandt said. “So I started going through those and importing them.” And when another long-time contributor and core maintainer, Charles Campbell, decided to retire, “I decided to invite a few more maintainers… mainly people who have been long-time contributors to Vim.”</p><p>But besides the source code, they also had to manage the project’s other infrastructure, and unfortunately, there were no documented processes, “so I had to find out all of this — how this is managed — basically, the hard way.”</p><p>And it seems like everything that could go wrong did.</p><ul><li>The site handling Vim’s vulnerability reports was <a href=\"https://www.businesswire.com/news/home/20230808746694/en/Protect-AI-Acquires-huntr-Launches-World%E2%80%99s-First-Artificial-Intelligence-and-Machine-Learning-Bug-Bounty-Platform\" rel=\"external \" onclick=\"this.target='_blank';\">acquired by an AI-security company</a> that Brabandt says “just wanted to concentrate on AI and only on AI… Open source vulnerability reporting was basically shut down almost immediately.” So the project turned to GitHub Security Advisory.</li><li>Brabandt learned the basic code of Vim’s home page hasn’t changed in 20 years. It still included PHP 7 code — though support for Php 7 <a href=\"https://www.php.net/releases/index.php\" rel=\"external \" onclick=\"this.target='_blank';\">ended in November of 2022</a>.</li><li>The service hosting their home page was acquired by Open Source China in July of 2023, and soon began serving visitors database errors, while support tickets went unanswered. So in the middle of restructuring the Vim project, the project team had to also find a new host for Vim’s home page — but, “Unfortunately, this also meant that we had to upgrade the home page from PHP 7 to at least PHP 8 support.”</li><li>The FTP server was still being run by the <a href=\"https://en.wikipedia.org/wiki/NLUUG\" rel=\"external \" onclick=\"this.target='_blank';\">Dutch Unix User Group</a>. “This was fine in the ’90s and maybe early 2000s,” Brabandt said, but “Nowadays I think people typically just download everything from GitHub or from the home page!” The Dutch Unix User Group was also reluctant to give Brabrandt access, and “It’s fine…” he said, “because we then decided to retire the old FTP server. And if a download needs to be done, it can be done via the Vim home page.”</li></ul><p>And since retiring FTP access, Brabandt says he hasn’t heard a single complaint.</p><p>It wasn’t until late 2024 that they realized the help pages still mentioned email addresses that were forwarding to Moolenaar’s old email account. “Just two weeks ago or so, I changed those, so now they have been forwarded to my address,” Brabandt told his audience in November.</p><p>Vim famously urges its users to contribute to Moolenaar’s favorite charity, the <a href=\"https://iccf-holland.org/\" rel=\"external \" onclick=\"this.target='_blank';\">International Child Care Fund Holland</a>, and Brabandt says the Moolenaar family is still maintaining <a href=\"https://www.paypal.com/donate?token=GuL3qWPYJL3FgOkjPAvH6zDTpScmwWX1L-e_6b58Oj-7yKhpaM9KeyMMGzfgTsICdLw2HDRrLssfR9sS\" rel=\"external \" onclick=\"this.target='_blank';\">Bram’s Paypal account for those donations</a> (still linked to from <a href=\"https://www.vim.org/\" rel=\"external \" onclick=\"this.target='_blank';\">Vim.org</a>). After Moolenaar passed a lot of people donated to the ICCF, with another 90,000 euros donated in 2024. Brabandt is also committed to making sure those donations go through as intended — and says he’s not planning to create any Vim sponsorships any time in the near future.</p><p>There was one change made: Bram Moolenaar’s feature which allowed ICCF donors to vote on future Vim features was shut down. It was hard to figure out which ICCF donations should be linked back to Vim.org users. (“I’m not sure how Bram did it in the past,” Brabandt says, and “the other people from the ICCF weren’t able to tell me!”) But in reality, it turns out that most of the new enhancement requests and issues are already coming from other sources like GitHub and Vim’s own to-do list.</p><p>So what does the future hold? Vim plans “a bit more potentially controversial changes” for the upcoming release of Vim 9.2, Brabandt told the audience. These include supporting the XDG specification’s <a href=\"https://www.freedesktop.org/wiki/Specifications/basedir-spec/\" rel=\"external \" onclick=\"this.target='_blank';\">base-directory specifications</a> (“The community has been wishing for it at least maybe 10 years.”) and better support for <a href=\"https://en.wikipedia.org/wiki/Wayland_(protocol)\" rel=\"external \" onclick=\"this.target='_blank';\">Wayland</a>. There are a few new options and plugins and some inevitable bug fixes.</p><p>So while changes are being made, this led Brabandt to a quietly momentous statement on the future of Vim. “However, currently I would say Vim is more or less in maintenance mode. I don’t think any of the maintainers can perform full-time work on Vim or bigger features.” As an example, he’s aware the Neovim community has been making big changes like support for parsing library <a href=\"https://tree-sitter.github.io/tree-sitter/\" rel=\"external \" onclick=\"this.target='_blank';\">Tree-sitter</a>, but adding that to Vim would take a “tremendous effort… I’m not quite sure we can achieve it, at least not in the near-term.”</p><p>But Brabandt announced another worthy goal: making sure that the community is healthy. And this means welcoming new contributors and making it easy for them to start contributing code. Brabandt has even imported some automatic code-formatting tools, since before Vim’s source code used an idiosyncratic formatting style that Brabandt called “strange. It’s basically Bram’s style of working, which is okay, but it doesn’t help new users.”</p><p>A later slide suggested things people could work on include “Tree-Sitter integration?” along with a GTK-version of Vim’s GUI interface and more advanced terminal features. Vim’s spell-checking code, for instance, “hasn’t been touched for a few years.”</p><p>“If you’re looking for big new features in the future, we do depend on the community to help us with this,” Brabandt said. But he always advises new contributors to “start small” while they’re first getting familiar with the codebase.</p><p>And for right now, “most of the changes that have been merged are relatively self-contained, small-feature sets, which can be easily tested and don’t have that much impact on other parts of the code.”</p><h2>Testing, Refactoring and Maybe Retiring That Python 2 Interface</h2><p>They’re still using “defensive and safe” C coding — Brabandt says refactoring everything into a modern programming language like Rust just isn’t an option right now. There’s a comprehensive suite of tests that he’s running over all changes — and every day they run the code-analyzing tool <a href=\"https://en.wikipedia.org/wiki/Coverity\" rel=\"external \" onclick=\"this.target='_blank';\">Coverity</a>. And going forward, they’ll refactor parts of the code “which are quite long and lengthy and complex and hard-to-understand.” (Does Vim really still need an external interface to Python 2? Since the Python community moved on to Python 3 years ago, Brabandt believes it’s an example of one of the outdated interfaces that could be retired “at some point in the future.”)</p><p>A big policy goal is making sure to continue Vim’s backward compatibility. And of course, learning from the past, Brabandt put up a slide titled “The new Vim Project — future,” which included a key “policy” bullet point: “Better documentation of (internal) processes.”</p><p>Brabandt said he came up with these policy principles while going through Moolenaar’s backlog of outstanding pull requests.</p><p>But another improvement he’d like to see is just a better understanding of Vim’s community — and he’s even considering a user survey. Toward the end of his talk, Brabandt told the audience what he’s learned since Moolenaar’s passing: that maintaining Vim is  — and that it’s a full-time job. “It’s not only about writing code; it’s about managing the community.” And that means  to that community — “Listening to their requests, fixing bugs that come up and making sure that we can keep up and do what the community wants.”</p><p>“It’s an open-source project — that means the community can contribute and should contribute and also help us steer the project into the future.”</p><p>And Brabandt said there’s already a clear signal of that <a href=\"https://thenewstack.io/open-source/\">healthy community</a>: the Vim conference itself.</p><div><svg width=\"68px\" height=\"31px\" viewBox=\"0 0 68 31\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg></div>","contentLength":10069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43068884"},{"title":"Unlock the Power of Neural Networks – From Scratch!","url":"https://dev.to/devinsights_blog_ed29ec86/unlock-the-power-of-neural-networks-from-scratch-3off","date":1739720592,"author":"DevInsights Blog","guid":652,"unread":true,"content":"<p>Have you ever wondered how machines can recognize images, translate languages, or even predict future trends? The secret lies in  – the backbone of modern AI.</p><p>Understanding how a neural network works can feel overwhelming, especially with so many complex libraries available. But what if you could actually <strong>build a neural network from scratch</strong> and understand every single step?</p><ul><li>The core concepts behind neural networks</li><li>Forward propagation, backpropagation, and loss calculation explained simply</li><li>A complete hands-on example in </li></ul><p>Building a neural network without relying on libraries like TensorFlow or PyTorch will give you  in AI. It’s like learning the fundamentals of a car engine before driving a sports car. Once you master this, using advanced tools will make far more sense.</p><p>Here’s a quick look at what you’ll be able to do:</p><div><pre><code></code></pre></div><p>This simple piece of code is part of a fully functional XOR solver you’ll build from scratch!</p><h4>\n  \n  \n  But That’s Just the Beginning...\n</h4><p>The full guide covers , from initializing weights to adjusting them through backpropagation – <strong>with clear explanations and complete working code.</strong></p><p>If you’re serious about AI and want to break free from black-box libraries, .</p><p><strong>Check it out now and start your deep learning journey today!</strong></p>","contentLength":1255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finished Auth App for Galileo! 🚀","url":"https://dev.to/khaled_abdelbar_43f8c0b1d/finished-auth-app-for-galileo-4md6","date":1739720424,"author":"Khaled Abdelbar","guid":651,"unread":true,"content":"<p>I’ve successfully finished building the authentication app for my Galileo project! 🎉</p><p>Next, I’ll focus on creating a Teams App to handle team functionalities, such as creating and joining teams. This will be a crucial step in enhancing collaboration within the project.</p><p>I’ll continue documenting each step of my journey here as I build Galileo, sharing insights and challenges I encounter. Stay tuned for updates on how I tackle the team management feature!</p>","contentLength":463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon Tests Robots For Automating Fulfillment Centers","url":"https://hardware.slashdot.org/story/25/02/16/0241242/amazon-tests-robots-for-automating-fulfillment-centers?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739720040,"author":"EditorDavid","guid":646,"unread":true,"content":"Yahoo Finance shares an interesting prediction. Amazon has an \"under-the-radar robot push\" that \"could boost its profit margins big-time, Morgan Stanley managing director Brian Nowak said.\"\n\n\nNowak said Amazon has quietly developed six significant next-generation fulfillment centers in the past three years that bring automation front and center... Amazon now has industrial robots that can increase efficiencies across the storage, inventory management, pick and packing, and sorting order fulfillment processes. \nFulfillment costs make up about 20% of Amazon's retail revenue, so he reasoned that automation could have a significant impact on long-term operating profit potential. Nowak says if 30% to 40% of Amazon's US units were fulfilled through next-generation robotics-enabled warehouses by 2030, it could lead to $10 billion-plus of savings... The investments in robots may already be paying off. Amazon's North America retail operating margins on a trailing 12-month basis have risen for five straight quarters. North America operating margins improved to 6.2% from 4.6% a year ago. \n\nNowak made the remarks on a Yahoo Finance podcast (at the top of their article) after touring one of Amazon's robot-enhanced sites in Louisiana. He believes robotics can drive down Amazon's costs compared to other retailers like Target (which he sees as lagging behind Amazon on robotics). \n\nMeanwhile workers at an Amazon facility in North Carolina held a vote Saturday on whether to unionize. But roughly 75% of the workers voted against unionization.","contentLength":1549,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Survivor's Edge The Hunger Games Experience","url":"https://dev.to/last_ride_a2626f5ed376637/survivors-edge-the-hunger-games-experience-1n9g","date":1739719821,"author":"Last Ride","guid":654,"unread":true,"content":"<p>Table of Contents\nIntroduction: The World of Hunger Game Simulators<p>\nWhat Makes a Great Hunger Game Simulator?</p>\nKey Features of Survivor's Edge: The Hunger Games Experience\nImmersive Arena Design<p>\nDynamic Survival Mechanics</p>\nCombat Systems and Strategies<p>\nReplayability and Endless Challenges</p>\nA Step-by-Step Guide to Playing Survivor's Edge: The Hunger Games Experience<p>\nMastering Survivor's Edge: Expert Strategies for Success</p>\nThe Hunger Game Simulator Community: Connecting with Players</p><ol><li>Introduction: The World of Hunger Game Simulators\nIn the world of gaming, battle royales and survival simulations have carved out their own unique space, offering players thrilling experiences in hostile environments. One such type of game that has captured the imagination of millions is the Hunger Game Simulator. These games, inspired by the intense and suspenseful nature of the popular Hunger Games franchise, challenge players to fight for survival, make strategic decisions, and outsmart their opponents.\nSurvivor's Edge: The Hunger Games Experience is one of the standout titles in this genre, delivering a refined and highly engaging Hunger Game Simulator experience. In this game, players are thrust into a deadly arena where only the strongest, smartest, and most adaptable can survive. The game offers immersive gameplay, dynamic arenas, and intricate survival mechanics that push players to their limits, making it a standout title for both new and experienced players alike.</li><li>What Makes a Great Hunger Game Simulator?\nA great Hunger Game Simulator goes beyond the basic premise of a fight to the death. While the fundamental goal remains to survive and outlast the competition, what truly sets the best games apart is their depth and complexity. A strong Hunger Game Simulator must include a variety of elements that challenge players' survival instincts, strategic thinking, and adaptability.\nKey Aspects of a Great Hunger Game Simulator\nRealistic survival elements: Managing hunger, health, stamina, and other survival factors adds realism and urgency to the game.\nDiverse environments: Multiple arenas with varying environmental conditions encourage diverse strategies and replayability.\nCharacter progression and customization: Players should have the ability to upgrade and personalize their characters, making each playthrough unique.\nDynamic gameplay: Random events, environmental hazards, and interactions with AI or other players create an ever-changing experience.\nSurvivor's Edge embodies all these characteristics, offering an exciting and multifaceted Hunger Game Simulator experience that stands out from the crowd.</li><li>Key Features of Survivor's Edge: The Hunger Games Experience\n3.1 Character Customization\nOne of the highlights of Survivor's Edge: The Hunger Games Experience is its in-depth character customization system. Players are given the opportunity to create and shape their character from the ground up, selecting not just appearance but also traits and abilities. Whether you want to play as a stealthy and nimble character or a powerhouse who excels in combat, the game allows for a wide variety of styles.\nCustomization goes beyond just visual aesthetics—it directly impacts how the character performs in the arena. For example, players can choose their character’s strength, endurance, agility, and intelligence, which affects how they interact with the environment and deal with challenges. This personalization allows for a unique Hunger Game Simulator experience each time, as players can experiment with different builds and strategies.\n3.2 Immersive Arena Design\nThe design of the arena in Survivor's Edge is a critical aspect that sets the game apart. Rather than offering a single, static arena, the game features multiple dynamic environments, each with its own challenges and opportunities. The arenas are beautifully crafted and designed to evoke a sense of urgency and danger, with different zones providing varying degrees of cover, obstacles, and hazards.\nFor instance, some arenas might be lush forests filled with resources and hiding spots, while others could be desolate wastelands where survival is more about scrounging for what little you can find. The changing weather, day-night cycles, and natural disasters like fires or floods make each arena feel alive and unpredictable, forcing players to adapt on the fly.\nThis level of immersion enhances the Hunger Game Simulator experience, making the environment an active participant in the battle for survival.\n3.3 Dynamic Survival Mechanics\nSurvival isn’t just about outlasting your opponents in Survivor’s Edge—it’s about thriving in the harsh conditions of the arena. The game introduces dynamic survival mechanics that require players to think strategically about their actions. Players need to manage essential resources like food, water, and medical supplies to keep their character alive and at their peak performance.\nThe mechanics are realistic: players must search for resources, hunt animals, gather plants, or scavenge abandoned buildings to find what they need. The stress of keeping track of your hunger, hydration, and stamina adds an extra layer of difficulty to the game, making each decision critical. Do you risk your life searching for more supplies or do you take a chance and confront another player? These tough decisions are what keep players on the edge of their seats.\n3.4 Combat Systems and Strategies\nCombat in Survivor’s Edge is fast-paced, intense, and highly strategic. The game features a wide array of weapons and tools, from melee items like knives and axes to ranged weapons such as bows and firearms. However, it’s not just about brute force. The game emphasizes smart combat—using the environment to your advantage, ambushing opponents, and managing your stamina during fights.\nPlayers can also craft makeshift weapons from available resources, creating even more opportunities for tactical gameplay. The combat system rewards strategy over mindless aggression, encouraging players to plan their attacks carefully. You’ll need to decide whether to engage in direct combat or use stealth to avoid detection and stay hidden. Every encounter is a chance to outsmart your opponent and claim victory.\n3.5 Replayability and Endless Challenges\nOne of the main draws of Survivor’s Edge is its replayability. No two matches are the same, thanks to the procedurally generated arenas, random events, and diverse character builds. The Hunger Game Simulator aspect of the game means that every match offers new challenges, forcing players to constantly evolve their strategies.\nWhether you’re trying out a new character with a different set of skills or tackling a new arena, the game’s depth ensures that you’ll never run out of challenges. Additionally, the game’s difficulty scales as you progress, ensuring that each victory feels earned and every defeat is a lesson.</li><li>A Step-by-Step Guide to Playing Survivor's Edge: The Hunger Games Experience\n4.1 Step 1: Create Your Character\nThe first step in Survivor’s Edge is creating your character. Choose your appearance, set your skills, and decide on the type of survivor you want to be. Make sure to think about how your character’s skills will impact your strategy in the arena. Will you focus on stealth, combat, or survival skills? Your choices here will set the stage for your entire gameplay experience.\n4.2 Step 2: Select an Arena\nAfter creating your character, it’s time to choose your arena. Each arena offers different strategic advantages, so carefully consider your strengths and weaknesses. Some arenas may be more suited to players who prefer ranged combat, while others might benefit stealthy, resourceful survivors. Select an arena that aligns with your chosen playstyle.\n4.3 Step 3: Enter the Arena\nOnce you’re in the arena, the game begins. Your primary goal is to gather resources, avoid dangers, and stay alive. Use the environment to your advantage—whether that means finding shelter, setting traps, or ambushing enemies. Keep an eye on your hunger, thirst, and stamina levels while preparing for combat when necessary.\n4.4 Step 4: Survive and Fight\nAs the game progresses, you’ll face other players or AI-controlled enemies. Use your weapons, skills, and the environment to engage in combat strategically. Always be mindful of your surroundings and adjust your tactics based on the changing conditions of the arena.\n4.5 Step 5: Claim Victory\nThe ultimate goal is to be the last one standing. If you can outlast all opponents, whether through combat or sheer survival, you’ll emerge as the victor. Every match is an opportunity to refine your strategies and become better at surviving in the Hunger Game Simulator environment.</li><li>Mastering Survivor's Edge: Expert Strategies for Success\n5.1 Understand Your Environment\nTake the time to learn the layout of each arena. Some areas may offer better cover for stealth, while others provide high ground for ranged attacks. Understanding the environment gives you a tactical advantage, helping you anticipate hazards and opportunities.\n5.2 Manage Resources Wisely\nEfficiently managing your resources is key to survival. Don’t waste food or water unnecessarily, and keep an eye on your stamina. Running low on resources can lead to dangerous situations, so always plan ahead.\n5.3 Combat with Precision\nRather than charging headfirst into combat, take time to observe your enemies. Look for openings, use the environment to your advantage, and strike when you’re sure you have the upper hand. Avoid unnecessary risks that might leave you vulnerable.\n5.4 Adapt to Changing Conditions\nBe ready for anything. The arena may change unexpectedly, with new hazards or events throwing off your strategy. Stay flexible and adapt to the changing conditions—those who can roll with the punches are the ones who survive.</li><li>The Hunger Game Simulator Community: Connecting with Players\nSurvivor’s Edge has a strong and active community of players who share tips, strategies, and experiences. By connecting with other fans of the Hunger Game Simulator genre, you can learn new strategies, discover hidden secrets, and become a better player. Whether through online forums, social media groups, or fan websites, engaging with the community enhances your gaming experience.</li><li>Conclusion\nSurvivor’s Edge: The Hunger Games Experience is a deep, thrilling, and highly engaging  that offers hours of entertainment. With its immersive environments, dynamic gameplay, and endless challenges, it provides players with an exciting and constantly evolving experience. Whether you're a casual player or a hardcore survival enthusiast, this game delivers on every front, pushing your skills to the limit. So step into the arena, fight for survival, and see if you have what it takes to be the last one standing.</li></ol>","contentLength":10850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Phishing Attacks: How Hackers Trick You into Giving Up Your Data","url":"https://dev.to/nightmare-lynx/phishing-attacks-how-hackers-trick-you-into-giving-up-your-data-52g0","date":1739719519,"author":"Your Nightmare","guid":653,"unread":true,"content":"<h2>\n  \n  \n  Phishing: A Deceptive Cyberattack Technique\n</h2><p>Phishing is a cyberattack technique in which an attacker uses fraudulent emails, text messages, phone calls, or websites to trick their target into revealing sensitive information. This stolen data can then be misused against the victim. Sensitive information may include social privacy details, personal information, and even banking credentials.</p><p>Phishing attacks fall under the category of social engineering, where the attacker does not need to directly hack a server or system. Instead, they exploit human error, psychology, pressure tactics, and manipulation skills to deceive their targets.</p><p>Here Is The Framework To Understand Even More Quickly!<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw2l5ulze309wgl70x9n1.jpg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fw2l5ulze309wgl70x9n1.jpg\" alt=\"Image description\" width=\"800\" height=\"417\"></a></p><h2>\n  \n  \n  Why phishing is a major cyberthreat\n</h2><p>Phishing is popular among cybercriminals and highly effective. According to IBM's Cost of a Data Breach report, phishing is the most common data breach vector, accounting for 15% of all breaches. Breaches caused by phishing cost organizations an average of USD 4.88 million.</p><p>Phishing is a significant threat because it exploits people rather than technological vulnerabilities. Attackers don't need to breach systems directly or outsmart cybersecurity tools. They can trick people who have authorized access to their target—be it money, sensitive information or something else—into doing their dirty work.</p><p>Phishers can be lone scammers or sophisticated criminal gangs. They can use phishing for many malicious ends, including identity theft, credit card fraud, monetary theft, extortion, account takeovers, espionage and more.</p>","contentLength":1575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flea-Scope: $18 Source Available USB Oscilloscope, Logic Analyzer and More [pdf]","url":"https://rtestardi.github.io/usbte/flea-scope.pdf","date":1739718511,"author":"burgerone","guid":692,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43068585"},{"title":"Firefox User Manages Experimental Browser Port To GTK4 Toolkit","url":"https://www.phoronix.com/news/Firefox-User-Ports-GTK4","date":1739718000,"author":"Michael Larabel","guid":633,"unread":true,"content":"<article>For four years there has been an open bug report for Mozilla Firefox requesting the browser's GTK widget support be updated for GTK4. An independent user/developer has taken it into his own hands and has managed to get Firefox using the GTK4 toolkit up and running on Linux...</article>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digest #160: Heroku to AWS, GPU Twists, AMI Attacks, FinOps Tools and Solving Crimes with SQL","url":"https://www.devopsbulletin.com/p/digest-160-heroku-to-aws-gpu-twists","date":1739716709,"author":"Mohamed Labouardy","guid":628,"unread":true,"content":"<p><strong>Welcome to this week’s edition of the DevOps Bulletin!</strong></p><p>We start with a big move—from Heroku to AWS—and even a surprising twist about GPUs. There’s also news on a new attack that uses AWS AMI names, ideas on keeping AI safe, and a fun debate about trailing commas in SQL.</p><p>Next, our featured podcast shows how finance and engineering work hand in hand to manage cloud costs, using tools like AWS CUR and CloudWatch. </p><p>If you’re looking for hands-on advice, our tutorials are packed with clear guides—from Terraform production tips and using BigQuery to easy guides for Azure Kubernetes, understanding unit economics, and more.</p><p>We also highlight open-source devtools like a tool to spot Nginx issues, a fun game that uses SQL to solve mysteries, a smart tool for managing config files, and even a Rust-based ping tool that tracks network speed.</p><p>All this and more in this week’s DevOps Bulletin—don’t miss out!</p><p>This podcast explores how finance and engineering teams collaborate in FinOps to manage cloud costs. Success comes from aligning goals, sharing insights, and using AWS CUR and CloudWatch.</p><ul><li><p> is a tool that analyzes Nginx configuration to prevent security misconfiguration and automate flaw detection.</p></li><li><p> is a game where you solve crimes with SQL queries and uncover evidence through data.</p></li><li><p> is a lightweight configuration management tool that updates local config files using key/value stores like etcd or Consul.</p></li><li><p> provides automated cost optimization for AWS and GCP. It includes a GCP Organization Recommender for cost-saving insights and an AWS Resource Cleanup tool to remove unused resources.</p></li></ul><ul><li><p> is a Rust-based Ping tool using the ICMP protocol, offering real-time latency tracking, visual charts, and concurrent pinging of multiple addresses.</p></li></ul><div><p>If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":1893,"flags":null,"enclosureUrl":"https://substackcdn.com/image/youtube/w_728,c_limit/_cjuQlc62uc","enclosureMime":"","commentsUrl":null},{"title":"Open source LLMs hit Europe’s digital sovereignty roadmap","url":"https://techcrunch.com/2025/02/16/open-source-llms-hit-europes-digital-sovereignty-roadmap/","date":1739716200,"author":"Paul Sawers","guid":667,"unread":true,"content":"<p>Large language models (LLMs) landed on Europe’s digital sovereignty agenda with a bang last week, as news emerged of a new program to develop a series of “truly” open source LLMs covering all European Union languages. This includes the current 24 official EU languages, as well as languages for countries currently negotiating for entry to […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":414,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python User Group Dhaka: Our Journey Begins – Event Recap","url":"https://dev.to/tamalchowdhury/python-user-group-dhaka-our-journey-begins-event-recap-2he7","date":1739715829,"author":"Tamal Anwar Chowdhury","guid":635,"unread":true,"content":"<p>On a cool February evening in Dhaka, a group of passionate Python enthusiasts gathered for something special—our city’s first-ever Python User Group meetup. What started as an idea a few weeks earlier turned into a diverse community event, laying the foundation for Python Dhaka’s journey.</p><p>Here’s how we made it happen and what we learned along the way.</p><p>The whole event was organized through our Facebook group. In Bangladesh, Facebook is the primary hub for tech enthusiasts to form communities and organize events. Unlike platforms like Meetup or Discord, Facebook groups often serve as the starting point for grassroots tech movements here. I launched a Facebook group for Python Dhaka a few weeks prior, and thanks to my personal network, it grew to 250 members within two weeks.</p><p>The next logical step was to organize an in-person meetup, because I want this group to be all about connecting with the local Python enthusiasts. After thinking about it for a few days, I locked in a date and venue for the first meetup.</p><p>I choose the 15th of February because it's the mid-month, and a Saturday. In Bangladesh, Fridays and Saturdays are official holidays. Friday is the most popular day for events, followed by Saturdays. I picked Chandrima Udyan for two reasons: 1. It has easy access to the bus and metro route; 2. It's a public park we can access for free.</p><p>I created the event on Facebook and started spreading the word on the FB group, my profile, Twitter, Linkedin, and my Instagram handle. I even created a short video announcing the event and posted it on all of my socials.</p><p>At 5 PM local time, I waited near the park entrance, feeling a mix of excitement and nerves. Soon, one by one, attendees started arriving, their calls guiding me to them. Here’s me waiting:</p><p>As we entered the park, a cool evening breeze greeted us—washing away the stress of city life and setting the perfect mood for our first meetup.</p><p>I started the meeting by sharing the Python Software Foundation's missions with the attendees, and how it relates to us:</p><ul><li><p><strong>To grow a diverse and international community of Python programmers:</strong> Organizing the Python Dhaka community is helping this mission.</p></li><li><p><strong>Encourage knowledge sharing, collaboration, and support devs of all backgrounds:</strong> Students from three universities joined this event. They were able to share knowledge and build future collaboration opportunities. This community is open to all levels of developer experience.</p></li><li><p><strong>Grow a diverse and welcoming community; support underrepresented communities in tech:</strong> One participant came from a non-computer science background, eager to transition into tech. This is exactly why Python Dhaka exists—to welcome everyone, regardless of their starting point.</p></li></ul><p>I then shared how I started evangelizing Python.</p><p>I am a professional JavaScript developer with React and NextJS expertise. I use JS for my daily work. My cousin wanted to learn Python, so I got into learning it. I found Python to be easy to learn and easy to teach. I was looking for a Python community in Dhaka, but there was none. That's why I decided to organize the Python User Group Dhaka.</p><p>I also mentioned that I don't own Python Dhaka. I am only organizing it for the time being. As this community grows, and if in the future I have to move cities, I will pass the torch to the next person to continue organizing our community activities.</p><p>We are not strictly Python fanatics. We would love to collaborate with our friends in the JavaScript, PHP, and Kotlin communities too from time to time.</p><p>We heard personal stories from the attendees, how they got into programming, and how they are using Python. Two students are doing competitive programming with C, C++ and thinking of switching to Python for CP and ML.</p><p>Two of the attendees came from different districts just so they could attend this meetup.&nbsp;</p><p><strong>One attendee traveled over 80 kilometers from Tangail just to be here. That level of dedication reminded me why this community matters.</strong></p><p>I shared many tech tips, and also informed them about the free GitHub Student Developer pack and how to avail it. We also discussed getting real-world experiences by putting an app out in the world.</p><p>We also announced the Campus Ambassador Program for Python Dhaka and nominated Abdullah to be the ambassador at Southeast University. A campus ambassador is a person in your college/university who will promote and evangelize Python programming language on your campus.</p><p>As the meetup was about to end, it was getting dark when we clicked this group photo.</p><p>Our journey is just beginning. We plan to host monthly meetups around the 15th of each month, but our ambition goes beyond that. We want to nurture the next generation of Python developers in Bangladesh, and one day, bring PyCon Bangladesh to life.</p><p>If you’re as passionate about Python as we are—whether you’re in Dhaka or anywhere in the world—we’d love your support. Find <a href=\"https://www.facebook.com/groups/pythondhaka\" rel=\"noopener noreferrer\">Python User Group Dhaka on Facebook</a>, or connect with me on Linkedin, X, and Instagram. Let's build this community together!</p>","contentLength":5017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mobile-First Approach for FastAPI Full-Stack Template Authentication: Migrating to phone_number/OTP","url":"https://dev.to/justjayzee/mobile-first-approach-for-fastapi-full-stack-template-authentication-migrating-to-phonenumberotp-m02","date":1739715631,"author":"Javad Zarezadeh","guid":634,"unread":true,"content":"<p>As you may know,  is one of the most admired frameworks for developing RESTful APIs. Another fantastic project by the same author, <a href=\"https://dev.to/tiangolo\">@tiangolo</a>, is the <a href=\"https://github.com/fastapi/full-stack-fastapi-template\" rel=\"noopener noreferrer\">Full Stack FastAPI Template</a>, which I previously wrote about <a href=\"https://dev.to/justjayzee/why-fastapi-full-stack-template-is-my-go-to-for-modern-web-development-1f2e\">here</a>.</p><p>In this post, I'll guide you through the process of replacing the email/password authentication flow in the template with a phone_number/OTP-based system. This approach is ideal for mobile-first applications and offers a user-friendly, secure way to authenticate users. My goal is to make minimal changes to the original project while maintaining its adherence to  and . Let’s dive in! 😉</p><h2>\n  \n  \n  1. Replace  and  with  and </h2><ul><li>Update the  value to a phone number, e.g.,  or .</li><li>Remove  as it is no longer necessary.</li></ul><p>Replace all instances of the  field with .</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>⚠️  This change requires updating the database schema using Alembic migrations.</p><p><code>./backend/app/api/routes/private.py</code>\nReplace all occurrences of  and  with  and .</p><p><code>./backend/app/api/routes/login.py</code> call:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>⚠️  Keep  and  unchanged due to OAuth2 standards.</p><p><code>./backend/app/api/routes/users.py</code></p><ul><li>Replace  with .</li><li>Remove the  in the  function related to email validation.</li></ul><ul><li>Replace  references with .</li><li>Remove  as it is no longer necessary.</li></ul><p> ./backend/app/crud.py  </p><ul><li>Replace all  and  references with  and .\n</li><li>Rename  to  and update all references to this function.</li></ul><h2>\n  \n  \n  2. Add an API Endpoint to Request OTP\n</h2><p><code>./backend/app/api/routes/login.py</code><p>\nAdd the following endpoint:</p></p><div><pre><code></code></pre></div><h2>\n  \n  \n  3. Nullify OTP After Login\n</h2><p><code>./backend/app/api/routes/login.py</code>\nIn the  function, nullify the OTP after successful login:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  4. Remove Unnecessary Functions\n</h2><p><code>./backend/app/api/routes/login.py</code><p>\nRemove the following functions:</p></p><ul><li><code>recover_password_html_content</code></li></ul><h2>\n  \n  \n  5. Remove Unnecessary Email Features\n</h2><p><code>./backend/app/api/routes/users.py</code><p>\nRemove the email-related logic, such as:</p></p><div><pre><code></code></pre></div><h2>\n  \n  \n  6. Remove Password Update and User Registration Functions\n</h2><p><code>./backend/app/api/routes/users.py</code></p><p>Remove the following functions:  </p><ul></ul><p>Since we are now using OTP-based authentication, these functions are redundant.  </p><p>Update all  fields to .  </p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3><strong>Remove OTP from :</strong></h3><p>For the  model, you don’t need to include the  field. Update it as follows:</p><div><pre><code></code></pre></div><p>This simplifies the creation process since OTP will be generated later during login.  </p><p>Add an  field to both the  and  models.  </p><div><pre><code></code></pre></div><h3><strong>Remove Unnecessary Models:</strong></h3><p>Delete models that are no longer needed, including:  </p><ul></ul><p>This cleanup ensures the models remain relevant to the new authentication system.  </p><ul><li><code>generate_reset_password_email</code></li><li><code>generate_new_account_email</code></li><li><code>generate_password_reset_token</code></li><li><code>verify_password_reset_token</code></li></ul><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Following these steps will transform the Full Stack FastAPI Template’s email/password flow into a phone_number/OTP-based system while keeping it aligned with best practices and standards. Happy coding! 🚀 </p><p>These changes to the  original project are available in my <a href=\"https://github.com/javadzarezadeh/phone-otp-auth-fastapi\" rel=\"noopener noreferrer\">GitHub</a>. It is important to use this project cautiously, since I have not yet had time to write the tests.</p>","contentLength":2908,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Willie Hobbs Moore: STEM Trailblazer","url":"https://spectrum.ieee.org/willie-hobbs-moore-profile","date":1739714403,"author":"Willie D. Jones","guid":606,"unread":true,"content":"<p>The first Black woman with a science Ph.D paved the way for underrepresented groups</p>","contentLength":83,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjQ4OTk0Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2NzAzOTE1Mn0.lUm5Wa5yQcpIZ0xdV8JvdAdFK4rGGfnMkR79x8IeWRs/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Evaluate your LLM! Ok, but what's next? 🤔","url":"https://dev.to/louis-dupont/evaluate-your-llm-ok-but-whats-next-3mk3","date":1739712180,"author":"Louis Dupont","guid":620,"unread":true,"content":"<p><strong>Everyone say you need to Evaluate your LLM. You just did it. Now what? 🤷‍♂️</strong></p><p>You got a score. Great. Now, here’s the trap:  </p><ul><li> ()\n</li><li> (<em>\"Tweak some stuff and re-run!\"</em>)\n</li></ul><h2><strong>Step 1: Stop staring at numbers.</strong></h2><p>Numbers feel scientific, but </p><p>Before doing anything, look at actual examples. </p><ul><li>Bad output? </li><li>Good output but bad score? </li><li>Both wrong? <strong>You’ve got bigger problems.</strong></li></ul><h2><strong>Step 2: Solve the right problem.</strong></h2><p>If your , tweak:  </p><ul></ul><p>If your , rethink:  </p><ul></ul><h2><strong>Step 3: Iterate like a maniac.</strong></h2><p>Change something → Run eval → Learn → Repeat.  </p><p><strong>Chasing numbers isn’t progress.</strong> Chasing the right insights is.</p>","contentLength":577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing User Experience with Voice User Interface Implementation","url":"https://dev.to/sista-ai/enhancing-user-experience-with-voice-user-interface-implementation-1749","date":1739711758,"author":"Sista AI","guid":619,"unread":true,"content":"<p>Implementing a Voice User Interface (VUI) on a website is a dynamic process that requires attention to detail and cutting-edge technology. By incorporating voice commands for common queries like pricing and product details, users can seamlessly interact with the interface, enhancing their overall experience.</p><h2>Best Practices for Seamless Integration</h2><p>Progressive enhancement is essential in voice UI design, ensuring users are comfortable with interactions. Performance optimization is critical for real-time responses, and user experience consideration like feedback and context-awareness elevate the VUI's usability.</p><h2>User-Centric Design Principles</h2><p>Understanding user needs and simplifying interactions are core to effective VUI design. Clear feedback, accessibility features, and graceful error handling contribute to a seamless user experience.</p><h2>Technical Implementation and Challenges</h2><p>Frontend components, backend services, and integration layers must work in harmony for a functional VUI. Significant resource investment is required, but the benefits of enhanced user experience and engagement justify the effort.</p><h2>Evolution of VUI Technology</h2><p>VUI design is constantly evolving, pushing boundaries in AI capabilities and design standards. Continuous learning and adaptation are crucial for staying on the cutting edge of interactive technology.</p><p>Visit <a href=\"https://smart.sista.ai/?utm_source=sista_blog&amp;utm_medium=blog_post&amp;utm_campaign=Enhancing%20User%20Experience%20with%20Voice%20User%20Interface%20Implementation\" rel=\"noopener noreferrer\">Sista AI</a> to explore how their AI Voice Assistant transforms user interactions seamlessly.</p><a href=\"https://smart.sista.ai?utm_source=sista_blog_devto&amp;utm_medium=blog_post&amp;utm_campaign=big_logo\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fvuic-assets.s3.us-west-1.amazonaws.com%2Fsista-make-auto-gen-blog-assets%2Fsista_ai.png\" alt=\"Sista AI Logo\" width=\"640\" height=\"170\"></a><a href=\"https://smart.sista.ai?utm_source=sista_blog_devto&amp;utm_medium=blog_post&amp;utm_campaign=product_btn_screenshot\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fsmart.sista.ai%2Fimages%2Fsista%2Fsista-button.png\" alt=\"AI ChatBot\" width=\"800\" height=\"360\"></a><a href=\"https://smart.sista.ai?utm_source=sista_blog_devto&amp;utm_medium=blog_post&amp;utm_campaign=product_admin_screenshot\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fsmart.sista.ai%2Fimages%2Fsista%2Fsista-admin-dark.png\" alt=\"AI Integration Platform\" width=\"800\" height=\"507\"></a>","contentLength":1432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kickstart Your AI Journey with This Free Course! 🌍","url":"https://dev.to/hrudu/kickstart-your-ai-journey-with-this-free-course-3o3n","date":1739711685,"author":"Hrudu Shibu","guid":618,"unread":true,"content":"<p>How to Learn Generative AI with Microsoft (No Experience Needed!)\nArtificial Intelligence is no longer the future—it’s the present! Learning AI skills is becoming essential for students, developers, and professionals alike. With Microsoft’s Explore AI Learn Plan, you can:</p><p>✔ Learn how generative AI works in a simple and practical way.\n✔ Experiment with Microsoft Copilot to create AI-generated content.<p>\n✔ Complete a real-world AI project in under an hour.</p></p><p>Who Is This For?\n👨‍🎓 Students who want to explore AI in a fun way.<p>\n👩‍💻 Developers interested in applying AI tools.</p>\n📢 Content creators looking for AI-powered creativity.</p><p>Start your AI journey today—no prerequisites required! 🚀</p>","contentLength":715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hands-On AI Learning with Microsoft Copilot! 🎨","url":"https://dev.to/hrudu/hands-on-ai-learning-with-microsoft-copilot-3ilb","date":1739711617,"author":"Hrudu Shibu","guid":617,"unread":true,"content":"<p>Ever Thought About Using AI for Creativity? Try This Free Course!\nAI isn’t just about coding—it’s about creativity! Microsoft’s Explore AI Learn Plan allows you to use AI to design your dream destination in an interactive and engaging way.</p><p>Why Should You Join?\n✨ Practical Learning – Experiment with Microsoft Copilot to generate content.<p>\n✨ Hands-on Project – Apply AI skills to create a fictional travel destination.</p>\n✨ Quick &amp; Free – Learn in just 53 minutes, with no cost!</p><p>What You’ll Gain:\n📌 New AI skills to use in real-world applications.<p>\n📌 A certificate of completion to showcase on your resume.</p>\n📌 Experience working with generative AI tools.</p><p>Let’s build something creative with AI! 🚀</p>","contentLength":723,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Learning for Everyone! 🚀","url":"https://dev.to/hrudu/ai-learning-for-everyone-2mgc","date":1739711504,"author":"Hrudu Shibu","guid":616,"unread":true,"content":"<p>Learn Generative AI with Microsoft Copilot – Free AI Course\nAI is revolutionizing the world, and now you can learn generative AI for free with Microsoft Copilot through the Explore AI Learn Plan! 🚀</p><p>What Will You Learn?\nIn this short 53-minute learning path, you will:<p>\n✅ Understand the basics of generative AI.</p>\n✅ Use Microsoft Copilot to research and create AI-generated content.<p>\n✅ Design your own dream destination using AI-powered tools.</p></p><p>Who Should Join?\n💡 Students who want to explore AI applications.<p>\n💡 Tech enthusiasts looking to boost their AI skills.</p>\n💡 Educators who want to introduce AI to their students.</p><p>No prior experience required—just a passion for learning! 🌟</p>","contentLength":694,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomInvert in PyTorch","url":"https://dev.to/hyperkai/randominvert-in-pytorch-5eb0","date":1739711271,"author":"Super Kai (Kazuya Ito)","guid":615,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is inverted or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Getting Buy-In: Overcoming Larman's Law • Allen Holub • YOW! 2024","url":"https://www.youtube.com/watch?v=XwhF_xbVpIQ","date":1739710846,"author":"GOTO Conferences","guid":614,"unread":true,"content":"<article>This presentation was recorded at YOW! Australia 2024. #GOTOcon #YOW\nhttps://yowcon.com\n\nAllen Holub - Helps You Build Software Better &amp; Build Better Software; Author of 10 Books &amp; Hundreds of Articles @AllenHolub \n\nRESOURCES\nhttps://bsky.app/profile/allenholub.bsky.social\nhttps://mstdn.social/@allenholub\nhttps://twitter.com/allenholub\nhttps://holub.com\nhttps://www.linkedin.com/in/allenholub\n\nLinks\nhttps://miro.com/app/board/uXjVL6ZfQvA=/\nhttps://www.craiglarman.com/wiki/index.php?title=Larman%27s_Laws_of_Organizational_Behavior\n\nABSTRACT\nLarman's Law states, \"Organizations are implicitly optimized to avoid changing the status quo…\" The third law is particularly relevant: 'Any change initiative will be derided as “purist,” \"theoretical,\" “revolutionary,\" \" “religion,” and “needing pragmatic customization for local concerns”—which deflects from addressing weaknesses and the manager/specialist status quo.'\n\nYou need to overcome that resistance to use innovative approaches like no-estimation or mob/ensemble programming.\n\nDevs, however, almost always go about that the wrong way. Senior management is rarely interested in the mechanics of what you're doing. They're interested in the bottom line, how the new approaches change how the business is managed, and whether the inevitable disruption is worth it.\n\nIn this talk, you'll learn how to convince the “suits” (and your clients) that innovation is a good thing. You'll learn how to create a business case for innovation that overcomes the resistance of Larman's Law. [...]\n\nTIMECODES\n00:00 Intro\n01:48 Larman's Laws of organizational behavior\n12:36 Bottom up\n20:20 Top down\n24:42 Business case\n36:10 Value stream mapping\n39:12 Mob programming\n53:00 Summary\n53:57 Outro\n\nRead the full abstract here:\nhttps://yowcon.com/brisbane-2024/sessions/3470\n\nRECOMMENDED BOOKS\nAllen Holub • Holub on Patterns • https://amzn.to/3Q1hwTX\nAllen Holub • Compiler design in C • https://amzn.to/4b0vXS9\nAllen Holub • Enough Rope to Shoot Yourself in the Foot • https://amzn.to/4hFIL2m\n\nhttps://bsky.app/profile/gotocon.com\nhttps://twitter.com/GOTOcon\nhttps://www.linkedin.com/company/goto-\nhttps://www.instagram.com/goto_con\nhttps://www.facebook.com/GOTOConferences\n#LarmansLaw #CraigLarman #EnsembleProgramming #TechLead #TechLeadership #PsychologicalSafety #SkunkWorks #ValueStreamMapping #AllenHolub #YOWcon\n\nCHANNEL MEMBERSHIP BONUS\nJoin this channel to get early access to videos &amp; other perks:\nhttps://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join\n\nLooking for a unique learning experience?\nAttend the next GOTO conference near you! Get your ticket at https://gotopia.tech\nSign up for updates and specials at https://gotopia.tech/newsletter\n\nSUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.\nhttps://www.youtube.com/user/GotoConferences/?sub_confirmation=1</article>","contentLength":2859,"flags":null,"enclosureUrl":"https://www.youtube.com/v/XwhF_xbVpIQ?version=3","enclosureMime":"","commentsUrl":null},{"title":"Building a URL Shortener in Go","url":"https://dev.to/luthfisauqi17/building-a-url-shortener-in-go-592d","date":1739709801,"author":"luthfisauqi17","guid":601,"unread":true,"content":"<p>Have you ever wondered how  or  work? Today, we're building our URL shortener in Golang!</p><p>By the end of this tutorial, you'll have a fully working URL shortener that generates short links and redirects users. Let’s get started!</p><p>Before we dive into coding, let's understand how a URL shortener works:</p><ol><li>The user enters a long URL</li><li>Save it in a memory or database</li><li>When someone visits the short link, we redirect them</li></ol><p>First, create a new project and initialize Go modules.</p><div><pre><code>go-url-shortener go-url-shortener\ngo mod init github.com/yourusername/go-url-shortener\ngo get github.com/gin-gonic/gin\n</code></pre></div><p>Now, open  and set up a simple Gin server.</p><div><pre><code></code></pre></div><p>This creates a basic Gin server. Now let’s add URL shortening!</p><h2>\n  \n  \n  Step 2: Generate Short URLs\n</h2><p>Now, we need a function to generate a short random URL.</p><div><pre><code></code></pre></div><p>Next, let’s create the /shorten endpoint that takes a long URL and returns a short one.</p><div><pre><code></code></pre></div><p>This stores the original URL in a map and returns a short URL.\nNow, let’s handle redirection!</p><h2>\n  \n  \n  Step 4: Redirect Short URLs\n</h2><p>We need an endpoint that looks up the short URL and redirects users.</p><div><pre><code></code></pre></div><p>Let’s test this API using cURL!\nRun the application by typing.</p><div><pre><code>curl -X POST http://localhost:8080/shorten -H \"Content-Type: application/json\" -d '{\"original_url\": \"https://google.com\"}'\n</code></pre></div><div><pre><code>{\n    \"short_url\": \"http://localhost:8080/abc123\"\n}\n</code></pre></div><h3>\n  \n  \n  Redirect (Visit the short URL)\n</h3><div><pre><code>curl -v http://localhost:8080/abc123\n</code></pre></div><p>There you go, that is how you build a URL Shortener using Golang. Thank you for reading, and have a nice day!</p>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A better pkg.go.dev","url":"https://dev.to/jacktt/a-better-pkggodev-hip","date":1739709306,"author":"JackTT","guid":579,"unread":true,"content":"<div><p>I have never been able to read a package's documentation on pkg.go.dev since all the code there is in black &amp; white...\n<a rel=\"noopener noreferrer\" href=\"https://github.com/huantt/better-pkg-go-dev./docs/before.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fgithub.com%2Fhuantt%2Fbetter-pkg-go-dev.%2Fdocs%2Fbefore.png\" alt=\"original.png\"></a></p><p>This is the reason I created this repository to make it possible to read.</p><p>Every time you want to read a package's documentation, you just need to replace <a href=\"https://pkg.go.dev\" rel=\"nofollow noopener noreferrer\">pkg.go.dev</a> with <a href=\"https://pkgo.dev\" rel=\"nofollow noopener noreferrer\">pkgo.dev</a>. .</p><p>We use Nginx as proxy server.</p><p>Every time a request come in, we forward it to the <a href=\"https://pkg.go.dev\" rel=\"nofollow noopener noreferrer\">pkg.go.dev</a> server, then append the <a href=\"https://highlightjs.org\" rel=\"nofollow noopener noreferrer\">highlight.js</a> script to the response before sending it back to the client.</p><p>Since some README files do not specify the code language, we also update  tags to <code>&lt;code class=\"language-go\"&gt;</code> to enable syntax highlighting.</p><p>If you don't trust me, no worry, you can deploy it yourself:</p><ul><li>Append <code>127.0.0.1       pkg.go.dev.local</code> to </li><li>Run <code>docker-compose up -d --build</code></li><li>Access </li></ul></div>","contentLength":768,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alleged 'CEO Shooter' Luigi Mangione Raises $297K Online","url":"https://yro.slashdot.org/story/25/02/16/0040252/alleged-ceo-shooter-luigi-mangione-raises-297k-online?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739709240,"author":"EditorDavid","guid":539,"unread":true,"content":"Luigi Mangione faces first-degree murder charges. On Valentine's Day he posted his first public comments online, reports People magazine, with Mangione saying he's grateful for the support he's receiving:\n\nEarlier this week, Mangione accepted $297,000 in donations to cover his legal bills from the December 4 Legal Committee, which is stewarding a fundraiser on GiveSendGo for his legal defense, according to a post shared by the group. (A source with knowledge of the situation confirmed to PEOPLE that the post was genuine.) The committee — named after the death date of Thompson — said the donations were offered by over 10,000 individual supporters. Mangione's lead attorney, Karen Friedman Agnifilo, told the committee that Mangione \"very much appreciates the outpouring of support....\" \n\nHis federal murder charges could land him the death penalty, and he also faces state murder charges that accuse him of committing an act of terrorism.","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The DeepSeek Revolution: The AI Game Changer You Need to Know About","url":"https://dev.to/nitdgplug/the-deepseek-revolution-the-ai-game-changer-you-need-to-know-about-1f57","date":1739708468,"author":"Ayush Bhartia","guid":590,"unread":true,"content":"<p>Hey everyone! The AI landscape is constantly evolving, with new models pushing the boundaries of what's possible. But one name has been making waves recently, i.e, . Unlike other AI tools, this powerhouse has rewritten the rulebook on efficiency, cost-effectiveness, and performance. If you haven't heard of it yet, here’s why it’s capturing global attention.</p><h3><strong>What Is DeepSeek and Why Is It Gaining Popularity?</strong></h3><p>In January 2025, DeepSeek, a Chinese AI company, emerged as a major player in the AI space. Developed by a Chinese AI lab, it rapidly climbed the app store rankings, even surpassing ChatGPT as the most popular free AI assistant in the US.\nBut what’s driving the hype? DeepSeek-R1 isn’t just another AI model—it’s an example of cutting-edge efficiency at a fraction of the usual cost. Unlike its competitors, which require massive investments,  was trained for just —a game-changing breakthrough in AI development.</p><h3><strong>Innovation Behind DeepSeek</strong></h3><p>DeepSeek’s strength lies in its  training method, which splits its AI into specialized sub-models. Instead of a single large neural network handling everything, each part of DeepSeek is optimized for specific tasks, making it both  and more .\nAnother key advantage is its , which adjusts processing power based on task complexity. This means DeepSeek can deliver high performance while using significantly fewer resources compared to other models.</p><p>Moreover, instead of relying on Nvidia’s high-end H100 chips, DeepSeek utilizes the more affordable , making AI training and deployment  cheaper than models like </p><h3><strong>DeepSeek-R1 helping open-source community</strong></h3><p>DeepSeek released its flagship model, DeepSeek-R1, under the . This model, developed with remarkable resource efficiency—using approximately 2,000 Nvidia H800 GPUs over 55 days demonstrated performance on par with leading AI models from established tech giants. DeepSeek has made significant strides in the artificial intelligence landscape by open-sourcing its advanced AI models. \nThe open-source release of DeepSeek-R1 has lowered barriers to AI development, fostering innovation and competition across the global AI community. The company has also released models like , designed to enhance code intelligence, and , aimed at advancing vision-language understanding. These models are available for public use and modification, promoting transparency and collaboration in AI research and application.</p><h3><strong>How DeepSeek Outperforms Other AI Models</strong></h3><p>Beyond its efficiency, DeepSeek is redefining AI reliability with its R1 Reasoning Model. Unlike traditional models, the probability of the R1 Reasoning Model giving appealing outputs is much higher as compared to other available models since this model is designed to validate its own reasoning supporting its output, improving accuracy in subjects like math, science, and fact-checking.\nThis makes DeepSeek particularly powerful for research, problem-solving, and advanced computations, offering a level of precision that many AI models like  from OpenAI,  from MetaAI, and  from Google, etc struggle to achieve.</p><h3><strong>Challenges and Limitations</strong></h3><p>Despite its impressive capabilities, DeepSeek isn’t without challenges. Its business model remains uncertain, as it offers services at a significantly lower cost than competitors, raising questions about long-term sustainability. If integrated into government or corporate infrastructure, DeepSeek AI could be exploited for surveillance by foreign adversaries, affecting nation's cybersecurity. Although being open-source, certain components of the model (e.g., training datasets and fine-tuned versions) might be controlled by external entities, leading to potential data leaks.\nAdditionally, being developed in China, DeepSeek follows strict content regulations. Topics such as <strong>Taiwan’s political status</strong> or the <strong>Tiananmen Square incident</strong> are restricted due to Chinese internet policies, which may affect its global adaptability.</p><h3><strong>What’s Next for DeepSeek and AI?</strong></h3><p>DeepSeek’s rise proves that bigger isn’t always better. With its cost-effective approach, rapid deployment, and self-reasoning abilities, it’s challenging industry giants and setting a new standard for AI development.\nWhether you’re a developer, researcher, or AI enthusiast, DeepSeek is worth watching. Have you tried it yet? Let us know your thoughts in the comments!<p>\nWe hope you found this insightful.</p></p><p><em>Also, do not forget to like and comment.</em></p><p>Until then,<strong>Keep exploring and May the Source Be With You!</strong></p>","contentLength":4476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"You Should Be Harder on Yourself, Despite What Self Care Gurus Say","url":"https://hackernoon.com/you-should-be-harder-on-yourself-despite-what-self-care-gurus-say?source=rss","date":1739708085,"author":"Scott D. Clary","guid":686,"unread":true,"content":"<p>We're obsessed with being gentle with ourselves.</p><p>\\\nEvery self-help guru and Instagram therapist preaches the same message: be kind to yourself, practice self-compassion, and celebrate small wins.</p><p>\\\nAnd sure, there's value in not being overly critical. Nobody wants to spiral into self-hatred or paralysis.</p><p>\\\nBut what if I told you that being harder on yourself is actually the key to freedom?</p><blockquote><p>Here's what most people miss:&nbsp;<strong>the most successful people in any field are simultaneously confident in their abilities and brutally honest about their shortcomings.</strong></p></blockquote><p>\\\nThink about that for a second.</p><p>\\\nCan you imagine how freeing it feels to already be successful and still know, deep down, that you're nowhere near your potential?</p><p>\\\nThat's not depressing. That's exciting.</p><p>\\\nBecause when you're genuinely successful, you no longer need to protect your ego. You don't need to defend your current position. You can look at yourself with crystal clear vision and think \"<em>I'm good, but I could be so much better.</em>\"</p><p>\\\nMost people never experience this freedom because they're trapped in a prison of their own making.</p><p>\\\nThey're stuck defending their current level of achievement. Protecting their ego. Making excuses for why they haven't progressed further.</p><p>\\\n<strong>Mediocrity isn't a position - it's a mindset.</strong></p><p>\\\nAnd the mindset comes from a deep fear that if we acknowledge our shortcomings, we'll feel worse about ourselves. So we settle into comfortable patterns, telling ourselves we're \"doing our best\" when we know we're not.</p><p>\\\nBut here's the thing about potential: it's like a muscle. The more you push against it, the more it grows. The moment you stop pushing, it starts to atrophy.</p><p>The bodybuilding community understands this better than anyone, because they can't hide from the mirror.</p><p>\\\nThere's a saying in bodybuilding circles (pre-competition):&nbsp;<em>you're always fatter than you think you are.</em></p><p>\\\nIt sounds harsh. Maybe even toxic to outsiders. But there's profound wisdom here that goes way beyond physique. It's about seeing yourself with uncompromising clarity.</p><p>\\\n<strong>Just like you can't argue with your reflection, you can't argue with reality.</strong></p><p>\\\nWhen you're standing on stage at 5% body fat, and striations showing everywhere, most people would call you shredded. But the elite?</p><p>They're analyzing every minor flaw. Every slight imbalance. Every area that could be just a bit tighter. Not because they hate themselves – but because they respect their potential enough to be honest about it.</p><p>\\\nThis isn't body dysmorphia - it's the relentless pursuit of excellence. The moment you stop defending your current level is the moment you start transcending it.</p><p>\\\n<strong>Just like every rep builds muscle, every honest assessment builds excellence.</strong></p><p>\\\nYour workouts become more focused because you stop pretending that \"feeling tired\" is a valid excuse. Your nutrition gets dialed in because you stop lying to yourself about \"just one cheat meal.\" Your recovery becomes sacred because you finally admit how much sleep you really need. You're never satisfied with \"good enough\" because you've tasted what lies beyond it.</p><p>Take this same uncompromising vision and apply it to business.</p><p>\\\n<strong>Just like a bodybuilder can't argue with the mirror, an entrepreneur can't argue with the market.</strong></p><p>\\\nEvery project you undertake will take three times longer than you think. Every marketing campaign will require more iterations than you planned. Every product launch will need more refinement than you expected. And pretending otherwise isn't optimism – it's delusion.</p><p>\\\nThe entrepreneurs who make it aren't the ones who nail everything on the first try. They're the ones who expect it to be hard and keep pushing anyway. They're the ones who look at their first prototype the way a bodybuilder looks in the mirror: with appreciation for progress, but crystal-clear vision about what needs to change.</p><p>\\\nThink about that startup you've been planning. You probably have a timeline in your head. Double it. Then double it again. Not because you're going to work slower, but because doing things&nbsp;&nbsp;takes time. Because you're finally being honest about how much refinement excellence requires.</p><p>\\\n<strong>Just like every failed rep makes you stronger, every setback makes you smarter.</strong></p><p>\\\nThis isn't pessimism - it's strategic realism. When you embrace how hard the journey will be, you stop getting discouraged by setbacks.</p><p>\\\n<em>They're not setbacks anymore. They're the process.</em></p><p>At the end of the day,&nbsp;<strong>you're not as good as you think you are.</strong></p><p>\\\nYour communication skills? Not as refined as you believe.</p><p>\\\nYour work ethic? Probably not as strong as you tell yourself.</p><p>\\\nYour talents? Still mostly untapped.</p><p>\\\nBut here's where it gets interesting. This reality check isn't meant to discourage you - it's meant to&nbsp;&nbsp;you. Because if you're not as good as you think you are, that means there's so much more room to grow.</p><p>\\\n<strong>The gap between where you are and where you could be is your opportunity.</strong></p><p>The moment you stop defending your current level, something magical happens. The energy you were using to maintain your self-image gets redirected into actual improvement.</p><p>\\\n<strong>Think about how much mental energy you spend:</strong></p><ul><li>Justifying your current position</li><li>Making excuses for missed opportunities</li><li>Comparing yourself to others who are \"doing worse\"</li></ul><p>\\\nWhat if you took all that energy and poured it into getting better instead?</p><p>\\\n<strong>True freedom comes when you stop needing to be \"good enough\" and start embracing being not good enough&nbsp;.</strong></p><p>\\\nIt's like taking off a heavy backpack you didn't even know you were carrying. The weight of maintaining your self-image drops away, replaced by the lightness of pure potential.</p><p>Being harder on yourself doesn't mean beating yourself up at 2 AM about past mistakes. It means looking at your day and asking \"<em>Where did I let myself off easy?</em>\"</p><p>\\\nWhen you finished that project early? Maybe you could have added another layer of polish.</p><p>\\\nWhen you hit your sales target? You probably could have made ten more calls.</p><p>\\\nWhen you felt tired at the gym? There were probably two more reps in you.</p><p>\\\n<strong>The key is to separate your worth from your performance.</strong>&nbsp;You're not a bad person for having room to improve. You're a human with unlimited potential who's choosing to see reality clearly.</p><p>\\\nIt's not about punishment - it's about&nbsp;.</p><p>\\\nThe most successful people I know aren't walking around in a cloud of self-hatred. They're energized by their potential. They're excited by how far they still have to go.</p><p>\\\nBecause here's the truth:&nbsp;<strong>the moment you think you've \"made it\" is the moment you start declining.</strong></p><p>\\\nBut when you embrace being harder on yourself, every day becomes an opportunity. Every mistake becomes data. Every setback becomes feedback.</p><p>\\\nAnd suddenly, you're not trapped by your current capabilities. You're not defensive about your flaws. You're just getting started.</p><p>\\\nNot the freedom to be comfortable, but the freedom to grow without limit.</p><p>\\\nThe freedom to look in the mirror and say \"I'm not good enough yet - and that's exactly why I'm going to win.\"</p>","contentLength":6976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GNOME 48 Beta Released With HDR Bits, gdctl, Adwaita Fonts Default & More","url":"https://www.phoronix.com/news/GNOME-48-Beta-Released","date":1739707647,"author":"Michael Larabel","guid":541,"unread":true,"content":"<article>The GNOME 48 Beta release was officially announced this morning as the latest stepping stone toward the official GNOME 48 desktop release due out in mid-March...</article>","contentLength":161,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intel Killer E5000 Ethernet Support For Linux 6.15","url":"https://www.phoronix.com/news/Intel-Killer-E5000-Linux-6.15","date":1739706300,"author":"Michael Larabel","guid":536,"unread":true,"content":"<article>The upcoming Linux 6.15 kernel cycle will be adding support for Intel Killer E5000 Ethernet...</article>","contentLength":94,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding Flow: Escaping digital distractions through deep work and slow living","url":"https://www.ssp.sh/blog/finding-flow/","date":1739706024,"author":"articsputnik","guid":673,"unread":true,"content":"<p>Johann Hari says in <a href=\"https://www.goodreads.com/book/show/57933306-stolen-focus\" target=\"_blank\" rel=\"noopener noreffer\">Stolen Focus</a> that rats and pigeons can be manipulated as we want. Just give them food whenever they do what you want them to. And shortly after, they will repeat that over and over again.</p><p>This made me think. In times when Instagram and other apps give us likes, hearts, and views on things we post, how much does big tech influence our behavior?</p><p>Aren’t they the same as the researcher, feeding us with dopamine to tell us to do what they want? Are they doing the same as the researcher who feeds the rats or pigeons?</p><p>This question, and recent improvements and tinkering with my flow as I started working for myself, made me ask how we can control the addiction and the influence and find a better way to slow living.</p><p>Johann continues that people who don’t follow the above dopamine flow are artists and painters.</p><p>Instead of sharing or selling them immediately and getting rewarded, they draw the next one. The drawing, the way to the end, is the reward, not the likes or selling the art. After hundreds of interviews, <a href=\"https://en.m.wikipedia.org/wiki/Mihaly_Csikszentmihalyi\" target=\"_blank\" rel=\"noopener noreffer\">Mihaly Csikszentmihalyi</a> said all artists mentioned something like a Flow State.  that they just tried to be there.</p><p>This is also what I find most rewarding and experienced as an author: writing in the flow state, as I’m writing these words right now when I am merely in control, but the words flow out of my fingers. I guide them, but I do not write or choose the words.</p><p>This flow and the outcome of what the article looks like after are just magical.</p><p>I have rediscovered the effects of the environment lately when working for myself when I had to find productivity in my everyday work without a boss telling me what to do. At home, I wasn’t always focused enough. I had all the distractions. Going to the library usually produces my best work.  Focused without distractions. But always going there, sometimes it’s open late, or closed. Another great way to write and be creative is to use coffee shops, specifically <strong>moving to the next every 1.5 to 2.5 hours</strong>. It’s the sweet spot for me between being creative and getting distracted. And interestingly enough, the same for <a href=\"https://www.youtube.com/cgpgrey\" target=\"_blank\" rel=\"noopener noreffer\">CGP Grey</a> in his <a href=\"https://share.snipd.com/episode/6d03cf6a-1744-412d-a158-57aef1097f5b\" target=\"_blank\" rel=\"noopener noreffer\">Redundant Office</a> podcast, he said the same.</p><p>Music, especially non-vocal music on repeat or dedicated <a href=\"https://www.ssp.sh/brain/music-that-keeps-you-focused/\" target=\"_blank\" rel=\"noopener noreffer\">Music that keeps you focused</a>, helps you stay in the flow longer. It’s another way of signaling an environment to the brain, like “Now is work time”. I also use a dedicated browser; for example, I use Zen browser for work and Brave for everything else. Zen, the work browser, has no extensions. No password manager to automatically log in and no history to quickly choose the latest website distraction. These little obstacles make it harder to distract, whereas my Brave has all the convenience at one tab away.</p><p>Another huge benefit of my writing work is that all my files are offline in my <a href=\"https://www.ssp.sh/brain/obsidian\" target=\"_blank\" rel=\"noopener noreffer\">Obsidian</a> Vault. No need to go online or log in to a web app to get going. It’s just there to start writing, brainstorming, and getting into the flow. Another way to stay in the flow is to use Markdown files and <a href=\"https://www.ssp.sh/brain/vim-language-and-motions/\" target=\"_blank\" rel=\"noopener noreffer\">Vim motions</a>. The Markdown files are simple text files that can be easily accessed and moved around. The key here is that the text’s formatting allows me to move and restructure entire chapters or ideas without friction. Adding the vim motions, I can do everything in the comfort of my keyboard. As I use them daily, they are muscle memory, and I can write at the speed of thought.</p><div><div><div><a href=\"https://brain.ssp.sh\" target=\"_blank\" rel=\"noopener noreffer\">My second brain</a>, offline notes in my Obsidian vault, allows me to create connections, observe the graph, and see connected or relevant notes that could influence my thinking on the topic I am writing. A note on a book I wrote 10 years ago, with some insight that didn’t touch much back then, but connected to the current thought, with more knowledge, can make all the difference.</div></div></div><p>One crucial ingredient that is mostly overlooked is breaks. Small drinking breaks and walking around as observed with the 1.5-2.5h sweet spot. But much more, long periods off, vacations where you don’t think of work or related. Your brain has time to wander. Good sleep to a certain extent is also a form of break. The more we learn about sleep (see <a href=\"https://www.goodreads.com/work/quotes/55587034-why-we-sleep-unlocking-the-power-of-sleep-and-dreams\" target=\"_blank\" rel=\"noopener noreffer\">Why We Sleep by Matthew Walker</a>), the more prevalent it is how important it is. I also notice to myself. If I sleep, the chances of being productive in the morning, achieving my goals early, and getting a sense of calm throughout the rest of my day are much higher. As we process primarily during sleep, especially with the <a href=\"https://en.wikipedia.org/wiki/Rapid_eye_movement_sleep\" target=\"_blank\" rel=\"noopener noreffer\">REM (Rapid Eye Movement)</a> sleep that we usually get only after sleeping for 7-8 hours, is where the brain is cleansing itself, and making it ready for the next day.</p><p>I am on vacation writing this article, and I haven’t written for a couple of days. I am enjoying time on vacation, nature, and with family, letting my brain drift and follow its thoughts. These ideas, connections, and words basically flow themselves, as my brain is super relaxed. While being offline and taking a break, the brain makes sense of things in the subconscious.</p><div><div>There Are Different Kinds of Rest</div><div><div><p>To unblock, you can unblock or rest in different types of rest:</p><ul></ul></div></div></div><p>Another way of taking breaks is small . I explored these in my <a href=\"https://www.ssp.sh/brain/pathless-path/\" target=\"_blank\" rel=\"noopener noreffer\">Pathless Path</a>. These are like Sabbaticals. I did many of these early in my life, and they heavily influenced who I am today and how my life unfolded. But more on this in <a href=\"https://ssp.sh/blog/finding-my-pathless-path/\" target=\"_blank\" rel=\"noopener noreffer\">Finding My Pathless Path</a>.</p><h2>Slowing down, Slow Living</h2><p>Slowing down, and just , is another essential concept. Slowing down, being more present. As Cal Newport says in his book <a href=\"https://www.goodreads.com/book/show/197773418-slow-productivity\" target=\"_blank\" rel=\"noopener noreffer\">Slow Productivity</a>, his main points are:</p><ol></ol><p>If we apply this to life, we get a <a href=\"https://www.ssp.sh/brain/deep-life\" target=\"_blank\" rel=\"noopener noreffer\">Deeper Life</a>. One that is more fulfilled and present. It is about finding the balance between the important and the urgent.</p><p>Turning off the phone, without the pressure or the urge to check if someone has messaged you, if you got a like on social media, your brain starts wondering. You start getting ideas and thoughts to ponder. But these don’t start immediately. If you have been distracted, living a fast life, are constantly on the go, and have no time to think about anything, without defined <a href=\"https://www.ssp.sh/brain/principles\" target=\"_blank\" rel=\"noopener noreffer\">principles</a> to work towards, without <a href=\"https://www.ssp.sh/brain/clarity/\" target=\"_blank\" rel=\"noopener noreffer\">clarity</a>, your brain can’t just switch, and be . But practicing it, like meditation or yoga, is undoubtedly worth it.</p><div><div><div>Another book by Cal Newport is Digital Minimalism, which goes in the same direction—trying to use fewer digital mediums, less social media, and less distraction. I thoroughly recommend it, too.</div></div></div><div><div>The basics: Food, Sleep, and Smartphones</div><div><div>The basis on top of these, is also a good sleep, nutritious food. And the phone needs to be away on do not disturb mode, or best in another room.</div></div></div><h3>Shallow vs. Deep Happiness</h3><p>These can also be summarized into what I call shallow happiness versus deep happiness. Buying something new, eating fast food, having another coffee, or watching a YouTube video all give us pleasure, but only in the . Writing an article or learning something hard, like coding or vim motions, is initially hard. But the reward is long-term. We get  from being in the flow and perfecting something we have learned and perfected over a long time.</p><p>Maybe that’s the secret of a <a href=\"https://www.ssp.sh/brain/deep-life\" target=\"_blank\" rel=\"noopener noreffer\">Deep Life</a>? Not reading news, journaling, praying, meditating, or following a Stoic practice. Reading books is a similar thing; it’s hard to listen to or read an entire book, but when we do, especially if we learn something new, without distractions, in a format the author spent a year or more to curate and put into its format, is so calming and almost peaceful. In fact,   is known to be the biggest and most common version of flow.</p><p>There’s much more to dig into, which I might in another article about “Why we are here on earth?”. This article has been cooking over many months and years already. It will go much deeper and more philosophical about why we even do what we do. It will be connected and even strengthen the flow more if we have clear principles, a clear path, and a passion for why we do something.</p><p>But now, instead, I want to focus on how to get into a flow state or how to unblock us when we feel stressed and not in the mood to do the <a href=\"https://www.ssp.sh/brain/deep-work\" target=\"_blank\" rel=\"noopener noreffer\">Deep Work</a>. If we easily get distracted.</p><p>These are all based on personal experience and books I read. They don’t always work, but one variation usually does the trick by iterating on them.</p><p>These are more pointers related to this article’s topic, thoughts, or steps I followed, some of which are already on my <a href=\"https://brain.ssp.sh\" target=\"_blank\" rel=\"noopener noreffer\">Second Brain</a>.</p><p>Walk every day; nature is tiding our thoughts, just like naps and sleep. Walking, or going outside, is suitable for mental, spiritual, and physical health. When I start walking, the ideas flow, my brain calms down, and I slowly begin to think and process anything I am pondering. It’s best when done in . Nature in itself is a remedy for calming down. It has helped all the big thinkers before our time and will help many after ours.</p><p>Like our inner monkeys or daemons, they don’t want us to go out of bed or get fresh air. Walking frees us from them—the opposite of what social media or smartphones do.</p><div><div><div>Sometimes, bring a paper notebook or camera to capture ideas or thoughts. But other times, go deliberately without.</div></div></div><p>Try to change the medium. If writing on a computer does not work, try pen or paper. Or try talking. As Jonny Miller said:</p><blockquote>There is no talking block, only writer's block.</blockquote><p>Interestingly, if we change the medium, we can unblock it:</p><ul><li>Change environment, physically as well as digitally:\n<ul><li>: Go to another room, a coffee shop, or what works best for me, Libraries.</li><li>: Also, changing your computer environment, e.g., changing from dark to light mode or changing the theme, can give you a sense of change and fresh energy.</li></ul></li><li>Timebox yourself, also called <a href=\"https://en.wikipedia.org/wiki/Pomodoro_Technique\" target=\"_blank\" rel=\"noopener noreffer\">Pomodoro Technique</a>. It has the same effect and works in the train with only the battery. The urge to finish before you run out of battery will give you focus and added clarity.</li><li>Change between Text and Visual: I use a lot of MindMap on paper, but <a href=\"https://obsidian.md/canvas\" target=\"_blank\" rel=\"noopener noreffer\">Obsidian Canvas</a>, Draw with ExaliDraw, and others work as well.</li><li>Try to get into the same habit when working (same place, same music, etc.). Your brain will know it needs to write before you even start.\n<ul><li>Check <a href=\"https://youtu.be/snAhsXyO3Ck?feature=shared\" target=\"_blank\" rel=\"noopener noreffer\">Spaceship You</a> by CGP Grey, showcasing how designated space helped during the pandemic, but also for clarity.</li></ul></li></ul><h3>How I get into Deep Work — Flow</h3><p>The above strategies to unblock are also related to <a href=\"https://www.ssp.sh/brain/deep-work/\" target=\"_blank\" rel=\"noopener noreffer\">Deep Work</a> and how I get into deep work:</p><ol><li>Journal before bed what 1-2 tasks you must/want to achieve tomorrow.</li><li>Go to bed early (you usually don’t achieve much at night).</li><li>Get up before the kids :) (don’t turn on the phone).</li><li>Later, take a walk in nature.</li><li>Change environment/room if stuck.</li></ol><ul><li>Clear some time in your calendar.</li><li>Get rid of distractions: Turn off notifications, close Slack, and put your phone out of reach.&nbsp;</li><li>Clear your space and your mind. Put on concentration music, and get rid of any clutter around.&nbsp;</li><li>Get to work on a specific task, taking periodic short breaks.</li><li>Deeper: Acknowledge your fears, write from authenticity and a place of .\n<ul><li> is stored in our body (not in our mind)</li><li> are like a GPS, help us to find truth and navigate around</li></ul></li></ul><p>Also, like , add boundaries and constraints like no internet, use another browser for searching the web, and suddenly ideas flow. Unlimited freedom is super-blocking, especially for creativity. Great read on the topic: <a href=\"https://sashachapin.substack.com/p/if-you-have-writers-block-maybe-you\" target=\"_blank\" rel=\"noopener noreffer\">If You Have Writer’s Block, Maybe You Should Stop Lying</a>.</p><div><div><div>When I don’t have inspiration or I have a block, I do nothing. . And it’s absolutely because of the deeper inspiration that I’m blessed to feel. I feel it’s been cultivated. I’m connected to it, and I know it’s real, and it doesn’t have to greet me every day. I know it’s there.—</div></div></div><h3>Turn Off, Shut Down, and Re-Energize for a Happy Life</h3><p>Something that is easy to say but hard to do. I enjoy  or a long bicycle ride. It does not matter what you do, but that you do something without distraction. I was listening to books on the walks for a long time, but this always felt like more work. Sure, if you have a great book, listening can be fun too, but I did my best to calm down when going without any decision—just me and nature.</p><p>From the book “The Well-Lived Life: A 102-Year-Old Doctor’s Six Secrets to Health and Happiness at Every Age”, a <a href=\"https://www.goodreads.com/book/show/62918481-the-well-lived-life\" target=\"_blank\" rel=\"noopener noreffer\">comment</a> by Liong on Goodreads says the six secrets may help us live a <strong>long, happy, and healthy life</strong>:</p><ol><li>Let go of the past (Forget the past regrets and anger).</li><li>Live in the present moment. (Live now; don’t stay in your history and worry about your future).</li><li>Connect with nature. (Read and find out yourself).</li><li>Eat a healthy diet. (Choose and be aware of what you eat daily).</li><li>Get regular exercise. (Try exercise every day).</li><li>Find your purpose. (Having a purpose is vital to living longer).</li></ol><h3>Nature: How It Flourishes Creativity</h3><p>Take inspiration from nature. Nature is so powerful.</p><p>For example, watching a tree through the four seasons: In summer, it is green with many leaves; in autumn, it loses them all; in winter, it looks dead. But instead, it builds and prepares its strength for the spring. Where the colorful flowers sprout, it’s calming and inspiring.</p><p>Take gardening; it takes so much patience to grow those three. It puts time into different perspectives. Threes  houses and plants are harvested over many years. If you sell a home, things might have been there long before the house.</p><p>I like to get inspiration from nature. I think of my <strong>creative process as gardening</strong>. Things inside us build up , but great things will come out with constant nurturing: Art, a thought, an idea, anything. It doesn’t matter which day. It’s seasonal, similar to creativity; we can’t be creative all year.</p><p>I hope any of these helped you; more so, it helped me to clarify focus in connection with the ever-growing (primarily digital) distractions to find a better life worth living—A slower and deeper life could be possible.</p><p>Books and related reads I recommend that gave me lots of inspiration:</p>","contentLength":13861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43067303"},{"title":"Btrfs-Progs 6.13 Released With \"mkfs.btrfs --compress\" Support","url":"https://www.phoronix.com/news/Btrfs-Progs-6.13","date":1739705883,"author":"Michael Larabel","guid":535,"unread":true,"content":"<article>Btrfs-Progs 6.13 was released this weekend as the newest routine update to the user-space utilities for the Btrfs file-system...</article>","contentLength":128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GaussianBlur in PyTorch (3)","url":"https://dev.to/hyperkai/gaussianblur-in-pytorch-3-56do","date":1739704832,"author":"Super Kai (Kazuya Ito)","guid":567,"unread":true,"content":"<div><pre><code></code></pre></div>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"134k Lines Of Code Posted As Latest Effort For COBOL Support Within GCC","url":"https://www.phoronix.com/news/134k-Lines-v2-COBOL-For-GCC","date":1739704755,"author":"Michael Larabel","guid":534,"unread":true,"content":"<article>While it's an old language, in recent months there's been a renewed effort over a COBOL language front-end for the GCC compiler. There's been out-of-tree COBOL support for GCC that is working to get into the mainline GNU Compiler Collection codebase. This weekend saw the latest iteration of those patches amounting to 134k lines of new code...</article>","contentLength":344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Getting Started with AWS Bedrock","url":"https://dev.to/alexypulivelil/getting-started-with-aws-bedrock-29n0","date":1739704009,"author":"Alexy Pulivelil","guid":589,"unread":true,"content":"<p>Developers can create and scale generative AI applications with Amazon Bedrock, a fully managed service, by utilising foundation models from AWS and other suppliers. In this guide, I’ll walk you through getting started with AWS Bedrock and invoking the Amazon Titan Text Lite v1 model for text generation.</p><p>\nBefore you begin, ensure that you have the following:</p><ul><li><p>AWS Account with access to Amazon Bedrock(For testing will be using AmazonBedrockFullAccess)</p></li><li><p>AWS CLI installed and configured with appropriate permissions</p></li><li><p>Boto3 (AWS SDK for Python) installed on your machine</p></li></ul><p>You can install Boto3 using:</p><p>Step 1: Set Up AWS Credentials</p><p>If you haven’t already configured your AWS credentials, run:</p><p>Enter your AWS Access Key ID, Secret Key, and select your preferred region where Amazon Bedrock is available (e.g., us-east-1).</p><p>Step 2: Initialize the Bedrock Client\nTo interact with Amazon Bedrock, we need to initialise the AWS Bedrock runtime client using Boto3:</p><div><pre><code># Initialize Bedrock client\nbedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\nStep 3: Invoke Amazon Titan Text Lite v\n</code></pre></div><p>Let’s create a simple script to invoke Amazon Titan Text Lite v1 for generating a text response.</p><div><pre><code># Define the input text\nquestion = \"What is the capital of India?\"\n\n# Prepare the payload\npayload = {\n    \"inputText\": question,\n    \"textGenerationConfig\": {\n        \"maxTokenCount\": 100,\n        \"temperature\": 0.5,\n        \"topP\": 0.9\n    }\n}\n\n# Invoke Titan Text Lite v1\nresponse = bedrock.invoke_model(\n    modelId=\"amazon.titan-text-lite-v1\",\n    contentType=\"application/json\",\n    accept=\"application/json\",\n    body=json.dumps(payload)\n)\n\n# Parse response\nresult = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n\n# Extract and print the output text\nif \"results\" in result and isinstance(result[\"results\"], list):\n    print(\"Answer:\", result[\"results\"][0][\"outputText\"].strip())\nelse:\n    print(\"Unexpected response format:\", result)\n</code></pre></div><p>Step 4: Running the Script</p><p>Save the script as invoke_bedrock.py and run it using:</p><p>Expected Output:<em>Answer: New Delhi is the capital of India. It is situated in the countrys federal district, which is known as the National Capital Territory of Delhi (NCT), and is located in the Indian subcontinent.</em></p><p>Step 5: Fine-tuning Model Parameters</p><p>Amazon Titan models allow temperature and topP tuning for response variation:</p><ul><li><p>temperature: Controls randomness (Lower = More deterministic, Higher = More creative)</p></li><li><p>topP: Controls sampling probability (Higher = More diverse responses)\nAdjust these values in the textGenerationConfig section for different results.</p></li></ul><p>Conclusion\nYou have successfully invoked the Amazon Titan Text Lite v1 model using AWS Bedrock! You can now integrate this into your applications for chatbots, summarization, and content generation.</p>","contentLength":2767,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Go’s Functional Programming is the Ultimate Coding Style","url":"https://dev.to/leapcell/why-gos-functional-programming-is-the-ultimate-coding-style-53ee","date":1739703684,"author":"Leapcell","guid":578,"unread":true,"content":"<p>When you hear \"functional programming,\" Go usually isn't the first language that comes to mind. You might think of Haskell, with its pure functions and monads (don't panic, we'll explain that in detail later), or JavaScript, which loves to showcase its features with higher - order functions and callbacks. But in fact, you can also do functional programming in Go, and the process is far from dull.</p><p>First of all, let's talk about higher - order functions. Higher - order functions can work well with other functions, either taking them as parameters or returning them as values. In the world of Go, implementing higher - order functions is not only possible but also quite ingenious.</p><div><pre><code></code></pre></div><p>You see, in this example, the  function takes an integer slice and a judgment function , and returns the elements in the slice that meet the judgment conditions. Doesn't it seem a bit like a faster JavaScript?</p><p>Next up is currying. It is the process of breaking down a function that takes multiple arguments into a series of functions, each taking a single argument. Currying is actually not as complicated as it might seem.</p><div><pre><code></code></pre></div><p>In this example, the  function takes an integer  and returns a new function. This new function takes another integer  and returns the result of . Simple, straightforward, and gets the job done without any frills.</p><p>One of the characteristics of functional programming is immutability. Once something is constructed, it doesn't change. Instead, if you need something different, you build a new one. This might sound wasteful at first, but in fact, it keeps the code clean and reduces side effects.</p><div><pre><code></code></pre></div><p>In this example, instead of directly modifying the original , we created a new  and modified it.</p><p>Pure functions are like tidy friends. They don't touch or modify anything outside their scope. What you pass in is what you use, and what they return is their only effect.</p><div><pre><code></code></pre></div><p>In this example, the  function only depends on the passed - in parameter  and doesn't affect any external variables.</p><p>In the simplest terms, functors are anything that can map a function. Think of the humble array, applying a function to each item and getting a new array. In Go, there is no built - in general  function, but we can build one ourselves.</p><div><pre><code></code></pre></div><p>Here, we defined a  function that takes an integer slice and a function and returns a new slice, where each element is the result of processing the original slice element by the function.</p><p>Now, let's talk about endofunctors. It's just a fancy way of saying a functor that maps a type to the same type. Simply put, starting from a Go slice, you end up with a Go slice of the same type. It's not rocket science, just a matter of type consistency.</p><p>Taking the previous  as an example, it's a kind of endofunctor in disguise. It takes  and returns  without type conversion.</p><p>Imagine a party where everyone has to bring a friend. Monoids are like that, but for types. They need two things: an operation that combines two types and a special value, which is like the most likable friend - it gets along with everyone but doesn't change anything about them.</p><p>In Go, you can see this with slices or numbers. Let's take numbers as an example because they're easier to work with:</p><div><pre><code></code></pre></div><p>Here, 0 is our hero, the identity element, which keeps the numbers unchanged.</p><p>\"When someone says, 'A monad is a monoid in the category of endofunctors,' they're basically showing off their computer - science vocabulary.\" To explain in detail: A monad is a programming construct that deals with types and functions in a super - special way - like some people are picky about how their coffee is brewed.</p><p>In the simplest terms, a monoid is about combining things together using a special rule, which includes a useless element or identity element. Now, add endofunctors, which are like ordinary old functions but stick to transforming things within their own little universe (category). Put it all together, and you'll see that a monad can be seen as a way to chain functions together in a sequence, but in a super - self - contained way while also respecting the original structure of the data. It's like saying, \"We're going on a road trip, but we can only take the scenic backroads, and we'll end up back where we started.\"</p><p>Monads are all - rounders. They can not only handle values with context (such as errors or lists) but also chain operations together by passing the context. In Go, it might be a bit difficult to mimic this, but let's take a look at error handling, which is a practical use of monads.</p><div><pre><code></code></pre></div><p>This makeshift monad can help us handle computations that might go wrong without causing panics and chaos in the code.</p><p>Functional programming in Go might not be the poster child of the functional paradigm, but it's entirely feasible and can even be fun. Who would have thought, right? Now, you should understand that Go can achieve functional programming just like other languages. With a little effort, you can write clean, efficient, and robust code. </p><p>Finally, I'd like to recommend a platform that's perfect for deploying Golang code: </p><h3>\n  \n  \n  1. Multi - Language Support\n</h3><ul><li>Develop with JavaScript, Python, Go, or Rust.\n</li></ul><h3>\n  \n  \n  2. Deploy unlimited projects for free\n</h3><ul><li>pay only for usage — no requests, no charges.</li></ul><h3>\n  \n  \n  3. Unbeatable Cost Efficiency\n</h3><ul><li>Pay - as - you - go with no idle charges.\n</li><li>Example: $25 supports 6.94M requests at a 60ms average response time.\n</li></ul><h3>\n  \n  \n  4. Streamlined Developer Experience\n</h3><ul><li>Intuitive UI for effortless setup.\n</li><li>Fully automated CI/CD pipelines and GitOps integration.\n</li><li>Real - time metrics and logging for actionable insights.\n</li></ul><h3>\n  \n  \n  5. Effortless Scalability and High Performance\n</h3><ul><li>Auto - scaling to handle high concurrency with ease.\n</li><li>Zero operational overhead — just focus on building.\n</li></ul>","contentLength":5701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 New eBook: The Developer’s Guide to UX Design Thinking – Future-Proof Your Career","url":"https://dev.to/ricky_creates/new-ebook-the-developers-guide-to-ux-design-thinking-future-proof-your-career-3m6k","date":1739703621,"author":"Ricky Synnot","guid":588,"unread":true,"content":"<h2>\n  \n  \n  How Developers Can Stay Ahead in an AI-Driven World\n</h2><p>Most developers think writing great code is enough. It’s not.</p><p>AI is automating more of our work every day. The developers who thrive in the next decade won’t be the ones who can ship the fastest—they’ll be the ones who understand what to build, why it matters, and <strong>how to create great user experiences</strong>.</p><p>This is exactly why I wrote The Developer’s Guide to UX Design Thinking—a practical book that helps developers go beyond code and become  team members.</p><p>Here’s a core concept from the book:</p><h2>\n  \n  \n  The Developer’s Career Risk Pyramid\n</h2><p>Most developers sit at one of three levels:</p><p>🔴 At Risk – Developers who only write code (AI is automating this fast)\n🟠 Safe for Now – Developers who solve problems, not just execute tasks<p>\n🟢 Future-Proofed – Developers who understand UX, product thinking, and collaboration</p></p><p>The higher you go, the safer your career. If you want to be more than a human compiler, you need to learn how to work with designers, influence product decisions, and create user-friendly solutions.</p><p>This isn’t a design book for designers—it’s a practical guide to UX for developers who want to build better products and advance their careers.</p><h3>\n  \n  \n  📖 Part 1: Foundations of UX Design Thinking\n</h3><ul><li>Why UX matters for developers</li><li>The rise of AI &amp; why UX is irreplaceable</li><li>Understanding the designer’s toolkit</li></ul><h3>\n  \n  \n  ⚡ Part 2: Applying UX in Development\n</h3><ul><li>Building empathy for users</li><li>Prototyping &amp; collaborating with designers</li><li>Designing for accessibility &amp; scalability</li></ul><h3>\n  \n  \n  🚀 Part 3: Enhancing Collaboration &amp; Future-Proofing Your Career\n</h3><ul><li>Bridging the developer-designer gap</li><li>How to influence design decisions as a developer</li><li>The evolving role of devs in product teams</li></ul><h3>\n  \n  \n  🛠 Part 4: Tools, Case Studies &amp; Advanced Topics\n</h3><ul><li>Real-world case studies from top tech companies</li><li>Essential UX &amp; dev tools you should be using</li><li>The future of UX in an AI-driven world</li><li>Quick-reference frameworks &amp; checklists for daily use</li></ul><p>To celebrate the launch, the book is available for just $5. No fluff—just real strategies, examples, and frameworks you can use immediately.</p><p>Would love to hear from the dev.to community—how often do you get involved in UX decisions? Or is it always just “build this” with no context? Drop your thoughts below!</p>","contentLength":2321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deepseek-R1: El Modelo Revolucionario que Eleva los Estándares de los LLM de Código Abierto","url":"https://dev.to/angel_rojas_6904bae237a0d/deepseek-r1-el-modelo-revolucionario-que-eleva-los-estandares-de-los-llm-de-codigo-abierto-42cc","date":1739702287,"author":"Angel Rojas","guid":587,"unread":true,"content":"<p>Deepseek-R1 es el modelo insignia desarrollado por Deepseek, una empresa china emergente. Con innovaciones clave como la combinación de aprendizaje por refuerzo y ajuste fino supervisado, así como la técnica Mixture-of-Experts (MoE), Deepseek-R1 redefine el panorama de los LLM de código abierto. Su capacidad para manejar contextos extensos y su arquitectura eficiente lo posicionan como un referente para desarrolladores y empresas.</p>","contentLength":438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Choose an Career in Ethical Hacking?","url":"https://dev.to/ankit_cyber/why-choose-an-career-in-ethical-hacking-1dlf","date":1739702266,"author":"ankit_Cyber","guid":586,"unread":true,"content":"<p>Pursue ethical hacking for a dynamic career where you protect organizations from cyber threats, with high demand, competitive salaries, and continuous learning with endless growth potential.</p>","contentLength":190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deepseek-R1: El Modelo Revolucionario que Eleva los Estándares de los LLM de Código Abierto","url":"https://dev.to/angel_rojas_6904bae237a0d/deepseek-r1-el-modelo-revolucionario-que-eleva-los-estandares-de-los-llm-de-codigo-abierto-4lne","date":1739701264,"author":"Angel Rojas","guid":585,"unread":true,"content":"<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqb9finvzd36xuxlxud2o.jpeg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqb9finvzd36xuxlxud2o.jpeg\" alt=\"Image description\" width=\"275\" height=\"183\"></a>\nLa revolución de los LLM de código abierto comenzó con proyectos pioneros como Alpaca, pero ahora, Deepseek-R1 llega para llevar estos modelos a un nivel superior, destacándose por su rendimiento, eficiencia y escalabilidad. 😎</p><p>¿Qué es Deepseek-R1? 🤔\nDeepseek-R1 es el modelo insignia desarrollado por Deepseek, una empresa china de inteligencia artificial fundada en 2023 por Liang Wenfeng. Este modelo de lenguaje de código abierto sobresale por su capacidad para procesar contextos extensos, resolver problemas complejos y ofrecer respuestas precisas en diversas tareas, posicionándose como una evolución significativa en el ecosistema de los LLM.</p><p>Mini Presentación: Origen y Filosofía 🌏\nOrigen: Nacida en el vibrante entorno tecnológico de China, Deepseek surgió en 2023 en medio de una explosión de innovación en inteligencia artificial.</p><p>Filosofía: Inspirándose en proyectos pioneros como Alpaca, Deepseek apuesta por la transparencia y la colaboración a través del código abierto. Su misión es democratizar el acceso a tecnologías avanzadas, permitiendo que desarrolladores de todo el mundo puedan utilizar, modificar e integrar sus modelos en una amplia variedad de proyectos.</p><p>Desarrollo y Evolución: De Alpaca a Deepseek-R1 🔄\nDeepseek-R1 representa la respuesta evolutiva a los primeros esfuerzos de la comunidad. Entre sus principales innovaciones destacan:</p><p>Métodos de Entrenamiento Híbridos: Combina técnicas de aprendizaje por refuerzo (RL) y ajuste fino supervisado (SFT), aprovechando enormes volúmenes de datos para adaptarse a múltiples tareas.\nInnovación Arquitectónica: Utiliza la técnica Mixture-of-Experts (MoE), que activa solo una parte de sus parámetros en cada consulta, optimizando el uso de recursos sin sacrificar la capacidad del modelo.<p>\nCapacidad de Contexto Extendido: Con la capacidad de manejar hasta 128,000 tokens en una sola entrada, Deepseek-R1 supera las limitaciones de modelos anteriores, permitiendo un análisis profundo y respuestas complejas.</p>\nLista de Modelos y Explicación de Cada Uno 📚<p>\nDeepseek ofrece no solo su modelo principal, sino también varias versiones destiladas que se adaptan a diferentes necesidades y entornos:</p></p><p>Deepseek-R1 (Modelo Principal): Con 671 mil millones de parámetros, este modelo ofrece un rendimiento excepcional para aplicaciones de alto rendimiento en investigación, empresas y desarrollos que requieren procesamiento intensivo.</p><p>Deepseek-R1-Distill-Qwen-1.5B: Variante destilada con 1.5 mil millones de parámetros, ideal para proyectos con recursos limitados o respuestas rápidas, como aplicaciones móviles.</p><p>Deepseek-R1-Distill-Qwen-7B: Con 7 mil millones de parámetros, ofrece un equilibrio perfecto entre rendimiento y eficiencia, adecuado para desarrollos empresariales.</p><p>Deepseek-R1-Distill-Qwen-14B: Con 14 mil millones de parámetros, ofrece tareas complejas y análisis profundos, ideal para proyectos que requieren alta capacidad de procesamiento.</p><p>Deepseek-R1-Distill-Llama-8B: Variante de 8 mil millones de parámetros que combina un buen entendimiento contextual con eficiencia operativa, ideal para sistemas generales.</p><p>Deepseek-R1-Distill-Llama-14B: Con 14 mil millones de parámetros, maneja sofisticados contextos y es ideal para proyectos que exigen procesamiento avanzado.</p><p>Deepseek-R1-Distill-Llama-70B: Con 70 mil millones de parámetros, esta versión ofrece un rendimiento de alta gama comparable con modelos comerciales avanzados, adecuado para aplicaciones de misión crítica.</p><p>¿Dónde Utilizarlos? 🌐\nDeepseek-R1 y sus variantes están diseñados para integrarse de forma versátil en distintos entornos:</p><p>Plataforma Web: Al igual que ChatGPT, Deepseek-R1 está disponible para demos y pruebas interactivas.\nAPI para Integración: Con opciones de API para facilitar la integración en proyectos, disponibles oficialmente o a través de terceros.<p>\nEn Local con Ollama: Al ser un modelo Open Source, puedes descargarlo en tu computadora y ejecutarlo localmente, incluso en versiones más pequeñas que pueden correr en equipos con especificaciones limitadas.</p>\n¿Por Qué Deepseek-R1 es una Revolución? ✨<p>\nDeepseek-R1 eleva los estándares en los LLM de código abierto con varias mejoras:</p></p><p>Rendimiento Excepcional: Su capacidad para gestionar contextos extensos y tareas complejas lo coloca entre los modelos más avanzados del ámbito abierto.\nEficiencia Operativa: Con la técnica Mixture-of-Experts y versiones destiladas, optimiza el uso de recursos, permitiendo su implementación incluso en entornos con hardware limitado.<p>\nInnovación Técnica: La combinación de métodos avanzados de entrenamiento y arquitecturas modernas prepara el terreno para futuros desarrollos en inteligencia artificial.</p>\nAccesibilidad y Colaboración: Distribuido bajo una licencia permisiva, fomenta la integración, modificación y mejora continua, impulsando la innovación global. 🌍\nDeepseek-R1 y todas sus variantes se distribuyen bajo la Licencia MIT, lo que significa:</p><p>Uso Gratuito: Tanto para fines comerciales como no comerciales.\nModificación y Redistribución: El código es completamente abierto, permitiendo que la comunidad lo adapte y mejore.<p>\nIntegración Sencilla: Facilita la incorporación de los modelos en proyectos propios sin restricciones onerosas.</p>\nConclusión<p>\nDeepseek-R1 no solo continúa la revolución iniciada por Alpaca en los LLM de código abierto, sino que establece nuevos estándares con su rendimiento sobresaliente, eficiencia operativa y filosofía de código abierto. Con sus versiones destiladas, Deepseek-R1 está preparado para impulsar una nueva era en la integración de la inteligencia artificial en aplicaciones, investigación y desarrollo colaborativo. ¡El futuro de los LLM se ve prometedor! 🌟</p></p><p>Para más información, visita el artículo completo <a href=\"https://makiai.com/deepseek-r1-el-modelo-revolucionario-que-eleva-los-estandares-de-los-llm-de-codigo-abierto-%f0%9f%9a%80/\" rel=\"noopener noreferrer\">aquí.</a></p>","contentLength":5819,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"“A calculator app? Anyone could make that”","url":"https://chadnauseam.com/coding/random/calculator-app","date":1739701009,"author":"pie_flavor","guid":611,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43066953"},{"title":"just a silly game","url":"https://dev.to/bankai2054/just-a-silly-game-29kl","date":1739700124,"author":"anas barkallah","guid":566,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Migrating from AWS CDK v1 to CDK v2","url":"https://dev.to/sami_jaballah/migrating-from-aws-cdk-v1-to-cdk-v2-21nd","date":1739698802,"author":"Sami Jaballah","guid":565,"unread":true,"content":"<p>If you’re currently using CDK v1 in your daily work, you’ve probably hit a roadblock trying to implement new AWS features. AWS isn’t adding those to CDK v1 anymore—so to keep up with the latest and greatest, you’ll need to migrate to CDK v2. It might sound like a hassle, but don’t worry—I’ve got your back. Let’s go through this step-by-step and get you up to speed, Python style.</p><h2>\n  \n  \n  Why Should You Care About CDK v2?\n</h2><p>Alright, let’s address the big question: why even bother migrating to CDK v2? Well, here are three solid reasons:</p><p>Simplified Dependencies: No more pulling in tons of packages for different AWS services. CDK v2 bundles everything into a single package: aws-cdk-lib. How awesome is that?</p><p>Stay Updated: CDK v1 isn’t getting any love anymore. If you want to keep up with the latest AWS features and updates, v2 is where it’s at.</p><p>Better Developer Experience: AWS has introduced some stability guarantees and cleaned up APIs, making it easier for us to write and maintain our infrastructure code.</p><p><strong>1. Consolidated Package Structure</strong>\nRemember the days of importing a separate package for each AWS service? That’s history now. CDK v2 unifies everything into aws-cdk-lib.</p><div><pre><code>from aws_cdk import core\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><div><pre><code>from aws_cdk import Stack\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><p><strong>2. Goodbye to Deprecated APIs</strong></p><p>Some APIs and constructs from v1 didn’t make the cut in v2. For instance, core.Construct has been replaced by constructs.Construct. A little cleanup never hurts, right?</p><p>There are some great new features, like improved stability guarantees for low-level (L1) constructs and better testing capabilities with assertions.</p><h2>\n  \n  \n  Let’s Get Migrating: Step-by-Step Guide\n</h2><p>Ready to dive in? Follow these steps to upgrade your Python CDK project to v2.</p><p><strong>1. Update Your Dependencies</strong></p><p>Start by upgrading your project dependencies to use CDK v2. Open your requirements.txt or Pipfile and update them:</p><div><pre><code>aws-cdk-lib&gt;=2.0.0\nconstructs&gt;=10.0.0\n</code></pre></div><p>Then, install the new dependencies:</p><div><pre><code>pip install -r requirements.txt\n</code></pre></div><p>This is where the magic happens. Go through your code and replace aws_cdk.core with aws_cdk.Stack, and adjust other imports to use aws-cdk-lib.</p><div><pre><code>from aws_cdk import core\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><div><pre><code>from aws_cdk import Stack\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><p><strong>3. Refactor Deprecated Constructs</strong></p><p>Some constructs have been replaced or removed. For example, core.Construct is now constructs.Construct. Update your code accordingly.</p><div><pre><code>class MyBucket(core.Construct):\n    def __init__(self, scope: core.Construct, id: str):\n        super().__init__(scope, id)\n        Bucket(self, \"MyBucket\")\n</code></pre></div><div><pre><code>from constructs import Construct\n\nclass MyBucket(Construct):\n    def __init__(self, scope: Construct, id: str):\n        super().__init__(scope, id)\n        Bucket(self, \"MyBucket\")\n</code></pre></div><p><strong>4. Remove Unnecessary Feature Flags</strong></p><p>CDK v2 has removed or integrated several feature flags that were necessary in v1. To clean up your cdk.json file, remove any obsolete flags.</p><div><pre><code>{\n  \"app\": \"python3 app.py\",\n  \"context\": {\n    \"@aws-cdk/core:newStyleStackSynthesis\": true,\n    \"@aws-cdk/aws-ec2:uniqueImds\": true,\n    \"@aws-cdk/core:stackRelativeExports\": true,\n    \"@aws-cdk/aws-secretsmanager:parseOwnedSecretName\": true,\n    \"@aws-cdk/aws-kms:defaultKeyPolicies\": true,\n    \"@aws-cdk/core:enableStackNameDuplicates\": true,\n    \"aws-cdk:enableDiffNoFail\": true,\n    \"@aws-cdk/aws-ecr-assets:dockerIgnoreSupport\": true,\n    \"@aws-cdk/aws-s3:grantWriteWithoutAcl\": true,\n    \"@aws-cdk/aws-efs:defaultEncryptionAtRest\": true\n  }\n}\n</code></pre></div><div><pre><code>{\n  \"app\": \"python3 app.py\"\n}\n</code></pre></div><p>Removing these flags ensures your project stays aligned with CDK v2 best practices.</p><p>Finally, make sure everything works as expected. Run these commands:</p><p>Fix any issues that pop up, and you’re good to go!</p><h2>\n  \n  \n  Migration Verification Checklist\n</h2><ul><li> All imports updated to aws-cdk-lib</li><li> Construct imports moved to constructs package</li><li> cdk diff shows expected changes</li></ul><p>And there you have it! Migrating from CDK v1 to v2 isn’t as scary as it might seem. With unified dependencies, better APIs, and future-proofing, this upgrade is worth the effort. Take it one step at a time, and don’t hesitate to ask for help if you hit a roadblock.</p><p>Have you already migrated to CDK v2? Or are you planning to? Share your experience (or any questions) in the comments below!</p><h2>\n  \n  \n  Useful links to help you along the way:\n</h2>","contentLength":4369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"50 Years of Travel Tips","url":"https://kk.org/thetechnium/50-years-of-travel-tips/","date":1739698262,"author":"marban","guid":672,"unread":true,"content":"<p>I’ve been seriously traveling for more than 50 years, and I’ve learned a lot.</p><p>I’ve traveled solo, and I’ve led a tour group of 40 friends. I’ve slept in dormitories and I’ve stayed in presidential suites with a butler. I’ve hitchhiked penniless for months, and I’ve flown by private jet. I’ve traveled months with siblings, and with total strangers. I’ve gone by slow boat and I’ve ridden my bicycle across America, twice. I’ve been to the largest gathering of humans on the planet, and trekked into remotest areas on the planet on my own. I’ve paid for luxury tours, and I’ve done my own self-guided tours. I regularly travel for business, and once I went to Hawaii on a door-prize award. I’ve circumnavigated the globe in only 48 hours, and I traveled uninterrupted for 9 months. I’ve gone first class and third class, sometimes on the same trip. So far I’ve visited half the world’s countries, and usually manage to get far from the capital city when I do. Here is what I know about how to travel.</p><p>There are two modes of travel; retreat or engage. People often travel to escape the routines of work, to recharge, relax, reinvigorate, and replenish themselves— R&amp;R. In this mode you travel to remove yourself from your routines, or to get the pampering and attention you don’t ordinarily get, and ideally to do fun things instead of work things. So you travel to where it is easy. This is called a vacation, or R&amp;R.&nbsp;</p><p>The other mode is engagement and experience, or E&amp;E. In this mode you travel to discover new things, to have new experiences, to lean into an adventure whose outcome is not certain, to meet otherness. You move to find yourself by encountering pleasures and challenges you don’t encounter at home. This kind of travel is a type of learning, and of the two modes, it is the one I favor in these tips.</p><p>Organize your travel around passions instead of destinations. An itinerary based on obscure cheeses, or naval history, or dinosaur digs, or jazz joints will lead to far more adventures, and memorable times than a grand tour of famous places. It doesn’t even have to be your passions; it could be a friend’s, family member’s, or even one you’ve read about. The point is to get away from the expected into the unexpected.</p><p>If you hire a driver, or use a taxi, offer to pay the driver to take you to visit their mother. They will ordinarily jump at the chance. They fulfill their filial duty and you will get easy entry into a local’s home, and a very high chance to taste some home cooking. Mother, driver, and you leave happy. This trick rarely fails.</p><p>Make no assumptions about whether something will be open. There are other norms at work. If possible check at the last minute, if not, have a plan B.</p><p>Crash a wedding. You are not a nuisance; you are the celebrity guest! The easiest way to do this is to find the local wedding hall where weddings happen on schedule and approach a wedding party with a request to attend. They will usually feel honored. You can offer the newlyweds a small token gift of cash if you want. You will be obliged to dance. Take photos of them; they will take photos of you. It will make your day and theirs. (I’ve crashed a wedding in most of the countries I have visited.)</p><p>Don’t balk at the spendy price of admission for a museum or performance. It will be a tiny fraction of your trip’s total cost and you invested too much and have come too far to let those relative minor fees stop you from seeing what you came to see.</p><p>Google maps will give you very detailed and reliable directions for taking public transit, including where to make transfers in most cities.</p><p>When visiting a foreign city for the first time, take a street food tour. Depending on the region, the tour will include food carts, food trucks, food courts, or smaller eateries. It will last a few hours, and the cost will include the food. You’ll get some of the best food available, and usually the host will also deliver a great introduction to the culture. Google “street food tour for city X.”</p><p>The most significant criteria to use when selecting travel companions is: do they complain or not, even when complaints are justified? No complaining! Complaints are for the debriefing afterwards when travel is over.</p><p>As in any art, constraints breed creativity. Give your travel creative constraints: Try traveling by bicycle, or with only a day bag for luggage, or below the minimum budget, or sleep only on overnight trains. Mix it up. Even vagabonding can become a rut.</p><p>Renting a car is easier than ever today, even in developing countries, and oftentimes the best bet for getting around if you are headed for many places outside of cities. It is an option worth considering, especially if you are 2 to 3 people traveling. On the other hand, there are still plenty of places where you don’t want to drive because of chaotic roads, lawless attitudes, and unfavorable liabilities. In those places hiring a driver plus car for a multi-day trip is often a surprisingly appealing bargain—especially if you have 2 to 3 people to split the costs. The total could be less than taking trains and taxis, and you get door to door service, and often a built-in guide who knows the local roads and also local festivities and best places to eat. They will be at least 2x the cost of renting a car, but for some kinds of travel 2x as good. If you are a spontaneous traveler, a hired driver is by far the best option allowing you to change your itinerary immediately as mood, weather, or lucky timing dictate. I usually find drivers by searching travel forums for recommendations. I score candidates primarily by their communication skills.</p><p>If you are fortunate, a fantastic way to share your fortune is to gift a friend the cost of travel with you. You both will have a great time.</p><p>Go to a cemetery. Look for sacred places. People live authentically there. Don’t just visit the markets, but also go to small workshops, hardware stores and pharmacies –&nbsp; places with easy access to local practices. See how it’s different and the same all at once.</p><p><a href=\"https://www.flightaware.com/\">FlightAware</a> is the best free phone app for the status of your flight. It will often tell you about delays hours before the airline will. Tip: use FlightAware to check whether your plane has even arrived at your departure airport.</p><p>Sketchy travel plans and travel to sketchy places are ok. Take a chance. If things fall apart, your vacation has just turned into an adventure. Perfection is for watches. Trips should be imperfect. There are no stories if nothing goes amiss.&nbsp;</p><p>Your enjoyment of a trip will be inversely related to the weight of your luggage. Counterintuitively, the longer your trip, the less stuff you should haul. Travelers still happy on a 6-week trip will only have carry-on luggage. That maximizes your flexibility, enabling you to lug luggage up stairs when there is no elevator, or to share a tuk-tuk, to pack and unpack efficiently, and to not lose stuff. Furthermore, when you go light you intentionally reduce what you take in order to increase your experience of living. And the reality of today is that you can almost certainly buy whatever you are missing on the road.</p><p>Getting an inside tour is the ultimate travel treat. How about a factory tour, a visit to an Amish home, or backstage at an opera? When I travel for business I will sometimes ask for inside access to an uncommon place in lieu of a speaking fee. You are aiming for experiences that simple money can’t buy. Good ones will take planning ahead.</p><p>It is always colder at night than you think it should be, especially in the tropics. Pack a layer no matter what.</p><p>Planning your itinerary: You want to see it all and you are likely to never return, so the temptation is to pile it on, maximize your visit. Since we are in X, and Y and Z&nbsp; are so close, we might as well see Y and Z….. Paradoxically when you are traveling you should minimize the amount of time you spend in transit—once you arrive. The hard-to-accept truth is that it is far better to spend more time in a few places than a little time in a bunch of places.&nbsp;</p><p>To book a train anywhere in the world outside your home country, your first stop should be <a href=\"https://www.seat61.com/\">The Man in Seat 61</a>, a sprawling website which will conveniently help you book the train you want.&nbsp;</p><p>In 53 years of travelling with all kinds of people, I’ve seen absolutely no correlation between where you eat and whether you have intestinal problems, so to maximize the enjoyment of local foods, my rule of thumb is to eat wherever healthy-looking locals eat.</p><p>The list of most coveted cities to visit have one striking thing in common—they are pedestrian centric. They reward walking.Better online hotel sites like<a href=\"http://booking.com\"> Booking.com</a> have map interfaces which allow you to select hotels by their location. Whenever possible I book my hotel near to where it is best to walk, so I can stroll out the door and begin to wander.&nbsp;</p><p>For a truly memorable trip, go without reservations, just winging it along the way. If you like somewhere, stay a day longer, or if you don’t, split a day earlier. If the train is full, take a bus. That freedom can be liberating.</p><p>The <a href=\"https://apps.apple.com/tr/app/google-translate/id414706506\">Google Translate app</a> for your phone is seriously good, and free. It will translate voice, text, or script to and from 250 languages. Use for deciphering menus, signs, talking with clerks, etc. It is often a lifesaver.</p><p>Large-scale luxury cruises have no appeal to me, yet a small boat cruise is an entirely different species and a valid option worth considering. The advantage of a cruise is that your hotel travels with you, so you unpack only once. It is especially useful for small groups because it eliminates the eternal negotiation of deciding where to eat. (You always eat on the boat.) The advantages of a small boat cruise over a huge boat are several: you disembark very quickly, very often, at smaller more intimate places than large boats can do. And the options for activities are more active than just shopping: such as snorkeling, kayaking, bicycling, hiking, visiting local families and communities. Overall you spend far more time doing things off the boat than on. I define a small boat as 40 passengers or fewer. The per day cost is high, but almost every minute of it is quality time, unlike a series of bus rides. Examples of places I’ve loved a small boat cruise; The Galapagos, Alaska inland passageway, Mekong River, Coast of Turkey, and Kerala, India.</p><p>The rate you go is not determined by how fast you walk, bike or drive, but by how long your breaks are. Slow down. Take lots of breaks. The most memorable moments—conversations with amazing strangers, an invite inside, a hidden artwork—will usually happen when you are not moving.</p><p>I generally find “professional” tour guides uninteresting, and too scripted. They are mostly repeating what can be found in guide books. So I rarely hire them. I much prefer to have a friend or local acquaintance show me what interests them in an ad hoc way, with no script. Let friends know you are coming to their area.</p><p>A few <a href=\"https://amzn.to/4gKbKAV\">laundry detergent sheets</a> in a tiny ziplock bag weigh nothing and won’t spill and are perfect for emergency laundry washing in the sink or shower.&nbsp;</p><p>These days it is mandatory that you are connected. You need cell coverage as well as wifi. You’ll want robust mobile coverage for navigation, translation apps, ride shares and a digital wallet for payments. Best option is to use a carrier with “free” international plans (such as T-Mobile or <a href=\"https://fi.google.com/about/\">Google Fi</a>) so you need to do nothing. Second best is to get either a sim card or e-sim for your phone for your visiting country. E-sim apps (such <a href=\"https://www.airalo.com/\">Airalo</a>) can be loaded by yourself virtually. Sims and e-sims are also sold at most international airports when you exit. Most are reputable. One tip, turn off your photo and video cloud backup while on the sim to reduce data usage.&nbsp;</p><p>People in other places are not saints. You might get cheated, swindled, or taken advantage of. Paradoxically, the best way to avoid that is to give strangers your trust and treat them well. Being good to them brings out their good. If you are on your best behavior, they will be on their best behavior. And vice versa. To stay safe, smile. Be humble and minimize your ego. I don’t know why that works everywhere in the world—even places with “bad” reputations—but it does.</p><p>You can get an inexpensive and authentic meal near a famous tourist spot simply by walking at least five blocks away from the epicenter.&nbsp;</p><p>Digital wallets on your phone are displacing local currencies in many places. For instance I did not use any cash on my last trips to the UK and China. And in places where it has not completely eliminated cash you can reduce your cash needs by half with mobile payments. Set up your Apple pay, Google Pay or Alipay before you leave. There is no need to exchange money anywhere, especially at airports. Get any cash you need at local ATMs, which are now everywhere. Use a card that does not charge, or reimburses, a foreign fee.&nbsp;</p><p>If you detect slightly more people moving in one direction over another, follow them. If you keep following this “gradient” of human movement, you will eventually land on something interesting—a market, a parade, a birthday party, an outdoor dance, a festival.&nbsp;</p><p>Splurge in getting the most recent version of a guidebook to your destination. It is worth the price of a lunch to get the latest, most helpful, reliable information. I supplement the latest guidebook research with recommendations suggested in travel forums online. Guidebooks have depth and breadth, while forums offer speed—results from a week ago.&nbsp;</p><p>If you are starting out and have seen little of the world, you can double the time you spend traveling by heading to the places it is cheapest to travel. If you stay at the budget end, you can travel twice as long for half price. Check out <a href=\"https://www.cheapestdestinationsblog.com/\">The Cheapest Destination Blog</a>. In my experience, these off-beat destinations are usually worth visiting.</p><p>In many parts of the world today motorcycles play the role of cars. That means you can hire a moto-taxi to take you on the back seat, or to summon a moto-taxi with an uber-like app, or to take a motorcycle tour with a guide doing the driving. In areas where motorcycles dominate they will be ten times more efficient than slowly going by car.</p><p>Even if you never go to McDonalds at home, visit the McDonalds on your travels. Surprisingly, their menus are very localized and reflect different cuisines in a fun and easy way, with unexpected versions of familiar things. Very illuminating!</p><p>Put inexpensive <a href=\"https://amzn.to/4hxt9y4\">Apple AirTags</a> into your bags, so you can track them when they are out of your sight. More and more airlines are integrating AirTags into their system to help find wayward bags. The tags work for luggage left in hotel storage, or stashed beneath the bus, or pieces you need to forward.&nbsp;</p><p>For the best travel experiences you need either a lot of money, or a lot of time. Of the two modes, it is far better to have more time than money. Although it tries, money cannot buy what time delivers. You have enough time to attend the rare festival, to learn some new words, to understand what the real prices are, to wait out the weather, or to get to that place that takes a week in a jeep. Time is the one resource you can give yourself, so take advantage of this if you are young without money.</p><p>Being beautiful, or well crafted, or cheap is not enough for a souvenir. It should have some meaning from the trip. A good question you may want to ask yourself when buying a souvenir is where will this live when I get home?</p><p>The best souvenirs from a trip are your memories of the trip so find a way to memorialize them; keep a journal, send updates to a friend, take a sketchbook, post some observations, make a photo book.&nbsp;</p><p>When asking someone for a restaurant recommendation, don’t ask them where is a good place you should eat; ask them where they eat. Where did they eat the last time they ate out?</p><p>Here in brief is the method I’ve honed to optimize a two-week vacation: When you arrive in a new country, immediately proceed to the farthest, most remote, most distant place you intend to reach during the trip. If there is a small village, remote spa, a friend’s farm, or a wild place you plan on seeing on the trip, go there immediately. Do not stop near the airport. Do not rest overnight in the arrival city. Do not pause to acclimate. If at all possible proceed by plane, bus, jeep, car directly to the furthest point without interruption. Make it an overnight journey if you have to. Then once you reach your furthest point, unpack, explore, and work your way slowly back to the big city, wherever your international departure airport is.</p><p>In other words you make a laser-straight rush for the end, and then meander back. Laser out, meander back. This method is somewhat contrary to many people’s first instincts, which are to immediately get acclimated to the culture in the landing city before proceeding to the hinterlands. The thinking is: get a sense of what’s going on, stock up, size up the joint. Then slowly work up to the more challenging, more remote areas. That’s reasonable, but not optimal because most big cities around the world are more similar than different. All big cities these days feel same-same on first arrival. In Laser-Back travel what happens is that you are immediately thrown into Very Different Otherness, the maximum difference that you will get on this trip. You go from your home to extreme differences so fast it is almost like the dissolve effect in a slide show. Bam! Your eyes are wide open. You are on your toes. All ears. And there at the end of the road (but your beginning), your inevitable mistakes are usually cheaper, easier to recover from, and more fun. You take it slower, no matter what country you are in. Then you use the allotted time to head back to the airport city, at whatever pace is your pace. But, when you arrive in the city after a week or so traveling in this strangeness, and maybe without many of the luxuries you are used to, you suddenly see the city the same way the other folks around you do. After eight days in less fancy digs, the bright lights, and smooth shopping streets, and late-night eateries dazzle you, and you embrace the city with warmth and eagerness. It all seems so … civilized and ingenious. It’s brilliant! The hustle and bustle are less annoying and almost welcomed. And the attractions you notice are the small details that natives appreciate. You see the city more like a native and less like a jaded tourist in a look-alike urban mall. You leave having enjoyed both the remote and the adjacent, the old and new, the slow and the fast, the small and the big. </p><p>We’ve also learned that this intensity works best if we aim for 12 days away from home. That means 10 days for in-country experience, plus a travel day (or two) on each end. We’ve found from doing this many times, with many travelers of all ages and interests, 14 days on the ground is two days too many. There seems to be a natural lull at about 10 days of intense kinetic travel. People start to tune out a bit. So we cut it there and use the other days to come and go and soften the transitions. On the other hand 8 days feels like the momentum is cut short. So 10 days of intensity, and 12 days in a country is what we aim for. Laser-back travel is not foolproof, nor always possible, but on average it tends to work better than the other ways I’ve tried.</p><p>If you work while you travel, or work remotely, you may enjoy our newsletter <a href=\"https://nomadico.substack.com/\">Nomadico</a>, which is a weekly one-pager with four brief travel tips. It’s free.&nbsp;</p><p>(Thanks to early readers, Craig Mod, Derek Sivers, Chris Michel and Will Milne.)</p>","contentLength":19756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43066720"},{"title":"[Boost]","url":"https://dev.to/coderoflagos/-2fjn","date":1739697631,"author":"Opemipo Disu","guid":584,"unread":true,"content":"<h2>Why CI/CD is a Bottleneck and How AI Can Help ⚙️</h2><h3>Opemipo Disu for Microtica ・ Feb 14</h3>","contentLength":89,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":[]}