{"id":"f33v1Y5Z","title":"Latest","displayTitle":"Latest","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":67,"items":[{"title":"Intel Killer E5000 Ethernet Support For Linux 6.15","url":"https://www.phoronix.com/news/Intel-Killer-E5000-Linux-6.15","date":1739706300,"author":"Michael Larabel","guid":536,"unread":true,"content":"<article>The upcoming Linux 6.15 kernel cycle will be adding support for Intel Killer E5000 Ethernet...</article>","contentLength":94,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Btrfs-Progs 6.13 Released With \"mkfs.btrfs --compress\" Support","url":"https://www.phoronix.com/news/Btrfs-Progs-6.13","date":1739705883,"author":"Michael Larabel","guid":535,"unread":true,"content":"<article>Btrfs-Progs 6.13 was released this weekend as the newest routine update to the user-space utilities for the Btrfs file-system...</article>","contentLength":128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"134k Lines Of Code Posted As Latest Effort For COBOL Support Within GCC","url":"https://www.phoronix.com/news/134k-Lines-v2-COBOL-For-GCC","date":1739704755,"author":"Michael Larabel","guid":534,"unread":true,"content":"<article>While it's an old language, in recent months there's been a renewed effort over a COBOL language front-end for the GCC compiler. There's been out-of-tree COBOL support for GCC that is working to get into the mainline GNU Compiler Collection codebase. This weekend saw the latest iteration of those patches amounting to 134k lines of new code...</article>","contentLength":344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Fast Programming Languages Gaining in Popularity?","url":"https://developers.slashdot.org/story/25/02/16/0332258/are-fast-programming-languages-gaining-in-popularity?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739694840,"author":"EditorDavid","guid":520,"unread":true,"content":"In January the TIOBE Index (estimating programming language popularity) declared Python their language of the year. (Though it was already #1 in their rankings, it had showed a 9.3% increase in their ranking system, notes InfoWorld.) TIOBE CEO Paul Jansen says this reflects how easy Python is to learn, adding that \"The demand for new programmers is still very high\" (and that \"developing applications completely in AI is not possible yet.\") \n\nIn fact on February's version of the index, the top ten looks mostly static. The only languages dropping appear to be very old languages. Over the last 12 months C and PHP have both fallen on the index — C from the #2 to the #4 spot, and PHP from #10 all the way to #14. (Also dropping is Visual Basic, which fell from #9 to #10.) \n\nBut TechRepublican cites another factor that seems to be affecting the rankings: language speed.\n\n\nFast programming languages are gaining popularity, TIOBE CEO Paul Jansen said in the TIOBE Programming Community Index in February. Fast programming languages he called out include C++ [#2], Go [#8], and Rust [#13 — up from #18 a year ago]. \n\nAlso, according to the updated TIOBE rankings... \n- C++ held onto its place at second from the top of the leaderboard.\n- Mojo and Zig are following trajectories likely to bring them into the top 50, and reached #51 and #56 respectively in February. \n\n\"Now that the world needs to crunch more and more numbers per second, and hardware is not evolving fast enough, speed of programs is getting important. Having said this, it is not surprising that the fast programming languages are gaining ground in the TIOBE index,\" Jansen wrote. The need for speed helped Mojo [#51] and Zig [#56] rise... \n\nRust reached its all-time high in the proprietary points system (1.47%.), and Jansen expects Go to be a common sight in the top 10 going forward.\n","contentLength":1863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US goverment seeks to rehire recently fired nuclear workers","url":"https://www.bbc.com/news/articles/c4g3nrx1dq5o","date":1739691949,"author":"niuzeta","guid":519,"unread":true,"content":"<div data-component=\"text-block\"><p>The US government is trying to bring back nuclear safety employees it fired on Thursday, but is struggling to let them know they should return to work, NBC News has reported.</p><p>The National Nuclear Security Administration workers were among hundreds of employees in the energy department who received termination letters.</p><p>An email obtained by NBC said the letters for some NNSA employees \"are being rescinded, but we do not have a good way to get in touch with those personnel\". </p><p>The terminations are part of massive effort by President Donald Trump to slash the ranks of the federal workforce, a project he began on his first day in office, less than a month ago.</p></div><div data-component=\"text-block\"><p>Last week, nearly 10,000 federal workers were let go, according to multiple US outlets. </p><p>That figure was in addition to the estimated 75,000 workers who have accepted an offer from the White House to leave voluntarily in the autumn. </p><p>The nuclear security officials who were laid off on Thursday helped oversee the nation's stockpile of nuclear weapons. That included staff who are stationed at facilities where the weapons are built, according to CNN. </p><p>Attempting to reach the workers, the email, which was sent to current employees, said: \"Please work with your supervisors to send this information (once you get it) to people's personal contact emails.\"</p><p>Trump is working to slash spending across the board, abroad and at home, and going so far as to call for eliminating the education department. He is getting help from the world's richest man, Elon Musk, who, through an effort called Department of Government Efficiency (Doge), has sent workers to comb through data at federal agencies and helped implement the \"buyout\" offer.</p><p>Last week, the Trump administration ordered agencies to fire nearly all probationary employees, those who had generally been in their positions for less than a year and not yet earned job protection. That included the NNSA staff members.</p><p>Altogether, the move could potentially affect hundreds of thousands of people. </p><p>Several of the Trump administration's efforts to shrink the government's size and spending have been met with legal challenges. </p><p>More than 60 lawsuits have been filed against the Trump administration since the president was inaugurated on 20 January.</p></div>","contentLength":2251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43066182"},{"title":"The TechBeat: Cybercrooks Are Using Fake Job Listings to Steal Crypto (2/16/2025)","url":"https://hackernoon.com/2-16-2025-techbeat?source=rss","date":1739689860,"author":"Techbeat","guid":523,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/diadkov\">@diadkov</a> [ 4 Min read ] \n Since March 2024, conspiracy theories about TikTok's ban have spread, citing espionage fears and geopolitical influences without solid evidence <a href=\"https://hackernoon.com/is-the-tiktok-ban-a-cover-up-the-internet-thinks-so\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/moonlock\">@moonlock</a> [ 19 Min read ] \n Moonlock Lab dives deep into a campaign tricking blockchain developers with fake job interviews to deploy malware that installs a backdoor and targets MetaMask. <a href=\"https://hackernoon.com/cybercrooks-are-using-fake-job-listings-to-steal-crypto\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/@javar97\">@@javar97</a> [ 7 Min read ] \n According to Stack Overflow's 2024 survey, 76% of developers are using or planning to use AI tools. <a href=\"https://hackernoon.com/ai-coding-tools-are-still-in-the-randd-stage\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bill-achola\">@bill-achola</a> [ 3 Min read ] \n Who really profits in a startup? Our deep dive into startup salaries reveals how executives secure big paydays while employees take on the risk. <a href=\"https://hackernoon.com/i-learned-the-hard-way-that-startup-high-executives-profit-while-employees-struggle\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ntoskrnl\">@ntoskrnl</a> [ 8 Min read ] \n Security mechanisms under the hood of simple file actions <a href=\"https://hackernoon.com/securitys-moving-parts-01-linux-access-control-mechanisms\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bigredeye\">@bigredeye</a> [ 21 Min read ] \n Perforator is a continuous profiling system developed by Yandex, now open-sourced.   <a href=\"https://hackernoon.com/yandexs-high-performance-profiler-is-going-open-source\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/andrei9735\">@andrei9735</a> [ 6 Min read ] \n Link prediction aims to predict the likelihood of a future or missing connection between nodes in a network.  <a href=\"https://hackernoon.com/learn-to-create-an-algorithm-that-can-predict-user-behaviors-using-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/thomascherickal\">@thomascherickal</a> [ 25 Min read ] \n Deep Research Prompts: Explore 30 ambitous, impactful ideas using emerging tech to tackle global crises. Discover research with world-changing potential. <a href=\"https://hackernoon.com/30-world-changing-prompts-openais-ai-singularity-deep-research-has-arrived\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bigmao\">@bigmao</a> [ 6 Min read ] \n The case against content marketing, and how to do inbound marketing in the post-content age.  <a href=\"https://hackernoon.com/no-startup-has-ever-failed-because-it-didnt-have-a-blog\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/vinitabansal\">@vinitabansal</a> [ 9 Min read ] \n While aggressive managers are difficult, they aren’t impossible to work with. With the right strategies, you can turn them around. <a href=\"https://hackernoon.com/dealing-with-an-aggressive-manager-is-simpler-than-you-think\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/silkdrive\">@silkdrive</a> [ 4 Min read ] \n Discover the future of software development with vibe coding—where creativity comes first, and coding happens effortlessly with AI. <a href=\"https://hackernoon.com/vibe-coding-creativity-without-code\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/editingprotocol\">@editingprotocol</a> [ 4 Min read ] \n If you want to become a top writer, here are 3 tips to help you rise to the cream of the crop.  <a href=\"https://hackernoon.com/climb-the-ranks-3-actionable-tips-to-become-a-top-writer\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/abhiyanampally_kob9nse8\">@abhiyanampally_kob9nse8</a> [ 40 Min read ] \n Dive into the comparitive analysis between logarithmic and floating-point arithmetic in neural nets using the commonly used MNIST dataset. <a href=\"https://hackernoon.com/deep-learning-runs-on-floating-point-math-what-if-thats-a-mistake\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/blackheart\">@blackheart</a> [ 6 Min read ] \n In Barbie, Ken struggles with identity, feeling like he exists in Barbie’s shadow. Many cybersecurity specialists can relate. <a href=\"https://hackernoon.com/the-ken-dilemma-in-cybersecurity\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/step\">@step</a> [ 6 Min read ] \n Language is a component of human consciousness. AI has a conversational and relatable language capability, could that be a fraction of consciousness? <a href=\"https://hackernoon.com/so-how-does-one-really-determine-ai-is-conscious\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mesciusinc\">@mesciusinc</a> [ 10 Min read ] \n Learn everything you need to know about the best Blazor UI Components and how to use them in your application. <a href=\"https://hackernoon.com/the-top-blazor-ui-components-everything-you-need-to-know\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/brightdata\">@brightdata</a> [ 8 Min read ] \n Let's see how OpenAI's Operator is handling CAPTCHAs and explore whether this is the best solution! <a href=\"https://hackernoon.com/openais-operator-vs-captchas-whos-winning\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/andrei9735\">@andrei9735</a> [ 7 Min read ] \n In this post we'll continue working on link prediction with the Twitch dataset. <a href=\"https://hackernoon.com/before-ai-predicts-your-next-friend-it-needs-to-do-this-first\">Read More.</a></p>","contentLength":2888,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Bugs Could Delay Upgrades for Both Siri and Alexa","url":"https://apple.slashdot.org/story/25/02/16/0138205/ai-bugs-could-delay-upgrades-for-both-siri-and-alexa?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739684040,"author":"EditorDavid","guid":512,"unread":true,"content":"Bloomberg reports that Apple's long-promised overhaul for Siri \"is facing engineering problems and software bugs, threatening to postpone or limit its release, according to people with knowledge of the matter....\"\n\nLast June, Apple touted three major enhancements coming to Siri: \n\n- the ability to tap into a customer's data to better answer queries and take actions.\n- a new system that would let the assistant more precisely control apps.\n- the capability to see what's currently on a device's screen and use that context to better serve users.... \n\nThe goal is to ultimately offer a more versatile Siri that can seamlessly tap into customers' information and communication. For instance, users will be able to ask for a file or song that they discussed with a friend over text. Siri would then automatically retrieve that item. Apple also has demonstrated the ability for Siri to quickly locate someone's driver's license number by reviewing their photos... Inside Apple, many employees testing the new Siri have found that these features don't yet work consistently... \nThe control enhancements — an upgraded version of something called App Intents — are central to the operation of the company's upcoming smart home hub. That product, an AI device for controlling smart home appliances and FaceTime, is slated for release later this year. \n\nAnd Amazon is also struggling with an AI upgrade for its digital assistant, reports the Washington Post:\nThe \"smarter and more conversational\" version of Alexa will not be available until March 31 or later, the employee said, at least a year and a half after it was initially announced in response to competition from OpenAI's ChatGPT. Internal messages seen by The Post confirmed the launch was originally scheduled for this month but was subsequently moved to the end of March... According to internal documents seen by The Post, new features of the subscriber-only, AI-powered Alexa could include the ability to adopt a personality, recall conversations, order takeout or call a taxi. Some of the new Alexa features are similar to Alexa abilities that were previously available free through partnerships with companies like Grubhub and Uber... \nThe AI-enhanced version of Alexa in development has been repeatedly delayed due to problems with incorrect answers, the employee working on the launch told The Post. As a popular product that is a decade old, the Alexa brand is valuable, and the company is hesitant to risk customer trust by launching a product that is not reliable, the person said.\n\n","contentLength":2551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Death of OpenAI whistleblower deemed suicide in new autopsy report","url":"https://techcrunch.com/2025/02/15/death-of-openai-whistleblower-deemed-suicide-in-new-autopsy-report/","date":1739682712,"author":"Connie Loizos","guid":511,"unread":true,"content":"<p>Suchir Balaji, a former OpenAI employee, was found dead in his San Francisco apartment on Nov. 26; on Friday, the city’s medical examiner ruled his death a suicide, countering suspicions by his family that had fueled widespread speculation online. Balaji made headlines in October when he accused OpenAI of illegally using copyrighted material to train […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Speedrunning Guide: Junior to Staff Engineer in 3 years","url":"https://blog.algomaster.io/p/speedrunning-guide-junior-to-staff","date":1739680258,"author":"Ashish Pratap Singh","guid":510,"unread":true,"content":"<p>Today’s newsletter features a special guest, , who was promoted from Junior to Staff Engineer at Meta in just 3 years.</p><p>In this article, Ryan will share his insights on how to fast track your career growth and get promoted faster.</p><p>Once you land that first software engineering job, the next big question becomes: how do you get promoted? Many engineers fall into the day-to-day routine of writing code without a clear idea of how to grow their careers.</p><p>This happened to me. At my first job at Amazon, I landed code without knowing what I could do to grow my skills. I left that job within eight months because I felt I wasn’t growing as an engineer. Three years later, I made it to Staff Software Engineer at Instagram after tons of mentorship. Early on, I learned that being good at coding wasn’t enough to get promoted; you have to think strategically about your career and often need to develop new behaviors to move up.</p><p>In this article, I’ll share everything that helped me fast-track my way up the ladder, from developing the right mindset to making key moves that many overlook. Even if rapid growth isn’t your goal, this guide has learnings for all tech career paths.</p><ul><li><p>Software Engineering Levels</p></li><li><p>An Algorithm for Promotion</p></li><li><p>Junior (IC3) → Mid-level (IC4)</p></li><li><p>Mid-level (IC4) → Senior (IC5)</p></li><li><p>Senior (IC5) → Staff (IC6)</p></li></ul><h2>Software Engineering Levels</h2><p><em>Note: “IC” = “Individual Contributor”</em></p><p>In software engineering, companies measure career progression by levels that measure both behaviors and impact within the company. While the exact titles and structure can vary between companies, most tech companies follow a similar system:</p><ul><li><p> - Early in your career, working on smaller, well-defined tasks with guidance from more experienced engineers.</p></li><li><p> - More autonomous, handling moderately complex projects, and beginning to take initiative in improving the codebase and what they build.</p></li><li><p> - Leading larger projects with team-level influence. You’ll mentor and guide the team while having a broad impact on the codebase.</p></li><li><p>: Focusing on cross-team collaboration and solving org-wide challenges. Staff engineers are strategic thinkers who influence the technical direction of their organization.</p></li><li><p><strong>Senior Staff Engineer and Beyond (IC7+): </strong>Senior staff engineers and up operate with top technical expertise, driving large-scale initiatives that have a broad impact on the company. Senior staff engineers mentor staff engineers and work closely with executive leadership to meet business objectives.</p></li></ul><p>Your impact and compensation increase as you progress, which is a lot more satisfying in my experience. Not to mention that the skills that get you promoted also let you control what you and the company work on.</p><p>Also, many companies consider only senior engineers (IC5) and higher to be “terminal levels.” You must eventually get promoted to IC5, or you’ll be managed out. Most engineers are promoted in time, so it’s not meant to scare you but to encourage you to grow.</p><h2>An Algorithm For Promotion</h2><p>There’s a common set of steps across all promotions that will get you to Staff:</p><p><strong>1) Exceed expectations at your current level</strong> - Your manager will be hesitant to find you opportunities at the next level if they have concerns about your performance at the current level. Also, when your manager puts together a promotion packet, it’ll contain a history of your past ratings. The promotion committee will have concerns about your packet if you have a history of only meeting expectations for your level. Work with your manager to understand the expectations for your level and how to exceed them.</p><p><strong>2) Be direct with your manager about promotion </strong>- Once you know you’re exceeding expectations for your level, ask your manager what next-level performance looks like. Your manager plays a huge role in your promotion. They build your case and advocate for it, so they have a lot of influence on this process. Also, the lower the level, the more control your manager has. IC3 -&gt; IC4 promotions are straightforward, so your manager’s perspective is usually what happens. For IC5 -&gt; IC6, there is a lot more ambiguity, so <a href=\"https://www.developing.dev/p/how-promotions-and-ratings-work\">your manager serves more as a middleman between you and the promotion committee</a>. Your manager still plays a significant role in writing your packet and delivering feedback.</p><p><strong>3) Find next-level scope - </strong>If you only work on projects that fit your level’s behaviors, you won’t get any closer to promotion, no matter how good your work is. One simple pattern for finding next-level scope is brainstorming projects with engineers who are 1-2 levels higher than you are. Often, they will have a lot of projects sitting in their backlog that are big enough to help you get promoted. If you take on one of their projects, they’ll often help mentor you, review your designs and code, and give you strong peer feedback for your future promotion packet. I wrote <a href=\"https://www.developing.dev/p/a-simple-pattern-for-finding-next\">more on this here</a>. Make sure to confirm with your manager that they agree that what you’re working on fits the behaviors of the next level.<strong>4) Maintain next-level behaviors and impact </strong>- The duration you need to perform at the next level varies depending on your level and your company. At minimum though, you need to maintain that performance for 6-12 months. This is because promotions are “lagging” in tech. You must prove that you’re already operating at the next level before getting promoted. This reduces the risk of failing to meet expectations at the new level.</p><p>Getting promoted faster is a matter of doing steps (1), (2) and (3) as fast as possible. The best you can do is immediately start exceeding expectations in your first half and working with your manager on the next level.</p><p>Almost every team has scope for more Senior Engineers (IC5). You can get promoted up to that level if you have the skills and behaviors. Past that, situation and business scope play a much larger role. <a href=\"https://www.developing.dev/p/staff-career-growth-product-or-infra\">Many teams don’t need someone who has Staff-level leadership and technical skills</a>. If you find yourself stuck at any point due to your situation, you’ll likely have to switch teams to continue growing your career.</p><p>Now that you have the algorithm that applies at any of these levels let’s get into the level-specific strategies. I’ll share what got me promoted and what I would change if I did it again.</p><h2>Junior (IC3) → Mid-level (IC4)</h2><p>The main difference between these levels is in the size of the scope that you can handle independently. Here’s a rule of thumb:</p><ul><li><p>IC3 - Can handle individual tasks (&lt;2 weeks of work) with minimal guidance</p></li><li><p>IC4 - Can handle medium-to-large features (&lt;2 months of work) with minimal guidance</p></li></ul><p>“Minimal guidance” doesn’t mean that you can’t ask for help—it simply means that you can unblock yourself and make consistent progress. Asking good questions is one of the most effective ways to unblock yourself.</p><p>You should drive full features and do the project management for them. You should break your project into tasks, set reasonable timelines, and keep stakeholders updated.</p><p>You will not be expected to come up with the projects yet at this level—Senior Engineers will often outline them. However, at the IC4 level, you’re expected to take more initiative:</p><ul><li><p>Initiate refactoring and code cleanups, and <a href=\"https://www.developing.dev/p/how-to-start-reviewing-code\">give thoughtful code reviews</a>. Leave the code in a better state than you found it.</p></li><li><p><strong>Contributing to production excellence - Participate in the team’s oncall, and help debug production breakages.</strong></p></li><li><p><strong>Own the health of what you build - Add test coverage, logging, and build dashboards to monitor correctness.</strong></p></li></ul><p>Optimize your dev velocity to grow faster at this level. <a href=\"https://www.developing.dev/p/shipping-code-faster\">Shipping code faster</a> creates a shorter feedback loop, accelerating your learning process. This core skill will help you ship IC4-scope projects and improve as an engineer.</p><p>Here’s an example of several promotion timelines for what you can expect:</p><ol><li><p><strong>Promotion in 6 months (exceptional) - </strong>Rare since this means meeting IC4 expectations while onboarding. This is easier for high-performing return interns since they skip onboarding and may have some past track record already.</p></li><li><p><strong>Promotion in 12 months (great) - I’d shoot for this timeline. It’s challenging yet reasonable since it gives you six months to onboard and then start meeting IC4 expectations.</strong></p></li></ol><p>Here’s my promotion timeline as an example:<strong>H1 (L3 Exceeds Expectations)</strong> - First, I took on any task that came my way. These were nice-to-have features that others didn’t have time for. I completed them quickly and started on a larger pipeline rewrite (L4 scope) that my tech lead offered me. Outside of my main project work, I made many contributions to removing dead code and speeding up existing code because I enjoyed it.</p><p>I started to hit L4 expectations in the last few months of the half. But, since I didn’t have six months track record, I didn’t meet the promotion criteria.</p><p><strong>H2 (L3→L4 Promotion, Greatly Exceeds Expectations)</strong> - I continued driving my L4-scope project independently with high engineering quality. I came up with the idea to build a test harness to validate this rewrite that was “comparable to L5 quality” execution. I continued my passion for improving the codebase and led the company in adding static type annotations that MonkeyType couldn’t.</p><p>At this point, I had delivered on L4 scope for over six months, so the promotion made sense.</p><p><strong>What I Would Have Changed:</strong></p><p>Looking back, I would have discussed what IC4 growth looked like with my manager. I wasted our one-on-one time on project updates instead of career growth. This led to two problems:</p><ul><li><p><strong>Spent time on work that wasn’t impactful</strong> - I took on any work that was passed my way, even though not all of it was impactful. I probably could’ve gotten more out of my time.</p></li></ul><ul><li><p><strong>Didn’t have accurate expectations</strong> - I had another engineer tell me my work was IC4 level and that I should get promoted in my first half. I knew nothing then, so I took their word for it. I was surprised when I didn’t get promoted, which could have been avoided if I had been in sync with my manager.</p></li></ul><p>Although I could have been more calculated, writing as much code as I did opened doors. My tech lead trusted me with an IC4 project because I showed I could handle it. Similarly, some of the engineering craft work I did for my own personal pleasure ended up being part of what got me promoted too. <strong>The more work you do, the luckier you get.</strong></p><h2>Mid-level (IC4) → Senior (IC5)</h2><p>The IC4 to IC5 gap is larger than the IC3 to IC4 one. This is because IC5 promotion requires significant behavior changes. Raw code output is no longer the top priority. You <a href=\"https://www.developing.dev/p/how-some-engineers-always-lead\">need to lead</a> and <a href=\"https://www.developing.dev/p/how-to-influence-without-authority\">have a larger influence</a> within your team too. Here are a few examples of those differences:</p><p>Example 1 - Improving the codebase</p><ul><li><p>IC4 - Initiates refactoring and code cleanups.</p></li><li><p>IC5 - Identifies areas of improvement, <strong>influences the team to take goals on improving it together</strong>, then leads the charge on those goals.</p></li></ul><p>Example 2 - Production excellence</p><ul><li><p>IC4 - Participates in team’s oncall and mitigating outages.</p></li><li><p>IC5 - Creates an “oncall improvement” workstream and <strong>builds a process for everyone to improve the team’s oncall.</strong></p></li></ul><p>Example 3 - Project direction</p><ul><li><p>IC4 - Owns the project management of a medium-to-large feature.</p></li><li><p>IC5 - <strong>Drives team planning and builds a roadmap</strong> of several medium-to-large features.</p></li></ul><p>I wouldn’t say the IC5 examples are harder, but they require a mindset shift to own things at the team level.</p><p>Also, you’ll need to work on projects of sufficient scope for an IC5. There are a few ways that tech companies measure scope. Here’s a comparison of the criteria for IC4 and IC5 levels:</p><p>These criteria aren’t a checklist. Your work can be IC5 scope by meeting only some of these criteria.</p><p>IC5 is also the first time engineers begin to focus on growing others. At this level, you should mentor others and build up the team’s culture, which includes <a href=\"https://www.developing.dev/p/how-to-drive-meetings\">driving meetings</a>, knowledge sharing, recruiting activities, and organizing team activities. Starting mentorship relationships early is a good idea since you can’t rush mentorship.</p><p>If you can learn the above behaviors quickly, you can expect promotion on these timelines:</p><ol><li><p><strong>Promotion in 6 months (exceptional) - </strong>This is rare since you need to exert team-level influence as soon as you join the team. I could see this happening for someone who was under-leveled and just got promoted to IC4.</p></li><li><p><strong>Promotion in 12 months (great) - </strong>If you’re ambitious, I’d aim for this goal. It is possible to do this if you find IC5 scope in your first half. If not, one more half should secure your promotion.</p></li></ol><p>Here’s my promotion timeline as an example:</p><p><strong>H1 (IC4 Exceeds Expectations)</strong> - This half I wrapped up the workstream that got me promoted to IC4 and picked up another IC4 project. I spent a ton of time on engineering craft this half because I enjoyed it. I deprecated a few legacy systems that no one else would because they were dangerous and not that impactful. I didn’t exhibit any IC5 behaviors this half.</p><p>My manager handed me an IC5 workstream (~6 eng) to <a href=\"https://about.instagram.com/blog/engineering/cutting-threads-send-latency-in-half\">cut video messaging latency in half</a>, which I led successfully. I also began a side project, which became a multiple-half collaboration with another team. Lastly, I took on an intern who did a phenomenal job helping me execute these two roadmaps I led. Although I started exhibiting IC5 behaviors, the company canceled performance reviews this half because of the pandemic.</p><p><strong>H3 (IC4 → IC5 Promotion, Greatly Exceeds Expectations)</strong> - My impact this half could’ve met expectations at the IC6 level. I doubled down on the cross-org scope I created in H2 and developed a multi-half roadmap. I influenced and led another team to invest several engineers to <a href=\"https://about.instagram.com/blog/engineering/making-instagram-video-ads-performant\">revamp the IG video ads pipeline</a> with great results. I built out a second workstream and mentored another engineer to deliver it. This half, I had massive impact, team-level influence, and mentorship, which is what got me promoted.</p><ul><li><p><strong>The Skill of Tech Leading</strong> - If you grew from L3 → L4 right, you should be exceptional at landing code. The L5 behavior of team-level influence is just helping others do the same. In my first half of leading an initiative, I remember feeling unsure about it since I only had two years of experience. Leaning on my strong execution skills helped me become comfortable leading others.</p></li><li><p><strong>Working Hard Led To More Opportunities</strong> - I worked a lot and had a ton of workstreams in flight at the same time. This approach increased my chances of having one that had a ton of impact. At the time, I didn’t know it and was just throwing myself at any problems that came my way. Looking back, it was a great way to derisk my promotion.</p></li><li><p> - In my first half as an L4, I took on projects that were time-intensive and not impactful. I did these migrations because I loved cleaning up tech debt. I would’ve had more impact if I had influenced someone else to do them while I found IC5 scope instead.</p></li></ul><h2>Senior (IC5) → Staff (IC6)</h2><p>Staff Engineers (IC6) are at the same level as engineering managers. They solve problems that few others can and play a critical role in setting team direction. They lead major initiatives and influence the engineering culture of teams around them.</p><p>Some say that promotion from IC5 → IC6 is harder than IC6 → IC7 due to the significant behavior changes needed. There are a few major differences between IC5 and IC6.</p><p><strong>1) Influence Across Teams - </strong>Staff Engineer’s projects often extend beyond their team. They take on larger problems by influencing other teams without authority.</p><p>Once IC6s establish these workstreams, they tackle the hardest problems and work through others. They focus on outcomes and don’t always do the work themselves. <strong>Working through delegation and influence across teams is the biggest mindset shift from IC5 → IC6.</strong></p><p>This style of working isn’t limited to their main project impact. IC6s should also use their influence to inspire a culture of higher engineering quality and reliability across teams.</p><p> - Senior Engineers (IC5) build roadmaps of several medium-to-large features that help achieve their team’s goals. In this case, the problem and its business impact are clear; we just need an engineer to create a plan to solve it.</p><p>Staff Engineers (L6) handle more ambiguity. They don’t just solve known problems; they create scope by finding impactful opportunities and problems. <strong>Managers work with their L6s to expand the scope of the team.</strong></p><p>- Big tech companies determine what level projects are in a few ways. Here’s a comparison of the criteria for L5 and L6 levels:</p><p>Project complexity also distinguishes IC6 scope. <strong>Problems that IC5s can’t solve are considered IC6 scope</strong>. This is why specialists often have IC6+ scope; others often can’t do their projects.</p><p>These criteria aren’t a checklist. Work can be IC6 scope by meeting only some of these. Your manager will use these criteria to argue that your work is IC6 scope. This is one of the reasons why it’s important to align with your manager on your work’s scope.</p><p> - Staff engineers uplift others around them. They should have the ability to help IC5 engineers grow. There are a few ways they uplift others:</p><ul><li><p>Mentorship - Dedicated mentorship, preferably with senior engineers</p></li><li><p>Knowledge sharing - Writing wikis, giving presentations, contributing to Q&amp;A groups</p></li><li><p>Collaborations - Growing others while working with them (e.g. code reviews, design reviews, discussions)</p></li></ul><p>IC6 engineers should also contribute to growing the organization. This means that they help with recruiting and partner with their manager to improve team health.</p><p>Getting to the Staff Engineering level can take a long time. Since IC5 is considered “terminal,” there is no external pressure to achieve IC6 fast. However, if you are eager to grow as fast as possible, here’s how fas you can expect promotion:</p><ul><li><p><strong>Promotion in 1 half (Ridiculous)</strong> - You’d need to start influencing outside your team as soon as you join. Even then, it’s unlikely you’d get promoted this fast unless you create something company-changing.</p></li><li><p><strong>Promotion in 2 halves (Exceptional) </strong>- Finding IC6 opportunities on your team is not always possible this fast. It’s a combination of situation and skill to get promoted in two halves, even if you execute well.</p></li><li><p><strong>Promotion in 3 halves (Great) </strong>- If you’re ambitious I’d aim for this goal. It gives you a year to find IC6 scope, which is a reasonable amount of time to pivot if needed. Also, your track record of successes in the first year will help build the narrative for promotion.</p></li></ul><p><strong>H1 (IC5 Exceeds Expectations)</strong> - I led two workstreams that were partnerships with other teams to hit our goals. I also landed a <a href=\"https://about.instagram.com/blog/engineering/making-instagram-video-ads-performant\">large win in an unplanned ads workstream</a>, which is what brought my rating above expectations. I was also one of the top contributors to code review and interviewing in my 70-person eng org. The hidden success here was that I bootstrapped a new workstream towards the end of the half that was certainly IC6 scope.</p><p><strong>H2 (IC5 → IC6 Promotion, Greatly Exceeds Expectations)</strong> - The IC6 workstream I created turned out to be a massive opportunity. <a href=\"https://engineering.fb.com/2022/11/04/video-engineering/instagram-video-processing-encoding-reduction/\">This work was a huge success</a>, resulting in a company-wide award and <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:6994387208375873536/\">public recognition from Mark Zuckerberg</a>. I also created a cross-org collaboration between 3 large orgs (70+ eng each), which received positive feedback from each director. Lastly, I ran infrastructure preparations for my org resulting in no major incidents during the most critical time of the year. The repeated influence and impact of these large initiatives is what got me promoted to IC6.</p><p> - My past context and relationships at Instagram helped me move a lot faster. I could lead several workstreams at once because I knew so much about the codebase. Also, it was easier to get work done in collaboration because I knew partner engineers from past work. Staying at one company for a longer time does have its benefits.</p><p> - When I was an IC4, I stumbled upon some IC6 scope without realizing it. I had strong initiative so I started solving problems without thinking through why it was impactful. I got lucky that the work had IC6 impact. I’ve since learned the importance of understanding the “why” before diving in. It helps you have consistent IC6 impact and makes it easier to get buy-in for your work.</p><p><strong>The Tech Lead Skillset Scales Well </strong>- In my promotion to Senior (IC5), I learned how to lead initiatives within my team. This skillset turned out to work well at higher levels too. The difference was just that more people were involved. This skill is a great way to continue your IC growth to the highest levels if you fit the “tech lead” archetype.</p><p>Growth to the Staff level can take a long time, and luck plays a role. As you move up the ladder, each promotion depends more and more on your situation in addition to your skill.</p><p>There are ways to increase your luck. For instance, you can go to growing companies and teams. You can pick business-critical projects. You can go where the most talented people are. None of these are foolproof, but they increase your chances.</p><p>Aside from picking your situation, one way to manufacture luck is to do as much good work as you can. Many growth opportunities came to me because of some past work I did. People would reach out to me to do more of it or because they wanted to ask me questions about something I had launched.</p><p>Although luck plays a role, there are aspects of getting promoted that rely less on luck. Here are four high-level areas:</p><p>Impact is any measurable and objective outcome that benefits your company. Promotions are a byproduct of your elevated, sustained impact. If you can learn what your organization considers impactful and you deliver that, you will be rewarded.</p><p><strong>2) Leverage is how you have more impact.</strong> Software engineers increase their leverage through people, writing, and code. Leverage is what differentiates higher-level ICs from lower-level ones. What I mean by each type of leverage:</p><ul><li><p>People - People leverage comes from technical leadership. This means setting direction, reviewing designs/code, and growing others.</p></li><li><p>Writing - Writing gives us leverage by influencing and helping others without your active involvement.</p></li><li><p>Code - Not all code is created equal. High-leverage code solves problems that few others can or helps engineers move faster at scale.</p></li></ul><p> When people hear “personal brand,” their minds often go to social media. But the brand that matters most is your “internal brand.” What do people within your company think about your work and its value? This is the brand that you should care most about.</p><p>Most of the top ICs I know are not well-known outside of Meta. They are legendary within the company, though because people see their impressive work. Build your internal brand by doing great work and letting others know about it (<a href=\"https://www.developing.dev/p/be-visible\">further reading here</a>).</p><p><strong>4) Build your soft skills.</strong> Working with others is a necessity to do anything of consequence. Also, being someone others want to work with makes it easier to find mentors who will uplift you along the way.</p><p>Soft skills are underrated among software engineers. It’s important to be an excellent IC, but you can go so much further if you also communicate well. Also, engineers don’t often prioritize soft skills, so having them will help you stand out and lead.</p><p>One last thing I’ll leave you with is something that I didn’t realize until looking back. When I first joined the industry, I was an absolute machine. I would get in early and stay until the last shuttle left at 9:27 PM. Although this might sound like hell to some people, I loved it. No one made me do that; I put in those hours because I enjoyed the work and thought it was interesting.</p><p>Looking back years later, I realize that was an unfair advantage I had. It let me put in a ton of work without getting burned out. Also, I got much more out of what I did because I was intrinsically motivated.</p><p>If there’s one thing I wish for you, it is that you find work at the intersection of what you enjoy and what will get you promoted. That is the best recipe for hyper-career growth.</p><p>Thanks for reading,Ryan Peterman</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><p>If you have any questions or suggestions, leave a comment.</p><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p>","contentLength":24211,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0173844b-0e38-4ca8-b779-b34f7f778872_1600x413.png","enclosureMime":"","commentsUrl":null},{"title":"Ask Slashdot: What Would It Take For You to Trust an AI?","url":"https://ask.slashdot.org/story/25/02/15/2047258/ask-slashdot-what-would-it-take-for-you-to-trust-an-ai?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739673240,"author":"EditorDavid","guid":500,"unread":true,"content":"Long-time Slashdot reader shanen has been testing AI clients. (They report that China's DeepSeek \"turned out to be extremely good at explaining why I should not trust it. Every computer security problem I ever thought of or heard about and some more besides.\") \n\nThen they wondered if there's also government censorship:\n\nIt's like the accountant who gets asked what 2 plus 2 is. After locking the doors and shading all the windows, the accountant whispers in your ear: \"What do you want it to be...?\" So let me start with some questions about DeepSeek in particular. Have you run it locally and compared the responses with the website's responses? My hypothesis is that your mileage should differ... \n\nIt's well established that DeepSeek doesn't want to talk about many \"political\" topics. Is that based on a distorted model of the world? Or is the censorship implemented in the query interface after the model was trained? My hypothesis is that it must have been trained with lots of data because the cost of removing all of the bad stuff would have been prohibitive... Unless perhaps another AI filtered the data first? \nBut their real question is: what would it take to trust an AI? \"Trust\" can mean different things, including data-collection policies. (\"I bet most of you trust Amazon and Amazon's secret AIs more than you should...\" shanen suggests.) Can you use an AI system without worrying about its data-retention policies?\n \n\nAnd they also ask how many Slashdot readers have read Ken Thompson's \"Reflections on Trusting Trust\", which raises the question of whether you can ever trust code you didn't create yourself. So is there any way an AI system can assure you its answers are accurate and trustworthy, and that it's safe to use? Share your own thoughts and experiences in the comments. \nWhat would it take for you to trust an AI?","contentLength":1846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Medical Research Cuts Would Hit Colleges and Hospitals in Every State","url":"https://www.nytimes.com/interactive/2025/02/13/upshot/nih-trump-funding-cuts.html","date":1739671645,"author":"erickhill","guid":509,"unread":true,"content":"<p>A proposal by the Trump administration to <a href=\"https://www.nytimes.com/2025/02/07/us/politics/medical-research-funding-cuts-university-budgets.html\">reduce the size of grants</a> for institutions conducting medical research would have far-reaching effects, and not just for elite universities and the coastal states where many are located.</p><p>Also at risk could be grants from the National Institutes of Health to numerous hospitals that conduct clinical research on major diseases, and to state universities across the country. North Carolina, Missouri and Pennsylvania could face disproportionate losses, because of the concentration of medical research in those states.</p><figure><div><div><div><p>Based on spending in the 2024 fiscal year.</p></div></div></div></figure><p>In the 2024 fiscal year, the N.I.H. spent at least $32 billion on nearly 60,000 grants, including medical research in areas like cancer, genetics and infectious disease. Of that, $23 billion went to “direct” research costs, such as microscopes and researchers’ salaries, according to an Upshot analysis of <a href=\"https://report.nih.gov/award/index.cfm?ot=&amp;fy=2024\">N.I.H. grant data</a>.</p><p>The other $9 billion went to the institutions’ overhead, or “indirect costs,” which can include laboratory upkeep, utility bills, administrative staff and access to hazardous materials disposal, all of which research institutions say is essential to making research possible.</p><p>The <a href=\"https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-068.html\">N.I.H. proposal</a>, which has been <a href=\"https://www.nytimes.com/2025/02/11/health/nih-research-funding-lawsuit-injunction.html\">put on hold by a federal court</a>, aims to reduce funding for those indirect costs to a set 15 percent rate that the administration says would save <a href=\"https://x.com/NIH/status/1888004759396958263\">about $4 billion a year</a>. The Upshot analysis estimates that a 15 percent rate would have reduced funding for the grants that received N.I.H. support in 2024 by at least $5 billion. The White House said the savings would be reinvested in more research, but the rate cuts would open up sizable budget holes in most projects at research institutions.</p><p>It is not clear whether those organizations can fill the gaps with other funding sources or by shifting how they apply for grants. Instead, many officials at universities and hospitals have said that they may have to pull back on medical or scientific research.</p><p>“It’s not an overstatement to say that a slash this drastic in total research funding slows research,” said Heather Pierce, senior director for science policy at the Association of American Medical Colleges, which has <a href=\"https://storage.courtlistener.com/recap/gov.uscourts.mad.280609/gov.uscourts.mad.280609.1.0.pdf\">sued along with other education and hospital associations</a> to block the policy. And slower scientific progress, she said, would affect anyone who depends on the development of new treatments, medical interventions and diagnostic tools.</p><p>We estimate that virtually all universities and hospitals would see fewer funds on similar projects in the future. The 10 institutions that receive the most money from N.I.H. stand to lose more than $100 million per year on average.</p><p>To understand how the change would work, let’s look at <a href=\"https://reporter.nih.gov/search/JbRgAtchAUKHOoW2X1vObQ/projects/map/project-details/10821480\">one grant for about $600,000</a> sent last year to the University of Alabama at Birmingham to study whether exercise can improve memory for people with epilepsy.</p><p>The calculation above, which we have repeated for every grant paid last year, is a bit simplified. In reality, the researchers would lose even more money than we’ve shown, because of the way indirect funding is calculated (see our methodology at the bottom of this article).</p><p>Our analysis also makes some other conservative assumptions given the policy’s uncertainty. We assume, for instance, that the new 15 percent rate is a flat rate that all grantees would receive, and not a maximum rate (a distinction left unclear in the N.I.H. guidance). We also assume that the change applies not just to institutions of higher education, but also to all kinds of grantees, including hospitals.</p><p>In a statement, the White House indicated it would reserve any savings for additional research grants. “Contrary to the hysteria, redirecting billions of allocated N.I.H. spending away from administrative bloat means there will be more money and resources available for legitimate scientific research, not less,” said Kush Desai, a White House spokesman.</p><p>The N.I.H. announcement, however, coincides with the Trump administration’s moves to cut spending across the government, and with the N.I.H.’s withholding of funding for grants — their direct and indirect costs alike — in <a href=\"https://www.nytimes.com/2025/01/31/us/trump-freeze-blocked.html\">apparent conflict</a><a href=\"https://www.nytimes.com/2025/01/28/us/politics/states-lawsuit-trump-federal-grants-pause.html\">with separate court orders</a>.</p><p>The N.I.H. <a href=\"https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-068.html\">guidance document</a> includes a number of conflicting statements and statistics the Upshot could not reconcile. The N.I.H. also declined to answer questions about the policy and about its public-facing data tracking grant spending.</p><p>The N.I.H. since 1950 has provided these overhead funds in a formulaic way, and since 1965, the government has used a rate individually calculated for each institution. Federal officials review cost summaries, floor plans and other information to determine that rate. That number can be higher for institutions in more expensive parts of the country, or for those that use more energy-intensive equipment. The proposal from the Trump administration would set aside those differences in standardizing the rate at 15 percent for every grantee.</p><p>The lists below estimate what would have happened to the 10 universities and hospitals that received the most N.I.H. grant money in the 2024 fiscal year, if the formula change had been in effect then.</p><figure><div><div><div><h3>Largest N.I.H. grant recipients among colleges, universities and medical schools</h3></div></div></div><div><div><div><table><thead data-svelte-h=\"svelte-1s18ww\"><tr></tr></thead><tbody><tr><td><div>University of California, San Francisco</div></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td><div>University of Pennsylvania</div></td></tr><tr><td></td></tr><tr><td><div>Columbia University Health Sciences</div></td></tr><tr><td></td></tr><tr><td></td></tr><tr><td></td></tr></tbody></table></div></div></div><div><div><div><p>Source: National Institutes of Health</p><p>Based on spending in the 2024 fiscal year.</p></div></div></div></figure><figure><div><div><div><h3>Largest N.I.H. grant recipients among hospitals</h3></div></div></div><div><div><div><table><thead data-svelte-h=\"svelte-1s18ww\"><tr></tr></thead><tbody><tr><td><div>Massachusetts General Hospital</div></td></tr><tr><td><div>Vanderbilt University Medical Center</div></td></tr><tr><td><div>Brigham and Women’s Hospital</div></td></tr><tr><td><div>Boston Children’s Hospital</div></td></tr><tr><td><div>University of Texas MD Anderson Cancer Center</div></td></tr><tr><td><div>Children’s Hospital of Philadelphia</div></td></tr><tr><td><div>Dana-Farber Cancer Institute</div></td></tr><tr><td><div>Cincinnati Childrens Hospital Medical Center</div></td></tr><tr><td><div>Beth Israel Deaconess Medical Center</div></td></tr><tr><td><div>Cedars-Sinai Medical Center</div></td></tr></tbody></table></div></div></div><div><div><div><p>Source: National Institutes of Health</p><p>Based on spending in the 2024 fiscal year, which extends from Oct. 1 to Sept. 30.</p></div></div></div></figure><p>If courts allow the change to move forward, some of its consequences are hard to predict.</p><p>Advocates for the policy change note that these organizations receive numerous other federal subsidies. Most universities and research hospitals are nonprofits that pay no federal taxes, for example. The N.I.H. announcement also noted that these same institutions often accept grants from charitable foundations that offer much lower overhead rates than the federal government, a signal that universities and hospitals willingly pursue research opportunities with less supplemental funding.</p><p>Because the indirect payments are based on broad formulas and not specific line items, critics say institutions may be diverting these federal dollars into unaccountable funds to pay for programs that taxpayers can’t see, such as the kinds of diversity, equity and inclusion programs targeted by the Trump administration.</p><p>“That’s how you get things like the ability of administrators to use larger overhead pools of money to build out D.E.I. bureaucracies, or to fund Ph.D. programs in the humanities,” said Jay Greene, a senior research fellow in the Center for Education Policy at the Heritage Foundation, a conservative research group. Mr. Greene was the coauthor of <a href=\"https://www.heritage.org/education/report/indirect-costs-how-taxpayers-subsidize-university-nonsense\">a 2022 article</a> urging the N.I.H. to cut or eliminate indirect grant funding. But he did not have specific examples to cite of research funds being spent in this way.</p><p>Researchers say the indirect funds have a branding problem, but are a necessary component of research.</p><p>“The term ‘indirect costs’ or the alternative term ‘overhead’ sounds dangerously close to ‘slush fund’ to some people,” said Jeremy Berg, who was the director of the National Institute of General Medical Sciences at the N.I.H. from 2003 to 2011. “There are real costs somebody has to pay for, and heating and cooling university laboratory buildings is a real cost.”</p><p>Some grant recipients already receive low overhead payments, but a large majority of them currently receive more than 15 percent, meaning they will need to make budgetary changes to absorb the loss. Among the 2024 grants that we analyzed, institutions that received more than $1 million in N.I.H. support got an average of 40 cents of indirect funding for every dollar of direct funding.</p><figure aria-label=\"graphic\"><div><div><div><h3>Distribution of overhead funding at N.I.H.-funded institutions in 2024</h3><p>As a share of direct funding</p></div></div></div><div><div><div><p>Source: National Institutes of Health</p><p>Calculated for 613 institutions that received at least $1 million in funding in fiscal year 2024. Federally negotiated rates are higher than these.</p></div></div></div></figure><p>Universities and hospitals may adjust their overall budgets to keep supporting medical research by cutting back on other things they do. Some might be able to raise money from donors to fill the shortfalls, though most universities are already raising as much philanthropic money as they can.</p><p>But many research institutions have said they would adjust by simply doing less medical research, because they would not be able to afford to do as much with less government help.</p><p>Universities and hospitals might also shift the kinds of research they do, avoiding areas that require more lab space, regulatory compliance or high-tech equipment, and focusing on types of research that will require them to provide less overhead funding themselves. That may mean disproportionate reductions in complex areas of research like genetics.</p><p>Those effects may be spread unevenly across the research landscape, as some organizations find a way to adjust, while others abandon medical research altogether.</p><p>We’ve compiled a list of institutions that received at least $1 million in N.I.H. funding in the 2024 fiscal year, along with our estimates of how much less they would have gotten under the new policy. Most of these institutions are universities or hospitals, but there are also some private companies and nonprofit research groups. Our numbers tend to be underestimates of the cuts.</p><figure><div><div><div><p>To estimate changes in funding, we relied on data from RePORT, the N.I.H.’s online <a href=\"https://report.nih.gov/award/index.cfm?ot=&amp;fy=2024\">registry</a> of grants and projects. We limited our analysis to grants listed within the 50 U.S. states, the District of Columbia or Puerto Rico. We also limited it to grants where the amount of indirect funding was known and where the combined indirect and direct funding was within five percent of the listed total funding. These filters resulted in removing many grants to private organizations such as domestic for-profits.</p><p>We calculated how much indirect funding each grant would have received under the new guidance by multiplying the listed direct funding amount by 15 percent. We then compared that number to the listed indirect funding amount for each great to estimate the impact of the policy.</p><p>There are two reasons our calculations are most likely conservative estimates of true reductions in funding. First, only a portion of the direct funding for each grant is considered to be “eligible” for the purposes of calculating indirect funding. For example, laboratory equipment and graduate student tuition reimbursements are deducted from the direct costs before applying the negotiated overhead rate, whereas our calculations assumed 100 percent of the listed direct costs would be eligible. We performed a more accurate version of our calculations for the 10 universities and 10 hospitals receiving the most N.I.H. funds by inferring their eligible direct costs from their reported negotiated rates. When we did this, we saw an additional increase in losses of about 20 percent.</p><p>Second, we applied a 15 percent rate to all grants in the database, including those with an initial indirect rate  15 percent. An <a href=\"https://jamessmurphy.com/2025/02/09/the-impact-of-an-nih-15-indirect-cost-rate/\">analysis by James Murphy</a> helped inform this approach. According to our analysis, then, some grants would actually receive more money under the new guidance. If the new rate operated more like a cap — and grants with rates currently below 15 percent did not change — the overall reductions in funding would be larger, as the reductions would no longer be offset by some small number of funding increases.</p></div></div></div></figure>","contentLength":12025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43064637"},{"title":"FreeBSD 13.5 Overcomes UFS Y2038 Problem To Push It Out To Year 2106","url":"https://www.phoronix.com/news/FreeBSD-13.5-Beta-2","date":1739669445,"author":"Michael Larabel","guid":488,"unread":true,"content":"<article>Following last week's FreeBSD 13.5 Beta 1 release to kick off this next FreeBSD 13 point release that will also end the series, FreeBSD 13.5 Beta 2 is out this weekend for testing...</article>","contentLength":182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Sims Game Design Documents (1997)","url":"https://donhopkins.com/home/TheSimsDesignDocuments/","date":1739667971,"author":"krykp","guid":508,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43064273"},{"title":"Show HN: Blunderchess.net – blunder for your opponent every five moves","url":"https://blunderchess.net/","date":1739665321,"author":"eviledamame","guid":514,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43063970"},{"title":"Despite Plans for AI-Powered Search, Reddit's Stock Fell 14% Thsi Week","url":"https://tech.slashdot.org/story/25/02/16/007234/despite-plans-for-ai-powered-search-reddits-stock-fell-14-thsi-week?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739664540,"author":"EditorDavid","guid":481,"unread":true,"content":"\"Reddit Answers\" uses generative AI to answer questions using what past Reddittors have posted. Announced in December, Reddit now plans to integrate it into their search results, reports TechCrunch, with Reddit's CEO saying the idea has \"incredible monetization potential.\" \n\nAnd yet Reddit's stock fell 14% this week. CNBC's headline? \"Reddit shares plunge after Google algorithm change contributes to miss in user numbers.\"\n\n\nA Google search algorithm change caused some \"volatility\" with user growth in the fourth quarter, but the company's search-related traffic has since recovered in the first quarter, Reddit CEO Steve Huffman said in a letter to shareholders. \"What happened wasn't unusual — referrals from search fluctuate from time to time, and they primarily affect logged-out users,\" Huffman wrote. \"Our teams have navigated numerous algorithm updates and did an excellent job adapting to these latest changes effectively....\" Reddit has said it is working to convince logged-out users to create accounts as logged-in users, which are more lucrative for its business. \n\n\nAs Yahoo Finance once pointed out, Reddit knew this day would come, acknowledging in its IPO filing that \"changes in internet search engine algorithms and dynamics could have a negative impact on traffic for our website and, ultimately, our business.\" And in the last three months of 2024 Reddit's daily active users dropped, Yahoo Finance reported this week. But logged-in users increased by 400,000 — while logged-out users dropped by 600,000 (their first drop in almost two years). \n\nMarketwatch notes that analyst Josh Beck sees this as a buying opportunity for Reddit's stock:\nBeck pointed to comments from Reddit's management regarding a sharp recovery in daily active unique users. That was likely driven by Google benefiting from deeper Reddit crawling, by the platform uncollapsing comments in search results and by a potential benefit from spam-reduction algorithm updates, according to the analyst. \"While the report did not clear our anticipated bar, we walk away encouraged by international upside,\" he wrote.\n","contentLength":2110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Despite Plans for AI-Powered Search, Reddit's Stock Fell 14% This Week","url":"https://tech.slashdot.org/story/25/02/16/007234/despite-plans-for-ai-powered-search-reddits-stock-fell-14-this-week?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739664540,"author":"EditorDavid","guid":484,"unread":true,"content":"\"Reddit Answers\" uses generative AI to answer questions using what past Reddittors have posted. Announced in December, Reddit now plans to integrate it into their search results, reports TechCrunch, with Reddit's CEO saying the idea has \"incredible monetization potential.\" \n\nAnd yet Reddit's stock fell 14% this week. CNBC's headline? \"Reddit shares plunge after Google algorithm change contributes to miss in user numbers.\"\n\n\nA Google search algorithm change caused some \"volatility\" with user growth in the fourth quarter, but the company's search-related traffic has since recovered in the first quarter, Reddit CEO Steve Huffman said in a letter to shareholders. \"What happened wasn't unusual — referrals from search fluctuate from time to time, and they primarily affect logged-out users,\" Huffman wrote. \"Our teams have navigated numerous algorithm updates and did an excellent job adapting to these latest changes effectively....\" Reddit has said it is working to convince logged-out users to create accounts as logged-in users, which are more lucrative for its business. \n\n\nAs Yahoo Finance once pointed out, Reddit knew this day would come, acknowledging in its IPO filing that \"changes in internet search engine algorithms and dynamics could have a negative impact on traffic for our website and, ultimately, our business.\" And in the last three months of 2024 Reddit's daily active users dropped, Yahoo Finance reported this week. But logged-in users increased by 400,000 — while logged-out users dropped by 600,000 (their first drop in almost two years). \n\nMarketwatch notes that analyst Josh Beck sees this as a buying opportunity for Reddit's stock:\nBeck pointed to comments from Reddit's management regarding a sharp recovery in daily active unique users. That was likely driven by Google benefiting from deeper Reddit crawling, by the platform uncollapsing comments in search results and by a potential benefit from spam-reduction algorithm updates, according to the analyst. \"While the report did not clear our anticipated bar, we walk away encouraged by international upside,\" he wrote.\n","contentLength":2110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"China's 'Salt Typhoon' Hackers Continue to Breach Telecoms Despite US Sanctions","url":"https://it.slashdot.org/story/25/02/15/2244220/chinas-salt-typhoon-hackers-continue-to-breach-telecoms-despite-us-sanctions?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739659620,"author":"EditorDavid","guid":470,"unread":true,"content":"\"Security researchers say the Chinese government-linked hacking group, Salt Typhoon, is continuing to compromise telecommunications providers,\" reports TechCrunch, \"despite the recent sanctions imposed by the U.S. government on the group.\" \n\nTechRadar reports that the Chinese state-sponsored threat actor is \"hitting not just American organizations, but also those from the UK, South Africa, and elsewhere around the world.\"\n\n\n\n\nThe latest intrusions were spotted by cybersecurity researchers from Recorded Future, which said the group is targeting internet-exposed web interfaces of Cisco's IOS software that powers different routers and switches. These devices have known vulnerabilities that the threat actors are actively exploiting to gain initial access, root privileges, and more. More than 12,000 Cisco devices were found connected to the wider internet, and exposed to risk, Recorded Future further explained. However, Salt Typhoon is focusing on a \"smaller subset\" of telecoms and university networks. \n\n\"The hackers attempted to exploit vulnerabilities in at least 1,000 Cisco devices,\" reports NextGov, \"allowing them to access higher-level privileges of the hardware and change their configuration settings to allow for persistent access to the networks they're connected on... Over half of the Cisco appliances targeted by Salt Typhoon were located in the U.S., South America and India, with the rest spread across more than 100 countries.\"\nBetween December and January, the unit, widely known as Salt Typhoon, \"possibly targeted\" — based on devices that were accessed — offices in the University of California, Los Angeles, California State University, Loyola Marymount University and Utah Tech University, according to a report from cyber threat intelligence firm Recorded Future... The Cisco devices were mainly associated with telecommunications firms, but 13 of them were linked to the universities in the U.S. and some in other nations... \"Often involved in cutting-edge research, universities are prime targets for Chinese state-sponsored threat activity groups to acquire valuable research data and intellectual property,\" said the report, led by the company's Insikt Group, which oversees its threat research. \n\nThe cyberspies also compromised Cisco platforms at a U.S.-based affiliate of a prominent United Kingdom telecom operator and a South African provider, both unnamed, the findings added. The hackers also \"carried out a reconnaissance of multiple IP addresses\" owned by Mytel, a telecom operator based in Myanmar... \n\n\"In 2023, Cisco published a security advisory disclosing multiple vulnerabilities in the web UI feature in Cisco IOS XE software,\" a Cisco spokesperson said in a statement. \"We continue to strongly urge customers to follow recommendations outlined in the advisory and upgrade to the available fixed software release.\"\n\n","contentLength":2874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jellyfin: The Free Software Media System","url":"https://jellyfin.org/","date":1739659159,"author":"doener","guid":480,"unread":true,"content":"<div><p>Jellyfin is Free Software, licensed under the GNU GPL. You can use it, study it, modify it, build it, and distribute it for free, as long as your changes are licensed the same way.</p></div><div><p>The project relies entirely on contributions from volunteers. Want to help out? There’s lots of ways to do so, and you don’t even have to code! See our <a href=\"https://jellyfin.org/contribute\">contribution guide</a> for more details.</p></div><div><p>The Jellyfin server and official clients are free to download, now and always. There are no costs, hidden or otherwise, to use Jellyfin, either for yourself, for your friends, or for your company. All our incidental costs are paid through donations from users like you.</p></div><div><p>Jellyfin has no tracking, phone-home, or central servers collecting your data. We believe in keeping our software open and transparent. We’re also not in the media business, so the only media you see is your own.</p></div>","contentLength":855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43063167"},{"title":"North Carolina Amazon workers vote against unionizing","url":"https://techcrunch.com/2025/02/15/north-carolina-amazon-workers-vote-against-unionizing/","date":1739657733,"author":"Anthony Ha","guid":433,"unread":true,"content":"<p>Workers at an Amazon warehouse in Garner, North Carolina voted against unionizing in election results announced today. According to Carolina Amazonians United for Solidarity and Empowerment (CAUSE), the worker group seeking to form the union, 3,276 ballots were cast in the election, with 25.3% of votes in favor of unionizing and 74.7% against. The results […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Infocon: green","url":"https://isc.sans.edu/diary.html?rss","date":1739655603,"author":"","guid":419,"unread":true,"content":"<article>The Danger of IP Volatility</article>","contentLength":27,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The World's Most Printed 3D Model, 3DBenchy, Is Now Public Domain","url":"https://hardware.slashdot.org/story/25/02/15/1949206/the-worlds-most-printed-3d-model-3dbenchy-is-now-public-domain?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739655240,"author":"EditorDavid","guid":190,"unread":true,"content":"Hackaday reports:\n\nGood news for everyone who cannot get enough from improbably shaped boats that get referred to as a bench: the current owner (NTI Group) of the copyright has announced that 3DBenchy has been released into the public domain. This comes not too long after Prusa's Printables website had begun to purge all derived models to adhere to the 'no derivatives' license. According to NTI, the removal of these derived models was not requested by NTI, but by a third-party report, unbeknownst to NTI or the original creator of the model. Recognizing its importance to the community, 3DBenchy can now be downloaded &amp; modified freely. \n\n\nNTI worked together with the original creator [Daniel Norée] and former Creative Tools CEO [Paulo Kiefe] to transition 3DBenchy and the associated website to the public domain\n\nMore details at Tom's Hardware and Fabbaloo.","contentLength":867,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Watt The Fox?","url":"https://h.43z.one/blog/2025-02-12/","date":1739655133,"author":"h43z","guid":507,"unread":true,"content":"<p>It's not nothing—about 1.5 Watt more.</p><p>So, with a heavy heart, I decided to disable email notifications—even though I really wanted to keep them—but eliminating the white noise was my priority.</p><p>I thought I had “fixed” the problem. Of course, I blamed Microsoft, right?</p><p>But the red sound indicator on my i3 status bar kept lighting up occasionally.<p>\n    And it turned out other websites were also triggering the white noise.</p></p><p>For instance, as soon as I clicked anywhere on <a href=\"https://x.com\" target=\"_blank\">x.com</a>, the noise started. Similarly, whenever I listened to a translation on <a href=\"https://translate.google.com\" target=\"_blank\">translate.google.com</a>, there was the noise.</p><p>So now I was really curious. What is going on here.</p><p>I started to look up how you can play audio with HTML/JavaScript.\n    There seem to be two ways: Either with the  tag or the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API\" target=\"_blank\">WebAudio API</a>.</p><p>As Outlook plays sound dynamically, I knew it must use the WebAudio API.\n    And to do anything with audio, you first have to create an .</p><pre><code>const audioCtx = new AudioContext();</code></pre><p>And already here I realized the problem. Just creating this AudioContext makes my speakers play white noise.</p><p>The MDN article is pretty clear about it.</p><pre><code>AudioContext.suspend()\nSuspends the progression of time in the audio context, temporarily halting audio hardware access and reducing CPU/battery usage in the process.\n\nAudioContext.resume()\nResumes the progression of time in an audio context that has previously</code></pre><p>Yet, most websites never bother suspending the AudioContext and create one without the immediate need for playing sound.</p><p>Chrome stops the battery/CPU waste automatically afte some time. Firefox not. It just keeps playing the whitenoise.</p><p>I understand that the websites are to blame here.</p><p>But still, Cmon Firefox, protect me from this resource theft?!</p><p>Oh and btw I suspect this also wastes my bluetooth headphones battery if they are connected?! Once I do a click on x.com the sending of white noise starts.</p><p>To address this total mess, I created an extension that automatically suspends the AudioContext while also\ntries to resume it if the websites wants to play sound.</p><p>It's not perfect as resuming takes a little bit of time and it\n  may not always resume, as there are multiple paths to starting audio. But it's good enough for me.</p><p>Some Relevant Bugzilla reports</p>\n\nMore fun stuff at <a href=\"https://h.43z.one\">h.43z.one</a>.\nUnshadow ban me at <a href=\"https://x.com/h43z\">𝕏</a>","contentLength":2276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43062546"},{"title":"The European Vat Is Not a Discriminatory Tax Against US Exports","url":"https://taxfoundation.org/blog/trump-reciprocal-tariffs-eu-vat-discriminatory/","date":1739654434,"author":"dzogchen","guid":469,"unread":true,"content":"<p>The <a href=\"https://taxfoundation.org/research/federal-tax/trump-administration-tax-proposals/\">Trump</a> administration has once again floated the idea of “reciprocal” <a href=\"https://taxfoundation.org/research/all/federal/trump-tariffs-trade-war/\">tariffs</a> on foreign countries. While it is unclear what formula the administration will use to determine what is “reciprocal,” the intention of responding to foreign charges—real and perceived—is clear enough.</p><p>In the past, the administration has made general assertions about different  and nontariff barriers that American exporters face that should be rectified by “reciprocal” US tariffs. Trump commonly <a href=\"https://www.politico.eu/article/european-carmakers-crossfire-united-states-europe-trade-war/\">mentions</a> that the EU charges a 10 percent import  on US vehicles while the US only levies a 2.5 percent tariff on European cars coming into the US. Though one can certainly find examples of higher trade barriers abroad, the <a href=\"https://www.cato.org/commentary/tariff-myths-debunked?gad_source=1&amp;gclid=CjwKCAiAqrG9BhAVEiwAaPu5zgSuxH-XwbT7Kgb7FE6uvM7OAkMDDekbiXjFKgJwJ802UUqtLUJabhoC4ukQAvD_BwE\">overall tariff gap</a> between the US and its trading partners is <a href=\"https://www.linkedin.com/posts/fernando-mart%C3%ADn-711a5790_us-export-coverages-activity-7295353938608820224-aT4M?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA_TE_ABEkKJ4JUzvJbx21NNIBaNIUkAoX8\">relatively minor</a>—and any increase in US tariffs will ultimately be paid by US businesses and consumers.  </p><p>However, when discussing trade with the EU specifically, White House deputy chief of staff, Stephen Miller, <a href=\"https://www.realclearpolitics.com/video/2025/02/09/miller_trump_trade_action_against_europe_will_offset_eu_regulations_and_vat_taxes_on_american_products.html\" rel=\"nofollow\">added</a> a new policy grievance to the mix: <a href=\"https://taxfoundation.org/taxedu/glossary/value-added-tax-vat/\">value-added taxes (VAT)</a>.  </p><p>“Did you know when you ship a car from the US to Europe, if they let it in at all because they have many nontariff barriers, between the VAT and duties, that car is taxed at 30%? The German car—or a European car sent the America is taxed at 2.5%—or basically 0.”</p><p>His statement assumes that a VAT discriminates against American car exports like a tariff, and conversely, that the VAT rebate provided to European car producers exporting to the US constitutes a subsidy and the car then simply faces a tariff and no VAT. (It is worth noting that both a domestic automobile and a European car sold in the US would face US state .) </p><p> may seem like a compelling political argument to justify across-the-board tariffs on the EU, it instead reflects a complete misunderstanding of what a VAT is and how it works. Worse, it misplaces the blame for a lack of US competitiveness on the European VAT instead of reevaluating the flaws of both the US federal and state tax systems.  </p><h2><strong>What is VAT and how does it work for exported goods?</strong></h2><p><a href=\"https://taxfoundation.org/taxedu/glossary/value-added-tax-vat/\">VATs</a> are border-adjusted, meaning they rebate tax on exports and impose tax on imports. Despite the appearance of subsidizing exports and punishing imports, however, a border-adjusted VAT is trade neutral. A border adjusted tax leads to currency appreciation for the imposing country, which would make it cheaper to import goods, more expensive to export goods, and thus would cancel out the apparent benefits of the tax on imports and the rebate on exports.</p><p>If there is a complaint to be made about tax policy and implications for US competitiveness in Europe, it is about uncompetitive state sales tax structures in the US system that yield what is known as “.”</p><h2><strong>What is US sales tax and how does it work for exported goods? </strong></h2><p>Unlike most countries, the <a href=\"https://taxfoundation.org/location/united-states/\">United States</a> does not impose a broad-based  at the national (federal) level, and state-level consumption taxes are designed as general sales taxes rather than value-added taxes. Whereas a VAT is imposed on the incremental increase in value of a good or service at each stage of production, a <a href=\"https://taxfoundation.org/taxedu/glossary/sales-tax/\">sales tax</a> is imposed on the total transaction price of any taxed good or service.</p><p>If a sales tax is imposed exclusively on final consumption, then VATs and sales taxes are economically identical. However, when the sales tax is applied to some intermediate transactions (“business inputs”), it results in tax pyramiding, where the tax is embedded in the price multiple times over.</p><p>Consider the following example of a 5 percent VAT and two versions of a 5 percent sales tax—one which only applies to final consumption, and one which applies to certain intermediate transactions as well.</p><h4>VATs and Ideal Sales Taxes are Economically Identical</h4><h5><em>A 5% VAT compared to a 5% ideal sales tax and a 5% sales tax with business input taxation</em></h5><p>Note that, while a VAT is imposed at every stage of the process, the net effect is to apply the rate one time to the final sales price. The tax is collected in increments (on the “value added” at each stage), but unlike with a pyramiding sales tax, it does not double tax inputs. The VAT and ideal sales tax share an identical  and, if imposed at the same rates, yield identical collections.</p><p>US sales taxes are typically destination-based, meaning that the tax is owed where the product is received or consumed. If a European resident orders from a US retailer, they do not pay US sales tax, just like a US consumer can obtain a VAT rebate on purchases of European products. Neither is a subsidy. These are simply consumption taxes falling on the consumer.</p><p>In practice, however, US sales taxes diverge sharply from the ideal. More than 40 percent of US sales tax revenue comes from intermediate transactions, which impose costs on US producers. This design flaw is not present in VATs, which do not double-tax intermediate transactions. Consequently, the sales tax imposes a penalty on domestic production that a VAT (or a better designed sales tax) would not. European VATs aren’t subsidizing anything—US states are just shooting themselves in the foot.</p><p>Crucially, this is true in domestic as well as international sales. If a state’s sales tax only applied to final consumption, it would never put in-state businesses at a disadvantage against rivals in other states, because consumers elsewhere are subject to their own state’s sales tax. A <a href=\"https://taxfoundation.org/location/maryland/\">Maryland</a> resident pays 6 percent sales tax on whatever she orders (that’s subject to Maryland’s sales tax), regardless of whether she buys from a retailer in Maryland, or <a href=\"https://taxfoundation.org/location/delaware/\">Delaware</a> (with no sales tax), or <a href=\"https://taxfoundation.org/location/louisiana/\">Louisiana</a> (with an average rate north of 10 percent). But when Maryland taxes business inputs, that imposes a cost on Maryland businesses that could be mitigated if businesses operated in lower-tax states or in states which include fewer inputs in their tax base.</p><p>The disadvantages created by the sales tax, therefore, aren’t unique to goods exported abroad. They aren’t the consequence of trade policy, but of poor tax policy. Europe’s VATs are not tariffs and are not subsidizing European exports. Instead, US states’ poorly-designed sales taxes are harming their own businesses’ competitiveness—whether they’re selling down the street, across state lines, or around the world.</p><h2><strong>What competitiveness issues remain with the US federal tax system?</strong></h2><p>Just like state sales tax systems can create a competitive disadvantage for producers, certain elements of the federal income tax system harm incentives to invest domestically. Despite progress made by the 2017 Tax Cuts and Jobs Act, the US maintains long  schedules for structures investment, now requires amortization for research and development expenses, and is phasing out  for machinery and equipment investment. The absence of full, immediate deductions for investment increases the cost of capital, and thus discourages investment and wage growth.</p><p>Rather than focus on raising tariffs, which i<a href=\"https://taxfoundation.org/testimony/tariffs-alternatives-boost-us-competitiveness/\">ncrease the cost</a> of operating in the United States and <a href=\"https://taxfoundation.org/testimony/tariffs-alternatives-boost-us-competitiveness/\">reduce total output and productivity</a>, fiscal policy reforms to improve the structure of the federal income tax system can better boost competitiveness of the US manufacturing sector.</p><p>Countries have many reasons why they apply different tariff rates to different products. In the case of the United States, some tariffs date back to the 1930s Smoot-Hawley tariff schedule, while other US trade barriers take on non-tariff forms. The Trump administration appears to be moving in a “reciprocal” policy direction despite the significant negative economic consequences for American consumers of across-the-board tariffs on goods coming into the US. However, the EU’s VAT system should not be used as a justification for retaliatory tariffs. </p><div data-id=\"1\"><h2>Stay informed on the tax policies impacting you.</h2><p>Subscribe to get insights from our trusted experts delivered straight to your inbox.</p><a href=\"https://taxfoundation.org/tax-newsletter\">Subscribe</a></div>","contentLength":7923,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43062457"},{"title":"Design, Manufacturing and Open-Loop Control of a Soft Pneumatic Arm: Bending Experiments","url":"https://hackernoon.com/design-manufacturing-and-open-loop-control-of-a-soft-pneumatic-arm-bending-experiments?source=rss","date":1739653203,"author":"EScholar: Electronic Academic Papers for Scholars","guid":478,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing </p><p>4 Data Acquisition and Open-Loop Control </p><p>The first experiment consisted of analysing the deflection of a segment versus swelling time. For this purpose, one of the bladders was inflated continuously, in intervals of 100 ms. For each time, PAUL end coord</p><p>\\\nwhere x0 and y0 denote initial position of PAUL end.</p><p>\\\nSince the weight of the subsequent modules influences the behaviour of the first segment, the experiment was repeated by placing first one and then two additional segments. The results are shown in Figure 18.</p><p>\\\nAs can be seen, PAUL is capable of bending up to 40◦ to its vertical axis and the addition of new segments does not cause any noticeable decrease in its bending capacity.</p><p>\\\nAlthough it remains far from the 80◦ of [28] or the 70◦ of [32], Pneunet segments and therefore more flexible, this is an acceptable bending capacity. Moreover, the fact that it does not substantially lose its bending capacity by adding segments makes it possible to concatenate bending movements and thus overcome obstacles that a rigid robot would not be able to overcome.</p><p>\\\nIn conjunction with this, a validation test was proposed whose purpose was to demonstrate PAUL’s ability to flex thanks to its deformable geometry. The aim was to point points in lateral planes. The results of this experiment are shown in Figure 19. The images, extracted from the video of Appendix A, show how the manipulator can adopt different shapes, is able to bend up to 40◦ and adapt, in case of obstacles, to a wide variety of geometries, which undoubtedly makes PAUL a fundamental ally in inspection and exploration operations in very cluttered environments.</p>","contentLength":2317,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Intelligence could arrive on Vision Pro in April","url":"https://techcrunch.com/2025/02/15/apple-intelligence-could-arrive-on-vision-pro-in-april/","date":1739652968,"author":"Anthony Ha","guid":54,"unread":true,"content":"<p>Apple is planning to add Apple Intelligence to its Vision Pro headset in an update that could come as early as April, according to Bloomberg’s Mark Gurman. Just a couple weeks after Apple Intelligence was first announced in June 2024, Gurman reported that Apple was looking to bring its suite of AI tools to the […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"America's Office-Occupancy Rates Drop by Double Digits - and More in San Francisco","url":"https://it.slashdot.org/story/25/02/15/1716204/americas-office-occupancy-rates-drop-by-double-digits---and-more-in-san-francisco?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739651640,"author":"EditorDavid","guid":189,"unread":true,"content":"SFGate shares the latest data on America's office-occupancy rates:\n\nAccording to Placer.ai's January 2025 Office Index, office visits nationwide were 40.2% lower in January 2025 compared with pre-pandemic numbers from January 2019. \n\nBut San Francisco is dragging down the average, with a staggering 51.8% decline in office visits since January 2019 — the weakest recovery of any major metro. Kastle's 10-City Daily Analysis paints an equally grim picture. From Jan. 23, 2025, to Jan. 28, 2025, even on its busiest day (Tuesday), San Francisco's office occupancy rate was just 53.7%, significantly lower than Houston's (74.8%) and Chicago's (70.4%). And on Friday, Jan. 24, office attendance in [San Francisco] was at a meager 28.5%, the worst of any major metro tracked... \n\n\nMeanwhile, other cities are seeing much stronger rebounds. New York City is leading the return-to-office trend, with visits in January down just 19% from 2019 levels, while Miami saw a 23.5% decline, per Placer.ai data. \n\n\"Placer.ai uses cellphone location data to estimate foot traffic, while Kastle Systems measures badge swipes at office buildings with its security systems...\"","contentLength":1159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NASA has a list of 10 rules for software development","url":"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm","date":1739651053,"author":"vyrotek","guid":479,"unread":true,"content":"<h2>NASA has a list of 10 rules for software development</h2><p>Those rules were written from the point of view of people writing\nembedded software for extremely expensive spacecraft, where tolerating\na lot of programming pain is a good tradeoff for not losing a mission.\nI do not know why someone in that situation does not use the SPARK\nsubset of Ada, which subset was explicitly designed for verification,\nand is simply a better starting point for embedded programming than C.\n</p><p>I am criticising them from the point of view of people writing\nprogramming language processors (compilers, interpreters, editors)\nand application software.\n</p><p>We are supposed to teach critical thinking.  This is an example.\n</p><ul><li>How have Gerard J. Holzmann's and my different contexts affected\nour judgement?\n</li><li>Can you blindly follow his advice without considering \ncontext?\n</li><li>Can you blindly follow  advice without considering\nyour context?\n</li><li>Would these rules necessarily apply to a different/better\nprogramming language?  What if <a href=\"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm#ppar\">function pointers\nwere tamed</a>?  What if the language provided opaque abstract\ndata types as Ada does?\n</li></ul><h3>1. Restrict all code to very simple control flow constructs —\ndo not use  statements,\n or  constructs,\nand direct or indirect .</h3><p>Note that  and \nare how C does exception handling, so this rule bans any use\nof exception handling.\n\n</p><p>It is true that banning recursion and jumps and loops without\nexplicit bounds means that you  your program is\ngoing to terminate.  It is also true that recursive functions\ncan be proven to terminate about as often as loops can, with\nreasonably well-understood methods.  What's more important here is\nthat “sure to terminate” does not imply\n“sure to terminate in my lifetime”:\n</p><pre>    int const N = 1000000000;\n    for (x0 = 0; x0 != N; x0++)\n    for (x1 = 0; x1 != N; x1++)\n    for (x2 = 0; x2 != N; x2++)\n    for (x3 = 0; x3 != N; x3++)\n    for (x4 = 0; x4 != N; x4++)\n    for (x5 = 0; x5 != N; x5++)\n    for (x6 = 0; x6 != N; x6++)\n    for (x7 = 0; x7 != N; x7++)\n    for (x8 = 0; x8 != N; x8++)\n    for (x9 = 0; x9 != N; x9++)\n        -- do something --;\n</pre><p>This does a bounded number of iterations.  The bound is N.\nIn this case, that's 10.  If each iteration of the loop body\ntakes 1 nsec, that's 10 seconds, or about 7.9×10\nyears.  What is the  difference between “will stop\nin 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000\nyears” and “will never stop”?\n\n</p><p>Worse still, taking a problem that is  expressed\nusing recursion and contorting it into something that manipulates an\nexplicit stack, while possible, turns clear maintainable code into\nbuggy spaghetti.  (I've done it, several times.  There's an example\non this web site.  It is  a good idea.)\n\n</p><h3>2. All loops must have a fixed upper-bound.  It must be trivially\npossible for a checking tool to prove statically that a preset\nupper-bound on the number of iterations of a loop cannot be exceeded.\nIf the loop-bound cannot be proven statically, the rule is considered\nviolated.</h3><p>This is an old idea.  As the example above shows, it is not enough\nby itself to be of any practical use.  You have to try to make the\nbounds reasonably , and you have to regard hitting an\nartificial bound as a run-time error.\n\n</p><p>By the way, note that putting depth bounds on recursive procedures\nmakes them every bit as safe as loops with fixed bounds.\n\n</p><h3>3. Do not use dynamic memory allocation after initialization.</h3><p>This is also a very old idea.  Some languages designed for embedded\nwork don't even  dynamic memory allocation.  The big\nthing, of course, is that embedded applications have a fixed amount of\nmemory to work with, are never going to get any more, and should not\ncrash because they couldn't handle another record.\n\n</p><p>Note that the rationale actually supports a much stronger rule:\ndon't even  dynamic memory allocation.  You can of\ncourse manage your own storage pool:\n</p><pre>    typedef struct Foo_Record *foo;\n    struct Foo_Record {\n\tfoo next;\n\t...\n    };\n    #define MAX_FOOS ...\n    static struct Foo_Record foo_zone[MAX_FOOS];\n    foo foo_free_list = 0;\n\n    void init_foo_free_list() {\n\tfor (int i = MAX_FOOS - 1; i &gt;= 0; i--) {\n\t    foo_zone[i].next = foo_free_list;\n\t    foo_free_list = &amp;foo_zone[i];\n\t}\n    }\n\n    foo malloc_foo() {\n\tfoo r = foo_free_list;\n\tif (r == 0) report_error();\n\tfoo_free_list = r-&gt;next;\n\treturn r;\n    }\n\n    void free_foo(foo x) {\n\tx-&gt;next = foo_free_list;\n\tfoo_free_list = x;\n    }\n</pre><p>This  satisfies the rule, but it\nviolates the  of the rule.  Simulating malloc()\nand free() this way is  than using the real\nthing, because the memory in foo_zone is permanently tied up\nfor Foo_Records, even if we don't need any of those at the\nmoment but do desperately need the memory for something else.\n\n</p><p>What you really need to do is to use a memory allocator\nwith known behaviour, and to prove that the amount of memory\nin use at any given time (data bytes + headers) is bounded\nby a known value.\n\n</p><p>Note also that SPlint can verify at compile time that\nthe errors NASA speak of do not occur.\n\n</p><p>One of the reasons given for the ban is that the performance\nof malloc() and free() is unpredictable.  Are these the only\nfunctions we use with unpredictable performance?  Is there\nanything about malloc() and free() which makes them\n unpredictable?  The existence of\nhard-real-time garbage collectors suggests not.\n\n</p><p>The rationale for this rule says that\n</p><blockquote>\nNote that the only way\nto dynamically claim memory in the absence of memory allocation from the\nheap is to use stack memory.  In the absence of recursion (Rule 1), an\nupper bound on the use of stack memory can derived statically, thus\nmaking it possible to prove that an application will always live within\nits pre-allocated memory means.\n</blockquote><p>Unfortunately, the sunny optimism shown here is unjustified.  Given\nthe ISO C standard (any version, C89, C99, or C11) it is \nto determine an upper bound on the use of stack memory.  There is not even\nany standard way to determine how much memory a compiler will use for the\nstack frame of a given function.  (There could have been.  There just isn't.)\nThere isn't even any requirement that two invocations of the same function\nwith the same arguments will use the same amount of memory.\nSuch a bound can only be calculated for a  version of a\nspecific compiler with specific options.  Here's a trivial example:\n</p><pre>void f() {\n    char a[100000];\n}\n</pre><p>How much memory will that take on the stack?  Compiled for debugging,\nit might take a full stack frame (however big that is) plus traceback\ninformation plus a million bytes for a[].  Compiled with optimisation,\nthe compiler might notice that a[] isn't used, and might even compile\ncalls to f() inline so that they generate no code and take no space.\nThat's an extreme example, but not really unfair.  If you want bounds\nyou can rely on, you had better  what your compiler does,\nand recheck every time anything about the compiler changes.\n\n</p><h3>4.  No function should be longer than what can be printed on\na single sheet of paper in a standard reference format with one line per\nstatement and one line per declaration.  Typically, this means no more\nthan about 60 lines of code per function.</h3><p>Since programmers these days typically read their code on-screen,\nnot on paper, it's not clear why the size of a sheet of paper is\nrelevant any longer.\n\n</p><p>The rule is arguably stated about the wrong thing.  The thing that\nneeds to be bounded is not the size of a function, but the size of a\nchunk that a programmer needs to read and comprehend.\n\n</p><p>There are also question marks about how to interpret this if you\nare using a sensible language (like Algol 60, Simula 67, Algol 68,\nPascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,\nF#, Clean, Haskell, or Fortran) that allows nested procedures.\nSuppose you have a folding editor that presents a procedure to\nyou like this:\n</p><pre>function Text_To_Floating(S: string, E: integer): Double;\n   � variables �\n   � procedure Mul(Carry: integer) �\n   � function Evaluate: Double �\n\n   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;\n   for N := 1 to S.length do begin\n       C := S[N];\n       if C = '.' then begin\n          Point := -1\n       end else\n       if C = '_' then begin\n          Base := Round(Evaluate);\n          Max, Min, Power := 0, 1, 0\n       end else\n       if Char ≠ ' ' then begin\n          Q := ord(C) - ord('0');\n          if Q &gt; 9 then Q := ord(C) - ord('A') + 10\n          Power := Point + Point\n          Mul(Q)\n       end\n    end;\n    Power := Power + Exp;\n    Value := Evaluate;\n    if Sign &lt; 0 then Value := -Value;\nend;\n</pre><p>which would be much bigger if the declarations\nwere expanded out instead of being hidden behind �folds�.\nWhich size do we count?  The folded size or the unfolded size?\n</p><p>I was using a folding editor called Apprentice on the Classic Mac\nback in the 1980s.  It was written by Peter McInerny and was lightning\nfast.\n\n</p><h3>5.  The  of the code should average to a minimum of\ntwo assertions per function.</h3><p>Assertions are wonderful documentation and the very best debugging tool\nI know of.  I have never seen any real code that had too many assertions.\n\n</p><p>The example here is one of the ugliest pieces of code I've seen in a while.\n</p><pre>if (!c_assert(p &gt;= 0) == true) {\n    return ERROR;\n}\n</pre><p>It should, of course, just be\n</p><pre>if (!c_assert(p &gt;= 0)) {\n    return ERROR;\n}\n</pre><p>Better still, it should be something like\n</p><pre>#ifdef NDEBUG\n#define check(e, c) (void)0\n#else\n#define check(e, c) if (!(c)) return bugout(c), (e)\n#ifdef NDEBUG_LOG\n#define bugout(c) (void)0\n#else\n#define bugout(c) \\\n    fprintf(stderr, \"%s:%d: assertion '%s' failed.\\n\", \\\n    __FILE__, __LINE__, #s)\n#endif\n#endif\n</pre><p>Ahem.  The more interesting part is the required density.\nI just checked an open source project from a large telecoms\ncompany, and 23 out of 704 files (not functions) contained\nat least one assertion.  I just checked my own Smalltalk\nsystem and one SLOC out of every 43 was an assertion, but\nthe average Smalltalk “function” is only a few\nlines.  If the biggest function allowed is 60 lines, then\nlet's suppose the average function is about 36 lines, so\nthis rule requires 1 assertion per 18 lines.\n</p><p>Assertions are good, but what they are especially good\nfor is expressing the requirements on data that come\nfrom outside the function.  I suggest then that\n</p><ul><li>Every argument whose validity is not guaranteed by\nits typed should have an assertion to check it.\n</li><li>Every datum that is obtained from an external\nsource (file, data base, message) whose validity is\nnot guaranteed by its type should have an assertion\nto check it.\n</li></ul><p>The NASA 10 rules are written for embedded systems, where\nreading stuff from sensors is fairly common.\n\n</p><h3>6.  Data objects must be declared at the smallest possible level of\nscope.</h3><p>This is excellent advice, but why limit it to data objects?\nOh yeah, the rules were written for crippled languages where you\n declare functions in the right place.\n\n</p><p>People using Ada, Pascal (Delphi), JavaScript, or functional\nlanguages should also declare types and functions as locally as\npossible.\n\n</p><h3>7.  The return value of non-void functions must be checked by each\ncalling function, and the validity of parameters must be checked inside\neach function.</h3><p>This again is mainly about C, or any other language that indicates\nfailure by returning special values.  “Standard libraries\nfamously violate this rule”?  No, the  library does.\n\n</p><p>You have to be reasonable about this: it simply isn't practical\nto check  aspect of validity for \nargument.  Take the C function\n</p><pre>void *bsearch(\n    void const *key  /* what we are looking for */,\n    void const *base /* points to an array of things like that */,\n    size_t      n    /* how many elements base has */,\n    size_t      size /* the common size of key and base's elements */\n    int (*      cmp)(void const *, void const *)\n);\n</pre><p>This does a binary search in an array.  We must have key≠0,\nbase≠0, size≠0, cmp≠0, cmp(key,key)=0, and for all\n1&lt;i&lt;n,\n</p><pre>cmp((char*)base+size*(i-1), (char*)base+size*i) &lt;= 0\n</pre><p>Checking the validity in full would mean checking\nthat [key..key+size) is a range of readable addresses,\n[base..base+size*n) is a range of readable addresses,\nand doing n calls to cmp.  But the whole point of binary\nsearch is to do O(log(n)) calls to cmp.\n\n</p><p>The fundamental rules here are\n</p><ul><li>Don't let run-time errors go un-noticed, and\n</li><li>any check is safer than no check.\n</li></ul><h3>8. The use of the preprocessor must be limited to the inclusion of\nheader files and simple macro definitions.  Token pasting, variable\nargument lists (ellipses), and recursive macro calls are not allowed.</h3><p>Recursive macro calls don't really work in C, so no quarrel there.\nVariable argument lists were introduced into macros in\nC99 so that you could write code like\n</p><pre>#define err_printf(level, ...) \\\n    if (debug_level &gt;= level) fprintf(stderr, __VA_ARGS__)\n...\n    err_printf(HIGH, \"About to frob %d\\n\", control_index);\n</pre><p>This is a  thing; conditional tracing like this is a\npowerful debugging aid.  It should be , not banned.\n\n</p><p>The rule goes on to ban macros that expand into things that are\nnot complete syntactic units.  This would, for example, prohibit\nsimulating try-catch blocks with macros.  (Fair enough, an earlier rule\nbanned exception handling anyway.)  Consider this code fragment, from\nan actual program.\n</p><pre>    row_flag = border;     \n    if (row_flag) printf(\"\\\\hline\");\n    for_each_element_child(e0, i, j, e1)\n        printf(row_flag ? \"\\\\\\\\\\n\" : \"\\n\");\n        row_flag = true;  \n        col_flag = false;\n        for_each_element_child(e1, k, l, e2)\n            if (col_flag) printf(\" &amp; \");\n            col_flag = true;\n            walk_paragraph(\"\", e2, \"\");\n        end_each_element_child\n    end_each_element_child\n    if (border) printf(\"\\\\\\\\\\\\hline\");\n    printf(\"\\n\\\\end{tabular}\\n\");\n</pre><p>It's part of a program converting slides written in something like HTML\ninto another notation for formatting.  The \n…  loops walk over a tree.  Using\nthese macros means that the programmer has no need to know and no reason to\ncare how the tree is represented and how the loop actually works.\nYou can easily see that  must have at\nleast one unmatched { and  must have at least one\nunmatched }.  That's the kind of macro that's banned by requiring\ncomplete syntactic units.  Yet the readability and maintainability of\nthe code is  improved by these macros.\n\n</p><p>One thing the rule covers, but does not at the beginning stress, is\n“no  macro processing”.  That is,\nno #if.  The argument against it is, I'm afraid, questionable.  If there\nare 10 conditions, there are 2 combinations to test,\nwhether they are expressed as compile-time conditionals or run-time\nconditionals.\n\n</p><p>In particular, the rule against conditional macro processing\nwould prevent you defining your own <a href=\"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm#check\">assertion macros</a>.\nIt is not obvious that that's a good idea.\n\n</p><h3>9.  The use of pointers should be restricted.  Specifically, no more\nthan one level of dereferencing is allowed.  Pointer dereference\noperations may not be hidden in macro definitions or inside typedef\ndeclarations.  Function pointers are not permitted.</h3><p>Let's look at the last point first.\n\n</p><pre>double integral(double (*f)(double), double lower, double upper, int n) {\n    // Compute the integral of f from lower to upper \n    // using Simpson's rule with n+1 points.\n    double const h = (upper - lower) / n;\n    double       s;\n    double       t;\n    int          i;\n    \n    s = 0.0;\n    for (i = 0; i &lt; n; i++) s += f((lower + h/2.0) + h*i);\n    t = 0.0;\n    for (i = 1; i &lt; n; i++) t += f(lower + h*i);\n    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);\n}\n</pre><p>This kind of code has been important in numerical calculations since\nthe very earliest days.  Pascal could do it.  Algol 60 could do it.\nIn the 1950s, Fortran could do it.  And NASA would ban it, because in\nC,  is a function pointer.\n\n</p><p>Now it's important to write functions like this once and only once.\nFor example, the code has at least one error.  The comment says n+1\npoints, but the function is actually evaluated at 2n+1 points.  If we\nneed to bound the number of calls to f in order to meet a deadline,\nhaving that number off by a factor of two will not help.\n</p><p>It's nice to have just one place to fix.\nPerhaps I should not have copied that code from a well-known source (:-).\nCertainly I should not have more than one copy!\n\n</p><p>What can we do if we're not allowed to use function pointers?\nSuppose there are four functions foo, bar, ugh, and zoo that we need\nto integrate.  Now we can write\n</p><pre>enum Fun {FOO, BAR, UGH, ZOO};\n\ndouble call(enum Fun which, double what) {\n    switch (which) {\n        case FOO: return foo(what);\n        case BAR: return bar(what);\n        case UGH: return ugh(what);\n        case ZOO: return zoo(what);\n    }\n}\n\ndouble integral(enum Fun which, double lower, double upper, int n) {\n    // Compute the integral of a function from lower to upper \n    // using Simpson's rule with n+1 points.\n    double const h = (upper - lower) / n;\n    double       s;\n    double       t;\n    int          i;\n    \n    s = 0.0;\n    for (i = 0; i &lt; n; i++) s += call(which, (lower + h/2.0) + h*i);\n    t = 0.0;\n    for (i = 1; i &lt; n; i++) t += call(which, lower + h*i);\n    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);\n}\n</pre><p>Has obeying NASA's rule made the code more reliable?  No, it has made\nthe code  to understand,  maintainable, and\n that it wasn't before.  Here's a call\nillustrating the mistake:\n</p><pre>x = integral(4, 0.0, 1.0, 10);</pre><p>I have checked this with two C compilers and a static checker at their\nhighest settings, and they are completely silent about this.\n\n</p><p>So there are legitimate uses for function pointers, and simulating\nthem makes programs , not better.\n\n</p><p>Now  in Fortran,\nAlgol 60, or Pascal.  Those languages had procedure \nbut not procedure . You could pass a subprogram name as\na parameter, and such a parameter could be passed on, but you could not\nstore them in variables.  You could have a  of C which\nallowed function pointer parameters, but made all function pointer\nvariables read-only.  That would give you a statically checkable subset\nof C that allowed integral().\n\n</p><p>The other use of function pointers is simulating object-orientation.\nImagine for example\n</p><pre>struct Channel {\n    void (*send)(struct Channel *, Message const *);\n    bool (*recv)(struct Channel *, Message *);\n    ...\n};\ninline void send(struct Channel *c, Message const *m) {\n    c-&gt;send(c, m);\n}\ninline bool recv(struct Channel *c, Message *m) {\n    return c-&gt;recv(c, m);\n}\n</pre><p>This lets us use a common interface for sending and receiving\nmessages on different kinds of channels.  This approach has been\nused extensively in operating systems (at least as far back as\nthe Burroughs MCP in the 1960s) to decouple the code that uses\na device from the actual device driver.     I would expect any\nprogram that controls more than one hardware device to do something\nlike this.  It's one of our key tools for controlling complexity.\n</p><p>Again, we can simulate this, but it makes adding a new kind of\nchannel harder than it should be, and the code is \nwhen we do it, not better.\n\n</p><p>The rule against more than one level of dereferencing is also\nan assault on good programming.  One of the key ideas that was\ndeveloped in the 1960s is the idea of ;\nthe idea that it should be possible for one module to define a\ndata type and operations on it and another module to use instances\nof that data type and its operations <em>without having to know\nanything about what the data type is</em>.\n</p><p>One of the things I detest about Java is that it spits in the\nface of the people who worked out that idea.  Yes, Java (now) has\ngeneric type parameters, and that's good, but you cannot use a\n type without knowing what that type is.\n\n</p><p>Suppose I have a module that offers operations\n</p><ul></ul><p>And suppose that I have two interfaces in mind.  One of them\nuses integers as tokens.\n</p><pre>// stasher.h, version 1.\ntypedef int token;\nextern token stash(item);\nextern item  recall(token);\nextern void  delete(token);\n</pre><p>Another uses pointers as tokens.\n</p><pre>// stasher.h, version 2.\ntypedef struct Hidden *token;\nextern  token stash(item);\nextern  item  recall(token);\nextern  void  delete(token);\n</pre><pre>void snoo(token *ans, item x, item y) {\n    if (better(x, y)) {\n\t*ans = stash(x);\n    } else {\n\t*ans = stash(y);\n    }\n}\n</pre><p>By the NASA rule, the function snoo() would not be accepted or rejected on\nits own merits.  With stasher.h, version 1, it would be accepted.\nWith stasher.h, version 2, it would be rejected.\n\n</p><p>One reason to prefer version 2 to version 1 is that version 2 gets\nmore use out of type checking.  There are ever so many ways to get an\nint in C.  Ask yourself if it ever makes sense to do\n</p><pre>token t1 = stash(x);\ntoken t2 = stash(y);\ndelete(t1*t2);\n</pre><p>I really do not like the idea of banning abstract data types.\n\n</p><h3>10.  All code must be compiled, from the first day of development,\nwith all compiler warnings enabled at the compiler’s\nmost pedantic setting.  All code must compile with these setting without\nany warnings.  All code must be checked daily with at least one, but\npreferably more than one, state-of-the-art static source code analyzer\nand should pass the analyses with zero warnings.</h3><p>This one is good advice.  Rule 9 is really about making your code\nworse in order to get more benefit from limited static checkers.  (Since\nC has no standard way to construct new functions at run time, the set of\nfunctions that a particular function pointer  point to can\nbe determined by a fixed-point data flow analysis, at least for most\nprograms.)  So is rule 1.  \n\n\n\n</p>","contentLength":21484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061977"},{"title":"Soft Robots and Smart Movement","url":"https://hackernoon.com/soft-robots-and-smart-movement?source=rss","date":1739650503,"author":"EScholar: Electronic Academic Papers for Scholars","guid":477,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing </p><p>4 Data Acquisition and Open-Loop Control </p><p>The size of the table to achieve acceptable kinematic modelling was set experimentally, as no previous references were available and previous works in the literature were very variable in terms of the number of data required. Furthermore, the possibility of an error occurring in the pneumatic system or the buffer of the vision acquisition system collapsing, together with the always present possibility of a leak in the segments, made it advisable to take data in small sessions and subsequently union of all of them. Since the data collection process was automated, this did not pose much of a problem.</p><p>\\\nAlthough the possibility that the ambient temperature was a factor that influenced the kinematics of the robot was considered during the dataset collection process, it was finally proven that small variations in temperature did not affect the behaviour.</p><p>\\\nTable 3 shows the datasets that were taken, the total time necessary to obtain them and the average time per point. It should be taken into account that not all captured positions were finally used, since, if the camera did not correctly detect the positions of the three beacon spheres, it could not calculate the orientation of the trihedral and therefore returned an error code. Of the 1200 samples collected, 5% had to be discarded, leaving 1146 finally usable. The average time per point, considering all the datasets collected, was 6.76 s, with a standard deviation of 0.63 s. The low variability between capture processes proves the effectiveness of the automated method designed.</p><p>\\\nOnce all the datasets were combined, the direct and inverse kinematic models presented here were validated. The validation of the direct model consisted of sending the robot a combination of inflation times and measuring the distance between the position reached, captured by the cameras, and that predicted by the table. Repeating the experiment for 40 points, the histogram of results presented in Figure 15 was observed. The average error is 4.27 mm, the median error 2.72 mm, and the standard deviation 1.99 mm.</p><p>\\\nThe high standard deviation and the shape of the histogram, tilted towards low values and with a very long tail, seem to indicate the existence of points where the model presents notable failures along with others with very good results. A future line of interest could be the detailed analysis of the workspace to locate where those regions of lower precision of the model are located and try to look for failures, perhaps leading to a greater density of points in the dataset.</p><p>\\\nIn the same way, the inverse kinematic model was tested. To do this, PAUL was given a reference position and orientation to achieve, the necessary times were calculated, using the procedures referred to in Equations (16), and (17) inflation was carried out. Subsequently, the position captured with the cameras was compared with the desired one.</p><p>\\\nAs expected, the existence of redundancies, in which equal position values are achieved with very different combinations of inflation times, introduces large uncertainties in the model, which the triangulation presented is not able to capture.</p><p>\\\nSpecifically, the inverse kinematic model has an average error of 10.78 mm, a median error of 9.22 mm and a standard deviation of 5.98 mm. While these errors may seem high, they are compared in Table 4 with other open-loop controllers presented in the literature. It can be clearly seen that they are in line with the results obtained and that they are even better than those obtained by smaller robots, where one would expect, due to the smaller working space, a higher accuracy (at least in data-driven models).</p><p>\\\nIt is worth highlighting, however, two experiments in which PAUL performed very satisfactorily, because the area of operation was restricted to a region where no redundancies were found to exist. They are available in the video of Appendix A.</p><p>\\\nIn the first of them, the robot was forced to reach a set of points located on the horizontal basis plane, also forcing the lower end of the last segment to be parallel to said</p><p>\\\nplane. In all of them, errors less than 7 mm were achieved. Figure 16 shows the results of said experiment. With the aim of facilitating the understanding of the experiment, the beacon was changed for a laser pointer that points to the desired points, on which targets with a radius of 5 mm have been marked, allowing the accuracy achieved to be checked.</p><p>\\\nIn the second experiment, shown in Figure 17, points on the lower horizontal plane were also taken as reference, but without imposing that the lower face of the robot should remain parallel to it. In this case, accuracies of 2 cm were achieved.</p><p>\\\nAlthough the inverse kinematic model therefore presents acceptable results, it must be commented that, in all these experiments, due to the geometry and material of the robot, at the moment in which PAUL reaches the desired position, it tends to acquire a movement damped oscillation. An attempt has been made to reduce it, despite everything, it is a very intrinsic phenomenon to the robot that is difficult to solve. A future line proposed, in this sense, is to try to rigidify the robot by introducing negative pressures that generate vacuum.</p>","contentLength":5938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity Deep Research","url":"https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research","date":1739650059,"author":"vinni2","guid":506,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061827"},{"title":"This Week In Techdirt History: February 9th – 15th","url":"https://www.techdirt.com/2025/02/15/this-week-in-techdirt-history-february-9th-15th/","date":1739649600,"author":"Leigh Beadon","guid":310,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Life in Weeks","url":"https://weeks.ginatrapani.org/","date":1739648069,"author":"bookofjoe","guid":468,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061498"},{"title":"Apple Invites Its Users Into Major Years-Long Health Study","url":"https://apple.slashdot.org/story/25/02/15/0610248/apple-invites-its-users-into-major-years-long-health-study?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739648040,"author":"EditorDavid","guid":188,"unread":true,"content":"Can the iPhone, AirPods, or the Apple Watch play a role in improving health? Apple says they want to find out. \n\"In medical research, discoveries are often limited by the number of participants who can be recruited, the amount of data that can be captured, and the duration of a given study,\" the company said in a blog post this week. \"But Apple devices expand the possibilities...\"\nThis new longitudinal, virtual study aims to understand how data from technology — including Apple and third-party devices — can be used to predict, detect, monitor, and manage changes in participants' health. Additionally, researchers will explore connections across different areas of health. \nCNBC reports:\n\n\nThe new study will likely influence future product development. Apple CEO Tim Cook previously said he believes health features will be the company's \"most important contribution to mankind....\" \n\nThe Apple Health Study will be available through the company's Research app, and participation is voluntary. Users will select each data type they are willing to share with researchers, and they can stop sharing or completely discontinue their participation at any time. Apple has no access to participants' identifiable information, the company said... The project will last at least five years and may expand beyond that. \nA Harvard Medical School professor and cardiologist — also a principal investigator on the Apple Health Study — says \"We've only just begun to scratch the surface of how technology can improve our understanding of human health.\"","contentLength":1553,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New SF public health chief was part of McKinsey opioid-marketing operation","url":"https://sfstandard.com/2025/02/14/san-francisco-department-public-health-daniel-tsai-opioids-mckinsey/","date":1739647936,"author":"iancmceachern","guid":467,"unread":true,"content":"<div><p>Dr. David Juurlink, an expert on tramadol, called the drug a “minor player in the opioid crisis, but a player nevertheless.”</p></div><div><p>He added, “To the extent that McKinsey helped advertise it as a notionally safer opioid, I think they did a disservice in doing so. The main reason I say tramadol is a minor player is because it wasn’t prescribed like candy, like OxyContin was.”</p></div><div><p>McKinsey’s method of targeting high-volume prescribers was part of its playbook to juice opioid sales, despite mounting evidence that the drugs could be highly addictive. In other emails uncovered in the McKinsey documents, employees wrote excitedly about finding doctors who were willing to write opioid prescriptions. <a href=\"https://www.industrydocuments.ucsf.edu/opioids/docs/#id=hlmn0255\">In one instance</a> in 2015, a McKinsey partner wrote, “The challenge which we need to start working on is to identify the sweet spot of docs so we can do targeting. … Fun be[g]ins on Monday!”</p></div><div><p>San Francisco’s fentanyl crisis is part of a broader trend of opioid overdoses <a href=\"https://www.cdc.gov/overdose-prevention/about/understanding-the-opioid-overdose-epidemic.html\">that traces back to the 1990s,</a> when prescription opioids became popular among doctors for chronic pain management. Companies such as Purdue Pharma, which manufactured OxyContin, brought in consulting firms like McKinsey to help with sales strategies. After a Department of Justice probe and settlement, McKinsey <a href=\"https://www.justice.gov/usao-ma/media/1380236/dl\">acknowledged that it knew</a> the dangers of OxyContin but continued working with Purdue Pharma — even after several of the drugmaker’s executives pled guilty in 2007 to misrepresenting addiction risks.&nbsp;</p></div><div><p>McKinsey, along with a slew of drug companies and pharmacies, agreed to pay billions in settlement funds over their roles in fueling opioid addiction. California received <a href=\"https://oag.ca.gov/news/press-releases/attorney-general-becerra-announces-573-million-nationwide-settlement-mckinsey\">roughly $60 million</a> from the 2021 McKinsey settlement. San Francisco, <a href=\"https://www.sfcityattorney.org/2023/05/17/san-francisco-city-attorney-announces-230-million-settlement-with-walgreens-after-victory-in-opioid-litigation/\">under the 2023 settlement</a> of an opioid-related lawsuit, was expected to receive about $230 million from Walgreens.</p></div><div><p>In both instances, the funds were slated to be used for opioid recovery efforts.</p></div><div><p>In 2019, McKinsey said it would no longer work on opioid-related businesses.&nbsp;Last year, McKinsey formally apologized for its Purdue Pharma work, <a href=\"https://www.mckinsey.com/about-us/opioidfacts\">saying it was “deeply sorry”</a> for its role in selling OxyContin. “This terrible public health crisis and our past work for opioid manufacturers will always be a source of profound regret for our firm,” the company said in a statement.</p></div><div><p>At the San Francisco Department of Public Health, Tsai replaced Dr. Grant Colfax, who took the reins in 2019 and led the city through the pandemic <a href=\"https://sfstandard.com/2025/01/16/grant-colfax-resign-san-francisco-public-health/\">before stepping down in January</a>. The role paid $546,133 in 2024, one of the highest city salaries.&nbsp;&nbsp;</p></div><div><p>On Monday, the San Francisco Health Commission unanimously nominated Tsai as director of Public Heath. Dr. Laurie Green, president of the commission, said the governing body conducted a “multi-hour” interview.</p></div>","contentLength":2776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061482"},{"title":"Schemesh: Fusion between Unix shell and Lisp REPL","url":"https://github.com/cosmos72/schemesh","date":1739646038,"author":"cosmos0072","guid":505,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061183"},{"title":"Proactive IT Career Growth: Take Control of Your Professional Journey","url":"https://hackernoon.com/proactive-it-career-growth-take-control-of-your-professional-journey?source=rss","date":1739646022,"author":"Ekaterina","guid":476,"unread":true,"content":"<p>There are a lot of good articles about possible career tracks that you can pursue in IT, however, I haven’t seen many that might be used as actual guidance to move up the career ladder.</p><p>\\\nCurrently, I am working in a company that has very clear requirements for the engineers’ promotion and what can be used as sufficient evidence of fulfillment of those requirements. A combination of these two factors gave me the idea that additional information on this topic might help other engineers who are employed by companies that do not have it to build a strategy that will allow them to get to the next level.</p><p>\\\nIn almost any more or less mature IT company, a common career track for a software engineer is linear and looks almost the same:</p><p>Associate Software Engineer is optional and may or may not be presented in the common IT department structure for a very simple reason: it’s net-negative for the first 12 months as it requires a lot of hand-holding so not all companies have resources and time to allow such positions in their structure.</p><p>\\\nThe further career track will depend on your inclinations, what you enjoy doing, and whether you are ready for the shift in the way you’re working.</p><p>There’s nothing wrong with staying a senior software engineer if you like to allocate the majority of your time to coding. However, if you feel the need to empower others and lead that’s the right moment to weigh all expectations for each role, your strengths, the things that drive you, and pick the most suitable track for yourself.</p><p>\\\nDespite the visual simplicity of the tracks above, it’s not clear how to get closer to the right end. The following insights will apply to the companies that have:</p><ul><li><p>a hierarchical structure where each employer has a line manager</p></li><li><p>a genuine interest in employee development</p><p>\\n Why is the above-mentioned is important?==The answer is quite simple: from day one you have an ally - your line manager==.</p></li></ul><p>\\\nEach line manager’s efficiency is based on the output of each person reporting to them: the faster you grow - the bigger your output - the better the line manager’s efficiency. Given all this, sooner or later, after you have joined your company, your line manager will approach you with the question: “Where do you see yourself after a certain time?” If it is not happening and you have regular one-to-ones, feel free to add this as a topic for discussion in the agenda.</p><p>\\\nVoicing your intentions and setting a goal is just the first step of your way. The next step is to gather the list of requirements for the higher role and compile a list of achievements that can serve as evidence of your qualifications that you can use as a guide that you should follow to get from point A to point B. In companies with transparent promotion processes, this should be already in place.</p><p>\\\nIf this is not the case, you and your manager could compose one. Remember that this process is beneficial for both sides: you are getting an agreement that after certain achievements, you will be praised with the promotion and your line manager can get increased output from the team, so it’s a win-win case.</p><p>\\\nDifferent companies may have different requirements for certain positions, and I won’t claim that the ones below are universal and will suit everyone. The main purpose is to give you an idea it might look like if you need one that can be further tailored to your needs.</p><p>\\n Guidelines for evidence can be used as a roadmap that brings you to the desired destination. The next steps for the common track might be</p><ul><li>Check the team’s roadmap for suitable projects or change requests that might fit the purpose of the evidence.</li></ul><ul><li>Voice up your intentions to the line manager so they can assist with the suitable project allocation and provide information about its priority, business value, and when it can be picked up for development.</li></ul><ul><li>Spot any potential areas for improvement in code, observability, extensibility, and security perspectives and raise them as ownership tickets.</li></ul><ul><li><p>Familiarize yourself with the current recruitment process in your company and ask for shadowing during recruitment sessions. Ask to switch roles where someone more senior will shadow you and ask for feedback. \\n </p><p>This is a short list of the roles that will be covered from requirements/guidelines for evidence perspectives:</p></li><li><p>Junior Software Engineer Requirements</p></li><li><p>Software Engineer Requirements</p></li><li><p>Senior Software Engineer Requirements</p></li><li><p>Lead Engineer Requirements</p></li><li><p>Senior Engineering Lead Requirements</p></li></ul><h4><strong>==Junior Software Engineer Requirements==</strong></h4><p>|  |  |  |\n|----|----|----|\n|  | Delivers tasks \\n · Clear requirements are needed (business and system) \\n · Designs/implements limited-scope technical solutions \\n · Limited guidance is required | 1. List of tasks completed \\n o Tasks should be complex enough to mention them \\n o Deadlines are met \\n o No major quality issues \\n o Tasks were completed with no handholding \\n 2. Input from the line manager confirming that all the requirements are met. |\n|  | Applies best practices \\n · Learns and constantly applies best practices \\n · Proficient with various dev tools \\n · Investigates and fixes complicated problems/bugs | <strong>Feedback from the line manager and peers confirming that all the requirements are met.</strong> |</p><p>\\\n<strong>==Software Engineer Requirements==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Delivers change requests (features) \\n · Takes business requirements as input \\n · Breaks work into tasks with a sufficient level of detail on the solution (what needs to be done and when it’s done) and the implementation (how it should be done) \\n · Provides accurate estimates on a task/user story level \\n · Pairs with other engineers to deliver faster | List of change requests delivered, conforming to the following requirements: \\n 1. The change request has been fully delivered and the deadline was met. \\n 2. The discovery part was completed by the employee (tickets, estimates). \\n 3. The change request is complex enough from a technical perspective (more than 2 man-weeks for 1 engineer to implement it). \\n 4. The change request provides a meaningful impact on the business. \\n 5. The change request is signed off by the business and is running in production. \\n 6. The employee has demonstrated a sufficient level of autonomy and quality (based on the feedback from the tech lead and the engineering manager). |\n|  | Designs services \\n · Designs and implements smaller services while taking into account all of the non-functional aspects (extensibility, security, observability, etc) \\n · Writes high-quality code with full adoption of engineering practices and methodologies \\n · Participates in code reviews to enforce best practices \\n · Fixes the root causes behind bugs and problems encountered | At least two services designed conforming to the following requirements: \\n 1. It can be a new service or a complete redesign of the existing service. \\n 2. It can be a standalone service, a library, or a component consumed by other services. \\n 3. The service shouldn’t be trivial from a design perspective. \\n 4. The engineer should have followed the formal design process: \\n · Obtain business and system requirements \\n · Identify the bounded context \\n · Identify non-functional requirements \\n · Break down context into services \\n · Get feedback on the solution \\n · Implement it \\n 5. Service is implemented and is running in production. |</p><p>\\\n<strong>==Senior Software Engineer==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Delivers project phases (epics) \\n · Takes requirements and high-level system design as input \\n · Creates system design for the service or the component, decides on the technologies and engineering practices to be used \\n · Breaks work into tasks or user stories with a sufficient level of detail on the solution (what needs to be done and when it’s done) and the implementation (how it should be done) \\n · Provides accurate estimates on task/user story level \\n · Leads a small team to deliver the scope \\n · Unblocks their team, resolves issues, and removes impediments | List of project phases/epics delivered, conforming to the following requirements: \\n 1. The epic/project phase has been fully delivered and the deadline was met. \\n 2. The discovery part was completed by the employee (tickets, estimates). \\n 3. The epic/project phase is complex enough from a technical perspective (requires at least 2 engineers for &gt;= 2 weeks). \\n 4. The epic/project phase provides a meaningful impact on the business. \\n 5. The functionality is signed off by the business and is running in production. \\n 6. The employee has demonstrated a sufficient level of autonomy and quality (based on the feedback from the tech lead and engineering manager). \\n 7. The engineer participated in the implementation as a technical lead. |\n|  | Designs subsystems \\n · It is the same as for a Software Engineer but focuses on more complex services or subsystems \\n · Proficient in the cloud and distributed systems design and implementation | At least 3 services designed conforming to the following requirements: \\n 1. It can be a new service or a complete redesign of the existing service. \\n 2. It can be a standalone service, a library, or a component consumed by other services. \\n 3. The service shouldn’t be trivial from a design perspective. \\n 4. The engineer should have followed the formal design process: \\n a. Obtain business and system requirements \\n b. Identify bounded context \\n c. Identify non-functional requirements \\n d. Break down context into services \\n e. Get feedback on the solution \\n f. Implement it \\n 5. Service is implemented and is running in production. |\n|  | Proposes changes \\n · Challenges the status quo and the assumptions made \\n · Find ways to improve the platform, processes, working environment, and the tech team in general | At least three significant changes were proposed, which can be any of the following: \\n 1. Functionality: proposed a change request that was prioritized and implemented (change request should be substantial enough to be considered as a change, not a cosmetic change). \\n 2. People: interviewed an engineer who was hired and passed probation (junior software engineer or higher, considered as a change to the team). \\n 3. Ownership: proposed an ownership project (included in the ownership roadmap, approved by CTO). |</p><p>\\\n\\\n<strong>==Lead Engineer Requirements==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Tech lead for projects (project proposals) \\n · Takes business requirements as input \\n · Find the most effective solution for the business problem (research alternatives, validate solutions using no-code/low-code approaches) \\n · Creates system design for the new service or subsystem, decides on the technologies and engineering practices to be used \\n · Breaks work into epics with a sufficient level of detail on the solution (what needs to be done and when it’s done) and the implementation (how it should be done) \\n · Provides accurate estimates on the project level, commits to dates \\n · Acts as a tech lead for the entire project \\n · Unblocks their team, resolves issues, and removes impediments \\n · Manages technology, implementation, and operational risks | List of projects delivered, conforming to the following requirements: \\n 1. The solution for the problem was proposed by the employee and it is considered to be effective. I.e. multiple alternatives were evaluated, and the best alternative was chosen based on the low-code/no-code validation. \\n 2. The discovery part was completed by the employee (tickets, estimates). \\n 3. The solution was architected by the employee. \\n 4. The project needs to be a “feature” project initiated through a project proposal. \\n 5. The engineer participated in the implementation as a technical lead (see requirements column for more details). |\n|  | Drives technical changes (squad) \\n · Proposes and implements initiatives to improve system quality and reduce technical debt \\n · Proposes and implements changes to improve developer experience and productivity \\n · Advocates and enforces clean code and clean architecture | List of major changes introduced (usually at least four), conforming to the following requirements: \\n 1. The change provides meaningful improvement to system quality (e.g. platform improvements), developer experience, or developer productivity. The change affects the entire squad. \\n 2. The engineer doesn’t have to be the one who proposed the change. The engineer should be the primary driving force behind the change (e.g. designed, acted as a tech lead, participated in the implementation). The change can be delivered by an engineer or as a team effort. \\n 3. The change should be fully implemented and used by the squad/platform (the change should be “sticky” and provide enough value to keep it). \\n 4. The change should be significant enough to mention. |\n|  | Mentor \\n · Mentors and supports less experienced engineers \\n · Conducts technical interviews effectively \\n · Acts as a “magnet” for great engineers during hiring (be a decisive factor where we are in competition for good talent vs. another company) | Possible evidence: \\n 1. Engineers interviewed, who were hired and passed probation. \\n 2. Feedback from upskilled engineers. \\n 3. Training sessions are organized/delivered for the entire tech team (e.g. Tech Sync, Engineering Dojo). \\n 4. When leading a working group a list of changes proposed/implemented in the scope of the working group can be used as evidence. |</p><p>\\\n<strong>==Senior Engineering Lead==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Tech Lead for complex projects (project proposals) \\n Same as Lead Engineer, but focuses on problems that are complex from technical, organizational, or business perspectives \\n · The project requires coordination across multiple squads \\n · The project involves 3rd party technology provider or stakeholder (e.g. partnership) \\n · a new product build while the product is in the discovery mode \\n · high priority/urgency project with fixed deadlines and many unknown | List of projects delivered, conforming to the following requirements: \\n 1. The project is considered to be complex (see examples on the left). \\n 2. The project has been fully delivered (all deliverables + DoD) and the deadline was met. \\n 3. The solution for the problem was proposed by the employee and it is considered to be effective (i.e. multiple alternatives were evaluated, and the best alternative was selected based on the low-code/no-code validation). \\n 4. The discovery part was completed by the employee (system requirements, tickets, estimates). \\n 5. The solution was architected by the employee. The project has a high complexity from a system design perspective. \\n 6. An engineer participated in the implementation as a technical lead. |\n|  | Drives technical changes (tech) \\n · Same as E5 but on the tech level \\n · System owner for at least one non-functional aspect (e.g. security, observability, etc). | List of major changes introduced (usually at least 4), conforming to the following requirements: \\n 1. The change provides meaningful improvement to system quality (e.g. platform improvements), developer experience, or developer productivity. The change affects multiple squads (e.g. technology adoption). \\n 2. The engineer doesn’t have to be the one who proposed the change. The engineer should be the primary driving force behind the change (e.g. designed, acted as a tech lead, participated in the implementation). The change itself can be delivered by an engineer or as a team effort. \\n 3. The change should be fully implemented and used by multiple squads (changes should be “sticky” and provide enough value to keep it). \\n 4. The change should be significant enough to mention. It should be tracked on the “upcoming projects” page as an ownership project (ownership in this context means changes to the platform, tooling, processes, etc, not just platform-related changes). \\n 5. At least 2 changes should be related to the non-functional aspect owned by the individual. |\n|  | Recognized expert \\n · Recognized expert within a given area of expertise on a company level, acts as a technical point of contact in tech within their area of expertise \\n · Monitors trends/technologies within the area of expertise and communicates updates and findings \\n · Actively and regularly shares expertise with other engineers (workshops, tech talks, training) \\n · Facilitates collaboration to find solutions for complex problems (working groups, etc) \\n · Conducts technical interviews effectively \\n · Mentors and supports less experienced engineers, guide their career from a professional development perspective \\n · Acts as a “magnet” for great engineers during hiring (be a decisive factor where we are in competition for good talent vs. another company) | Possible evidence: \\n 1. Interviewed engineers, who were hired and passed probation. \\n 2. Feedback from upskilled engineers. \\n 3. Training sessions are organized/delivered for the entire tech team (e.g. Tech Sync, Engineering Dojo). \\n 4. Leading a working group, a list of changes proposed/implemented in the scope of the working group can be used as evidence. |</p><p>|  |  |  |\n|----|----|----|\n|  | Delivers squad roadmap \\n · Leads a squad of 3-6 engineers \\n · Acts as a project manager for multiple concurrent initiatives \\n · Able to deliver results having only business requirements as input (able to create and sign off system requirements) \\n · Focuses on business impact, driven by business value \\n · Communicates commitments, status, and risks to business stakeholders \\n · Ensures that all squad members have all the information they need \\n · Communicates to 3rd parties within the scope of initiatives/ownership \\n · Finds the right balance between feature delivery and system quality \\n · All requirements for Senior Software Engineer | New projects delivered by the squad conforming to the following requirements: \\n 1. Project initiated through a project proposal. \\n 2. The project has met its impact metrics, and the public commitment was met. \\n 3. Projects reported in the previous promotion cycle can’t be included in the list. |\n|  | Drives managerial changes (squad) \\n · Measures and continuously improves squad performance \\n · Identifies and establishes best practices within the squad with a focus on productivity \\n · Maintains high quality of delivery \\n · Ensures transparency on progress, risks, results | 1. Squad productivity (performance) metric values. \\n 2. Major changes (at least 4) introduced, conforming to the following requirements: \\n a. It solves a problem related to the owned squad or tribe, the problem needs to be included in the TOP 5 problems and agreed upon with the line manager. \\n b. The change should be fully implemented and used by the squad (change should be “sticky” and provide enough value to keep it). \\n c. The change should provide meaningful improvement to productivity, engagement, or quality of delivery. \\n d. The manager doesn’t have to be the one who proposed the change. The EM should be the primary driving force behind the change. The change can be delivered by an engineer or as a team effort. |\n|  | Line manager (&gt;=3 direct reports) \\n · Manages 3-6 direct reports \\n · Coaches and supports engineers \\n · Supports and guides career progressions \\n · Reconciles differences of opinion and helps manage and resolve conflicts \\n · Encourages a positive team culture and collaboration | 1. Squad engagement metric values. \\n 2. List of engineers, who were hired and passed probation (can be skipped if we are not hiring, EM should be a hiring manager). |</p><p>\\\n</p><p>|  |  |  |\n|----|----|----|\n|  | Delivers roadmap for multiple squads \\n · Ensures delivery across 2-3 squads \\n · Fulfils Engineering Manager role in one of the squads \\n · Owns partnerships with 3rd parties \\n · All requirements from the Engineering Manager | New projects delivered by the squads conforming to the following requirements: \\n 1. Project initiated through a project proposal (not a BAU activity). \\n 2. The project has met its impact metrics and the public commitment was met. \\n 3. Project results were presented as a Tech Feature session. \\n 4. Projects reported in the previous promotion cycle can’t be included in the list. \\n 5. At least 2 projects should be recognized as key projects on a company level (e.g. a new product, etc, can be confirmed with CTO). |\n|  | Drives managerial changes (multiple squads/tech) \\n · All requirements from Engineering but across multiple squads \\n · System owner for at least one process (e.g. support, etc) | 1. Squads’ productivity (performance) metric values across multiple squads. \\n 2. Major changes (at least 6) introduced, conforming to the following requirements: \\n a. It solves a problem related to the squads or tribe, a problem needs to be included in the TOP 5 problems and agreed upon with the line manager. \\n b. The change should be fully implemented and used by the squads (change should be “sticky” and provide enough value to keep it). \\n c. The change should provide meaningful improvement to productivity, engagement, or quality of delivery. \\n d. The manager doesn’t have to be the one who proposed the change. The Engineering Director should be the primary driving force behind the change. The change can be delivered by an engineer or as a team effort. \\n e. At least 2 changes should be related to the process owned by the director. |\n|  | Line manager (&gt;=10 reports, including indirect reports) \\n · All requirements for Engineering Manager \\n · Coaches and supports engineers \\n · Supports and guides career progressions \\n · Manages churn, reduces “regrettable churn” | 1. Squads’ engagement metric values across multiple squads. \\n 2. List of engineers, who were hired and passed probation (can be skipped if we are not hiring). \\n 3. List of engineers promoted (can be skipped if there is no business need for promotions). |</p>","contentLength":22065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How a Soft Robot Arm Moves Using Air, Not Motors","url":"https://hackernoon.com/how-a-soft-robot-arm-moves-using-air-not-motors?source=rss","date":1739646019,"author":"EScholar: Electronic Academic Papers for Scholars","guid":475,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing</p><p>4 Data Acquisition and Open-Loop Control</p><p>\\\nAlthough the layout of the pneumatic bench allows working with up to 4 segments, it was thought that using 3 would allow the different problems linked to redundancy to be tackled without increasing the weight of the robot too much or requiring the tubes –which pass through the interior of the segments – to have an excessive amount of space.</p><p>\\\nIt is true that the tubes of the other three could pass through the first module, nevertheless, it was thought that the stiffness they would introduce by being so compressed could make it difficult to bend the initial segment. Since it is also the segment that has to exert the most force, as it is the one that supports the weight of the other segments, the risk of punctures could be increased.</p><p>\\\nTherefore, a robot consisting of three identical modules was assembled, standing at a total height of 390 mm (with each segment measuring 100 mm, intersegment connections 20 mm each, and the vision trihedron rod 30 mm). Under these configurations, the estimated weight of PAUL’s arm is around 600 g. The structure protecting the manipulator is a cube with a side of 500 mm. Pressure of the pneumatic line was established in 1.2 bar.</p><p>\\\nExamples of PAUL reaching different positions are depicted in Figure 13.</p><p>The analysis of the workspace has been carried out experimentally, based on the data taken to generate the dataset. Figure 14 shows the workspace of a segment.</p><p>\\\nAs can be seen, this is a surface, as the segment has two degrees of freedom if the condition that at least one valve should remain deflated is imposed. The surface can be considered as the union of three surfaces intersecting at the central point, which corresponds to the configuration of all deflated bladders. The three surfaces are roughly spherical in shape. If the PCC model were completely valid for the robot, these would be perfect spheres, as the ends of a set of equal-length arcs of circumference with a common origin engrench a circle. Since this is not exactly the case, the generated surfaces only resemble the sphericity predicted by the constant curvature model.</p><p>\\\nThe addition of a second segment already generates a 4-D workspace that is difficult to represent. The generation of this is a consequence of the fact that, from each point on the surface of the workspace of a segment, another similar surface is generated. The</p><p>\\\n\\\nunion of all of these surfaces, which arise from the points on the surface of the first segment, results in the two-segment workspace. This is a volume in which, in addition, each point can be reached from two different orientations, thus leaving latent the four degrees of freedom that PAUL would have with only two modules.</p>","contentLength":3400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiple Russian Threat Actors Targeting Microsoft Device Code Authentication","url":"https://www.volexity.com/blog/2025/02/13/multiple-russian-threat-actors-targeting-microsoft-device-code-authentication/","date":1739645979,"author":"ChrisArchitect","guid":466,"unread":true,"content":"<h2>Multiple Russian Threat Actors Targeting Microsoft Device Code Authentication</h2><p>by Charlie Gardner, Steven Adair, Tom Lancaster</p><ul><li><em>Volexity has observed multiple Russian threat actors conducting social-engineering and spear-phishing campaigns targeting organizations with the ultimate goal of compromising Microsoft 365 accounts via Device Code Authentication phishing.</em></li><li><em>Device Code Authentication phishing follows an atypical workflow to that expected by users, meaning users may not recognize it as phishing.</em></li><li><em>Recent campaigns observed have been politically themed, particularly around the new administration in the United States and the changes this might mean for nations around the world.</em></li></ul><p>Starting in mid-January 2025, Volexity identified several social-engineering and spear-phishing campaigns by Russian threat actors aimed at compromising Microsoft 365 (M365) accounts. These attack campaigns were highly targeted and carried out in a variety of ways. The majority of these attacks originated via spear-phishing emails with different themes. In one case, the eventual breach began with highly tailored outreach via Signal.</p><p>Through its investigations, Volexity discovered that Russian threat actors were impersonating a variety of individuals in order to socially engineer targets, including impersonating individuals from the following:</p><ul><li>United States Department of State</li><li>Ukrainian Ministry of Defence</li><li>European Union Parliament</li><li>Prominent research institutions</li></ul><p>Communications carried a variety of different themes and messages, but they all ultimately resulted in the attacker inviting the targeted user to one of the following:</p><ul><li>Microsoft Teams Meeting / Video Conference</li><li>Access to applications and data as an external M365 user</li><li>Join a chatroom on a secure chat application</li></ul><p>When these attacks were successful and the attackers gained access to accounts, the post-exploitation phase often had unique characteristics in each case:</p><ul><li>The way the attackers accessed material from compromised organizations (scripts versus native applications)</li><li>The infrastructure used to access stolen accounts</li></ul><p>Despite the differences, Volexity found the attacks had one thing in common: they were all <strong>Device Code Authentication</strong> attacks. While this attack method is not new, it is one that is definitely lesser known and not commonly leveraged by nation-state actors. Details on the social-engineering and spear-phishing campaigns, along with how Device Code Authentication attacks work, will be covered further in this blog post. What Volexity has observed is that this method has been more effective at successfully compromising accounts than most other targeted spear-phishing campaigns.</p><p>Volexity assesses with high confidence that the series of attacks described in this blog post are from Russia-based threat actors. At this time, Volexity is tracking this activity under three different threat actors and assesses with medium confidence that at least one of them is  (overlapping with DarkHalo, APT29, Midnight Blizzard, CozyDuke). Volexity is tracking the remaining activity under  and . It is possible that all the activity described in this blog post is a single threat actor, but despite the similar targeting, timing, and attack method, other observed components of the operations are different enough to be tracked separately, for now.</p><h3>From Secure Chat to Insecure Authentication</h3><p>The discovery of this threat activity started toward the end of January 2025, when Volexity uncovered a highly targeted attack that had successfully compromised the M365 account of one of its customers. This breach was discovered after Volexity identified suspicious sign-in activity to the account, which was followed by a rapid download of files from the user's OneDrive. All authentication and download events came from virtual private server (VPS) and Tor IP addresses, which is not the most subtle way to access an account. Volexity noted this activity was likely scripted, as the User-Agent string for later access and file downloads was the Python User-Agent string .</p><p>Volexity then performed a detailed investigation into this incident, in an effort to identify how the account was compromised. A review of login activity showed the legitimate user had logged in and approved a multi-factor authentication (MFA) request. However, subsequent access was not from the legitimate user's IP address. This caused Volexity to initially suspect a phishing attack involving an adversary-in-the-middle (AiTM) framework. As a result, Volexity reviewed emails to the user leading up the time of the authentication event. This review identified a suspicious email just moments before the login activity from an email address purporting to be from someone with the name of a high-ranking official from the Ukrainian Ministry of Defence. The email was structured to look like a meeting invite for a chatroom on the messaging application, <a href=\"https://element.io/\">Element</a>. Element is another encrypted messaging application that offers the ability for users to self-host a server with functionality that includes group video chats. The “invitation” email sent is shown below .</p><p>Microsoft <a href=\"https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-device-code\">describes</a> the purpose of this workflow as allowing <em>'\"users to sign in to input-constrained devices such as a smart TV, IoT device, or a printer.</em>” However, in this case, it means if an attacker can convince a user to enter a specific code into this dialogue (and log in), they are granted long-term access to the user’s account.</p><p>After working with its customer more closely, Volexity learned that the victim had been contacted on Signal by an individual purporting to be from the Ukrainian Ministry of Defence. This individual then requested the victim move off Signal to another secure chat application called Element. The attacker then had the victim join an Element server they controlled under the domain . This allowed the attacker to further communicate with the victim in real time and inform them they needed to click a link from an email to join a secure chat room. This is where the email Volexity had discovered came into play. The message was a ploy to fool the user into thinking they were being invited into a secure chat, when in reality they were giving the attacker access to their account. The generated Device Codes are only valid for 15 minutes once they are created. As a result, the real-time communication with the victim, and having them expect the \"invitation\", served to ensure the phish would succeed through timely coordination.</p><p>The diagram below s</p><p>Volexity tracks the threat actor behind this campaign as . Through research conducted on the custom domain used by UTA0304 to operate its own Element server, Volexity was able to pivot and discover additional infrastructure it believes is likely operated by the group. The table below represents the list of infrastructure that Volexity has tied to this threat actor.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td width=\"246\"><code>chromeelevationservice[.]com</code></td></tr><tr></tr></tbody></table><h3>Spoofing the United States Department of State</h3><p>In early February 2025, Volexity observed multiple spear-phishing campaigns targeting users with fake Microsoft invitations purporting to be from the United States (US) Department of State. These emails were themed as invitations to join the US Department of State’s Microsoft tenant as an external user, or as invitations to a Microsoft Teams chat named “<em><strong>Measuring Influence Operations</strong>\".</em></p><p>Similar to the campaign conducted by UTA0304, these fake US Department of State emails were targeting users with a <a href=\"https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth2-device-code\">Device Code OAuth</a> phishing workflow. Each email was aimed at convincing the user to accept the invitation and enter a unique code provided in the phishing email. The link in the invitations would direct users to the Microsoft Device Code <a href=\"https://login.microsoftonline.com/common/oauth2/deviceauth\">authentication page</a>. If the user entered the code provided in the phishing email, the authentication page would subsequently authorize the threat actor to access to the user’s account. However, it is worth noting that this campaign was sent out of the blue, with no precursor or build up to the emails, so users would not be expecting these messages. Even if they were to fall for the campaign, they would have to have done it within 15 minutes of receiving the email. This dramatically decreased the likelihood that this attack would be successful.</p><p>After reviewing various parts of the attack, Volexity assesses with medium confidence that the Russian threat actor CozyLarch (aka APT29 or Midnight Blizzard) was behind these US Department of State themed spear-phishing campaigns. Additional details on each campaign are described in the sections that follow.</p><h4>Campaign 1: M365 Tenant External User Invitation</h4><p>CozyLarch sent invitations to several users, inviting them to access applications within the M365 tenant for the US Department of State. The invitation email was designed to look like a real invitation that would be sent from Microsoft, as shown below.</p><p>The redirect link takes the user to the Microsoft Device Code OAuth workflow, and it is the same URL that UTA0304 directly embedded in their phishing campaign. However, unlike UTA0304, CozyLarch opted to use the redirect URL rather than the final login URL, perhaps because it may look even more recognizable to a discerning user, given that it is hosted on the main Microsoft domain. If the user entered the code provided from the email and continued through the authentication process, the attacker was granted access to the user’s M365 account.</p><h4>Campaign 2: M365 Teams Chat Invitation</h4><p>CozyLarch launched a second campaign, in which they targeted users with a fake invitation to join a Microsoft Teams chat named “<strong><em>Measuring Influence Operations</em></strong>”. The email made it appear as though there were already 37 other members in the chat. &nbsp;A screenshot of one of the observed spear-phishing messages is shown below.</p><p>The “Sign in to Microsoft Teams” button in the email body is a hyperlink that leads to the same <a href=\"https://www.microsoft.com/devicelogin\">https://www.microsoft.com/devicelogin</a> URL observed in the other campaign. The attack flow and end goal are the same, with only a small difference in the theme of the emails.</p><p>The emails are designed to appear as though they come from Microsoft. The messages used mixed encoding in the “friendly” name that make the address difficult to discern. An example of the full “from” header used in one phishing email is given below:</p><blockquote><p><code>\\\"Mic\\udb40\\udc30\\udb40\\udc30\\udb40\\udc30\\u200br\\udb40\\udc30\\udb40\\udc30o\\udb40\\udc30\\udb40\\udc30soft Invitations on behal f of US Dep\\udb40\\udc30\\udb40\\udc30\\udb40\\udc30artme\\udb40\\udc30\\udb40\\udc30\\udb40\\udc30nt of St\\udb40\\udc30\\udb40\\udc30\\udb40\\udc30 ate \\uff1cinvites\\uff20mic\\udb40\\udc30\\udb40\\udc30\\udb40\\udc30\\u200br\\udb40\\udc30\\udb40\\udc30o\\udb40\\udc30\\udb40\\udc30soft.co m\\uff1e\\u180e\\u3000\\u180e\\u3000\\u180e \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e \\u180e\\u3000\\u180e \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e \\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e&nbsp; \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000\\u180e\\u3000 \\u180e\\u3000\\u180e\\u3000\\u180e \\u061c Cc:\\\" &lt;<a href=\"https://www.volexity.com/cdn-cgi/l/email-protection\" data-cfemail=\"0c7f64696560616d6b626978784c6b616d6560226f6361\">[email&nbsp;protected]</a>&gt;</code></p></blockquote><p>The attacker attempted to make it appear as if the emails were from , and also set the  header as . However, the true address could be seen at the end of the  field; all messages were sent via Google Gmail accounts. Volexity observed the following Gmail accounts as the actual senders of the messages observed:</p><ul><li><code>kaylassammers@gmail[.]com</code></li><li><code>kendisggibson@gmail[.]com</code></li><li><code>leslytthomson@gmail[.]com</code></li></ul><p>These addresses are believed to be controlled by CozyLarch and can be used to reliably detect phishing emails that may have been sent.</p><h4>Using Wireless Proxy Networks for Email Distribution</h4><p>Volexity also noted that the sending IP address associated with each spear-phishing email was recorded in the headers. Looking at the  header in the messages, it became apparent that the attacker was using Proxy IP addresses based in the US to send messages. Volexity observed nearly a dozen IP addresses belonging to mobile networks in the US (AT&amp;T and Verizon Wireless).</p><h3>European Parliament and Donald Trump</h3><p>Starting in late January through to the publication of this blog (February 13, 2025), Volexity has observed another campaign by a Russian threat actor it tracks as  targeting numerous organizations. UTA0307 created a fake email under the identity of a member of the European Parliament who is on the Committee on Foreign Affairs. The threat actor reached out to numerous individuals with personalized emails requesting a Microsoft Teams meeting to discuss Donald Trump and his impact on relations between the US and the European Union. Volexity also observed a smaller set of campaigns centered on discussing China's foreign policy and China-European Union relations.</p><p>The email subject lines used in these various campaigns are listed below:</p><ul><li><em>Discussion on Eastern Europe and the Caucasus</em></li><li><em>Discussion about Donald Trump's new term</em></li><li><em>Discussion about Trump &amp; US relations with Europe</em></li><li><em>Collaboration on China and East Asia Research</em></li></ul><p>The image below shows an example spear phish that was sent by UTA0307.</p><p>None of the initial emails contained any malicious content or links at the onset. The threat actor was leveraging a tactic that has become commonplace for numerous nation-state actors, where they wait until a conversation has started prior to sending anything malicious. This serves the purpose of knowing they have an engaged target, and that the target's guard is potentially down. In the specific cases of Device Code Authentication phishing, it is especially important to have a responsive target, as the threat actor has only 15 minutes to convince the target to enter the code that has been generated.</p><h4>A Different Device Code OAuth Phishing Technique</h4><p>Volexity actually discovered the operations of UTA0307 following a successful compromise. Similar to the initial discovery of UTA0304, Volexity worked backwards from detecting a breach to identifying the above spear-phishing emails. In this case, the victim had engaged from the initial email and had several messages back and forth with UTA0307 regarding a meeting being set up. They agreed to join a Microsoft Teams meeting, and a fake invitation email was sent. However, this time the link in the email did not go to Microsoft. The target received an email with the subject \"\", and the body of the email, shown below, was designed to look like a real invitation.</p><p>The “Join the meeting now” hyperlink, however, linked to a website controlled by UTA0307 (). This page in turn was set up to automatically generate a new Microsoft Device Code each time it was visited. The website was designed to appear as an official Microsoft interstitial page before the user can join a Microsoft Teams meeting. The message that appears on the landing page (shown below) claims that the victim needs to pass a security check by copying a code and entering it on a subsequent page</p><p>When the user clicks the “Next” button, a new tab is opened with the real Microsoft Device Code Authentication interface that requests an authentication code. If the victim enters the code supplied by the phishing page, they grant UTA0307 access to their M365 account. Interestingly, in the background of the initial phishing page, Volexity noted that the website would continuously poll the domain . It appears this domain was set up to monitor successful Device Code Authentication and, if detected, would redirect the user to a real Microsoft Teams meeting URL in an effort to make the activity appear legitimate.</p><p>The threat actor never joined this Microsoft Teams meeting. However, UTA0307 did add authorization for an authentication application under their control to enable multi-factor authentication when logging into the compromised account. Volexity assesses with medium confidence that this was a requirement of logging into the account, even with the stolen authentication token.</p><p>One benefit of this attack workflow versus other previously observed DeviceID phishing workflows is that, when a DeviceID code is generated, it is only valid for 15 minutes. Having an interstitial page that automatically generates new codes means UTA0307 does not have to worry about their phishing content expiring.</p><h4>UTA0307 Post-compromise Activities, Targeting and Attribution</h4><p>Volexity observed UTA0307 exfiltrating documents from a compromised M365 account that would be of interest to a Russian threat actor. This was determined based on identification of <a href=\"https://learn.microsoft.com/en-us/purview/audit-log-activities\">FileDownloaded operations</a> observed in M365 audit log data. Given this information about the threat actor’s objectives, their targeting, and their use of a highly similar technique to that used in recent days and weeks by CozyLarch and UTA0304, Volexity assesses with medium confidence that UTA0307 is also a Russian threat actor.</p><p>However, the exact implementation of the DeviceID OAuth phishing technique used in this activity differs slightly from those previously documented by Volexity, which provides some evidence that this activity may have been conducted by a separate threat actor. For example, while the previously observed phishing campaigns saw the attacker use the client ID for Microsoft Office when handling Device Code Authentication, this activity instead used the client ID for Microsoft Teams, as shown below (note that Microsoft uses  and  interchangeably in their logs when referring to the ID for an application):</p><blockquote><p><code>\"appDisplayName\": \"Microsoft Teams\",</code></p><p><code>\"appId\": \"1fec8e78-bce4-4aaf-ab1b-5451cc387264\",</code></p></blockquote><p>Another difference between this and the UTA0304 campaign is that in this case, all subsequent access to the compromised account occurred via Mullad VPN exit nodes (versus the other observed VPS and Tor IP addresses). Based on these two factors, Volexity has chosen to track this activity under the UTA0307 alias, rather than CozyLarch or UTA0304.</p><h3>Detecting Device Code Authentication</h3><p>Volexity identified a way to reliably detect this attack through monitoring and analysis of Microsoft Entra ID sign-in logs. When a user enters a device code and subsequently authenticates, it results in a login to the application associated with the generated code. This can be a common application like Microsoft Office that is frequently accessed by users and would not be a reliable indicator. However, the good news is that Device Code Authentications result in the  field being set with the value .</p><p>The line below is what will appear in the JSON data in the Entra ID sign-in logs when a Device Code Authentication occurs:</p><blockquote><p><code>“authenticationProtocol\": \"deviceCode\",</code></p></blockquote><p>Volexity further noted that as authenticated sessions refresh and are kept alive, subsequent sign-ins that initially occurred via a  often do not have anything set for  but they contain the following entry:</p><blockquote><p><code> “originalTransferMethod\": \"deviceCodeFlow\",</code></p></blockquote><p>These values can be searched and filtered on in the Entra Admin center by adding filters for \"Authentications Protocol\" and \"Original Transfer Method\". The latter can be filtered in both  and  sign-ins. The frequency and legitimacy of these values occurring in the sign-in logs for a particular organization may vary, as this is a legitimate Microsoft feature. An organization can evaluate their risk and usage of these workflows, and potentially use this information as a proactive detection mechanism.</p><p>If an organization has the ability to monitor URLs that are being accessed by users or sent in email, there are additional detection opportunities to discover Device Code Authentication attacks. The following official URLs can be monitored for as related to Microsoft Device Code Authentication:</p><ul><li><code>https://login.microsoftonline.com/common/oauth2/deviceauth</code></li><li><code>https://www.microsoft.com/devicelogin</code></li><li><code>https://aka.ms/devicelogin</code></li></ul><p>Organizations can monitor for access to these URLs or for their presence in various communication methods, such as email. Attackers can find other means to redirect users to these URLs, but one of the main advantages of using the list above in phishing attacks is that the URL displayed is hosted on a legitimate Microsoft domain.</p><h3>Preventing Device Code Authentication</h3><p>Volexity believes the most effective way to prevent this potential attack vector is through conditional access policies on an organization's M365 tenant. It is possible for organizations to create a conditional access policy that disallows device code authentication altogether. It is fairly trivial to set up, and Microsoft provides <a href=\"https://learn.microsoft.com/en-us/entra/identity/conditional-access/policy-block-authentication-flows\">online guidance on exactly how to do this</a>. Based on Volexity's own testing, blocking the \"Device code flow\" from \"Authentications flows\" prevents this attack from working.&nbsp; The image below shows what a conditional access policy would look like once it's set up and in place to block this authentication flow.</p><p>Prior to implementing such a policy, organizations should evaluate the use of Device Code Authentication in their environment. This feature is used legitimately, and blocking it could have a negative impact. Volexity's review of its own customers identified several instances of legitimate access to resources via these means. However, at the majority of Volexity's customers, there was either no recent Device Code Authentication activity or there was only activity tied to the attacks described in this blog post.</p><p>Volexity continues to track multiple spear-phishing campaigns targeting Device Code Authentication. This blog post serves to cover a few of the larger and unique campaigns observed. Volexity has observed other similar spear-phishing campaigns in recent weeks targeting Device Code Authentication that it believes are the work of Russian threat actors. Further, it should be noted that it is possible this is the work of a single threat actor running multiple, different campaigns. However, at this time, Volexity believes this activity is sufficiently different enough to warrant tracking this activity under two different unknown threat actors and one it believes is likely CozyLarch.</p><p>While Device Code Authentication attacks are not new, they appear to have been rarely leveraged by nation-state threat actors . Volexity's visibility into targeted attacks indicates this particular method has been far more effective than the combined effort of years of other social-engineering and spear-phishing attacks conducted by the same (or similar) threat actors. It appears that these Russian threat actors have made a concerted effort to launch several campaigns against organizations with a goal of simultaneously abusing this method before the targets catch on and implement countermeasures.</p><p>The detection mechanisms and countermeasures to these attacks have been available for years. However, Volexity believes they are seldom implemented and that most organizations are not even aware of this authentication flow, let alone the means to detect its misuse. These attacks serve as a reminder that threat actors will constantly look for ways to abuse legitimate features, and organizations must continually evaluate and implement methods to detect and prevent such attacks.</p><p>These attacks also serve as a good opportunity to engage with users and remind them to be on the lookout for anything out of the ordinary when it comes to accessing resources when they are asked for login credentials or authorization grants. This phishing workflow has proven useful for an attacker, as many traditional sources of evidence and detection, both for a user and network defenders, are not present. For example:</p><ol><li>There is no “malicious” link or attachment. The only link is to the provider’s infrastructure (in this case, Microsoft). This means users cannot easily identify the link as being suspicious, and automated solutions detecting malicious emails will likely fail to do so for the same reason.</li><li>Users are generally less aware of attacks that leverage legitimate services, and may be even less aware when it comes to those that involve entering a device code rather than their username or password.</li><li>After successful authentication, the logs will show the authenticating application as a legitimate or benign application, reducing signal that can be keyed off of in sign-in logs by detection teams.</li></ol><p>These are items that organizations should look to further train users on and implement technical countermeasures against where possible.</p><p><a href=\"https://github.com/volexity/threat-intel/blob/main/2025/2025-02-13%20Device%20Code%20Phishing/iocs.csv\">Volexity GitHub</a>.</p><p><em>If you believe you have been targeted by a similar attack and want to share details with Volexity for informational purposes, additional investigation, or incident response, please <a href=\"https://www.volexity.com/company/contact/\">contact us</a>.</em></p>","contentLength":25345,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43061173"},{"title":"xAI’s “Colossus” supercomputer raises health questions in Memphis","url":"https://techcrunch.com/2025/02/15/xais-colossus-supercomputer-raises-health-questions-in-memphis/","date":1739645584,"author":"Connie Loizos","guid":53,"unread":true,"content":"<p>Elon Musk’s AI startup xAI plans to continue using 15 gas turbines to power its “Colossus” supercomputer in Memphis, Tennessee, according to an operating permit with the Shelby County Health Department for non-stop turbine use from June 2025 to June 2030. Why does it matter? The Commercial Appeal, a news outlet that obtained the documents, […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":416,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elevate Your Night Shift Productivity Levels: 8 Strategies for Thriving - Not Just Surviving","url":"https://hackernoon.com/elevate-your-night-shift-productivity-levels-8-strategies-for-thriving-not-just-surviving?source=rss","date":1739645102,"author":"Beth Rush","guid":474,"unread":true,"content":"<p>Almost every worker will tell you how tough it is to keep productivity levels high at work. However, the struggle is doubled when you work nights. Thankfully, there are a few techniques that can help you survive and thrive in your field, even when working those late hours.</p><h2>The Challenges of Long Hours in the Evening</h2><p>The regular nine-to-five shift is the norm across many industries, especially in the corporate world. However, there are a few outliers, such as the hospitality and manufacturing sectors. Health care, security, and firefighting departments also need hands on deck at all times in emergencies, which requires plenty of alertness.</p><p>\\\nThere are several benefits, like less competition between workers and extra pay. However, it takes some work to get used to it as you have to adjust your body clock. The night shift can <a href=\"https://www.theguardian.com/us-news/2022/nov/18/us-workers-night-shift-takes-toll\">spark plenty of employee turnover</a> since most people can’t sacrifice their daytime hours.</p><p>\\\nThose who decide to take on the night shift find themselves at a crossroads. The hours may seem much longer despite being the same eight hours that workers typically work. It can be hard to keep energy levels up or stay awake.</p><h2><strong>Most Effective Night Shift Tips</strong></h2><p>Dealing with a night shift schedule requires several adjustments, including your sleep schedule, caffeine intake, physical activities and so much more. Here are tried-and-true strategies to help you out.</p><h3><strong>Have a Routine Before the Shift</strong></h3><p>Good sleep should be the highest priority in your pre-work routine. It’s much easier to stay alert and concentrate on your tasks on the job when you’ve had enough shut-eye. Unfortunately, <a href=\"https://www.trustaff.com/blog/tips-for-surviving-night-shift\">over one-third of Americans</a> sleep less than seven hours. Heading to bed in the middle of the day can also seem foreign to your body.</p><p>\\\nThus, the first step is to have a consistent schedule. For example, people in health care may have to start their eight hours of work at 7 p.m. or so. Others might deal with extended hours, bringing the total closer to 12.</p><p>\\\nThus, the ideal night-shift nurse sleep schedule will start at 9 a.m. if you want to achieve eight hours and an extra two hours to prep and commute. The leeway can also be used as additional time to sleep ahead of those longer work times.</p><p>\\\nIf it’s your first time heading into the shift, give your body a week to gradually acclimate to the new sleep schedule. Complete and total sleep deprivation can be too exhausting for your body.</p><p>\\\nBe mindful of your sleeping quarters during the adjustment period. Blackout curtains are ideal for completely darkening the room and tricking your body into thinking it’s nighttime. You should put your phone in silent mode to avoid calls that may disrupt your sleep.</p><p>\\\nYou can also experiment with other sleeping aids. For example, some people find aromatherapy relaxing, easing them into slumber during the daytime. White noise machines can also provide the best background sound as you fall asleep.</p><p>\\\nAfter a long sleep, you should feel recharged. Awaken your senses by preparing a well-balanced dinner before your shift, and incorporate fruits and vegetables into your meals to get your nutrition fix. Afterward, wear your clothing of choice and head out to conquer the night shift.</p><h3>Design a Post-Work Ritual</h3><p>Now that you know how to prepare for a 12-hour night shift, it’s time to move on to winding down. Ideally, the routine gives you something to look forward to, such as having a shower to feel clean. You can also move your skincare routine to this time.</p><p>\\\nTry to give yourself a bit of leisure after work. Some use this time to watch shows, listen to music, or play video games. Social media is also a good outlet for entertainment. Just remember to use it in moderation to avoid doomscrolling.</p><h3><strong>Exercise Caution Around Caffeine</strong></h3><p>Caffeine is a helpful tool to keep your energy levels as high as possible. A cup of your favorite coffee or tea just before work can help you energize throughout the day. However, it’s best to avoid the temptation of getting a second or third drink in the middle of the shift. Too much caffeine will result in quite a crash afterward.</p><p>\\\nIt’s also best to schedule your intake. Prolonged caffeine ingestion <a href=\"https://academic.oup.com/sleepadvances/article/4/1/zpad014/7040153\">can disrupt the circadian rhythm</a>, disrupting your sleep routine. Meanwhile, short exposure boosts sleep fragmentation, increasing your awakenings while sleeping and ruining the quality of your shut-eye. Once a week is a good rule of thumb to follow.</p><p>One thing most people forget to realize when moving into the night shift is that everyone else still maintains their day routines. Your family and friends will likely have lunch, run errands, and do their own jobs when you’re asleep. When you’re awake and working, they’ll likely be sleeping.</p><p>\\\nHuman interaction is essential to enduring the night shift. Try to find overlaps and make time to catch up. You can invite a friend over for dinner before you head to work or chat on the phone while you’re home.</p><p>\\\nYou can also turn to your fellow employees going through the night shift. Camaraderie helps to overcome a challenge. Get to know these people, and build strong working relationships. The bond can even help you increase energy levels and boost productivity.</p><h3><strong>Maintain Physical Activity</strong></h3><p>Night shift workers have limited time and energy. However, it’s still important to maintain some form of physical movement. Exercise <a href=\"https://www.nature.com/articles/s42003-024-05962-8\">may lessen the likelihood of disorders</a> like depression, cardiovascular conditions, and metabolic diseases in chronic shift workers.</p><p>\\\nTry to find breaks to just take a walk. Aside from getting fresh air, you can move your muscles and wake yourself up. If you’re already a veteran for the evening hours, you may even consider heading to the gym right after work.</p><h3><strong>Move Your Responsibilities</strong></h3><p>Keeping productivity levels high throughout the shift can take time and practice. However, one hack you can try is intentionally scheduling the most important responsibilities at the beginning of your shift. After all, you will likely have more energy at the start.</p><p>\\\nOnce you’ve completed the bulk of the most important work, the next goal is to stretch out your energy throughout the rest of the hours and stay awake. You can try to space out your duties and give yourself breaks when you’re faltering.</p><h3><strong>Keep the Same Schedule on Off Days</strong></h3><p>It's important to maintain your current schedule to get your body used to the night shift. This solution is best for working full time, allowing you to fully adjust your body clock and move your most productive times to match your work schedule.</p><p>\\\nIf you have obligations during your off days that need to be done during the daytime, try to reschedule. You can also proceed and readjust afterward with some intentionally planned naps. Minimize deviating and the changes in your productivity should follow suit.</p><h3><strong>Manage Your Mindset and Stress</strong></h3><p>Night shift workers can experience stress throughout the adjustment period. Unfortunately, this pressure <a href=\"https://www.health.harvard.edu/topics/stress\">can alter the immune system</a> and make you more vulnerable to illnesses. It can also build a negative mindset toward your schedule.</p><p>\\\nMake sure to have stress management techniques under your belt. Meditation is a good way to come to terms with your thoughts, while breathing exercises can assist you in calming down systemic turmoil. Activities like journaling and yoga can also improve your mood.</p><h2><strong>Thrive With an Unconventional Schedule</strong></h2><p>Having a night shift schedule is a challenge worth conquering. You get to be present for the people who need aid and assistance, and it brings fulfillment to your long-term career development. You just have to adapt to the circumstances to reap the rewards.</p>","contentLength":7590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity launches its own freemium ‘deep research’ product","url":"https://techcrunch.com/2025/02/15/perplexity-launches-its-own-freemium-deep-research-product/","date":1739644754,"author":"Anthony Ha","guid":52,"unread":true,"content":"<p>Perplexity has become the latest AI company to release an in-depth research tool, with a new feature announced Friday. Google unveiled a similar feature for its Gemini AI platform in December. Then OpenAI launched its own research agent earlier this month. All three companies even have given the feature the same name: Deep Research. The […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bored With Chess? Magnus Carlsen Wants to Remake the Game","url":"https://games.slashdot.org/story/25/02/15/053254/bored-with-chess-magnus-carlsen-wants-to-remake-the-game?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739644440,"author":"EditorDavid","guid":187,"unread":true,"content":"\"Magnus Carlsen, the world's top chess player, is bored of chess,\" the Washington Post wrote Friday:\n\nCarlsen has spent much of the past year appearing to dismiss the game he has mastered: It was no longer exciting to play, he told a podcast in March. In December, he withdrew from defending a world championship because he was penalized for wearing jeans to the tournament. \nHow would the world's best player spice up the game? Change the rules, and add a touch of reality TV. \n\nTen of the world's top players gathered in a German villa on the Baltic coast this week to play in the first tournament of a new chess circuit, the Freestyle Chess Grand Slam Tour, that Carlsen co-founded. The twist: The tour randomizes the starting positions of the chess board's most important pieces, so each game begins with the queen, rooks and knights in a jumble. [It's sometimes called \"Chess960\" or Fischer random chess — with both players starting with the same arrangement of pieces.] Players have to adapt on the fly. Carlsen is backed by a cadre of investors who see a chance to dramatize chess with the theatrics of a television show. Players wear heart-rate monitors and give confession-booth interviews mid-match where they strategize and fret to the audience. Some purists are skeptical. So is the International Chess Federation, which sent a barrage of legal threats to Freestyle Chess before it launched this week's event. \nAt stake is a lucrative global market of hundreds of millions of chess players that has only continued to grow since the coronavirus pandemic launched a startling chess renaissance — and, perhaps, the authority to decide if and how a centuries-old game should evolve... The format is an antidote to the classical game, where patterns and strategies have been so rigorously studied that it's hard to innovate, Carlsen said. \"It's still possible to get a [competitive] game, but you have to sort of dig deeper and deeper,\" Carlsen said. \"I just find that there's too little scope for creativity.\" \n\nThe article also includes this quote from American grand master Hikaru Nakamura who runs a chess YouTube channel with 2.7 million subscribers). \"An integral part of regular chess is that when you play, you spend hours preparing your opening strategy before the game. But with Fischer Random ... it's a little bit looser and more enjoyable.\" And German entrepreneur Jan Henric Buettner (one of the investors) says they hope to bring the drama of Formula One racecars. (\"Cameras mounted at table level peer up at each player during games,\" the article notes at one point.) \n\nThe first Freestyle Chess Grand Slam Tour (with a $750,000 prize pool) concluded Friday, according to the article, but \"Carlsen did not play in it,\" the Post points out. \"He was upset in the semifinals by German grand master Vincent Keymer.\" Carlsen's reaction? \"I definitely find Freestyle harder.\" \n\nBut Chess.com reports that Carlsen will be back to playing regular chess very soon:\n\nGlobal esports powerhouse Team Liquid has announced the signings of not just one, but two superstars of chess. Five-time World Champion and world number-one Magnus Carlsen and the 2018 challenger, world number-two Fabiano Caruana will represent the club ahead of the 2025 Esports World Cup (EWC)... Carlsen and Caruana, fresh from competing in the Weissenhaus Freestyle Chess Grand Slam, will first represent Team Liquid in the $150,000 Chessable Masters, which begins on February 16 and serves as the first of two qualifying events in the 2025 Champions Chess Tour. The top-12 players from the tour qualify for the EWC. \n\nIn an announcement video Carlsen reportedly trolls the FIDE, according to Indian Express. \"The announcement video sees Carlsen wear a Team Liquid jersey along with a jacket and jeans. He then asks: 'Do I have to change?' To this, someone responds: 'Don't worry, we're pretty chill in esports. Welcome to Team Liquid.'\"","contentLength":3925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PAROL6: 3D-printed desktop robotic arm","url":"https://source-robotics.github.io/PAROL-docs/","date":1739644009,"author":"bo0tzz","guid":504,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43060818"},{"title":"Basketball has evolved into a game of calculated decision-making","url":"https://nabraj.com/blog/basketball-solved-sport/","date":1739643670,"author":"nabaraz","guid":503,"unread":true,"content":"<p>Basketball has evolved from a game of unpredictability into a game of calculated decision-making with the use of data and analytics. From a game of points, assists, and rebounds, it has progressed into using thousands of data points to optimize every element of the game.</p><p>All decisions are made based on numbers not intuition. Long-range shooting and layups are preferred over mid-range shooting. Players are no longer do-it-alls; they are now given specialized roles.</p><p>In the last decade, long-range shooting has gone from a secondary option to a primary choice for building offense. Recently, teams have realized three-pointers have higher point value despite their lower scoring percentage. This has led to a revolution in structuring an offense around taking long-range shots. The Golden State Warriors, led by Stephen Curry, probably jump-started this trend with 34 three-pointer attempts per game in the 2018-19 season, twice as much from five years ago. Celtics, this season, have averaged almost 50 three-pointers attempt this season (2024-25 season).</p><p>In the past, the team built its roster around a big name like Shaq. Most of the offense were from the center. This has now changed, with the primary strategy being to stretch the opposition and take long-range shots.</p><p>The 3-and-D model refers to a player, usually a wing player, who is just above average at three-pointers and plays competent defense.  Forget about positions; just get a guy who can do some 3s and Ds.</p><p>Danny Green is probably the father of this model, with his 40% career three-point field goal percentage and he also made into all-defensive team.</p><p>In  recent years, every team has had at least one 3-and-D model player on the roster.</p><p>Gone are the days of an all-around player. There is no longer a need for a player who does everything. Look at players like Kobe Bryant and Lebron James (early career); they not only scored but guarded defense, caught rebounds and played the role of playmakers.</p><p>Now, it’s all about creating lineups with specialized players. A team typically consists of a three-point shooter, a defensive specialist, a playmaker, and rebounders. They all have specific roles assigned to them.</p><p>A catch-all word for statistics, technology has played a pivotal role in shaping this game. </p><p>In addition to data collection, biomechanics and motion cameras track every player’s movement. NBA even brought SportVU from football; it follows the ball and supposedly captures images 25 times per second. Coaches can now use this to analyze the speed, position, form, and motion of each player on the court. </p><p>In the end, it’s all about optimizing every ball possession. </p><p>Basketball might have lost its flair; every move is now predictable and measured. What is the future of basketball, is anyone’s guess? Maybe a rule change is around the corner?</p>","contentLength":2824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43060769"},{"title":"OpenAI teases a ‘simplified’ GPT-5 model","url":"https://techcrunch.com/2025/02/15/openai-teases-a-simplified-gpt-5-model/","date":1739642700,"author":"Cody Corrall","guid":51,"unread":true,"content":"<p>Welcome back to Week in Review. This week we’re looking at OpenAI canceling the release of o3; TikTok returning to U.S. app stores nearly a month after it was removed; more complications in Elon Musk’s bid to buy OpenAI for $97.4 billion; and more! Let’s do it. OpenAI effectively canceled the release of o3, which […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jill – a functional programming language for the Nand2Tetris platform","url":"https://github.com/mpatajac/jillc","date":1739642652,"author":"mailgolub","guid":502,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43060603"},{"title":"Alzheimer's biomarkers now visible up to a decade ahead of symptoms","url":"https://newatlas.com/brain/alzheimers-dementia/alzheimers-biomarkers-visible-decade-before-symptoms/","date":1739642537,"author":"01-_-","guid":465,"unread":true,"content":"<p>Researchers at the University of Pittsburgh have devised a biomarker test that can spot small amounts of clumping tau protein in the brain and cerebrospinal fluid, which lead to Alzheimer's disease.</p><p>Catching these clumps early while still in minute quantities can enable effective intervention. This test can help detect the tangled proteins years in advance of them appearing prominently in brain scans – as much as up to a decade.</p><p>That's heartening because Alzheimer's disease not only has devastating impact on patients' lives long term, but is also currently incurable. It begins to show up as forgetfulness, and progresses to confusion and disorientation, delusions, hallucinations, and trouble sleeping. As the condition worsens, patients may experience difficulty eating, moving around, incontinence, loss of speech, and significant memory loss.</p><p>“Early detection is key to more successful therapies for Alzheimer’s disease since trials show that patients with little-to-no quantifiable insoluble tau tangles are more likely to benefit from new treatments than those with a significant degree of tau brain deposits,\" explained Thomas Karikari, senior author of the <a href=\"https://www.nature.com/articles/s41591-024-03400-0\" target=\"_blank\" data-cms-ai=\"0\">paper published in </a> this week.</p><p>Here's a quick bit of context on what's happening in and around the brain. <a href=\"https://www.nih.gov/news-events/nih-research-matters/scientists-build-largest-maps-date-cells-human-brain\" target=\"_blank\" data-cms-ai=\"0\">Humans have some 86 billion nerve cells</a>, and they're connected by what are called synapses. These synapses are supported by 'rail tracks' that enable the flow of essential nutrients and information, and they're called microtubules.</p><p>Tau proteins (abbreviated from tubulin associated unit) stabilize these microtubules, and help keep the brain healthy. These proteins can malfunction, clump together and create tangles – preventing the microtubules from functioning properly.</p><p>Here's where this new biomarker test comes in. The researchers have found an important section of the tau protein that causes it to form harmful tangles in the brain. This section is made up of 111 building blocks (amino acids). Within this section, they discovered two specific spots that, when modified, can tell doctors if tau proteins are starting to clump together. This is important because if doctors can detect these changes early enough, they might be able to treat the problem before it gets worse.</p><p>The two specific spots they found (called p-tau-262 and p-tau-356) work like early warning signals, letting doctors know that tau proteins are beginning to malfunction. This could help identify Alzheimer's disease sooner, when treatments might be more effective.</p>","contentLength":2523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43060587"},{"title":"How to Scale AI Infrastructure With Kubernetes and Docker","url":"https://hackernoon.com/how-to-scale-ai-infrastructure-with-kubernetes-and-docker?source=rss","date":1739642406,"author":"Natapong Sornprom","guid":473,"unread":true,"content":"<p>Firms increasingly make use of artificial intelligence (AI) infrastructures to host and manage autonomous workloads. Consequently,<a href=\"https://www.cio.com/article/3577669/as-ai-scales-infrastructure-challenges-emerge.html\"></a> as well as resilient infrastructures that will be able to meet heterogeneous application or cloud requirements. Organizations use<a href=\"https://hackernoon.com/an-intro-to-kubernetes-for-docker-developers\"></a> and<a href=\"https://hackernoon.com/optimizing-docker-images-is-more-than-just-a-one-and-done-thing\"></a> to meet such needs because firms realize that both are highly effective use cases that deliver scalable AI infrastructures.</p><p>\\\nDeploying AI infrastructure typically provides adequate computation power to execute and process large datasets. These demands can translate into the need for scalable methods that enable AI models to run on large workloads without hurting performance.</p><p>, nonetheless, are also resource-intensive, normally demanding both high computing capacity and the ability to process high levels of data. As more advanced AI applications and a larger scale become required, scalability becomes more critical. Scalability ensures that AI systems can handle increasing workloads without any loss of performance.</p><p>The growing amount of data is a concern for AI systems in many facets. Most AI models, especially those based on deep learning, heavily depend on large amounts of data during training and inference. However, without adequate scalable infrastructure, processing and interpreting such<a href=\"https://www.mckinsey.com/west-coast/~/media/mckinsey/featured%20insights/artificial%20intelligence/notes%20from%20the%20ai%20frontier%20applications%20and%20value%20of%20deep%20learning/notes-from-the-ai-frontier-insights-from-hundreds-of-use-cases-discussion-paper.pdf\"></a>.</p><p>Scalable AI hardware supports reliable and stable performance despite drastically overwhelming computational loads. With Kubernetes, horizontal scaling of AI jobs is a breeze, and the dynamic resizing of replica numbers can be done as a function of necessity. In contrast, Docker containers support lean, isolated environments for running AI models where resource conflict is not a performance bottleneck.</p><h3><strong>Effective Resource Management</strong></h3><p>Efficient use of resources is the key to cost-effective and sustainable AI deployment. Kubernetes' resource requests and limits allow for fine-grained CPU and memory resource management by avoiding underprovisioning and overprovisioning. Docker's resource management fills the gap by isolating container resources.</p><h2><strong>Scaling AI Infrastructure With Kubernetes and Docker</strong></h2><p>Containerization is one of the milestones in the evolution of scalable artificial intelligence infrastructure. Containerization of the AI application and its dependencies in a Docker container ensures consistency throughout the development, testing, and deployment environments.</p><p>\\\nFirst, you must define a Dockerfile in order to install the environment. The Dockerfile is a series of instructions about how to build a Docker image. It declares a base image, the dependencies required, and the initial setup commands that apply to your app. The following is a basic Dockerfile for a Python machine-learning model:</p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory in the container\nWORKDIR /usr/src/app\n\n# Copy the current directory contents into the container\nCOPY . .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Expose the port the app runs on\nEXPOSE 5000\n\n# Define environment variable\nENV NAME World\n\n# Run the app\nCMD [\"python\", \"./app.py\"]\n</code></pre><p>\\\nIf the Dockerfile is ready, then you can build the Docker image and run the container. Run the following commands: \\n </p><pre><code># Build the Docker image\ndocker build -t ml-model:latest .\n\n# Run the container\ndocker run -p 5000:5000 ml-model:latest\n</code></pre><h2><strong>Deploying the Dockerized AI Model to Kubernetes</strong></h2><p> provides a wide range of orchestration features that enable efficient application management in the containerized infrastructure. Deployment of the Docker image on Kubernetes ensures that a specified number of application replicas is always running. The following is an example of deployment.yaml file that you can use to<a href=\"https://dev.to/pavanbelagatti/deploy-any-aiml-application-on-kubernetes-a-step-by-step-guide-2i37\"></a>:</p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ml-model-deployment\nspec:\n  replicas: 3  \n  selector:\n    matchLabels:\n      app: ml-model\n  template:\n    metadata:\n      labels:\n        app: ml-model\n    spec:\n      containers:\n      - name: ml-model-container\n        image: ml-model:latest\n        ports:\n        - containerPort: 5000\n</code></pre><p>\\n The above code snippet shows how to deploy the AI model, but you also need to make the model externally accessible. You will need to expose it by defining a Kubernetes Service. The service.yaml below illustrates an example:</p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: ml-model-service\nspec:\n  selector:\n    app: ml-model\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 5000\n  type: LoadBalancer\n</code></pre><p>\\n Use the kubectl command-line tool to apply the deployment and service configurations:</p><pre><code># Deploy the application\nkubectl apply -f deployment.yaml\n\n# Expose the service\nkubectl apply -f service.yaml\n</code></pre><p>Kubernetes provides excellent scaling capabilities to AI environments, maximizing resource utilization and performance. Horizontal scaling is done by adding additional containers, and vertical scaling involves adding additional resources like CPU or memory to a container.</p><p>Horizontal scaling is used to scale up the number of replicas (Pods) of an AI system to handle a higher workload. The process requires enabling dynamic scaling depending on the number of replicas. The command used to enable such a process is `kubectl scale`. The particular command is used to set up the deployment to function up to a maximum of five replicas:</p><p>\\\n`kubectl scale --replicas=5 deployment/ml-model-deployment`</p><p>\\\nThe command scales up the ml-model-deployment to use five replicas of the machine-learning model container. The system dynamically provisions more Pods to meet the required number afterward.</p><h3><strong>Automatic Scaling using the Horizontal Pod Autoscaler (HPA)</strong></h3><p>Kubernetes facilitates auto-scaling using the Horizontal Pod Autoscaler (HPA). The HPA dynamically adjusts the number of replicas based on resource use, i.e., CPU or memory, in relation to set limits. The YAML configuration shown below is a relevant example of an HPA that dynamically scales for ml-model-deployment in response to CPU use:</p><pre><code>apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ml-model-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ml-model-deployment\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 50\n</code></pre><p>\\n In this setup, scaleTargetRef is used to define the Deployment to be scaled, i.e., ml-model-deployment. The minimum replica count is set using MinReplicas, while the maximum replica count is controlled using maxReplicas. In addition, the CPU utilization percentage is set using targetCPUUtilizationPercentage, i.e., to 50%. </p><p>\\\nCPU utilization of more than 50% across all Pods results in scaling up the replica count to a maximum of 10 automatically. As soon as CPU utilization drops below the set percentage, Kubernetes automatically reduces the replica count in order to release resources.</p><p>Horizontal scaling is mainly to cope with more traffic, whereas vertical scaling provides more resources (such as CPU or memory) to existing containers. The process is to scale up or down resource requests and limits in the Kubernetes Deployment. In order to scale up the CPU and memory limits of the ml-model-deployment, one would need to open the deployment.yaml file:</p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ml-model-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ml-model\n  template:\n    metadata:\n      labels:\n        app: ml-model\n    spec:\n      containers:\n      - name: ml-model-container\n        image: ml-model:latest\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: \"1\"\n            memory: \"2Gi\"\n          limits:\n            cpu: \"2\"\n            memory: \"4Gi\"\n</code></pre><p>\\\nIn this updated configuration:</p><ul><li>requests specify the minimum resources required for the container.</li><li>limits define the maximum resources the container can use.</li></ul>","contentLength":7854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"'Mass Theft': Thousands of Artists Call for AI Art Auction to be Cancelled","url":"https://slashdot.org/story/25/02/15/0351257/mass-theft-thousands-of-artists-call-for-ai-art-auction-to-be-cancelled?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739640840,"author":"EditorDavid","guid":186,"unread":true,"content":"An anonymous reader shared this report from the Guardian:\n\nThousands of artists are urging the auction house Christie's to cancel a sale of art created with artificial intelligence, claiming the technology behind the works is committing \"mass theft\". The Augmented Intelligence auction has been described by Christie's as the first AI-dedicated sale by a major auctioneer and features 20 lots with prices ranging from $10,000 to $250,000... \nThe British composer Ed Newton-Rex, a key figure in the campaign by creative professionals for protection of their work and a signatory to the letter, said at least nine of the works appearing in the auction appeared to have used models trained on artists' work. However, other pieces in the auction do not appear to have used such models. \nA spokesperson for Christie's said that \"in most cases\" the AI used to create art in the auction had been trained on the artists' \"own inputs\". \n\nMore than 6,000 people have now signed the letter, which states point-blank that \"Many of the artworks you plan to auction were created using AI models that are known to be trained on copyrighted work without a license.\"\n\nThese models, and the companies behind them, exploit human artists, using their work without permission or payment to build commercial AI products that compete with them. Your support of these models, and the people who use them, rewards and further incentivizes AI companies' mass theft of human artists' work. We ask that, if you have any respect for human artists, you cancel the auction. \n\nLast week ARTnews spoke to Nicole Sales Giles, Christie's vice-president and director of digital art sales (before the open letter was published). And Giles insisted one of the major themes of the auction is \"that AI is not a replacement for human creativity.\"\n\"You can see a lot of human agency in all of these works,\" Giles said. \"In every single work, you're seeing a collaboration between an AI model, a robot, or however the artist has chosen to incorporate AI. It is showing how AI is enhancing creativity and not becoming a substitute for it.\" \n\nOne of the auction's headline lots is a 12-foot-tall robot made by Matr Labs that is guided by artist Alexander Reben's AI model. It will paint a new section of a canvas live during the sale every time the work receives a bid. Reben told ARTnews that he understands the frustrations of artists regarding the AI debate, but he sees \"AI as an incredible tool... AI models which are trained on public data are done so under the idea of 'fair use,' just as search engines once faced scrutiny for organizing book data (which was ultimately found to fall under fair use),\" he said.... \"AI expands creative potential, offering new ways to explore, remix, and evolve artistic expression rather than replace it. The future of art isn't about AI versus artists — it's about how artists wield AI to push boundaries in ways we've never imagined before....\" \n\nDigital artist Jack Butcher has used the open letter to create a minted digital artwork called Undersigned Artists. On X he wrote that the work \"takes a collective act of dissent — an appeal to halt an AI art auction — and turns it into the very thing it resists: a minted piece of digital art. The letter, originally a condemnation of AI-generated works trained on unlicensed human labor, now becomes part of the system it critiques.\" \n\nChristie's will accept cryptocurrency payments for the majority of lots in the sale.\n","contentLength":3474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Programmer's Guide to Game Design: The Major Ingredients You Should Know","url":"https://hackernoon.com/a-programmers-guide-to-game-design-the-major-ingredients-you-should-know?source=rss","date":1739638805,"author":"Chenuli J.","guid":117,"unread":true,"content":"<p>Software developers spend their whole time with complicated problems, and they try to learn almost everything about algorithms, structures, frameworks, and blah-blah-blah. While playing games and coffee have become stress-busters in our lives, why can't building games be the best one?</p><p>\\\nEven though some people find game development insane, it’s easy for software developers because they have almost every skill needed: math, Programming, UX/UI, and the usual stuff. If a normal person takes 6 months to learn a Game Engine, the Developer will take a maximum of 3 months or less.</p><p>\\\nMany people don’t know this, but I started my tech career as a game developer, although I later turned my back on game development and became a Python developer. And no lies, they were a few of the best years in my life.</p><p>\\\nThis article is dedicated to any developer who wants to try something new or exciting (which is game development, yes). Scroll down!</p><p>Software developers who are interested in game development have a remarkable head start. Programming skills are the core of game development. Even if you’re more comfortable with Swift or Ruby, which is not commonly used in game development, you can quickly pick up other Object-oriented programming languages that are much more commonly used for game development, like C# or C++, easier than anyone!</p><p>\\\nIf you're a Python lover, you will love to hear this: There are really good, AAA games made with Python such as Battlefield 2, EVE Online, Civilization IV, and more!</p><p>Not only Python, but almost every commonly used Programming language has libraries that support making games. For example:</p><ul><li>Flutter has Flame, a game engine that supports Flutter language.</li><li>Ruby has Gosu, a library that makes it easy to develop 2D games.</li><li>Python has PyGame, a library that empowers you to create both 2D and 3D games.</li><li>Phaser allows you to create games with JavaScript and HTML5.</li></ul><p>For every beginner, either a software engineer or a novice, I give one common piece of advice— Start as small as possible. It doesn't matter how small pieces of code it has or how messy it is, you've won the challenge!</p><p>\\\nIf you love simple 2D games, I would recommend that you make a Pong game. Pong features simple graphics (2 rectangles and a circle) you can create on your own, minimal sounds, and a game loop. If you want to learn about making multiplayer games, allow matches to occur between two human players over a network. If you want to learn about AI, allow the player to challenge the computer.</p><p>\\\nAnd if you love 3D games like me, start with Cube Run. I haven't made it without a Game engine because, 3D becomes a bit harder with Python or others but with Unity, it's the best game I recommend to take a start.</p><h2><strong>Major Ingredients of A Game</strong></h2><p>If you make a cake without Sugar, no one will eat it. Except for diabetic people, of course, like my mom.</p><p>\\\nThe same goes for video games. It’s full of ingredients, some of which are required for every game and some of which are optional. I’ll introduce them briefly.</p><p>In many games, the level itself is a challenge, trickier than the smartest AI enemies. Series like Tomb Raider also emphasize complex and challenging level design.</p><p>\\\nWhile the advent of open-world games like GTA may make the level design seem less important than in bygone times, it’s worth noting that even open-world games have ‘levels’, such as a particular building, structure, or map area you must enter to achieve a goal.</p><p>\\\nTo reduce player feelings of being railroaded, levels will ideally have multiple possible paths through them.</p><p>In games, you can’t really rely on natural light sources to illuminate your video game.</p><p>\\\n(You don’t think there’s a sun inside a game engine, do you?)</p><p>\\\nEvery light source in a video game must be added by hand and light manipulation is incredibly important. Light can be used for all of the following:</p><ul><li>Controlling the player’s ability . In horror and survival games, light is a resource that must be carefully managed.</li></ul><ul><li>Controlling a player’s ability . In games with an element of stealth, dark areas can provide cover while well-lit areas represent a difficult challenge.</li></ul><ul><li>Setting the mood. The quality of light can be used to set the mood, with sunny and bright lighting associated with happy times and brooding light associated with dark times.</li></ul><ul><li>Lighting the way. Light can be used to direct the player’s attention. The best-designed levels in video games often make clever use of light to guide the player in the right direction when they might otherwise be lost.</li></ul><p>Game art is the medium through which the game world is presented to the player. In a sense, all the programming effort that goes into making video games is an attempt to turn game art into something that feels responsive and alive. Game art is an umbrella term that includes textures, 3D models, sprites, particle effects, and lighting.</p><p>\\\nAnd yes, it’s a broad area, best to not cover it in this article.</p><p>Unlike in the real world, video game sounds cannot be made by accident. Every sound in the game universe must be added by hand, and it is through layering these sounds that the game world starts to feel lifelike. You also need to be mindful of sounds triggered by the player, by other characters, and ambient sounds that create the game world environment.</p><p>\\\nAs an example, the player accidentally hits a metal object; if it doesn't emit a sound, it doesn't feel natural, or the scientist who claimed that metals have sonorous properties got it wrong.</p><p>\\\nAnother ever-present fact of video games is music, used to create an emotional response in the player or removed entirely to leave behind an eerie silence. Unlike most compositions, video game music must loop seamlessly. It must also transition smoothly to new compositions based on in-game events, such as being spotted by an enemy.</p><p>\\\nHere are some of my favorite places to find sound:</p><p>You have made graphics, levels, and everything but the whole game feels like a dead body. If you want your game to be alive, you need some lines of code.</p><p>\\\nMost of my readers are probably a developer already, so you probably know the importance of programming anywhere. I'm not going to give a lecture on \"what is programming\", but here’s a bit about programming in game design.</p><p>\\\nFirst of all, you need to decide one thing: Are you making a game by using a Game Engine + Language or building a game from scratch with Python or something? It's your choice but let me help you: If you're going to build games for fun, choose the first option because it's easier. </p><p>\\\nAs I said in the beginning, programmers always juggle complex problems so you probably shouldn't take more stress with that too.</p><p>\\\n(Game Engines are the software used to create video games.)</p><p>\\\nThere are many of them, but the most popular ones are:</p><ul><li><p>Unity (Great for beginners, recommended)</p></li></ul><p>Next, choose a language to get started. Mostly, C# and C are used. Don't worry, you already got a headstart in knowing at least one programming language; most people start even before they know what the word “programming“stands for.</p><p>\\\nAnd that’s about it. Well, of course, there are many other optional ingredients that video games consist of but in general, the above are basically the starter pack.</p><p>Starting game development is easier for software developers than anyone. As someone with programming skills, you have a massive head-start on the average video game hobbyist who wants to learn how to make a game. If I scroll to the top of the article, I can list the below points as key takeaways.</p><ul><li><p>It's a lot easier to find a game dev library in your comfortable programming language to get started.</p></li><li><p>Start with a small game, maybe a clone of an existing game.</p></li><li><p>Game Engines make your life a lot easier.</p></li><li><p>If you want to make a big, impressive game but don’t have a lot of time to spare, consider teaming up with others or joining a modding community.</p></li></ul><p>And that's all for now, Happy Designing! 🏎</p><p>\\\nIf you loved this article, make sure to subscribe using your email, so you can read all my content inside your inbox without missing any!</p><p>It’s totally free of charge and I don’t even have time to send spam emails.</p>","contentLength":8132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Marc Andreessen dreams of making a16z a lasting company, beyond partnerships","url":"https://techcrunch.com/2025/02/15/marc-andreessen-dreams-of-making-a16z-a-lasting-company-beyond-partnerships/","date":1739638800,"author":"Marina Temkin","guid":50,"unread":true,"content":"<p>Many venture industry observers have wondered whether Andreessen Horowitz, a firm that manages $45 billion, has its sights on eventually becoming a publicly traded company. Co-founder Marc Andreessen said he isn’t “chomping at the bit to take the firm public,” on this week’s Invest Like the Best podcast. But he discussed his goal of building […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"More Solar and Battery Storage Added to TX Grid Than Other Power Src Last Year","url":"https://insideclimatenews.org/news/10022025/solar-battery-storage-texas-grid/","date":1739637424,"author":"indigodaddy","guid":464,"unread":true,"content":"<p>As the market for renewables in Texas continues to strengthen and innovate, the power makeup of the state’s electric grid is slated to keep shifting toward adding more renewables. Last year, solar and battery storage installation led capacity growth within Texas’ electric grid, according to <a href=\"https://www.dallasfed.org/research/economics/2025/0114\">research from the Federal Reserve Bank of Dallas</a> published in January.&nbsp;</p><p>Texas added nearly 1,500 megawatts of battery storage to the grid’s summer rated capacities in 2023. That figure almost tripled to 4,374 megawatts added in 2024, according to the report.&nbsp;</p><p>Capacity from solar power added to the grid enjoyed a similar trajectory. In 2023, 4,570 megawatts were added to the grid, while in 2024, nearly 9,700 megawatts were added.&nbsp;</p><p>Given that Texas has its own isolated energy grid, the Electricity Reliability Council of Texas (ERCOT) is responsible for managing the majority of energy for its residents.&nbsp;</p><div><p>Please take a look at the new openings in our newsroom.</p><a href=\"https://insideclimatenews.org/about/jobs/\" target=\"_blank\">See jobs</a></div><p>Historically, ERCOT, which manages 90 percent of the state’s power load, has primarily relied on natural gas. But other energy sources, like wind and solar, have played a critical role in offsetting high demand.&nbsp;</p><p>ERCOT added 3,400 megawatts from natural gas power plants in 2024. That’s after more than 1,000 megawatts of natural gas power were inactive within the grid from 2021 to 2023, according to Dallas Fed data.&nbsp;</p><p>The capacity growth from solar and battery storage allowed the grid to manage another hot Texas summer in 2024, reported Garret Golding, an assistant vice president for energy program at the Dallas Fed.&nbsp;</p><p>Battery storage is relatively new to ERCOT. One of the first battery power storage plants to be connected to ERCOT was in 2021, southeast of Dallas in Kaufman County.&nbsp;</p><p>The 50-megawatt plant run by Enel, one of the largest renewable energy owners and operators in the country, was the first of Enel’s 14 battery projects it has since developed across the state.&nbsp;</p><p>The forecasted power demand in Texas from population growth and heavy load users like data centers, cryptocurrency mining and artificial intelligence, alongside the competitive market for batteries, is dictating the rising use of storage within ERCOT, said Randald Bartlett, a senior director of operations and management for battery energy storage systems at Enel North America.</p><p>Texas’ permitting processes and ability to develop has made it easier to add and build new capacity in comparison to other states with more laborious entryways, Bartlett said.&nbsp;</p><p>Before, there wasn’t really adequate criteria and evidence to forecast what batteries would contribute to the grid, ERCOT CEO Pablo Vegas said during a board of directors meeting on Tuesday.&nbsp;</p><p>Now, the grid operator added battery contribution to its report forecasting future capacity, demand and reserves.&nbsp;</p><p>Battery storage in the ERCOT grid has nearly doubled every year since 2021, Vegas said. At the end of 2024, there were nearly 10,000 megawatts from batteries within ERCOT.</p><p>Vegas said the capacity from batteries made a significant difference in ERCOT during bridge hours, or the winter morning hours when the sun has yet to rise and in the evenings after the sun sets.&nbsp;</p><p>“Batteries made a meaningful contribution to what those shoulder periods look like and how much scarcity we get into during these peak events,” Vegas said when analyzing the grid’s performance during recent winter storms.&nbsp;</p><p>In the spring of 2024, Texas’ installation of utility scale solar outpaced California’s, and jumped from 1,900 megawatts in 2019 to over 20,000 megawatts in 2024. As a result, solar met nearly 50 percent of the state’s peak power demand on some days.&nbsp;</p><p>The state’s quick deployment of utility scale solar didn’t happen overnight. It started in 2005, when the legislature instructed the Public Utility Commission of Texas to create competitive renewable energy zones, where the state planned transmission lines to connect cities to renewable energy sources in west Texas.</p><p>Initially, it was intended to capture wind power but was able to quickly include solar because of the existing infrastructure, said Dustin Mulvaney, an environmental studies professor at San Jose State University and an author of Planning to Build Faster: A Solar Energy Case Study, published in October by the Roosevelt Institute.&nbsp;</p><p>That forward-looking plan is often held up as a model renewable energy advocates and developers say the Federal Energy Regulatory Commission could implement across other regional transmission organizations, by requiring proactive planning and by creating rules of how to pay for transmission systems.&nbsp;</p><p>As the state’s grid continues to experience a rapid shift in the type of generation available to serve demand, the state’s grid operator is looking to build a higher voltage transmission system, upgrading from 345-kilovolt lines to 765-kilovolt lines.&nbsp;</p><p>In 2024, nearly 78 gigawatts of transmission-connected wind, solar and battery energy storage capacity was installed to the grid. And more than 102 gigawatts of transmission-connected renewable capacity is expected to be installed by the end of 2025, according to a December ERCOT report.</p><p>It’s that growth of both demand and renewables connected to the grid that’s led ERCOT to ask the Public Utility Commission to consider upgrading the state’s transmission system rather than expanding its existing one.&nbsp;</p><p>The 765-kV lines would provide significant economic and reliability benefits to the ERCOT system, wrote Kristi Hobbs, ERCOT’s vice president of system planning and weatherization, in the grid operator’s submission of its regional transmission plans to the commission in late January.</p><p>Regardless of which transmission plan is chosen, Hobbs wrote, the explosive growth projected throughout the next six years and beyond will require major public investment.&nbsp;</p><div><div><p>Perhaps you noticed: This story, like all the news we publish, is free to read. That’s because Inside Climate News is a 501c3 nonprofit organization. We do not charge a subscription fee, lock our news behind a paywall, or clutter our website with ads. We make our news on climate and the environment freely available to you and anyone who wants it.</p><p>That’s not all. We also share our news for free with scores of other media organizations around the country. Many of them can’t afford to do environmental journalism of their own. We’ve built bureaus from coast to coast to report local stories, collaborate with local newsrooms and co-publish articles so that this vital work is shared as widely as possible.</p><p>Two of us launched ICN in 2007. Six years later we earned a Pulitzer Prize for National Reporting, and now we run the oldest and largest dedicated climate newsroom in the nation. We tell the story in all its complexity. We hold polluters accountable. We expose environmental injustice. We debunk misinformation. We scrutinize solutions and inspire action.</p><p>Donations from readers like you fund every aspect of what we do. If you don’t already, will you support our ongoing work, our reporting on the biggest crisis facing our planet, and help us reach even more readers in more places? </p><p>Please take a moment to make a tax-deductible donation. Every one of them makes a difference.</p></div></div><div><div><h4>Reporter, Texas Renewables</h4><p>Arcelia Martin is an award-winning journalist at Inside Climate News. She covers renewable energy in Texas from her base in Dallas. Before joining ICN in 2025, Arcelia was a staff writer at The Dallas Morning News and at The Tennessean. Originally from San Diego, California, she’s a graduate of Gonzaga University and Columbia University Graduate School of Journalism.</p></div></div>","contentLength":7659,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43059826"},{"title":"ISS Astronauts Give Space-to-Earth Interview Weeks Before Finally Returning to Earth","url":"https://science.slashdot.org/story/25/02/15/033223/iss-astronauts-give-space-to-earth-interview-weeks-before-finally-returning-to-earth?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739637240,"author":"EditorDavid","guid":185,"unread":true,"content":"Last June two NASA astronauts flew to the International Space Station on the first crewed test flight of Boeing's Starliner. But they aren't stranded there, and they weren't abandoned, the astronauts reminded CNN this week in a rare space-to-earth interview:\n\n\"That's been the rhetoric. That's been the narrative from day one: stranded, abandoned, stuck — and I get it. We both get it,\" [NASA astronaut Butch] Wilmore said. \"But that is, again, not what our human spaceflight program is about. We don't feel abandoned, we don't feel stuck, we don't feel stranded.\" Wilmore added a request: \"If you'll help us change the rhetoric, help us change the narrative. Let's change it to 'prepared and committed.' \n\n\"That's what we prefer,\" he said... \n[NASA astronaut Suni] Williams also reiterated a sentiment she has expressed on several occasions, including in interviews conducted before she left Earth. \"Butch and I knew this was a test flight,\" she told CNN's Cooper, acknowledging the pair has been prepared for contingencies and understood that the stay in space might be extended. \"We knew that we would probably find some things (wrong with Starliner) and we found some stuff, and so that was not a surprise,\" she said. \nWhen Cooper opened the interview by asking the astronauts how they're doing, Williams answers \"We're doing pretty darn good, actually,\" pointing out they had plenty of food and great crew members. And Wilmore added that crews come to the space station on a careful cycle, and \"to alter that cycle sends ripple effects all the way down the chain. We would never expect to come back just special for us or anyone unless it was a medical issue or something really out of the circumstances along those lines. So we need to come back and keep the normal cycle going...\" \n\nCNN's article notes a new announcement from NASA Tuesday that the astronauts might return a couple weeks early \"after opting to change the SpaceX Crew Dragon capsule it will use.\" That mission's targeted launch date is now March 12. \n\nIn the meantime, Williams says in the interview, \"We do have some internet connection up here, so we can get some internet live. We've gotten football. It's been this crew's go-to this past fall. Also YouTube or something like that. It's not continuous — it has chunks of time that we get it. And we use that same system also to make phone calls home, so we can talk to our families, and do videoconferences even on the weekends as well. This place is a pretty nice place to live, for the most part.\" \n\nAnd they're also \"working on with folks on the ground\" to test the NASA's cube-shaped, free-flying robotic Astrobees.","contentLength":2649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5","url":"https://github.com/b4rtaz/distributed-llama/discussions/162","date":1739635889,"author":"b4rtazz","guid":463,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43059579"},{"title":"NTSYNC Driver Fix Being Worked On For Proper User Permissions","url":"https://www.phoronix.com/news/Linux-NTSYNC-Permissions-Issue","date":1739635532,"author":"Michael Larabel","guid":367,"unread":true,"content":"<article>One of the great new features of Linux 6.14 is the NTSYNC driver being completed for better emulating the Microsoft Windows NT synchronization primitives so that software like Wine and Proton (Steam Play) can provide for better performance when running Windows games on Linux. But it turns out an oversight up to now has meant that in practice it's not really too usable out-of-the-box...</article>","contentLength":388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: No Startup Has Ever Failed Because it Didn’t Have a Blog (2/15/2025)","url":"https://hackernoon.com/2-15-2025-newsletter?source=rss","date":1739635457,"author":"Noonification","guid":116,"unread":true,"content":"<p>🪐 What’s happening in tech today, February 15, 2025?</p><p>By <a href=\"https://hackernoon.com/u/realgpp\">@realgpp</a> [ 9 Min read ] Learn how to read thread dumps and take control of your application’s runtime behaviour.\n <a href=\"https://hackernoon.com/how-to-expose-and-fix-hidden-bottlenecks-in-adobe-experience-manager\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bigmao\">@bigmao</a> [ 6 Min read ] The case against content marketing, and how to do inbound marketing in the post-content age.  <a href=\"https://hackernoon.com/no-startup-has-ever-failed-because-it-didnt-have-a-blog\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/loadbalancer\">@loadbalancer</a> [ 5 Min read ] Researchers have optimized Layer-7 load balancing using programmable SmartNICs to improve efficiency, cost, and energy use in cloud data centers. <a href=\"https://hackernoon.com/cloud-giants-spend-a-fortune-on-load-balancersthis-research-could-change-that\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jeep Claims 'Software Glitch' Disabled Opting-Out of In-Vehicle Pop-Up Ads in 'a Few' Cases","url":"https://tech.slashdot.org/story/25/02/15/0149202/jeep-claims-software-glitch-disabled-opting-out-of-in-vehicle-pop-up-ads-in-a-few-cases?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739633640,"author":"EditorDavid","guid":184,"unread":true,"content":"Remember Jeep's new in-dash pop-up ads which reportedly appeared every time you stopped? \n\"Since I'm a journalist, or at least close enough, I decided that I should at least get Stellantis/Jeep's side of things,\" writes car-culture site The Autopian:\n\n\nWould Stellantis do something so woefully misguided and annoying? I reached out to our Stellantis/Jeep contact to ask and was initially told that they were \"investigating\" on their end, which to me felt like a stalling tactic while the proper ass-covering plans were conceived. I eventually got this response from a Stellantis spokesperson: \n\n \"This was an in-vehicle message designed to inform Jeep customers about Mopar extended vehicle care options. A temporary software glitch affected the ability to instantly opt out in a few isolated cases, though instant opt-out is the standard for all our in-vehicle messages. Our team had already identified and corrected the error, and we are following up directly with the customer to ensure the matter is fully resolved...\" \n\nI suppose a glitch is possible, though I've not seen any examples of this ad popping up with the instant opt-out option available, but I guess it must exist, since not all Jeep owners seem to have had to deal with these ads. I suspect if this was happening to more people than these \"few isolated cases\" we'd still be cleaning up from the aftermath of the riots and uprisings. \n\nBecause, as they write, \"Really, I can't think of a quicker way to incur the wrath of nearly every human...\"","contentLength":1513,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Carbon capture more costly than switching to renewables, researchers find","url":"https://techxplore.com/news/2025-02-carbon-capture-renewables.html","date":1739631990,"author":"Brajeshwar","guid":462,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058997"},{"title":"Dust from car brakes more harmful than exhaust, study finds","url":"https://e360.yale.edu/digest/brake-pads-lung-damage-study","date":1739631975,"author":"Brajeshwar","guid":461,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058993"},{"title":"What is an encryption backdoor?","url":"https://techcrunch.com/2025/02/15/what-is-an-encryption-backdoor/","date":1739631600,"author":"Natasha Lomas","guid":49,"unread":true,"content":"<p>Talk of backdoors in encrypted services is once again doing the rounds after reports emerged that the U.K. government is seeking to force Apple to open up iCloud’s end-to-end encrypted (E2EE) device backup offering. Officials were said to be leaning on Apple to create a “backdoor” in the service that would allow state actors to […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Antarctica's Only Insect","url":"https://www.404media.co/antarcticas-only-insect/","date":1739628013,"author":"Becky Ferreira","guid":389,"unread":true,"content":"<img src=\"https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-09.17.41@2x.png\" alt=\"Antarctica's Only Insect\"><p>Welcome <a href=\"https://www.404media.co/neanderthals-would-rather-die-than-talk-to-you-3/\" rel=\"noreferrer\">back to the Abstract</a>, 404 Media's weekly roundup of scientific studies to distract us from our present dystopia!</p><p>This week, we are traveling back in time to 16th century Transylvania, so please make sure you are up to date on your bubonic plague shots. A study reconstructed wild weather events through the eyes of record-keepers during this fraught period, opening a tantalizing window into climate extremes unleashed by a vengeful God (according to contemporary reports).</p><p>Then: making love the medaka way (get those anal fins ready). Next, the chillest insect in Antarctica (also: the only one). Finally, these turtles will dance for food, and yes, it’s very cute.</p><h3><strong>The Haunting Weather Reports of 16th Century Transylvania</strong></h3><p>Rejoice, for this week has delivered one of the best varieties of study: Science via historical documents. Sure, ice cores and geological strata are great for reconstructing past climates, but nobody can bitch about the weather better than a good old-fashioned red-blooded member of team .&nbsp;</p><p>To that end, researchers searched for mentions of weird weather across a trove of diaries, monastery records, travel notes, and other documents from 16th century Transylvania, during a “pivotal moment in climate history” when a centuries-long cooling event called the Little Ice Age intensified, according to researchers led by Ovidiu Răzvan Gaceu of the University of Oradea.&nbsp;</p><p>These types of studies are packed with colorful human testimonies that can corroborate natural records. More importantly, though, they are just fun to read, especially during such an evocative time and place, freshly haunted by the vampiric spirit of Vlad the Impaler. Some highlights:</p><p>In August 1526, heavy rainfall caused freak floods in Braşov that “washed the walls of the fortress, demolished the main gate, and the fish also got caught in the big church,” according to the Annals of Brașov. Fish in the church! The ultimate baptism.&nbsp;</p><p>&nbsp;In autumn 1553, people in the city of Cluj reported unusual weather events including “October strawberries.” For real, October is for pumpkins, get out of here with the strawbs. Turned out it was a bad omen—there was a plague the following winter. Keep that in mind if you see any late autumn strawberries: Kill on sight.</p><p>Naturally, a lot of these accounts are heartbreaking. Locusts “sometimes covered the whole sky and destroyed grain crops” and caused terrible famines. A storm-related fire “killed 14 people and made 60 poor.” On September 29, 1582, “there was such a big storm, as it was said that it had never been seen before in the city of Cluj, which uprooted the trees and raised the roofs of the houses, people believed that it is sent by divinity to punish the crimes committed by them.”&nbsp;</p><p>I mean, I’m not saying these people weren’t doing crimes. It’s 16th century Transylvania. Do what you gotta do. But that's not why there is extreme weather. You’re just in the Little Ice Age.&nbsp;</p><p>The study ultimately identified “multiple pieces of evidence associated with extreme weather events, including 40 unusually warm summers and several years of excess precipitation or drought.” Taken together with natural archives, the documents paint a picture of troubled times, exacerbated by an unstable climate and possible emergent vampires. Relatable!&nbsp;</p><p>Valentine’s Day is over, but the romantic mood is still in the air—or in the water, if you’re a medaka (flawless segue). Scientists have discovered that wild medaka, also known as Japanese rice fish, are fans of late-night booty calls, which is a behavior that has not been observed in captivity.</p><p>“Although medaka and other model organisms are invaluable in laboratories, their ecology in the wild remains largely unknown,” said researchers led by Yuki Kondo of Osaka Metropolitan University. “This study showed that medaka in the wild initiate spawning during late nocturnal hours and exhibit vigorous courtship behavior at midnight.”</p><p>Kondo and her colleagues recorded this vigorous courtship by placing GoPros into streams over the course of several summer nights in Gifu, Japan. The tapes revealed that medaka like to spawn in the dark, possibly to avoid predators during copulation. The results “provide the first empirical evidence that medaka mating begins significantly earlier than previously reported in the laboratory.”&nbsp;&nbsp;</p><p>For anyone who feels clueless about courtship, may I offer a page from the Medaka Sutra:&nbsp;</p><p>“The spawning behavior of medaka follows a sequence of events: the male chases the female (following), the male swims rapidly around the female (quick circle), the male wraps his dorsal and anal fins around the female (wrapping), the female releases eggs, the male releases sperm (egg and sperm release), and the male leaves the female (leaving),” according to Kondo’s team.</p><p>The only true love language is, indeed, spoken with anal fins.</p><p>Major bonus points also go to Osaka Metropolitan University’s press team for <a href=\"https://www.eurekalert.org/multimedia/1058890?ref=404media.co\"><u>throwing together this version</u></a> of Edward Hopper’s famous “Nighthawks” painting with medaka getting drinks at a bar that is also named Medaka. It is genuinely one of the most inspired public relations efforts I have ever seen, and I’m going to get a print of it to hang on my wall.</p><h3><strong>The Insect at the Edge of Earth</strong></h3><p>, or the Antarctic midge, is the only insect that lives year-round on its namesake continent. Do you know how weird you have to be to be the  insect somewhere? But this midge doesn’t care. It just lives out its bug life, which lasts two years, in an otherwise bugless wasteland.&nbsp;</p><p>Humans definitely care about the midge, though—how could we not? What is it doing there? How is it not dead? What can it teach us about cryopreservation? These questions are addressed in a new study that resolved mysteries about the animal’s interesting life cycle.</p><p>“Freeze tolerance and cryoprotective dehydration are cold tolerance strategies used by various invertebrate species in polar regions and indeed,  utilises both for overwintering,” said researchers led by Mizuki Yoshida of the Ohio State University, who completed the project while at Osaka Metropolitan University (OMU killing it this week).&nbsp;</p><p>“Larvae that are frozen in ice and cryoprotectively dehydrated readily survived 32 days of simulated overwintering,” the team said. “Unlike many insects restricted to highly specific microhabitats,  larvae inhabit a remarkably diverse range of substrates that differ in vegetation, substrate type, slope, drainage, and thermal and hydric conditions.”</p><p>I love the phrasing of “readily survived” as if the midges were eager to show off their cryoprotective superpowers. After this 32-day period they emerged with “That all you got?” energy. By studying the bugs in these simulated conditions, the researchers confirmed that they rely on multiple overwintering strategies, including a state of arrested development called “obligate diapause.”&nbsp;</p><p>“Diapause has long been assumed to be uncommon in Antarctic species, but the present study reveals that  utilises diapause for seasonal adaptation, as in many temperate species,” Yoshida and her colleagues said.&nbsp;</p><p>In addition to being the only endemic Antarctic insect, this midge has the smallest genome of any known insect while also being the largest fully terrestrial animal on the continent, even though it’s only a few millimeters long. In other words, it is the biggest animal in Antarctica that doesn’t fly or swim. Okay, Antarctic midge. You just keep doing you.</p><p>Last, turtles do a little victory dance when they find food. Yes, it is cute. Yes, there is a video.</p><p>The footage (along with <a href=\"https://youtu.be/IcI6yHr6JXo?si=DAGsav7HY4S3uhKn&amp;ref=404media.co\"></a>) is part of a study that tested if turtles could distinguish the magnetic signatures of two geographical areas. When the turtles were exposed to signatures associated with an area they associated with food, they danced in anticipation of a meal, demonstrating that they could tell the signals apart—and party accordingly.&nbsp;&nbsp;</p><p>“Hallmarks of the behaviour include some or all of the following: tilting the body vertically, holding the head near or above water, opening the mouth, rapid alternating movement of the front flippers, and, occasionally, even spinning in place, hence the name ‘turtle dance,’” said researchers led by Kayla Goforth of Texas A&amp;M University. “Turtles exhibited significantly higher levels of turtle dance behaviour when experiencing the field in which they had been fed.”</p><p>With that, let’s all tilt vertically, spin in place, and shell-abrate the long weekend.&nbsp;</p><p>Thanks for reading! See you next week.&nbsp;&nbsp;</p>","contentLength":8613,"flags":null,"enclosureUrl":"https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-09.17.41@2x.png","enclosureMime":"","commentsUrl":null},{"title":"Paragraf Is Building a \"Blank Canvas\" Graphene Foundry","url":"https://spectrum.ieee.org/paragraf-graphene-foundry","date":1739628003,"author":"Liam Critchley","guid":78,"unread":true,"content":"<p>The company wants to make graphene sensors more accessible to industries</p>","contentLength":72,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjQ2NTMyMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4MzQyMzgzM30.wJGF4-_y2VSVjXLsYFhhL9DuC-xHiTHWB-Ciq4DHQTU/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Diablo hackers uncovered a speedrun scandal","url":"https://arstechnica.com/gaming/2025/02/the-diablo-hackers-that-debunked-a-record-speedrun/","date":1739628000,"author":"pitwin","guid":460,"unread":true,"content":"<p>But simply splitting a run into segments doesn't explain away all of the problems the TAS team found. Getting Naj's Puzzler on dungeon level 9, for instance, still requires outside modification of a save file, which is specifically prohibited by <a href=\"https://kb.speeddemosarchive.com/Rules\">longstanding Speed Demos Archive rules</a> that \"manually editing/adding/removing game files is generally not allowed.\" Groobo's apparent splicing of multiple game versions and differently seeded save files also seems to go against SDA rules, which say that \"there obviously needs to be continuity between segments in terms of inventory, experience points or whatever is applicable for the individual game.\"</p><p>After being presented with the TAS team's evidence, SDA <a href=\"https://speeddemosarchive.com/\">wrote</a> that \"it has been determined that Groobo's run very likely does not stem from only legitimate techniques, and as such, has itself been banished barring new developments.\" But Groobo's record is <a href=\"https://www.guinnessworldrecords.com/world-records/110580-fastest-completion-of-an-rpg-videogame\">still listed as the \"Fastest completion of an RPG videogame\"</a> by Guinness World Records, which has not offered a substantive response to the team's findings (Guinness has not responded to a request for comment from Ars Technica).</p><figure><div><div>\n      A recent  speedrun on a confirmed legitimate dungeon seed.\n\n          </div></div></figure><p>This might seem like a pretty petty issue to spend weeks of time and attention debunking. But at a recent presentation attended by Ars, Cecil said he was motivated to pursue it because \"it did harm. Groobo's alleged cheating in 2009 completely stopped interest in speedrunning this category [of ]. No one tried, no one could.\"</p><p>Because of Groobo's previously unknown modifications to make an impossible-to-beat run, \"this big running community just stopped trying to run this game in that category,\" Cecil said. \"For more than a decade, this had a chilling impact on that community.\" With Groobo's run out of the way, though, new runners are <a href=\"https://www.youtube.com/watch?v=bXG1vW6VEKA\">setting new records on confirmed legitimate RNG seeds</a>, and <a href=\"https://www.youtube.com/watch?v=F9mn5CpQCFw\">with the aid of TAS tools</a>.</p><p>In the end, Cecil said he hopes the evidence regarding Groobo's run will make people look more carefully at other record submissions. \"Groobo had created a number of well-respected ... speedruns,\" he said. \"[People thought] there wasn't any good reason to doubt him. In other words, there was bias in familiarity. This was a familiar character. Why would they cheat?\"</p>","contentLength":2302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058522"},{"title":"These Google Photos alternatives offer tons of storage options at a reasonable price","url":"https://techcrunch.com/2025/02/15/these-google-photos-alternatives-offer-tons-of-storage-options-at-a-reasonable-price/","date":1739628000,"author":"Ivan Mehta","guid":48,"unread":true,"content":"<p>Google Photos is a great service for storing images across devices. But Google Drive and Gmail only offer 15GB of storage for free. Google Photos used to offer free unlimited storage of images, but that is not the case anymore. If you are looking for a better photo storage plan, different features, or just want […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Will AI Lead to the Disintermediation of Knowledge?","url":"https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/","date":1739626129,"author":"Bill Schmarzo","guid":63,"unread":true,"content":"<p>Key Blog Points: For decades, organizations have operated under the central assumption that knowledge flows downward. Senior leaders, industry veterans, and domain experts have traditionally been the primary gatekeepers of critical information. Their insights, honed over years of experience, have been the cornerstone of strategic decision-making. Enter artificial intelligence (AI). Many folks are concerned that…&nbsp;<a href=\"https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/\" rel=\"bookmark\">Read More »</a></p>","contentLength":431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I Built a Reddit-style Bluesky client – still rough, but open to ideas","url":"https://threadsky.app/","date":1739625557,"author":"lakshikag","guid":453,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058285"},{"title":"Beyond Chat: Bringing Models to The Canvas • Lu Wilson • YOW! 2024","url":"https://www.youtube.com/watch?v=pLvMsGG7zE8","date":1739624470,"author":"GOTO Conferences","guid":327,"unread":true,"content":"<article>This presentation was recorded at YOW! Australia 2024. #GOTOcon #YOW\nhttps://yowcon.com\n\nLu Wilson - Software Engineer at tldraw @TodePond \n\nRESOURCES\nhttps://bsky.app/profile/todepond.com\nhttps://mastodon.social/@TodePond\nhttps://twitter.com/TodePond\nhttps://www.todepond.com\n\nLinks\nhttps://tldraw.dev\nhttps://makereal.tldraw.com\nhttps://drawfast.tldraw.com\nhttps://teach.tldraw.com\n\nABSTRACT\nWhenever a new technology appears, our first instinct as developers is to offer \"text\" as the primary method of interaction. This has happened throughout computing history, with the computer terminal, with early smartphones, and now it's happening again with AI.\n\nAt tldraw, we’ve been working on moving AI interaction away from the chat-based interface, towards a richer canvas environment. It hasn't been easy! I'll show you all the challenges we've faced, and how we're currently overcoming them. Some of the solutions have been surprising. [...]\n\nTIMECODES\n00:00 Intro\n00:58 The canvas\n06:16 Beyond chat?\n07:09 Demo\n09:25 Demo: Make Real\n14:52 Demo: Draw Fast\n19:01 Demo: Teach\n27:28 Demo: Computer\n35:42 Demo: Fight Simulator\n28:13 Conclusion\n41:10 Outro\n\nRead the full abstract here:\nhttps://yowcon.com/brisbane-2024/sessions/3533\n\nRECOMMENDED BOOKS\nAlex Castrounis • AI for People and Business • https://amzn.to/3NYKKTo\nPhil Winder • Reinforcement Learning • https://amzn.to/3t1S1VZ\nHolden Karau, Trevor Grant, Boris Lublinsky, Richard Liu &amp; Ilan Filonenko • Kubeflow for Machine Learning • https://amzn.to/3JVngcx\nKelleher &amp; Tierney • Data Science (The MIT Press Essential Knowledge series) • https://amzn.to/3AQmIRg\nLakshmanan, Robinson &amp; Munn • Machine Learning Design Patterns • https://amzn.to/2ZD7t0x\nLakshmanan, Görner &amp; Gillard • Practical Machine Learning for Computer Vision • https://amzn.to/3m9HNjP\n\nhttps://bsky.app/profile/gotocon.com\nhttps://twitter.com/GOTOcon\nhttps://www.linkedin.com/company/goto-\nhttps://www.instagram.com/goto_con\nhttps://www.facebook.com/GOTOConferences\n#AI #GenAI #GenerativeAI #ArtificialIntelligence #ChatGPT #ML #MakeRealtldraw #MakeReal #tldraw #Teachtldraw #AIDriven&nbsp;#LuWilson #YOWcon\n\nCHANNEL MEMBERSHIP BONUS\nJoin this channel to get early access to videos &amp; other perks:\nhttps://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join\n\nLooking for a unique learning experience?\nAttend the next GOTO conference near you! Get your ticket at https://gotopia.tech\nSign up for updates and specials at https://gotopia.tech/newsletter\n\nSUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.\nhttps://www.youtube.com/user/GotoConferences/?sub_confirmation=1</article>","contentLength":2626,"flags":null,"enclosureUrl":"https://www.youtube.com/v/pLvMsGG7zE8?version=3","enclosureMime":"","commentsUrl":null},{"title":"The IRS Is Buying an AI Supercomputer From Nvidia","url":"https://tech.slashdot.org/story/25/02/15/0540249/the-irs-is-buying-an-ai-supercomputer-from-nvidia?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739624400,"author":"BeauHD","guid":183,"unread":true,"content":"According to The Intercept, the IRS is set to purchase an Nvidia SuperPod AI supercomputer to enhance its machine learning capabilities for tasks like fraud detection and taxpayer behavior analysis. From the report: With Elon Musk's so-called Department of Government Efficiency installing itself at the IRS amid a broader push to replace federal bureaucracy with machine-learning software, the tax agency's computing center in Martinsburg, West Virginia, will soon be home to a state-of-the-art Nvidia SuperPod AI computing cluster. According to the previously unreported February 5 acquisition document, the setup will combine 31 separate Nvidia servers, each containing eight of the company's flagship Blackwell processors designed to train and operate artificial intelligence models that power tools like ChatGPT. The hardware has not yet been purchased and installed, nor is a price listed, but SuperPod systems reportedly start at $7 million. The setup described in the contract materials notes that it will include a substantial memory upgrade from Nvidia.\n \nThough small compared to the massive AI-training data centers deployed by companies like OpenAI and Meta, the SuperPod is still a powerful and expensive setup using the most advanced technology offered by Nvidia, whose chips have facilitated the global machine-learning spree. While the hardware can be used in many ways, it's marketed as a turnkey means of creating and querying an AI model. Last year, the MITRE Corporation, a federally funded military R&amp;D lab, acquired a $20 million SuperPod setup to train bespoke AI models for use by government agencies, touting the purchase as a \"massive increase in computing power\" for the United States.\n \nHow exactly the IRS will use its SuperPod is unclear. An agency spokesperson said the IRS had no information to share on the supercomputer purchase, including which presidential administration ordered it. A 2024 report by the Treasury Inspector General for Tax Administration identified 68 different AI-related projects underway at the IRS; the Nvidia cluster is not named among them, though many were redacted. But some clues can be gleaned from the purchase materials. \"The IRS requires a robust and scalable infrastructure that can handle complex machine learning (ML) workloads,\" the document explains. \"The Nvidia Super Pod is a critical component of this infrastructure, providing the necessary compute power, storage, and networking capabilities to support the development and deployment of large-scale ML models.\"\n \nThe document notes that the SuperPod will be run by the IRS Research, Applied Analytics, and Statistics division, or RAAS, which leads a variety of data-centric initiatives at the agency. While no specific uses are cited, it states that this division's Compliance Data Warehouse project, which is behind this SuperPod purchase, has previously used machine learning for automated fraud detection, identity theft prevention, and generally gaining a \"deeper understanding of the mechanisms that drive taxpayer behavior.\"","contentLength":3057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":[]}