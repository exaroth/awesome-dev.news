{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"I wrote a command line C compiler that asks ChatGPT to generate x86 assembly. Yes, it's cursed.","url":"https://github.com/Sawyer-Powell/chatgcc","date":1738988807,"author":"/u/sunmoi","guid":577,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ikeyvj/i_wrote_a_command_line_c_compiler_that_asks/"},{"title":"Docker Bake is Now Generally Available","url":"https://www.docker.com/blog/ga-launch-docker-bake/","date":1738986189,"author":"/u/h4l","guid":576,"unread":true,"content":"<p>We’re excited to announce the General Availability of  with <a href=\"https://www.docker.com/blog/docker-desktop-4-38/\">Docker Desktop 4.38</a>! This powerful build orchestration tool takes the hassle out of managing complex builds and offers simplicity, flexibility, and performance for teams of all sizes.</p><p>Docker Bake is an orchestration tool that streamlines Docker builds, similar to how Compose simplifies managing runtime environments. With Bake, you can define build stages and deployment environments in a declarative file, making complex builds easier to manage. It also leverages BuildKit’s parallelization and optimization features to speed up build times.</p><p>While Dockerfiles are excellent for defining image build steps, teams often need to build multiple images and execute helper tasks like testing, linting, and code generation. Traditionally, this meant juggling numerous  commands with their own options and arguments – a tedious and error-prone process.</p><p>Bake changes the game by introducing a declarative file format that encapsulates all options and image dependencies, referred to as <a href=\"https://docs.docker.com/build/bake/targets/\" rel=\"nofollow noopener\" target=\"_blank\"></a>. Additionally, Bake’s ability to parallelize and deduplicate work ensures faster and more efficient builds.</p><h3>Challenges with complex Docker Build configuration:</h3><ul><li>Managing long, complex build commands filled with countless flags and environment variables.</li><li>Tedious workflows for building multiple images.</li><li>Difficulty declaring builds for specific targets or environments.</li><li>Requires a script or 3rd-party tool to make things manageable</li></ul><p>Docker Bake tackles these challenges with a better way to manage complex builds with a simple, declarative approach.</p><h3>Key benefits of Docker Bake</h3><ul><li>: Replace complex chains of Docker build commands and scripts with a single  command while maintaining clear, version-controlled configuration files that are easy to understand and modify.</li><li>: Express sophisticated build logic through HCL syntax and matrix builds, enabling dynamic configurations that adapt to different environments and requirements while supporting custom functions for advanced use cases.</li><li>: Maintain standardized build configurations across teams and environments through version-controlled files and inheritance patterns, eliminating environment-specific build issues and reducing configuration drift.</li><li>: Automatically parallelize independent builds and eliminate redundant operations through context deduplication and intelligent caching, dramatically reducing build times for complex multi-image workflows.</li></ul><p>: One simple Docker buildx bake command to replace all the flags and environment variables.</p><h3>Use cases for Docker Bake</h3><h4>1. Monorepo and Image Bakery</h4><p>Docker Bake can help developers efficiently manage and build multiple related Docker images from a single source repository. Plus, they can leverage shared configurations and automated dependency handling to enforce organizational standards.</p><ul><li> Teams can maintain consistent build logic across dozens or hundreds of microservices in a single repository, reducing configuration drift and maintenance overhead.</li><li> Shared base images and contexts are automatically deduplicated, dramatically reducing build times and storage costs.</li><li> Enforce organizational standards through inherited configurations, ensuring all services follow security, tagging, and testing requirements.</li><li> A single source of truth for build configurations makes it easier to implement organization-wide changes like base image updates or security patches.</li></ul><p>Docker Bake provides seamless compatibility with existing docker-compose.yml files, allowing direct use of your current configurations. Existing Compose users are able to get started using Bake with minimal effort.</p><ul><li> Teams can incrementally adopt advanced build features while still leveraging their existing compose workflows and knowledge.</li><li> Use the same configuration for both local development (via compose) and production builds (via Bake), eliminating “works on my machine” issues.</li><li>: Access powerful features like matrix builds and HCL expressions while maintaining compatibility with familiar compose syntax.</li><li>: Seamlessly integrate with existing CI/CD pipelines that already understand compose files while adding Bake’s advanced build capabilities.</li></ul><h4>3. Complex build configurations</h4><ul><li><strong>Cross-Platform Compatibility:</strong> Matrix builds enable teams to efficiently manage builds across multiple architectures, OS versions, and dependency combinations from a single configuration.</li><li> HCL expressions allow builds to adapt to different environments, git branches, or CI variables without maintaining multiple configurations.</li><li> Custom functions enable sophisticated logic for things like version calculation, tag generation, and conditional builds based on git history.</li><li> Variable validation and inheritance ensure consistent configuration across complex build scenarios, reducing errors and maintenance burden.</li><li> Groups and targets help organize large-scale build systems with dozens or hundreds of permutations, making them manageable and maintainable.</li></ul><p>With Bake-optimized builds as the foundation, developers can achieve more efficient Docker Build Cloud performance and faster builds.</p><ul><li><strong>Enhanced Docker Build Cloud Performance:</strong> Instantly parallelize matrix builds across cloud infrastructure, turning hour-long build pipelines into minutes without managing build infrastructure.</li><li> Leverage Build Cloud’s distributed caching and deduplication to dramatically reduce bandwidth usage and build times, which is especially valuable for remote teams.</li><li> Save cost with DBC  Bake’s precise target definitions mean you only consume cloud resources for exactly what needs to be built.</li><li> Teams can run complex multi-architecture builds without powerful local machines, enabling development from any device while maintaining build performance.</li><li> Offload resource-intensive builds from CI runners to Build Cloud, reducing CI costs and queue times while improving reliability.</li></ul><h2>What’s New in Bake for GA?</h2><p>Docker Bake has been an experimental feature for several years, allowing us to refine and improve it based on user feedback. So, there is already a strong set of ingredients that users love, such as <a href=\"https://docs.docker.com/build/bake/targets/\" rel=\"nofollow noopener\" target=\"_blank\">targets</a> and <a href=\"https://docs.docker.com/build/bake/targets/#grouping-targets\" rel=\"nofollow noopener\" target=\"_blank\">groups</a>, <a href=\"https://docs.docker.com/build/bake/variables/\" rel=\"nofollow noopener\" target=\"_blank\">variables</a>, <a href=\"https://docs.docker.com/build/bake/funcs/\" rel=\"nofollow noopener\" target=\"_blank\">HCL Expression Support</a>, <a href=\"https://docs.docker.com/build/bake/inheritance/\" rel=\"nofollow noopener\" target=\"_blank\">inheritance</a> capabilities, <a href=\"https://docs.docker.com/build/bake/matrices/\" rel=\"nofollow noopener\" target=\"_blank\">matrix targets</a>, and additional <a href=\"https://docs.docker.com/build/bake/contexts/\" rel=\"nofollow noopener\" target=\"_blank\">contex</a>ts. With this GA release, Bake is now ready for production use, and we’ve added several enhancements to make it more efficient, secure, and easier to use:</p><ul><li><a href=\"https://docs.docker.com/build/bake/contexts/#deduplicate-context-transfer\" rel=\"nofollow noopener\" target=\"_blank\"><strong>Deduplicated Context Transfers</strong></a> Significantly speeds up build pipelines by eliminating redundant file transfers when multiple targets share the same build context.</li><li><a href=\"https://docs.docker.com/reference/cli/docker/buildx/bake/#allow\" rel=\"nofollow noopener\" target=\"_blank\"></a> Enhances security and resource management by providing fine-grained control over what capabilities and resources builders can access during the build process.</li><li><a href=\"https://docs.docker.com/build/bake/inheritance/\" rel=\"nofollow noopener\" target=\"_blank\"></a> Simplifies configuration management by allowing teams to define reusable attribute sets that can be mixed, matched, and overridden across different targets.</li><li><a href=\"https://docs.docker.com/build/bake/variables/#validating-variables\" rel=\"nofollow noopener\" target=\"_blank\"></a> Prevents wasted time and resources by catching configuration errors before the actual build process begins.</li></ul><h3>Deduplicate context transfers</h3><p>When you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This can significantly impact build time, depending on your build configuration.</p><p>Previously, the workaround required users to define a named context that loads the context files and then have each target reference the named context. But with Bake, this will be handled automatically now.</p><p>Bake can automatically deduplicate context transfers from targets sharing the same context. When you build targets concurrently using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it’s used. This more efficient approach leads to much faster build time.&nbsp;</p><p>Read more about how to speed up your build time in our <a href=\"https://docs.docker.com/build/bake/contexts/#deduplicate-context-transfer\" rel=\"nofollow noopener\" target=\"_blank\">docs</a>.&nbsp;</p><p>Bake now includes entitlements to control access to privileged operations, aligning with Build. This prevents unintended side effects and security risks. If Bake detects a potential issue — like a privileged access request or an attempt to access files outside the current directory — the build will fail unless explicitly allowed.</p><p>To be consistent, the Bake command now supports the flag to grant access to additional entitlements. The following entitlements are <a href=\"https://docs.docker.com/reference/cli/docker/buildx/bake/#allow\" rel=\"nofollow noopener\" target=\"_blank\">currently supported for Bake</a>.</p><ul><li>Build equivalents\n<ul><li> Allows executions with host networking.</li><li><code>--allow security.insecure</code> Allows executions without sandbox. (i.e. —privileged)</li></ul></li><li>File system: Grant filesystem access for builds that need access files outside the working directory. This will impact <code>context, output, cache-from, cache-to, dockerfile, secret</code><ul><li> Grant read and write access to files outside the working directory.</li><li> Grant read access to files outside the working directory.</li><li><code>--allow fs.write=&lt;path|*&gt; </code> Grant write access to files outside the working directory.</li></ul></li><li>ssh\n<ul><li>– Allows exposing SSH agent.</li></ul></li></ul><p>Several attributes previously had to be defined in CSV (e.g. ). These were challenging to read and couldn’t be easily overridden. The following can now be defined as structured objects:</p><pre><code>target \"app\" {\n\t\tattest = [\n\t\t\t{ type = \"provenance\", mode = \"max\" },\n\t\t\t{ type = \"sbom\", disabled = true}\n\t\t]\n\n\t\tcache-from = [\n\t\t\t{ type = \"registry\", ref = \"user/app:cache\" },\n\t\t\t{ type = \"local\", src = \"path/to/cache\"}\n\t\t]\n\n\t\tcache-to = [\n\t\t\t{ type = \"local\", dest = \"path/to/cache\" },\n\t\t]\n\n\t\toutput = [\n\t\t\t{ type = \"oci\", dest = \"../out.tar\" },\n\t\t\t{ type = \"local\", dest=\"../out\"}\n\t\t]\n\n\t\tsecret = [\n\t\t\t{ id = \"mysecret\", src = \"path/to/secret\" },\n\t\t\t{ id = \"mysecret2\", env = \"TOKEN\" },\n\t\t]\n\n\t\tssh = [\n\t\t\t{ id = \"default\" },\n\t\t\t{ id = \"key\", paths = [\"path/to/key\"] },\n\t\t]\n}</code></pre><p>As such, the attributes are now composable. Teams can mix, match, and override attributes across different targets which simplifies configuration management.</p><pre><code> target \"app-dev\" {\n    attest = [\n\t\t\t{ type = \"provenance\", mode = \"min\" },\n\t\t\t{ type = \"sbom\", disabled = true}\n\t\t]\n  }\n\n  target \"app-prod\" {\n    inherits = [\"app-dev\"]\n\n    attest = [\n\t\t\t{ type = \"provenance\", mode = \"max\" },\n\t\t]\n  }\n</code></pre><p>Bake now supports validation for variables similar to<a href=\"https://developer.hashicorp.com/terraform/language/values/variables#custom-validation-rules\" rel=\"nofollow noopener\" target=\"_blank\"> Terraform</a> to help developers catch and resolve configuration errors early. The GA for Bake also supports the following use cases.</p><p>To verify that the value of a variable conforms to an expected type, value range, or other condition, you can define custom validation rules using the  block.</p><pre><code>variable \"FOO\" {\n  validation {\n    condition = FOO != \"\"\n    error_message = \"FOO is required.\"\n  }\n}\n\ntarget \"default\" {\n  args = {\n    FOO = FOO\n  }\n}\n</code></pre><p>To evaluate more than one condition, define multiple  blocks for the variable. All conditions must be true.</p><pre><code>variable \"FOO\" {\n  validation {\n    condition = FOO != \"\"\n    error_message = \"FOO is required.\"\n  }\n  validation {\n    condition = strlen(FOO) &gt; 4\n    error_message = \"FOO must be longer than 4 characters.\"\n  }\n}\n\ntarget \"default\" {\n  args = {\n    FOO = FOO\n  }\n}\n</code></pre><p><strong>Dependency on other variables</strong></p><p>You can reference other <a href=\"https://docs.docker.com/build/bake/variables/#validating-variables\" rel=\"nofollow noopener\" target=\"_blank\">Bake variables</a> in your condition expression, enabling validations that enforce dependencies between variables. This ensures that dependent variables are set correctly before proceeding.</p><pre><code>variable \"FOO\" {}\nvariable \"BAR\" {\n  validation {\n    condition = FOO != \"\"\n    error_message = \"BAR requires FOO to be set.\"\n  }\n}\n\ntarget \"default\" {\n  args = {\n    BAR = BAR\n  }\n}\n</code></pre><p>In addition to updating the Bake configuration, we’ve added a new –list option. Previously, if you were unfamiliar with a project or wanted a reminder of the supported targets and variables, you would have to read through the file. Now, the list option will allow you to quickly query a list of them. It also supports the JSON format option if you need programmatic access.</p><p>Quickly get a list of the targets available in your Bake configuration.</p><ul><li><code>docker buildx bake --list targets</code></li><li><code>docker buildx bake --list type=targets,format=json</code></li></ul><p>Get a list of variables available for your Bake configuration.</p><ul><li><code>docker buildx bake --list variables</code></li><li><code>docker buildx bake --list type=variables,format=json</code></li></ul><p>These improvements build on a powerful feature set, ensuring Bake is both reliable and future-ready.</p><h2>Get started with Docker Bake</h2><p>Ready to simplify your builds? Update to Docker Desktop 4.38 today and start using Bake. With its declarative syntax and advanced features, Docker Bake is here to help you build faster, more efficiently, and with less effort.</p><p>Explore the <a href=\"https://docs.docker.com/build/bake/\" rel=\"nofollow noopener\" target=\"_blank\">documentation</a> to learn how to create your first Bake file and experience the benefits of streamlined builds firsthand.</p><p>Let’s bake something amazing together!</p>","contentLength":12611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ike6b5/docker_bake_is_now_generally_available/"},{"title":"Jujutsu VCS Introduction and Patterns","url":"https://kubamartin.com/posts/introduction-to-the-jujutsu-vcs/","date":1738956260,"author":"/u/Alexander_Selkirk","guid":573,"unread":true,"content":"<p><a href=\"https://github.com/jj-vcs/jj\">Jujutsu</a> (jj), a new version control system written in Rust, has popped up on my radar a few times over the past year. Looked interesting based on a cursory look, but being actually pretty satisfied with Git, and not having major problems with it, I haven’t checked it out.</p><p>That is, until last week, when I finally decided to give it a go! I dived into a couple blog posts for a few of hours, and surprisingly (noting that we’re talking about a VCS) I found myself enjoying it a lot, seeing the consistent design, and overall simplicity it managed to achieve. This post is meant to give you a feel for what’s special about jj, and also describe a few patterns that have been working well for me, and which really are the reason I’m enjoying it.</p><p>Before we dive in, one last thing you should take note of, is that most people use jj with its Git backend. You can use jj with your existing Git repos and reap its benefits in a way that is completely transparent to others you’re collaborating with. Effectively, you can treat it like a Git frontend.</p><p>Before I get to the meat, something that will surely be very useful for any of your experimentation with jj, and something I would’ve loved to have had in Git when I was learning it - you can undo any jj operation using , and view an operation log using .</p><p>Changes are the core primitive you’ll be working with in jj. In Git you had commits, in jj you have changes. A jj change is, however, . A change can be modified. It’s ID, however, is immutable and randomly generated - it stays constant while you iterate on the change. A change refers to a  (otherwise called a snapshot) which for our purposes is always a Git commit. As you keep working on and modifying a change, the commit SHA it refers to will change.</p><p>So to recap - immutable change IDs, mutable changes, mutable revision IDs (Git SHAs), immutable underlying revisions (snapshots - commits). Changes may eventually be marked as immutable - by default this happens (more or less) when they become part of the main branch or a Git tag.</p><p>While in Git you generally organize your commits in branches, and a commit that’s not part of a branch is scarily called a “detached HEAD”, in jj it’s completely normal to work on changes that are not on branches.  is the main command to view the history and tree of changes, and will default to showing you a very reasonable set of changes that should be relevant to you right now - that is (more or less) any local mutable changes, as well as some additional changes for context (like the tip of your main branch).</p><p>Changes in jj  be marked with bookmarks (what jj calls branches), but you’d generally do that only for the purpose of pushing to a remote Git server, like GitHub, not local organization.</p><p>Let’s see a sample of a  invocation:</p><div>Quick tip: you can use <a href=\"https://github.com/theZiz/aha\">aha</a> to convert colorized shell output to HTML.</div><p>The IDs on the left side are change IDs, while on the right size you have revisions (commit SHAs). Unique prefixes of those IDs are marked in color, and you can use those prefixes instead of the full form when referring to changes. A change can be reffered to in jj commands by its ID, the underlying commit SHA, or any bookmark referring to it.</p><p>There is one more, slightly crazy, thing about jj changes. Git has a special feature called the Git index to hold any as of yet uncommitted changes (where you , and then  them). In jj any files you modify are always in the scope of a change. Any modifications you make automatically become part of the current change, which is called the working copy. In the  output, this working copy is indicated by the  symbol. You create a new working change using  (by default as a child of the current change), and give it a description using  (it starts out having an empty message). You can use  to get the metadata of the current change.</p><p>This much said, here’s a quick demo of changing a file as part of a new change.</p><div>Editing a file in a jj repo</div><p> on its own is an alias for .</p><div><div>Most jj commands default to operating on the current working change, but let you operate on an arbitrary change (or set of changes!) via the  flag.</div></div><p>This whole thing about the working copy being a Change may sound weird, but it brings with it an important feature - you can operate on the working copy like on any other change. Personally, I’m a frequent user of . When I’m working on something, I often want to pause for a moment and work on / try something else, only to later come back to what I was working on (while leaving that other experiment as yet another stash).</p><p>In jj, I can just , which will start a new change from the parent of my current working copy - leaving my working copy as a normal change with all its file modifications “committed”, so I can later come back to it via .</p><div><div>The  after the  means \"go one level up\". It's basically \"take the working copy change, and go to its parent\". We'll come back to this briefly later, but suffice it to say that there's a whole sensible expression language here, and  is similarly valid to refer to your grandparent change.</div></div><h2>Editing Changes in Weird Places<a hidden=\"\" aria-hidden=\"true\" href=\"https://kubamartin.com/posts/introduction-to-the-jujutsu-vcs/#editing-changes-in-weird-places\">#</a></h2><p>Another cool thing about changes is that you can freely edit any mutable change.  lets you “check out” and edit a given change. You can also squish a new change right after another (between it and its children). You can then keep modifying files in the context of that change, and any descendants will automatically be rebased on top of it (with their underlying commit SHAs likely changing, but change IDs staying the same). That sounds a bit scary if you’re used to Git (and dealing with conflicts in the middle of a rebase) but fear not, it’s actually pretty seamless, and we’ll come back to it later.</p><p>You can pass  and  to  to indicate that you want to squish a new change after, or before, another change.</p><p>With this, when I notice a mistake in a change 3 levels back, I can just  (with my working copy remaining there for me to come back to), make a fix, which will auto-rebase all following changes (including my original working change), and then  to go back to my original working change.</p><div>Editing previous changes with automatic rebase</div><p>Occasionally you also need to rebase (move) a set of changes onto some change x, you can do that by using <code>jj rebase -s &lt;change-id-to-rebase&gt; -d &lt;destination-change&gt;</code>. The  will bring all descendants along with the rebased change, and there are other variations of this command for different scenarios. E.g. <code>jj rebase -b &lt;branch&gt; -d &lt;destination-change&gt;</code> will rebase the entire given branch onto a change, and with no arguments it just defaults to , so the current branch. In other words, to rebase the current branch onto main, it’s enough to run .</p><h3>Pattern: Squishing a Fix Before the Current Change<a hidden=\"\" aria-hidden=\"true\" href=\"https://kubamartin.com/posts/introduction-to-the-jujutsu-vcs/#pattern-squishing-a-fix-before-the-current-change\">#</a></h3><p>You noticed a mistake in your last change, don’t want to fix it as part of this one, and instead want to squish a fix right before it, but you have already done some “uncommitted” work? Just do . The  means it’s squished before the current working change (referred to via ), and after the previous one (its parent). After you make the fix, you can go back to the original working change via  ( is the opposite of ). You could also make the fix in the original working change and run  to use your favorite diff ui to select what to push down to a separate change placed right before the current one.</p><p>In jj, instead of branches we have bookmarks. You create them using <code>jj bookmark create &lt;name&gt;</code> (abbreviated to ). You update a bookmark to point the current change using . You track remote bookmarks (which will create local corresponding bookmarks that update on ) by doing .</p><p>When you add changes on top of a change that a bookmark is attached to, the bookmark won’t automatically move to your new change (like it would with a branch in Git), you have to  it manually. You can push bookmarks attached to the current change via .  will fetch updates to bookmarks from your remote (so generally updates to branches).</p><p>A common use case with services like GitHub is to split up a big change into multiple PRs, let’s say Multipart 1, Multipart 2 and Multipart 3 (from branches multipart-1, multipart-2, and multipart-3 respectively). Each of them is based on the previous one, so you effectively have the following graph, with bookmark pointers on the way:</p><p>Now let’s say Multipart 1 got reviewed and you need to fix something. In Git, once you add a commit to it (or modify an existing one), you would have to manually rebase all the other branches. Annoying!</p><p>How does jj help us here? We can just run  ( is a unique prefix of the broken change id), or , make the fix, and run <code>jj git push -b \"glob:multipart-*\"</code>. Everything will be automatically rebased, and all the bookmarks will be updated and pushed. Effortless!</p><div>Making fixes in stacked PRs</div><p>Merges in jj are pretty boring - in a good way. You just create a change with multiple parents. E.g.  will create a change with three parents.</p><p>One topic I haven’t mentioned yet, and is surely by now giving you an unsettling feeling deep inside about all I’ve said before, are conflicts. With all this rebasing, that’s  to become a pain, right? Well, it doesn’t!</p><p>As opposed to Git, where conflicts kind of break your workflow, in the sense that you have to resolve them prior to doing anything else, jj handles conflicts in a first-class manner. A change can just “be conflicted”. You can switch away from a conflicted change, you can create a new change on top of an existing conflicted change (and that will in turn also start out conflicted), you can edit a conflicted change, you can do anything.  and  will mention the conflict, but it won’t block you.</p><pre>\n├─╮  \n│ ○  \n│ │  add 'cccc'\n○ │  \n├─╯  add 'bbbb'\n○  \n│  add 'aaaa'\n\nWorking copy changes:\n\nfile.txt    \nWorking copy : \nParent commit:  add 'bbbb'\nParent commit:  add 'cccc'\n</pre><p>The conflict will be represented in the code via conflict markers:</p><pre>aaaa\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; Conflict 1 of 1\n%%%%%%% Changes from base to side #1\n+bbbb\n+++++++ Contents of side #2\ncccc\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Conflict 1 of 1 ends\n</pre><p>and you can either manually resolve this conflict by editing the code itself, or use  to bring up your favorite three-way-merge tool (e.g. I’ve configured my jj to bring up a visual conflict resolver in Goland) and resolve it there.</p><p>Once you fix the conflict in a change, all descendants of this change will also cease being conflicted. You could leave a change conflicted and only resolve the conflict in a follow-up child change - that is a completely valid and supported approach.</p><div>Merge conflict resolution</div><h3>Pattern: Working on Two Things at the Same Time<a hidden=\"\" aria-hidden=\"true\" href=\"https://kubamartin.com/posts/introduction-to-the-jujutsu-vcs/#pattern-working-on-two-things-at-the-same-time\">#</a></h3><p>I’ve mentioned stacked PRs, but another situation you might have is working on two independent things in parallel. Let’s say on bookmarks  and .</p><p>In order to have both things simultaneosuly active in your codebase, you can create a development change via <code>jj new thing-1 thing-2 -m \"dev\"</code>, that will be a merge between both of them, but will stay local and you’ll never push it ( just lets you give a description to a change when creating it, without a separate  invocation). You will however  this development change to do your work.</p><p>Then, whenever you want to move some modifications into one of the branches, you can use <code>jj squash --into &lt;target-change-id&gt; &lt;files&gt;</code> to move all modifications to a set of files down into one of the branches. There’s also  where you can use a diff tool to choose modifications to squash into another change, and finally there’s a <a href=\"https://jj-vcs.github.io/jj/latest/cli-reference/#jj-absorb\">newer  command</a> which can automate this process in certain scenarios.</p><div>Editing files in a merged dev-change andselectively squashing the changes into branches</div><p>In conclusion, you can keep working in a local-only merge-change of your branches, and selectively push down any modifications to the relevant branch (This setup would’ve seemed pretty scary before jj, right? I hope it’s a bit less scary now.), and then push just those branches themselves to the remote.</p><p>jj commands operate on revisions or sets of revisions (revsets). You can refer to those directly, or use a special expression language to describe them. You’ve seen me refer to a change previously via . That was a very simple expression that evaluated to the parent.</p><p>There is, however, much more. There are functions - like  to get the parents of a change - and operators - like  to refer to the child of x, or  for all descendants of x including x.</p><p> accepts a revset expression, so you can use it to experiment with them. The default revset it displays is also configurable. Overall, the expression language is powerful and consistent, with simple things being generally easy, and harder things being (presumably, I’ve honestly spent too little time with it) possible.</p><p>See <a href=\"https://v5.chriskrycho.com/essays/jj-init/#revisions-and-revsets\">this article</a> for a much more extensive exploration of the jj expression language, and the <a href=\"https://jj-vcs.github.io/jj/latest/revsets/\">jj docs</a> themselves.</p><h3>Pattern: Partial Stashes<a hidden=\"\" aria-hidden=\"true\" href=\"https://kubamartin.com/posts/introduction-to-the-jujutsu-vcs/#pattern-partial-stashes\">#</a></h3><p>Another kind of stash I occasionally like to do is partial, where I temporarily roll back changes to a set of files to verify the before-after (e.g. confirm that a passing test was failing before).</p><p>In jj the split command works well for this. Just <code>jj split --parallel modifiedFile.txt</code> will move the file into a parallel change. You can do whatever you want to do, and later run <code>jj squash --from parallel_change_id</code> to get the file modifications back into the current change.</p><div>Partial stash using </div><h2>Setting Up jj with an Existing Git Repo<a hidden=\"\" aria-hidden=\"true\" href=\"https://kubamartin.com/posts/introduction-to-the-jujutsu-vcs/#setting-up-jj-with-an-existing-git-repo\">#</a></h2><p>It’s trivial to start using jj with an existing git repo, though I’d advise cloning it fresh into a new directory.</p><p>In a directory where you already have a git repo, you can just run . Your  directory will stay in place, and jj will keep it updated, so e.g. your editor won’t be confused what’s happening. It integrates fairly well, with e.g. the working change - even though it’s backed by a commit - being presented as the git index, so your editor can still show files “modified in this change”.</p><p>The cost of switching is low, as it integrates seamlessly with your existing workflow. It frankly also takes a day tops to get used to, and there’s something to be said for using  things. Sure, you can make excellent tea in any food-safe kettle, but if you have a nice tea kettle, you’ll enjoy it every time you make tea. I use my vcs quite a lot, so why not make that pleasant too?</p><p>I hope the above gave you an intuition for what Jujutsu, the VCS, is all about, and ideally even encouraged you to take a look at it.</p><p>If you’d like to do some more readings about jj, I’ve used the below articles and guides when learning it, and a lot of what I wrote above is inspired by parts of them. Check them out!</p>","contentLength":14610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ik3akf/jujutsu_vcs_introduction_and_patterns/"},{"title":"GitHub - perpetual-ml/perpetual: A self-generalizing gradient boosting machine which doesn't need hyperparameter optimization","url":"https://github.com/perpetual-ml/perpetual","date":1738953356,"author":"/u/mutlu_simsek","guid":572,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ik24y0/github_perpetualmlperpetual_a_selfgeneralizing/"},{"title":"I Automated My Taxes using AutoHotKey","url":"https://www.preethamrn.com/posts/automating-taxes-autohotkey","date":1738947332,"author":"/u/preethamrn","guid":575,"unread":true,"content":"<p>7 February 2025 · </p><div><p>I hate doing taxes. I don't mind paying them but I'm not a fan of all the effort involved in collecting all the forms and making sure to fill out all the boxes correctly or else risk getting audited. It's especially painful for me because my job gives me RSUs which are automatically sold to cover taxes and at the end of the year, sends me a 1099-B with every single sale of every single stock grant which usually amounts to 90+ sales that I have to report.</p><p>In theory, I could file a summary of sales and send out a Form 8949 but the first year I did this, I didn't do the process properly which resulted in me getting audited.</p><p>Ever since that incident, I've diligently copied every single row from my 1099 to TurboTax which usually took 3 hours and sometimes 2 people. This year I decided to change things up. Most of the process is extremely repetitive - you read a row, click on the form, fill the required details, and then move on to the next row. Seemed like the perfect job for autohotkey.</p><h3><a href=\"https://www.preethamrn.com/posts/automating-taxes-autohotkey#step-1-figuring-out-where-to-click\"></a>Step 1: Figuring out where to click</h3><p>I could do this using trial and error but it was much simpler to automate this part too. I wrote a script that would tell me my mouse cursor position for each click</p><pre><code>F6::\nMouseGetPos, xpos, ypos\nFileAppend, %xpos%`, %ypos%`n, mouse_positions.txt\nreturn\n</code></pre><p>One thing that I needed to keep in mind was that I wouldn't be able to scroll so I had to shrink the text size to make everything fit. And with that, I had a list of all the places I'd need to click and what to enter at each location</p><p>I could probably automate the PDF parsing as well, but it turned out to be much simpler to just copy paste the details into a text file. After cleaning up all the unnecessary details using Sublime's regex find and replace, I ended up with a text file  like this:</p><h3><a href=\"https://www.preethamrn.com/posts/automating-taxes-autohotkey#step-3-writing-the-automation-script\"></a>Step 3: Writing the automation script</h3><p>Now for the fun part. I hadn't worked with autohotkey before but I knew that this was the exact kind of thing that it was built for. Turns out I was right and for once in my life the coding went by pretty smoothly.</p><p>The main structure of the program was like this:</p><pre><code>#Persistent\n#SingleInstance Force\nSetTitleMatchMode, 2\n\n; Give time to focus the TurboTax window\nSleep, 2000\n\ndataFile := \"data.txt\" ; Specify the text file to read from\nLoop, Read, %dataFile% {\n    line := A_LoopReadLine\n    elements := StrSplit(line, \" \") ; Split the line by spaces\n\n    ; open dropdown\n    MouseClick, left, 252, 419\n    Sleep, 100\n\n    ; select from drop down\n    MouseClick, left, 259, 462\n    Sleep, 100\n\n    ; go to description field\n    MouseClick, left, 244, 556\n    Sleep, 50\n\n    ; enter description\n    Send, % elements[1] . \" UBER\"\n\n    ....\n    ; check for wash sale\n    if (elements.Length() &gt;= 7) {\n        ... ; enter wash sale adjustment\n        ; click submit\n        MouseClick, left, 524, 1220\n        Sleep 5000\n    } else {\n        ; click submit\n        MouseClick, left, 516, 957\n        Sleep 5000\n    }\n    \n    ; add new row\n    ; can't use a mouseclick here because the \"Add new\" button moves down as the table grows\n    SetKeyDelay, 50\n    Loop, 4\n    Send, {rshift down}{tab}{rshift up}\n    SendInput, {enter}\n    SetKeyDelay, -1\n\n    Sleep, 3000\n}\n\nreturn\n\n; Exit script on Escape key\nEsc::ExitApp\n</code></pre><p>A few interesting things that I figured out while testing</p><ol><li>When adding a new row, I needed to cycle through the buttons using tab backwards since the \"Add new row\" button position changes as the table size grows with each new row.</li><li>The most time consuming part ended up being tuning the delays. TurboTax starts out fast but after entering a few rows, it slows to a crawl and even a 5 second delay isn't enough. Things never really got into a rhythm and I had to keep adjusting the wait time to give the app time to keep up with the script.</li><li>TurboTax only allows 25 rows per page so halfway through, I needed to change the script again to account for the new location of the \"Add new row\" button. Instead of 4 tabs back, I needed to change it to 7 (and then again to 8. and then 9... the table grew pretty large)</li></ol><p>With the script ready, it was time to run it. And boy did it run. Right away, it finished submitting around 15 rows before I even figured out what I was planning on doing with all my new found free time.</p><p>The excitement was short lived though because I hit a snag pretty quickly. TurboTax would occasionally take an extra second loading but my script didn't care and would go on clicking all over the place so I'd have to babysit it and make sure to press Escape before it did anything crazy.</p><p>Whenever I stopped the script, I had to find the last row that was submitted and delete all the data that was processed from the txt file. In total, I had to do this about 25 times.</p><p>All things considered I'd consider it a success. The script took me about 45 minutes to write and debug and then around another 30 minutes to run/fix any hidden errors. Even if it took me around 1 minute to manually enter each row, it was already almost twice as fast and I can reuse this script next year.</p><p>Also, whenever I did this manually, I usually made at least one data entry error so it would usually take another 10-15 minutes double checking all my data to figure out where the error was. In this case, once the script completed, I just checked the totals to make sure everything lined up - either I'd get a perfect answer or I'd get something radically off. It worked on the first try.</p><p>I'm not sure what the moral of the story here is but maybe there are some cases where it actually does make sense to automate things in my life. I'll be looking out for those opportunities more now.</p></div><a href=\"https://www.preethamrn.com/posts/tag/Programming\"> #Programming</a><p> Written by <a href=\"https://www.twitter.com/preethamrn\">@preethamrn</a>: Software developer at Uber with a degree in CS. Go, Storage, Distributed Systems, Bouldering, Rubik's Cubes. <a href=\"https://www.github.com/preethamrn\">Github</a></p>","contentLength":5762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ijzni6/i_automated_my_taxes_using_autohotkey/"},{"title":"default/override - An Elegant Schema for User Settings","url":"https://double.finance/blog/default_override","date":1738946246,"author":"/u/Ok-Eye7251","guid":574,"unread":true,"content":"<p>Every app, even basic ones, needs to manage user settings.  It always starts simple, right? \"Does the user want the Weekly Email?\" But then features grow, and suddenly you're dealing with tons of settings:  themes, notifications, currencies, time zones, the list goes on.  If you're building something complex, like Double – which is for investing –  settings get  important.</p><p>We have to manage things like fraud settings, drift limits and wash sale prevention settings. Some of these we want to be user controlled, some should only be editable by an admin user. How you store and handle these settings really matters for your app's speed, how well it scales, and how easy it is to maintain.  I've tried a bunch of ways to manage user settings, and some are way better than others.  Let's look at common mistakes first, and then I'll show you a schema that we came up with recently that's been working really well for us.</p><h3>Common Mistakes:  What  To Do</h3><p>One common mistake to avoid is the wide table trap, where the initial thought for managing new settings is to simply keep adding columns. It may seem easy at first; for example, setting up daily emails might look as simple as adding a  boolean column to your  table with a quick  command. However, this approach quickly leads to columns getting out of control.</p><p>Before long, you'll need columns for newsletters, app notifications, dark mode, time zones, and other features, causing your  table to become a massive, wide structure. Wide tables can negatively impact performance, as queries take longer because the database has to process huge rows, many of which are sparsely populated. Indexes become more complicated, and backups and restores take significantly longer.</p><p>Schema changes also become a pain point. Adding another setting involves another risky  operation in a live application. Remembering the purpose of each column becomes a documentation nightmare, and changing or removing old settings becomes a major project.</p><p>Finally, because most users only modify a few settings, your  table becomes filled with wasted space in the form of numerous NULL values, further slowing down queries and wasting storage.</p><p>Another approach that may seem appealing after rejecting wide tables is the \"grab bag\" table, often implemented as a generic  table with columns like , , and .</p><p>At first glance, this key-value structure appears flexible, because it promises easy addition of new settings without altering the main table. However, bulk updates soon become a significant challenge. For example, adding a new setting to all users could require millions of insert operations, resulting in large, risky database transactions.  This can lead to server instability and data inconsistencies, often necessitating complex error handling, retry mechanisms, and background jobs to ensure data integrity.</p><p>Data quality also becomes a concern because the  field can accommodate any data type – text, numbers, booleans – making it difficult for the database to enforce consistency.  Consequently, all data validation and type handling logic must be implemented in the application code, increasing complexity and the potential for bugs.  Finally, user signup processes can become slower.</p><p>Setting default settings for new users may require inserting numerous rows into the  table, adding latency to the signup flow and increasing the chances of failures during this critical operation.</p><p><strong>3. The Schemaless Temptation</strong></p><p>Another tempting approach, especially with the rise of NoSQL databases like MongoDB, is to store settings as JSON documents directly within each user's record.  The schemaless nature of NoSQL databases initially appears advantageous, and using JSON for settings feels like a natural fit.  Adding a new setting seems as simple as adding a new key to the JSON document. However, querying across users for specific settings becomes challenging.</p><p>While document databases excel at retrieving all data for a single user, asking questions about a particular setting across all users becomes slow and inefficient.  For instance, answering \"How many users use dark mode?\" could require a full database scan, which does not scale efficiently.</p><p>Furthermore, the perceived benefit of being \"schemaless\" diminishes when data consistency is required. Ensuring that every user has a specific setting or validating data types becomes the application's responsibility, as the database provides no inherent mechanisms for these rules.</p><p>Finally, while NoSQL promises flexibility, settings migrations over time remain complex.  Updating settings across numerous NoSQL documents often necessitates custom scripts and background jobs, mirroring the same challenges encountered with the \"grab bag\" SQL table approach in terms of complexity and potential issues.</p><h3>A Better SQL Schema: /</h3><p>After struggling with these problems, I've found a SQL schema that actually works well. It balances flexibility, performance, and keeps your data sane.  It uses two key tables:  and .</p><p><strong> Table:  The Master List of Settings</strong></p><p>Think of  as the single place where you define all possible settings in your app.  It's like the blueprint for your settings system.</p><table><thead><tr></tr></thead><tbody><tr><td>Unique name for the setting (primary key).</td></tr><tr><td>The default value everyone gets.</td></tr><tr><td><code>ENUM('boolean', 'string', 'integer', 'enum')</code></td><td>What is the data type of this setting?</td></tr><tr><td>(Optional, for 'enum' type)  List of valid choices as JSON.</td><td><code>['daily', 'weekly', 'monthly']</code> (for rebalance cadence)</td></tr><tr><td>Can users change this setting themselves?</td><td> (for rebalance cadence),  (for internal flags)</td></tr></tbody></table><p><strong> Table:  User-Specific Changes</strong></p><p> only stores settings where a user has changed the default.  It's kept lean and efficient.</p><table><thead><tr></tr></thead><tbody><tr><td>Links to the  table (part of primary key).</td></tr><tr><td>Links to  (part of primary key).</td></tr><tr><td>The user's chosen value for the setting.</td></tr><tr><td>When the user made this change.</td></tr><tr><td>When the user last updated this setting.</td></tr></tbody></table><p>This system is designed to be simple and effective in practice.  Adding a new setting is straightforward; you only need to insert a single row into the  table.  Here, you define the setting's name, its default value, the data type, any valid options, and whether users are allowed to edit this setting.  Crucially, this process requires no database schema alterations and avoids mass data modifications.</p><p>To retrieve a user's setting, the  function (or its equivalent in your ORM) efficiently handles the logic.  It prioritizes checking the  table. If a user-specific setting exists there, that value is used.  Otherwise, the function falls back to the default value defined in the  table.</p><pre><code>SELECT COALESCE(so.value, sd.value) AS setting_value\nFROM default sd\nLEFT JOIN override so ON sd.name = so.name AND so.user_id = :user_id\nWHERE sd.name = :setting_name;\n</code></pre><p>Retrieving multiple settings for a user simultaneously remains efficient through the use of joins and filters.  Furthermore, data validation is enforced at the SQL level. When a setting is modified, a new row is inserted into the  table.  Before saving this override, validation can be performed based on the  and  columns defined in , allowing the system to reject invalid setting changes proactively.</p><p>For user settings management, users can easily retrieve all their settings, encompassing both their personalized overrides and the system defaults, or filter specifically for settings they are permitted to modify.  Users can then update their settings via the application's API, which incorporates built-in validation checks and persists valid changes to the  table.</p><p> This approach offers several advantages. It scales well, allowing for fast addition and retrieval of settings.  Since only user modifications are stored, the database remains efficient. Data consistency is ensured by  which establishes data types and valid options, with database-level checks promoting reliability.</p><p>Settings management is simplified with  acting as a central list, eliminating ambiguity about setting definitions. The design inherently maintains data consistency, negating the need for supplementary background processes to synchronize settings.</p><p>The system is clean and organized, clearly separating defaults from user-specific changes, enhancing overall system clarity and usability.   provides control over user modifications, enabling specification of settings users can adjust, which is beneficial for sophisticated applications.</p><p>Finally, it facilitates efficient bulk operations and the creation of user-facing settings interfaces, making it easy to retrieve numerous settings at once.</p><p> This system also presents some drawbacks. The primary disadvantage is its complexity compared to simpler methods like wide tables or key-value based approaches.</p><p>Additionally, gaining easy insight into historical settings is limited. As user settings rely on two tables, tracking a setting's value at a specific past time is not straightforward, requiring periodic snapshots of both tables for reconstruction, which is less than ideal.</p><p>Lastly, retrieving settings necessitates querying the application's backend to access the database (or use a cache). While slightly slower than direct column access from a  table, the benefits generally outweigh this minor performance difference. Database views can also be implemented to mitigate this speed concern in many scenarios.</p><h2>In Short: Ditch Giant Tables, Use The Default-Override Method</h2><p>For apps that need to be reliable and handle lots of users, especially complex ones like investing platforms, how you store settings really matters.  Moving away from simple wide tables and using a structured schema like / has been a great move for us here at Double.  It's cleaner, scales better, easier to maintain, and keeps our data in good shape.</p><p>What's your experience with handling user settings?  Run into similar problems? Found different solutions?</p>","contentLength":9726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ijz7vm/defaultoverride_an_elegant_schema_for_user/"}],"tags":["dev","reddit"]}