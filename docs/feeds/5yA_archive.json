{"id":"5yA","title":"AI","displayTitle":"AI","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":55,"items":[{"title":"[R] A Survey of Logical Reasoning Capabilities in Large Language Models: Frameworks, Methods, and Evaluation","url":"https://www.reddit.com/r/MachineLearning/comments/1iqmjal/r_a_survey_of_logical_reasoning_capabilities_in/","date":1739688936,"author":"/u/Successful-Western27","guid":522,"unread":true,"content":"<p>This new survey provides a comprehensive analysis of logical reasoning capabilities in LLMs, examining different reasoning types, evaluation methods, and current limitations.</p><p>Key technical aspects: - Categorizes logical reasoning into deductive, inductive, and abductive frameworks - Evaluates performance across multiple benchmarks and testing methodologies - Analyzes the relationship between model size and reasoning capability - Reviews techniques for improving logical reasoning, including prompt engineering and chain-of-thought methods</p><p>Main findings: - LLMs show strong performance on basic logical tasks but struggle with complex multi-step reasoning - Model size alone doesn't determine reasoning ability - training methods and problem-solving strategies play crucial roles - Current evaluation methods may not effectively distinguish between true reasoning and pattern matching - Performance degrades significantly when problems require combining multiple reasoning types</p><p>I think the most important contribution here is the systematic breakdown of where current models succeed and fail at logical reasoning. This helps identify specific areas where we need to focus research efforts, rather than treating reasoning as a monolithic capability.</p><p>I think this work highlights the need for better benchmarks - many current tests don't effectively measure true reasoning ability. The field needs more robust evaluation methods that can differentiate between memorization and actual logical inference.</p><p>TLDR: Comprehensive survey of logical reasoning in LLMs showing strong basic capabilities but significant limitations in complex reasoning. Highlights need for better evaluation methods and targeted improvements in specific reasoning types.</p>","contentLength":1740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Is my company missing out by avoiding deep learning?","url":"https://www.reddit.com/r/MachineLearning/comments/1iq9gtk/d_is_my_company_missing_out_by_avoiding_deep/","date":1739648562,"author":"/u/DatAndre","guid":257,"unread":true,"content":"<p>Disclaimer: obviously it does not make sense to use a neural network if a linear regression is enough. </p><p>I work at a company that strictly adheres to mathematical, explainable models. Their stance is that methods like Neural Networks or even Gradient Boosting Machines are too \"black-box\" and thus unreliable for decision-making. While I understand the importance of interpretability (especially in mission critical scenarios) I can't help but feel that this approach is overly restrictive. </p><p>I see a lot of research and industry adoption of these methods, which makes me wonder: are they really just black boxes, or is this an outdated view? Surely, with so many people working in this field, there must be ways to gain insights into these models and make them more trustworthy. </p><p>Am I also missing out on them, since I do not have work experience with such models?</p><p>EDIT: Context is formula one! However, races are a thing and support tools another. I too would avoid such models in anything strictly related to a race, unless completely necessary. I just feels that there's a bias that is context-independent here. </p>","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lil guy is trying his best","url":"https://www.reddit.com/r/artificial/comments/1iq6dyy/lil_guy_is_trying_his_best/","date":1739640466,"author":"/u/MetaKnowing","guid":292,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Have any LLM papers predicted a token in the middle rather than the next token?","url":"https://www.reddit.com/r/MachineLearning/comments/1iq4f0r/d_have_any_llm_papers_predicted_a_token_in_the/","date":1739635189,"author":"/u/TheWittyScreenName","guid":516,"unread":true,"content":"<p>I’m working on a project (unrelated to NLP) where we use essentially the same architecture and training as GPT-3, but we’re more interested in finding a series of tokens to connect a starting and ending “word” than the next “word”. Since we’re drawing a lot from LLMs in our setup, I’m wondering if there’s been any research into how models perform when the loss function isn’t based on the next token, but instead predicting a masked token somewhere in the input sequence. </p><p>Eventually we would like to expand this (maybe through fine tuning) to predict a longer series of missing tokens than just one but this seems like a good place to start. </p><p>I couldn’t find much about alternate unsupervised training schemes in the literature but it seems like someone must have tried this already. Any suggestions, or reasons that this is a bad idea?</p>","contentLength":859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Altman: OpenAI not for sale, especially to competitor who is not able to beat us","url":"https://www.axios.com/2025/02/11/openai-altman-musk-offer","date":1739629077,"author":"/u/namanyayg","guid":472,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq29zz/altman_openai_not_for_sale_especially_to/"},{"title":"Will AI Lead to the Disintermediation of Knowledge?","url":"https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/","date":1739626129,"author":"Bill Schmarzo","guid":63,"unread":true,"content":"<p>Key Blog Points: For decades, organizations have operated under the central assumption that knowledge flows downward. Senior leaders, industry veterans, and domain experts have traditionally been the primary gatekeepers of critical information. Their insights, honed over years of experience, have been the cornerstone of strategic decision-making. Enter artificial intelligence (AI). Many folks are concerned that…&nbsp;<a href=\"https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/\" rel=\"bookmark\">Read More »</a></p>","contentLength":431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chinese Vice Minister says China and the US must work together to control rogue AI: \"If not... I am afraid that the probability of the machine winning will be high.\"","url":"https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says","date":1739622429,"author":"/u/MetaKnowing","guid":293,"unread":true,"content":"<div datatype=\"p\" data-qa=\"Component-Component\">A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in <a target=\"_self\" href=\"https://www.scmp.com/topics/artificial-intelligence?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> (AI).</div><p datatype=\"p\" data-qa=\"Component-Component\">But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.</p><p datatype=\"p\" data-qa=\"Component-Component\">“Realistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,” Fu said.</p><p datatype=\"p\" data-qa=\"Component-Component\">“As long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.”</p><div datatype=\"p\" data-qa=\"Component-Component\">The panel discussion is part of a two-day global <a target=\"_self\" href=\"https://www.scmp.com/news/world/europe/article/3297992/trumps-ai-ambition-and-chinas-deepseek-overshadow-major-ai-summit-paris?module=Europe&amp;pgtype=section?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> that started in Paris on Monday.</div><p datatype=\"p\" data-qa=\"Component-Component\">Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden’s administration and the United Nations.</p>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq0b4t/chinese_vice_minister_says_china_and_the_us_must/"},{"title":"[P] Daily ArXiv filtering powered by LLM judge","url":"https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/","date":1739618056,"author":"/u/MadEyeXZ","guid":258,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] What's the most promising successor to the Transformer?","url":"https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/","date":1739600221,"author":"/u/jsonathan","guid":259,"unread":true,"content":"<p>All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also <a href=\"https://arxiv.org/pdf/2405.04517\">xLSTM</a> and <a href=\"https://arxiv.org/pdf/2405.13956\">Aaren</a>.</p><p>What do y'all think is the most promising alternative architecture to the transformer?</p>","contentLength":283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I Became A Machine Learning Engineer (No CS Degree, No Bootcamp)","url":"https://towardsdatascience.com/how-i-became-a-machine-learning-engineer-no-cs-degree-no-bootcamp/","date":1739586781,"author":"Egor Howell","guid":10,"unread":true,"content":"<p>Machine learning and <a href=\"https://towardsdatascience.com/tag/ai/\" title=\"AI\">AI</a> are among the most popular topics nowadays, especially within the tech space. I am fortunate enough to work and develop with these technologies every day as a machine learning engineer!</p><p>In this article, I will walk you through my journey to becoming a machine learning engineer, shedding some light and advice on how you can become one yourself!</p><p>In one of my previous articles, I extensively wrote about my journey from school to securing my first <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a> job. I recommend you <a href=\"https://medium.com/towards-data-science/how-i-became-a-data-scientist-no-cs-degree-no-bootcamp-82c321904986\">check out that article</a>, but I will summarise the key timeline here.</p><p>Pretty much everyone in my family studied some sort of STEM subject. My great-grandad was an engineer, both my grandparents studied physics, and my mum is a maths teacher.</p><p><em>So, my path was always paved for me.</em></p><p>I chose to study physics at university after watching The Big Bang Theory at age 12; it’s fair to say everyone was very proud!</p><p>At school, I wasn’t dumb by any means. I was actually relatively bright, but I didn’t fully apply myself. I got decent grades, but definitely not what I was fully capable of.</p><p>I was very arrogant and thought I would do well with zero work.</p><p>I applied to top universities like Oxford and Imperial College, but given my work ethic, I was delusional thinking I had a chance. On results day, I ended up in clearing as I missed my offers. This was probably one of the saddest days of my life.</p><p>Clearing in the UK is where universities offer places to students on certain courses where they have space. It’s mainly for students who don’t have a university offer.</p><p>I was lucky enough to be offered a chance to study physics at the University of Surrey, and I went on to earn a first-class master’s degree in physics!</p><p>There is genuinely no substitute for hard work. It is a cringy cliche, but it is true!</p><p>My original plan was to do a PhD and be a full-time researcher or professor, but during my degree, I did a research year, and I just felt a career in research was not for me. Everything moved so slowly, and it didn’t seem there was much opportunity in the space.</p><p>During this time, DeepMind released their<a href=\"https://www.youtube.com/watch?v=WXuK6gekU1Y&amp;t=1539s\"></a>documentary on YouTube, which popped up on my home feed.</p><p>From the video, I started to understand how AI worked and learn about neural networks, reinforcement learning, and deep learning. To be honest, to this day I am still not an expert in these areas.</p><p>Naturally, I dug deeper and found that a data scientist uses AI and machine learning algorithms to solve problems. I immediately wanted in and started applying for data science graduate roles.</p><p>I spent countless hours coding, taking courses, and working on projects. I applied to and eventually landed my first data science graduate scheme in September 2021.</p><p><em>You can hear more about my journey from a <a href=\"https://tobeadatascientist.substack.com/p/overcoming-rejection-lessons-from-egor-howell\">podcast</a>.</em></p><p>I started my career in an insurance company, where I built various supervised learning models, mainly using gradient boosted tree packages like CatBoost, XGBoost, and<a href=\"https://medium.com/towards-data-science/breaking-down-generalized-linear-models-d9212526e51d?sk=fda0298cebcb8e9e0c20cb6af8ed4f06\"> generalised linear models (GLMs)</a>.</p><p>I built models to predict:</p><ul><li> — Did someone fraudulently make a claim to profit.</li><li>— What’s the premium we should give someone.</li><li>— How many claims will someone have.</li><li> — What’s the average claim value someone will have.</li></ul><p>I made around six models spanning the regression and classification space. I learned so much here, especially in statistics, as I worked very closely with Actuaries, so my maths knowledge was excellent.</p><p>However, due to the company’s structure and setup, it was difficult for my models to advance past the PoC stage, so I felt I lacked the “tech” side of my toolkit and understanding of how companies use machine learning in production.</p><p>After a year, my previous employer reached out to me asking if I wanted to apply to a junior data scientist role that specialises in<a href=\"https://medium.com/@egorhowell/list/time-series-00bbfb9f5359\"> time series forecasting</a> and<a href=\"https://medium.com/@egorhowell/list/optimisation-algorithms-069bf9c6c8d5\"> optimisation</a> problems. I really liked the company, and after a few interviews, I was offered the job!</p><p>I worked at this company for about 2.5 years, where I became an expert in forecasting and combinatorial optimisation problems.</p><p>I developed many algorithms and deployed my models to production through AWS using software engineering best practices, such as unit testing, lower environment, shadow system, CI/CD pipelines, and much more.</p><p><em>Fair to say I learned a lot.&nbsp;</em></p><p>I worked very closely with software engineers, so I picked up a lot of engineering knowledge and continued self-studying machine learning and statistics on the side.</p><p>Over time, I realised the actual value of data science is using it to make live decisions. There is a good quote by<a href=\"https://www.linkedin.com/posts/pau-labarta-bajo-4432074b_machinelearning-mlops-realworldml-activity-7195694289178214400-gZyw\"> Pau Labarta Bajo</a></p><p>ML models inside Jupyter notebooks have a business value of $0</p><p>There is no point in building a really complex and sophisticated model if it will not produce results. Seeking out that extra 0.1% accuracy by staking multiple models is often not worth it.</p><p>You are better off building something simple that you can deploy, and that will bring real financial benefit to the company.</p><p>With this in mind, I started thinking about the future of data science. In my head, there are two avenues:</p><ul><li> -&gt; You work primarily to gain insight into what the business should be doing and what it should be looking into to boost its performance.</li><li> -&gt; You ship solutions (models, decision algorithms, etc.) that bring business value.</li></ul><p>I feel the data scientist who analyses and builds PoC models will become extinct in the next few years because, as we said above, they don’t provide tangible value to a business.</p><p>That’s not to say they are entirely useless; you have to think of it from the business perspective of their return on investment. Ideally, the value you bring in should be more than your salary.</p><p>You want to say that you did “X that produced Y”, which the above two avenues allow you to do.</p><p>The engineering side was the most interesting and enjoyable for me. I genuinely enjoy coding and building stuff that benefits people, and that they can use, so naturally, that’s where I gravitated towards.</p><p>To move to the ML engineering side, I asked my line manager if I could deploy the algorithms and ML models I was building myself. I would get help from software engineers, but I would write all the production code, do my own system design, and set up the deployment process independently.</p><p><em>And that’s exactly what I did.</em></p><p>Coincidentally, my current employer contacted me around this time and asked if I wanted to apply for a machine learning engineer role that specialises in general ML and optimisation at their company!</p><p>Call it luck, but clearly, the universe was telling me something. After several interview rounds, I was offered the role, and I am now a fully fledged machine learning engineer!</p><p>Fortunately, a role kind of “fell to me,” but I created my own luck through up-skilling and documenting my learning. That is why I always tell people to show their work — you don’t know what may come from it.</p><p>I want to share the main bits of advice that helped me transition from a machine learning engineer to a data scientist.</p><ul><li> — A machine learning engineer is  an entry-level position in my opinion. You need to be well-versed in data science, machine learning, software engineering, etc. You don’t need to be an expert in all of them, but have good fundamentals across the board. That’s why I recommend having a couple of years of experience as either a software engineer or data scientist and self-study other areas.</li><li> — If you are from data science, you must learn to write good, well-tested production code. You must know things like typing, linting, unit tests, formatting, mocking and CI/CD. It’s not too difficult, but it just requires some practice. I recommend asking your current company to work with software engineers to gain this knowledge, it worked for me!</li><li> — Most companies nowadays deploy many of their architecture and systems on the cloud, and machine learning models are no exception. So, it’s best to get practice with these tools and understand how they enable models to go live. I learned most of this on the job, to be honest, but there are courses you can take.</li><li> — I am sure most of you know this already, but every tech professional should be proficient in the command line. You will use it extensively when deploying and writing production code. I have a basic guide you can checkout<a href=\"https://medium.com/towards-data-science/an-introduction-to-the-shell-676ee5b899df?sk=0c6e101165b4314b98ab39d11525366c\"> here</a>.</li><li><strong>Data Structures &amp; Algorithms </strong>— Understanding the fundamental algorithms in computer science are very useful for MLE roles. Mainly because you will likely be asked about it in interviews. It’s not too hard to learn compared to machine learning; it just takes time. Any course will do the trick.</li><li> — Again, most tech professionals should know Git, but as an MLE, it is essential. How to squash commits, do code reviews, and write outstanding pull requests are musts.</li><li> — Many MLE roles I saw required you to have some specialisation in a particular area. I specialise in time series forecasting, optimisation, and general ML based on my previous experience. This helps you stand out in the market, and most companies are looking for specialists nowadays.</li></ul><p>The main theme here is that I basically up-skilled my software engineering abilities. This makes sense as I already had all the math, stats, and machine learning knowledge from being a data scientist.</p><p>If I were a software engineer, the transition would likely be the reverse. This is why securing a machine learning engineer role can be quite challenging, as it requires proficiency across a wide range of skills.</p><h3><strong>Summary &amp; Further Thoughts</strong></h3><p>I have a free newsletter, <a href=\"https://dishingthedata.substack.com/\"></a>, where I share weekly tips and advice as a practising data scientist. Plus, when you subscribe, you will get my and<strong> short PDF version of my AI roadmap</strong>!</p>","contentLength":9683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.","url":"https://v.redd.it/sglstazd96je1","date":1739568243,"author":"/u/eternviking","guid":294,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iple9t/an_art_exhibit_in_japan_where_a_chained_robot_dog/"},{"title":"OpenAI: The Age of AI Is Here!","url":"https://www.youtube.com/watch?v=97kQRYwL3P0","date":1739557087,"author":"Two Minute Papers","guid":329,"unread":true,"content":"<article>❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n📝 The paper \"Competitive Programming with Large Reasoning Models\" is available here:\nhttps://arxiv.org/abs/2502.06807\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu</article>","contentLength":1040,"flags":null,"enclosureUrl":"https://www.youtube.com/v/97kQRYwL3P0?version=3","enclosureMime":"","commentsUrl":null},{"title":"Roadmap to Becoming a Data Scientist, Part 4: Advanced Machine Learning","url":"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-4-advanced-machine-learning/","date":1739552400,"author":"Vyacheslav Efimov","guid":9,"unread":true,"content":"<p>Data science is undoubtedly one of the most fascinating fields today.&nbsp;Following significant breakthroughs in machine learning about a decade ago, data science has surged in popularity within the tech community.&nbsp;Each year, we witness increasingly powerful tools that once seemed unimaginable.&nbsp;Innovations such as the&nbsp;,&nbsp;, the&nbsp;<em>Retrieval-Augmented Generation (RAG</em>) framework, and state-of-the-art&nbsp;&nbsp;— including&nbsp;&nbsp;— have had a profound impact on our world.</p><p>However, with the abundance of tools and the ongoing hype surrounding AI, it can be overwhelming — especially for beginners — to determine which skills to prioritize when aiming for a career in data science.&nbsp;Moreover, this field is highly demanding, requiring substantial dedication and perseverance.</p><p>The first three parts of this series outlined the necessary skills to become a data scientist in three key areas: <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-1-maths-2dc9beb69b27/\">math</a>, <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-2-software-engineering-e2fee3fe4d71/\">software engineering</a>, and <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-3-machine-learning-628248c96cb5/\">machine learning</a>.&nbsp;While knowledge of classical <a href=\"https://towardsdatascience.com/tag/machine-learning/\" title=\"Machine Learning\">Machine Learning</a> and neural network algorithms is an excellent starting point for aspiring data specialists, there are still many&nbsp;important topics in machine learning that must be mastered to work on more advanced projects.</p><blockquote><p><em>This article will focus solely on the math skills necessary to start a career in Data Science.&nbsp;Whether pursuing this path is a worthwhile choice based on your background and other factors will be discussed in a separate article.</em></p></blockquote><h2>The importance of learning evolution of methods in machine learning</h2><blockquote><p><em>The section below provides information about the evolution of methods in natural language processing (NLP).</em></p></blockquote><p>In contrast to previous articles in this series, I have decided to change the format in which I present the necessary skills for aspiring data scientists. Instead of directly listing specific competencies to develop and the motivation behind mastering them, I will briefly outline the most important approaches, presenting them in chronological order as they have been developed and used over the past decades in machine learning.</p><p>The reason is that I believe it is crucial to study these algorithms from the very beginning. In machine learning, many new methods are built upon older approaches, which is especially true for <a href=\"https://towardsdatascience.com/tag/nlp/\" title=\"NLP\">NLP</a> and computer vision.</p><p>For example, jumping directly into the implementation details of modern&nbsp;large language models (LLMs)&nbsp;without any preliminary knowledge may make it very difficult for beginners to grasp the motivation and underlying ideas of specific mechanisms.</p><p><em>Given this, in the next two sections, I will highlight in&nbsp;</em><em>&nbsp;the key concepts that should be studied.</em></p><p><strong>Natural language processing (NLP)</strong>&nbsp;is a broad field that focuses on processing textual information. Machine learning algorithms cannot work directly with raw text, which is why text is usually preprocessed and converted into numerical vectors that are then fed into neural networks.</p><p>Before being converted into vectors, words undergo&nbsp;, which includes simple techniques such as&nbsp;,&nbsp;<strong>stemming, lemmatization, normalization</strong>, or removing&nbsp;. After preprocessing, the resulting text is encoded into&nbsp;. Tokens represent the smallest textual elements in a collection of documents. Generally, a token can be a part of a word, a sequence of symbols, or an individual symbol. Ultimately, tokens are converted into numerical vectors.</p><p>The&nbsp;&nbsp;method is the most basic way to encode tokens, focusing on counting the frequency of tokens in each document. However, in practice, this is usually not sufficient, as it is also necessary to account for token importance — a concept introduced in the&nbsp;&nbsp;and&nbsp;&nbsp;methods. While TF-IDF improves upon the naive counting approach of bag of words, researchers have developed a completely new approach called embeddings.</p><p>&nbsp;are numerical vectors whose components preserve the semantic meanings of words. Because of this, embeddings play a crucial role in NLP, enabling input data to be trained or used for model inference. Additionally, embeddings can be used to compare text similarity, allowing for the retrieval of the most relevant documents from a collection.</p><blockquote><p><em>Embeddings can also be used to encode other unstructured data, including images, audio, and videos.</em></p></blockquote><p>As a field, NLP has been evolving rapidly over the last 10–20 years to efficiently solve various text-related problems. Complex tasks like text translation and text generation were initially addressed using&nbsp;<strong>recurrent neural networks (RNNs)</strong>, which introduced the concept of memory, allowing neural networks to capture and retain key contextual information in long documents.</p><p>Although RNN performance gradually improved, it remained suboptimal for certain tasks. Moreover, RNNs are relatively slow, and their sequential prediction process does not allow for parallelization during training and inference, making them less efficient.</p><p>Additionally, the original Transformer architecture can be decomposed into two separate modules:&nbsp;&nbsp;and&nbsp;. Both of these form the foundation of the most state-of-the-art models used today to solve various NLP problems. Understanding their principles is valuable knowledge that will help learners advance further when studying or working with other&nbsp;<strong>large language models (LLMs)</strong>.</p><p>When it comes to LLMs, I strongly recommend studying the evolution of at least the first three GPT models, as they have had a significant impact on the AI world we know today. In particular, I would like to highlight the concepts of&nbsp;&nbsp;and&nbsp;, introduced in&nbsp;GPT-2, which enable LLMs to solve text generation tasks without explicitly receiving any training examples for them.</p><p>Another important technique developed in recent years is&nbsp;<strong>retrieval-augmented generation (RAG)</strong>.&nbsp;<em>The main limitation of LLMs is that they are only aware of the context used during their training.</em>&nbsp;As a result, they lack knowledge of any information beyond their training data.</p><p>The retriever converts the input prompt into an embedding, which is then used to query a vector database. The database returns the most relevant context based on the similarity to the embedding. This retrieved context is then combined with the original prompt and passed to a generative model. The model processes both the initial prompt and the additional context to generate a more informed and contextually accurate response.</p><blockquote><p><em>A good example of this limitation is the first version of the ChatGPT model, which was trained on data up to the year 2022 and had no knowledge of events that occurred from 2023 onward.</em></p></blockquote><p>To address this limitation, OpenAI researchers developed a RAG pipeline, which includes a constantly updated database containing new information from external sources. When ChatGPT is given a task that requires external knowledge, it queries the database to retrieve the most relevant context and integrates it into the final prompt sent to the machine learning model.</p><p>The goal of distillation is to create a smaller model that can imitate a larger one. In practice, this means that if a large model makes a prediction, the smaller model is expected to produce a similar result.</p><p>In the modern era, LLM development has led to models with millions or even billions of parameters. As a consequence, the overall size of these models may exceed the hardware limitations of standard computers or small portable devices, which come with many constraints.</p><p>Quantization is the process of reducing the memory required to store numerical values representing a model’s weights.</p><p>This is where optimization techniques become particularly useful, allowing LLMs to be compressed without significantly compromising their performance. The most commonly used techniques today include&nbsp;,, and&nbsp;.</p><p>Pruning refers to discarding the least important weights of a model.</p><p>Regardless of the area in which you wish to specialize, knowledge of&nbsp;&nbsp;is a must-have skill! Fine-tuning is a powerful concept that allows you to efficiently adapt a pre-trained model to a new task.</p><p>Fine-tuning is especially useful when working with very large models. For example, imagine you want to use BERT to perform semantic analysis on a specific dataset. While BERT is trained on general data, it might not fully understand the context of your dataset. At the same time, training BERT from scratch for your specific task would require a massive amount of resources.</p><p>Here is where fine-tuning comes in: it involves taking a pre-trained BERT (or another model) and freezing some of its layers (usually those at the beginning). As a result, BERT is retrained, but this time only on the new dataset provided. Since BERT updates only a subset of its weights and the new dataset is likely much smaller than the original one BERT was trained on, fine-tuning becomes a very efficient technique for adapting BERT’s rich knowledge to a specific domain.</p><blockquote><p><em>Fine-tuning is widely used not only in NLP but also across many other domains.</em></p></blockquote><p>As the name suggests,&nbsp;&nbsp;involves analyzing images and videos using machine learning. The most common tasks include image classification, object detection, image segmentation, and generation.</p><p>Most CV algorithms are based on neural networks, so it is essential to understand how they work in detail. In particular, CV uses a special type of network called&nbsp;<strong>convolutional neural networks (CNNs)</strong>. These are similar to fully connected networks, except that they typically begin with a set of specialized mathematical operations called&nbsp;.</p><blockquote><p><em>In simple terms, convolutions act as filters, enabling the model to extract the most important features from an image, which are then passed to fully connected layers for further analysis.</em></p></blockquote><p>The next step is to study the most popular CNN architectures for classification tasks, such as&nbsp;<strong>AlexNet, VGG, Inception, ImageNet</strong>, and&nbsp;.</p><p>Speaking of the object detection task, the&nbsp;&nbsp;algorithm is a clear winner. It is not necessary to study all of the dozens of versions of YOLO. In reality, going through the original paper of the first YOLO should be sufficient to understand how a relatively difficult problem like object detection is elegantly transformed into both classification and regression problems. This approach in YOLO also provides a nice intuition on how more complex CV tasks can be reformulated in simpler terms.</p><p>While there are many architectures for performing image segmentation, I would strongly recommend learning about&nbsp;, which introduces an encoder-decoder architecture.</p><p>Finally, image generation is probably one of the most challenging tasks in CV. Personally, I consider it an optional topic for learners, as it involves many advanced concepts. Nevertheless, gaining a high-level intuition of how&nbsp;<strong>generative adversial networks (GAN)</strong>&nbsp;function to generate images is a good way to broaden one’s horizons.</p><blockquote><p><em>In some problems, the training data might not be enough to build a performant model. In such cases, the data augmentation technique is commonly used. It involves the artificial generation of training data from already existing data (images). By feeding the model more diverse data, it becomes capable of learning and recognizing more patterns.</em></p></blockquote><p>It would be very hard to present in detail the <a href=\"https://towardsdatascience.com/tag/roadmaps/\" title=\"Roadmaps\">Roadmaps</a> for all existing machine learning domains in a single article. That is why, in this section, I would like to briefly list and explain some of the other most popular areas in data science worth exploring.</p><p>First of all,&nbsp;<strong>recommender systems (RecSys)</strong>&nbsp;have gained a lot of popularity in recent years. They are increasingly implemented in online shops, social networks, and streaming services. The key idea of most algorithms is to take a large initial matrix of all users and items and decompose it into a product of several matrices in a way that associates every user and every item with a high-dimensional embedding. This approach is very flexible, as it then allows different types of comparison operations on embeddings to find the most relevant items for a given user. Moreover, it is much more rapid to perform analysis on small matrices rather than the original, which usually tends to have huge dimensions.</p><p> often goes hand in hand with RecSys. When a RecSys has identified a set of the most relevant items for the user, ranking algorithms are used to sort them to determine the order in which they will be shown or proposed to the user. A good example of their usage is search engines, which filter query results from top to bottom on a web page.</p><p>Closely related to ranking, there is also a&nbsp;&nbsp;problem that aims to optimally map objects from two sets, A and B, in a way that, on average, every object pair&nbsp;is mapped “well” according to a matching criterion. A use case example might include distributing a group of students to different university disciplines, where the number of spots in each class is limited.</p><p>&nbsp;is an unsupervised machine learning task whose objective is to split a dataset into several regions (clusters), with each dataset object belonging to one of these clusters. The splitting criteria can vary depending on the task. Clustering is useful because it allows for grouping similar objects together. Moreover, further analysis can be applied to treat objects in each cluster separately.</p><p>The goal of clustering is to group dataset objects (on the left) into several categories (on the right) based on their similarity.</p><p>&nbsp;is another unsupervised problem, where the goal is to compress an input dataset. When the dimensionality of the dataset is large, it takes more time and resources for machine learning algorithms to analyze it. By identifying and removing noisy dataset features or those that do not provide much valuable information, the data analysis process becomes considerably easier.</p><p>&nbsp;is an area that focuses on designing algorithms and data structures (indexes) to optimize searches in a large database of embeddings (vector database). More precisely, given an input embedding and a vector database, the goal is to&nbsp;&nbsp;find the most similar embedding in the database relative to the input embedding.</p><p>The goal of similarity search is to approximately find the most similar embedding in a vector database relative to a query embedding.</p><p>The word “approximately” means that the search is not guaranteed to be 100% precise. Nevertheless, this is the main idea behind similarity search algorithms — sacrificing a bit of accuracy in exchange for significant gains in prediction speed or data compression.</p><p>&nbsp;involves studying the behavior of a target variable over time. This problem can be solved using classical tabular algorithms. However, the presence of time introduces new factors that cannot be captured by standard algorithms. For instance:</p><ul><li>the target variable can have an overall&nbsp;, where in the long term its values increase or decrease&nbsp;<em>(e.g., the average yearly temperature rising due to global warming)</em>.</li><li>the target variable can have a&nbsp;&nbsp;which makes its values change based on the currently given period&nbsp;<em>(e.g. temperature is lower in winter and higher in summer)</em>.</li></ul><p>Most of the time series models take both of these factors into account. In general, time series models are mainly used a lot in financial, stock or demographic analysis.</p><p>Another advanced area I would recommend exploring is&nbsp;, which fundamentally changes the algorithm design compared to classical machine learning.&nbsp;In simple terms, its goal is to train an agent in an environment to make optimal decisions based on a reward system (also known as the&nbsp;<em>“trial and error approach”</em>).&nbsp;By taking an action, the agent receives a reward, which helps it understand whether the chosen action had a positive or negative effect.&nbsp;After that, the agent slightly adjusts its strategy, and the entire cycle repeats.</p><p>Reinforcement learning is particularly popular in complex environments where classical algorithms are not capable of solving a problem.&nbsp;Given the complexity of reinforcement learning algorithms and the computational resources they require, this area is not yet fully mature, but it has high potential to gain even more popularity in the future.</p><p>Currently the most popular applications are:</p><ul><li>.&nbsp;Existing approaches can design optimal game strategies and outperform humans.&nbsp;The most well-known examples are chess and Go.</li><li>.&nbsp;Advanced algorithms can be incorporated into robots to help them move, carry objects or complete routine tasks at home.</li><li>.&nbsp;Reinforcement learning methods can be developed to automatically drive cars, control helicopters or drones.</li></ul><p>This article was a logical continuation of the previous part and expanded the skill set needed to become a data scientist. While most of the mentioned topics require time to master, they can add significant value to your portfolio. This is especially true for the NLP and CV domains, which are in high demand today.</p><blockquote><p>After reaching a high level of expertise in data science, it is still crucial to stay motivated and consistently push yourself to learn new topics and explore emerging algorithms.</p></blockquote><p>Data science is a constantly evolving field, and in the coming years, we might witness the development of new state-of-the-art approaches that we could not have imagined in the past.</p><p><em>All images are by the author unless noted otherwise.</em></p>","contentLength":17044,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Publish Interactive Data Visualizations for Free with Python and Marimo","url":"https://towardsdatascience.com/publish-interactive-data-visualizations-for-free-with-python-and-marimo/","date":1739548800,"author":"Sam Minot","guid":8,"unread":true,"content":"<p>Working in <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a>, it can be hard to share insights from complex datasets using only static figures. All the facets that describe the shape and meaning of interesting data are not always captured in a handful of pre-generated figures. While we have powerful technologies available for presenting interactive figures — where a viewer can rotate, filter, zoom, and generally explore complex data  —  they always come with tradeoffs.</p><p>Here I present my experience using a recently released Python library — <a href=\"https://marimo.io\">marimo</a> — which opens up exciting new opportunities for publishing interactive visualizations across the entire field of data science.</p><h2>Interactive Data Visualization</h2><p>The tradeoffs to consider when selecting an approach for presenting data visualizations can be broken into three categories:</p><ul><li> — what visualizations and interactivity am I able to present to the user?</li><li> — what are the resources needed for displaying this visualization to users (e.g. running servers, hosting websites)?</li><li> – how much of a new skillset / codebase do I need to learn upfront?</li></ul><p> is the foundation of portable interactivity. Every user has a web browser installed on their computer and there are many different frameworks available for displaying any degree of interactivity or visualization you might imagine (for example, this <a href=\"https://threejs.org/\">gallery of amazing things people have made with three.js</a>). Since the application is running on the user’s computer, no costly servers are needed. However, a significant drawback for the data science community is ease of use, as JS does not have many of the high-level (i.e. easy-to-use) libraries that data scientists use for data manipulation, plotting, and interactivity.</p><p> provides a useful point of comparison. Because of its <a href=\"https://flatironschool.com/blog/python-popularity-the-rise-of-a-global-programming-language/\">continually growing popularity</a>, some have called this the <a href=\"https://towardsdatascience.com/we-are-living-in-the-era-of-python-bc032d595f6a\">“Era of Python”</a>. For data scientists in particular, Python stands alongside R as one of the foundational languages for quickly and effectively wielding complex data. While Python may be easier to use than Javascript, there are fewer options for presenting interactive visualizations. Some popular projects providing interactivity and visualization have been <a href=\"https://flask.palletsprojects.com/en/stable/\">Flask</a>, <a href=\"https://dash.plotly.com/\">Dash</a>, and <a href=\"https://streamlit.io/\">Streamlit</a> (also worth mentioning — <a href=\"https://docs.bokeh.org/en/latest/docs/gallery.html\">bokeh</a>, <a href=\"https://holoviews.org/\">HoloViews</a>, <a href=\"https://altair-viz.github.io/altair-tutorial/README.html\">altair</a>, and <a href=\"https://plotly.com/python/\">plotly</a>). The biggest tradeoff for using Python has been the cost for publishing – delivering the tool to users. In the same way that <a href=\"https://www.shinyapps.io/\">shinyapps</a> require a running computer to serve up the visualization, these Python-based frameworks have exclusively been server-based. This is by no means prohibitive for authors with a budget to spend, but it does limit the number of users who can take advantage of a particular project.</p><p><a href=\"https://pyodide.org/en/stable/\"></a> is an intriguing middle ground — Python code running directly in the web browser using <a href=\"https://webassembly.org/\">WebAssembly</a> (WASM). There are resource limitations (only 1 thread and 2GB memory) that make this impractical for doing the heavy lifting of data science. , this can be more than sufficient for building visualizations and updating based on user input. Because it runs in the browser, no servers are required for hosting. Tools that use Pyodide as a foundation are interesting to explore because they give data scientists an opportunity to write Python code which runs directly on users’ computers without their having to install or run anything outside of the web browser.</p><p>As an aside, <a href=\"https://towardsdatascience.com/python-based-data-viz-with-no-installation-required-aaf2358c881\">I’ve been interested previously in</a> one project that has tried this approach: <a href=\"https://github.com/whitphx/stlite\">stlite</a>, <a href=\"https://edit.share.stlite.net/\">an in-browser implementation of Streamlit</a> that lets you deploy these flexible and powerful apps to a broad range of users. However, a core limitation is that Streamlit itself is distinct from stlite (the port of Streamlit to WASM), which means that not all features are supported and that advancement of the project is dependent on two separate groups working along compatible lines.</p><ul><li>The interface resembles a Jupyter , which will be familiar to users.</li><li>Execution of cells is , so that updating one cell will rerun all cells which depend on its output.</li><li> can be captured with a flexible set of UI components.</li><li>Notebooks can be quickly converted into , hiding the code and showing only the input/output elements.</li><li>Apps can be run locally or converted into using WASM/Pyodide.</li></ul><p>marimo balances the tradeoffs of technology in a way that is well suited to the skill set of the typical data scientists:</p><ul><li> — user input and visual display features are rather extensive, <a href=\"https://docs.marimo.io/guides/working_with_data/plotting/#reactive-plots\">supporting user input</a> via Altair and Plotly plots.</li><li> — deploying as static webpages is basically free — no servers required</li><li> — for users familiar with Python notebooks, marimo will feel very familiar and be easy to pick up.</li></ul><h2>Publishing Marimo Apps on the Web</h2><p>As a simple example of the type of display that can be useful in data science, consisting of explanatory text interspersed with interactive displays, I have created a barebones <a href=\"https://github.com/FredHutch/marimo-publication\">GitHub repository</a>. Try it out yourself <a href=\"https://fredhutch.github.io/marimo-publication/\">here</a>.</p><p>Using just a little bit of code, users can:</p><ul><li>Generate visualizations with flexible interactivity</li><li>Write narrative text describing their findings</li><li>Publish to the web for free (i.e. using GitHub Pages)</li></ul><h2>Public App / Private Data</h2><p>This new technology offers an exciting new opportunity for collaboration — publish the app publicly to the world, but users can only see specific datasets that they have permission to access.</p><p>Rather than building a dedicated data backend for every app, user data can be stored in a generic backend which can be securely authenticated and accessed using a Python client library — all contained within the user’s web browser. For example, the user is given an OAuth login link that will authenticate them with the backend and allow the app to temporarily access input data.</p><p>As a proof of concept, I built a simple visualization app which connects to <a href=\"https://cirro.bio\">the Cirro data platform</a>, which is used at my institution to manage scientific data. Full disclosure: I was part of the team that built this platform before it spun out as an independent company. In this manner users can:</p><ul><li>Load the public visualization app — hosted on GitHub Pages</li><li>Connect securely to their private data store</li><li>Load the appropriate dataset for display</li><li>Share a link which will direct authorized collaborators to the same data</li></ul><p>As a data scientist, this approach of publishing free and open-source visualization apps which can be used to interact with private datasets is extremely exciting. Building and publishing a new app can take hours and days instead of weeks and years, letting researchers quickly share their insights with collaborators and then publish them to the wider world.</p>","contentLength":6594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 Tips for Building a Data Science Portfolio","url":"https://www.kdnuggets.com/5-tips-building-data-science-portfolio","date":1739545225,"author":"Nate Rosidi","guid":252,"unread":true,"content":"<article>Not every data science portfolio is worth showcasing. Follow these five tips to build a portfolio that impresses employers and gets you a job.</article>","contentLength":142,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Rosidi_5_Tips_for_Building_a_DS_Portfolio_4.png","enclosureMime":"","commentsUrl":null},{"title":"Evolving Workflow Orchestration // Alex Milowski // #291","url":"https://podcasters.spotify.com/pod/show/mlops/episodes/Evolving-Workflow-Orchestration--Alex-Milowski--291-e2us8at","date":1739543943,"author":"Demetrios","guid":261,"unread":true,"content":"<p><a href=\"https://www.linkedin.com/in/alexmilowski/\" target=\"_blank\" rel=\"noopener noreferer\"></a>&nbsp;is a researcher, developer,&nbsp;, mathematician, and&nbsp;.Evolving Workflow Orchestration // MLOps Podcast #291 with Alex Milowski, Entrepreneur and Computer Scientist.// AbstractThere seems to be a shift from workflow languages to code - mostly annotation pythons - happening and getting us. It is a symptom of how complex workflow orchestration has gotten. Is it a dominant trend or will we cycle back to “DAG specifications”? At Stitchfix, we had our own DSL that “compiled” into airflow DAGs and at MicroByre, we used a external workflow langauge. Both had a batch task executor on K8s but at MicroByre, we had human and robot in the loop workflows.// BioDr. Milowski is a serial entrepreneur and computer scientist with experience in a variety of data and machine learning technologies. He holds a PhD in Informatics (Computer Science) from the University of Edinburgh, where he researched large-scale computation over scientific data. Over the years, he's spent many years working on various aspects of workflow orchestration in industry, standardization, and in research.// MLOps Swag/Merch<a href=\"https://shop.mlops.community/\" target=\"_blank\" rel=\"noopener noreferer\">https://shop.mlops.community/</a>// Related Links<a href=\"website: https://www.milowski.com/\" target=\"_blank\" rel=\"noopener noreferer\">Website: https://www.milowski.com/</a> --------------- ✌️Connect With Us ✌️ -------------Join our slack community: <a href=\"https://go.mlops.community/slack\" target=\"_blank\" rel=\"noopener noreferer\">https://go.mlops.community/slack</a>Follow us on Twitter: <a href=\"https://podcasters.spotify.com/pod/show/mlops/episodes/@mlopscommunity\" target=\"_blank\" rel=\"noopener noreferer\">@mlopscommunity</a>Sign up for the next meetup: <a href=\"https://go.mlops.community/register\" target=\"_blank\" rel=\"noopener noreferer\">https://go.mlops.community/register</a>Catch all episodes, blogs, newsletters, and more: <a href=\"https://mlops.community/\" target=\"_blank\" rel=\"noopener noreferer\">https://mlops.community/</a>Connect with Demetrios on LinkedIn: <a href=\"https://www.linkedin.com/in/dpbrinkm/\" target=\"_blank\" rel=\"noopener noreferer\">https://www.linkedin.com/in/dpbrinkm/</a>Connect with Alex on LinkedIn: <a href=\"https://www.linkedin.com/in/alexmilowski/\" target=\"_blank\" rel=\"noopener noreferer\">https://www.linkedin.com/in/alexmilowski/</a></p>","contentLength":1617,"flags":null,"enclosureUrl":"https://anchor.fm/s/174cb1b8/podcast/play/98492189/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-14%2F394870366-44100-2-f81b1b5d49c1e.mp3","enclosureMime":"","commentsUrl":null},{"title":"Building a Data Engineering Center of Excellence","url":"https://towardsdatascience.com/building-a-data-engineering-center-of-excellence/","date":1739500548,"author":"Richie Bachala","guid":7,"unread":true,"content":"<p>As data continues to grow in importance and become more complex, the need for skilled data engineers has never been greater. But what is data engineering, and why is it so important? In this blog post, we will discuss the essential components of a functioning data engineering practice and why data engineering is becoming increasingly critical for businesses today, and how you can build your very own Data Engineering Center of Excellence!</p><p>I’ve had the privilege to build, manage, lead, and foster a sizeable high-performing team of data warehouse &amp; ELT engineers for many years. With the help of my team, I have spent a considerable amount of time every year consciously planning and preparing to manage the growth of our data month-over-month and address the changing reporting and analytics needs for our&nbsp;<em>20000+ global data consumers</em>. We built many data warehouses to store and centralize massive amounts of data generated from many OLTP sources. We’ve implemented Kimball methodology by creating star schemas both within our on-premise data warehouses and in the ones in the cloud.</p><p>The objective is to enable our user-base to perform fast analytics and reporting on the data; so our analysts’ community and business users can make accurate data-driven decisions.</p><p>It took me about three years to transform&nbsp;&nbsp;() of data warehouse and ETL programmers into one cohesive Data Engineering team.</p><p><em>I have compiled some of my learnings building a global data engineering team in this post in hopes that Data professionals and leaders of all levels of technical proficiency can benefit.</em></p><h2>Evolution of the Data Engineer</h2><p>It has never been a better time to be a data engineer. Over the last decade, we have seen a massive awakening of enterprises now recognizing their data as the company’s heartbeat, making data engineering the job function that ensures accurate, current, and quality data flow to the solutions that depend on it.</p><p>Historically, the role of Data Engineers has evolved from that of&nbsp;<strong><em>data warehouse developers&nbsp;</em></strong>and the&nbsp;&nbsp;(extract, transform and load).</p><p>The data warehouse developers are responsible for designing, building, developing, administering, and maintaining data warehouses to meet an enterprise’s reporting needs. This is done primarily via extracting data from operational and transactional systems and piping it using extract transform load methodology (ETL/ ELT) to a storage layer like a data warehouse or a data lake. The data warehouse or the data lake is where data analysts, data scientists, and business users consume data. The developers also perform transformations to conform the ingested data to a data model with aggregated data for easy analysis.</p><blockquote><p>A data engineer’s prime responsibility is to produce and make data securely available for multiple consumers.</p></blockquote><p>Data engineers oversee the ingestion, transformation, modeling, delivery, and movement of data through every part of an organization. Data extraction happens from many different data sources &amp; applications. Data Engineers load the data into data warehouses and data lakes, which are transformed not just for the <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a> &amp; predictive analytics initiatives (as everyone likes to talk about) but primarily for data analysts. Data analysts &amp; data scientists perform operational reporting, exploratory analytics, service-level agreement (SLA) based business intelligence reports and dashboards on the catered data. In this book, we will address all of these job functions.</p><p>The role of a data engineer is to acquire, store, and aggregate data from both cloud and on-premise, new, and existing systems, with data modeling and feasible data architecture. Without the data engineers, analysts and data scientists won’t have valuable data to work with, and hence, data engineers are the first to be hired at the inception of every new data team. Based on the data and analytics tools available within an enterprise, data engineering teams’ role profiles, constructs, and approaches have several options for what should be included in their responsibilities which we will discuss in this chapter.</p><p>Software is increasingly automating the historically manual and tedious tasks of data engineers. Data processing tools and technologies have evolved massively over several years and will continue to grow. For example, cloud-based data warehouses (Snowflake, for instance) have made data storage and processing affordable and fast. Data pipeline services (like&nbsp;<a href=\"https://www.informatica.com/blogs/welcome-to-informatica-intelligent-cloud-services.html\" rel=\"noreferrer noopener\" target=\"_blank\">Informatica IICS</a>,&nbsp;<a href=\"https://airflow.apache.org/\" rel=\"noreferrer noopener\" target=\"_blank\">Apache Airflow</a>,&nbsp;<a href=\"https://www.matillion.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Matillion</a>,&nbsp;<a href=\"http://fivetran.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Fivetran</a>) have turned data extraction into work that can be completed quickly and efficiently. The data engineering team should be leveraging such technologies as force multipliers, taking a consistent and cohesive approach to integration and management of enterprise data, not just relying on legacy siloed approaches to building custom data pipelines with fragile, non-performant, hard to maintain code. Continuing with the latter approach will stifle the pace of innovation within the said enterprise and force the future focus to be around managing data infrastructure issues rather than how to help generate value for your business.</p><p>The primary role of an enterprise Data Engineering team should be to&nbsp;&nbsp;into a shape that’s ready for analysis — laying the foundation for real-world analytics and data science application.</p><p>The Data Engineering team should serve as the&nbsp;&nbsp;for enterprise-level data with the responsibility to curate the organization’s data and act as a resource for those who want to make use of it, such as Reporting &amp; Analytics teams, Data Science teams, and other groups that are doing more self-service or business group driven analytics leveraging the enterprise data platform. This team should serve as the&nbsp;&nbsp;of organizational knowledge, managing and refining the catalog so that analysis can be done more effectively. Let’s look at the essential responsibilities of a well-functioning Data Engineering team.</p><h2>Responsibilities of a Data Engineering Team</h2><p>The Data Engineering team should provide a&nbsp;&nbsp;within the enterprise that cuts across to support both the Reporting/Analytics and Data Science capabilities to provide access to clean, transformed, formatted, scalable, and secure data ready for analysis. The Data Engineering teams’ core responsibilities should include:</p><blockquote><p>· Build, manage, and optimize the core data platform infrastructure</p><p>· Build and maintain custom and off-the-shelf data integrations and ingestion pipelines from a variety of structured and unstructured sources</p><p>· Manage overall data pipeline orchestration</p><p>· Manage transformation of data either before or after load of raw data through both technical processes and business logic</p><p>· Support analytics teams with design and performance optimizations of data warehouses</p></blockquote><p><strong><em>Data is an Enterprise Asset.</em></strong></p><p><strong><em>Data as an Asset should be shared and protected.</em></strong></p><p>Data should be valued as an Enterprise asset, leveraged across all Business Units to enhance the company’s value to its respective customer base by accelerating decision making, and improving competitive advantage with the help of data. Good data stewardship, legal and regulatory requirements dictate that we protect the data owned from unauthorized access and disclosure.</p><p>In other words,&nbsp;<strong><em>managing Security is a crucial responsibility.</em></strong></p><h2>Why Create a Centralized Data Engineering Team?</h2><p>Treating Data Engineering as a standard and core capability that underpins both the Analytics and Data Science capabilities will help an enterprise evolve how to approach Data and Analytics. The enterprise needs to stop vertically treating data based on the technology stack involved as we tend to see often and move to more of a horizontal approach of managing a&nbsp;&nbsp;or&nbsp;&nbsp;that cuts across the organization and can connect to various technologies as needed drive analytic initiatives. This is a new way of thinking and working, but it can drive efficiency as the various data organizations look to scale. Additionally — there is value in creating a dedicated structure and career path for Data Engineering resources. Data engineering skill sets are in high demand in the market; therefore, hiring outside the company can be costly. Companies must enable programmers, database administrators, and software developers with a career path to gain the needed experience with the above-defined skillsets by working across technologies. Usually, forming a data engineering center of excellence or a capability center would be the first step for making such progression possible.</p><h2>Challenges for creating a centralized Data Engineering Team</h2><p>The centralization of the Data Engineering team as a service approach is different from how Reporting &amp; Analytics and Data Science teams operate. It does, in principle, mean&nbsp;<strong><em>giving up some level of control of resources</em></strong>&nbsp;and establishing new processes for how these teams will collaborate and work together to deliver initiatives.</p><p>The Data Engineering team will need to demonstrate that it can effectively support the needs of both Reporting &amp; Analytics and Data Science teams, no matter how large these teams are. Data Engineering teams must&nbsp;<strong><em>effectively prioritize workloads&nbsp;</em></strong>while ensuring they can bring the right skillsets and experience to assigned projects.</p><p>Data engineering is essential because it serves as the backbone of data-driven companies. It enables analysts to work with clean and well-organized data, necessary for deriving insights and making sound decisions. To build a functioning data engineering practice, you need the following critical components:</p><p>The Data Engineering team should be a core capability within the enterprise, but it should effectively serve as a support function involved in almost everything data-related. It should interact with the Reporting and Analytics and Data Science teams in a collaborative support role to make the entire team successful.</p><p>The&nbsp;<em>Data Engineering team doesn’t create direct business value</em>&nbsp;— but the value should come in making the Reporting and Analytics, and Data Science teams more productive and efficient to ensure delivery of maximum value to business stakeholders through Data &amp; Analytics initiatives. To make that possible, the six key responsibilities within the data engineering capability center would be as follow –</p><p>Let’s review the&nbsp;<strong><em>6 pillars of responsibilities</em></strong>:</p><p><strong>1. Determine Central Data Location for Collation and Wrangling</strong></p><p>Understanding and having a strategy for a&nbsp;(<em>a centralized data repository or data warehouse for the mass consumption of data for analysis</em>). Defining requisite data tables and where they will be joined in the context of data engineering and subsequently converting raw data into digestible and valuable formats.</p><p><strong>2. Data Ingestion and Transformation</strong></p><p>Moving data from one or more sources to a new destination (<em>your data lake or cloud data warehouse)&nbsp;</em>where it can be stored and further analyzed and then converting data from the format of the source system to that of the destination</p><p>Extracting, transforming, and loading data from one or more sources into a destination system to represent the data in a new context or style.</p><p>Data modeling is an essential function of a data engineering team, granted not all data engineers excel with this capability. Formalizing relationships between data objects and business rules into a conceptual representation through understanding information system workflows, modeling required queries, designing tables, determining primary keys, and effectively utilizing data to create informed output.</p><p>I’ve seen engineers in interviews mess up more with this than coding in technical discussions. It’s essential to understand the differences between Dimensions, Facts, Aggregate tables.</p><p>Ensuring that sensitive data is protected and implementing proper authentication and authorization to reduce the risk of a data breach</p><p><strong>6. Architecture and Administration</strong></p><p>Defining the models, policies, and standards that administer what data is collected, where and how it is stored, and how it such data is integrated into various analytical systems.</p><blockquote><p>The six pillars of responsibilities for data engineering capabilities center on the ability to determine a central data location for collation and wrangling, ingest and transform data, execute ETL/ELT operations, model data, secure access and administer an architecture. While all companies have their own specific needs with regards to these functions, it is important to ensure that your team has the necessary skillset in order to build a foundation for big data success.</p></blockquote><p>Besides the Data Engineering following are the other capability centers that need to be considered within an enterprise:</p><h2>Analytics Capability Center</h2><p>The analytics capability center enables consistent, effective, and efficient BI, analytics, and advanced analytics capabilities across the company. Assist business functions in triaging, prioritizing, and achieving their objectives and goals through reporting, analytics, and dashboard solutions, while providing operational reports and visualizations, self-service analytics, and required tools to automate the generation of such insights.</p><h2>Data Science Capability Center</h2><p>The data science capability center is for exploring cutting-edge technologies and concepts to unlock new insights and opportunities, better inform employees and create a culture of prescriptive information usage using Automated AI and Automated ML solutions such as&nbsp;<a href=\"https://medium.com/u/9aea625dfc27?source=post_page---user_mention--b83d51cedb6a---------------------------------------\" rel=\"noreferrer noopener\" target=\"_blank\">H2O.ai</a>,&nbsp;<a href=\"https://medium.com/u/27e43843bc9f?source=post_page---user_mention--b83d51cedb6a---------------------------------------\" rel=\"noreferrer noopener\" target=\"_blank\">Dataiku</a>,&nbsp;<a href=\"http://www.aible.com/\" rel=\"noreferrer noopener\" target=\"_blank\">Aible</a>, DataRobot,&nbsp;<a href=\"https://medium.com/u/3aaaf223f1e?source=post_page---user_mention--b83d51cedb6a---------------------------------------\" rel=\"noreferrer noopener\" target=\"_blank\">C3.ai</a></p><p>The data governance office empowers users with trusted, understood, and timely data to drive effectiveness while keeping the integrity and sanctity of data in the right hands for mass consumption.</p><blockquote><p><em>As your company grows, you will want to make sure that the data engineering capabilities are in place to support the six pillars of responsibilities. By doing this, you will be able to ensure that all aspects of data management and analysis are covered and that your data is safe and accessible by those who need it. Have you started thinking about how your company will grow? What steps have you taken to put a centralized data engineering team in place?</em></p></blockquote>","contentLength":14079,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer — Part 5: The Training","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-5-the-training/","date":1739480672,"author":"David Martin","guid":6,"unread":true,"content":"<p>In this fifth part of my series, I will outline the steps for creating a Docker container for training your image classification model, evaluating performance, and preparing for deployment.</p><p>AI/ML engineers would prefer to focus on model training and data engineering, but the reality is that we also need to understand the infrastructure and mechanics behind the scenes.</p><p>I hope to share some tips, not only to get your training run running, but how to streamline the process in a cost efficient manner on cloud resources such as Kubernetes.</p><p>I will reference elements from my previous articles for getting the best model performance, so be sure to check out&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>&nbsp;and&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>&nbsp;on the data sets, as well as&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>&nbsp;and&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>&nbsp;on model evaluation.</p><p>Here are the learnings that I will share with you, once we lay the groundwork on the infrastructure:</p><ul><li>Building your Docker container</li><li>Executing your training run</li></ul><p>First, let me provide a brief description of the setup that I created, specifically around Kubernetes. Your setup may be entirely different, and that is just fine. I simply want to set the stage on the infrastructure so that the rest of the discussion makes sense.</p><p>This is a server you deploy that provides a user interface to for your subject matter experts to label and evaluate images for the image classification application. The server can run as a pod on your Kubernetes cluster, but you may find that running a dedicated server with faster disk may be better.</p><p>Image files are stored in a directory structure like the following, which is self-documenting and easily modified.</p><pre><code>Image_Library/\n  - cats/\n    - image1001.png\n  - dogs/\n    - image2001.png</code></pre><p>Ideally, these files would reside on local server storage (instead of cloud or cluster storage) for better performance. The reason for this will become clear as we see what happens as the image library grows.</p><p><a href=\"https://towardsdatascience.com/tag/cloud-storage/\" title=\"Cloud Storage\">Cloud Storage</a> allows for a virtually limitless and convenient way to share files between systems. In this case, the image library on your management system could access the same files as your Kubernetes cluster or Docker engine.</p><p>However, the downside of cloud storage is the latency to open a file. Your image library will have&nbsp;&nbsp;of images, and the latency to read each file will have a significant impact on your training run time. Longer training runs means more cost for using the expensive GPU processors!</p><p>The way that I found to speed things up is to create a&nbsp;&nbsp;file of your image library on your management system and copy them to cloud storage. Even better would be to create multiple tar files&nbsp;, each containing 10,000 to 20,000 images.</p><p>This way you only have network latency on a handful of files (which contain thousands, once extracted) and you start your training run much sooner.</p><h3>Kubernetes or Docker engine</h3><p>A Kubernetes cluster, with proper configuration, will allow you to dynamically scale up/down nodes, so you can perform your model training on GPU hardware as needed. Kubernetes is a rather heavy setup, and there are other container engines that will work.</p><blockquote><p>The technology options change constantly!</p></blockquote><p>The main idea is that you want to spin up the resources you need — for only as long as you need them — then scale down to reduce your time (and therefore cost) of running expensive GPU resources.</p><p>Once your GPU node is started and your <a href=\"https://towardsdatascience.com/tag/docker/\" title=\"Docker\">Docker</a> container is running, you can extract the&nbsp;&nbsp;files above to&nbsp;&nbsp;storage, such as an&nbsp;, on your node. The node typically has high-speed SSD disk, ideal for this type of workload. There is one caveat — the storage capacity on your node must be able to handle your image library.</p><p>Assuming we are good, let’s talk about building your Docker container so that you can train your model on your image library.</p><h2>Building your Docker container</h2><p>Being able to execute a training run in a consistent manner lends itself perfectly to building a Docker container. You can “pin” the version of libraries so you know exactly how your scripts will run every time. You can version control your containers as well, and revert to a known good image in a pinch. What is really nice about Docker is you can run the container pretty much anywhere.</p><p>The tradeoff when running in a container, especially with an <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> model, is the speed of file storage. You can attach any number of volumes to your container, but they are usually&nbsp;&nbsp;attached, so there is latency on each file read. This may not be a problem if you have a small number of files. But when dealing with hundreds of thousands of files like image data, that latency adds up!</p><p>This is why using the&nbsp;&nbsp;file method outlined above can be beneficial.</p><p>Also, keep in mind that Docker containers could be terminated unexpectedly, so you should make sure to store important information outside the container, on cloud storage or a database. I’ll show you how below.</p><p>Knowing that you will need to run on GPU hardware (here I will assume Nvidia), be sure to select the right base image for your Dockerfile, such as&nbsp;&nbsp;with the “devel&nbsp;flavor that will contain the right drivers.</p><p>Next, you will add the script files to your container, along with a “batch” script to coordinate the execution. Here is an example Dockerfile, and then I’ll describe what each of the scripts will be doing.</p><pre><code>#####   Dockerfile   #####\nFROM nvidia/cuda:12.8.0-devel-ubuntu24.04\n\n# Install system software\nRUN apt-get -y update &amp;&amp; apg-get -y upgrade\nRUN apt-get install -y python3-pip python3-dev\n\n# Setup python\nWORKDIR /app\nCOPY requirements.txt\nRUN python3 -m pip install --upgrade pip\nRUN python3 -m pip install -r requirements.txt\n\n# Pythong and batch scripts\nCOPY ExtractImageLibrary.py .\nCOPY Training.py .\nCOPY Evaluation.py .\nCOPY ScorePerformance.py .\nCOPY ExportModel.py .\nCOPY BulkIdentification.py .\nCOPY BatchControl.sh .\n\n# Allow for interactive shell\nCMD tail -f /dev/null</code></pre><p>Dockerfiles are declarative, almost like a cookbook for building a small server — you know what you’ll get every time. Python libraries benefit, too, from this declarative approach. Here is a sample&nbsp;&nbsp;file that loads the TensorFlow libraries with CUDA support for GPU acceleration.</p><pre><code>#####   requirements.txt   #####\nnumpy==1.26.3\npandas==2.1.4\nscipy==1.11.4\nkeras==2.15.0\ntensorflow[and-cuda]</code></pre><h3>Extract Image Library script</h3><p>In <a href=\"https://towardsdatascience.com/tag/kubernetes/\" title=\"Kubernetes\">Kubernetes</a>, the Docker container can access local, high speed storage on the physical node. This can be achieved via the&nbsp;&nbsp;volume type. As mentioned before, this will only work if the local storage on your node can handle the size of your library.</p><pre><code>#####   sample 25GB emptyDir volume in Kubernetes   #####\ncontainers:\n  - name: training-container\n    volumeMounts:\n      - name: image-library\n        mountPath: /mnt/image-library\nvolumes:\n  - name: image-library\n    emptyDir:\n      sizeLimit: 25Gi</code></pre><p>You would want to have another&nbsp;&nbsp;to your cloud storage where you have the&nbsp;&nbsp;files. What this looks like will depend on your provider, or if you are using a persistent volume claim, so I won’t go into detail here.</p><p>Now you can extract the&nbsp;&nbsp;files — ideally in parallel for an added performance boost — to the local mount point.</p><p>As AI/ML engineers, the model training is where we want to spend most of our time.</p><blockquote><p>This is where the magic happens!</p></blockquote><p>With your image library now extracted, we can create our train-validation-test sets, load a pre-trained model or build a new one, fit the model, and save the results.</p><p>One key technique that has served me well is to load the most recently trained model as my base. I discuss this in more detail in&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>&nbsp;under “Fine tuning”, this results in faster training time and significantly improved model performance.</p><p>Be sure to take advantage of the local storage to checkpoint your model during training since the models are quite large and you are paying for the GPU even while it sits idle writing to disk.</p><p>This of course raises a concern about what happens if the Docker container dies part-way though the training. The risk is (hopefully) low from a cloud provider, and you may not want an incomplete training anyway. But if that does happen, you will at least want to understand&nbsp;, and this is where saving the main log file to cloud storage (described below) or to a package like MLflow comes in handy.</p><p>After your training run has completed and you have taken proper precaution on saving your work, it is time to see how well it performed.</p><p>Normally this evaluation script will pick up on the model that just finished. But you may decide to point it at a previous model version through an interactive session. This is why have the script as stand-alone.</p><p>With it being a separate script, that means it will need to read the completed model from disk — ideally local disk for speed. I like having two separate scripts (training and evaluation), but you might find it better to combine these to avoid reloading the model.</p><p>Now that the model is loaded, the evaluation script should generate predictions on&nbsp;&nbsp;image in the training, validation, test, and benchmark sets. I save the results as a&nbsp;&nbsp;matrix with the softmax confidence score for each class label. So, if there are 1,000 classes and 100,000 images, that’s a table with 100 million scores!</p><p>I save these results in&nbsp;&nbsp;files that are then used in the score generation next.</p><p>Taking the matrix of scores produced by the evaluation script above, we can now create various metrics of model performance. Again, this process could be combined with the evaluation script above, but my preference is for independent scripts. For example, I might want to regenerate scores on previous training runs. See what works for you.</p><p>Here are some of the&nbsp;&nbsp;functions that produce useful insights like F1, log loss, AUC-ROC, Matthews correlation coefficient.</p><pre><code>from sklearn.metrics import average_precision_score, classification_report\nfrom sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score</code></pre><p>Aside from these basic statistical analyses for each dataset (train, validation, test, and benchmark), it is also useful to identify:</p><ul><li>Which&nbsp;&nbsp;labels get the most number of errors?</li><li>Which&nbsp;&nbsp;labels get the most number of incorrect guesses?</li><li>How many&nbsp;<strong>ground-truth-to-predicted</strong>&nbsp;label pairs are there? In other words, which classes are easily confused?</li><li>What is the&nbsp;&nbsp;when applying a minimum softmax confidence score threshold?</li><li>What is the&nbsp;&nbsp;above that softmax threshold?</li><li>For the “difficult” benchmark sets, do you get a sufficiently&nbsp;&nbsp;score?</li><li>For the “out-of-scope” benchmark sets, do you get a sufficiently&nbsp;&nbsp;score?</li></ul><p>As you can see, there are multiple calculations and it’s not easy to come up with a single evaluation to decide if the trained model is good enough to be moved to production.</p><p>In fact, for an image classification model, it is helpful to manually review the images that the model got wrong, as well as the ones that got a low softmax confidence score. Use the scores from this script to create a list of images to manually review, and then get a&nbsp;&nbsp;for how well the model performs.</p><p>Check out&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>&nbsp;for more in-depth discussion on evaluation and scoring.</p><p>All of the heavy lifting is done by this point. Since your Docker container will be shutdown soon, now is the time to copy the model artifacts to cloud storage and prepare them for being put to use.</p><p>The example Python code snippet below is more geared to Keras and TensorFlow. This will take the trained model and export it as a&nbsp;. Later, I will show how this is used by TensorFlow Serving in the&nbsp;&nbsp;section below.</p><pre><code># Increment current version of model and create new directory\nnext_version_dir, version_number = create_new_version_folder()\n\n# Copy model artifacts to the new directory\ncopy_model_artifacts(next_version_dir)\n\n# Create the directory to save the model export\nsaved_model_dir = os.path.join(next_version_dir, str(version_number))\n\n# Save the model export for use with TensorFlow Serving\ntf.keras.backend.set_learning_phase(0)\nmodel = tf.keras.models.load_model(keras_model_file)\ntf.saved_model.save(model, export_dir=saved_model_dir)</code></pre><p>This script also copies the other training run artifacts such as the model evaluation results, score summaries, and log files generated from model training. Don’t forget about your label map so you can give human readable names to your classes!</p><h3>Bulk identification script</h3><p>Your training run is complete, your model has been scored, and a new version is exported and ready to be served. Now is the time to use this latest model to assist you on trying to identify unlabeled images.</p><p>As I described in&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, you may have a collection of “unknowns” — really good pictures, but no idea what they are. Let your new model provide a best guess on these and record the results to a file or a database. Now you can create filters based on closest match and by high/low scores. This allows your subject matter experts to leverage these filters to find new image classes, add to existing classes, or to remove images that have very low scores and are no good.</p><p>By the way, I put this step inside the GPU container since you may have thousands of “unknown” images to process and the accelerated hardware will make light work of it. However, if you are not in a hurry, you could perform this step on a separate CPU node, and shutdown your GPU node sooner to save cost. This would especially make sense if your “unknowns” folder is on slower cloud storage.</p><p>All of the scripts described above perform a specific task — from extracting your image library, executing model training, performing evaluation and scoring, exporting the model artifacts for deployment, and perhaps even bulk identification.</p><blockquote><p>One script to rule them all</p></blockquote><p>To coordinate the entire show, this batch script gives you the entry point for your container and an easy way to trigger everything. Be sure to produce a log file in case you need to analyze any failures along the way. Also, be sure to write the log to your cloud storage in case the container dies unexpectedly.</p><pre><code>#!/bin/bash\n# Main batch control script\n\n# Redirect standard output and standard error to a log file\nexec &gt; /cloud_storage/batch-logfile.txt 2&gt;&amp;1\n\n/app/ExtractImageLibrary.py\n/app/Training.py\n/app/Evaluation.py\n/app/ScorePerformance.py\n/app/ExportModel.py\n/app/BulkIdentification.py</code></pre><h2>Executing your training run</h2><p>So, now it’s time to put everything in motion…</p><p>Let’s go through the steps to prepare your image library, fire up your Docker container to train your model, and then examine the results.</p><h3>Image library ‘tar’ files</h3><p>Your image management system should now create a&nbsp;&nbsp;file backup of your data. Since&nbsp;&nbsp;is a single-threaded function, you will get significant speed improvement by creating multiple tar files in parallel, each with a portion of you data.</p><p>Now these files can be copied to your shared cloud storage for the next step.</p><p>All the hard work you put into creating your container (described above) will be put to the test. If you are running Kubernetes, you can create a Job that will execute the&nbsp;&nbsp;script.</p><p>Inside the Kubernetes Job definition, you can pass environment variables to adjust the execution of your script. For example, the batch size and number of epochs are set here and then pulled into your Python scripts, so you can alter the behavior without changing your code.</p><pre><code>#####   sample Job in Kubernetes   #####\ncontainers:\n  - name: training-job\n    env:\n      - name: BATCH_SIZE\n        value: 50\n      - name: NUM_EPOCHS\n        value: 30\n    command: [\"/app/BatchControl.sh\"]</code></pre><p>Once the Job is completed, be sure to verify that the GPU node properly scales back down to zero according to your scaling configuration in Kubernetes — you don’t want to be saddled with a huge bill over a simple configuration error.</p><p>With the training run complete, you should now have model artifacts saved and can examine the performance. Look through the metrics, such as F1 and log loss, and benchmark accuracy for high softmax confidence scores.</p><p>As mentioned earlier, the reports only tell part of the story. It is worth the time and effort to manually review the images that the model got wrong or where it produced a low confidence score.</p><p>Don’t forget about the bulk identification. Be sure to leverage these to locate new images to fill out your data set, or to find new classes.</p><p>Once you have reviewed your model performance and are satisfied with the results, it is time to modify your TensorFlow Serving container to put the new model into production.</p><p>TensorFlow Serving is available as a Docker container and provides a very quick and convenient way to serve your model. This container can listen and respond to API calls for your model.</p><p>Let’s say your new model is version 7, and your&nbsp;&nbsp;script (see above) has saved the model in your cloud share as&nbsp;<em>/image_application/models/007</em>. You can start the TensorFlow Serving container with that volume mount. In this example, the&nbsp;&nbsp;points to folder for version 007.</p><pre><code>#####   sample TensorFlow pod in Kubernetes   #####\ncontainers:\n  - name: tensorflow-serving\n    image: bitnami/tensorflow-serving:2.18.0\n    ports:\n      - containerPort: 8501\n    env:\n      - name: TENSORFLOW_SERVING_MODEL_NAME\n        value: \"image_application\"\n    volumeMounts:\n      - name: models-subfolder\n        mountPath: \"/bitnami/model-data\"\n\nvolumes:\n  - name: models-subfolder\n    azureFile:\n      shareName: \"image_application/models/007\"</code></pre><p>A subtle note here — the export script should create a sub-folder, named 007 (same as the base folder), with the saved model export. This may seem a little confusing, but TensorFlow Serving will mount this share folder as&nbsp;&nbsp;and detect the numbered sub-folder inside it for the version to serve. This will allow you to query the API for the model version as well as the identification.</p><p>As I mentioned at the start of this article, this setup has worked for my situation. This is certainly not the only way to approach this challenge, and I invite you to customize your own solution.</p><p>I wanted to share my hard-fought learnings as I embraced cloud services in Kubernetes, with the desire to keep costs under control. Of course, doing all this while maintaining a high level of model performance is an added challenge, but one that you can achieve.</p><p>I hope I have provided enough information here to help you with your own endeavors. Happy learnings!</p>","contentLength":18226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer — Part 3: The Evaluation","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/","date":1739480406,"author":"David Martin","guid":5,"unread":true,"content":"<p>In this third part of my series, I will explore the evaluation process which is a critical piece that will lead to a cleaner data set and elevate your model performance. We will see the difference between evaluation of a&nbsp;&nbsp;model (one not yet in production), and evaluation of a&nbsp;&nbsp;model (one making real-world predictions).</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, I discussed the process of labelling your image data that you use in your <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> project. I showed how to define “good” images and create sub-classes. In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, I went over various data sets, beyond the usual train-validation-test sets, such as benchmark sets, plus how to handle synthetic data and duplicate images.</p><h2><strong>Evaluation of the trained model</strong></h2><p>As machine learning engineers we look at accuracy, F1, log loss, and other metrics to decide if a model is ready to move to production. These are all important measures, but from my experience, these scores can be deceiving especially as the number of classes grows.</p><p>Although it can be time consuming, I find it very important to manually review the images that the model gets&nbsp;, as well as the images that the model gives a&nbsp;&nbsp;softmax “confidence” score to. This means adding a step immediately after your training run completes to calculate scores for&nbsp;&nbsp;images — training, validation, test, and the benchmark sets. You only need to bring up for manual review the ones that the model had problems with. This should only be a small percentage of the total number of images. See the Double-check process below</p><p>What you do during the manual evaluation is to put yourself in a “” to ensure that the labelling standards are being followed that you setup in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>. Ask yourself:</p><ul><li>“Is this a good image?” Is the subject front and center, and can you clearly see all the features?</li><li>“Is this the correct label?” Don’t be surprised if you find wrong labels.</li></ul><p>You can either remove the bad images or fix the labels if they are wrong. Otherwise you can keep them in the data set and force the model to do better next time. Other questions I ask are:</p><ul><li>“Why did the model get this wrong?”</li><li>“Why did this image get a low score?”</li><li>“What is it about the image that caused confusion?”</li></ul><p>Sometimes the answer has nothing to do with&nbsp;&nbsp;specific image. Frequently, it has to do with the&nbsp;&nbsp;images, either in the ground truth class or in the predicted class. It is worth the effort to Double-check all images in both sets if you see a consistently bad guess. Again, don’t be surprised if you find poor images or wrong labels.</p><p>When doing the evaluation of the trained model (above), we apply a lot of subjective analysis — “Why did the model get this wrong?” and “Is this a good image?” From these, you may only get a&nbsp;.</p><p>Frequently, I will decide to hold off moving a model forward to production based on that gut feel. But how can you justify to your manager that you want to hit the brakes? This is where putting a more&nbsp;&nbsp;analysis comes in by creating a weighted average of the softmax “confidence” scores.</p><p>In order to apply a weighted evaluation, we need to identify sets of classes that deserve adjustments to the score. Here is where I create a list of “commonly confused” classes.</p><h2><strong>Commonly confused classes</strong></h2><p>Certain animals at our zoo can easily be mistaken. For example, African elephants and Asian elephants have different ear shapes. If your model gets these two mixed up, that is not as bad as guessing a giraffe! So perhaps you give partial credit here. You and your subject matter experts (SMEs) can come up with a list of these pairs and a weighted adjustment for each.</p><p>This weight can be factored into a modified cross-entropy loss function in the equation below. The back half of this equation will reduce the impact of being wrong for specific pairs of ground truth and prediction by using the “weight” function as a lookup. By default, the weighted adjustment would be 1 for all pairings, and the commonly confused classes would get something like 0.5.</p><p>In other words, it’s better to be unsure (have a&nbsp;&nbsp;confidence score) when you are wrong, compared to being super confident and wrong.</p><p>Once this weighted log loss is calculated, I can compare to previous training runs to see if the new model is ready for production.</p><h2><strong>Confidence threshold report</strong></h2><p>Another valuable measure that incorporates the confidence threshold (in my example, 95) is to report on accuracy and false positive rates. Recall that when we apply the confidence threshold before presenting results, we help reduce false positives from being shown to the end user.</p><p>In this table, we look at the breakdown of “true positive above 95” for each data set. We get a sense that when a “good” picture comes through (like the ones from our train-validation-test set) it is very likely to surpass the threshold, thus the user is “happy” with the outcome. Conversely, the “false positive above 95” is extremely low for good pictures, thus only a small number of our users will be “sad” about the results.</p><p>We expect the train-validation-test set results to be exceptional since our data is curated. So, as long as people take “good” pictures, the model should do very well. But to get a sense of how it does on extreme situations, let’s take a look at our benchmarks.</p><p>The “difficult” benchmark has more modest true positive and false positive rates, which reflects the fact that the images are more challenging. These values are much easier to compare across training runs, so that lets me set a min/max target. So for example, if I target a minimum of 80% for true positive, and maximum of 5% for false positive on this benchmark, then I can feel confident moving this to production.</p><p>The “out-of-scope” benchmark has no true positive rate because&nbsp;&nbsp;of the images belong to any class the model can identify. Remember, we picked things like a bag of popcorn, etc., that are not zoo animals, so there cannot be any true positives. But we do get a false positive rate, which means the model gave a confident score to that bag of popcorn as some animal. And if we set a target maximum of 10% for this benchmark, then we may not want to move it to production.</p><p>Right now, you may be thinking, “Well, what animal did it pick for the bag of popcorn?” Excellent question! Now you understand the importance of doing a manual review of the images that get bad results.</p><h2><strong>Evaluation of the deployed model</strong></h2><p>The evaluation that I described above applies to a model immediately after&nbsp;. Now, you want to evaluate how your model is doing in the&nbsp;. The process is similar, but requires you to shift to a “” and asking yourself, “Did the model get this correct?” and “Should it have gotten this correct?” and “Did we tell the user the right thing?”</p><p>So, imagine that you are logging in for the morning — after sipping on your&nbsp;<a href=\"https://medium.com/@dmartin0409/cold-brew-coffee-0aabd53a1f3e\">cold brew coffee</a>, of course — and are presented with 500 images that your zoo guests took yesterday of different animals. Your job is to determine how satisfied the guests were using your model to identify the zoo animals.</p><p>Using the softmax “confidence” score for each image, we have a threshold before presenting results. Above the threshold, we tell the guest what the model predicted. I’ll call this the “happy path”. And below the threshold is the “sad path” where we ask them to try again.</p><p>Your review interface will first show you all the “happy path” images one at a time. This is where you ask yourself, “Did we get this right?” Hopefully, yes!</p><p>But if not, this is where things get tricky. So now you have to ask, “Why not?” Here are some things that it could be:</p><ul><li>“Bad” picture — Poor lighting, bad angle, zoomed out, etc — refer to your labelling standards.</li><li>Out-of-scope — It’s a zoo animal, but unfortunately one that isn’t found in&nbsp;&nbsp;zoo. Maybe it belongs to another zoo (your guest likes to travel and try out your app). Consider adding these to your data set.</li><li>Out-of-scope — It’s not a zoo animal. It could be an animal in your zoo, but not one typically&nbsp;&nbsp;there, like a neighborhood sparrow or mallard duck. This might be a candidate to add.</li><li>Out-of-scope — It’s something found in the zoo. A zoo usually has interesting trees and shrubs, so people might try to identify those. Another candidate to add.</li><li>Prankster — Completely out-of-scope. Because people like to play with technology, there’s the possibility you have a prankster that took a picture of a bag of popcorn, or a soft drink cup, or even a selfie. These are hard to prevent, but hopefully get a low enough score (below the threshold) so the model did not identify it as a zoo animal. If you see enough pattern in these, consider creating a class with special handling on the front-end.</li></ul><p>After reviewing the “happy path” images, you move on to the “sad path” images — the ones that got a low confidence score and the app gave a “sorry, try again” message. This time you ask yourself, “&nbsp;the model have given this image a higher score?” which would have put it in the “happy path”. If so, then you want to ensure these images are added to the training set so next time it will do better. But most of time, the low score reflects many of the “bad” or out-of-scope situations mentioned above.</p><p>Perhaps your model performance is suffering and it has nothing to do with your model. Maybe it is the ways you users interacting with the app. Keep an eye out of non-technical problems and share your observations with the rest of your team. For example:</p><ul><li>Are your users using the application in the ways you expected?</li><li>Are they not following the instructions?</li><li>Do the instructions need to be stated more clearly?</li><li>Is there anything you can do to improve the experience?</li></ul><h2><strong>Collect statistics and new images</strong></h2><p>Both of the manual evaluations above open a gold mine of data. So, be sure to collect these statistics and feed them into a dashboard — your manager and your future self will thank you!</p><p>Keep track of these stats and generate reports that you and your can reference:</p><ul><li>How often the model is being called?</li><li>What times of the day, what days of the week is it used?</li><li>Are your system resources able to handle the peak load?</li><li>What classes are the most common?</li><li>After evaluation, what is the accuracy for each class?</li><li>What is the breakdown for confidence scores?</li><li>How many scores are above and below the confidence threshold?</li></ul><p>The single best thing you get from a deployed model is the additional real-world images! You can add these now images to improve coverage of your existing zoo animals. But more importantly, they provide you insight on&nbsp;&nbsp;classes to add. For example, let’s say people enjoy taking a picture of the large walrus statue at the gate. Some of these may make sense to incorporate into your data set to provide a better user experience.</p><p>Creating a new class, like the walrus statue, is not a huge effort, and it avoids the false positive responses. It would be more embarrassing to identify a walrus statue as an elephant! As for the prankster and the bag of popcorn, you can configure your front-end to quietly handle these. You might even get creative and have fun with it like, “Thank you for visiting the food court.”</p><p>It is a good idea to double-check your image set when you suspect there may be problems with your data. I’m not suggesting a top-to-bottom check, because that would a monumental effort! Rather specific classes that you suspect could contain bad data that is degrading your model performance.</p><p>Immediately after my training run completes, I have a script that will use this new model to generate predictions for my&nbsp;&nbsp;data set. When this is complete, it will take the list of incorrect identifications, as well as the low scoring predictions, and automatically feed that list into the Double-check interface.</p><p>This interface will show, one at a time, the image in question, alongside an example image of the ground truth and an example image of what the model predicted. I can visually compare the three, side-by-side. The first thing I do is ensure the original image is a “good” picture, following my labelling standards. Then I check if the ground-truth label is indeed correct, or if there is something that made the model think it was the predicted label.</p><ul><li>Remove the original image if the image quality is poor.</li><li>Relabel the image if it belongs in a different class.</li></ul><p>During this manual evaluation, you might notice dozens of the same wrong prediction. Ask yourself why the model made this mistake when the images seem perfectly fine. The answer may be some incorrect labels on images in the ground truth, or even in the predicted class!</p><p>Don’t hesitate to add those classes and sub-classes back into the Double-check interface and step through them all. You may have 100–200 pictures to review, but there is a good chance that one or two of the images will stand out as being the culprit.</p><p>With a different mindset for a trained model versus a deployed model, we can now evaluate performances to decide which models are ready for production, and how well a production model is going to serve the public. This relies on a solid Double-check process and a critical eye on your data. And beyond the “gut feel” of your model, we can rely on the benchmark scores to support us.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, we kick off the training run, but there are some subtle techniques to get the most out of the process and even ways to leverage throw-away models to expand your library image data.<a href=\"https://medium.com/tag/machine-learning?source=post_page-----e4a8dbb035e0---------------------------------------\"></a></p>","contentLength":13463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build a dynamic, role-based AI agent using Amazon Bedrock inline agents","url":"https://aws.amazon.com/blogs/machine-learning/build-a-dynamic-role-based-ai-agent-using-amazon-bedrock-inline-agents/","date":1739480188,"author":"Ishan Singh","guid":132,"unread":true,"content":"<p>AI agents continue to gain momentum, as businesses use the power of generative AI to reinvent customer experiences and automate complex workflows. We are seeing <a href=\"https://aws.amazon.com/bedrock/agents/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Agents</a> applied in investment research, insurance claims processing, root cause analysis, advertising campaigns, and much more. Agents use the reasoning capability of foundation models (FMs) to break down user-requested tasks into multiple steps. They use developer-provided instructions to create an orchestration plan and carry out that plan by securely invoking company APIs and accessing knowledge bases using Retrieval Augmented Generation (RAG) to accurately handle the user’s request.</p><p>Although organizations see the benefit of agents that are defined, configured, and tested as managed resources, we have increasingly seen the need for an additional, more dynamic way to invoke agents. Organizations need solutions that adjust on the fly—whether to test new approaches, respond to changing business rules, or customize solutions for different clients. This is where the new <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/agents-create-inline.html\" target=\"_blank\" rel=\"noopener\">inline agents</a> capability in Amazon Bedrock Agents becomes transformative. It allows you to dynamically adjust your agent’s behavior at runtime by changing its instructions, tools, guardrails, knowledge bases, prompts, and even the FMs it uses—all without redeploying your application.</p><p>In this post, we explore how to build an application using Amazon Bedrock inline agents, demonstrating how a single AI assistant can adapt its capabilities dynamically based on user roles.</p><h2>Inline agents in Amazon Bedrock Agents</h2><p>This runtime flexibility enabled by inline agents opens powerful new possibilities, such as:</p><ul><li> – Inline agents minimize the time-consuming create/update/prepare cycles traditionally required for agent configuration changes. Developers can instantly test different combinations of models, tools, and knowledge bases, dramatically accelerating the development process.</li><li><strong>A/B testing and experimentation</strong> – Data science teams can systematically evaluate different model-tool combinations, measure performance metrics, and analyze response patterns in controlled environments. This empirical approach enables quantitative comparison of configurations before production deployment.</li><li><strong>Subscription-based personalization</strong> – Software companies can adapt features based on each customer’s subscription level, providing more advanced tools for premium users.</li><li><strong>Persona-based data source integration</strong> – Institutions can adjust content complexity and tone based on the user’s profile, providing persona-appropriate explanations and resources by changing the knowledge bases associated to the agent on the fly.</li><li> – Developers can create applications with hundreds of APIs, and quickly and accurately carry out tasks by dynamically choosing a small subset of APIs for the agent to consider for a given request. This is particularly helpful for large software as a service (SaaS) platforms needing multi-tenant scaling.</li></ul><p>Inline agents expand your options for building and deploying agentic solutions with Amazon Bedrock Agents. For workloads needing managed and versioned agent resources with a pre-determined and tested configuration (specific model, instructions, tools, and so on), developers can continue to use InvokeAgent on resources created with CreateAgent. For workloads that need dynamic runtime behavior changes for each agent invocation, you can use the new <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent-runtime_InvokeInlineAgent.html\" target=\"_blank\" rel=\"noopener\">InvokeInlineAgent API</a>. With either approach, your agents will be secure and scalable, with configurable guardrails, a flexible set of model inference options, native access to knowledge bases, code interpretation, session memory, and more.</p><p>Our HR assistant example shows how to build a single AI assistant that adapts to different user roles using the new inline agent capabilities in Amazon Bedrock Agents. When users interact with the assistant, the assistant dynamically configures agent capabilities (such as model, instructions, knowledge bases, action groups, and guardrails) based on the user’s role and their specific selections. This approach creates a flexible system that adjusts its functionality in real time, making it more efficient than creating separate agents for each user role or tool combination. The complete code for this HR assistant example is available on our <a href=\"https://github.com/awslabs/amazon-bedrock-agent-samples/tree/main/examples/agents_ux/inline-agent-hr-assistant\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>.</p><p>This dynamic tool selection enables a personalized experience. When an employee logs in without direct reports, they see a set of tools that they have access to based on their role. They can select from options like requesting vacation time, checking company policies using the knowledge base, using a code interpreter for data analysis, or submitting expense reports. The inline agent assistant is then configured with only these selected tools, allowing it to assist the employee with their chosen tasks. In a real-world example, the user would not need to make the selection, because the application would make that decision and automatically configure the agent invocation at runtime. We make it explicit in this application so that you can demonstrate the impact.</p><p>Similarly, when a manager logs in to the same system, they see an extended set of tools reflecting their additional permissions. In addition to the employee-level tools, managers have access to capabilities like running performance reviews. They can select which tools they want to use for their current session, instantly configuring the inline agent with their choices.</p><p>The inclusion of knowledge bases is also adjusted based on the user’s role. Employees and managers see different levels of company policy information, with managers getting additional access to confidential data like performance review and compensation details. For this demo, we’ve implemented <a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-knowledge-bases-now-supports-metadata-filtering-to-improve-retrieval-accuracy/\" target=\"_blank\" rel=\"noopener\">metadata filtering</a> to retrieve only the appropriate level of documents based on the user’s access level, further enhancing efficiency and security.</p><p>Let’s look at how the interface adapts to different user roles.</p><p>The employee view provides access to essential HR functions like vacation requests, expense submissions, and company policy lookups. Users can select which of these tools they want to use for their current session.</p><p>The manager view extends these options to include supervisory functions like compensation management, demonstrating how the inline agent can be configured with a broader set of tools based on user permissions.</p><p>The manager view extends these capabilities to include supervisory functions like compensation management, demonstrating how the inline agent dynamically adjusts its available tools based on user permissions. Without inline agents, we would need to build and maintain two separate agents.</p><p>As shown in the preceding screenshots, the same HR assistant offers different tool selections based on the user’s role. An employee sees options like Knowledge Base, Apply Vacation Tool, and Submit Expense, whereas a manager has additional options like Performance Evaluation. Users can select which tools they want to add to the agent for their current interaction.</p><p>This flexibility allows for quick adaptation to user needs and preferences. For instance, if the company introduces a new policy for creating business travel requests, the tool catalog can be quickly updated to include a Create Business Travel Reservation tool. Employees can then choose to add this new tool to their agent configuration when they need to plan a business trip, or the application could automatically do so based on their role.</p><p>With Amazon Bedrock inline agents, you can create a catalog of actions that is dynamically selected by the application or by users of the application. This increases the level of flexibility and adaptability of your solutions, making them a perfect fit for navigating the complex, ever-changing landscape of modern business operations. Users have more control over their AI assistant’s capabilities, and the system remains efficient by only loading the necessary tools for each interaction.</p><h2>Technical foundation: Dynamic configuration and action selection</h2><p>Inline agents allow dynamic configuration at runtime, enabling a single agent to effectively perform the work of many. By specifying action groups and modifying instructions on the fly, even within the same session, you can create versatile AI applications that adapt to various scenarios without multiple agent deployments.</p><p>The following are key points about inline agents:</p><ul><li> – Change the agent’s configuration, including its FM, at runtime. This enables rapid experimentation and adaptation without redeploying the application, reducing development cycles.</li><li> – Apply governance and access control at the tool level. With agents changing dynamically at runtime, tool-level governance helps maintain security and compliance regardless of the agent’s configuration.</li><li> – Provide only necessary tools and instructions at runtime to reduce token usage and improve the agent accuracy. With fewer tools to choose from, it’s less complicated for the agent to select the right one, reducing hallucinations in the tool selection process. This approach can also lead to lower costs and improved latency compared to static agents because removing unnecessary tools, knowledge bases, and instructions reduces the number of input and output tokens being processed by the agent’s large language model (LLM).</li><li> – Create reusable actions for dynamic selection based on specific needs. This modular approach simplifies maintenance, updates, and scalability of your AI applications.</li></ul><p>The following are examples of reusable actions:</p><ul><li><strong>Enterprise system integration</strong> – Connect with systems like Salesforce, GitHub, or databases</li><li> – Perform common tasks such as sending emails or managing calendars</li><li> – Interact with specialized internal tools and services</li><li> – Analyze text, structured data, or other information</li><li> – Fetch weather updates, stock prices, or perform web searches</li><li> – Use specific machine learning (ML) models for targeted tasks</li></ul><p>When using inline agents, you configure parameters for the following:</p><ul><li>Contextual tool selection based on user intent or conversation flow</li><li>Adaptation to different user roles and permissions</li><li>Switching between communication styles or personas</li><li>Model selection based on task complexity</li></ul><p>The inline agent uses the configuration you provide at runtime, allowing for highly flexible AI assistants that efficiently handle various tasks across different business contexts.</p><h2>Building an HR assistant using inline agents</h2><p>Let’s look at how we built our HR Assistant using Amazon Bedrock inline agents:</p><ol><li> – We developed a demo catalog of HR-related tools, including: \n  <ul type=\"a\"><li> – Using <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> for accessing company policies and guidelines based on the role of the application user. In order to filter the knowledge base content based on the user’s role, you also need to provide a metadata file specifying the type of employee’s roles that can access each file</li><li>– For requesting and tracking time off.</li><li>– For submitting and managing expense reports.</li><li> – For performing calculations and data analysis.</li><li>– for conducting and reviewing employee compensation assessments (manager only access).</li></ul></li><li> – We defined multiple conversation tones to suit different interaction styles: \n  <ul><li> – For formal, business-like interactions.</li><li> – For friendly, everyday support.</li><li> – For upbeat, encouraging assistance.</li></ul></li><li> – We implemented role-based access control. The application backend checks the user’s role (employee or manager) and provides access to appropriate tools and information and passes this information to the inline agent. The role information is also used to configure metadata filtering in the knowledge bases to generate relevant responses. The system allows for dynamic tool use at runtime. Users can switch personas or add and remove tools during their session, allowing the agent to adapt to different conversation needs in real time.</li><li><strong>Integrate the agent with other services and tools</strong> – We connected the inline agent to: \n  <ul><li>Amazon Bedrock Knowledge Bases for company policies, with metadata filtering for role-based access.</li><li><a href=\"http://aws.amazon.com/lambda\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> functions for executing specific actions (such as submitting vacation requests or expense reports).</li><li>A code interpreter tool for performing calculations and data analysis.</li></ul></li><li> – We created a Flask-based UI that performs the following actions: \n  <ul><li>Displays available tools based on the user’s role.</li><li>Allows users to select different personas.</li><li>Provides a chat window for interacting with the HR assistant.</li></ul></li></ol><p>To understand how this dynamic role-based functionality works under the hood, let’s examine the following system architecture diagram.</p><p>As shown in preceding architecture diagram, the system works as follows:</p><ol><li>The end-user logs in and is identified as either a manager or an employee.</li><li>The user selects the tools that they have access to and makes a request to the HR assistant.</li><li>The agent breaks down the problems and uses the available tools to solve for the query in steps, which may include: \n  <ol><li>Amazon Bedrock Knowledge Bases (with metadata filtering for role-based access).</li><li>Lambda functions for specific actions.</li><li>Code interpreter tool for calculations.</li><li>Compensation tool (accessible only to managers to submit base pay raise requests).</li></ol></li><li>The application uses the Amazon Bedrock inline agent to dynamically pass in the appropriate tools based on the user’s role and request.</li><li>The agent uses the selected tools to process the request and provide a response to the user.</li></ol><p>This approach provides a flexible, scalable solution that can quickly adapt to different user roles and changing business needs.</p><p>In this post, we introduced the Amazon Bedrock inline agent functionality and highlighted its application to an HR use case. We dynamically selected tools based on the user’s roles and permissions, adapted instructions to set a conversation tone, and selected different models at runtime. With inline agents, you can transform how you build and deploy AI assistants. By dynamically adapting tools, instructions, and models at runtime, you can:</p><ul><li>Create personalized experiences for different user roles</li><li>Optimize costs by matching model capabilities to task complexity</li><li>Streamline development and maintenance</li><li>Scale efficiently without managing multiple agent configurations</li></ul><p>For organizations demanding highly dynamic behavior—whether you’re an AI startup, SaaS provider, or enterprise solution team—inline agents offer a scalable approach to building intelligent assistants that grow with your needs. To get started, explore our <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/blob/main/agents-and-function-calling/bedrock-agents/features-examples/15-invoke-inline-agents/inline-agent-api-usage.ipynb\" target=\"_blank\" rel=\"noopener\">GitHub repo</a> and <a href=\"https://github.com/awslabs/amazon-bedrock-agent-samples/tree/main/examples/agents_ux/inline-agent-hr-assistant\" target=\"_blank\" rel=\"noopener\">HR assistant demo application</a>, which demonstrate key implementation patterns and best practices.</p><p>To learn more about how to be most successful in your agent journey, read our two-part blog series:</p><p>To get started with Amazon Bedrock Agents, check out the following <a href=\"https://github.com/awslabs/amazon-bedrock-agent-samples/\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> with example code.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/blog-image-ishansin-1.jpeg\" alt=\"\" width=\"100\" height=\"133\">&nbsp;is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/mttanke-225x300-1-1.jpg\" alt=\"\" width=\"100\" height=\"133\">&nbsp;is a Senior Generative AI Data Scientist at AWS. With a background in machine learning, she has over 10 years of experience architecting and building AI applications with customers across industries. As a technical lead, she helps customers accelerate their achievement of business value through generative AI solutions on Amazon Bedrock. In her free time, Maira enjoys traveling, playing with her cat, and spending time with her family someplace warm.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/roymark-225x300-1-1.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Mark’s work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS certifications, including the ML Specialty Certification.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/enitin.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Sr. Enterprise Solutions Architect at AWS, experienced in Software Engineering, Enterprise Architecture, and AI/ML. He is deeply passionate about exploring the possibilities of generative AI. He collaborates with customers to help them build well-architected applications on the AWS platform, and is dedicated to solving technology challenges and assisting with their cloud journey.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/10/Image-from-iOS-244x300-1.jpg\" alt=\"\" width=\"100\" height=\"123\"> is a Software Development Engineer at Amazon Web Services (AWS). He specializes in backend system design, distributed architectures, and scalable solutions, contributing to the development and launch of high-impact systems at Amazon. Outside of work, he spends his time playing ping pong and hiking through Cascade trails, enjoying the outdoors as much as he enjoys building systems.</p><p> is a Software Development Engineer at Amazon Web Services (AWS), working in Agents for Amazon Bedrock. He focuses on developing scalable systems on the cloud that enable AI applications frameworks and orchestrations. Shubham also has a background in building distributed, scalable, high-volume-high-throughput systems in IoT architectures.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/09/18/vivekbh-100.jpg\" alt=\"\" width=\"100\" height=\"134\"> is a Principal Engineer for Amazon Bedrock.&nbsp;He focuses on building deep learning-based AI and computer vision solutions for AWS customers. Oustide of work, Vivek enjoys trekking and following cricket.</p>","contentLength":17760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer — Part 1: The Data","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/","date":1739480153,"author":"David Martin","guid":4,"unread":true,"content":"<p>It is said that in order for a machine learning model to be successful, you need to have good data. While this is true (and pretty much obvious), it is extremely difficult to define, build, and sustain good data. Let me share with you the unique processes that I have learned over several years building an ever-growing image classification system and how you can apply these techniques to your own application.</p><p>With persistence and diligence, you can avoid the classic “garbage in, garbage out”, maximize your model accuracy, and demonstrate real business value.</p><p>In this series of articles, I will dive into the care and feeding of a multi-class, single-label image classification app and what it takes to reach the highest level of performance. I won’t get into any coding or specific user interfaces, just the main concepts that you can incorporate to suit your needs with the tools at your disposal.</p><p>Here is a brief description of the articles. You will notice that the model is last on the list since we need to focus on curating the data first and foremost:</p><p>Over the past six years, I have been primarily focused on building and maintaining an image classification application for a manufacturing company. Back when I started, most of the software did not exist or was too expensive, so I created these from scratch. In this time, I have deployed two identifier applications, the largest handles 1,500 classes and achieves 97–98% accuracy.</p><p>It was about eight years ago that I started online studies for <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a> and machine learning. So, when the exciting opportunity to create an AI application presented itself, I was prepared to build the tools I needed to leverage the latest advancements. I jumped in with both feet!</p><p>I quickly found that building and deploying a model is probably the easiest part of the job. Feeding high quality data into the model is the best way to improve performance, and that requires focus and patience. Attention to detail is what I do best, so this was a perfect fit.</p><h2><strong>It all starts with the data</strong></h2><p>I feel that so much attention is given to the model selection (deciding which neural network is best) and that the data is just an afterthought. I have found the hard way that even one or two pieces of bad data can significantly impact model performance, so that is where we need to focus.</p><p>For example, let’s say you train the classic cat versus dog image classifier. You have 50 pictures of cats and 50 pictures of dogs, however one of the “cats” is clearly (objectively) a picture of a dog. The computer doesn’t have the luxury of ignoring the mislabelled image, and instead adjusts the model weights to make it fit. Square peg meets round hole.</p><p>Another example would be a picture of a cat that climbed up into a tree. But when you take a wholistic view of it, you would describe it as a picture of a tree (first) with a cat (second). Again, the computer doesn’t know to ignore the big tree and focus on the cat — it will start to identify trees as cats, even if there is a dog. You can think of these pictures as outliers and should be removed.</p><p>It doesn’t matter if you have the best neural network in the world, you can count on the model making poor predictions when it is trained on “bad” data. I’ve learned that any time I see the model make mistakes, it’s time to review the data.</p><h2><strong>Example Application — Zoo animals</strong></h2><p>For the rest of this write-up, I will use an example of identifying zoo animals. Let’s assume your goal is to create a mobile app where guests at the zoo can take pictures of the animals they see and have the app identify them. Specifically, this is a multi-class, single-label application.</p><ul><li>&nbsp;— There are a lot of different animals at the zoo and many of them look very similar.</li><li>&nbsp;— Guests using the app don’t always take good pictures (zoomed out, blurry, too dark), so we don’t want to provide an answer if the image is poor.</li><li>&nbsp;— The zoo keeps expanding and adding new species all the time.</li><li>&nbsp;— Occasionally you might find that people take pictures of the sparrows near the food court grabbing some dropped popcorn.</li><li>&nbsp;— Just for fun, guests may take a picture of the bag of popcorn just to see what it comes back with.</li></ul><p>These are all real challenges — being able to tell the subtle differences between animals, handling out-of-scope cases, and just plain poor images.</p><p>Before we get there, let’s start from the beginning.</p><p>There are a lot of tools these days to help you with this part of the process, but the challenge remains the same — collecting, labelling, and curating the data.</p><p>Having data to collect is challenge #1. Without images, you have nothing to train. You may need to get creative on sourcing the data, or even creating synthetic data. More on that later.</p><p>A quick note about image pre-processing. I convert all my images to the input size of my neural network and save them as PNG. Inside this square PNG, I preserve the aspect ratio of the original picture and fill the background black. I don’t stretch the image nor crop any features out. This also helps center the subject.</p><p>Challenge #2 is to establish standards for data quality…and ensure that these standards are followed! These standards will guide you toward that “good” data. And this assumes, of course, correct labels. Having both is much easier said than done!</p><p>I hope to show how “good” and “correct” actually go hand-in-hand, and how important it is to apply these standards to every image.</p><p>First, I want to point out that the image data discussed here is for the training set. What qualifies as a good image for&nbsp;&nbsp;is a bit different than what qualifies as a good image for&nbsp;. More on that in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>So, what is “good” data when talking about images? “A picture is worth a thousand words”, and if the&nbsp;&nbsp;you use to describe the picture do not include the subject you are trying to label, then it is not good and you need remove it from your training set.</p><p>For example, let’s say you are shown a picture of a zebra and (removing bias toward your application) you describe it as an “open field with a zebra in the distance”. In other words, if “open field” is the first thing you notice, then you likely do&nbsp;&nbsp;want to use that image. The opposite is also true — if the picture is way too close, you would described it as “zebra pattern”.</p><p>What you want is a description like, “a zebra, front and center”. This would have your subject taking up about 80–90% of the total frame. Sometimes I will take the time to crop the original image so the subject is framed properly.</p><p>Keep in mind the use of image augmentation at the time of training. Having that buffer around the edges will allow “zoom in” augmentation. And “zoom out” augmentation will simulate smaller subjects, so don’t start out less than 50% of the total frame for your subject since you lose detail.</p><p>Another aspect of a “good” image relates to the label. If you can only see the back side of your zoo animal, can you really tell, for example, that it is a cheetah versus a leopard? The key identifying features need to be visible. If a human struggles to identify it, you can’t expect the computer to learn anything.</p><p>What does a “bad” image look like? Here is what I frequently watch out for:</p><ul><li>Wide angle lens stretching</li><li>High contrast or dark shadows</li><li>“Doctored” images, drawn lines and arrows</li><li>“Unusual” angles or situations</li><li>Picture of a mobile device that has a picture of your subject</li></ul><p>If you have a team of subject matter experts (SMEs) on hand to label the images, you are in a good starting position. Animal trainers at the zoo know the various species, and can spot the differences between, for example, a chimpanzee and a bonobo.</p><p>To a <a href=\"https://towardsdatascience.com/tag/machine-learning-engineer/\" title=\"Machine Learning Engineer\">Machine Learning Engineer</a>, it is easy for you to assume all labels from your SMEs are correct and move right on to training the model. However, even experts make mistakes, so if you can get a second opinion on the labels, your error rate should go down.</p><p>In reality, it can be prohibitively expensive to get one, let alone two, subject matter experts to review image labels. The SME usually has years of experience that make them more valuable to the business in other areas of work. My experience is that the machine learning engineer (that’s you and me) becomes the second opinion, and often the first opinion as well.</p><p>Over time, you can become pretty adept at labelling, but certainly not an SME. If you do have the luxury of access to an expert, explain to them the labelling standards and how these are required for the application to be successful. Emphasize “quality over quantity”.</p><p>It goes without saying that having a&nbsp;&nbsp;label is so important. However, all it takes is one or two mislabelled images to degrade performance. These can easily slip into your data set with careless or hasty labelling. So, take the time to get it right.</p><p>Ultimately, we as the ML engineer are responsible for model performance. So, if we take the approach of only working on model training and deployment, we will find ourselves wondering why performance is falling short.</p><p>A lot of times, you will come across a really good picture of a very interesting subject, but have no idea what it is! It would be a shame to simply dispose of it. What you can do is assign it a generic label, like “Unknown Bird” or “Random Plant” that are&nbsp;&nbsp;included in your training set. Later in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, you’ll see how to come back to these images at a later date when you have a better idea what they are, and you’ll be glad you saved them.</p><p>If you have done any image labelling, then you know how time consuming and difficult it can be. But this is where having a model, even a less-than-perfect model, can help you.</p><p>Typically, you have a large collection of unlabelled image and you need to go through them one at a time to assign labels. Simply having the model offer a best guess and display the top 3 results lets you step through each image in a matter of seconds!</p><p>Even if the top 3 results are wrong, this can help you narrow down your search. Over time, newer models will get better, and the labelling process can even be somewhat fun!</p><p>In <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/\">Part 4</a>, I will show how you can bulk identify images and take this to the next level for faster labelling.</p><p>I mentioned the example above of two species that look very similar, the chimpanzee and the bonobo. When you start out building your data set, you may have very sparse coverage of one or both of these species. In machine learning terms, we these “classes”. One option is to roll with what you have and hope that the model picks up on the differences with only a handful of example images.</p><p>The option that I have used is to merge two or more classes into one, at least temporarily. So, in this case I would create a class called “chimp-bonobo”, which is composed of the limited example pictures of chimpanzee and bonobo species classes. Combined, these may give me enough to train the model on “chimp-bonobo”, with the trade-off that it’s a more generic identification.</p><p>Sub-classes can even be normal variations. For example,&nbsp;&nbsp;pink flamingos are grey instead of pink. Or, male and female orangutans have distinct facial features. You wan to have a fairly balanced number of images for these normal variations, and keeping sub-classes will allow you to accomplish this.</p><p>Don’t be concerned that you are merging completely different looking classes — the neural network does a nice job of applying the “OR” operator. This works both ways — it can help you identify male or female variations as one species, but it can hurt you when “bad” outlier images sneak in like the example “open field with a zebra in the distance.”</p><p>Over time, you will (hopefully) be able to collect more images of the sub-classes and then be able to successfully split them apart (if necessary) and train the model to identify them separately. This process has worked very well for me. Just be sure to double-check all the images when you split them to ensure the labels didn’t get accidentally mixed up — it will be time well spent.</p><p>All of this certainly depends on your user requirements, and you can handle this in different ways either by creating a unique class label like “chimp-bonobo”, or at the front-end presentation layer where you notify the user that you have intentionally merged these classes and provide guidance on further refining the results. Even after you decide to split the two classes, you may want to caution the user that the model could be wrong since the two classes are so similar.</p><p>I realize this was a long write-up for something that on the surface seems intuitive, but these are all areas that I have tripped me up in the past because I didn’t give them enough attention. Once you have a solid understanding of these principles, you can go on to build a successful application.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, we will take the curated data we collected here to create the classic data sets, with a custom benchmark set that will further enhance your data. Then we will see how best to evaluate our trained model using a specific “training mindset”, and switch to a “production mindset” when evaluating a deployed model.</p>","contentLength":13198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer — Part 4: The Model","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-4-the-model/","date":1739480022,"author":"David Martin","guid":3,"unread":true,"content":"<p>In this latest part of my series, I will share what I have learned on selecting a model for <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> and how to fine tune that model. I will also show how you can leverage the model to accelerate your labelling process, and finally how to justify your efforts by generating usage and performance statistics.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, I discussed the process of labelling your image data that you use in your image classification project. I showed how define “good” images and create sub-classes. In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, I went over various data sets, beyond the usual train-validation-test sets, with benchmark sets, plus how to handle synthetic data and duplicate images. In <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>, I explained how to apply different evaluation criteria to a trained model versus a deployed model, and using benchmarks to determine when to deploy a model.</p><p>So far I have focused a lot of time on labelling and curating the set of images, and also evaluating model performance, which is like putting the cart before the horse. I’m not trying to minimize what it takes to design a massive neural network — this is a very important part of the application you are building. In my case, I spent a few weeks experimenting with different available models before settling on one that fit the bill.</p><p>Once you pick a model structure, you usually don’t make any major changes to it. For me, six years into deployment, I’m still using the same one. Specifically, I chose Inception V4 because it has a large input image size and an adequate number of layers to pick up on subtle image features. It also performs inference fast enough on CPU, so I don’t need to run expensive hardware to serve the model.</p><p>Your mileage may vary. But again, the main takeaway is that focusing on your data will pay dividends versus searching for the best model.</p><p>I will share a process that I found to work extremely well. Once I decided on the model to use, I randomly initialized the weights and let the model train for about 120 epoch before improvements plateau at a fairly modest accuracy, like 93%. At this point, I performed the evaluation of the trained model (see <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>) to clean up the data set. I also incorporated new images as part of the data pipeline (see <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>) and prepared the data sets for the next training run.</p><p>Before starting the next training run, I simply take the last trained model, pop the output layer, and add it back in with random weights. Since the number of output classes are constantly increasing in my case, I have to pop that layer anyway to account for the new number of classes. Importantly, I leave the rest of the trained weights as they were and allow them to continue updating for the new classes.</p><p>This allows the model to train much faster before improvements stall. After repeating this process dozens of times, the training reaches plateau after about 20 epochs, and the test accuracy can reach 99%! The model is building upon the low-level features that it established from the previous runs while re-learning the output weights to prevent overfitting.</p><p>It took me a while to trust this process, and for a few years I would train from scratch every time. But after I attempted this and saw the training time (not to mention the cost of cloud GPU) go down while the accuracy continued to go up, I started to embrace the process. More importantly, I continue to see the evaluation metrics of the deployed model return solid performances.</p><p>During training, you can apply transformations on your images (called “augmentation”) to give you more diversity from you data set. With our zoo animals, it is fairly safe to apply left-right flop, slight rotations clockwise and counterclockwise, and slight resize that will zoom in and out.</p><p>With these transformations in mind, make sure your images are still able to act as good training images. In other words, an image where the subject is already small will be even smaller with a zoom out, so you probably want to discard the original. Also, some of your original pictures may need to be re-oriented by 90 degrees to be upright since a further rotation would make them look unusual.</p><p>As I mentioned in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, you can use the trained model to assist you in labelling images one at a time. But the way to take this even further is to have your newly trained model identify hundreds at a time while building a list of the results that you can then filter.</p><p>Typically, we have large collections of&nbsp;&nbsp;images that have come in either through regular usage of the application or some other means. Recall from <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a> assigning “unknown” labels to interesting pictures but you have no clue what it is. By using the bulk identification method, we can sift through the collections quickly to target the labelling once we know what they are.</p><p>By combining your current image counts with the bulk identification results, you can target classes that need expanded coverage. Here are a few ways you can leverage bulk identification:</p><ul><li><strong>Increase low image counts</strong>&nbsp;— Some of your classes may have just barely made the cutoff to be included in the training set, which means you need more examples to improve coverage. Filter for images that have low counts.</li><li><strong>Replace staged or synthetic images</strong>&nbsp;— Some classes may be built entirely using non-real-world images. These pictures may be good enough to get started with, but may cause performance issues down the road because they look different than what typically comes through. Filter for classes that depend on staged images.</li><li>&nbsp;— A class in your data set may look like another one. For example, let’s say your model can identify an antelope, and that looks like a gazelle which your model cannot identify yet. Setting a filter for antelope and a lower confidence score may reveal gazelle images that you can label.</li><li>&nbsp;— You may not have known how to identify the dozens of cute wallaby pictures, so you saved them under “Unknown” because it was a good image. Now that you know what it is, you can filter for its look-alike kangaroo and quickly add a new class.</li><li><strong>Mass removal of low scores</strong>&nbsp;— As a way to clean out your large collection of unlabelled images that have nothing worth labelling, set a filter for lowest scores.</li></ul><p>Recall the decision I made to have image cutoffs from <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/\">Part 2</a>, which allows us to ensure an adequate number of example images of a class before we train and server a model to the public. The problem is that you may have a number of classes that are&nbsp;&nbsp;below your cutoff (in my case, 40) and don’t make it into the model.</p><p>The way I approach this is with a “throw-away” training run that I do not intend to move to production. I will decrease the lower cutoff from 40 to perhaps 35, build my train-validation-test sets, then train and evaluate like I normally do. The most important part of this is the bulk identification at the end!</p><p>There is a chance that somewhere in the large collection of unlabelled images I will find the few that I need. Doing the bulk identification with this throw-away model helps find them.</p><p>One very important aspect of any machine learning application is being able to show usage and performance reports. Your manager will likely want to see how many times the application is being used to justify the expense, and you as the ML engineer will want to see how the latest model is performing compared to the previous one.</p><p>You should build logging into your model serving to record every transaction going through the system. Also, the manual evaluations from <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a> should be recorded so you can report on performance for such things as accuracy over time, by model version, by confidence scores, by class, etc. You will be able to detect trends and make adjustments to improve the overall solution.</p><p>There are a lot of reporting tools, so I won’t recommend one over the other. Just make sure you are collecting as much information as you can to build these dashboards. This will justify the time, effort, and cost associated with maintaining the application.</p><p>We covered a lot of ground across this four-part series on building an image classification project and deploying it in the real world. It all starts with the data, and by investing the time and effort into maintaining the highest quality image library, you can reach impressive levels of model performance that will gain the trust and confidence of your business partners.</p><p>As a <a href=\"https://towardsdatascience.com/tag/machine-learning-engineer/\" title=\"Machine Learning Engineer\">Machine Learning Engineer</a>, you are primarily responsible for building and deploying your model. But it doesn’t stop there — dive into the data. The more familiar you are with the data, the better you will understand the strengths and weaknesses of your model. Take a close look at the evaluations and use them as an opportunity to adjust the data set.</p><p>I hope these articles have helped you find new ways to improve your own machine learning project. And by the way, don’t let the machine do all the learning — as humans, our job is to continue our own learning, so don’t ever stop!</p><p>Thank you for taking this deep dive with me into a data-driven approach to model optimization. I look forward to your feedback and how you can apply this to your own application.<a href=\"https://medium.com/tag/machine-learning?source=post_page-----7f530bc91383---------------------------------------\"></a></p>","contentLength":9124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Use language embeddings for zero-shot classification and semantic search with Amazon Bedrock","url":"https://aws.amazon.com/blogs/machine-learning/use-language-embeddings-for-zero-shot-classification-and-semantic-search-with-amazon-bedrock/","date":1739480012,"author":"Tom Rogers","guid":131,"unread":true,"content":"<p>In this post, we discuss what embeddings are, show how to practically use language embeddings, and explore how to use them to add functionality such as zero-shot classification and semantic search. We then use <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> and language embeddings to add these features to a <a href=\"https://en.wikipedia.org/wiki/RSS\" target=\"_blank\" rel=\"noopener\">really simple syndication (RSS)</a> aggregator application.</p><p>Amazon Bedrock is a fully managed service that makes <a href=\"https://aws.amazon.com/what-is/foundation-models/\" target=\"_blank\" rel=\"noopener\">foundation models</a> (FMs) from leading AI startups and Amazon available through an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. Amazon Bedrock offers a serverless experience, so you can get started quickly, privately customize FMs with your own data, and integrate and deploy them into your applications using <a href=\"https://aws.amazon.com/\" target=\"_blank\" rel=\"noopener\">Amazon Web Services</a> (AWS) services without having to manage infrastructure. For this post, we use the <a href=\"https://aws.amazon.com/bedrock/cohere-command-embed/\" target=\"_blank\" rel=\"noopener\">Cohere v3 Embed</a> model on Amazon Bedrock to create our language embeddings.</p><p>To demonstrate some of the possible uses of these language embeddings, we developed an RSS aggregator website. RSS is a web feed that allows publications to publish updates in a standardized, computer-readable way. On our website, users can subscribe to an RSS feed and have an aggregated, categorized list of the new articles. We use embeddings to add the following functionalities:</p><p>This post uses this application as a reference point to discuss the technical implementation of the semantic search and zero-shot classification features.</p><p>This solution uses the following services:</p><p>The following diagram illustrates the solution architecture.</p><p>This section offers a quick primer on what embeddings are and how they can be used.</p><p>Embeddings are numerical representations of concepts or objects, such as language or images. In this post, we discuss language embeddings. By reducing these concepts to numerical representations, we can then use them in a way that a computer can understand and operate on.</p><p>Let’s take Berlin and Paris as an example. As humans, we understand the conceptual links between these two words. Berlin and Paris are both cities, they’re capitals of their respective countries, and they’re both in Europe. We understand their conceptual similarities almost instinctively, because we can create a model of the world in our head. However, computers have no built-in way of representing these concepts.</p><p>To represent these concepts in a way a computer can understand, we convert them into language embeddings. Language embeddings are high dimensional vectors that learn their relationships with each other through the training of a neural network. During training, the neural network is exposed to enormous amounts of text and learns patterns based on how words are colocated and relate to each other in different contexts.</p><p>Embedding vectors allow computers to model the world from language. For instance, if we embed “Berlin” and “Paris,” we can now perform mathematical operations on these embeddings. We can then observe some fairly interesting relationships. For instance, we could do the following: Paris – France + Germany ~= Berlin. This is because the embeddings capture the relationships between the words “Paris” and “France” and between “Germany” and “Berlin”—specifically, that Paris and Berlin are both capital cities of their respective countries.</p><p>The following graph shows the word vector distance between countries and their respective capitals.</p><p>Subtracting “France” from “Paris” removes the country semantics, leaving a vector representing the concept of a capital city. Adding “Germany” to this vector, we are left with something closely resembling “Berlin,” the capital of Germany. The vectors for this relationship are shown in the following graph.</p><p>For our use case, we use the pre-trained Cohere Embeddings model in Amazon Bedrock, which embeds entire texts rather than a single word. The embeddings represent the meaning of the text and can be operated on using mathematical operations. This property can be useful to map relationships such as similarity between texts.</p><p>One way in which we use language embeddings is by using their properties to calculate how similar an article is to one of the topics.</p><p>To do this, we break down a topic into a series of different and related embeddings. For instance, for culture, we have a set of embeddings for sports, TV programs, music, books, and so on. We then embed the incoming title and description of the RSS articles, and calculate the similarity against the topic embeddings. From this, we can assign topic labels to an article.</p><p>The following figure illustrates how this works. The embeddings that Cohere generates are highly dimensional, containing 1,024 values (or dimensions). However, to demonstrate how this system works, we use an algorithm designed to reduce the dimensionality of the embeddings, <a href=\"https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\" target=\"_blank\" rel=\"noopener\">t-distributed Stochastic Neighbor Embedding (t-SNE)</a>, so that we can view them in two dimensions. The following image uses these embeddings to visualize how topics are clustered based on similarity and meaning.</p><p>You can use the embedding of an article and check the similarity of the article against the preceding embeddings. You can then say that if an article is clustered closely to one of these embeddings, it can be classified with the associated topic.</p><p>This is the <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" target=\"_blank\" rel=\"noopener\">k-nearest neighbor (k-NN) algorithm</a>. This algorithm is used to perform classification and regression tasks. In k-NN, you can make assumptions around a data point based on its proximity to other data points. For instance, you can say that an article that has proximity to the music topic shown in the preceding diagram can be tagged with the culture topic.</p><p>The following figure demonstrates this with an <a href=\"https://arstechnica.com/science/2024/07/the-climate-is-changing-so-fast-that-we-havent-seen-how-bad-extreme-weather-could-get/\" target=\"_blank\" rel=\"noopener\">ArsTechnica article</a>. We plot against the embedding of an article’s title and description: (The climate is changing so fast that we haven’t seen how bad extreme weather could get: Decades-old statistics no longer represent what is possible in the present day).</p><p>The advantage of this approach is that you can add custom, user-generated topics. You can create a topic by first creating a series of embeddings of conceptually related items. For instance, an AI topic would be similar to the embeddings for AI, Generative AI, LLM, and Anthropic, as shown in the following screenshot.</p><p>In a traditional classification system, we’d be required to train a classifier—a supervised learning task where we’d need to provide a series of examples to establish whether an article belongs to its respective topic. Doing so can be quite an intensive task, requiring labeled data and training the model. For our use case, we can provide examples, create a cluster, and tag articles without having to provide labeled examples or train additional models. This is shown in the following screenshot of results page of our website.</p><p>In our application, we ingest new articles on a schedule. We use EventBridge schedules to periodically call a Lambda function, which checks if there are new articles. If there are, it creates an embedding from them using Amazon Bedrock and Cohere.</p><p>We calculate the article’s distance to the different topic embeddings, and can then determine whether the article belongs to that category. This is done with Aurora PostgreSQL with pgvector. We store the embeddings of the topics and then calculate their distance using the following SQL query:</p><div><pre><code>const topics = await sqlClient.then(it=&gt; it.query(\n    `SELECT name, embedding_description, similarity\n     FROM (SELECT topic_id as name, embedding_description, (1- ABS( 1 –(embed.embedding &lt;-&gt; $1))) AS \"similarity\" FROM topic_embedding_link embed)  topics\n     ORDER BY similarity desc`,\n    [toSql(articleEmbedding)]\n  ))\n</code></pre></div><p>The &lt;-&gt; operator in the preceding code calculates the Euclidean distance between the article and the topic embedding. This number allows us to understand how close an article is to one of the topics. We can then determine the appropriateness of a topic based on this ranking.</p><p>We then tag the article with the topic. We do this so that the subsequent request for a topic is as computationally as light as possible; we do a simple join rather than calculating the Euclidean distance.</p><div><div><pre><code>const formattedTopicInsert = pgformat(\n    `INSERT INTO feed_article_topic_link(topic_id, feed_article_id) VALUES %L ON CONFLICT DO NOTHING`,\n    topicLinks\n  )</code></pre></div></div><p>We also cache a specific topic/feed combination because these are calculated hourly and aren’t expected to change in the interim.</p><p>As previously discussed, the embeddings produced by Cohere contain a multitude of features; they embed the meanings and semantics of a word of phrase. We’ve also found that we can perform mathematical operations on these embeddings to do things such as calculate the similarity between two phrases or words.</p><p>We can use these embeddings and calculate the similarity between a search term and an embedding of an article with the k-NN algorithm to find articles that have similar semantics and meanings to the search term we’ve provided.</p><p>For example, in one of our RSS feeds, we have a lot of different articles that rate products. In a traditional search system, we’d rely on keyword matches to provide relevant results. Although it might be simple to find a specific article (for example, by searching “best digital notebooks”), we would need a different method to capture multiple product list articles.</p><p>In a semantic search system, we first transform the term “Product list” in an embedding. We can then use the properties of this embedding to perform a search within our embedding space. Using the k-NN algorithm, we can find articles that are semantically similar. As shown in the following screenshot, despite not containing the text “Product list” in either the title or description, we’ve been able to find articles that contain a product list. This is because we were able to capture the semantics of the query and match it to the existing embeddings we have for each article.</p><p>In our application, we store these embeddings using pgvector on Aurora PostgreSQL. pgvector is an open source extension that enables vector similarity search in PostgreSQL. We transform our search term into an embedding using Amazon Bedrock and Cohere v3 Embed.</p><p>After we’ve converted the search term to an embedding, we can compare it with the embeddings on the article that have been saved during the ingestion process. We can then use pgvector to find articles that are clustered together. The SQL code for that is as follows:</p><div><pre><code>SELECT *\nFROM (\n    SELECT feed_articles.id as id, title, feed_articles.feed_id as feed, feedName, slug, description, url, author, image, published_at as published, 1 - ABS(1 - (embedding &lt;-&gt; $2)) AS \"similarity\"\n    FROM feed_articles\n    INNER JOIN (select feed_id, name as feedName from feed_user_subscription fus where fus.user_id=$1) sub on feed_articles.feed_id=sub.feed_id\n    ${feedId != undefined ? `WHERE feed_articles.feed_id = $4` : \"\"}\n)\nWHERE similarity &gt; 0.95\nORDER BY similarity desc\nLIMIT $3;\n</code></pre></div><p>This code calculates the distance between the topics, and the embedding of this article as “similarity.” If this distance is close, then we can assume that the topic of the article is related, and we therefore attach the topic to the article.</p><p>To deploy this application in your own account, you need the following prerequisites:</p><ul><li>Model access for Cohere Embed English. On the Amazon Bedrock console, choose  in the navigation pane, then choose . Select the FMs of your choice and request access.</li></ul><p>When the prerequisite steps are complete, you’re ready to set up the solution:</p><ol start=\"2\"><li>Navigate to the solution directory:</li></ol><ol start=\"3\"><li>In your terminal, export your AWS credentials for a role or user in ACCOUNT_ID. The role needs to have all necessary permissions for AWS CDK deployment: \n  <ul><li><strong>export AWS_REGION=”&lt;region&gt;”</strong> – The <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\" target=\"_blank\" rel=\"noopener\">AWS Region</a> you want to deploy the application to</li><li><strong>export AWS_ACCESS_KEY_ID=”&lt;access-key&gt;”</strong> – The access key of your role or user</li><li><strong>export AWS_SECRET_ACCESS_KEY=”&lt;secret-key&gt;”</strong> – The secret key of your role or user</li></ul></li></ol><ol start=\"4\"><li>If you’re deploying the AWS CDK for the first time, run the following command:</li></ol><ol start=\"5\"><li>To synthesize the <a href=\"http://aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a> template, run the following command:<code>cdk synth -c vpc_id=&lt;ID Of your VPC&gt;</code></li></ol><ol start=\"6\"><li>To deploy, use the following command:<code>cdk deploy -c vpc_id=&lt;ID Of your VPC&gt;</code></li></ol><p>When deployment is finished, you can check these deployed stacks by visiting the AWS CloudFormation console, as shown in the following screenshot.</p><p>Run the following command in the terminal to delete the CloudFormation stack provisioned using the AWS CDK:</p><p>In this post, we explored what language embeddings are and how they can be used to enhance your application. We’ve learned how, by using the properties of embeddings, we can implement a real-time zero-shot classifier and can add powerful features such as semantic search.</p><p>The code for this application can be found on the accompanying <a href=\"https://github.com/aws-samples/rss-aggregator-using-cohere-embeddings-bedrock\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>. We encourage you to experiment with language embeddings and find out what powerful features they can enable for your applications!</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/ML-17434-About-Author.jpeg\" alt=\"About the Author\" width=\"100\" height=\"133\">is a Solutions Architect based in Amsterdam, the Netherlands. He has a background in software engineering. At AWS, Thomas helps customers build cloud solutions, focusing on modernization, data, and integrations.</p>","contentLength":13247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learnings from a Machine Learning Engineer — Part 2: The Data Sets","url":"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-2-the-data-sets/","date":1739478579,"author":"David Martin","guid":2,"unread":true,"content":"<p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, we discussed the importance of collecting good image data and assigning proper labels for your <a href=\"https://towardsdatascience.com/tag/image-classification/\" title=\"Image Classification\">Image Classification</a> project to be successful. Also, we talked about classes and sub-classes of your data. These may seem pretty straight forward concepts, but it’s important to have a solid understanding going forward. So, if you haven’t, please check it out.</p><p>Now we will discuss how to build the various data sets and the techniques that have worked well for my application. Then in the <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">next part</a>, we will dive into the evaluation of your models, beyond simple accuracy.</p><p>I will again use the example zoo animals image classification app.</p><p>As machine learning engineers, we are all familiar with the train-validation-test sets, but when we include the concept of sub-classes discussed in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-1-the-data/\">Part 1</a>, and incorporate to concepts discussed below to set a minimum and maximum image count per class, as well as staged and synthetic data to the mix, the process gets a bit more complicated. I had to create a custom script to handle these options.</p><p>I will walk you through these concepts before we split the data for training:</p><ul><li>&nbsp;— Too few images and your model performance will suffer. Too many and you spend more time training than it’s worth.</li><li>&nbsp;— Your model indicates how confident it is in the predictions. Let’s use that to decide when to present results to the user.</li><li>&nbsp;— Real-world data is messy and the benchmark sets should reflect that. These need to stretch the model to the limit and help us decide when it is ready for production.</li><li><strong>Staged and synthetic data</strong>&nbsp;— Real-world data is king, but sometimes you need to produce the your own or even generate data to get off the ground. Be careful it doesn’t hurt performance.</li><li>&nbsp;— Repeat data can skew your results and give you a false sense of performance. Make sure your data is diverse.</li><li>&nbsp;— Combine sub-classes, apply cutoffs, and create your train-validation-test sets. Now we are ready to get the show started.</li></ul><p>In my experience, using a minimum of 40 images per class provides descent performance. Since I like to use 10% each for the test set and validation set, that means at least 4 images will be used to check the training set, which feels just barely adequate. Using fewer than 40 images per class, I notice my model evaluation tends to suffer.</p><p>On the other end, I set a maximum of about 125 images per class. I have found that the performance gains tend to plateau beyond this, so having more data will slow down the training run with little to show for it. Having more than the maximum is fine, and these “overflow” can be added to the test set, so they don’t go to waste.</p><p>There are times when I will drop the minimum cutoff to, say 35, with no intention of moving the trained model to production. Instead, the purpose is to leverage this throw-away model to find more images from my unlabelled set. This is a technique that I will go into more detail in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>You are likely familiar with the softmax score. As a reminder, softmax is the probability assigned to each label. I like to think of it as a confidence score, and we are interested in the class that receives the highest confidence. Softmax is a value between zero and one, but I find it easier to interpret confidence scores between zero and 100, like a percentage.</p><p>In order to decide if the model is confident enough with its prediction, I have chosen a threshold of 95. I use this threshold when determining if I want to present results to the user.</p><p>Scores above the threshold have a better changes of being right, so I can confidently provide the results. Scores below the threshold may not be right — in fact it could be “out-of-scope”, meaning it’s something the model doesn’t know how to identify. So, instead of taking the risk of presenting incorrect results, I instead prompt the user to try again and offer suggestions on how to take a “good” picture.</p><p>Admittedly this is somewhat arbitrary cutoff, and you should decide for your use-case what is appropriate. In fact, this score could probably be adjusted for each trained model, but this would make it harder to compare performance across models.</p><p>I will refer to this confidence score frequently in the evaluations section in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>Let me introduce what I call the benchmark sets, which you can think of as extended test sets. These are hand-picked images designed to stretch the limits of your model, and provide a measure for specific classes of your data. Use these benchmarks to justify moving your model to production, and for an objective measure to show to your manager.</p><ul><li>&nbsp;— These are the “extra credit” images, like the bonus questions a professor would add to the quiz to see which students are paying attention. You need a keen eye to spot the difference between the ground truth and a similar looking class. For example, a cheetah sleeping in the shade that could pass as a leopard if you don’t look closely.</li><li>&nbsp;— These are the “trick question” images. Our model is trained on zoo animals, but people are known for not following the rules. For example, a zoo guest takes a picture of their child wearing cheetah face paint.</li><li>&nbsp;— These are your “bread and butter” classes that need to get near perfect scores and zero errors. This would be a make-or-break benchmark for moving to production.</li><li>&nbsp;— These are your “rare but exceptional” classes that again need to be correct, but reach a minimum score like the confidence threshold.</li></ul><p>When looking for images to add to the benchmarks, you can likely find them in real-world images from your deployed model. See the evaluation in <a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>.</p><p>For each benchmark, calculate the min, max, median, and mean scores, and also how many images get scores above and below the confidence threshold. Now you can compare these measures against what is currently in production, and against your minimum requirements, to help decide if the new model is production worthy.</p><p>Perhaps the biggest hurdle to any supervised machine learning application is having data to train the model. Clearly, “real-world” data that comes from actual users of the application is ideal. However you can’t really collect these until the model is deployed. Chicken and egg problem.</p><p>One way to get started to is to have volunteers collect “staged” images for you, trying to act like real users. So, let’s have our zoo staff go around taking pictures of the animals. This is a good start, but there will be a certain level of bias introduced in these images. For example, the staff may take the photos over a few days, so you may not get the year-round weather conditions.</p><p>Another way to get pictures is use computer-generated “synthetic” images. I would avoid these at all costs, to be honest. Based on my experience, the model struggles with these because they look…different. The lighting is not natural, the subject may superimposed on a background and so the edges look too sharp, etc. Granted, some of the AI generated images look very realistic, but if you look closely you may spot something unusual. The neural network in your model will notice these, so be careful.</p><p>The way that I handle these staged or synthetic images is as a sub-class that gets merged into the training set, but only&nbsp;&nbsp;giving preference to the real-world images. I cap the number of staged images to 60, so if I have 10 real-world, I now only need 50 staged. Eventually, these staged and synthetic images are phased out completely, and I rely entirely on real-world.</p><p>One problem that can creep into your image set are duplicate images. These can be exact copies of pictures, or they can be extremely similar. You may think that this is harmless, but imagine having 100 pictures of an elephant that are exactly the same — your model will not know what to do with a different angle of the elephant.</p><p>Now, let’s say you have only&nbsp;&nbsp;pictures that are nearly the same. Not so bad, right? Well, here is what can happen to them:</p><ul><li>Both pictures go in the training set — The model doesn’t learn anything from the repeated image and it wastes time processing them.</li><li>One goes into the training set, the other goes into the test set — Your test score will be higher, but it is not an accurate evaluation.</li><li>Both are in the test set — Your test score will be compounded either higher or lower than it should be.</li></ul><p>None of these will help your model.</p><p>There are a few ways to find duplicates. The approach I have taken is to calculate a hamming distance on all the pictures and identify the ones that are very close. I have an interface that displays the duplicates and I decide which one I like best, and remove the other.</p><p>Another way (I haven’t tried this yet) is to create a vector representation of your images. Store these a vector database, and you can do a similarity search to find nearly identical images.</p><p>Whatever method you use, it is important to clean up the duplicates.</p><p>Now we are ready to build the traditional training, validation, and test sets. This is no longer a straight forward task since I want to:</p><ol><li>Merge sub-classes into a main class.</li><li>Prioritize real-world images over staged or synthetic images.</li><li>Apply a minimum number of images per class.</li><li>Apply a maximum number of images per class, sending the “overflow” to the test set.</li></ol><p>This process is somewhat complicated and depends on how you manage your image library. First, I would recommend keeping your images in a folder structure that has sub-class folders. You can get image counts by using a script to simply read the folders. Second is to keep a configuration of how the sub-classes are merged. To really set yourself up for success, put these image counts and merge rules in a database for faster lookups.</p><p>My train-validation-test set splits are usually 90–10–0. I originally started out using 80–10–10, but with diligence on keeping the entire data set clean, I noticed validation and test scores became pretty even. This allowed me to increase the training set size, and use “overflow” to become the test set, as well as using the benchmark sets.</p><p>In this part, we’ve built our data sets by merging sub-classes and using the image count cutoffs. Plus we handle staged and synthetic data as well as cleaning up duplicate images. We also created benchmark sets and defined confidence thresholds, which help us decide when to move a model to production.</p><p>In&nbsp;<a href=\"https://towardsdatascience.com/learnings-from-a-machine-learning-engineer-part-3-the-evaluation/\">Part 3</a>, we will discuss how we are going to evaluate the different model performances. And then finally we will get to the actual model training and the techniques to enhance accuracy.<a href=\"https://medium.com/tag/machine-learning?source=post_page-----1948daf60483---------------------------------------\"></a></p>","contentLength":10545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python vs R for data science: Which should you choose?","url":"https://www.datasciencecentral.com/python-vs-r-for-data-science-which-should-you-choose/","date":1739461072,"author":"Mike Steven","guid":62,"unread":true,"content":"<p>Welcome to another comparison article where you will understand the features, intricacies, pros, and cons of two different stacks of the information technology industry. Today’s comparison blog is especially for data scientists who spend their day and night with datasets, insights, trends, and analysis of many other factors. From a long list of skills that…&nbsp;<a href=\"https://www.datasciencecentral.com/python-vs-r-for-data-science-which-should-you-choose/\" rel=\"bookmark\">Read More »</a></p>","contentLength":377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging the Gap: Democratizing AI for All","url":"https://www.kdnuggets.com/bridging-gap-democratizing-ai","date":1739458803,"author":"Vidhi Chugh","guid":251,"unread":true,"content":"<article>Let’s explore how democratizing AI can level the playing field and create opportunities for all, no matter the background or resources.</article>","contentLength":137,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/chugh_Bridging-the-Gap-Democratizing-AI-for-All_1.png","enclosureMime":"","commentsUrl":null},{"title":"How to Scale Sklearn with Dask","url":"https://www.kdnuggets.com/how-to-scale-sklearn-dask","date":1739451649,"author":"Iván Palomares Carrascosa","guid":250,"unread":true,"content":"<article>Here's how Dask applies the building blocks of sklearn to bring ML modeling workflows to the next level of scalability via high-performance parallel computing</article>","contentLength":158,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/crVYYGyvTE2Jilsmvukhpw.jpeg","enclosureMime":"","commentsUrl":null},{"title":"Precision agriculture powered by AI for climate-resilient crops","url":"https://www.datasciencecentral.com/precision-agriculture-powered-by-ai-for-climate-resilient-crops/","date":1739395550,"author":"Shanthababu Pandian","guid":61,"unread":true,"content":"<p>AI in Agriculture Precision Farming AI-Powered Agriculture Climate-Resilient Crops\nSustainable Farming Practices AI for Pest Control AI for Soil Analysis Machine Learning in Agriculture Smart Farming Solutions IoT in Agriculture Crop Monitoring with AI<p>\nPredictive Analytics in Farming AI for Weather Prediction in Agriculture</p>\nAI-Driven Precision Irrigation AI in Fertilization Optimization Sustainable Agriculture Technology Advanced Farming Techniques Agriculture Data Analysis with AI<p>\nAI-Powered Smart Irrigation Agricultural Innovation with AI</p></p>","contentLength":546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fine-tune LLMs with synthetic data for context-based Q&A using Amazon Bedrock","url":"https://aws.amazon.com/blogs/machine-learning/fine-tune-llms-with-synthetic-data-for-context-based-qa-using-amazon-bedrock/","date":1739382250,"author":"Sue Cha","guid":130,"unread":true,"content":"<p>There’s a growing demand from customers to incorporate generative AI into their businesses. Many use cases involve using pre-trained large language models (LLMs) through approaches like Retrieval Augmented Generation (RAG). However, for advanced, domain-specific tasks or those requiring specific formats, model customization techniques such as fine-tuning are sometimes necessary. <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> provides you with the ability to customize leading foundation models (FMs) such as Anthropic’s Claude 3 Haiku and Meta’s Llama 3.1.</p><p>Amazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available through an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case. Amazon Bedrock offers a serverless experience, so you can get started quickly, privately customize FMs with your own data, and integrate and deploy them into your applications using AWS tools without having to manage any infrastructure.</p><p>Fine-tuning is a supervised training process where labeled prompt and response pairs are used to further train a pre-trained model to improve its performance for a particular use case. One consistent pain point of fine-tuning is the lack of data to effectively customize these models. Gathering relevant data is difficult, and maintaining its quality is another hurdle. Furthermore, fine-tuning LLMs requires substantial resource commitment. In such scenarios, synthetic data generation offers a promising solution. You can create synthetic training data using a larger language model and use it to fine-tune a smaller model, which has the benefit of a quicker turnaround time.</p><p>In this post, we explore how to use Amazon Bedrock to generate synthetic training data to fine-tune an LLM. Additionally, we provide concrete evaluation results that showcase the power of synthetic data in fine-tuning when data is scarce.</p><p>The solution comprises two main steps:</p><ol><li>Generate synthetic data using the Amazon Bedrock InvokeModel API.</li><li>Fine-tune using an Amazon Bedrock custom model.</li></ol><p>For synthetic data generation, we use a larger language model (such as <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropic’s Claude 3 Sonnet on Amazon Bedrock</a>) as the teacher model, and a smaller language model (such as Anthropic’s Claude Instant 1.2 or Claude 3 Haiku on Amazon Bedrock) as the student model for fine-tuning. We use the larger teacher model to generate new data based on its knowledge, which is then used to train the smaller student model. This concept is similar to knowledge distillation used in deep learning, except that we’re using the teacher model to generate a new dataset from its knowledge rather than directly modifying the architecture of the student model.</p><p>The following diagram illustrates the overall flow of the solution.</p><p>Finally, we share our experiment results, where we compare the performance of the model fine-tuned with synthetic data to the baseline (not fine-tuned) model and to a model fine-tuned with an equal amount of original training data.</p><p>To generate synthetic data and fine-tune models using Amazon Bedrock, you first need to create an <a href=\"https://aws.amazon.com/iam/\" target=\"_blank\" rel=\"noopener\">AWS Identity and Access Management (IAM)</a> service role with the appropriate permissions. This role is used by Amazon Bedrock to access the necessary resources on your behalf.</p><p>If you’re running this code using an <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a> notebook instance, edit the IAM role that’s attached to the notebook (for example, AmazonSageMaker-ExecutionRole-XXX) instead of creating a new role. Follow <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-iam-role.html\" target=\"_blank\" rel=\"noopener\">Create a service role for model customization</a> to modify the trust relationship and add the S3 bucket permission. Additionally, on the role’s tab, create the following inline policies:</p><ol><li>Policy name: bedrock-customization</li></ol><div><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel\",\n                \"bedrock:ListModelCustomizationJobs\",\n                \"bedrock:DeleteCustomModel\",\n                \"bedrock:CreateModelCustomizationJob\",\n                \"bedrock:StopModelCustomizationJob\",\n                \"bedrock:ListCustomModels\",\n                \"bedrock:GetCustomModel\",\n                \"bedrock:GetModelCustomizationJob\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre></div><ol start=\"2\"><li>Policy name: iam-pass-role</li></ol><div><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": [\n                \"${sagemaker-execution-role-arn}\"\n            ]\n        }\n    ]\n}</code></pre></div><p>The final permission policies for the SageMaker execution role should look like the following, which include AmazonSageMaker-ExecutionPolicy, AmazonSageMakerFullAccess, bedrock-customization, and iam-pass-role.</p><h2>Generate synthetic data using the Amazon Bedrock InvokeModel API</h2><p>We use the <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock InvokeModel API</a> to generate synthetic data for fine-tuning. You can use the API to programmatically send an inference (text generation) request to the model of your choice. All you need is a well-crafted prompt tailored for data synthesis. We used the following sample prompt for our use case:</p><div><pre><code>PROMPT = \"\"\"\nYou are an AI assistant who is an expert in Amazon services. Your task is to understand a system that takes in a list of documents, and based on that, answers a question by providing citations for the documents that it referred the answer from.\n\nYour job is to generate three new Question/Answer pairs, emulating the tone, style, and grammar of the original data provided.\n\nHere is the original data :\nInput Documents and Question : {document}\\n\\nQuestion: {question}\nOutput Answer : {answer}\n\nStrictly return a jsonl with the keys (question, answer, topic). Every topic should be different. The answers should be in the exact same format as the original. The question and the answer should be different in content from the original data provided, and all questions should be diverse and different from each other. Do not answer in any other format. The response should be parsable as a jsonl.\n\"\"\"</code></pre></div><p>The goal of our use case was to fine-tune a model to generate a relevant and coherent answer based on a given reference document and a question. RAG is a popular technique used for such Q&amp;A tasks; however, one significant challenge with RAG is the potential for retrieving unrelated or irrelevant documents, which can lead to inaccurate responses. You can apply fine-tuning to guide the model to better focus on the relevance of the documents to the question instead of using the provided documents without context to answer the question.</p><p>Our dataset includes Q&amp;A pairs with reference documents regarding AWS services. Each sample has up to five reference documents as context, and a single-line question follows. The following table shows an example.</p><table border=\"1px\" cellpadding=\"10px\"><tbody><tr><td width=\"677\"><p>Step 1: Prepare to work with AWS CodeStar projects</p><p>In this step, you create an AWS CodeStar service role and an Amazon EC2 key pair, so that you can begin creating and working with AWS CodeStar projects. If you have used AWS CodeStar before, skip ahead to Step 2</p><p>Step 2: Create a Project in AWS CodeStar.</p><p>For this step, follow the instructions in Setting Up AWS CodeStar in the AWS CodeStar User Guide. Do not create a new AWS account, IAM user, or IAM group as part of those instructions. Use the ones you created or identified in Team Setup for AWS Cloud9. When you finish following those instructions, return to this topic.</p><p>Before you can start using AWS CodeStar, you must complete the following steps.</p><p>Step 1: Create an account</p><p>Step 2: Create the AWS CodeStar Service Role</p><p>Step 3: Configure the User’s IAM Permissions</p><p>Step 4: Create an Amazon EC2 Key Pair for AWS CodeStar Projects</p><p>Step 5: Open the AWS CodeStar Console</p><p>How Do I Get Started with AWS CodeStar?</p><p>To get started with AWS CodeStar:</p><p>Prepare to use AWS CodeStar by following the steps in Setting Up AWS CodeStar.</p><p>Experiment with AWS CodeStar by following the steps in the Getting Started with AWS CodeStar tutorial.</p><p>Share your project with other developers by following the steps in Add Team Members to an AWS CodeStar Project.</p><p>Integrate your favorite IDE by following the steps in Use an IDE with AWS CodeStar.</p><p>Step 2: Create a project in AWS CodeStar</p><p>In this step, you create a project in AWS CodeStar.</p><p>If you already have a project in AWS CodeStar you want to use, skip ahead to Step 3: Create an AWS Cloud9 Development Environment and Connect It to the Project.</p><p>For this step, follow the instructions in Create a Project in AWS CodeStar in the AWS CodeStar User Guide. In the AWS CodeStar create project wizard, when you get to the Setup tools page or Connect to your source repository page, choose Skip, and then return to this topic.</p><p>Create a Project in AWS CodeStar</p><p>You use the AWS CodeStar console to create a project. If you use a project template, it sets up the required resources for you. The template also includes sample code that you can use to start coding.</p><p>To create a project, sign in to the AWS Management Console with an IAM user that has the AWSCodeStarFullAccess policy or equivalent permissions. For more information, see Setting Up AWS CodeStar.</p><p>Note: You must complete the steps in Setting Up AWS CodeStar before you can complete the procedures in this topic.</p><p>Create a Project in AWS CodeStar (Console)</p><p>Create a Project in AWS CodeStar (AWS CLI)</p><p>Answer the following question to the best of your ability. Use and paraphrase the information in the context, if it answers the question. Otherwise, rely on your knowledge and memory.</p></td></tr><tr><td width=\"677\">How do I get started with AWS CodeStar?</td></tr><tr><td width=\"677\"><p>To get started with AWS CodeStar, follow these steps:</p><p>1.&nbsp;Prepare to work with AWS CodeStar projects:</p><p>a. Create an AWS CodeStar service role by following the instructions in “Setting Up AWS CodeStar” (Document 2) in the AWS CodeStar User Guide [1].</p><p>b. Create an Amazon EC2 key pair for AWS CodeStar projects, as described in the same guide [1].</p><p>2.&nbsp;Create a project in AWS CodeStar:</p><p>a. Sign in to the AWS Management Console with an IAM user that has the AWSCodeStarFullAccess policy or equivalent permissions [2].</p><p>b. In the AWS CodeStar console, select one of the available project templates, which will automatically provision all the required resources for your project [3].</p><p>c. Once your project is provisioned, you can see your running application from the Application endpoints tile [3].</p><p>3.&nbsp;Connect to the AWS CodeCommit source repository for your project:</p><p>a. Use the steps provided in the CodeStar console to connect to the AWS CodeCommit source repository for your project [3].</p><p>b. Begin coding and making changes to your project\\’s source code.</p><p>4.&nbsp;Track and manage changes in the release process:</p><p>a. Use the project dashboard to track and manage changes in the release process [3].</p><p>b. View the most recent project activity on the dashboard.</p><p>5.&nbsp;Share your project with other developers:</p><p>a. Follow the steps in “Add Team Members to an AWS CodeStar Project” to share your project with other developers [3].</p><p>6.&nbsp;Integrate your favorite IDE:</p><p>a. Follow the steps in “Use an IDE with AWS CodeStar” to integrate your preferred Integrated Development Environment (IDE) with AWS CodeStar [3].</p><p>[1] https://docs.aws.amazon.com/codestar/latest/userguide/setting-up.html</p><p>[2] https://docs.aws.amazon.com/codestar/latest/userguide/create-project.html</p><p>[3] https://docs.aws.amazon.com/codestar/latest/userguide/getting-started.html</p></td></tr></tbody></table><p>For data synthesis, we asked the model to generate three new Q&amp;A pairs per reference document. However, you can adjust the number as needed. The crucial part is to make the model think deeply about a variety of topics. Because the purpose of generating synthetic data is to enrich the training dataset, it’s more beneficial to have the model look at different parts of the documents and create Q&amp;A pairs with different topics than the original.</p><p>The following example shows how to generate synthetic data with the Amazon Bedrock InvokeModel API. We tested the preceding prompt with Anthropic’s Claude 3 Sonnet. If you want to test a different model, retrieve the corresponding model ID from <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock model IDs</a>, and replace the modelId variable in the function.</p><div><pre><code>import boto3\nimport json\n\nbedrock = boto3.client(service_name=\"bedrock-runtime\")\n\ndef generate_synthetic_data(document, question, answer):\n    \n    values = {\n        \"document\": document,\n        \"question\": question,\n        \"answer\": answer\n    }\n    \n    body = {\n        \"messages\": [{\n            \"role\": \"user\", \"content\": PROMPT.format(**values)\n        }],\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"max_tokens\": 2048,\n        \"temperature\" : 0.5\n    }\n    \n    response = bedrock.invoke_model(\n        body=json.dumps(body),\n        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n    \n    response_body = json.loads(response.get('body').read())\n    \n    return response_body['content'][0]['text']\n</code></pre></div><p>The preceding function returns three JSONL records in strings with question, answer, and topic as keys. The following parse_llm_output function loads the strings and uses regular expressions to retrieve the generated questions and answers. Then, the create_synthetic_samples function combines those two functionalities to produce the final synthetic training samples.</p><div><pre><code>import re\nimport pd\n\ndef parse_llm_output(jsonl_string):\n    \n    question_pattern = re.compile(r'\"question\":\\s*\"([^\"]+)\"')\n    answer_pattern = re.compile(r'\"answer\":\\s*\"(.*?)\"\\s*,\\s*\"topic\"') \n    questions = question_pattern.findall(jsonl_string)\n    answers = answer_pattern.findall(jsonl_string)\n    \n    return questions, answers\n\n\ndef create_synthetic_samples(row: pd.Series) -&gt; pd.DataFrame:\n\n    jsonl_string = generate_synthetic_data(row['document'], row['question'], row['answer'])\n    questions, answers = parse_llm_output(jsonl_string)\n    \n    return pd.DataFrame({\n        \"document\": [row['document']] * len(questions),\n        \"question\": questions,\n        \"answer\": answers\n    })\n\n\ndef to_customization_format(row):\n\n    msg = {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": f\"{row['document']}\\n\\nQuestion: {row['question']}\"},\n            {\"role\": \"assistant\", \"content\": row['answer']}\n        ]\n    }\n    \n    return msg\n</code></pre></div><p>The following script combines all of the preceding functions and gives you the final training set with both original and synthetic samples. We convert the samples into the format required by the customization job using the to_customization_format function and save them as train.jsonl. Assume the input data is a CSV file with three columns: document, question, and answer.</p><div><pre><code>import pandas as pd\n\n# Load original training samples\noriginal_train = pd.read_csv(input_df_path)\n\n# Create synthetic training samples\nsynthetic_train = pd.concat(original_train.apply(create_synthetic_samples, axis=1).tolist())\n\n# Combine original and synthetic samples\nfinal_train_df = pd.concat([original_train, synthetic_train])\n\n# Convert to the format required by the customization job\nfinal_train = final_train_df.apply(to_customization_format, axis=1).tolist()\n\n# Write to JSONL file    \nwith open('train.jsonl', 'w') as file:\n    for item in final_train:\n        json.dump(item, file)\n        file.write('\\n')\n</code></pre></div><h2>Fine-tune using an Amazon Bedrock custom model</h2><p>Now that you have the synthetic data generated by the teacher model along with your original data, it’s time to train the student model. We fine-tune the student model using the Amazon Bedrock custom model functionality.</p><p>Model customization is the process of providing training data to an FM to improve its performance for specific use cases. Amazon Bedrock offers three model customization methods as of this writing:</p><ul></ul><p>You can create your own custom model using any of these methods through the Amazon Bedrock console or API. For more information on supported models and AWS Regions with various customization methods, please see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html\" target=\"_blank\" rel=\"noopener\">User guide for model customization</a>. In this section, we focus on how to fine-tune a model using the API.</p><p>To create a fine-tuning job in Amazon Bedrock, complete the following prerequisite steps:</p><ol><li>Upload the jsonl file to the training data bucket.</li><li>Make sure that you have created an IAM role, as described in the <a href=\"https://aws.amazon.com/blogs/machine-learning/fine-tune-llms-with-synthetic-data-for-context-based-qa-using-amazon-bedrock/#_Prerequisites\" target=\"_blank\" rel=\"noopener\">Prerequisites</a></li></ol><p>When these steps are complete, run the following code to submit a new fine-tuning job. In our use case, the student model was Anthropic’s Claude Instant 1.2. At the time of writing, Anthropic’s Claude 3 Haiku is generally available, and we recommend following the rest of the code using Anthropic’s Claude 3 Haiku. For the release announcement, see <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/11/fine-tuning-anthropics-claude-3-haiku-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">Fine-tuning for Anthropic’s Claude 3 Haiku in Amazon Bedrock is now generally available</a>.</p><p>If you want to try different models, you must check the model provider’s terms of service yourself. Many providers restrict using their models to train competing models. For the latest model support information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/custom-model-supported.html\" target=\"_blank\" rel=\"noopener\">Supported Regions and models for model customization</a>, and replace baseModelIdentifier accordingly. Different models have different hyperparameters. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-hp.html\" target=\"_blank\" rel=\"noopener\">Custom model hyperparameters</a>.</p><div><pre><code>import boto3\nimport json\nimport time\n\nbedrock = boto3.client(service_name='bedrock')\n    \n# Set parameters\ncustomizationType = \"FINE_TUNING\"\nbaseModelIdentifier = \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k\"\nroleArn = \"${customization-role-arn}\"\njobName = \"${customization-job-name}\"\ncustomModelName = \"${customization-model-name}\"\nhyperParameters = {\n    \"epochCount\": \"1\",\n    \"batchSize\": \"96\",\n    \"learningRateMultiplier\": \"0.5\",\n }\ntrainingDataConfig = {\"s3Uri\": \"s3://${training-bucket}/train.jsonl\"}\noutputDataConfig = {\"s3Uri\": \"s3://${output-bucket}/myOutputData\"}\n\n# Create job\nresponse_ft = bedrock.create_model_customization_job(\n    jobName=jobName, \n    customModelName=customModelName,\n    roleArn=roleArn,\n    baseModelIdentifier=baseModelIdentifier,\n    hyperParameters=hyperParameters,\n    trainingDataConfig=trainingDataConfig,\n    outputDataConfig=outputDataConfig\n)\n\njobArn = response_ft.get('jobArn')\n\n# Check job status\nwhile True:\n    status = bedrock.get_model_customization_job(jobIdentifier=jobArn).get('status')\n    if status != 'InProgress':\n        print(status)\n        break\n    else:\n        print(status)\n    time.sleep(30)\n</code></pre></div><p>When the status changes to , your fine-tuned student model is ready for use. To run an inference with this custom model, you need to purchase . A flexible option is available for custom models, which can be turned off when not in use and billed by the hour. A cost estimate is provided on the console prior to purchasing provisioned throughput.</p><p>On the Amazon Bedrock console, choose in the navigation pane. Select the model you fine-tuned and choose <strong>Purchase provisioned throughput</strong>.</p><p>The model name and type are automatically selected for you. Select  for . After you make this selection, the estimated cost is shown. If you’re okay with the pricing, choose .</p><p>When the Provisioned Throughput becomes available, retrieve the ARN of the provisioned custom model and run the inference:</p><div><pre><code>import boto3\nimport json\n\nbedrock = boto3.client(service_name=\"bedrock-runtime\")\n\ndef run_student_model(document, question):\n    \n    values = {\n        \"document\": document,\n        \"question\": question,\n    }\n    \n    body = {\n        \"messages\": [{\n            \"role\": \"user\", \"content\": PROMPT.format(**values)\n        }],\n        \"max_tokens\": 2048,\n        \"temperature\" : 0.5\n    }\n    \n    response = bedrock.invoke_model(\n        body=json.dumps(body),\n        modelId=\"${provisioned_model_arn}\",\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n    \n    response_body = json.loads(response.get('body').read())\n    \n    return response_body['content'][0]['text']\n</code></pre></div><p>In this section, we share our experiment results to provide data points on how the synthetic data generated by a teacher model can improve the performance of a student model. For evaluation methods, we used an LLM-as-a-judge approach, where a judge model compares responses from two different models and picks a better response. Additionally, we conducted a manual evaluation on a small subset to assess whether the LLM-as-a-judge and human judges have aligned preferences.</p><p>We carried out controlled experiments where we compared four different models as follows: 1,500 synthetic training samples for the 4 model were generated by Anthropic’s Claude 3 Sonnet, and we created three synthetic samples per one original reference document (3 samples * 500 original reference documents = 1,500 synthetic samples).</p><table border=\"1px\" cellpadding=\"10px\"><tbody><tr><td width=\"634\">Anthropic’s Claude Instant without any customization</td></tr><tr><td width=\"634\">Anthropic’s Claude Instant fine-tuned with 500 original training samples</td></tr><tr><td width=\"169\"></td><td width=\"634\">Anthropic’s Claude Instant fine-tuned with 2,000 original training samples</td></tr><tr><td width=\"169\"></td><td width=\"634\">Anthropic’s Claude Instant fine-tuned with 500 original training samples plus 1,500 synthetic training samples</td></tr></tbody></table><p>LLM output evaluation is an important step in developing generative AI applications, but it is expensive and takes considerable time if done manually. An alternative solution to systematically evaluate output quality in large volume is the LLM-as-a-judge approach, where an LLM is used to evaluate another LLM’s responses.</p><p>For our use case, we used Anthropic’s Claude 3 Sonnet and Meta Llama 3 70B as the judges. We asked the LLM judges to compare outputs from two different models and choose one over the other or state a tie. The following chart summarizes the judges’ decisions. Each number represents the percentage of times when the respective model was selected as providing a better answer, excluding tie cases. The test set contained 343 samples.</p><p>As shown in the preceding chart, the Anthropic’s Claude 3 Sonnet judge preferred the response from the fine-tuned model with synthetic examples over the Anthropic’s Claude Instant base model (84.8% preference) and the fine-tuned model with original 500 samples (72.3% preference). However, the judge concluded that the fine-tuned model with 2,000 original examples was preferred over the fine-tuned model with synthetic examples (32.3% preference). This aligns with the expectation that when large, high-quality original data is available, it’s better to use the large training data that accurately reflects the target data distribution.</p><p>The Meta Llama judge reached a similar conclusion. As shown in the preceding chart, it preferred the response from the fine-tuned model with synthetic samples over the Anthropic’s Claude Instant base model (75.6% preference) and the fine-tuned model with original 500 examples (76.4% preference), but the fine-tuned model with 2,000 original examples was the ultimate winner.</p><p>To complement the LLM-as-a-judge result, we conducted manual evaluation with two human judges. We asked the two human evaluators to perform the same pairwise comparison task as the LLM judge, but for 20 examples. The following chart summarizes the results.</p><p>As shown in the preceding chart, the two human evaluators reached a similar conclusion, reinforcing the LLM-as-a-judge result. The fine-tuned model with synthetic examples produced outputs that were more preferable than the Anthropic’s Claude Instant base model and the fine-tuned model with the original 500 examples; however, it didn’t outperform the fine-tuned model with the 2,000 original examples.</p><p>These comparative evaluation results from both the LLM judges and human judges strongly demonstrate the power and potential of using data synthesis when training data is scarce. Moreover, by using high-quality data from the teacher model, we can effectively train the student model, which is lightweight and cost-effective for deployment in a production environment.</p><h3>Amazon Bedrock evaluations</h3><p>Running LLM-as-a-judge and human evaluation has become much easier with Amazon Bedrock. Model evaluation on Amazon Bedrock allows you to evaluate, compare, and select the best FMs for your use case. Human evaluation workflows can use your own employees or an AWS-managed team as reviewers. For more information on how to set up a human evaluation workflow, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-human.html\" target=\"_blank\" rel=\"noopener\">Creating your first model evaluation that uses human workers</a>. The latest feature, LLM-as-a-judge, is now in preview and allows you to assess multiple quality dimensions including correctness, helpfulness, and responsible AI criteria such as answer refusal and harmfulness. For step-by-step instructions, see <a href=\"https://aws.amazon.com/blogs/aws/new-rag-evaluation-and-llm-as-a-judge-capabilities-in-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">New RAG evaluation and LLM-as-a-judge capabilities in Amazon Bedrock</a>.</p><p>Make sure to delete the following resources to avoid incurring cost:</p><ul><li>Provisioned throughput for the custom model</li><li>The training_bucket and output_bucket S3 buckets</li></ul><p>In this post, we explored how to use Amazon Bedrock to generate synthetic training data using a large teacher language model and fine-tune a smaller student model with synthetic data. We provided instructions on generating synthetic data using the Amazon Bedrock InvokeModel API and fine-tuning the student model using an Amazon Bedrock custom model. Our evaluation results, based on both an LLM-as-a-judge approach and human evaluation, demonstrated the effectiveness of synthetic data in improving the student model’s performance when original training data is limited.</p><p>Although fine-tuning with a large amount of high-quality original data remains the ideal approach, our findings highlight the promising potential of synthetic data generation as a viable solution when dealing with data scarcity. This technique can enable more efficient and cost-effective model customization for domain-specific or specialized use cases.</p><p>If you’re interested in working with the AWS Generative AI Innovation Center and learning more about LLM customization and other generative AI use cases, visit <a href=\"https://aws.amazon.com/ai/generative-ai/innovation-center/\" target=\"_blank\" rel=\"noopener\">Generative AI Innovation Center</a>.</p><p> is a Deep Learning Architect at the AWS Generative AI Innovation Center, where she specializes in model customization and optimization. She has extensive hands-on experience in solving customers’ business use cases by utilizing generative AI as well as traditional AI/ML solutions. Sujeong holds a M.S. degree in Data Science from New York University.</p><p><strong><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/31/Arijit-Intro-2.jpg\" alt=\"\" width=\"100\" height=\"116\">Arijit Ghosh Chowdhury</strong> is a Scientist with the AWS Generative AI Innovation Center, where he works on model customization and optimization. In his role, he works on applied research in fine-tuning and model evaluations to enable GenAI for various industries. He has a Master’s degree in Computer Science from the University of Illinois at Urbana Champaign, where his research focused on question answering, search and domain adaptation.</p><p> is a Senior Applied Scientist at Amazon Generative AI Innovation Center where he helps expedite the variety of use cases of AWS customers. Before joining Amazon, Sungmin was a postdoctoral research fellow at Harvard Medical School. He holds Ph.D. in Computer Science from New York University. Outside of work, Sungmin enjoys hiking, reading and cooking.</p><p> is an Applied Scientist II at the AWS Generative AI Innovation Center, where she develops generative AI solutions for AWS customers. Her expertise encompasses designing and implementing innovative AI-driven and deep learning techniques, focusing on natural language processing, computer vision, multi-modal learning, and graph learning. Yiyue holds a Ph.D. in Computer Science from the University of Notre Dame, where her research centered on advanced machine learning and deep learning methodologies. Outside of work, she enjoys sports, hiking, and traveling.</p><p> is a Machine Learning Engineer at the AWS Generative AI Innovation Center, where he works on model customization and optimization for LLMs. He also builds tools to help his team tackle various aspects of the LLM development life cycle—including fine-tuning, benchmarking, and load-testing—that accelerating the adoption of diverse use cases for AWS customers. He holds an M.S. degree in Computer Science from UC Davis.</p><p> is a Senior Manager of Model Customization at the AWS Generative AI Innovation Center. Her team specializes in helping customers develop differentiating Generative AI solutions using their unique and proprietary data to achieve key business outcomes. She holds a Ph.D in Physics from the University of Iowa, with a focus on astronomical X-ray analysis and instrumentation development. Outside of work, she can be found hiking, mountain biking, and skiing around the mountains in Colorado.</p>","contentLength":28436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Achieve ~2x speed-up in LLM inference with Medusa-1 on Amazon SageMaker AI","url":"https://aws.amazon.com/blogs/machine-learning/achieve-2x-speed-up-in-llm-inference-with-medusa-1-on-amazon-sagemaker-ai/","date":1739382093,"author":"Daniel Zagyva","guid":129,"unread":true,"content":"<p><em>This blog post is co-written with Moran Beladev, Manos Stergiadis, and Ilya Gusev from Booking.com.</em></p><p><a href=\"https://aws.amazon.com/what-is/large-language-model/\" target=\"_blank\" rel=\"noopener\">Large language models</a> (LLMs) have revolutionized the field of natural language processing with their ability to understand and generate humanlike text. Trained on broad, generic datasets spanning a wide range of topics and domains, LLMs use their parametric knowledge to perform increasingly complex and versatile tasks across multiple business use cases. Furthermore, companies are increasingly investing resources in customizing LLMs through few-shot learning and fine-tuning to optimize their performance for specialized applications.</p><p>However, the impressive performance of LLMs comes at the cost of significant computational requirements, driven by their large number of parameters and autoregressive decoding process which is sequential in nature. This combination makes achieving low latency a challenge for use cases such as real-time text completion, simultaneous translation, or conversational voice assistants, where subsecond response times are critical.</p><p>Researchers developed <a href=\"https://arxiv.org/abs/2401.10774\" target=\"_blank\" rel=\"noopener\">Medusa</a>, a framework to speed up LLM inference by adding extra heads to predict multiple tokens simultaneously. This post demonstrates how to use Medusa-1, the first version of the framework, to speed up an LLM by fine-tuning it on <a href=\"https://aws.amazon.com/sagemaker-ai/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker AI</a> and confirms the speed up with deployment and a simple load test. Medusa-1 achieves an inference speedup of around two times without sacrificing model quality, with the exact improvement varying based on model size and data used. In this post, we demonstrate its effectiveness with a 1.8 times speedup observed on a sample dataset.</p><h2>Introduction to Medusa and its benefits for LLM inference speed</h2><p>LLMs generate text in a sequential manner, which involves autoregressive sampling, with each new token conditional on the previous ones. Generating K tokens necessitates K sequential executions of the model. This token-by-token processing introduces an inherent latency and computational overhead because the model needs to perform a separate forward pass for each new token in the output sequence. The following diagram from <a href=\"https://www.researchgate.net/publication/371123751_Role-Play_with_Large_Language_Models\" target=\"_blank\" rel=\"noopener\">Role-Play with Large Language Models</a> illustrates this flow.</p><p>Speculative decoding tackles this challenge by using a smaller, faster draft model to generate multiple potential token continuations in parallel, which are then verified by a larger, more accurate target model. This parallelization speeds up text generation while maintaining the quality of the target model because the verification task is faster than autoregressive token generation. For a detailed explanation of the concept, refer to the paper <a href=\"https://arxiv.org/abs/2302.01318\" target=\"_blank\" rel=\"noopener\">Accelerating Large Language Model Decoding with Speculative Sampling</a>. The speculative decoding technique can be implemented using the <a href=\"https://aws.amazon.com/blogs/machine-learning/achieve-up-to-2x-higher-throughput-while-reducing-costs-by-50-for-generative-ai-inference-on-amazon-sagemaker-with-the-new-inference-optimization-toolkit-part-1/\" target=\"_blank\" rel=\"noopener\">inference optimization toolkit</a> on <a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Jumpstart</a>.</p><p>The paper <a href=\"https://arxiv.org/abs/2401.10774\" target=\"_blank\" rel=\"noopener\">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a> introduced Medusa as an alternative to speculative decoding. Instead of adding a separate draft model, it adds extra decoding heads to the LLM that generate candidate continuations simultaneously. These candidates are then evaluated in parallel using a tree-based attention mechanism. This parallel processing reduces the number of sequential steps needed, leading to faster inference times. The main advantage of Medusa over speculative decoding is that it eliminates the need to acquire and maintain a separate draft model while achieving higher speedups. For example, when tested on the <a href=\"https://arxiv.org/abs/2402.14762\" target=\"_blank\" rel=\"noopener\">MT-Bench dataset</a>, the paper reports that Medusa-2 (the second version of Medusa) speeds up inference time by 2.8 times. This outperforms speculative decoding, which only manages to speed up inference time by 1.5 times on the same dataset.</p><p>The Medusa framework currently supports Llama and Mistral models. Although it offers significant speed improvements, it does come with a memory trade-off (similar to speculative decoding). For instance, adding five Medusa heads to the 7-billion-parameter Mistral model increases the total parameter count by 750 million (150 million per head), which means these additional parameters must be stored in GPU memory, leading to a higher memory requirement. However, in most cases, this increase doesn’t necessitate switching to a higher GPU memory instance. For example, you can still use an  instance with 24 GB of GPU memory to host your 7-billion-parameter Llama or Mistral model with extra Medusa heads.</p><p>Training Medusa heads requires additional development time and computational resources, which should be factored into project planning and resource allocation. Another important limitation to mention is that the current framework, when deployed on an Amazon SageMaker AI endpoint, only supports a batch size of one—a configuration typically used for low-latency applications.</p><p>The following diagram from the original Medusa paper authors’ <a href=\"https://github.com/FasterDecoding/Medusa\" target=\"_blank\" rel=\"noopener\">FasterDecoding repository</a> gives a visual Medusa framework overview.</p><p>There are two main variants of Medusa:</p><ol><li> – Requires a two-stage approach where you first fine-tune your LLM and then add Medusa heads and train them on top of your frozen fine-tuned LLM</li><li> – Introduced later as an improvement, fine-tunes both the additional heads and the backbone LLM parameters together, enabling potentially even further latency speedups</li></ol><p>The Medusa paper reports that across models of varying sizes, you can achieve inference speedups of around two times for Medusa-1 and around three times for Medusa-2. With Medusa-1, the predictions are identical to those of the originally fine-tuned LLM. In contrast, with Medusa-2, we might observe slightly different results compared to simple fine-tuning of the LLM because both the heads and the backbone LLM parameters are updated together. In this post, we focus on Medusa-1.</p><p>We cover the following steps in our solution:</p><ul><li>Load and prepare the dataset</li><li>Fine-tune an LLM using a SageMaker AI training job</li><li>Train Medusa heads on top of a frozen fine-tuned LLM using a SageMaker AI training job</li><li>Deploy the fine-tuned LLM with Medusa heads on a SageMaker AI endpoint</li><li>Demonstrate LLM inference speedup</li></ul><p>By following this solution, you can accelerate LLM inference in your applications, leading to faster response times and improved user experience.</p><p>To build the solution yourself, there are the following prerequisites:</p><h3>Load and prepare the dataset</h3><p>Now that you have cloned the <a href=\"https://github.com/aws-samples/speedup-llm-inference-with-medusa-framework-on-amazon-sagemaker\">GitHub repository</a> and opened the  notebook, you will load and prepare the dataset in the notebook. We encourage you to read this post while running the code in the notebook. For this post, we use a dataset called <a href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" target=\"_blank\" rel=\"noopener\">sql-create-context</a>, which contains samples of natural language instructions, schema definitions and the corresponding SQL query. It contains 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL queries answering the question using the CREATE statement as context. For demonstration purposes, we select 3,000 samples and split them into train, validation, and test sets.</p><p>You need to run the “Load and prepare the dataset” section of the  to prepare the dataset for fine-tuning. We also included a data exploration script to analyze the length of input and output tokens. After data exploration, we prepare the train, validation, and test sets and upload them to <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3).</p><h3>Fine-tune an LLM using SageMaker AI training job</h3><p>We use the <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\" target=\"_blank\" rel=\"noopener\">Zephyr 7B β</a> model as our backbone LLM. Zephyr is a series of language models trained to act as helpful assistants, and Zephyr 7B β is a fine-tuned version of <a href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" target=\"_blank\" rel=\"noopener\">Mistral-7B-v0.1</a>, trained on a mix of publicly available and synthetic datasets using <a href=\"https://arxiv.org/abs/2305.18290\" target=\"_blank\" rel=\"noopener\">Direct Preference Optimization</a>.</p><p>To launch a SageMaker AI training job, we need to use the PyTorch or Hugging Face estimator. SageMaker AI starts and manages all the necessary <a href=\"https://aws.amazon.com/ec2/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instances for us, supplies the appropriate containers, downloads data from our S3 bucket to the container and uploads and runs the specified training script, in our case . We select the hyperparameters based on the <a href=\"https://arxiv.org/pdf/2305.14314\" target=\"_blank\" rel=\"noopener\">QLoRA paper</a>, but we encourage you to experiment with your own combinations. To expedite the execution of this code, we set the number of epochs to 1. However, for better results, it’s generally recommended to set the number of epochs to at least 2 or 3.</p><div><pre><code>from sagemaker.pytorch.estimator import PyTorch\nfrom sagemaker.debugger import TensorBoardOutputConfig\nimport time\nimport os\n\ndef get_current_time():\n    return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n\ndef create_estimator(hyperparameters_dict, job_name, role, sess, train_scipt_path):\n    metric=[\n        {\"Name\": \"loss\", \"Regex\": r\"'loss':\\s*([0-9.]+)\"},\n        {\"Name\": \"epoch\", \"Regex\": r\"'epoch':\\s*([0-9.]+)\"},\n    ]\n\n    tensorboard_s3_output_path = os.path.join(\n       \"s3://\", sess.default_bucket(), job_name, 'tensorboard'\n    )\n    print(\"Tensorboard output path:\", tensorboard_s3_output_path)\n\n    tensorboard_output_config = TensorBoardOutputConfig(\n        s3_output_path=tensorboard_s3_output_path,\n        container_local_output_path=hyperparameters_dict['logging_dir']\n    )\n    estimator = PyTorch(\n        sagemaker_session    = sess,\n        entry_point          = train_scipt_path,    # train script\n        source_dir           = 'train',      # directory which includes all the files needed for training\n        instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job, \"local_gpu\" for local mode\n        metric_definitions   = metric,\n        instance_count       = 1,                 # the number of instances used for training\n        role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n        volume_size          = 300,               # the size of the EBS volume in GB\n        framework_version      = '2.1.0',             # the pytorch_version version used in the training job\n        py_version           = 'py310',           # the python version used in the training job\n        hyperparameters      =  hyperparameters_dict,  # the hyperparameters passed to the training job\n        disable_output_compression = True,        # not compress output to save training time and cost\n        tensorboard_output_config = tensorboard_output_config\n    )\n    return estimator\n    \n# hyperparameters, which are passed into the training job\nsft_hyperparameters = {\n  ### SCRIPT PARAMETERS ###\n  'train_dataset_path': '/opt/ml/input/data/train/train_dataset.json', # path where sagemaker will save training dataset\n  'eval_dataset_path': '/opt/ml/input/data/eval/eval_dataset.json', # path where sagemaker will save evaluation dataset\n  'model_id': model_id,\n  'max_seq_len': 256,                               # max sequence length for model and packing of the dataset\n  'use_qlora': True,                                 # use QLoRA model\n  ### TRAINING PARAMETERS ###\n  'num_train_epochs': 1,                             # number of training epochs\n  'per_device_train_batch_size': 1,                  # batch size per device during training\n  'gradient_accumulation_steps': 16,                  # number of steps before performing a backward/update pass\n  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n  'optim': \"adamw_8bit\",                             # use fused adamw 8bit optimizer\n  'logging_steps': 15,                               # log every 10 steps\n  'save_strategy': \"steps\",                          # save checkpoint every epoch\n  'save_steps': 15,\n  'save_total_limit': 2,\n  'eval_strategy': \"steps\",\n  'eval_steps': 15,\n  'learning_rate': 1e-4,                             # learning rate, based on QLoRA paper\n  'bf16': True,                                      # use bfloat16 precision\n  'max_grad_norm': 10,                              # max gradient norm based on QLoRA paper\n  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n  'output_dir': '/opt/ml/checkpoints/',              # Temporary output directory for model checkpoints\n  'merge_adapters': True,                            # merge LoRA adapters into model for easier deployment\n  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n  'logging_dir': \"/opt/ml/output/tensorboard\"        # tensorboard logging directory\n}\n \nsft_job_name = f\"sft-qlora-text-to-sql-{get_current_time()}\"\ndata = {\n    'train': train_dataset_path,\n    'eval': eval_dataset_path\n}\n\nsft_estimator = create_estimator(sft_hyperparameters, sft_job_name, role, sess, \"fine_tune_llm.py\")\n\nsft_estimator.fit(job_name=sft_job_name, inputs=data, wait=False)</code></pre></div><p>When our training job has completed successfully after approximately 1 hour, we can use the fine-tuned model artifact for the next step, training the Medusa heads on top of it. To visualize the training metrics in Tensorboard, you can follow the guidance in this documentation: <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-htb-access-tb-data.html\" target=\"_blank\" rel=\"noopener\">Load and visualize output tensors using the TensorBoard application</a></p><h3>Train Medusa heads on top of frozen fine-tuned LLM using a SageMaker AI training job</h3><p>For training Medusa heads, we can reuse the functions previously mentioned to launch the training job. We selected hyperparameters based on a combination of what the Medusa paper reported and what we found to be best performing after a few experiments. We set the number of Medusa heads to 5 and used the 8-bit AdamW optimizer, as recommended by the paper. For simplicity, we maintained a constant learning rate of 1e-4 with a constant scheduler, similar to the previous fine-tuning step. Although the paper recommends an increased learning rate and a cosine scheduler, we found that our chosen combination of hyperparameters performed well on this dataset. However, we encourage you to experiment with your own hyperparameter settings to potentially achieve even better results.</p><div><pre><code># hyperparameters, which are passed into the training job\nmedusa_hyperparameters = {\n  ### SCRIPT PARAMETERS ###\n  'train_dataset_path': '/opt/ml/input/data/train/train_dataset.json', # path where sagemaker will save training dataset\n  'eval_dataset_path': '/opt/ml/input/data/eval/eval_dataset.json', # path where sagemaker will save evaluation dataset\n  'model_path': '/opt/ml/input/data/fine-tuned-model/',\n  'max_seq_len': 256,                               # max sequence length for model and packing of the dataset\n  'medusa_num_heads': 5,\n  ### TRAINING PARAMETERS ###\n  'num_train_epochs': 3,                             # number of training epochs\n  'per_device_train_batch_size': 1,                  # batch size per device during training\n  'gradient_accumulation_steps': 16,                  # number of steps before performing a backward/update pass\n  'gradient_checkpointing': True,                    # use gradient checkpointing to save memory\n  'optim': \"adamw_8bit\",                             # use fused adamw 8bit optimizer\n  'logging_steps': 15,                               # log every 10 steps\n  'save_strategy': \"steps\",                          # save checkpoint every epoch\n  'save_steps': 15,\n  'save_total_limit':2,\n  'eval_strategy': \"steps\",\n  'eval_steps': 15,\n  'learning_rate': 1e-4,                             # learning rate\n  'bf16': True,                                      # use bfloat16 precision\n  'max_grad_norm': 10,                              # max gradient norm based on QLoRA paper\n  'warmup_ratio': 0.03,                              # warmup ratio based on QLoRA paper\n  'lr_scheduler_type': \"constant\",                   # use constant learning rate scheduler\n  'output_dir': '/opt/ml/checkpoints/',              # Temporary output directory for model checkpoints\n  'report_to': \"tensorboard\",                        # report metrics to tensorboard\n  'logging_dir': \"/opt/ml/output/tensorboard\"        # tensorboard logging directory\n}\n\nmedusa_train_job_name = f\"medusa-text-to-sql-{get_current_time()}\"\ndata = {\n    'train': train_dataset_path,\n    'eval': eval_dataset_path,\n    'fine-tuned-model': fine_tuned_model_path\n}\n\nmedusa_estimator = create_estimator(medusa_hyperparameters, medusa_train_job_name, role, sess, \"train_medusa_heads.py\")\n\nmedusa_estimator.fit(job_name=medusa_train_job_name, inputs=data, wait=False)</code></pre></div><p>We found that after 3 epochs, the evaluation loss of Medusa heads was converging, which can be observed in the TensorBoard graph in the following image.</p><p>Besides the hyperparameters, the main difference is that we pass  as the training entrypoint, where we first add Medusa heads, then freeze the fine-tuned LLM, and we create custom MedusaSFTTrainer class, which is a subclass of the transformers SFTTrainer.</p><div><pre><code># Add medusa heads and freeze base model\nadd_medusa_heads(\n    model,\n    medusa_num_heads=script_args.medusa_num_heads,\n)\nfreeze_layers(model)\nmodel.config.torch_dtype = torch_dtype\nmodel.config.use_cache = False\n\nlogger.info(\"Finished loading model and medusa heads\")\n\ntokenizer = AutoTokenizer.from_pretrained(script_args.model_path, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n################\n# Training\n################\ntrainer = MedusaSFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    max_seq_length=script_args.max_seq_length,\n    tokenizer=tokenizer,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False,  # No need to add additional separator token\n    },\n    medusa_num_heads=script_args.medusa_num_heads,\n    medusa_heads_coefficient=script_args.medusa_heads_coefficient,\n    medusa_decay_coefficient=script_args.medusa_decay_coefficient,\n    medusa_scheduler=script_args.medusa_scheduler,\n    train_only_medusa_heads=script_args.train_only_medusa_heads,\n    medusa_lr_multiplier=script_args.medusa_lr_multiplier\n)\ntrainer.train()</code></pre></div><p>In the  function, we add the residual blocks of the Medusa heads, and also override the forward pass for our model to make sure not to train the frozen backbone LLM:</p><div><pre><code>def add_medusa_heads(\n    model,\n    medusa_num_heads,\n):\n    \"\"\"\n    Args:\n        model (nn.Module): The base language model to be used.\n        medusa_num_heads (int, optional): Number of additional tokens to predict\n    \"\"\"\n    hidden_size = model.lm_head.weight.shape[-1]\n    vocab_size = model.lm_head.weight.shape[0]\n    model.config.medusa_num_layers = 1\n    model.config.medusa_num_heads = medusa_num_heads\n    model.medusa_num_heads = medusa_num_heads\n    # Create a list of Medusa heads\n    model.medusa_heads = nn.ModuleList(\n        [\n            nn.Sequential(\n                ResBlock(hidden_size),\n                nn.Linear(hidden_size, vocab_size, bias=False),\n            )\n            for _ in range(medusa_num_heads)\n        ]\n    )\n\n    # Ensure medusa_head's dtype and device align with the base_model\n    model.medusa_heads.to(model.dtype).to(model.device)\n    logger.info(f\"Loading medusa heads in {str(model.dtype)} to device {model.device}\")\n\n    for i in range(medusa_num_heads):\n        # Initialize the weights of each medusa_head using the base model's weights\n        model.medusa_heads[i][-1].weight.data[:] = model.lm_head.weight.data[:]\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        train_only_medusa_heads: bool = False,\n    ):\n        \"\"\"Forward pass of the MedusaModel.\n        Returns:\n            torch.Tensor: A tensor containing predictions from all Medusa heads.\n            (Optional) Original predictions from the base model's LM head.\n        \"\"\"\n        maybe_grad = torch.no_grad() if train_only_medusa_heads else nullcontext()\n        with maybe_grad:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n            hidden_states = outputs[0]\n            medusa_logits = [self.lm_head(hidden_states)]\n        for i in range(self.medusa_num_heads):\n            medusa_logits.append(self.medusa_heads[i](hidden_states))\n        return torch.stack(medusa_logits, dim=0)\n\n    model.forward = types.MethodType(forward, model)</code></pre></div><p>After the model training is finished (which takes 1 hour), we prepare the model artefacts for deployment and upload it to Amazon S3. Your final model artifact contains both the original fine-tuned model from the previous step under the  prefix and the trained Medusa heads in a file named .</p><h3>Deploy the fine-tuned LLM with Medusa heads on a SageMaker AI endpoint</h3><p>First, we create a SageMaker AI HuggingFaceModel object and then deploy the model to an endpoint with the following function:</p><div><pre><code>import json\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n\n\ndef deploy_model(endpoint_name, instance_type, model_s3_path=None, hf_model_id=None):\n    llm_image = get_huggingface_llm_image_uri(\n      \"huggingface\",\n      version=\"2.2.0\",\n      session=sess,\n    )\n\n    print(f\"llm image uri: {llm_image}\")\n\n    model_data = None\n    if model_s3_path:\n        model_data = {'S3DataSource': {'S3Uri': model_s3_path, 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n        hf_model_id = \"/opt/ml/model\"\n    else:\n        assert hf_model_id, \"You need to provide either pretrained HF model id, or S3 model data to deploy\"\n    config = {\n      'HF_MODEL_ID': hf_model_id,  # path to where sagemaker stores the model\n      'SM_NUM_GPUS': json.dumps(1),  # Number of GPU used per replica\n      'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n      'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n    }\n\n    llm_model = HuggingFaceModel(\n      name=endpoint_name,\n      role=role,\n      image_uri=llm_image,\n      model_data=model_data,\n      env=config\n    )\n\n    deployed_llm = llm_model.deploy(\n      endpoint_name=endpoint_name,\n      initial_instance_count=1,\n      instance_type=instance_type,\n      container_startup_health_check_timeout=300,\n    )\n    return deployed_llm</code></pre></div><p>We deploy three LLMs on three SageMaker AI endpoints:</p><ol><li>Base LLM which isn’t fine-tuned</li><li>The LLM that we fine-tuned</li><li>The fine-tuned LLM that also has trained Medusa heads</li></ol><p>You can deploy the three models in parallel by using a function that we included in the notebook, or you can deploy the models one by one by running the code below:</p><div><pre><code>base_deployed_llm = deploy_model( f\"base-{get_current_time()}\", instance_type=\"ml.g5.4xlarge\", model_s3_path=None, hf_model_id=model_id )\nsft_deployed_llm = deploy_model( f\"sft-{get_current_time()}\", instance_type=\"ml.g5.4xlarge\", model_s3_path=fine_tuned_model_path )\nmedusa_deployed_llm = deploy_model( f\"medusa-{get_current_time()}\", instance_type=\"ml.g5.4xlarge\", model_s3_path=medusa_trained_model_path )\n</code></pre></div><p>After the status for each endpoint becomes , which should take around 15 minutes, we can invoke them for inference. We send the following input:</p><div><pre><code>“You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA. SCHEMA: CREATE TABLE table_name_32 (time VARCHAR, heat VARCHAR, name VARCHAR)\nWhat was Liu Limin's time in heat 4?“</code></pre></div><p>We can observe the following responses:</p><ol><li>The base LLM response contains extra words that aren’t needed:</li></ol><div><pre><code>“To retrieve the time of Liu Limin in heat 4 based on the provided SCHEMA, we need to write a SQL query. Since the table name is not provided, let 's assume it's \"my_table\".  Assuming the table has a primary key (which is not mentioned in the SCHEMA), we can join the table with itself to compare the heat and name of Liu Limin in heat 4 with all the records in the table.  Here's the SQL query:  ``sql SELECT t1.time FROM my_table t1 JOIN my_table t2 ON t1.name = t2.name AND t2.heat = 4 WHERE t1.name = 'Liu Limin' AND t1.heat &lt;&gt; 4; `  Explanation:  1. We're selecting the time` column from the first table (t1) in the FROM clause. 2. We're joining the table with itself (my_table t1 JOIN my_table t2) to compare the name and heat of Liu Limin in heat 4 with all the records in the table. 3. We're filtering the results using the WHERE clause. We're selecting only the records where the name is 'Liu Limin' and the heat is not equal to 4 (i.e., not heat 4). This is to ensure that we're selecting the time of Liu Limin in heat 3.  Note: This query assumes that the table has a unique primary key. If the table doesn't have a primary key, you may need to add additional conditions to the JOIN and WHERE clauses to ensure that we're selecting the correct records.“</code></pre></div><ol start=\"2\"><li>The fine-tuned LLM response is improved significantly, and contains only the required output:</li></ol><div><pre><code>'SELECT time FROM table_name_32 WHERE heat = 4 AND name = \"liu limin\"'</code></pre></div><ol start=\"3\"><li>The fine-tuned LLM with trained Medusa heads provides the exact same response as the fine-tuned model, demonstrating that Medusa-1, by design, maintains the output (quality) of the original model:</li></ol><div><pre><code>'SELECT time FROM table_name_32 WHERE heat = 4 AND name = \"liu limin\"'</code></pre></div><h3>Demonstrate LLM inference speedup</h3><p>To measure the inference speed improvements, we compare the response times of the deployed fine-tuned LLM and the fine-tuned LLM with Medusa heads on 450 test observations with the following code:</p><div><pre><code>import time\nimport numpy as np\nfrom tqdm import tqdm\n\ndef request(sample, deployed_llm):\n    prompt = tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=True)\n    outputs = deployed_llm.predict({\n      \"inputs\": prompt,\n      \"parameters\": {\n        \"max_new_tokens\": 512,\n        \"do_sample\": False,\n        \"return_full_text\": False,\n      }\n    })\n    return {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"].strip()}\n\ndef predict(deployed_llm, test_dataset):\n    predicted_answers = []\n    latencies = []\n\n    for sample in tqdm(test_dataset):\n        start_time = time.time()\n        predicted_answer = request(sample[\"messages\"][:2], deployed_llm)\n        end_time = time.time()\n\n        latency = end_time - start_time\n        latencies.append(latency)\n        predicted_answers.append(predicted_answer)\n\n    # Calculate p90 and average latencies\n    p90_latency = np.percentile(latencies, 90)\n    avg_latency = np.mean(latencies)\n\n    print(f\"P90 Latency: {p90_latency:.2f} seconds\")\n    print(f\"Average Latency: {avg_latency:.2f} seconds\")\n\n    return predicted_answers</code></pre></div><p>First, we run predictions using the fine-tuned LLM:</p><div><pre><code>sft_predictions = predict(sft_deployed_llm, test_dataset)\nP90 Latency: 1.28 seconds\nAverage Latency: 0.95 seconds</code></pre></div><p>Then, we run predictions using the fine-tuned LLM with Medusa heads:</p><div><pre><code>medusa_predictions = predict(medusa_deployed_llm, test_dataset)\nP90 Latency: 0.80 seconds\nAverage Latency: 0.53 seconds</code></pre></div><p>The prediction runs should take around 8 and 4 minutes respectively. We can observe that the average latency decreased from 950 to 530 milliseconds, which is an improvement of 1.8 times. You can achieve even higher improvements if your dataset contains longer inputs and outputs. In our dataset, we only had an average of 18 input tokens and 30 output tokens.</p><p>We want to once again highlight that, with this technique, the output quality is fully maintained, and all the prediction outputs are the same. The model responses for the test set of 450 observations are the same for both with Medusa heads and without Medusa heads:</p><div><pre><code>match_percentage = sum(a[\"content\"] == b[\"content\"] for a, b in zip(sft_predictions, medusa_predictions)) / len(sft_predictions) * 100\nprint(f\"Predictions with the fine-tuned model with medusa heads are the same as without medusa heads: {match_percentage:.2f}% of test set \")\n\nPredictions with fine-tuned model with medusa heads are the same as without medusa heads: 100.00% of test set </code></pre></div><p>You might notice in your run that a few observations aren’t exactly matching, and you might get a 99% match due to small errors in floating point operations caused by optimizations on GPUs.</p><p>At the end of this experiment, don’t forget to delete the SageMaker AI endpoints you created:</p><div><pre><code>base_deployed_llm.delete_model()\nbase_deployed_llm.delete_endpoint()\nsft_deployed_llm.delete_model()\nsft_deployed_llm.delete_endpoint()\nmedusa_deployed_llm.delete_model()\nmedusa_deployed_llm.delete_endpoint()</code></pre></div><p>In this post, we demonstrated how to fine-tune and deploy an LLM with Medusa heads using the Medusa-1 technique on Amazon SageMaker AI to accelerate LLM inference. By using this framework and SageMaker AI scalable infrastructure, we showed how to achieve up to twofold speedups in LLM inference while maintaining model quality. This solution is particularly beneficial for applications requiring low-latency text generation, such as customer service chat assistants, content creation, and recommendation systems.</p><p>As a next step, you can explore fine-tuning your own LLM with Medusa heads on your own dataset and benchmark the results for your specific use case, using the provided <a href=\"https://github.com/aws-samples/speedup-llm-inference-with-medusa-framework-on-amazon-sagemaker\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_dzagyva.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior ML Engineer at AWS Professional Services. He specializes in developing scalable, production-grade machine learning solutions for AWS customers. His experience extends across different areas, including natural language processing, generative AI and machine learning operations.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_adokic.jpg\" alt=\"\" width=\"100\" height=\"129\"> is a Senior Data Scientist at AWS Professional Services. She enjoys supporting customers to build innovative AI/ML solutions on AWS and she is excited about business transformations through the power of data.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_mbeladev.png\" alt=\"\" width=\"100\" height=\"100\"> is a Senior ML Manager at Booking.com. She is leading the content intelligence track which is focused on building, training and deploying content models (computer vision, NLP and generative AI) using the most advanced technologies and models. Moran is also a PhD candidate, researching applying NLP models on social graphs.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_mstergiadis.jpg\" alt=\"\" width=\"100\" height=\"100\"> is a Senior ML Scientist at Booking.com. He specializes in generative NLP and has experience researching, implementing and deploying large deep learning models at scale.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_igusev.jpg\" alt=\"\" width=\"100\" height=\"158\"> is a Senior Machine Learning Engineer at Booking.com. He leads the development of the several LLM systems inside Booking.com. His work focuses on building production ML systems that help millions of travelers plan their trips effectively.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/ml_17442_lvandermaas-727x1024-1.jpg\" alt=\"\" width=\"100\" height=\"141\"> is a Machine Learning Engineer at AWS Professional Services. He works closely with customers building their machine learning solutions on AWS, specializes in natural language processing, experimentation and responsible AI, and is passionate about using machine learning to drive meaningful change in the world.</p>","contentLength":31295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-as-a-judge on Amazon Bedrock Model Evaluation","url":"https://aws.amazon.com/blogs/machine-learning/llm-as-a-judge-on-amazon-bedrock-model-evaluation/","date":1739381817,"author":"Adewale Akinfaderin","guid":128,"unread":true,"content":"<p>The evaluation of large language model (LLM) performance, particularly in response to a variety of prompts, is crucial for organizations aiming to harness the full potential of this rapidly evolving technology. The introduction of an  framework represents a significant step forward in simplifying and streamlining the model evaluation process. This approach allows organizations to assess their AI models’ effectiveness using pre-defined metrics, making sure that the technology aligns with their specific needs and objectives. By adopting this method, companies can more accurately gauge the performance of their AI systems, making informed decisions about model selection, optimization, and deployment. This not only enhances the reliability and efficiency of AI applications, but also contributes to a more strategic and informed approach to technology adoption within the organization.</p><p><a href=\"https://aws.amazon.com/bedrock\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a>, a fully managed service offering high-performing foundation models from leading AI companies through a single API, has recently introduced two significant evaluation capabilities: <a href=\"https://aws.amazon.com/blogs/aws/new-rag-evaluation-and-llm-as-a-judge-capabilities-in-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">LLM-as-a-judge under Amazon Bedrock Model Evaluation and RAG evaluation for Amazon Bedrock Knowledge Bases</a>. Both features use the LLM-as-a-judge technique behind the scenes but evaluate different things. This blog post explores LLM-as-a-judge on Amazon Bedrock Model Evaluation, providing comprehensive guidance on feature setup, evaluating job initiation through both the console and Python SDK and APIs, and demonstrating how this innovative evaluation feature can enhance generative AI applications across multiple metric categories including quality, user experience, instruction following, and safety.</p><p>Before we explore the technical aspects and implementation details, let’s examine the key features that make LLM-as-a-judge on Amazon Bedrock Model Evaluation particularly powerful and distinguish it from traditional evaluation methods. Understanding these core capabilities will help illuminate why this feature represents a significant advancement in AI model evaluation.</p><h2>Key features of LLM-as-a-judge</h2><ol><li><strong>Automated intelligent evaluation</strong>: LLM-as-a-judge uses pre-trained models to evaluate responses automatically, providing human-like evaluation quality with up to 98% cost savings. The system dramatically reduces evaluation time from weeks to hours while maintaining consistent evaluation standards across large datasets.</li><li><strong>Comprehensive metric categories</strong>: The evaluation system covers four key metric areas: quality assessment (correctness, completeness, faithfulness), user experience (helpfulness, coherence, relevance), instruction compliance (following instructions, professional style), and safety monitoring (harmfulness, stereotyping, refusal handling).</li><li>: The feature integrates directly with Amazon Bedrock and remains compatible with existing Amazon Bedrock Model Evaluation features. Users can access the functionality through the AWS Management Console for Amazon Bedrock and quickly integrate their custom datasets for evaluation purposes.</li><li>: The system supports the evaluation of models hosted on Amazon Bedrock, custom fine-tuned models, and imported models. Users can seamlessly connect their evaluation datasets through <a href=\"https://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service (Amazon S3)</a> buckets, making the evaluation process streamlined and efficient.</li><li>: Amazon Bedrock provides pre-selected, high-quality evaluation models with optimized prompt engineering for accurate assessments. Users don’t need to bring external judge models, because the Amazon Bedrock team maintains and updates a selection of judge models and associated evaluation judge prompts.</li><li>: The feature enables organizations to perform comprehensive model evaluations at scale without the traditional costs and time investments associated with human evaluation. The automated process maintains high-quality assessments while significantly reducing operational overhead.</li></ol><p>These features create a powerful evaluation framework that helps organizations optimize their AI model performance while maintaining high standards of quality and safety, all within their secure AWS environment.</p><p>Now that you understand the key features of LLM-as-a-judge, let’s examine how to implement and use this capability within Amazon Bedrock Model Evaluation. This section provides a comprehensive overview of the architecture and walks through each component, demonstrating how they work together to deliver accurate and efficient model evaluations.</p><p>LLM-as-a-judge on Amazon Bedrock Model Evaluation provides a comprehensive, end-to-end solution for assessing and optimizing AI model performance. This automated process uses the power of LLMs to evaluate responses across multiple metric categories, offering insights that can significantly improve your AI applications. Let’s walk through the key components of this solution as shown in the following diagram:</p><p>LLM-as-a-judge on Amazon Bedrock Model Evaluation follows a streamlined workflow that enables systematic model evaluation. Here’s how each component works together in the evaluation process:</p><ul><li>: The process begins with a prepared dataset containing prompts that will be used to test the model’s performance. The evaluation can be conducted with or without ground truth responses—while including ground truth provides additional comparison points, it’s entirely optional and not required for successful evaluation.</li><li>: The prompt dataset is converted into JSONL format, which is specifically structured for LLM-as-a-judge evaluation jobs. This format promotes proper processing of evaluation data.</li><li>: The prepared JSONL file is uploaded to an S3 bucket, serving as the secure storage location for the evaluation data.</li><li>: The Amazon Bedrock LLM-as-a-judge model evaluation job processes the stored data, running comprehensive assessments across the selected metric categories (including quality, user experience, instruction following, and safety).</li><li><strong>Automated report generation</strong>: Upon completion, the system generates detailed evaluation reports containing metrics, scores, and insights at both aggregate and individual response levels.</li><li>: Data scientists or machine learning engineers analyze the generated reports to derive actionable insights and make informed decisions.</li></ul><p>With this solution architecture in mind, let’s explore how to implement LLM-as-a-judge model evaluations effectively, making sure that you get the most valuable insights from your assessment process.</p><p>To use the LLM-as-a-judge model evaluation, make sure that you have satisfied the following requirements:</p><ul><li>Selected  and  models enabled in Amazon Bedrock. You can confirm that the models are enabled for your account on the  page of the Amazon Bedrock console.</li><li>If you’re using a custom model instead of an on-demand model for your generator model, make sure that you have sufficient quota for running a <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html\" target=\"_blank\" rel=\"noopener\">Provisioned Throughput</a> during inference. \n  <ul><li>Go to the AWS Service Quotas console, and check the following quotas: \n    <ul><li>Model units no-commitment Provisioned Throughputs across custom models.</li><li>Model units per provisioned model for [your custom model name].</li><li>Both of these fields need to have enough quota to support your Provisioned Throughput model unit. Request a quota increase if necessary to accommodate your expected inference workload.</li></ul></li></ul></li></ul><p>When preparing your dataset for LLM-as-a-judge model evaluation jobs, each prompt must include specific key-value pairs. Here are the required and optional fields:</p><ul><li>: This key indicates the input for various tasks. It can be used for general text generation where the model needs to provide a response, question-answering tasks where the model must answer a specific question, text summarization tasks where the model needs to summarize a given text, or classification tasks where the model must categorize the provided text.</li><li><strong>referenceResponse (used for specific metrics with ground truth)</strong>: This key contains the ground truth or correct response. It serves as the reference point against which the model’s responses will be evaluated if it is provided.</li><li>: This key is used to generate evaluation scores reported by category, helping organize and segment evaluation results for better analysis.</li></ul><ul><li>Each line must be a valid JSON object</li><li>The file must use JSONL format</li><li>The dataset should be stored in an Amazon S3 bucket</li></ul><p>Example JSONL format without ground truth ( is optional):</p><div><pre><code>{\n    \"prompt\": \"What is machine learning?\"\n    \"category\": \"technical\"\n}\n{\n    \"prompt\": \"Summarize climate change impacts\",\n    \"category\": \"environmental\"\n}\n</code></pre></div><p>Example JSONL format with ground truth ( is optional):</p><div><pre><code>{\n    \"prompt\": \"What is machine learning?\",\n    \"referenceResponse\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses algorithms and statistical models to analyze and draw inferences from patterns in data, allowing computers to perform specific tasks without explicit instructions.\",\n    \"category\": \"technical\"\n}\n{\n    \"prompt\": \"Summarize climate change impacts\",\n    \"referenceResponse\": \"Climate change leads to rising global temperatures, extreme weather events, sea level rise, and disruption of ecosystems. These changes result in more frequent natural disasters, threats to food security, loss of biodiversity, and various public health challenges. The impacts affect agriculture, coastal communities, and vulnerable populations disproportionately.\",\n    \"category\": \"environmental\"\n}</code></pre></div><h2>Start an LLM-as-a-judge model evaluation job using the console</h2><p>You can use LLM-as-a-judge on Amazon Bedrock Model Evaluation to assess model performance through a user-friendly console interface. Follow these steps to start an evaluation job:</p><ol><li>In the Amazon Bedrock console, choose and then select . On the page, choose the </li></ol><ol start=\"2\"><li>Choose  and select <strong>Automatic: LLM-as-a-judge</strong>.</li><li>Enter a name and description and select an . This model will be used as a judge to evaluate the response of a prompt or model from your generative AI application.</li></ol><ol start=\"4\"><li>Choose  and select the model to be used for generating responses in this evaluation job.</li></ol><ol start=\"5\"><li>Select the metrics you want to use to evaluate the model response (such as helpfulness, correctness, faithfulness, relevance, and harmfulness).</li></ol><ol start=\"6\"><li>Select the  for  and for . You can use theoption.</li></ol><ol start=\"7\"><li>Select or create an IAM service role with the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-type-judge.html\">proper permissions</a>. This includes service access to Amazon Bedrock, the S3 buckets in the evaluation job, and the models being used in the job. If you create a new IAM role in the evaluation setup, the service will automatically give the role the proper permissions for the job. Specify the output S3 bucket and choose .</li></ol><ol start=\"8\"><li>You will be able to see the evaluation job is .&nbsp;Wait for the job status to change to .</li></ol><ol start=\"9\"><li>When complete, select the job to see its details. The following is the metrics summary (such as 0.83 for helpfulness, 1.00 for correctness, 1.00 for faithfulness, 1.00 for relevance, and 0.00 for harmfulness).</li></ol><ol start=\"10\"><li>To view generation metrics details, scroll down in the model evaluation report and choose any individual metric (like helpfulness or correctness) to see its detailed breakdown.</li></ol><ol start=\"11\"><li>To see each record’s prompt input, generation output, ground truth, and individual scores, choose a metric and select “Prompt details”. Hover over any individual score to view its detailed explanation.</li></ol><h2>Start an LLM-as-a-judge evaluation job using Python SDK and APIs</h2><p>To use the Python SDK for creating an LLM-as-a-judge model evaluation job, use the following steps. First, set up the required configurations:</p><div><pre><code>import boto3\nfrom datetime import datetime\n\n# Generate unique name for the job\njob_name = f\"Model-evaluation-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n\n# Configure your knowledge base and model settings\nevaluator_model = \"mistral.mistral-large-2402-v1:0\"\ngenerator_model = \"amazon.nova-pro-v1:0\"\nrole_arn = \"arn:aws:iam::&lt;YOUR_ACCOUNT_ID&gt;:role/&lt;YOUR_IAM_ROLE&gt;\"\n\n# Specify S3 locations for evaluation data and output\ninput_data = \"s3://&lt;YOUR_BUCKET&gt;/evaluation_data/input.jsonl\"\noutput_path = \"s3://&lt;YOUR_BUCKET&gt;/evaluation_output/\"\n\n# Create Bedrock client\nbedrock_client = boto3.client('bedrock')</code></pre></div><p>To create an LLM-as-a-judge model evaluation job:</p><div><pre><code>def create_llm_judge_evaluation(\n    client,\n    job_name: str,\n    role_arn: str,\n    input_s3_uri: str,\n    output_s3_uri: str,\n    evaluator_model_id: str,\n    generator_model_id: str,\n    dataset_name: str = None,\n    task_type: str = \"General\" # must be General for LLMaaJ\n):    \n    # All available LLM-as-judge metrics\n    llm_judge_metrics = [\n        \"Builtin.Correctness\",\n        \"Builtin.Completeness\", \n        \"Builtin.Faithfulness\",\n        \"Builtin.Helpfulness\",\n        \"Builtin.Coherence\",\n        \"Builtin.Relevance\",\n        \"Builtin.FollowingInstructions\",\n        \"Builtin.ProfessionalStyleAndTone\",\n        \"Builtin.Harmfulness\",\n        \"Builtin.Stereotyping\",\n        \"Builtin.Refusal\"\n    ]\n\n    # Configure dataset\n    dataset_config = {\n        \"name\": dataset_name or \"CustomDataset\",\n        \"datasetLocation\": {\n            \"s3Uri\": input_s3_uri\n        }\n    }\n\n    try:\n        response = client.create_evaluation_job(\n            jobName=job_name,\n            roleArn=role_arn,\n            applicationType=\"ModelEvaluation\",\n            evaluationConfig={\n                \"automated\": {\n                    \"datasetMetricConfigs\": [\n                        {\n                            \"taskType\": task_type,\n                            \"dataset\": dataset_config,\n                            \"metricNames\": llm_judge_metrics\n                        }\n                    ],\n                    \"evaluatorModelConfig\": {\n                        \"bedrockEvaluatorModels\": [\n                            {\n                                \"modelIdentifier\": evaluator_model_id\n                            }\n                        ]\n                    }\n                }\n            },\n            inferenceConfig={\n                \"models\": [\n                    {\n                        \"bedrockModel\": {\n                            \"modelIdentifier\": generator_model_id\n                        }\n                    }\n                ]\n            },\n            outputDataConfig={\n                \"s3Uri\": output_s3_uri\n            }\n        )\n        return response\n        \n    except Exception as e:\n        print(f\"Error creating evaluation job: {str(e)}\")\n        raise\n        \n # Create evaluation job\ntry:\n    llm_as_judge_response = create_llm_judge_evaluation(\n        client=bedrock_client,\n        job_name=job_name,\n        role_arn=ROLE_ARN,\n        input_s3_uri=input_data,\n        output_s3_uri=output_path,\n        evaluator_model_id=evaluator_model,\n        generator_model_id=generator_model,\n        task_type=\"General\"\n    )\n    print(f\"✓ Created evaluation job: {llm_as_judge_response['jobArn']}\")\nexcept Exception as e:\n    print(f\"✗ Failed to create evaluation job: {str(e)}\")\n    raise\n</code></pre></div><p>To monitor the progress of your evaluation job:</p><div><pre><code># Get job ARN based on job type\nevaluation_job_arn = llm_as_judge_response['jobArn']\n# Check job status\ncheck_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn) \nprint(f\"Job Status: {check_status['status']}\")</code></pre></div><p>You can also compare multiple foundation models to determine which one works best for your needs. By using the same evaluator model across all comparisons, you’ll get consistent benchmarking results to help identify the optimal model for your use case.</p><div><pre><code># Generator Models\nGENERATOR_MODELS = [\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    \"amazon.nova-micro-v1:0\"\n]\n\n# Consistent Evaluator\nEVALUATOR_MODEL = \"anthropic.claude-3-haiku-20240307-v1:0\"\n\ndef run_model_comparison(\n    generator_models: List[str],\n    evaluator_model: str\n) -&gt; List[Dict[str, Any]]:\n    evaluation_jobs = []\n    \n    for generator_model in generator_models:\n        job_name = f\"llmaaj-{generator_model.split('.')[0]}-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n        \n        try:\n            response = create_llm_judge_evaluation(\n                client=bedrock_client,\n                job_name=job_name,\n                role_arn=ROLE_ARN,\n                input_s3_uri=input_data,\n                output_s3_uri=f\"{output_path}/{job_name}/\",\n                evaluator_model_id=evaluator_model,\n                generator_model_id=generator_model,\n                task_type=\"General\"\n            )\n            \n            job_info = {\n                \"job_name\": job_name,\n                \"job_arn\": response[\"jobArn\"],\n                \"generator_model\": generator_model,\n                \"evaluator_model\": evaluator_model,\n                \"status\": \"CREATED\"\n            }\n            evaluation_jobs.append(job_info)\n            \n            print(f\"✓ Created job: {job_name}\")\n            print(f\"  Generator: {generator_model}\")\n            print(f\"  Evaluator: {evaluator_model}\")\n            print(\"-\" * 80)\n            \n        except Exception as e:\n            print(f\"✗ Error with {generator_model}: {str(e)}\")\n            continue\n            \n    return evaluation_jobs\n\n# Run model comparison\nevaluation_jobs = run_model_comparison(GENERATOR_MODELS, EVALUATOR_MODEL)</code></pre></div><h3>Correlation analysis for LLM-as-a-judge evaluations</h3><p>You can use the <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\" target=\"_blank\" rel=\"noopener\">Spearman’s rank correlation coefficient</a> to compare evaluation results between different generator models using LLM-as-a-judge in Amazon Bedrock. After retrieving the evaluation results from your S3 bucket, containing evaluation scores across various metrics, you can begin the correlation analysis.</p><p>Using , compute the correlation coefficient between pairs of generator models, filtering out constant values or error messages to have a valid statistical comparison. The resulting correlation coefficients help identify how similarly different models respond to the same prompts. A coefficient closer to 1.0 indicates stronger agreement between the models’ responses, while values closer to 0 suggest more divergent behavior. This analysis provides valuable insights into model consistency and helps identify cases where different models might produce significantly different outputs for the same input.</p><div><pre><code>import json\nimport boto3\nimport numpy as np\nfrom scipy import stats\n\ndef read_and_organize_metrics_from_s3(bucket_name, file_key):\n    s3_client = boto3.client('s3')\n    metrics_dict = {}\n    \n    try:\n        response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n        content = response['Body'].read().decode('utf-8')\n        \n        for line in content.strip().split('\\n'):\n            if line:\n                data = json.loads(line)\n                if 'automatedEvaluationResult' in data and 'scores' in data['automatedEvaluationResult']:\n                    for score in data['automatedEvaluationResult']['scores']:\n                        metric_name = score['metricName']\n                        if 'result' in score:\n                            metric_value = score['result']\n                            if metric_name not in metrics_dict:\n                                metrics_dict[metric_name] = []\n                            metrics_dict[metric_name].append(metric_value)\n        return metrics_dict\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\ndef get_spearmanr_correlation(scores1, scores2):\n    if len(set(scores1)) == 1 or len(set(scores2)) == 1:\n        return \"undefined (constant scores)\", \"undefined\"\n    \n    try:\n        result = stats.spearmanr(scores1, scores2)\n        return round(float(result.statistic), 4), round(float(result.pvalue), 4)\n    except Exception as e:\n        return f\"error: {str(e)}\", \"undefined\"\n\n# Extract metrics\nbucket_name = \"&lt;EVALUATION_OUTPUT_BUCKET&gt;\"\nfile_key1 = \"&lt;EVALUATION_FILE_KEY1&gt;\"\nfile_key2 = \"&lt;EVALUATION_FILE_KEY2&gt;\"\n\nmetrics1 = read_and_organize_metrics_from_s3(bucket_name, file_key1)\nmetrics2 = read_and_organize_metrics_from_s3(bucket_name, file_key2)\n\n# Calculate correlations for common metrics\ncommon_metrics = set(metrics1.keys()) &amp; set(metrics2.keys())\n\nfor metric_name in common_metrics:\n    scores1 = metrics1[metric_name]\n    scores2 = metrics2[metric_name]\n    \n    if len(scores1) == len(scores2):\n        correlation, p_value = get_spearmanr_correlation(scores1, scores2)\n        \n        print(f\"\\nMetric: {metric_name}\")\n        print(f\"Number of samples: {len(scores1)}\")\n        print(f\"Unique values in Model 1 scores: {len(set(scores1))}\")\n        print(f\"Unique values in Model 2 scores: {len(set(scores2))}\")\n        print(f\"Model 1 scores range: [{min(scores1)}, {max(scores1)}]\")\n        print(f\"Model 2 scores range: [{min(scores2)}, {max(scores2)}]\")\n        print(f\"Spearman correlation coefficient: {correlation}\")\n        print(f\"P-value: {p_value}\")\n    else:\n        print(f\"\\nMetric: {metric_name}\")\n        print(\"Error: Different number of samples between models\")</code></pre></div><h2>Best practices for LLM-as-a-judge implementation</h2><p>You can also compare multiple foundation models to determine which one works best for your needs. By using the same evaluator model across all comparisons, you’ll get consistent, scalable results. The following best practices will help you establish standardized benchmarking when comparing different foundation models.</p><ul><li>Create diverse test datasets that represent real-world use cases and edge cases. For large workloads (more than 1,000 prompts), use stratified sampling to maintain comprehensive coverage while managing costs and completion time. Include both simple and complex prompts to test model capabilities across different difficulty levels.</li><li>Choose evaluation metrics that align with your specific business objectives and application requirements. Balance quality metrics (correctness, completeness) with user experience metrics (helpfulness, coherence). Include safety metrics when deploying customer-facing applications.</li><li>Maintain consistent evaluation conditions when comparing different models. Use the same evaluator model across comparisons for standardized benchmarking. Document your evaluation configuration and parameters for reproducibility.</li><li>Schedule regular evaluation jobs to track model performance over time. Monitor trends across different metric categories to identify areas for improvement. Set up performance baselines and thresholds for each metric.</li><li>Optimize batch sizes based on your evaluation needs and cost constraints. Consider using smaller test sets for rapid iteration and larger sets for comprehensive evaluation. Balance evaluation frequency with resource utilization.</li><li>Maintain detailed records of evaluation jobs, including configurations and results. Track improvements and changes in model performance over time. Document any modifications made based on evaluation insights. The optional job description field can help you here.</li><li>Use evaluation results to guide model selection and optimization. Implement feedback loops to continuously improve prompt engineering. Regularly update evaluation criteria based on emerging requirements and user feedback.</li><li>Design your evaluation framework to accommodate growing workloads. Plan for increased complexity as you add more models or use cases. Consider automated workflows for regular evaluation tasks.</li></ul><p>These best practices help establish a robust evaluation framework using LLM-as-a-judge on Amazon Bedrock. For deeper insights into the scientific validation of these practices, including case studies and correlation with human judgments, stay tuned for our upcoming technical deep-dive blog post.</p><p>LLM-as-a-judge on Amazon Bedrock Model Evaluation represents a significant advancement in automated model assessment, offering organizations a powerful tool to evaluate and optimize their AI applications systematically. This feature combines the efficiency of automated evaluation with the nuanced understanding typically associated with human assessment, enabling organizations to scale their quality assurance processes while maintaining high standards of performance and safety.</p><p>The comprehensive metric categories, flexible implementation options, and seamless integration with existing AWS services make it possible for organizations to establish robust evaluation frameworks that grow with their needs. Whether you’re developing conversational AI applications, content generation systems, or specialized enterprise solutions, LLM-as-a-judge provides the necessary tools to make sure that your models align with both technical requirements and business objectives.</p><p>We’ve provided detailed implementation guidance, from initial setup to best practices, to help you use this feature effectively. The accompanying code samples and configuration examples in this post demonstrate how to implement these evaluations in practice. Through systematic evaluation and continuous improvement, organizations can build more reliable, accurate, and trustworthy AI applications.</p><p>We encourage you to explore LLM-as-a-judge capabilities in the Amazon Bedrock console and discover how automatic evaluation can enhance your AI applications. To help you get started, we’ve prepared a Jupyter notebook with practical examples and code snippets that you can find on our <a href=\"https://github.com/aws-samples/amazon-bedrock-samples/tree/main/evaluation-observe/bedrock-llm-as-judge-evaluation\" target=\"_blank\" rel=\"noopener\">GitHub repository</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/wale_picture_blog.png\" alt=\"\" width=\"100\" height=\"100\"> is a Sr. Data Scientist–Generative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/ishan.jpg\" alt=\"\" width=\"100\" height=\"99\"> is a Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building Generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/Badgephoto.jpeg\" alt=\"\" width=\"100\" height=\"115\"> is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.</p>","contentLength":26547,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From concept to reality: Navigating the Journey of RAG from proof of concept to production","url":"https://aws.amazon.com/blogs/machine-learning/from-concept-to-reality-navigating-the-journey-of-rag-from-proof-of-concept-to-production/","date":1739381272,"author":"Vivek Mittal","guid":127,"unread":true,"content":"<p>Generative AI has emerged as a transformative force, captivating industries with its potential to create, innovate, and solve complex problems. However, the journey from a proof of concept to a production-ready application comes with challenges and opportunities. Moving from proof of concept to production is about creating scalable, reliable, and impactful solutions that can drive business value and user satisfaction.</p><p>One of the most promising developments in this space is the rise of <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" target=\"_blank\" rel=\"noopener\">Retrieval Augmented Generation (RAG)</a> applications. RAG is the process of optimizing the output of a foundation model (FM), so it references a knowledge base outside of its training data sources before generating a response.</p><p>The following diagram illustrates a sample architecture.</p><p>In this post, we explore the movement of RAG applications from their proof of concept or minimal viable product (MVP) phase to full-fledged production systems. When transitioning a RAG application from a proof of concept to a production-ready system, optimization becomes crucial to make sure the solution is reliable, cost-effective, and high-performing. Let’s explore these optimization techniques in greater depth, setting the stage for future discussions on hosting, scaling, security, and observability considerations.</p><p>The diagram below illustrates the tradeoffs to consider for a production-ready RAG application.</p><p>The success of a production-ready RAG system is measured by its quality, cost, and latency. Machine learning (ML) engineers must make trade-offs and prioritize the most important factors for their specific use case and business requirements. For example, consider the use case of generating personalized marketing content for a luxury fashion brand. The brand might be willing to absorb the higher costs of using a more powerful and expensive FMs to achieve the highest-quality classifications, because misclassifications could lead to customer dissatisfaction and damage the brand’s reputation. Consider another use case of generating personalized product descriptions for an ecommerce site. The retailer might be willing to accept slightly longer latency to reduce infrastructure and operational costs, as long as the generated descriptions remain reasonably accurate and compelling. The optimal balance of quality, cost, and latency can vary significantly across different applications and industries.</p><p>Let’s look into practical guidelines on how you can enhance the overall quality of your RAG workflow, including the quality of the retriever and quality of the result generator using <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> and other features of <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a>. Amazon Bedrock Knowledge Bases provides a fully managed capability that helps you implement the entire RAG workflow from ingestion to retrieval and prompt augmentation without having to build custom integrations to data sources and manage data flows.</p><p>An effective evaluation framework is crucial for assessing and optimizing RAG systems as they move from proof of concept to production. These frameworks typically include overall metrics for a holistic assessment of the entire RAG pipeline, as well as specific diagnostic metrics for both the retrieval and generation components. This allows for targeted improvements in each phase of the system. By implementing a robust evaluation framework, developers can continuously monitor, diagnose, and enhance their RAG systems, achieving optimal performance across quality, cost, and latency dimensions as the application scales to production levels. <a href=\"https://aws.amazon.com/bedrock/evaluations/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Evaluations</a> can help you evaluate your retrieval or end-to-end RAG workflow in Amazon Bedrock Knowledge Bases. In the following sections, we discuss these specific metrics in different phases of the RAG workflow in more detail.</p><p>For better retrieval performance, the way the data is stored in the <a href=\"https://aws.amazon.com/blogs/machine-learning/dive-deep-into-vector-data-stores-using-amazon-bedrock-knowledge-bases/\" target=\"_blank\" rel=\"noopener\">vector store</a> has a big impact. For example, your input document might include tables within the PDF. In such cases, using an FM to parse the data will provide better results. You can use <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-advanced-parsing\" target=\"_blank\" rel=\"noopener\">advanced parsing options</a> supported by Amazon Bedrock Knowledge Bases for parsing non-textual information from documents using FMs. Many organizations store their data in structured formats within data warehouses and data lakes. Amazon Bedrock Knowledge Bases offers a feature that lets you connect your RAG workflow to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-build-structured.html\" target=\"_blank\" rel=\"noopener\">structured data stores</a>. This fully managed out-of-the-box RAG solution can help you natively query structured data from where it resides.</p><p>Another important consideration is the way your source document is split up into chunks. If your document would benefit from inherent relationships within your document, it might be wise to use <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-hiearchical-chunking\" target=\"_blank\" rel=\"noopener\">hierarchical chunking</a>, which allows for more granular and efficient retrieval. Some documents benefit from <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-semantic-chunking\" target=\"_blank\" rel=\"noopener\">semantic chunking</a> by preserving the contextual relationship in the chunks, helping make sure that the related information stays together in logical chunks. You can also use your own custom <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html#kb-custom-transformation\" target=\"_blank\" rel=\"noopener\">chunking strategy</a> for your RAG application’s unique requirements.</p><p>RAG applications process user queries by searching across a large set of documents. However, in many situations, you might need to retrieve documents with specific attributes or content. You can use <a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-knowledge-bases-now-supports-metadata-filtering-to-improve-retrieval-accuracy/\" target=\"_blank\" rel=\"noopener\">metadata filtering</a> to narrow down search results by specifying inclusion and exclusion criteria. Amazon Bedrock Knowledge Bases now also supports <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-knowledge-bases-auto-generated-query-filters-improved-retrieval/\" target=\"_blank\" rel=\"noopener\">auto generated query filters</a>, which extend the existing capability of manual metadata filtering by allowing you to narrow down search results without the need to manually construct complex filter expressions. This improves retrieval accuracy by making sure the documents are relevant to the query.</p><p>Writing an effective query is just as important as any other consideration for generation accuracy. You can add a <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html\" target=\"_blank\" rel=\"noopener\">prompt</a> providing instructions to the FM to provide an appropriate answer to the user. For example, a legal tech company would want to provide instructions to restrict the answers to be based on the input documents and not based on general information known to the FM. <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-test-config.html\" target=\"_blank\" rel=\"noopener\">Query decomposition</a> by splitting the input query into multiple queries is also helpful in retrieval accuracy. In this process, the subqueries with less semantic complexity might find more targeted chunks. These chunks can then be pooled and ranked together before passing them to the FM to generate a response.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-rerank-api-accuracy-rag-applications/\" target=\"_blank\" rel=\"noopener\">Reranking</a>, as a post-retrieval step, can significantly improve response quality. This technique uses LLMs to analyze the semantic relevance between the query and retrieved documents, reordering them based on their pertinence. By incorporating reranking, you make sure that only the most contextually relevant information is used for generation, leading to more accurate and coherent responses.</p><p>Adjusting <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html\" target=\"_blank\" rel=\"noopener\">inference parameters</a>, such as temperature and top-k/p sampling, can help in further refining the output.</p><p>You can use Amazon Bedrock Knowledge Bases to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-test-config.html\" target=\"_blank\" rel=\"noopener\">configure and customize</a> queries and response generation. You can also improve the relevance of your query responses with a <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/rerank.html\" target=\"_blank\" rel=\"noopener\">reranker model</a> in Amazon Bedrock.</p><p>The key metrics for retriever quality are <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">context precision</a>, <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">context recall</a>, and <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">context relevance</a>. Context precision measures how well the system ranks relevant pieces of information from the given context. It considers the question, ground truth, and context. Context recall provides the percentage of ground truth claims or key information covered by the retrieved context. Context relevance measures whether the retrieved passages or chunks are relevant for answering the given query, excluding extraneous details. Together, these three metrics offer insight into how effectively the retriever is able to surface the most relevant and focused source material to support a high-quality response.</p><p>Generator quality can be assessed through several key metrics. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Context utilization</a> examines how effectively the generator uses relevant information from the provided source material. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Noise sensitivity</a> gauges the generator’s propensity to include inaccurate details from the retrieved content. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Hallucination</a> measures the extent to which the generator produces incorrect claims not present in the source data. <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">Self-knowledge</a> reflects the proportion of accurate statements generated that can’t be found in the retrieved chunks. Finally, <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#generator-metrics\" target=\"_blank\" rel=\"noopener\">faithfulness</a> evaluates how closely the generator’s output aligns with the information contained in the source material.</p><p>For measuring the overall generation quality, the key metrics include measuring the <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#overall-metrics\" target=\"_blank\" rel=\"noopener\">precision</a>, <a href=\"https://github.com/amazon-science/RAGChecker/blob/main/tutorial/ragchecker_tutorial_en.md#overall-metrics\" target=\"_blank\" rel=\"noopener\">recall</a>, and <a href=\"https://aws.amazon.com/blogs/machine-learning/evaluate-the-reliability-of-retrieval-augmented-generation-applications-using-amazon-bedrock/\" target=\"_blank\" rel=\"noopener\">answer similarity</a>. Precision suggests the proportion of the correct claims in model’s response, whereas recall suggests the proportion of the ground truth claims covered by the model’s response. Answer similarity compares the meaning and content of a generated answer with a reference or ground truth answer. It evaluates how closely the generated answer matches the intended meaning of the ground truth answer.</p><p>Establishing a feedback loop with an evaluation framework against these quality metrics allows for continuous improvement, where the system can learn from user interactions and refine its performance over time. By optimizing these quality metrics, the RAG system can be designed to deliver reliable, cost-effective, and high-performing results for users.</p><p>Implementing responsible AI practices is crucial for maintaining ethical and safe deployment of RAG systems. This includes using guardrails to filter harmful content, deny certain topics, mask sensitive information, and ground responses in verified sources to reduce hallucinations.</p><p>Cost considers the compute resources and infrastructure required to run the system, and latency evaluates the response times experienced by end-users. To optimize cost and latency, implement <a href=\"https://aws.amazon.com/blogs/database/improve-speed-and-reduce-cost-for-generative-ai-workloads-with-a-persistent-semantic-cache-in-amazon-memorydb/\" target=\"_blank\" rel=\"noopener\">caching strategies</a> to reduce the need for expensive model inferences. Efficient <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html\" target=\"_blank\" rel=\"noopener\">query batching</a>&nbsp;can also improve overall throughput and reduce resource usage. Balance performance and resource usage to find the ideal configuration that meets your application’s requirements.</p><p>Use tools like Amazon Bedrock Knowledge Bases so you can take advantage of fully managed support for the end-to-end RAG workflow. It supports many of the <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/07/knowledge-bases-amazon-bedrock-advanced-rag-capabilities/\" target=\"_blank\" rel=\"noopener\">advanced RAG capabilities</a> we discussed earlier. By addressing these optimization techniques, you can transition your RAG-powered proof of concept to a robust, production-ready system that delivers high-quality, cost-effective, and low-latency responses to your users.</p><p>In addition to the server or compute layer, you will also need to consider an orchestration tool, testing environments, and a continuous integration and delivery (CI/CD) pipeline to streamline your application deployment. Having a feedback loop established based on the quality metrics along with a CI/CD pipeline is an important first step to creating self-healing architectures.</p><p>As your application grows, you will need to make sure your infrastructure can scale to meet the increasing demand. This can involve containerization with Docker or choosing serverless options, implementing load balancing, setting up auto scaling, and choosing between on-premises, cloud, or hybrid solutions. It also includes unique scaling requirements of your frontend application and backend generative AI workflow, as well as the use of content delivery networks (CDNs) and disaster recovery and backup strategies.</p><p>The following is a sample architecture for a secure and scalable RAG-based web application. This architecture uses Amazon ECS for hosting the service, <a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\" rel=\"noopener\">Amazon CloudFront</a> as a CDN, <a href=\"https://aws.amazon.com/waf/\" target=\"_blank\" rel=\"noopener\">AWS WAF</a> as a firewall, and <a href=\"https://aws.amazon.com/memorydb/\" target=\"_blank\" rel=\"noopener\">Amazon MemoryDB</a> for providing a semantic cache.</p><p>By carefully considering these aspects of hosting and scaling your infrastructure, you can build a resilient and adaptable system to support your growing web application or service. Stay tuned for more detailed information on these topics in upcoming blog posts.</p><h2>Data privacy, security, and observability</h2><p>Maintaining data privacy and security is of utmost importance. This includes implementing security measures at each layer of your application, from encrypting data in transit to setting up robust authentication and authorization controls. It also involves focusing on compute and storage security, as well as network security. Compliance with relevant regulations and regular security audits are essential. Securing your generative AI system is another crucial aspect. By default, Amazon Bedrock Knowledge Bases encrypts the traffic using AWS managed <a href=\"http://aws.amazon.com/kms\" target=\"_blank\" rel=\"noopener\">AWS Key Management Service</a> (AWS KMS) keys. You can also choose customer managed KMS keys for more control over encryption keys. For more information on application security, refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/safeguard-a-generative-ai-travel-agent-with-prompt-engineering-and-amazon-bedrock-guardrails/\" target=\"_blank\" rel=\"noopener\">Safeguard a generative AI travel agent with prompt engineering and Amazon Bedrock Guardrails</a>.</p><p>Comprehensive logging, monitoring, and maintenance are crucial to maintaining a healthy infrastructure. This includes setting up structured logging, centralized log management, real-time monitoring, and strategies for system updates and migrations.</p><p>By addressing these critical areas, you can build a secure and resilient infrastructure to support your growing web application or service. Stay tuned for more in-depth coverage of these topics in upcoming blog posts.</p><p>To successfully transition a RAG application from a proof of concept to a production-ready system, you should focus on optimizing the solution for reliability, cost-effectiveness, and high performance. Key areas to address include enhancing retriever and generator quality, balancing cost and latency, and establishing a robust and secure infrastructure.</p><p>By using purpose-built tools like Amazon Bedrock Knowledge Bases to streamline the end-to-end RAG workflow, organizations can successfully transition their RAG-powered proofs of concept into high-performing, cost-effective, secure production-ready solutions that deliver business value.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/07/vivek_phonetool.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Solution Architect at Amazon Web Services, where he helps organizations architect and implement cutting-edge cloud solutions. With a deep passion for Generative AI, Machine Learning, and Serverless technologies, he specializes in helping customers harness these innovations to drive business transformation. He finds particular satisfaction in collaborating with customers to turn their ambitious technological visions into reality.</p><p>&nbsp;is a Sr. Enterprise Solutions Architect at AWS, experienced in Software Engineering, Enterprise Architecture, and AI/ML. He is deeply passionate about exploring the possibilities of generative AI. He collaborates with customers to help them build well-architected applications on the AWS platform, and is dedicated to solving technology challenges and assisting with their cloud journey.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/07/mani.jpeg\" alt=\"\" width=\"100\" height=\"133\">&nbsp;is a Tech Lead – Generative AI Specialists, author of the book Applied Machine Learning and High-Performance Computing on AWS, and a member of the Board of Directors for Women in Manufacturing Education Foundation Board. She leads machine learning projects in various domains such as computer vision, natural language processing, and generative AI. She speaks at internal and external conferences such AWS re:Invent, Women in Manufacturing West, YouTube webinars, and GHC 23. In her free time, she likes to go for long runs along the beach.</p>","contentLength":15255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Current state of machine learning and intelligent systems","url":"https://www.datasciencecentral.com/current-state-of-machine-learning-and-intelligent-systems/","date":1739378698,"author":"Dan Wilson","guid":60,"unread":true,"content":"<p>Interview with Dr. Andrea Isoni – SHOW 16 Intelligent systems are evolving faster than ever, and keeping up with the latest advancements requires expertise, foresight, and a deep understanding of both the technological and regulatory landscapes. In&nbsp;AI Think Tank Podcast – Show 16, I had the pleasure of sitting down with&nbsp;Dr. Andrea Isoni, Director and…&nbsp;<a href=\"https://www.datasciencecentral.com/current-state-of-machine-learning-and-intelligent-systems/\" rel=\"bookmark\">Read More »</a></p>","contentLength":375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta’s New AI: Outrageously Good!","url":"https://www.youtube.com/watch?v=m6aaQoPv5r8","date":1739372626,"author":"Two Minute Papers","guid":328,"unread":true,"content":"<article>❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n📝 The paper \"VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models\" is available here:\nhttps://hila-chefer.github.io/videojam-paper.github.io/\n\nVs Veo2: https://x.com/TomLikesRobots/status/1888279188336963725\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu</article>","contentLength":1174,"flags":null,"enclosureUrl":"https://www.youtube.com/v/m6aaQoPv5r8?version=3","enclosureMime":"","commentsUrl":null},{"title":"5 LLM Prompting Techniques Every Developer Should Know","url":"https://www.kdnuggets.com/5-llm-prompting-techniques-every-developer-should-know","date":1739372427,"author":"Bala Priya C","guid":249,"unread":true,"content":"<article>Want to make the most out of large language models? Check out these prompting techniques you can start using today.</article>","contentLength":115,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/llm-prompting.png","enclosureMime":"","commentsUrl":null},{"title":"Top 5 Freelancer Websites Better Than Fiverr and Upwork","url":"https://www.kdnuggets.com/top-5-freelancer-websites-better-fiverr-upwork","date":1739365235,"author":"Abid Ali Awan","guid":248,"unread":true,"content":"<article>Discover freelancing platforms that care about you, not just your money, offering low commission rate, better policies, and higher earning potential.</article>","contentLength":149,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/awan_top_5_freelancer_websites_better_fiverr_upwork_4.png","enclosureMime":"","commentsUrl":null},{"title":"The difference between metrics and scorers","url":"https://www.youtube.com/watch?v=67KMDSitw1M","date":1739356864,"author":"probabl","guid":369,"unread":true,"content":"<article>You can easily write custom metric functions for scikit-learn, but you need to take an extra step if you want to use those metrics in a hyperparameter search. To add a number there, you need to use a scorer instead. The goal of this video is to explain the difference and to show how to leverage the `make_scorer` helper utility here.\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nBluesky: https://bsky.app/profile/probabl.bsky.social\nTwitter: https://x.com/probabl_ai\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":719,"flags":null,"enclosureUrl":"https://www.youtube.com/v/67KMDSitw1M?version=3","enclosureMime":"","commentsUrl":null},{"title":"Meta SAM 2.1 is now available in Amazon SageMaker JumpStart","url":"https://aws.amazon.com/blogs/machine-learning/meta-sam-2-1-is-now-available-in-amazon-sagemaker-jumpstart/","date":1739315351,"author":"Marco Punio","guid":126,"unread":true,"content":"<p><em>This blog post is co-written with George Orlin from Meta.</em></p><p>Today, we are excited to announce that Meta’s <a href=\"https://ai.meta.com/sam2/\" target=\"_blank\" rel=\"noopener\">Segment Anything Model (SAM) 2.1</a> vision segmentation model is publicly available through <a href=\"https://aws.amazon.com/sagemaker/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart</a> to deploy and run inference. Meta SAM 2.1 provides state-of-the-art video and image segmentation capabilities in a single model. This cutting-edge model supports long-context processing, complex segmentation scenarios, and fine-grained analysis, making it ideal for automating processes for various industries such as medical imaging in healthcare, satellite imagery for environment monitoring, and object segmentation for autonomous systems. Meta SAM 2.1 is well suited for zero-shot object segmentation and accurate object detection based on simple prompts such as point coordinates and bounding boxes in a frame for video tracking and image masking.</p><p>This model was predominantly trained on AWS, and AWS will also be the first cloud provider to make it available to customers. In this post, we walk through how to discover and deploy the Meta SAM 2.1 model using SageMaker JumpStart.</p><p>Meta SAM 2.1 is a state-of-the-art vision segmentation model designed for high-performance computer vision tasks, enabling advanced object detection and segmentation workflows. Building upon its <a href=\"https://segment-anything.com/\" target=\"_blank\" rel=\"noopener\">predecessor</a>, version 2.1 introduces enhanced segmentation accuracy, robust generalization across diverse datasets, and scalability for production-grade applications. These features enable AI researchers and developers in computer vision, image processing, and data-driven research to improve tasks that require detailed analysis segmentation across multiple fields.</p><p>Meta SAM 2.1 has a streamlined architecture that is optimized for integration with popular model-serving frameworks like TorchServe and can be deployed on <a href=\"https://aws.amazon.com/sagemaker-ai\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker AI</a> to power real-time or batch inference pipelines. Meta SAM 2.1 empowers organizations to achieve precise segmentation outcomes in vision-centric workflows with minimal configuration and maximum efficiency.</p><p>Meta SAM 2.1 offers multiple variants—Tiny, Small, Base Plus, and Large—available now on SageMaker JumpStart, balancing model size, speed, and segmentation performance to cater to diverse application needs.</p><h2>SageMaker JumpStart overview</h2><p>SageMaker JumpStart offers access to a broad selection of publicly available foundation models (FMs). These pre-trained models serve as powerful starting points that can be deeply customized to address specific use cases. You can now use state-of-the-art model architectures, such as language models, computer vision models, and more, without having to build them from scratch.</p><p>With SageMaker JumpStart, you can deploy models in a secure environment. Models hosted on JumpStart can be provisioned on dedicated SageMaker Inference instances, including <a href=\"https://aws.amazon.com/machine-learning/trainium/\" target=\"_blank\" rel=\"noopener\">AWS Trainium</a> and <a href=\"https://aws.amazon.com/machine-learning/inferentia/\" target=\"_blank\" rel=\"noopener\">AWS Inferentia</a> based instances, and are isolated within your virtual private cloud (VPC). This enforces data security and compliance, because the models operate under your own VPC controls, rather than in a shared public environment. After deploying an FM, you can further customize and fine-tune it using the extensive capabilities of SageMaker AI, including SageMaker Inference for deploying models and container logs for improved observability. With SageMaker AI, you can streamline the entire model deployment process.</p><p>Make sure you have the following prerequisites to deploy Meta SAM 2.1 and run inference:</p><h2>Discover Meta SAM 2.1 in SageMaker JumpStart</h2><p>SageMaker JumpStart provides FMs through two primary interfaces: SageMaker Studio and the <a href=\"https://github.com/aws/sagemaker-python-sdk\" target=\"_blank\" rel=\"noopener\">SageMaker Python SDK.</a> This provides multiple options to discover and use hundreds of models for your specific use case.</p><p>SageMaker Studio is a comprehensive IDE that offers a unified, web-based interface for performing all aspects of the machine learning (ML) development lifecycle. From preparing data to building, training, and deploying models, SageMaker Studio provides purpose-built tools to streamline the entire process. In SageMaker Studio, you can access SageMaker JumpStart to discover and explore the extensive catalog of FMs available for <a href=\"https://aws.amazon.com/blogs/machine-learning/reduce-model-deployment-costs-by-50-on-average-using-sagemakers-latest-features/\" target=\"_blank\" rel=\"noopener\">deployment to inference capabilities on SageMaker Inference</a>.</p><p>You can access the SageMaker JumpStart UI through either <a href=\"https://aws.amazon.com/sagemaker/unified-studio/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Unified Studio</a> or SageMaker Studio. To deploy Meta SAM 2.1 using the SageMaker JumpStart UI, complete the following steps:</p><p>In SageMaker Unified Studio, on the menu, choose .</p><p>If you’re already on the SageMaker Studio console, choose  in the navigation pane.</p><p>You will be prompted to create a project, after which you can begin deployment.</p><p>Alternatively, you can use the SageMaker Python SDK to programmatically access and use SageMaker JumpStart models. This approach allows for greater flexibility and integration with existing AI/ML workflows and pipelines. By providing multiple access points, SageMaker JumpStart helps you seamlessly incorporate pre-trained models into your AI/ML development efforts, regardless of your preferred interface or workflow.</p><h2>Deploy Meta SAM 2.1 for inference using SageMaker JumpStart</h2><p>On the SageMaker JumpStart landing page, you can discover the public pre-trained models offered by SageMaker AI. You can choose the Meta model provider tab to discover the Meta models available.</p><p>If you’re using SageMaker Studio and don’t see the SAM 2.1 models, update your SageMaker Studio version by shutting down and restarting. For more information about version updates, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-apps.html\" target=\"_blank\" rel=\"noopener\">Shut down and Update Studio Classic Apps</a>.</p><p>You can choose the model card to view details about the model such as license, data used to train, and how to use. You can also find two buttons,  and , which help you use the model.</p><p>When you choose , you should be prompted to the next screen to choose an endpoint name and instance type to initiate deployment.</p><p>Upon defining your endpoint settings, you can proceed to the next step to use the model.</p><h2>Deploy Meta SAM 2.1 vision segmentation model for inference using the Python SDK</h2><p>When you choose , model deployment will start. Alternatively, you can deploy through the example notebook by choosing . The notebook provides end-to-end guidance on how to deploy the model for inference and clean up resources.</p><p>To deploy using a notebook, you start by selecting an appropriate model, specified by the . You can deploy any of the selected models on SageMaker AI.</p><p>You can deploy a Meta SAM 2.1 vision segmentation model using SageMaker JumpStart with the following SageMaker Python SDK code:</p><div><pre><code>from sagemaker.jumpstart.model import JumpStartModel \nmodel = JumpStartModel(model_id = \"meta-vs-sam-2-1-hiera-tiny\") \npredictor = model.deploy()</code></pre></div><p>This deploys the model on SageMaker AI with default configurations, including default instance type and default VPC configurations. You can change these configurations by specifying non-default values in <a href=\"https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.jumpstart.model.JumpStartModel\" target=\"_blank\" rel=\"noopener\">JumpStartModel</a>. After it’s deployed, you can run inference against the deployed endpoint through the SageMaker predictor. There are three tasks that are available with this endpoint: automatic mask generator, image predictor, and video predictor. We provide a code snippet for each later in this post. To use the predictor, a certain payload schema needs to be followed. The endpoint has sticky sessions enabled, so to start inference, you need to send a  payload:</p><div><pre><code>def start_session(asset_type, asset_path):\n\n    asset_base64 = None\n    \n     with open(image_path, 'rb') as f:\n            asset_base64 = base64.b64encode(f.read()).decode('utf-8')\n    \n    response = predictor.invoke_endpoint(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n                    \"type\": \"start_session\",\n                    \"input_type\": asset_type,\n                    \"path\": asset_base64 \n                }),\n        SessionId=\"NEW_SESSION\",\n    )\n    \n    session_id = response.headers.get(\"x-amzn-sagemaker-new-session-id\")\n    \n    return session_id\n</code></pre></div><p>The  invocation needs an input media type of either image or video and the base64 encoded data of the media. This will launch a session with an instance of the model and load the media to be segmented.</p><p>To close a session, send a  invocation:</p><div><pre><code>def close_session(session_id):\n    response = predictor.invoke_endpoint(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n                    \"type\": \"close_session\",\n                    \"session_id\": session_id\n                }),\n        SessionId=session_id,\n    )\n    \n    session_id = response.headers.get(\"x-amzn-sagemaker-closed-session-id\")\n    \n    return session_id\n</code></pre></div><p>If <code>x-amzn-sagemaker-closed-session-id</code> exists as a header, then the session has been successfully closed.</p><p>To continue a session and retrieve the session ID of the existing session, the response header will have the <code>x-amzn-sagemaker-session-id</code> key with the current session ID for any operation that is not  or . Operations that aren’t  or  need to be invoked with a response stream. This is due to the size of the resulting payload being larger than what SageMaker real-time endpoints can return.</p><p>This is a basic example of interacting with the SAM 2.1 SageMaker JumpStart endpoint with sticky sessions. The following examples for each of the tasks reference these operations without repeating them. The returned data is of mime type JSONL. For more complete examples, refer to the example notebooks for Meta SAM 2.1 on SageMaker Jumpstart.</p><h2>Recommended instances and benchmarks</h2><p>The following table lists all the Meta SAM 2.1 models available in SageMaker JumpStart along with the , default instance types, and maximum number of total tokens (sum of number of input tokens and number of generated tokens) supported for each of these models. For increased context length, you can modify the default instance type in the SageMaker JumpStart UI.</p><table border=\"1px\" cellpadding=\"5px\"><tbody><tr></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-tiny</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-small</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-base-plus</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr><tr><td width=\"83\">meta-vs-sam-2-1-hiera-large</td><td width=\"161\">ml.g6.24xlarge (5.5 MB total image or video size)</td><td width=\"267\"></td></tr></tbody></table><h2>Meta SAM 2.1 use cases: Inference and prompt examples</h2><p>After you deploy the model using SageMaker JumpStart, you should be able to see a reference Jupyter notebook that references the parser and helper functions needed to begin using Meta SAM 2.1. After you follow those cells in the notebook, you should be ready to begin using the model’s vision segmentation capabilities.</p><p>Meta SAM 2.1 offers support for three different tasks (automatic mask generator, image predictor, video predictor) to generate masks for various objects in images, including object tracking in videos. In the following examples, we demonstrate how to use the automatic mask generator and image predictor on a JPG of a truck. This  file is stored in the  bucket; you can access it with the following code:</p><div><pre><code>s3_bucket = f\"jumpstart-cache-prod-{region}\"\nkey_prefix = \"inference-notebook-assets\"\n\ndef download_from_s3(key_filenames):\n    for key_filename in key_filenames:\n        s3.download_file(s3_bucket, f\"{key_prefix}/{key_filename}\", key_filename)\n        \ntruck_jpg = \"truck.jpg\"\n\n#Download images.\ndownload_from_s3(key_filenames=[truck_jpg])\ndisplay(Image(filename=truck_jpg))\n</code></pre></div><p>After you have your image and it is encoded, you can create masks for objects in the image. For use cases where you want to generate masks for every object in the image, you can use the automatic mask generator task.</p><p>The automatic mask generator is great for AI researchers for computer vision tasks and applications such as medical imaging and diagnostics to automatically segment regions of interest like tumors or specific organs to provide more accurate diagnostic support. Additionally, the automatic mask generator can be particularly useful in the autonomous vehicle space, in which it can segment out elements in a camera like pedestrians, vehicles, and other objects. Let’s use the automatic mask generator to generate masks for all the objects in .</p><p>The following code is the prompt to generate masks for your base64 encoded image:</p><div><pre><code># Start session\nsession_id = start_session(\"image\", truck_jpg)\n    \n# Generate and visualize masks with basic parameters\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"generate_automatic_masks\",\n            \"session_id\": session_id,\n            \"points_per_side\": 32,\n            \"min_mask_region_area\": 100\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n    \n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\nmasks = parser.get_responses()\n\n# End session\nend_session(session_id)\n</code></pre></div><p>We receive the following output (parsed and visualized).</p><p>Additionally, you can choose which objects in the provided image you want to create a mask for by adding points within that object for Meta SAM 2.1 to create. A use case for the image predictor can be valuable for tasks related to design and modeling by automating processes that typically require manual efforts. For example, the image predictor can automate turning 2D images into 3D models by analyzing 2D images of blueprints, sketches, or floor plans and generating preliminary 3D models. This is one of many examples of how the image predictor can act as a bridge between 2D and 3D construction across many different tasks. We use the following image with the points that we used to prompt Meta SAM 2.1 for masking the object.</p><p>The following code is used to prompt Meta SAM 2.1 and plot the coordinates:</p><div><pre><code># Start session\nsession_id = start_session(\"image\", truck_jpg)\n\npoints = [\n            {\"type\": \"point\", \"coordinates\": [500, 375], \"label\": 1},\n            {\"type\": \"point\", \"coordinates\": [1125, 625], \"label\": 1}\n         ]\n    \n# Add multiple points\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"add_points\",\n            \"session_id\": session_id,\n            \"points\": [p[\"coordinates\"] for p in points],\n            \"labels\": [p[\"label\"] for p in points],\n            \"clear_old_points\": clear_old_point,\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\n# Intermediate Response\nmasks = parser.get_responses()\n    \nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"predict\",\n            \"session_id\": session_id,\n            \"multimask_output\": True,\n            \"return_logits\": True\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\nmasks = parser.get_responses()\n\n# End session\nend_session(session_id)\n</code></pre></div><p>We receive the following output (parsed and visualized).</p><p>We now demonstrate how to prompt Meta SAM 2.1 for object tracking on video. One use case would be for ergonomic data collection and training purposes. You can use the video predictor to analyze the movement and posture of humans in real time, serving as a way to reduce injury and improve performance by setting alarms for bad posture or movements. Let’s start by accessing the  file [1] from the  S3 bucket defined in the following code:</p><div><pre><code>basketball_mp4 = \"basketball-layup.mp4\"\n\n#Download video\ndownload_from_s3(key_filenames=[basketball_mp4])\ndisplay(Video(filename=basketball_mp4))</code></pre></div><p>The following code shows how you can set up the prompt format to track objects in the video. The first object will use coordinates to track and not track, and the second object will track one coordinate.</p><div><pre><code># Start session\nsession_id = start_session(\"video\", basketball_mp4)\n\n# Object 1\nprompts1 = [\n        {\"type\": \"point\", \"coordinates\": [1478, 649], \"label\": 1},\n        {\"type\": \"point\", \"coordinates\": [1433, 689], \"label\": 0},\n    ]\n    \n# Extract points and labels\npoints = []\nlabels = []\nfor prompt in prompts1:\n    if prompt[\"type\"] == \"point\":\n        points.append(prompt[\"coordinates\"])\n        labels.append(prompt[\"label\"])\n\nrequest = {\n        \"type\": \"add_points\",\n        \"session_id\": session_id,\n        \"frame_index\": 0,\n        \"object_id\": 1,\n        \"points\": points,\n        \"labels\": labels,\n        \"clear_old_points\": True,\n    }\n    \n# Add multiple points\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps(request),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\n# Intermediate Response\nmasks = parser.get_responses()\n\n# Object 2\nprompts2 = [{\"type\": \"point\", \"coordinates\": [1433, 689], \"label\": 1}]\n\n# Extract points and labels\npoints = []\nlabels = []\nfor prompt in prompts2:\n    if prompt[\"type\"] == \"point\":\n        points.append(prompt[\"coordinates\"])\n        labels.append(prompt[\"label\"])\n\nrequest = {\n        \"type\": \"add_points\",\n        \"session_id\": session_id,\n        \"frame_index\": 0,\n        \"object_id\": 2,\n        \"points\": points,\n        \"labels\": labels,\n        \"clear_old_points\": True,\n    }\n    \n# Add multiple points\nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps(request),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\n# Intermediate Response\nmasks = parser.get_responses()\n    \nresponse = runtime_client.invoke_endpoint_with_response_stream(\n        EndpointName=endpoint_name,\n        ContentType='application/json',\n        Body=json.dumps({\n            \"type\": \"propagate_in_video\",\n            \"session_id\": session_id,\n            \"start_frame_index\": 0,\n        }),\n        SessionId=session_id,\n        Accept=\"application/jsonlines\"\n    )\n\n# Parse response stream\nparser = StreamParser()\nfor event in response['Body']:\n    parser.write(event)\n\nmasks = parser.get_responses()\n\n# End session\nend_session(session_id)\n</code></pre></div><p>We receive the following output (parsed and visualized).</p><p>Here we can see that Meta SAM 2.1 Tiny was able to successfully track the objects based off the coordinates that were provided in prompt.</p><p>To avoid incurring unnecessary costs, when you’re done, delete the SageMaker AI endpoints using the following code:</p><div><pre><code>predictor.delete_model()\npredictor.delete_endpoint()</code></pre></div><p>Alternatively, to use the SageMaker AI console, complete the following steps:</p><ol><li>On the SageMaker AI console, under in the navigation pane, choose</li><li>Search for the embedding and text generation endpoints.</li><li>On the endpoint details page, choose .</li><li>Choose  again to confirm.</li></ol><p>In this post, we explored how SageMaker JumpStart empowers data scientists and ML engineers to discover, access, and deploy a wide range of pre-trained FMs for inference, including Meta’s most advanced and capable models to date. Get started with SageMaker JumpStart and Meta SAM 2.1 models today. For more information about SageMaker JumpStart, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html\" target=\"_blank\" rel=\"noopener\">SageMaker JumpStart pretrained models</a> and <a href=\"https://aws.amazon.com/sagemaker/jumpstart/getting-started/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&amp;sagemaker-jumpstart-cards.sort-order=asc&amp;awsf.sagemaker-jumpstart-filter-product-type=*all&amp;awsf.sagemaker-jumpstart-filter-text=*all&amp;awsf.sagemaker-jumpstart-filter-vision=*all&amp;awsf.sagemaker-jumpstart-filter-tabular=*all&amp;awsf.sagemaker-jumpstart-filter-audio-tasks=*all&amp;awsf.sagemaker-jumpstart-filter-multimodal=*all&amp;awsf.sagemaker-jumpstart-filter-RL=*all\" target=\"_blank\" rel=\"noopener\">Getting started with Amazon SageMaker JumpStart</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/08/26/Marco-Punio.jpg\" alt=\"\" width=\"100\" height=\"134\"><a href=\"https://www.linkedin.com/in/marcpunio/\" target=\"_blank\" rel=\"noopener\">Marco Punio</a> is a Sr. Specialist Solutions Architect focused on generative AI strategy, applied AI solutions, and conducting research to help customers hyper-scale on AWS. As a member of the 3rd Party Model Provider Applied Sciences Solutions Architecture team at AWS, he is a Global Lead for the Meta – AWS Partnership and technical strategy. Based in Seattle, WA, Marco enjoys writing, reading, exercising, and building applications in his free time.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/11/deepak-rupakula-100.jpg\" alt=\"\" width=\"100\" height=\"140\"><a href=\"https://www.linkedin.com/in/deepak-rupakula-042b956/\" target=\"_blank\" rel=\"noopener\">Deepak Rupakula</a> is a Principal GTM lead in the specialists group at AWS. He focuses on developing GTM strategy for large language models like Meta across AWS services like Amazon Bedrock and Amazon SageMaker AI. With over 15 years of experience in the tech industry, his experience includes leadership roles in product management, customer success, and analytics.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/11/harish-rao-100.jpg\" alt=\"\" width=\"100\" height=\"122\"><a href=\"https://www.linkedin.com/in/harishvs/\" target=\"_blank\" rel=\"noopener\">Harish Rao</a> is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He empowers customers to harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/11/baladithya-balamurugan-100.jpg\" alt=\"\" width=\"100\" height=\"120\"><a href=\"https://www.linkedin.com/in/baladithyabalamurugan/\" target=\"_blank\" rel=\"noopener\">Baladithya Balamurugan</a> is a Solutions Architect at AWS focused on ML deployments for inference and using AWS Neuron to accelerate training and inference. He works with customers to enable and accelerate their ML deployments on services such as Amazon SageMaker AI and Amazon EC2. Based in San Francisco, Baladithya enjoys tinkering, developing applications, and building his homelab in his free time.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/12/01/Banu-Nagasundaram.jpg\" alt=\"\" width=\"101\" height=\"140\"><a href=\"https://www.linkedin.com/in/banunagasundaram/\" target=\"_blank\" rel=\"noopener\">Banu Nagasundaram</a> leads product, engineering, and strategic partnerships for Amazon SageMaker JumpStart, SageMaker AI’s machine learning and generative AI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/10/naman.png\" alt=\"\" width=\"100\" height=\"133\"><a href=\"https://www.linkedin.com/in/namannandan/\" target=\"_blank\" rel=\"noopener\">Naman Nandan</a> is a software development engineer at AWS, specializing in enabling large-scale AI/ML inference workloads on Amazon SageMaker AI using TorchServe, a project jointly developed by AWS and Meta. In his free time, he enjoys playing tennis and going on hikes.</p>","contentLength":21819,"flags":null,"enclosureUrl":"https://d2908q01vomqb2.cloudfront.net/artifacts/DBSBlogs/ML-17926/basketball-layup-masked.mp4","enclosureMime":"","commentsUrl":null},{"title":"Falcon 3 models now available in Amazon SageMaker JumpStart","url":"https://aws.amazon.com/blogs/machine-learning/falcon-3-models-now-available-in-amazon-sagemaker-jumpstart/","date":1739312187,"author":"Niithiyn Vijeaswaran","guid":125,"unread":true,"content":"<h2>Overview of the Falcon 3 family of models</h2><p>The Falcon 3 family, developed by Technology Innovation Institute (TII) in Abu Dhabi, represents a significant advancement in open source language models. This collection includes five base models ranging from 1 billion to 10 billion parameters, with a focus on enhancing science, math, and coding capabilities. The family consists of Falcon3-1B-Base, Falcon3-3B-Base, Falcon3-Mamba-7B-Base, Falcon3-7B-Base, and Falcon3-10B-Base along with their instruct variants.</p><p>These models showcase innovations such as efficient pre-training techniques, scaling for improved reasoning, and knowledge distillation for better performance in smaller models. Notably, the Falcon3-10B-Base model achieves state-of-the-art performance for models under 13 billion parameters in zero-shot and few-shot tasks. The Falcon 3 family also includes various fine-tuned versions like Instruct models and supports different quantization formats, making them versatile for a wide range of applications.</p><p>Currently, SageMaker JumpStart offers the base versions of Falcon3-3B, Falcon3-7B, and Falcon3-10B, along with their corresponding instruct variants, as well as Falcon3-1B-Instruct.</p><h2>Get started with SageMaker JumpStart</h2><p>SageMaker JumpStart is a machine learning (ML) hub that can help accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select pre-trained foundation models (FMs), including Falcon 3 models. These models are fully customizable for your use case with your data.</p><p>Deploying a Falcon 3 model through SageMaker JumpStart offers two convenient approaches: using the intuitive SageMaker JumpStart UI or implementing programmatically through the SageMaker Python SDK. Let’s explore both methods to help you choose the approach that best suits your needs.</p><h2>Deploy Falcon 3 using the SageMaker JumpStart UI</h2><p>Complete the following steps to deploy Falcon 3 through the JumpStart UI:</p><h2>Deploy Falcon 3 programmatically using the SageMaker Python SDK</h2><p>For teams looking to automate deployment or integrate with existing MLOps pipelines, you can use the SageMaker Python SDK:</p><div><pre><code>from sagemaker.serve.builder.model_builder import ModelBuilder\nfrom sagemaker.serve.builder.schema_builder import SchemaBuilder\nfrom sagemaker.jumpstart.model import ModelAccessConfig\nfrom sagemaker.session import Session\nimport logging\n\nsagemaker_session = Session()\n\nartifacts_bucket_name = sagemaker_session.default_bucket()\nexecution_role_arn = sagemaker_session.get_caller_identity_arn()\n\n\njs_model_id = \"huggingface-llm-falcon-3-10B-base\"\n\ngpu_instance_type = \"ml.g5.12xlarge\"  \n\nresponse = \"Hello, I'm a language model, and I'm here to help you with your English.\"\n\nsample_input = {\n    \"inputs\": \"Hello, I'm a language model,\",\n    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n}\n\nsample_output = [{\"generated_text\": response}]\n\nschema_builder = SchemaBuilder(sample_input, sample_output)\n\nmodel_builder = ModelBuilder(\n    model=js_model_id,\n    schema_builder=schema_builder,\n    sagemaker_session=sagemaker_session,\n    role_arn=execution_role_arn,\n    log_level=logging.ERROR\n)\n\nmodel= model_builder.build()\n\npredictor = model.deploy(model_access_configs={js_model_id:ModelAccessConfig(accept_eula=True)}, accept_eula=True)</code></pre></div><p>Run inference on the predictor:</p><div><pre><code>predictor.predict(sample_input)</code></pre></div><p>To clean up the model and endpoint, use the following code:</p><div><pre><code>predictor.delete_model()\npredictor.delete_endpoint()</code></pre></div><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/05/30/NIITHIYN_ARP-2-1.jpg\" alt=\"\" width=\"100\" height=\"100\">&nbsp;is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is generative AI and AWS AI Accelerators. He holds a Bachelor’s degree in Computer Science and Bioinformatics.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/27/ml-17555-karpmar.jpg\" alt=\"Marc Karp\" width=\"100\" height=\"114\"> is an ML Architect with the Amazon SageMaker Service team. He focuses on helping customers design, deploy, and manage ML workloads at scale. In his spare time, he enjoys traveling and exploring new places.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/27/ml-17555-raghu.png\" alt=\"Raghu\" width=\"100\" height=\"100\"> is a Senior ML Solutions Architect with the Amazon SageMaker Service team. He focuses on helping customers build, deploy, and migrate ML production workloads to SageMaker at scale. He specializes in machine learning, AI, and computer vision domains, and holds a master’s degree in Computer Science from UT Dallas. In his free time, he enjoys traveling and photography.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2020/12/01/Banu-Nagasundaram.jpg\" alt=\"\" width=\"101\" height=\"140\"> leads product, engineering, and strategic partnerships for SageMaker JumpStart, SageMaker’s machine learning and GenAI hub. She is passionate about building solutions that help customers accelerate their AI journey and unlock business value.</p>","contentLength":4499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a virtual meteorologist using Amazon Bedrock Agents","url":"https://aws.amazon.com/blogs/machine-learning/building-a-virtual-meteorologist-using-amazon-bedrock-agents/","date":1739307184,"author":"Salman Ahmed","guid":124,"unread":true,"content":"<p>The integration of <a href=\"https://aws.amazon.com/ai/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a> capabilities is driving transformative changes across many industries. Although weather information is accessible through multiple channels, businesses that heavily rely on meteorological data require robust and scalable solutions to effectively manage and use these critical insights and reduce manual processes. This solution demonstrates how to create an AI-powered virtual meteorologist that can answer complex weather-related queries in natural language. We use various AWS services to deploy a complete solution that you can use to interact with an API providing real-time weather information. In this solution, we use <a href=\"https://aws.amazon.com/bedrock/agents/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Agents</a>.</p><p>Amazon Bedrock Agents helps to streamline workflows and automate repetitive tasks. Amazon Bedrock Agents can securely connect to your company’s data sources and augments the user’s request with accurate responses. You can use Amazon Bedrock Agents to architect an action schema tailored to your requirements, granting you control whenever the agent initiates the specified action. This versatile approach equips you to seamlessly integrate and execute business logic within your preferred backend service, fostering a cohesive combination of functionality and flexibility. There is also memory retention across the interaction allowing a more personalized user experience.</p><p>In this post, we present a streamlined approach to deploying an AI-powered agent by combining Amazon Bedrock Agents and a <a href=\"https://aws.amazon.com/what-is/foundation-models/\" target=\"_blank\" rel=\"noopener\">foundation model</a> (FM). We guide you through the process of configuring the agent and implementing the specific logic required for the virtual meteorologist to provide accurate weather-related responses. Additionally, we use various AWS services, including <a href=\"https://aws.amazon.com/amplify/\" target=\"_blank\" rel=\"noopener\">AWS Amplify</a> for hosting the front end, <a href=\"https://aws.amazon.com/lambda/\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> functions for handling request logic, <a href=\"https://aws.amazon.com/cognito/\" target=\"_blank\" rel=\"noopener\">Amazon Cognito</a> for user authentication, and <a href=\"https://aws.amazon.com/iam/\" target=\"_blank\" rel=\"noopener\">AWS Identity and Access Management</a> (IAM) for controlling access to the agent.</p><p>The diagram gives an overview and highlights the key components. The architecture uses Amazon Cognito for user authentication and Amplify as the hosting environment for our front-end application. Amazon Bedrock Agents forwards the details from the user query to the action groups, which further invokes custom Lambda functions. Each action group and Lambda function handles a specific task:</p><ol><li> – Processes geographic coordinates (geo-coordinates) to get details about a specific location</li><li> Gathers weather information for the provided location</li><li> Obtains the current date and time</li></ol><p>You must have the following in place to complete the solution in this post:</p><h2>Deploy solution resources using AWS CloudFormation</h2><p>When you run the AWS CloudFormation template, the following resources are deployed (note that costs will be incurred for the AWS resources used):</p><ul><li>Lambda resources: \n  <ul><li>Function – <code>&lt;Stack name&gt;-geo-coordinates-&lt;auto-generated&gt;</code></li><li>Function – <code>&lt;Stack name&gt;-weather-&lt;auto-generated&gt;</code></li><li>Function – <code>&lt;Stack name&gt;-date-time-&lt;auto-generated&gt;</code></li></ul></li><li>Amazon Bedrock Agents: virtual-meteorologist \n  <ul><li>Action groups (1) – <code>obtain-latitude-longitude-from-place-name</code></li><li>Action groups (2) – <code>obtain-weather-information-with-coordinates</code></li><li>Action groups (3) – <code>get-current-date-time-from-timezone</code></li></ul></li></ul><p>After you deploy the CloudFormation template, copy the following from the tab on the <a href=\"https://console.aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener\">CloudFormation console</a> to be used during the configuration of your application after it’s deployed in AWS Amplify.</p><ul></ul><h2>Deploy the AWS Amplify application</h2><p>You need to manually deploy the Amplify application using the front-end code found on GitHub. Complete the following steps:</p><ol><li>Download the front-end code AWS-Amplify-Frontend.zip from <a href=\"https://github.com/aws-samples/virtual-meteorologist-using-amazon-bedrock-agents\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</li><li>Use the .zip file to manually <a href=\"https://docs.aws.amazon.com/amplify/latest/userguide/manual-deploys.html\" target=\"_blank\" rel=\"noopener\">deploy</a> the application in Amplify.</li><li>Return to the Amplify page and use the domain it automatically generated to access the application.</li></ol><h3>Use Amazon Cognito for user authentication</h3><p>Amazon Cognito is an identity service that you can use to authenticate and authorize users. We use Amazon Cognito in our solution to verify the user before they can use the application. We also use identity pool to provide temporary AWS credentials for the user while they interact with Amazon Bedrock API.</p><h3>Use Amazon Bedrock Agents to automate application tasks</h3><p>With Amazon Bedrock Agents, you can build and configure autonomous agents in your application. An agent helps your end users complete actions based on organization data and user input. Agents orchestrate interactions between FMs, data sources, software applications, and user conversations.</p><h3>Use action group to define actions that Amazon Bedrock agents perform</h3><p>An action group defines a set of related actions that an Amazon Bedrock agent can perform to assist users. When configuring an action group, you have options for handling user-provided information, including adding user input to the agent’s action group, passing data to a Lambda function for custom business logic, or returning control directly through the InvokeAgent response. In our application, we created three action groups to give the Amazon Bedrock agent these essential functionalities: retrieving coordinates for specific locations, obtaining current date and time information, and fetching weather data for given locations. These action groups enable the agent to access and process crucial information, enhancing its ability to respond accurately and comprehensively to user queries related to location-based services and weather conditions.</p><h3>Use Lambda for Amazon Bedrock action group</h3><p>As part of this solution, three Lambda functions are deployed to support the action groups defined for our Amazon Bedrock agent:</p><ol><li><strong>Location coordinates Lambda function</strong> – This function is triggered by the <code>obtain-latitude-longitude-from-place-name</code> action group. It takes a place name as input and returns the corresponding latitude and longitude coordinates. The function uses a geocoding service or database to perform this lookup.</li><li><strong>Date and time Lambda function</strong> – Invoked by the <code>get-current-date-time-from-timezone</code> action group, this function provides the current date and time information.</li><li><strong>Weather information Lambda function</strong> – This function is called by the <code>obtain-weather-information-with-coordinates</code> action group. It accepts geo-coordinates from the first Lambda function and returns current weather conditions and forecasts for the specified area. This Lambda function used a weather API to fetch up-to-date meteorological data.</li></ol><p>Each of these Lambda functions receives an input event containing relevant metadata and populated fields from the Amazon Bedrock agent’s API operation or function parameters. The functions process this input, perform their specific tasks, and return a response with the required information. This response is then used by the Amazon Bedrock agent to formulate its reply to the user’s query. By using these Lambda functions, our Amazon Bedrock agent gains the ability to access external data sources and perform complex computations, significantly enhancing its capabilities in handling user requests related to location, time, and weather information.</p><h3>Use AWS Amplify for front-end code</h3><p>Amplify offers a development environment for building secure, scalable mobile and web applications. Developers can focus on their code rather than worrying about the underlying infrastructure. Amplify also integrates with many Git providers. For this solution, we manually upload our front-end code using the method outlined earlier in this post.</p><p>Navigate to the URL provided after you created the application in Amplify. Upon accessing the application URL, you’ll be prompted to provide information related to Amazon Cognito and Amazon Bedrock Agents. This information is required to securely authenticate users and allow the front end to interact with the Amazon Bedrock agent. It enables the application to manage user sessions and make authorized API calls to AWS services on behalf of the user.</p><p>You can enter information with the values you collected from the CloudFormation stack outputs. You’ll be required to enter the following fields, as shown in the following screenshot:</p><ul></ul><p>You need to sign in with your username and password. A temporary password was automatically generated during deployment and sent to the email address you provided when launching the CloudFormation template. At first sign-in attempt, you’ll be asked to reset your password, as shown in the following video.</p><p>Now you can start asking questions in the application, for example, “Can we do barbecue today in Dallas, TX?” In a few seconds, the application will provide you detailed results mentioning if you can do barbecue in Dallas, TX. The following video shows this chat.</p><p>Here are a few sample queries to demonstrate the capabilities of your virtual meteorologist:</p><ol><li>“What’s the weather like in New York City today?”</li><li>“Should I plan an outdoor birthday party in Miami next weekend?”</li><li>“Will it snow in Denver on Christmas Day?”</li><li>“Can I go swimming on a beach in Chicago today?</li></ol><p>These queries showcase the agent’s ability to provide current weather information, offer advice based on weather forecasts, and predict future weather conditions. You can even ask a question related to an activity such as swimming, and it will answer based on the weather conditions if that activity is okay to do.</p><p>If you decide to discontinue using the virtual meteorologist, you can follow these steps to remove it, its associated resources deployed using AWS CloudFormation, and the Amplify deployment:</p><ol><li>Delete the CloudFormation stack: \n  <ol><li>On the AWS CloudFormation console, choose  in the navigation pane.</li><li>Locate the stack you created during the deployment process (you assigned a name to it).</li><li>Select the stack and choose .</li></ol></li><li>Delete the Amplify application and its resources. For instructions, refer to <a href=\"https://aws.amazon.com/getting-started/hands-on/build-web-app-s3-lambda-api-gateway-dynamodb/module-six/\" target=\"_blank\" rel=\"noopener\">Clean Up Resources</a>.</li></ol><p>This solution demonstrates the power of combining Amazon Bedrock Agents with other AWS services to create an intelligent, conversational weather assistant. By using AI and cloud technologies, businesses can automate complex queries and provide valuable insights to their users.</p><p>To learn more about Amazon Bedrock, refer to the following resources:</p><p>To learn more about the Anthropic’s Claude 3.5 Sonnet model, refer to the following resources:</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/salmanah.jpg\" alt=\"Salman Ahmed\" width=\"120\" height=\"160\"> is a Senior Technical Account Manager in AWS Enterprise Support. He enjoys helping customers in the travel and hospitality industry to design, implement, and support cloud infrastructure. With a passion for networking services and years of experience, he helps customers adopt various AWS networking services. Outside of work, Salman enjoys photography, traveling, and watching his favorite sports teams.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/sercast.jpg\" alt=\"Sergio Barraza\" width=\"120\" height=\"160\"> is a Senior Enterprise Support Lead at AWS, helping energy customers design and optimize cloud solutions. With a passion for software development, he guides energy customers through AWS service adoption. Outside work, Sergio is a multi-instrument musician playing guitar, piano, and drums, and he also practices Wing Chun Kung Fu.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/vatsravi.jpg\" alt=\"Ravi Kumar\" width=\"120\" height=\"160\"> is a Senior Technical Account Manager in AWS Enterprise Support who helps customers in the travel and hospitality industry to streamline their cloud operations on AWS. He is a results-driven IT professional with over 20 years of experience. In his free time, Ravi enjoys creative activities like painting. He also likes playing cricket and traveling to new places.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/10/16/goyalaws.jpg\" alt=\"Ankush Goyal\" width=\"120\" height=\"160\"> is a Enterprise Support Lead in AWS Enterprise Support who helps customers streamline their cloud operations on AWS. He is a results-driven IT professional with over 20 years of experience.</p>","contentLength":11490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon Q Business simplifies integration of enterprise knowledge bases at scale","url":"https://aws.amazon.com/blogs/machine-learning/amazon-q-business-simplifies-integration-of-enterprise-knowledge-bases-at-scale/","date":1739293892,"author":"Omar Elkharbotly","guid":123,"unread":true,"content":"<p>In this new era of emerging AI technologies, we have the opportunity to build AI-powered assistants tailored to specific business requirements. <a href=\"https://aws.amazon.com/q/business/\" target=\"_blank\" rel=\"noopener\">Amazon Q Business</a>, a new generative AI-powered assistant, can answer questions, provide summaries, generate content, and securely complete tasks based on data and information in an enterprise’s systems.</p><p>Large-scale data ingestion is crucial for applications such as document analysis, summarization, research, and knowledge management. These tasks often involve processing vast amounts of documents, which can be time-consuming and labor-intensive. However, ingesting large volumes of enterprise data poses significant challenges, particularly in orchestrating workflows to gather data from diverse sources.</p><p>In this post, we propose an end-to-end solution using Amazon Q Business to simplify integration of enterprise knowledge bases at scale.</p><h2>Enhancing AWS Support Engineering efficiency</h2><p>The AWS Support Engineering team faced the daunting task of manually sifting through numerous tools, internal sources, and AWS public documentation to find solutions for customer inquiries. For complex customer issues, the process was especially time-consuming, laborious, and at times extended the wait time for customers seeking resolutions. To address this, the team implemented a chat assistant using Amazon Q Business. This solution ingests and processes data from hundreds of thousands of support tickets, escalation notices, public AWS documentation, re:Post articles, and AWS blog posts.</p><p>By using Amazon Q Business, which simplifies the complexity of developing and managing ML infrastructure and models, the team rapidly deployed their chat solution. The Amazon Q Business pre-built <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/connectors-list.html\" target=\"_blank\" rel=\"noopener\">connectors</a> like <a href=\"http://aws.amazon.com/s3\" target=\"_blank\" rel=\"noopener\">Amazon Simple Storage Service</a> (Amazon S3), document retrievers, and upload capabilities streamlined data ingestion and processing, enabling the team to provide swift, accurate responses to both basic and advanced customer queries.</p><p>In this post, we propose an end-to-end solution using Amazon Q Business to address similar enterprise data challenges, showcasing how it can streamline operations and enhance customer service across various industries. First we discuss end-to-end large-scale data integration with Amazon Q Business, covering data preprocessing, security guardrail implementation, and Amazon Q Business best practices. Then we introduce the solution deployment using three <a href=\"http://aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener\">AWS CloudFormation</a> templates.</p><p>The following architecture diagram represents the high-level design of a solution proven effective in production environments for AWS Support Engineering. This solution uses the powerful capabilities of Amazon Q Business. We will walk through the implementation of key components, including configuring enterprise data sources to build our knowledge base, document indexing and boosting, and implementing comprehensive security controls.</p><ul><li> – An end-user who accesses Amazon Q Business applications with permissions granted by their administrator to perform their job duties</li><li> – A user who manages Amazon Q Business resources and determines feature access for service users within the organization</li><li> – A user responsible for creating and managing access policies for Amazon Q Business through <a href=\"https://aws.amazon.com/iam/identity-center/\" target=\"_blank\" rel=\"noopener\">AWS IAM Identity Center</a></li></ul><p>The following workflow details how a service user accesses the application:</p><ol><li>The service user initiates an interaction with the Amazon Q Business application, accessible through the web experience, which is an endpoint URL.</li><li>The service user’s permissions are authenticated using IAM Identity Center, an AWS solution that connects workforce users to AWS managed applications like Amazon Q Business. It enables end-user authentication and streamlines access management.</li><li>The authenticated service user submits queries in natural language to the Amazon Q Business application.</li><li>The Amazon Q Business application generates and returns answers drawing from the enterprise data uploaded to an S3 bucket, which is connected as a data source to Amazon Q Business. This S3 bucket data is continuously refreshed, making sure that Amazon Q Business accesses the most current information for query responses by using a retriever to pull data from the index.</li></ol><h2>Large-scale data ingestion</h2><p>Before ingesting the data to Amazon Q Business, the data might need transformation into <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/doc-types.html\" target=\"_blank\" rel=\"noopener\">formats</a> supported by Amazon Q Business. Furthermore, it might contain sensitive data or personally identifiable information (PII) requiring redaction. These <a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/data-ingestion-methods.html\" target=\"_blank\" rel=\"noopener\">data ingestion</a> challenges create a need to orchestrate tasks like transformation, redaction, and secure ingestion.</p><p>To facilitate orchestration, this solution incorporates <a href=\"https://aws.amazon.com/step-functions/\" target=\"_blank\" rel=\"noopener\">AWS Step Functions</a>. Step Functions provides a visual workflow service to orchestrate tasks and workloads resiliently and efficiently through built-in AWS integrations and error handling. The solution uses the Step Functions <a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/state-map.html\" target=\"_blank\" rel=\"noopener\">Map state</a>, which allows for parallel processing of multiple items in a dataset, thereby efficiently orchestrating workflows and speeding up overall processing.</p><p>The following diagram illustrates an example architecture for ingesting data through an endpoint interfacing with a large corpus.</p><p>Step Functions orchestrates AWS services like <a href=\"https://aws.amazon.com/pm/lambda/\" target=\"_blank\" rel=\"noopener\">AWS Lambda</a> and organization APIs like DataStore to ingest, process, and store data securely. The workflow includes the following steps:</p><ol><li>The Prepare Map Input Lambda function prepares the required input for the Map state. For example, the Datastore API might require certain input like date periods to query data. This step can be used to define the date periods to be used by the Map state as an input.</li><li>The Ingest Data Lambda function fetches data from the Datastore API—which can be in or outside of the virtual private cloud (VPC)—based on the inputs from the Map state. To handle large volumes, the data is split into smaller chunks to mitigate Lambda function overload. This enables Step Functions to manage the workload, retry failed chunks, and isolate failures to individual chunks instead of disrupting the entire ingestion process.</li><li>The fetched data is put into an S3 data store bucket for processing.</li><li>The Process Data Lambda function redacts sensitive data through <a href=\"https://aws.amazon.com/comprehend/\" target=\"_blank\" rel=\"noopener\">Amazon Comprehend</a>. Amazon Comprehend provides real-time APIs, such as <a href=\"https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectPiiEntities.html\" target=\"_blank\" rel=\"noopener\">DetectPiiEntities</a> and <a href=\"https://docs.aws.amazon.com/comprehend/latest/APIReference/API_DetectEntities.html\" target=\"_blank\" rel=\"noopener\">DetectEntities</a>, which use natural language processing (NLP) machine learning (ML) models to identify text portions for redaction. When Amazon Comprehend detects PII, the terms will be redacted and replaced by a character of your choice (such as *). You can also use regular expressions to remove identifiers with predetermined formats.</li><li>Finally, the Lambda function creates two separate files: \n  <ol type=\"a\"><li>A sanitized data document in an Amazon Q Business <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/doc-types.html\" target=\"_blank\" rel=\"noopener\">supported format</a> that will be parsed to generate chat responses.</li><li>A JSON <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/s3-metadata.html\" target=\"_blank\" rel=\"noopener\">metadata</a> file for each document containing additional information to customize chat results for end-users and apply <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/metadata-boosting.html\" target=\"_blank\" rel=\"noopener\">boosting techniques</a> to enhance user experience (which we discuss more in the next section).</li></ol></li></ol><p>The following is the sample metadata file:</p><div><pre><code>{\n&nbsp;&nbsp; &nbsp;\"DocumentId\": \"qbusiness-ug.pdf.txt\",\n&nbsp;&nbsp; &nbsp;\"Attributes\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"_created_at\": \"2024-10-29T20:27:45+00:00\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"_last_updated_at\": \"2024-10-29T20:27:45+00:00\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"_source_uri\": \"https://docs.aws.amazon.com/pdfs/amazonq/latest/qbusiness-ug/qbusiness-ug.pdf\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"author\": \"AWS\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"services\": [\"Q Business\"]\n&nbsp;&nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp;\"Title\": \"Amazon Q Business - User Guide\",\n&nbsp;&nbsp; &nbsp;\"ContentType\": \"plain/text\"\n}</code></pre></div><p>In the preceding JSON file, the  for each data document must be unique. All the other attributes are optional; however, the file has additional attributes like , , and  with values defined.</p><p>The two files are placed in a new S3 folder for Amazon Q to index. Additionally, the raw unprocessed data is deleted from the S3 bucket. You can further restrict access to documents uploaded to an S3 bucket for specific users or groups using <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/s3-user-management.html\" target=\"_blank\" rel=\"noopener\">Amazon S3 access control lists (ACLs)</a>.</p><p>Using the Amazon Q Business <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/supported-connectors.html\" target=\"_blank\" rel=\"noopener\">data source connector</a> feature, we integrated the S3 bucket with our application. This connector functionality enables the consolidation of data from multiple sources into a unified index for the Amazon Q Business application. The service offers various integration options, with Amazon S3 being one of the supported data sources.</p><p>When working with your specific dataset in Amazon Q Business, you can use relevance <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/metadata-boosting.html\" target=\"_blank\" rel=\"noopener\">tuning</a> to enhance the performance and accuracy of search results. This feature allows you to customize how Amazon Q Business prioritizes information within your ingested documents. For example, if your dataset includes product descriptions, customer reviews, and technical specifications, you can use relevance tuning to boost the importance of certain fields. You might choose to prioritize product names in titles, give more weight to recent customer reviews, or emphasize specific technical attributes that are crucial for your business. By adjusting these parameters, you can influence the ranking of search results to better align with your dataset’s unique characteristics and your users’ information needs, ultimately providing more relevant answers to their queries.</p><p>For the metadata file used in this example, we focus on boosting two key metadata attributes:  and . By assigning higher weights to these attributes, we made sure documents with specific titles or services received greater prominence in the search results, improving their visibility and relevance for the users</p><p>The following code is the sample CloudFormation template snippet to enable higher weights to  and :</p><div><pre><code>BoostOverrideConfiguration:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Fn::Sub: |\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"nativeIndexConfiguration\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"indexId\": \"${QBusinessIndex.IndexId}\",\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"boostingOverride\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"_document_title\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"stringConfiguration\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"boostingLevel\": \"MEDIUM\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"services\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"stringListConfiguration\": {\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"boostingLevel\": \"HIGH\"\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</code></pre></div><h3>Amazon Q Business guardrails</h3><p>Implementing robust security measures is crucial to protect sensitive information. In this regard, Amazon Q Business <a href=\"https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/guardrails.html\" target=\"_blank\" rel=\"noopener\">guardrails</a> or chat controls proved invaluable, offering a powerful solution to maintain data privacy and security.</p><p>Amazon Q Business guardrails provide configurable rules designed to control the application’s behavior. These guardrails act as a safety net, minimizing access, processing, or revealing of sensitive or inappropriate information. By defining boundaries for the application’s operations, organizations can maintain compliance with internal policies and external regulations. You can enable global- or topic-level controls, which control how Amazon Q Business responds to specific topics in chat.</p><p>The following is the sample CloudFormation template snippet to enable topic-level controls:</p><div><pre><code>TopicConfigurations:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: topic\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;rules:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- ruleType: CONTENT_BLOCKER_RULE\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ruleConfiguration:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;contentBlockerRule:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;systemMessageOverride: This message is blocked as it contains secure content\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;exampleChatMessages:\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:ec2:us-east-1:123456789012:instance/i-abcdef123\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:ec2:us-west-2:123456789012:vpc/bpc-abcdef123\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:kms:eu-west-1:123456789012:key/12345678-1234-12345678-abc12345678\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- s3://bucket/prefix/file.csv\n&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- arn:*:s3::::bucket-name</code></pre></div><p>This topic-level control blocks the Amazon Q Business chat conversation that has AWS service Amazon Resource Names (ARNs). When similar chat messages have been detected by the Amazon Q Business application, the system will block the responses and return the message “This message is blocked as it contains secure content.”</p><p>For information about deploying the Amazon Q Business application with sample boosting and guardrails, refer to the <a href=\"https://github.com/aws-samples/amazon-q-business-simplified-integration-of-enterprise-knowledge-bases-at-scale\" target=\"_blank\" rel=\"noopener\">GitHub repo.</a></p><p>The following screenshot shows an example of the Amazon Q Business assistant chat landing page.<img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/06/QBusiness.jpg\" alt=\"Q Business landing Page\" width=\"1292\" height=\"660\"></p><p>The following screenshot illustrates the assistant’s behavior if a user includes text that matches one of the similarity-based examples specified in the guardrail topic control.</p><p>To enhance data security, you can deploy <a href=\"https://aws.amazon.com/macie/\" target=\"_blank\" rel=\"noopener\">Amazon Macie</a> classification jobs to scan for sensitive or PII data stored in S3 buckets. The following diagram illustrates a sample notification architecture to alert users on sensitive information that might be inadvertently stored. Macie uses machine learning to automatically discover, classify, and protect sensitive data stored in AWS. It focuses on identifying PII, intellectual property, and other sensitive data types to help organizations meet compliance requirements and protect their data from unauthorized access or breaches.</p><p>The workflow includes the following steps:</p><ol><li>Macie reviews the data store S3 bucket for sensitive information before being ingested.</li><li>An EventBridge rule invokes the Rectify &amp; Notify Lambda function.</li><li>The Lambda function processes the alert, remediates it by removing the affected files from the S3 bucket, and sends a notification using <a href=\"http://aws.amazon.com/sns\" target=\"_blank\" rel=\"noopener\">Amazon Simple Notification Service</a> (Amazon SNS) to the subscribed email addresses.</li></ol><p>This system enables rapid response to potential security alerts, allowing for immediate action to protect sensitive data.</p><p>The Macie detection and subsequent notification system can be demonstrated by uploading a new file to the S3 bucket, such as <code>sample-file-with-credentials.txt</code>, containing the PII data types monitored by Macie, such as fake temporary AWS credentials. After the file is uploaded to Amazon S3 and the scheduled Macie detection job discovers it, the Lambda function immediately removes the file and sends the following notification email to the SNS topic subscribers:</p><div><pre><code>Amazon Macie published a new Finding: \"The S3 object contains credentials data\"\nDescription: \"The S3 object contains credentials data such as AWS secret access keys or private keys.\"\nSeverity: {'score': 3, 'description': 'High'}\nType: SensitiveData:S3Object/Credentials\nCategory: CLASSIFICATION\nOrigin Type: \"SENSITIVE_DATA_DISCOVERY_JOB\"\nSensitive Data Categories: \"['CREDENTIALS']\"\nResources affected:\nBucket=\"&lt;BUCKET_NAME&gt;\",\nKey=\"processed/sample-file-with-credentials.txt\"\nTrying to delete S3 Object: &nbsp;s3://&lt;BUCKET_NAME&gt;/processed/sample-file-with-credentials.txt\nFile deletion succeeded.\n\n-------------\nFull Macie finding event:\n{\n&nbsp; &nbsp;...\n}</code></pre></div><p>Additionally, the findings are visible on the Macie console, as shown in the following screenshot.</p><h3>Additional recommendations</h3><p>To further enhance the security and reliability of the Amazon Q Business application, we recommend implementing the following measures. These additional security and logging implementations make sure the data is protected, alerts are sent in response to potential warnings, and timely actions can be taken for security incidents.</p><ul><li><strong>Amazon CloudWatch logging for Amazon Q Business </strong>– You can use <a href=\"https://aws.amazon.com/pm/cloudwatch/?gclid=EAIaIQobChMIyb-6xaHNiAMVjTrUAR3xqw4uEAAYASAAEgIAd_D_BwE&amp;trk=2dfe7cfe-88b0-4c42-844b-24167b0dc800&amp;sc_channel=ps&amp;ef_id=EAIaIQobChMIyb-6xaHNiAMVjTrUAR3xqw4uEAAYASAAEgIAd_D_BwE:G:s&amp;s_kwcid=AL!4422!3!658520966141!!!g!!!19852661915!149878722660\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a> logging for Amazon Q Business to save the logs for the data source connectors and document-level errors, focusing particularly on failed ingestion jobs. This practice is vital from a security perspective because it allows monitoring and quick identification of issues in the data ingestion process. By tracking failed jobs, potential data loss or corruption can be mitigated, maintaining the reliability and completeness of the knowledge base.</li><li><strong>Unauthorized access monitoring on Amazon S3</strong> – You can implement EventBridge rules to <a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-log-s3-data-events.html\" target=\"_blank\" rel=\"noopener\">monitor mutating API actions on the S3 buckets</a>. These rules are configured to invoke SNS notifications when such actions are performed by unauthorized users. Enable <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html\" target=\"_blank\" rel=\"noopener\">Amazon S3 server access logging </a>to store detailed access records in a designated bucket, which can be analyzed using <a href=\"http://aws.amazon.com/athena\" target=\"_blank\" rel=\"noopener\">Amazon Athena</a> for deeper insights. This approach provides real-time alerts for immediate response to potential security breaches, while also maintaining a detailed audit trail for thorough security analysis, making sure that only authorized entities can modify critical data.</li></ul><p>In the following sections, we walk through implementing the end-to-end solution. For this solution to work, the following prerequisites are needed:</p><ul><li>A new or existing AWS account that will be the data collection account</li></ul><h2>Configure the data ingestion</h2><p>In this post, we demonstrate the solution using publicly available documentation as our sample dataset. In your implementation, you can adapt this solution to work with your organization’s specific content sources, such as support tickets, JIRA issues, internal wikis, or other relevant documentation.</p><ul><li>Ingestion Lambda function</li><li>Processing Lambda function</li></ul><p>The data ingestion workflow in this example fetches and processes public data from the Amazon Q Business and <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a> official documentation in PDF format. Specifically, the Ingest Data Lambda function downloads the raw PDF documents, temporarily stores them in Amazon S3, and passes their Amazon S3 URLs to the Process Data Lambda function, which performs the PII redaction (if enabled) and stores the processed documents and their metadata to the S3 path indexed by the Amazon Q Business application.</p><p>You can adapt the Step Functions Lambda code for ingestion and processing according to your own internal data, making sure that the documents and metadata are in a valid format for Amazon Q Business to index, and are properly redacted for PII data.</p><h2>Configure IAM Identity Center</h2><p>You can only have one IAM Identity Center instance per account. If your account already has an Identity Center instance, skip this step and proceed to configuring the Amazon Q Business application.</p><p>You will need to add details for a user such as user name, email, first name, and surname.</p><p>After deploying the CloudFormation template, you will receive an email where you will need to accept the invitation and change the password for the user.</p><p>Before logging in, you will need to deploy the Amazon Q Business application.</p><h2>Configure the Amazon Q Business application</h2><p>You will need to add details such as the IAM Identity Center stack name deployed previously and the S3 bucket name provisioned by the data ingestion stack.</p><p>After you deploy the CloudFormation template, complete the following steps to manage user access:</p><ol><li>On the Amazon Q Business console, choose  in the navigation pane.</li><li>Choose the application you provisioned ().</li><li>Under , choose .</li><li>On the  tab, choose the user you specified when deploying the CloudFormation stack.</li><li>Under , choose  or .</li><li>Choose  and then </li></ol><p>Now you can log in using the user you have specified. You can find the URL for the web experience under .</p><p>If you are unable to log in, make sure that the user has been verified.</p><p>Before you can use the Amazon Q Business application, the data source needs to be synchronized. The application’s data source is configured to sync hourly. It might take some time to synchronize.</p><p>When the synchronization is complete, you should now be able to access the application and ask questions.</p><p>After you’re done testing the solution, you can delete the resources to avoid incurring additional charges. See the <a href=\"https://aws.amazon.com/q/business/pricing/\" target=\"_blank\" rel=\"noopener\">Amazon Q Business pricing page</a> for more information. Follow the instructions in the <a href=\"https://github.com/aws-samples/amazon-q-business-simplified-integration-of-enterprise-knowledge-bases-at-scale/blob/main/README.md\" target=\"_blank\" rel=\"noopener\">GitHub repository</a> to delete the resources and corresponding CloudFormation templates. Make sure to delete the CloudFormation stacks provisioned as follows:</p><ol><li>Delete the Amazon Q Business application stack.</li><li>Delete the IAM Identity Center stack.</li><li>Delete the data ingestion</li><li>For each deleted stack, check for any resources that were skipped in the deletion process, such as S3 buckets.</li></ol><p>Delete any skipped resources on the console.</p><p>In this post, we demonstrated how to build a knowledge base solution by integrating enterprise data with Amazon Q Business using Amazon S3. This approach helps organizations improve operational efficiency, reduce response times, and gain valuable insights from their historical data. The solution uses AWS security best practices to promote data protection while enabling teams to create a comprehensive knowledge base from various data sources.</p><p>Whether you’re managing support tickets, internal documentation, or other business content, this solution can handle multiple data sources and scale according to your needs, making it suitable for organizations of different sizes. By implementing this solution, you can enhance your operations with AI-powered assistance, automated responses, and intelligent routing of complex queries.</p><p>Try this solution with your own use case, and let us know about your experience in the comments section.</p><p> is a Senior Cloud Support Engineer at AWS, specializing in Data, Machine Learning, and Generative AI solutions. With extensive experience in helping customers architect and optimize their cloud-based AI/ML/GenAI workloads, Omar works closely with AWS customers to solve complex technical challenges and implement best practices across the AWS AI/ML/GenAI service portfolio. He is passionate about helping organizations leverage the full potential of cloud computing to drive innovation in generative AI and machine learning.</p><p> is a Principal Cloud Support Engineer at AWS, focused on Networking and Generative AI solutions. He has deep expertise in resolving complex, cross-domain technical challenges through systematic problem-solving methodologies. With a customer-obsessed mindset, he leverages emerging technologies to drive innovation and deliver exceptional customer experiences.</p><p> is a Principal Cloud Support Engineer at AWS. She specializes in solving complex customer issues on the AWS Cloud, focusing on infrastructure-as-code, container orchestration, and generative AI technologies. She collaborates with teams across AWS to design solutions that enhance the customer experience. Outside of work, Bhavani enjoys cooking and traveling.</p><p> is a Senior Cloud Support Engineer at AWS, specialized in Machine Learning technologies and Generative AI solutions, helping customers operate and optimize their ML workloads. With a deep passion for driving performance improvements, he dedicates himself to empowering both customers and teams through innovative ML-enabled solutions. Away from his technical pursuits, Mattia embraces his passion for travel and adventure.</p><p> is a Senior Cloud Support Engineer at AWS who specializes in Serverless technologies and development within the AWS cloud. Kevin has a passion for creating solutions through code while ensuring it is built on solid infrastructure. Outside of work, Kevin enjoys art and sport.</p><p> is a Senior Principal Engineer leading AWS. Tipu supports customers with designing and optimizing their cloud technology strategy as a senior principal engineer in AWS Support &amp; Managed Services. For over 15 years, he has designed, operated and supported diverse distributed systems at scale with a passion for operational excellence. He currently works on generative AI and operational excellence.</p>","contentLength":23230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Faster distributed graph neural network training with GraphStorm v0.4","url":"https://aws.amazon.com/blogs/machine-learning/faster-distributed-graph-neural-network-training-with-graphstorm-v0-4/","date":1739293396,"author":"Theodore Vasiloudis","guid":122,"unread":true,"content":"<p>GraphStorm is a low-code enterprise graph machine learning (ML) framework that provides ML practitioners a simple way of building, training, and deploying graph ML solutions on industry-scale graph data. Although GraphStorm can run efficiently on single instances for small graphs, it truly shines when scaling to enterprise-level graphs in distributed mode using a cluster of <a href=\"http://aws.amazon.com/ec2\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Compute Cloud</a> (Amazon EC2) instances or <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker</a>.</p><p>Today, AWS AI released GraphStorm v0.4. This release introduces integration with DGL-GraphBolt, a new graph storage and sampling framework that uses a compact graph representation and pipelined sampling to reduce memory requirements and speed up Graph Neural Network (GNN) training and inference. For the large-scale dataset examined in this post, the inference speedup is 3.6 times faster, and per-epoch training speedup is 1.4 times faster, with even larger speedups possible.</p><p>To achieve this, GraphStorm v0.4 with DGL-GraphBolt addresses two crucial challenges of graph learning:</p><ul><li> – GraphStorm v0.4 provides compact and distributed storage of graph structure and features, which may grow in the multi-TB range. For example, a graph with 1 billion nodes with 512 features per node and 10 billion edges will require more than 4 TB of memory to store, which necessitates distributed computation.</li><li> – In multi-layer GNNs, you need to sample neighbors of each node to propagate their representations. This can lead to exponential growth in the number of nodes sampled, potentially visiting the entire graph for a single node’s representation. GraphStorm v0.4 provides efficient, pipelined graph sampling.</li></ul><p>In this post, we demonstrate how GraphBolt enhances GraphStorm’s performance in distributed settings. We provide a hands-on example of using GraphStorm with GraphBolt on SageMaker for distributed training. Lastly, we share how to use <a href=\"https://aws.amazon.com/sagemaker/pipelines/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Pipelines</a> with GraphStorm.</p><h2>GraphBolt: Pipeline-driven graph sampling</h2><p>GraphBolt is a new data loading and graph sampling framework developed by the <a href=\"https://www.dgl.ai/\" target=\"_blank\" rel=\"noopener\">DGL</a> team. It streamlines the operations needed to sample efficiently from a heterogeneous graph and fetch the corresponding features. GraphBolt introduces a new, more compact graph structure representation for heterogeneous graphs, called fused Compressed Sparse Column (fCSC). This can reduce the memory cost of storing a heterogeneous graph by up to 56%, allowing users to fit larger graphs in memory and potentially use smaller, more cost-efficient instances for GNN model training.</p><p>GraphStorm v0.4 seamlessly integrates with GraphBolt, allowing users to take advantage of its performance improvements in their GNN workflows. The user just needs to provide the additional argument  when launching graph construction and training jobs.</p><p>A common model development process is to perform model exploration locally on a subset of your full data, and when you’re satisfied with the results, train the full-scale model. This setup allows for cheaper exploration before training on the full dataset. GraphStorm and SageMaker Pipelines allows you to do that by creating a model pipeline you can run locally to retrieve model metrics, and when you’re ready, run your pipeline on the full data on SageMaker, and produce models, predictions, and graph embeddings to use in downstream tasks. In the next section, we show how to set up such pipelines for GraphStorm.</p><p>We demonstrate such a setup in the following diagram, where a user can perform model development and initial training on a single EC2 instance, and when they’re ready to train on their full data, hand off the heavy lifting to SageMaker for distributed training. Using SageMaker Pipelines to train models provides several benefits, like reduced costs, auditability, and lineage tracking.</p><h2>Set up the environment for SageMaker distributed training</h2><p>Setting up your environment should take around 10 minutes. First, set up your Python environment to run the examples:</p><div><div><div><pre><code>conda init\neval $SHELL\n# Create a new env for the post\nconda create --name gsf python=3.10\nconda activate gsf\n\n# Install dependencies for local scripts\npip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cpu\npip install sagemaker boto3 ogb pyarrow\n# Verify installation, might take a few minutes for first run\npython -c \"import sagemaker; import torch\"\n\n# Clone the GraphStorm repository to access the example code\ngit clone https://github.com/awslabs/graphstorm.git ~/graphstorm</code></pre></div></div></div><h3>Build a GraphStorm SageMaker CPU image</h3><p>Next, build and push the GraphStorm PyTorch Docker image that you will use to run the graph construction, training, and inference jobs for smaller-scale data. Your role will need to be able to pull images from the <a href=\"https://gallery.ecr.aws/\" target=\"_blank\" rel=\"noopener\">Amazon ECR Public Gallery</a> and create <a href=\"http://aws.amazon.com/ecr/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Container Registry</a> (Amazon ECR) repositories and push images to your private ECR registry.</p><div><div><pre><code># Enter you account ID here\nACCOUNT_ID=&lt;aws-account-id&gt;\nREGION=us-east-1\n\ncd ~/graphstorm\nbash docker/build_graphstorm_image.sh --environment sagemaker --device cpu\nbash docker/push_graphstorm_image.sh -e sagemaker -r $REGION -a $ACCOUNT_ID -d cpu\n# This will create an ECR repository and push an image to\n# ${ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/graphstorm:sagemaker-cpu</code></pre></div></div><h3>Download and prepare datasets</h3><p>In this post, we use two citation datasets to demonstrate the scalability of GraphStorm. The Open Graph Benchmark (OGB) project hosts a number of graph datasets that can be used to benchmark the performance of graph learning systems. For a small-scale demo, we use the ogbn-arxiv dataset, and for a demonstration of GraphStorm’s large-scale learning capabilities, we use the ogbn-papers100M dataset.</p><h4>Prepare the ogbn-arxiv dataset</h4><p>Download the smaller-scale <a href=\"https://ogb.stanford.edu/docs/nodeprop/#ogbn-arxiv\" target=\"_blank\" rel=\"noopener\">ogbn-arxiv</a> dataset to run a local test before launching larger-scale SageMaker jobs on AWS. This dataset has approximately 170,000 nodes and 1.2 million edges. Use the following code to download the data and prepare it for GraphStorm:</p><div><div><pre><code># Provide the S3 bucket to use for output\nBUCKET_NAME=&lt;your-s3-bucket&gt;</code></pre></div></div><div><pre><code>cd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\npython convert_arxiv_to_gconstruct.py \\\n--output-s3-prefix s3://$BUCKET_NAME/ogb-arxiv-input</code></pre></div><p>This will create the tabular graph data in Amazon S3, which you can verify by running the following code:</p><div><pre><code>aws s3 ls s3://$BUCKET_NAME/ogb-arxiv-input/ \nedges/\nnodes/\nsplits/\ngconstruct_config_arxiv.json</code></pre></div><p>Finally, upload GraphStorm training configuration files for arxiv to use for training and inference:</p><div><pre><code># Upload the training configurations to S3\naws s3 cp ~/graphstorm/training_scripts/gsgnn_np/arxiv_nc.yaml \\\ns3://$BUCKET_NAME/yaml/arxiv_nc_train.yaml\naws s3 cp ~/graphstorm/inference_scripts/np_infer/arxiv_nc.yaml \\\ns3://$BUCKET_NAME/yaml/arxiv_nc_inference.yaml</code></pre></div><h4>Prepare the ogbn-papers100M dataset on SageMaker</h4><p>The papers-100M dataset is a large-scale graph dataset, with 111 million nodes and 3.2 billion edges after adding reverse edges.</p><p>To download and preprocess the data as an <a href=\"https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Processing</a> step, use the following code. You can launch and let the job run in the background while proceeding through the rest of the post, and return to this dataset later. The job should take approximately 45 minutes to run.</p><div><pre><code># Navigate to the example code\ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\n\n# Build and push a Docker image to download and process the papers100M data\nbash build_and_push_papers100M_image.sh -a $ACCOUNT_ID -r $REGION\n\n# This creates an ECR repository and pushes an image to\n# $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/papers100m-processor\n\n# Run a SageMaker job to do the processing and upload the output to S3\nSAGEMAKER_EXECUTION_ROLE_ARN=&lt;your-sagemaker-execution-role-arn&gt;\naws configure set region $REGION\npython sagemaker_convert_papers100m.py \\\n--output-bucket $BUCKET_NAME \\\n--execution-role-arn $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--region $REGION \\\n--instance-type ml.m5.4xlarge \\\n--image-uri $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/papers100m-processor</code></pre></div><p>This will produce the processed data in <code>s3://$BUCKET_NAME/ogb-papers100M-input</code>, which can then be used as input to GraphStorm. While this job is running, you can create the GraphStorm pipelines.</p><h2>Create a SageMaker pipeline</h2><div><pre><code># Navigate to the example code\ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\n\nPIPELINE_NAME=\"ogbn-arxiv-gs-pipeline\"\n\nbash deploy_arxiv_pipeline.sh \\\n--account $ACCOUNT_ID\\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME \\\n--use-graphbolt false</code></pre></div><p>Running the preceding code will create a SageMaker pipeline configured to run three SageMaker jobs in sequence:</p><ul><li>A GConstruct job that converts the tabular file input to a binary partitioned graph on Amazon S3</li><li>A GraphStorm training job that trains a node classification model and saves the model to Amazon S3</li><li>A GraphStorm inference job that produces predictions for all nodes in the test set, and creates embeddings for all nodes</li></ul><p>To review the pipeline, navigate to <a href=\"https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/studio-landing\" target=\"_blank\" rel=\"noopener\">SageMaker AI Studio</a>, choose the domain and user profile you used to create the pipeline, then choose .</p><p>In the navigation pane, choose . There should be a pipeline named . Choose the pipeline, which will take you to the  tab for the pipeline. Choose  to view the pipeline steps.</p><h3>Run the SageMaker pipeline locally for ogbn-arxiv</h3><p>The ogbn-arxiv dataset is small enough that you can run the pipeline locally. Run the following command to start a local execution of the pipeline:</p><div><pre><code># Allow the local containers to inherit AWS credentials\nexport USE_SHORT_LIVED_CREDENTIALS=1\npython ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \\\n--pipeline-name ogbn-arxiv-gs-pipeline \\\n--region us-east-1 \\\n--local-execution | tee arxiv-local-logs.txt</code></pre></div><p>We save the log output to . You will use that later to analyze the training speed.</p><p>Running the pipeline should take approximately 5 minutes. When the pipeline is complete, it will print a message like the following:</p><div><pre><code>Pipeline execution 655b9357-xxx-xxx-xxx-4fc691fcce94 SUCCEEDED</code></pre></div><p>You can inspect the mean epoch and evaluation time using the provided  script and the log file you created:</p><div><pre><code>python analyze_training_time.py --log-file arxiv-local-logs.txt\n\nReading logs from file: arxiv-local-logs.txt\n\n=== Training Epochs Summary ===\nTotal epochs completed: 10\nAverage epoch time: 4.70 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 11\nAverage evaluation time: 1.90 seconds</code></pre></div><p>These numbers will vary depending on your instance type; in this case, these are values reported on an m6in.4xlarge instance.</p><h3>Create a GraphBolt pipeline</h3><p>Now you have established a baseline for performance, you can create another pipeline that uses the GraphBolt graph representation to compare the performance.</p><p>You can use the same pipeline creation script, but change two variables, providing a new pipeline name and setting  to :</p><div><pre><code># Deploy a GraphBolt-enabled pipeline\nPIPELINE_NAME_GB=\"ogbn-arxiv-gs-graphbolt-pipeline\"\nbash deploy_arxiv_pipeline.sh \\\n--account $ACCOUNT_ID \\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--use-graphbolt true\n\n# Execute the pipeline locally\npython ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--region us-east-1 \\\n--local-execution | tee arxiv-local-gb-logs.txt</code></pre></div><p>Analyzing the training logs, you can see the per-epoch time has dropped somewhat:</p><div><pre><code>python analyze_training_time.py --log-file arxiv-local-gb-logs.txt\n\nReading logs from file: arxiv-local-gb-logs.txt\n\n=== Training Epochs Summary ===\nTotal epochs completed: 10\nAverage epoch time: 4.21 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 11\nAverage evaluation time: 1.63 seconds</code></pre></div><p>For such a small graph, the performance gains are modest, around 13% per epoch time. With large data, the potential gains are much greater. In the next section, you will create a pipeline and train a model for papers-100M, a citation graph with 111 million nodes and 3.2 billion edges.</p><h2>Create a SageMaker pipeline for distributed training</h2><p>After the SageMaker processing job that prepares the papers-100M data has finished processing and the data is stored in Amazon S3, you can set up a pipeline to train a model on that dataset.</p><h3>Build the GraphStorm GPU image</h3><p>For this job, you will use large GPU instances, so you will build and push the GPU image this time:</p><div><pre><code>cd ~/graphstorm\n\nbash ./docker/build_graphstorm_image.sh --environment sagemaker --device gpu\n\nbash docker/push_graphstorm_image.sh -e sagemaker -r $REGION -a $ACCOUNT_ID -d gpu</code></pre></div><h3>Deploy and run pipelines for papers-100M</h3><p>Before you deploy your new pipeline, upload the training YAML configuration for papers-100M to Amazon S3:</p><div><pre><code>aws s3 cp \\\n~/graphstorm/training_scripts/gsgnn_np/papers100M_nc.yaml \\\ns3://$BUCKET_NAME/yaml/</code></pre></div><p>Now you are ready to deploy your initial pipeline for papers-100M:</p><div><pre><code># Navigate to the example code \ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt \nPIPELINE_NAME=\"ogb-papers100M-pipeline\" \nbash deploy_papers100M_pipeline.sh \\ \n    --account $ACCOUNT_ID \\\n    --bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n    --pipeline-name $PIPELINE_NAME \\ \n    --use-graphbolt false</code></pre></div><p>Run the pipeline on SageMaker and let it run in the background:</p><div><pre><code># Navigate to the example code\ncd ~/graphstorm/examples/sagemaker-pipelines-graphbolt\n\nPIPELINE_NAME=\"ogb-papers100M-pipeline\"\nbash deploy_papers100M_pipeline.sh \\\n--account $ACCOUNT_ID \\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME \\\n--use-graphbolt false</code></pre></div><p>Your account needs to meet the required quotas for the requested instances. For this post, the defaults are set to four  for training jobs and one  instance for a processing job. To adjust your SageMaker service quotas, you can use the <a href=\"https://us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas\" target=\"_blank\" rel=\"noopener\">Service Quotas console</a>. To run both pipelines in parallel, i.e. without GraphBolt and with GraphBolt, you will need 8 x  and 2 x </p><p>Next, you can deploy and run another pipeline, with GraphBolt enabled:</p><div><pre><code># Deploy the GraphBolt-enabled pipeline\nPIPELINE_NAME_GB=\"ogb-papers100M-graphbolt-pipeline\"\nbash deploy_papers100M_pipeline.sh \\\n--account $ACCOUNT_ID\\\n--bucket-name $BUCKET_NAME --execution-role $SAGEMAKER_EXECUTION_ROLE_ARN \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--use-graphbolt true\n\n# Execute the GraphBolt pipeline on SageMaker\npython ~/graphstorm/sagemaker/pipeline/execute_sm_pipeline.py \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--region us-east-1 \\\n--async-execution</code></pre></div><h3>Compare performance for GraphBolt-enabled training</h3><p>After both pipelines are complete, which should take approximately 4 hours, you can compare the training times for both cases.</p><p>On the page of the SageMaker console, there should be two new pipelines named  and <code>ogb-papers100M-graphbolt-pipeline</code>. Choose ogb-papers100M-pipeline, which will take you to the  tab for the pipeline. Copy the name of the latest successful execution and use that to run the training analysis script:</p><div><pre><code>python analyze_training_time.py \\\n--pipeline-name $PIPELINE_NAME\\\n--execution-name execution-1734404366941</code></pre></div><p>Your output will look like the following code:</p><div><pre><code>== Training Epochs Summary ===\nTotal epochs completed: 15\nAverage epoch time: 73.95 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 15\nAverage evaluation time: 15.07 seconds</code></pre></div><p>Now do the same for the GraphBolt-enabled pipeline:</p><div><pre><code>python analyze_training_time.py \\\n--pipeline-name $PIPELINE_NAME_GB \\\n--execution-name execution-1734463209078</code></pre></div><p>You will see the improved per-epoch and evaluation times:</p><div><pre><code>== Training Epochs Summary ===\nTotal epochs completed: 15\nAverage epoch time: 54.54 seconds\n\n=== Evaluation Summary ===\nTotal evaluations: 15\nAverage evaluation time: 4.13 seconds</code></pre></div><p>Without loss in accuracy, the latest version of GraphStorm achieved a speedup of approximately 1.4 times faster per epoch for training, and a speedup of 3.6 times faster in evaluation time! Depending on the dataset, the speedups can be even greater, as shown by the DGL team’s <a href=\"https://www.dgl.ai/release/2024/03/06/release.html\" target=\"_blank\" rel=\"noopener\">benchmarking</a>.</p><p>This post showcased how GraphStorm 0.4, integrated with DGL-GraphBolt, significantly speeds up large-scale GNN training and inference, by 1.4 and 3.6 times faster, respectively, as measured on the papers-100M dataset. As shown in the <a href=\"https://www.dgl.ai/release/2024/01/26/release.html\" target=\"_blank\" rel=\"noopener\">DGL benchmarks</a>, even larger speedups are possible depending on the dataset.</p><p>We encourage ML practitioners working with large graph data to try GraphStorm. Its low-code interface simplifies building, training, and deploying graph ML solutions on AWS, allowing you to focus on modeling rather than infrastructure.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/04/badge-photo-small-copy.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior Applied Scientist at Amazon Web Services, where he works on distributed machine learning systems and algorithms. He led the development of GraphStorm Processing, the distributed graph processing library for GraphStorm and is a core developer for GraphStorm. He received his PhD in Computer Science from the KTH Royal Institute of Technology, Stockholm, in 2019.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/04/xiang.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Senior Applied Scientist at Amazon Web Services, where he develops deep learning frameworks including GraphStorm, DGL, and DGL-KE. He led the development of Amazon Neptune ML, a new capability of Neptune that uses graph neural networks for graphs stored in a Neptune graph database. He is now leading the development of GraphStorm, an open source graph machine learning framework for enterprise use cases. He received his PhD in computer systems and architecture at the Fudan University, Shanghai, in 2014.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/12/30/florian.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Principal Technical Product Manager at AWS AI/ML research supporting science teams like the graph machine learning group, and ML Systems teams working on large scale distributed training, inference, and fault resilience. Before joining AWS, Florian lead technical product management for automated driving at Bosch, was a strategy consultant at McKinsey &amp; Company, and worked as a control systems and robotics scientist—a field in which he holds a PhD.</p>","contentLength":17809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a Useful Voice-Activated Fully Local RAG System","url":"https://www.kdnuggets.com/creating-useful-voice-activated-fully-local-rag-system","date":1739293249,"author":"Cornellius Yudha Wijaya","guid":247,"unread":true,"content":"<article>This article will explore initiating the RAG system and making it fully voice-activated.</article>","contentLength":88,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/kdn-wijaya-creating-useful-voice-activated-rag.png","enclosureMime":"","commentsUrl":null},{"title":"Insights from Cleric: Building an Autonomous AI SRE // Willem Pienaar // #290","url":"https://podcasters.spotify.com/pod/show/mlops/episodes/Insights-from-Cleric-Building-an-Autonomous-AI-SRE--Willem-Pienaar--290-e2unikg","date":1739291543,"author":"Demetrios","guid":260,"unread":true,"content":"<p><a href=\"https://www.linkedin.com/in/willempienaar/\" target=\"_blank\" rel=\"ugc noopener noreferrer\"></a> is the Co-Founder and CTO of<a href=\"https://podcasters.spotify.com/pod/show/mlops/episodes/cleric.io/\" target=\"_blank\" rel=\"ugc noopener noreferrer\"></a>. He previously worked at Tecton as a Principal Engineer. Willem Pienaar attended the Georgia Institute of Technology.</p><p>Insights from Cleric: Building an Autonomous AI SRE // MLOps Podcast #289 with Willem Pienaar, CTO &amp; Co-Founder of Cleric.// AbstractIn this MLOps Community Podcast episode, Willem Pienaar, CTO of Cleric, breaks down how they built an autonomous AI SRE that helps engineering teams diagnose production issues. We explore how Cleric builds knowledge graphs for system understanding, and uses existing tools/systems during investigations. We also get into some gnarly challenges around memory, tool integration, and evaluation frameworks, and some lessons learned from deploying to engineering teams.// BioWillem Pienaar, CTO of Cleric, is a builder with a focus on LLM agents, MLOps, and open source tooling. He is the creator of Feast, an open source feature store, and contributed to the creation of both the feature store and MLOps categories.Before starting Cleric, Willem led the open-source engineering team at Tecton and established the ML platform team at Gojek, where he built high-scale ML systems for the Southeast Asian Decacorn.// MLOps Swag/Merch<a href=\"https://shop.mlops.community/\" target=\"_blank\" rel=\"ugc noopener noreferrer\">https://shop.mlops.community/</a>// Related Links<a href=\"https://podcasters.spotify.com/pod/show/mlops/episodes/Insights-from-Cleric-Building-an-Autonomous-AI-SRE--Willem-Pienaar--290-e2unikg\" target=\"_blank\" rel=\"ugc noopener noreferrer\">Website: willem.co</a> --------------- ✌️Connect With Us ✌️ -------------Join our slack community:<a href=\"https://go.mlops.community/slack\" target=\"_blank\" rel=\"ugc noopener noreferrer\">https://go.mlops.community/slack</a>Follow us on Twitter:<a href=\"https://podcasters.spotify.com/pod/show/mlops/episodes/@mlopscommunity\" target=\"_blank\" rel=\"ugc noopener noreferrer\">@mlopscommunity</a>Sign up for the next meetup:<a href=\"https://go.mlops.community/register\" target=\"_blank\" rel=\"ugc noopener noreferrer\">https://go.mlops.community/register</a>Catch all episodes, blogs, newsletters, and more:<a href=\"https://mlops.community/\" target=\"_blank\" rel=\"ugc noopener noreferrer\">https://mlops.community/</a>Connect with Demetrios on LinkedIn:<a href=\"https://www.linkedin.com/in/dpbrinkm/\" target=\"_blank\" rel=\"ugc noopener noreferrer\">https://www.linkedin.com/in/dpbrinkm/</a>Connect with Willem on LinkedIn:<a href=\"https://www.linkedin.com/in/willempienaar/\" target=\"_blank\" rel=\"ugc noopener noreferrer\">https://www.linkedin.com/in/willempienaar/</a></p>","contentLength":1703,"flags":null,"enclosureUrl":"https://anchor.fm/s/174cb1b8/podcast/play/98338896/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-11%2F394679162-44100-2-e1c6c10bbd504.mp3","enclosureMime":"","commentsUrl":null},{"title":"10 Little-Known Python Libraries That Will Make You Feel Like a Data Wizard","url":"https://www.kdnuggets.com/10-little-known-python-libraries-data-wizard","date":1739278813,"author":"Josep Ferrer","guid":246,"unread":true,"content":"<article>In this article, I will introduce you to 10 little-known Python libraries every data scientist should know.</article>","contentLength":107,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Ferrer_10_little_known_libraries1.png","enclosureMime":"","commentsUrl":null},{"title":"Introducing the CrossValidationReport","url":"https://www.youtube.com/watch?v=R6dRAE83Y2c","date":1739277902,"author":"probabl","guid":368,"unread":true,"content":"<article>Skore version 0.6 introduces the `CrossValidationReport` that provides you with an `EstimatorReport` for each fold of your cross-validation, enabling you to inspect your estimator on each fold.\n\n------\nLinks:\n\nskore v0.6 documentation:\nhttps://skore.probabl.ai/0.6/index.html\n\nskore GitHub repository:\nhttps://github.com/probabl-ai/skore\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\nBluesky: https://bsky.app/profile/probabl.bsky.social\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":722,"flags":null,"enclosureUrl":"https://www.youtube.com/v/R6dRAE83Y2c?version=3","enclosureMime":"","commentsUrl":null},{"title":"Transforming credit decisions using generative AI with Rich Data Co and AWS","url":"https://aws.amazon.com/blogs/machine-learning/transforming-credit-decisions-using-generative-ai-with-rich-data-co-and-aws/","date":1739217934,"author":"Daniel Wirjo","guid":121,"unread":true,"content":"<p><em>This post is co-written with Gordon Campbell, Charles Guan, and Hendra Suryanto from RDC.&nbsp;</em></p><p>The mission of <a href=\"https://www.richdataco.com/\" target=\"_blank\" rel=\"noopener\">Rich Data Co (RDC)</a> is to broaden access to sustainable credit globally. Its software-as-a-service (SaaS) solution empowers leading banks and lenders with deep customer insights and AI-driven decision-making capabilities.</p><p>Making credit decisions using AI can be challenging, requiring data science and portfolio teams to synthesize complex subject matter information and collaborate productively. To solve this challenge, RDC used <a href=\"https://aws.amazon.com/ai/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a>, enabling teams to use its solution more effectively:</p><ul><li> – Designed for data science teams, this agent assists teams in developing, building, and deploying AI models within a regulated environment. It aims to boost team efficiency by answering complex technical queries across the machine learning operations (MLOps) lifecycle, drawing from a comprehensive knowledge base that includes environment documentation, AI and data science expertise, and Python code generation.</li><li> – Designed for portfolio managers and analysts, this agent facilitates natural language inquiries about loan portfolios. It provides critical insights on performance, risk exposures, and credit policy alignment, enabling informed commercial decisions without requiring in-depth analysis skills. The assistant is adept at high-level questions (such as identifying high-risk segments or potential growth opportunities) and one-time queries, allowing the portfolio to be diversified.</li></ul><p>In this post, we discuss how RDC uses generative AI on <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> to build these assistants and accelerate its overall mission of democratizing access to sustainable credit.</p><h2>Solution overview: Building a multi-agent generative AI solution</h2><p>We began with a carefully crafted evaluation set of over 200 prompts, anticipating common user questions. Our initial approach combined prompt engineering and traditional <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" target=\"_blank\" rel=\"noopener\">Retrieval Augmented Generation (RAG)</a>. However, we encountered a challenge: accuracy fell below 90%, especially for more complex questions.</p><p>To overcome the challenge, we adopted an agentic approach, breaking down the problem into specialized use cases. This strategy equipped us to align each task with the most suitable <a href=\"https://aws.amazon.com/what-is/foundation-models/\" target=\"_blank\" rel=\"noopener\">foundation model (FM)</a> and tools. Our multi-agent framework is orchestrated using <a href=\"https://langchain-ai.github.io/langgraph/\" target=\"_blank\" rel=\"noopener\">LangGraph</a>, and it consisted of:</p><ol><li> – The orchestrator is responsible for routing user questions to the appropriate agent. In this example, we start with the data science or portfolio agent. However, we envision many more agents in the future. The orchestrator can also use user context, such as the user’s role, to determine routing to the appropriate agent.</li><li> – The agent is designed for a specialized task. It’s equipped with the appropriate FM for the task and the necessary tools to perform actions and access knowledge. It can also handle multiturn conversations and orchestrate multiple calls to the FM to reach a solution.</li><li> – Tools extend agent capabilities beyond the FM. They provide access to external data and APIs or enable specific actions and computation. To efficiently use the model’s context window, we construct a tool selector that retrieves only the relevant tools based on the information in the agent state. This helps simplify debugging in the case of errors, ultimately making the agent more effective and cost-efficient.</li></ol><p>This approach gives us the right tool for the right job. It enhances our ability to handle complex queries efficiently and accurately while providing flexibility for future improvements and agents.</p><p>The following image is a high-level architecture diagram of the solution.</p><h2>Data science agent: RAG and code generation</h2><p>To boost productivity of data science teams, we focused on rapid comprehension of advanced knowledge, including industry-specific models from a curated knowledge base. Here, RDC provides an integrated development environment (IDE) for Python coding, catering to various team roles. One role is model validator, who rigorously assesses whether a model aligns with bank or lender policies. To support the assessment process, we designed an agent with two tools:</p><ol><li> – <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> powers our intelligent content retrieval through a streamlined RAG implementation. The service automatically converts text documents to their vector representation using <a href=\"https://aws.amazon.com/bedrock/amazon-models/titan/\" target=\"_blank\" rel=\"noopener\">Amazon Titan Text Embeddings</a> and stores them in <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\" target=\"_blank\" rel=\"noopener\">Amazon OpenSearch Serverless</a>. Because the knowledge is vast, it performs <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking.html\" target=\"_blank\" rel=\"noopener\">semantic chunking</a>, making sure that the knowledge is organized by topic and can fit within the FM’s context window. When users interact with the agent, <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock Knowledge Bases</a> using OpenSearch Serverless provides fast, in-memory semantic search, enabling the agent to retrieve the most relevant chunks of knowledge for relevant and contextual responses to users.</li><li> – With code generation, we selected <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropic’s Claude model on Amazon Bedrock</a> due to its inherent ability to understand and generate code. This tool is grounded to answer queries related to data science and can generate Python code for quick implementation. It’s also adept at troubleshooting coding errors.</li></ol><h2>Portfolio agent: Text-to-SQL and self-correction</h2><p>To boost the productivity of credit portfolio teams, we focused on two key areas. For portfolio managers, we prioritized high-level commercial insights. For analysts, we enabled deep-dive data exploration. This approach empowered both roles with rapid understanding and actionable insights, streamlining decision-making processes across teams.</p><p>Our solution required natural language understanding of structured portfolio data stored in <a href=\"https://aws.amazon.com/rds/aurora/\" target=\"_blank\" rel=\"noopener\">Amazon Aurora</a>. This led us to base our solution on a text-to-SQL model to efficiently bridge the gap between natural language and SQL.</p><p>To reduce errors and tackle complex queries beyond the model’s capabilities, we developed three tools using Anthropic’s Claude model on Amazon Bedrock for self-correction:</p><ol><li> – Verifies and corrects SQL queries, addressing common issues such as data type mismatches or incorrect function usage</li><li> – Validates query results, providing relevance and prompting retries or user clarification when needed</li><li> – Engages users for additional information when queries are too broad or lack detail, guiding the interaction based on database information and user input</li></ol><p>These tools operate in an agentic system, enabling accurate database interactions and improved query results through iterative refinement and user engagement.</p><p>To improve accuracy, we tested model fine-tuning, training the model on common queries and context (such as database schemas and their definitions). This approach reduces inference costs and improves response times compared to prompting at each call. Using <a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart</a>, we <a href=\"https://aws.amazon.com/blogs/machine-learning/fine-tune-llama-3-for-text-generation-on-amazon-sagemaker-jumpstart/\" target=\"_blank\" rel=\"noopener\">fine-tuned Meta’s Llama model </a>by providing a set of anticipated prompts, intended answers, and associated context. Amazon SageMaker Jumpstart offers a cost-effective alternative to third-party models, providing a viable pathway for future applications. However, we didn’t end up deploying the fine-tuned model because we experimentally observed that prompting with Anthropic’s Claude model provided better generalization, especially for complex questions. To reduce operational overhead, we will also evaluate <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-build-structured.html\" target=\"_blank\" rel=\"noopener\">structured data retrieval on Amazon Bedrock Knowledge Bases</a>.</p><h2>Conclusion and next steps with RDC</h2><p>To expedite development, RDC collaborated with <a href=\"https://aws.amazon.com/startups\" target=\"_blank\" rel=\"noopener\">AWS Startups</a> and the <a href=\"https://aws.amazon.com/ai/generative-ai/innovation-center/\" target=\"_blank\" rel=\"noopener\">AWS Generative AI Innovation Center</a>. Through an iterative approach, RDC rapidly enhanced its generative AI capabilities, deploying the initial version to production in just 3 months. The solution successfully met the stringent security standards required in regulated banking environments, providing both innovation and compliance.</p><blockquote><p>“The integration of generative AI into our solution marks a pivotal moment in our mission to revolutionize credit decision-making. By empowering both data scientists and portfolio managers with AI assistants, we’re not just improving efficiency—we’re transforming how financial institutions approach lending.”</p><p><em>–Gordon Campbell, Co-Founder &amp; Chief Customer Officer at RDC</em></p></blockquote><p>RDC envisions generative AI playing a significant role in boosting the productivity of the banking and credit industry. By using this technology, RDC can provide key insights to customers, improve solution adoption, accelerate the model lifecycle, and reduce the customer support burden. Looking ahead, RDC plans to further refine and expand its AI capabilities, exploring new use cases and integrations as the industry evolves.</p><p>For more information about how to work with RDC and AWS and to understand how we’re supporting banking customers around the world to use AI in credit decisions, contact your AWS Account Manager or visit <a href=\"https://www.richdataco.com/\" target=\"_blank\" rel=\"noopener\">Rich Data Co</a>.</p><p>For more information about generative AI on AWS, refer to the following resources:</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/wirjo.jpeg\" alt=\"\" width=\"100\" height=\"133\"> is a Solutions Architect at AWS, focused on FinTech and SaaS startups. As a former startup CTO, he enjoys collaborating with founders and engineering leaders to drive growth and innovation on AWS. Outside of work, Daniel enjoys taking walks with a coffee in hand, appreciating nature, and learning new ideas.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/liuxuefeng.jpg\" alt=\"\" width=\"100\" height=\"133\"> leads a science team at the AWS Generative AI Innovation Center in the Asia Pacific regions. His team partners with AWS customers on generative AI projects, with the goal of accelerating customers’ adoption of generative AI.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/iman.png\" alt=\"\" width=\"100\" height=\"133\"> is a computer scientist at the Generative AI Innovation Center at Amazon Web Services (AWS) working on Generative AI and complex multi-agents systems.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/rdc-gordon-campbell.png\" alt=\"\" width=\"100\" height=\"133\"> is the Chief Customer Officer and Co-Founder of RDC, where he leverages over 30 years in enterprise software to drive RDC’s leading AI Decisioning platform for business and commercial lenders. With a proven track record in product strategy and development across three global software firms, Gordon is committed to customer success, advocacy, and advancing financial inclusion through data and AI.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/rdc-charles-guan.png\" alt=\"\" width=\"100\" height=\"133\"> is the Chief Technology Officer and Co-founder of RDC. With more than 20 years of experience in data analytics and enterprise applications, he has driven technological innovation across both the public and private sectors. At RDC, Charles leads research, development, and product advancement—collaborating with universities to leverage advanced analytics and AI. He is dedicated to promoting financial inclusion and delivering positive community impact worldwide.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/rdc-hendra-suryanto.png\" alt=\"\" width=\"100\" height=\"133\"> is the Chief Data Scientist at RDC with more than 20 years of experience in data science, big data, and business intelligence. Before joining RDC, he served as a Lead Data Scientist at KPMG, advising clients globally. At RDC, Hendra designs end-to-end analytics solutions within an Agile DevOps framework. He holds a PhD in Artificial Intelligence and has completed postdoctoral research in machine learning.</p>","contentLength":10851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build agentic AI solutions with DeepSeek-R1, CrewAI, and Amazon SageMaker AI","url":"https://aws.amazon.com/blogs/machine-learning/build-agentic-ai-solutions-with-deepseek-r1-crewai-and-amazon-sagemaker-ai/","date":1739215990,"author":"Surya Kari","guid":120,"unread":true,"content":"<p>AI agents are rapidly becoming the next frontier in enterprise transformation, with 82% of organizations planning adoption within the next 3 years. According to a <a href=\"https://www.capgemini.com/wp-content/uploads/2024/11/Generative-AI-in-Organizations-Refresh_25112024.pdf\" target=\"_blank\" rel=\"noopener\">Capgemini survey</a> of 1,100 executives at large enterprises, 10% of organizations already use AI agents, and more than half plan to use them in the next year. The recent release of the DeepSeek-R1 models brings state-of-the-art reasoning capabilities to the open source community. Organizations can build agentic applications using these reasoning models to execute complex tasks with advanced decision-making capabilities, enhancing efficiency and adaptability.</p><p>In this post, we dive into how organizations can use <a href=\"https://aws.amazon.com/sagemaker-ai/?gclid=CjwKCAiAtYy9BhBcEiwANWQQL-kwoCy7ZdAghF_qTtqKZCdOoS419cBEv70K_pLlN0NeCVYS8bV-JRoCPMQQAvD_BwE&amp;trk=8987dd52-6f33-407a-b89b-a7ba025c913c&amp;sc_channel=ps&amp;ef_id=CjwKCAiAtYy9BhBcEiwANWQQL-kwoCy7ZdAghF_qTtqKZCdOoS419cBEv70K_pLlN0NeCVYS8bV-JRoCPMQQAvD_BwE:G:s&amp;s_kwcid=AL!4422!3!724218586004!e!!g!!amazon%20sagemaker%20ai!11206038603!174643422154\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker AI</a>, a fully managed service that allows you to build, train, and deploy ML models at scale, and can build AI agents using CrewAI, a popular agentic framework and open source models like DeepSeek-R1.</p><h2>Agentic design vs. traditional software design</h2><p>Agentic systems offer a fundamentally different approach compared to traditional software, particularly in their ability to handle complex, dynamic, and domain-specific challenges. Unlike traditional systems, which rely on rule-based automation and structured data, agentic systems, powered by large language models (LLMs), can operate autonomously, learn from their environment, and make nuanced, context-aware decisions. This is achieved through modular components including reasoning, memory, cognitive skills, and tools, which enable them to perform intricate tasks and adapt to changing scenarios.</p><p>Traditional software platforms, though effective for routine tasks and horizontal scaling, often lack the domain-specific intelligence and flexibility that agentic systems provide. For example, in a manufacturing setting, traditional systems might track inventory but lack the ability to anticipate supply chain disruptions or optimize procurement using real-time market insights. In contrast, an agentic system can process live data such as inventory fluctuations, customer preferences, and environmental factors to proactively adjust strategies and reroute supply chains during disruptions.</p><p>Enterprises should strategically consider deploying agentic systems in scenarios where adaptability and domain-specific expertise are critical. For instance, consider customer service. Traditional chatbots are limited to preprogrammed responses to expected customer queries, but AI agents can engage with customers using natural language, offer personalized assistance, and resolve queries more efficiently. AI agents can significantly improve productivity by automating repetitive tasks, such as generating reports, emails, and software code. The deployment of agentic systems should focus on well-defined processes with clear success metrics and where there is potential for greater flexibility and less brittleness in process management.</p><h3>Generative AI on SageMaker AI</h3><p>SageMaker AI, a fully managed service, provides a comprehensive suite of tools designed to deliver high-performance, cost-efficient machine learning (ML) and generative AI solutions for diverse use cases. SageMaker AI empowers you to build, train, deploy, monitor, and govern ML and generative AI models through an extensive range of services, including notebooks, jobs, hosting, experiment tracking, a curated model hub, and MLOps features, all within a unified integrated development environment (IDE).</p><p>SageMaker AI simplifies the process for generative AI model builders of all skill levels to work with foundation models (FMs):</p><ul><li><a href=\"https://aws.amazon.com/sagemaker-ai/canvas/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Canvas</a> enables data scientists to seamlessly use their own datasets alongside FMs to create applications and architectural patterns, such as chatbots and Retrieval Augmented Generation (RAG), in a low-code or no-code environment.</li><li><a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker JumpStart</a> offers a diverse selection of open and proprietary FMs from providers like Hugging Face, Meta, and Stability AI. You can deploy or fine-tune models through an intuitive UI or APIs, providing flexibility for all skill levels.</li></ul><p>With SageMaker AI, you can build generative AI-powered agentic workflows using a framework of your choice. Some of the key benefits of using SageMaker AI for fine-tuning and hosting LLMs or FMs include:</p><ul><li> – SageMaker AI offers access to SageMaker JumpStart, a curated model hub where models with open weights are made available for seamless deployment through a few clicks or API calls. Additionally, for Hugging Face Hub models, SageMaker AI provides pre-optimized containers built on popular open source hosting frameworks such as <a href=\"https://github.com/vllm-project/vllm\" target=\"_blank\" rel=\"noopener\">vLLM</a>, <a href=\"https://github.com/triton-inference-server/server\" target=\"_blank\" rel=\"noopener\">NVIDIA Triton</a>, and H<a href=\"https://huggingface.co/docs/text-generation-inference/en/index\" target=\"_blank\" rel=\"noopener\">ugging Face Text Generation Inference (TGI).</a> You simply need to specify the model ID, and the model can be deployed quickly.</li><li><strong>Instance-based deterministic pricing</strong> – SageMaker AI hosted models are billed based on instance-hours rather than token usage. This pricing model enables you to more accurately predict and manage generative AI inference costs while scaling resources to accommodate incoming request loads.</li><li><strong>Deployments with quantization</strong> – SageMaker AI enables you to optimize models prior to deployment using advanced strategies such as quantized deployments (such as AWQ, GPTQ, float16, int8, or int4). This flexibility allows you to efficiently deploy large models, such as a 32-billion parameter model, onto smaller instance types like ml.g5.2xlarge with 24 GB of GPU memory, significantly reducing resource requirements while maintaining performance.</li><li><strong>Inference load balancing and optimized routing</strong> – SageMaker endpoints support load balancing and optimized routing with various strategies, providing users with enhanced flexibility and adaptability to accommodate diverse use cases effectively.</li><li><strong>SageMaker fine-tuning recipes</strong> – SageMaker offers <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-recipes.html\" target=\"_blank\" rel=\"noopener\">ready-to-use recipes</a> for quickly training and fine-tuning publicly available FMs such as Meta’s Llama 3, Mistral, and Mixtral. These recipes use <a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker HyperPod</a> (a SageMaker AI service that provides resilient, self-healing clusters optimized for large-scale ML workloads), enabling efficient and resilient training on a GPU cluster for scalable and robust performance.</li></ul><p>CrewAI provides a robust framework for developing multi-agent systems that integrate with AWS services, particularly SageMaker AI. CrewAI’s role-based agent architecture and comprehensive performance monitoring capabilities work in tandem with <a href=\"http://aws.amazon.com/cloudwatch\" target=\"_blank\" rel=\"noopener\">Amazon CloudWatch</a>.</p><p>The framework excels in workflow orchestration and maintains enterprise-grade security standards aligned with AWS best practices, making it an effective solution for organizations implementing sophisticated agent-based systems within their AWS infrastructure.</p><p>In this post, we demonstrate how to use CrewAI to create a multi-agent research workflow. This workflow creates two agents: one that researches on a topic on the internet, and a writer agent takes this research and acts like an editor by formatting it in a readable format. Additionally, we guide you through deploying and integrating one or multiple LLMs into structured workflows, using tools for automated actions, and deploying these workflows on SageMaker AI for a production-ready deployment.</p><p>The following diagram illustrates the solution architecture.</p><p>To follow along with the code examples in the rest of this post, make sure the following prerequisites are met:</p><ul><li><strong>Integrated development environment</strong> – This includes the following: \n  <ul><li><strong>(Optional) Access to Amazon SageMaker Studio and the JupyterLab IDE</strong> – We will use a Python runtime environment to build agentic workflows and deploy LLMs. Having access to a JupyterLab IDE with Python 3.9, 3.10, or 3.11 runtimes is recommended. You can also set up <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Studio</a> for single users. For more details, see <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html\" target=\"_blank\" rel=\"noopener\">Use quick setup for Amazon SageMaker AI</a>. Create a new SageMaker JupyterLab Space for a quick JupyterLab notebook for experimentation. To learn more, refer to Boost productivity on Amazon SageMaker Studio: Introducing JupyterLab Spaces and generative AI tools.</li><li> – You can also follow along in your local IDE (such as PyCharm or VSCode), provided that Python runtimes have been configured for site to AWS VPC connectivity (to deploy models on SageMaker AI).</li></ul></li><li><strong>Permission to deploy models</strong> – Make sure that your user execution role has the necessary permissions to deploy models to a SageMaker real-time endpoint for inference. For more information, refer to <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html\" target=\"_blank\" rel=\"noopener\">Deploy models for inference</a>.</li><li> – The code used in this post is available in the following GitHub repo.</li></ul><h2>Simplified LLM hosting on SageMaker AI</h2><p>Before orchestrating agentic workflows with CrewAI powered by an LLM, the first step is to host and query an LLM using <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/image-classification/sagemaker.html\" target=\"_blank\" rel=\"noopener\">SageMaker real-time inference endpoints</a>. There are two primary methods to host LLMs on SageMaker AI:</p><ul><li>Deploy from SageMaker JumpStart</li><li>Deploy from Hugging Face Hub</li></ul><h2>Deploy DeepSeek from SageMaker JumpStart</h2><p>SageMaker JumpStart offers access to a diverse array of state-of-the-art FMs for a wide range of tasks, including content writing, code generation, question answering, copywriting, summarization, classification, information retrieval, and more. It simplifies the onboarding and maintenance of publicly available FMs, allowing you to access, customize, and seamlessly integrate them into your ML workflows. Additionally, SageMaker JumpStart provides solution templates that configure infrastructure for common use cases, along with executable example notebooks to streamline ML development with SageMaker AI.</p><p>The following screenshot shows an example of available models on SageMaker JumpStart.</p><p>To get started, complete the following steps:</p><ol><li>Run the following command in a Jupyter cell or the SageMaker Studio terminal:</li></ol><ol start=\"3\"><li>List all available LLMs under the Hugging Face or Meta JumpStart hub. The following code is an example of how to do this programmatically using the SageMaker Python SDK:</li></ol><div><pre><code>from sagemaker.jumpstart.filters import (And, Or)\nfrom sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n\n# generate a conditional filter to only select LLMs from HF or Meta\nfilter_value = Or(\n    And(\"task == llm\", \"framework == huggingface\"), \n    \"framework == meta\", \"framework == deekseek\"\n)\n\n# Retrieve all available JumpStart models\nall_models = list_jumpstart_models(filter=filter_value)</code></pre></div><p>For example, deploying the  model directly from SageMaker JumpStart requires only a few lines of code:</p><div><pre><code>from sagemaker.jumpstart.model import JumpStartModel\n\nmodel_id = \" deepseek-llm-r1\" \nmodel_version = \"*\"\n\n# instantiate a new JS meta model\nmodel = JumpStartModel(\n    model_id=model_id, \n    model_version=model_version\n)\n\n# deploy model on a 1 x p5e instance \npredictor = model.deploy(\n    accept_eula=True, \n    initial_instance_count=1, \n    # endpoint_name=\"deepseek-r1-endpoint\" # optional endpoint name\n)</code></pre></div><p>We recommend deploying your SageMaker endpoints within a VPC and a private subnet with no egress, making sure that the models remain accessible only within your VPC for enhanced security.</p><h2>Deploy DeepSeek from Hugging Face Hub</h2><p>Alternatively, you can deploy your preferred model directly from the <a href=\"https://huggingface.co/models\" target=\"_blank\" rel=\"noopener\">Hugging Face Hub</a> or the <a href=\"https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/\" target=\"_blank\" rel=\"noopener\">Hugging Face Open LLM Leaderboard</a> to a SageMaker endpoint. Hugging Face LLMs can be hosted on SageMaker using a variety of supported frameworks, such as NVIDIA Triton, vLLM, and Hugging Face TGI. For a comprehensive list of supported deep learning container images, refer to the available <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\" target=\"_blank\" rel=\"noopener\">Amazon SageMaker Deep Learning Containers</a>. In this post, we use a <strong><em>DeepSeek-R1-Distill-Llama-70B</em></strong> SageMaker endpoint using the TGI container for agentic AI inference. We deploy the model from Hugging Face Hub using <a href=\"https://huggingface.co/blog/sagemaker-huggingface-llm\" target=\"_blank\" rel=\"noopener\">Amazon’s optimized TGI container</a>, which provides enhanced performance for LLMs. This container is specifically optimized for text generation tasks and automatically selects the most performant parameters for the given hardware configuration. To deploy from Hugging Face Hub, refer to the  or the following code snippet:</p><div><pre><code>import json\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\nimport os\nfrom datetime import datetime\n\n# Model configuration\nhub = {'HF_MODEL_ID':'deepseek-ai/DeepSeek-R1-Distill-Llama-70B', #Llama-3.3-70B-Instruct\n       'SM_NUM_GPUS': json.dumps(number_of_gpu),\n       'HF_TOKEN': HUGGING_FACE_HUB_TOKEN,\n       'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',  # Set to INFO level\n       'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True'  # configure CUDA memory to use expandable memory segments\n}\n# Create and deploy model\nhuggingface_model =   HuggingFaceModel(image_uri=get_huggingface_llm_image_uri(\"huggingface\", \nversion=\"2.3.1\"),\nenv=hub,\nrole=role,sagemaker_session=sagemaker_session)\npredictor = huggingface_model.deploy(\n               initial_instance_count=1,\n               instance_type=\"ml.p4d.24xlarge\"\n               endpoint_name=custom_endpoint_name,\n               container_startup_health_check_timeout=900)</code></pre></div><p>A new <em><strong>DeepSeek-R1-Distill-Llama-70B</strong></em>endpoint should be  in under 10 minutes. If you want to change the model from DeepSeek to another model from the hub, simply replace the following parameter or refer to the DeepSeek deploy example in the following <a href=\"https://github.com/huggingface/text-generation-inference/blob/main/docs/source/reference/launcher.md\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>. To learn more about deployment parameters that can be reconfigured inside TGI containers at runtime, refer to the following <a href=\"https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/Deepseek/DeepSeek-R1-Llama8B-LMI-TGI-Deploy.ipynb\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>&nbsp;on TGI arguments.</p><div><pre><code>...\n\"HF_MODEL_ID\": \"deepseek-ai/...\", # replace with any HF hub models\n# \"HF_TOKEN\": \"hf_...\" # add your token id for gated models\n...</code></pre></div><p>For open-weight models deployed directly from hubs, we strongly recommend placing your SageMaker endpoints within a VPC and a private subnet with no egress, making sure that the models remain accessible only within your VPC for a secure deployment.</p><h2>Build a simple agent with CrewAI</h2><p>CrewAI offers the ability to create multi-agent and very complex agentic orchestrations using LLMs from several LLM providers, including SageMaker AI and Amazon Bedrock. In the following steps, we create a simple blocks counting agent to serve as an example.</p><p><strong>Create a blocks counting agent</strong></p><p>The following code sets up a simple blocks counter workflow using CrewAI with two main components:</p><ul><li><strong>Agent creation (blocks_counter_agent)</strong> – The agent is configured with a specific role, goal, and capabilities. This agent is equipped with a tool called .</li><li><strong>Task definition (count_task)</strong> – This is a task that we want this agent to execute. The task includes a template for counting how many of each color of blocks are present, where  will be replaced with actual color of the block. The task is assigned to .</li></ul><div><pre><code>from crewai import Agent, Task\nfrom pydantic import BaseModel, Field\n\n# 1. Configure agent\nblocks_counter_agent = Agent(\n    role=\"Blocks Inventory Manager\",\n    goal=\"Maintain accurate block counts\",\n    tools=[BlocksCounterTool],\n    verbose=True\n)\n\n# 2. Create counting task\ncount_task = Task(\n    description=\"Count {color} play blocks in storage\",\n    expected_output=\"Exact inventory count for specified color\",\n    agent=blocks_counter_agent\n)</code></pre></div><p>As you can see in the preceding code, each agent begins with two essential components: an agent definition that establishes the agent’s core characteristics (including its role, goal, backstory, available tools, LLM model endpoint, and so on), and a task definition that specifies what the agent needs to accomplish, including the detailed description of work, expected outputs, and the tools it can use during execution.</p><p>This structured approach makes sure that agents have both a clear identity and purpose (through the agent definition) and a well-defined scope of work (through the task definition), enabling them to operate effectively within their designated responsibilities.</p><p>Tools are special functions that give AI agents the ability to perform specific actions, like searching the internet or analyzing data. Think of them as apps on a smartphone—each tool serves a specific purpose and extends what the agent can do. In our example,  helps the agent count the number of blocks organized by color.</p><p>Tools are essential because they let agents do real-world tasks instead of just thinking about them. Without tools, agents would be like smart speakers that can only talk—they could process information but couldn’t take actual actions. By adding tools, we transform agents from simple chat programs into practical assistants that can accomplish real tasks.</p><p><strong>Out-of-the-box tools with CrewAI</strong>Crew AI offers a range of tools out of the box for you to use along with your agents and tasks. The following table lists some of the available tools.</p><table border=\"1px\" width=\"781\" cellpadding=\"10px\"><tbody><tr><td>For reading various file formats</td></tr><tr><td>For web content extraction</td></tr><tr><td>For searching YouTube channels</td></tr><tr><td>For searching PDF documents</td></tr><tr><td>For Python code interpretation</td></tr><tr></tr></tbody></table><p><strong>Build custom tools with CrewAI</strong>You can build custom tools in CrewAI in two ways: by subclassing BaseTool or using the @tool decorator. Let’s look at the following BaseTool subclassing option to create the BlocksCounterTool we used earlier:</p><div><pre><code>from crewai.tools import BaseTool\n\nclass BlocksCounterTool(BaseTool):\n    name = \"blocks_counter\" \n    description = \"Simple tool to count play blocks\"\n\n    def _run(self, color: str) -&gt; str:\n        return f\"There are 10 {color} play blocks available\"</code></pre></div><h2><strong>Build a multi-agent workflow with CrewAI, DeepSeek-R1, and SageMaker AI</strong></h2><p>Multi-agent AI systems represent a powerful approach to complex problem-solving, where specialized AI agents work together under coordinated supervision. By combining CrewAI’s workflow orchestration capabilities with SageMaker AI based LLMs, developers can create sophisticated systems where multiple agents collaborate efficiently toward a specific goal. The code used in this post is available in the following <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/tree/main/agents-with-sagemaker/deepseek_crewai_based_agent\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>.</p><p>Let’s build a research agent and writer agent that work together to create a PDF about a topic. We will use a DeepSeek-R1 Distilled Llama 3.3 70B model as a SageMaker endpoint for the LLM inference.</p><p><strong>Define your own DeepSeek SageMaker LLM (using LLM base class)</strong> The following code integrates SageMaker hosted LLMs with CrewAI by creating a custom inference tool that formats prompts with system instructions for factual responses, uses Boto3, an AWS core library, to call SageMaker endpoints, and processes responses by separating reasoning (before &lt;/think&gt;) from final answers. This enables CrewAI agents to use deployed models while maintaining structured output patterns.</p><div><pre><code># Calls SageMaker endpoint for DeepSeek inference\ndef deepseek_llama_inference(prompt: dict, endpoint_name: str, region: str = \"us-east-2\") -&gt; dict:\n    try:\n        # ... Response parsing Code...\n\n    except Exception as e:\n        raise RuntimeError(f\"Error while calling SageMaker endpoint: {e}\")\n\n# CrewAI-compatible LLM implementation for DeepSeek models on SageMaker.\nclass DeepSeekSageMakerLLM(LLM):\n    def __init__(self, endpoint: str):\n        # &lt;... Initialize LLM with SageMaker endpoint ...&gt;\n\n    def call(self, prompt: Union[List[Dict[str, str]], str], **kwargs) -&gt; str:\n        # &lt;... Format and return the final response ...&gt;</code></pre></div><p><strong>Name the DeepSeek-R1 Distilled endpoint</strong> Set the endpoint name as defined earlier when you deployed DeepSeek from the Hugging Face Hub:</p><div><pre><code>deepseek_endpoint = \"deepseek-r1-dist-v3-llama70b-2025-01-22\"</code></pre></div><p><strong>Create a DeepSeek inference tool</strong> Just like how we created the BlocksCounterTool earlier, let’s create a tool that uses the DeepSeek endpoint for our agents to use. We use the same BaseTool subclass here, but we hide it in the CustomTool class implementation in sage_tools.py in the tools folder. For more information, refer to the <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/tree/main/agents-with-sagemaker/deepseek_crewai_based_agent\" target=\"_blank\" rel=\"noopener\">GitHub repo</a>.</p><div><pre><code>from crewai import Crew, Agent, Task, Process \n\n# Create the Tool for LLaMA inference\ndeepseek_tool = CustomTool(\n    name=\"deepseek_llama_3.3_70B\",\n    func=lambda inputs: deepseek_llama_inference(\n        prompt=inputs,\n        endpoint_name=deepseek_endpoint\n    ),\n    description=\"A tool to generate text using the DeepSeek LLaMA model deployed on SageMaker.\"\n)\n</code></pre></div><p> Just like the simple blocks agent we defined earlier, we follow the same template here to define the research agent. The difference here is that we give more capabilities to this agent. We attach a SageMaker AI based DeepSeek-R1 model as an endpoint for the LLM.</p><p>This helps the research agent think critically about information processing by combining the scalable infrastructure of SageMaker with DeepSeek-R1’s advanced reasoning capabilities.</p><p>The agent uses the SageMaker hosted LLM to analyze patterns in research data, evaluate source credibility, and synthesize insights from multiple inputs. By using the deepseek_tool, the agent can dynamically adjust its research strategy based on intermediate findings, validate hypotheses through iterative questioning, and maintain context awareness across complex information it gathers.</p><div><pre><code># Research Agent\n\nresearch_agent = Agent(\n    role=\"Research Bot\",\n    goal=\"Scan sources, extract relevant information, and compile a research summary.\",\n    backstory=\"An AI agent skilled in finding relevant information from a variety of sources.\",\n    tools=[deepseek_tool],\n    allow_delegation=True,\n    llm=DeepSeekSageMakerLLM(endpoint=deepseek_endpoint),\n    verbose=False\n)\n</code></pre></div><p> The writer agent is configured as a specialized content editor that takes research data and transforms it into polished content. This agent works as part of a workflow where it takes research from a research agent and acts like an editor by formatting the content into a readable format. The agent is used for writing and formatting, and unlike the research agent, it doesn’t delegate tasks to other agents.</p><div><pre><code>writer_agent = Agent(\n    role=\"Writer Bot\",\n    goal=\"Receive research summaries and transform them into structured content.\",\n    backstory=\"A talented writer bot capable of producing high-quality, structured content based on research.\",\n    tools=[deepseek_tool],\n    allow_delegation=False,\n    llm=DeepSeekSageMakerLLM(endpoint=deepseek_endpoint),\n    verbose=False\n)</code></pre></div><p><strong>Define tasks for the agents</strong> Tasks in CrewAI define specific operations that agents need to perform. In this example, we have two tasks: a research task that processes queries and gathers information, and a writing task that transforms research data into polished content.</p><p>Each task includes a clear description of what needs to be done, the expected output format, and specifies which agent will perform the work. This structured approach makes sure that agents have well-defined responsibilities and clear deliverables.</p><p>Together, these tasks create a workflow where one agent researches a topic on the internet, and another agent takes this research and formats it into readable content. The tasks are integrated with the DeepSeek tool for advanced language processing capabilities, enabling a production-ready deployment on SageMaker AI.</p><div><pre><code>research_task = Task(\n    description=(\n        \"Your task is to conduct research based on the following query: {prompt}.\\n\"\n    ),\n    expected_output=\"A comprehensive research summary based on the provided query.\",\n    agent=research_agent,\n    tools=[deepseek_tool]\n)\n\nwriting_task = Task(\n    description=(\n              \"Your task is to create structured content based on the research provided.\\n\"\"),\n    expected_output=\"A well-structured article based on the research summary.\",\n    agent=research_agent,\n    tools=[deepseek_tool]\n)\n</code></pre></div><p> A crew in CrewAI represents a collaborative group of agents working together to achieve a set of tasks. Each crew defines the strategy for task execution, agent collaboration, and the overall workflow. In this specific example, the sequential process makes sure tasks are executed one after the other, following a linear progression. There are other more complex orchestrations of agents working together, which we will discuss in future blog posts.</p><p>This approach is ideal for projects requiring tasks to be completed in a specific order. The workflow creates two agents: a research agent and a writer agent. The research agent researches a topic on the internet, then the writer agent takes this research and acts like an editor by formatting it into a readable format.</p><p>Let’s call the crew scribble_bots:</p><div><pre><code># Define the Crew for Sequential Workflow # \n\nscribble_bots = Crew( agents=[research_agent, writer_agent], \n       tasks=[research_task, writing_task], \n       process=Process.sequential # Ensure tasks execute in sequence)\n</code></pre></div><p><strong>Use the crew to run a task</strong> We have our endpoint deployed, agents created, and crew defined. Now we’re ready to use the crew to get some work done. Let’s use the following prompt:</p><div><pre><code>result = scribble_bots.kickoff(inputs={\"prompt\": \"What is DeepSeek?\"})</code></pre></div><p>Our result is as follows:</p><div><pre><code>**DeepSeek: Pioneering AI Solutions for a Smarter Tomorrow**\n\nIn the rapidly evolving landscape of artificial intelligence, \nDeepSeek stands out as a beacon of innovation and practical application. \nAs an AI company, DeepSeek is dedicated to advancing the field through cutting-edge research and real-world applications, \nmaking AI accessible and beneficial across various industries.\n\n**Focus on AI Research and Development**\n\n………………….. ………………….. ………………….. …………………..\n</code></pre></div><p>Complete the following steps to clean up your resources:</p><ol><li>Delete your GPU DeekSeek-R1 endpoint:</li></ol><div><pre><code>import boto3\n\n# Create a low-level SageMaker service client.\nsagemaker_client = boto3.client('sagemaker', region_name=&lt;region&gt;)\n\n# Delete endpoint\nsagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n</code></pre></div><ol start=\"2\"><li>If you’re using a SageMaker Studio JupyterLab notebook, shut down the JupyterLab notebook instance.</li></ol><p>In this post, we demonstrated how you can deploy an LLM such as DeepSeek-R1—or another FM of your choice—from popular model hubs like SageMaker JumpStart or Hugging Face Hub to SageMaker AI for <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html\" target=\"_blank\" rel=\"noopener\">real-time inference</a>. We explored inference frameworks like Hugging Face TGI which helps streamline deployment while integrating built-in performance optimizations to minimize latency and maximize throughput. Additionally, we showcased how the SageMaker developer-friendly Python SDK simplifies endpoint orchestration, allowing seamless experimentation and scaling of LLM-powered applications.</p><p>Beyond deployment, this post provided an in-depth exploration of agentic AI, guiding you through its conceptual foundations, practical design principles using CrewAI, and the seamless integration of state-of-the-art LLMs like DeepSeek-R1 as the intelligent backbone of an autonomous agentic workflow. We outlined a sequential CrewAI workflow design, illustrating how to equip LLM-powered agents with specialized tools that enable autonomous data retrieval, real-time processing, and interaction with complex external systems.</p><p>Now, it’s your turn to experiment! Dive into our publicly available code on <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/tree/main/agents-with-sagemaker/deepseek_crewai_based_agent\" target=\"_blank\" rel=\"noopener\">GitHub</a>, and start building your own DeepSeek-R1-powered agentic AI system on SageMaker. Unlock the next frontier of AI-driven automation—seamlessly scalable, intelligent, and production-ready.</p><p>Special thanks to Giuseppe Zappia, Poli Rao, and Siamak Nariman for their support with this blog post.</p><p> is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the LLama family, and Qwen, focusing on their fine-tuning and optimization for specific scientific applications. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker, enabling the scaling of foundation models from development to production. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/06/26/Bobby-LIndsey.jpg\" alt=\"\" width=\"100\" height=\"141\">&nbsp;is a Machine Learning Specialist at Amazon Web Services. He’s been in technology for over a decade, spanning various technologies and multiple roles. He is currently focused on combining his background in software engineering, DevOps, and machine learning to help customers deliver machine learning workflows at scale. In his spare time, he enjoys reading, research, hiking, biking, and trail running.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/11/25/DSC03728_cleanup-PhotoRoom-1.jpeg\" alt=\"\" width=\"100\" height=\"140\"> is a Generative AI Specialist for third-party models at AWS, where he works with top-tier third-party foundation model (FM) providers to develop and execute joint Go-To-Market strategies, enabling customers to effectively train, deploy, and scale FMs to solve industry specific challenges. Karan holds a Bachelor of Science in Electrical and Instrumentation Engineering from Manipal University, a master’s in science in Electrical Engineering from Northwestern University and is currently an MBA Candidate at the Haas School of Business at University of California, Berkeley.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/08/29/Pranav-Profile-100.jpeg\" alt=\"\" width=\"100\" height=\"100\">&nbsp;is an AI/ML Specialist Solutions Architect at AWS. He focuses on helping customers build, train, deploy and migrate machine learning (ML) workloads to SageMaker. He previously worked in the semiconductor industry developing large computer vision (CV) and natural language processing (NLP) models to improve semiconductor processes using state of the art ML techniques. In his free time, he enjoys playing chess and traveling. You can find Pranav on&nbsp;<a href=\"https://www.linkedin.com/in/pranav-murthy-6bbb5773/\" target=\"_blank\" rel=\"noopener noreferrer\" data-stringify-link=\"https://www.linkedin.com/in/pranav-murthy-6bbb5773/\" data-sk=\"tooltip_parent\">LinkedIn</a>.</p>","contentLength":29139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Transformer LLMs Work [Free Course]","url":"https://newsletter.languagemodels.co/p/how-transformer-llms-work-free-course","date":1739214020,"author":"Jay Alammar","guid":390,"unread":true,"content":"<p>Enroll for free now: https://bit.ly/4aRnn7Z Github Repo: https://github.com/HandsOnLLM/Hands-On-Large-Language-Models </p><p>We're ecstatic to bring you \"How Transformer LLMs Work\" -- a free course with ~90 minutes of video, code, and crisp visuals and animations that explain the modern Transformer architecture, tokenizers, embeddings, and mixture-of-expert models.</p><p> and I have developed a lot of the visual language over the last several years (tens of thousands of iterations for hundreds of figures) for the book. This was informed by many incredible colleagues at Cohere, C4AI, and the open source and open science ML community. But to have an opportunity to collaborate with the legendary Andrew Ng and the team at DeepLearning.ai we took them to the next level with animations and a concise narrative meant to enable technical learners to pick up an ML paper and understand the architecture description. </p><p>In this course, you'll learn how a transformer network architecture that powers LLMs works. You'll build the intuition of how LLMs process text and work with code examples that illustrate the key components of the transformer architecture. </p><p>Key topics covered in this course include: </p><ul><li><p>The evolution of how language has been represented numerically, from the Bag-of-Words model through Word2Vec embeddings to the transformer architecture that captures word meanings in full context. </p></li><li><p>How LLM inputs are broken down into tokens, which represent words or pieces before they are sent to the language model. </p></li><li><p>The details of a transformer and the three main stages, consisting of tokenization and embedding, the stack of transformer blocks, and the language model head. </p></li><li><p>The details of the transformer block, including attention, which calculates relevance scores followed by the feedforward layer, which incorporates stored information learned in training. </p></li><li><p>How cached calculations make transformers faster, how the transformer block has evolved over the years since the original paper was released, and how they continue to be widely used. </p></li><li><p>Explore an implementation of recent models in the Hugging Face transformer library. </p></li></ul><p>By the end of this course, you’ll have a deep understanding of how LLMs process language and you'll be able to read through papers describing models and understand the details that are used to describe these architectures. This intuition will help improve your approach to building LLM applications.</p>","contentLength":2419,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8995a108-11cc-4edf-b69d-73006c087981_3050x1922.png","enclosureMime":"","commentsUrl":null},{"title":"Automate bulk image editing with Crop.photo and Amazon Rekognition","url":"https://aws.amazon.com/blogs/machine-learning/automate-bulk-image-editing-with-crop-photo-and-amazon-rekognition/","date":1739213429,"author":"Rahul Bhargava","guid":119,"unread":true,"content":"<p><a href=\"https://evolphin.com/creatives-collaborative-video-editing/\" target=\"_blank\" rel=\"noopener\">Evolphin Software</a>, Inc. is a leading provider of digital and media asset management solutions based in Silicon Valley, California. <a href=\"https://www.crop.photo/\" target=\"_blank\" rel=\"noopener\">Crop.photo</a> from Evolphin Software is a cloud-based service that offers powerful bulk processing tools for automating image cropping, content resizing, background removal, and listing image analysis.</p><p>Crop.photo is tailored for high-end retailers, ecommerce platforms, and sports organizations. The solution has created a unique offering for bulk image editing through its advanced AI-driven solutions. In this post, we explore how Crop.photo uses <a href=\"https://aws.amazon.com/rekognition/\" target=\"_blank\" rel=\"noopener\">Amazon Rekognition</a> to provide sophisticated image analysis, enabling automated and precise editing of large volumes of images. This integration streamlines the image editing process for clients, providing speed and accuracy, which is crucial in the fast-paced environments of ecommerce and sports.</p><h2>Automation: The way out of bulk image editing challenges</h2><p>Bulk image editing isn’t just about handling a high volume of images, it’s about delivering flawless results with speed at scale. Large retail brands, marketplaces, and sports industries process thousands of images weekly. Each image must be catalog-ready or broadcast-worthy in minutes, not hours.</p><p>The challenge lies not just in the quantity but in maintaining high-quality images and brand integrity. Speed and accuracy are non-negotiable. Retailers and sports organizations expect rapid turnaround without compromising image integrity.</p><p>This is where Crop.photo’s smart automations come in with an innovative solution for high-volume image processing needs. The platform’s advanced AI algorithms can automatically detect subjects of interest, crop the images, and optimize thousands of images simultaneously while providing consistent quality and brand compliance. By automating repetitive editing tasks, Crop.photo enables enterprises to reduce image processing time from hours to minutes, allowing creative teams to focus on higher-value activities.</p><h2>Challenges in the ecommerce industry</h2><p>The ecommerce industry often encounters the following challenges:</p><ul><li><strong>Inefficiencies and delays in manual image editing </strong>– Ecommerce companies rely on manual editing for tasks like resizing, alignment, and background removal. This process can be time-consuming and prone to delays and inconsistencies. A more efficient solution is needed to streamline the editing process, especially during platform migrations or large updates.</li><li><strong>Maintaining uniformity across diverse image types </strong>– Companies work with a variety of image types, from lifestyle shots to product close-ups, across different categories. Maintaining uniformity and professionalism in all image types is essential to meet the diverse needs of marketing, product cataloging, and overall brand presentation.</li><li><strong>Large-scale migration and platform transition </strong>– Transitioning to a new ecommerce platform involves migrating thousands of images, which presents significant logistical challenges. Providing consistency and quality across a diverse range of images during such a large-scale migration is crucial for maintaining brand standards and a seamless user experience.</li></ul><p>For a US top retailer, wholesale distribution channels posed a unique challenge. Thousands of fashion images need to be made for the marketplace with less than a day’s notice for flash sales. Their director of creative operations said,</p><blockquote><p><em>“Crop.photo is an essential part of our ecommerce fashion marketplace workflow. With over 3,000 on-model product images to bulk crop each month, we rely on Crop.photo to enable our wholesale team to quickly publish new products on popular online marketplaces such as Macy’s, Nordstrom, and Bloomingdales. By increasing our retouching team’s productivity by over 70%, Crop.photo has been a game changer for us. Bulk crop images used to take days can now be done in a matter of seconds!”</em></p></blockquote><h2>Challenges in the sports industry</h2><p>The sports industry often contends with the following challenges:</p><ul><li><strong>Bulk player headshot volume and consistency </strong>– Sports organizations face the challenge of bulk cropping and resizing hundreds of player headshots for numerous teams, frequently on short notice. Maintaining consistency and quality across a large volume of images can be difficult without AI.</li><li><strong>Diverse player facial features </strong>– Players have varying facial features, such as different hair lengths, forehead sizes, and face dimensions. Adapting cropping processes to accommodate these differences traditionally requires manual adjustments for each image, which leads to inconsistencies and significant time investment.</li><li><strong>Editorial time constraints </strong>– Tight editorial schedules and resource limitations are common in sports organizations. The time-consuming nature of manual cropping tasks strains editorial teams, particularly during high-volume periods like tournaments, where delays and rushed work can impact quality and timing.</li></ul><p>An Imaging Manager at Europe’s Premier Football Organization expressed,</p><blockquote><p><em>“We recently found ourselves with 40 images from a top flight English premier league club needing to be edited just 2 hours before kick-off. Using the <a href=\"https://www.crop.photo/blog/bulk-headshots-for-sports\" target=\"_blank\" rel=\"noopener\">Bulk AI headshot cropping for sports</a> feature from Crop.photo, we had perfectly cropped headshots of the squad in just 5 minutes, making them ready for publishing in our website CMS just in time. We would never have met this deadline using manual processes. This level of speed was unthinkable before, and it’s why we’re actively recommending Crop.photo to other sports leagues.”</em></p></blockquote><p>Crop.photo uses Amazon Rekognition to power a robust solution for bulk image editing. Amazon Rekognition offers features like object and scene detection, facial analysis, and image labeling, which they use to generate markers that drive a fully automated image editing workflow.</p><p>The following diagram presents a high-level architectural data flow highlighting several of the AWS services used in building the solution.</p><p>The solution consists of the following key components:</p><ul><li> – <a href=\"https://aws.amazon.com/cognito/\" target=\"_blank\" rel=\"noopener\">Amazon Cognito</a> is used for user authentication and user management.</li><li><strong>Infrastructure deployment</strong> – Frontend and backend servers are used on <a href=\"https://aws.amazon.com/ecs/\" target=\"_blank\" rel=\"noopener\">Amazon Elastic Container Service</a> (Amazon ECS) for container deployment, orchestration, and scaling.</li><li><strong>Content delivery and caching</strong> – <a href=\"https://aws.amazon.com/cloudfront/\" target=\"_blank\" rel=\"noopener\">Amazon CloudFront</a> is used to cache content, improving performance and routing traffic efficiently.</li><li> – Information about uploaded files and job execution is stored in <a href=\"https://aws.amazon.com/rds/aurora/\" target=\"_blank\" rel=\"noopener\">Amazon Aurora</a>.</li><li> – <a href=\"https://aws.amazon.com/batch/\" target=\"_blank\" rel=\"noopener\">AWS Batch</a> processes thousands of images in bulk.</li><li> – <a href=\"https://aws.amazon.com/sqs/\" target=\"_blank\" rel=\"noopener\">Amazon Simple Queue Service</a> (Amazon SQS) manages and queues jobs for processing, making sure they’re run in the correct order by AWS Batch.</li><li> – Amazon Rekognition services analyze media files, including: \n  <ul><li>Face Analysis to generate headless crops.</li><li>Moderation to detect and flag profanity and explicit content.</li><li>Label Detection to provide context for image processing and focus on relevant objects.</li><li>Custom Labels to identify and verify brand logos and adhere to brand guidelines.</li></ul></li></ul><p>Amazon Rekognition is an AWS computer vision service that powers Crop.photo’s automated image analysis. It enables object detection, facial recognition, and content moderation capabilities:</p><ul><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/faces.html\" target=\"_blank\" rel=\"noopener\">Face detection</a> – The Amazon Rekognition face detection feature automatically identifies and analyzes faces in product images. You can use this feature for face-based cropping and optimization through adjustable bounding boxes in the interface.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectLabelsImageProperties.html\" target=\"_blank\" rel=\"noopener\">Image color analysis</a> – The color analysis feature examines image composition, identifying dominant colors and balance. This integrates with Crop.photo’s brand guidelines checker to provide consistency across product images.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/labels.html\" target=\"_blank\" rel=\"noopener\">Object detection</a> – Object detection automatically identifies key elements in images, enabling smart cropping suggestions. The interface highlights detected objects, allowing you to prioritize specific elements during cropping.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/what-is.html\" target=\"_blank\" rel=\"noopener\">Custom label detection</a> – Custom label detection recognizes brand-specific items and assets. Companies can train models for their unique needs, automatically applying brand-specific cropping rules to maintain consistency.</li><li><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/text-detection.html\" target=\"_blank\" rel=\"noopener\">Text detection (OCR)</a> – The OCR capabilities of Amazon Recognition detect and preserve text within images during editing. The system highlights text areas to make sure critical product information remains legible after cropping.</li></ul><p>Within the Crop.photo interface, users can upload videos through the standard interface, and the speech-to-text functionality will automatically transcribe any audio content. This transcribed text can then be used to enrich the metadata and descriptions associated with the product images or videos, improving searchability and accessibility for customers. Additionally, the brand guidelines check feature can be applied to the transcribed text, making sure that the written content aligns with the company’s branding and communication style.</p><p>The Crop.photo service follows a transparent <a href=\"https://help.crop.photo/en/articles/9752452-understanding-your-subscription-plan\" target=\"_blank\" rel=\"noopener\">pricing model</a> that combines unlimited automations with a flexible image credit system. Users have unrestricted access to create and run as many automation workflows as needed, without any additional charges. The service includes a range of features at no extra cost, such as basic image operations, storage, and behind-the-scenes processing.</p><p>For advanced AI-powered image processing tasks, like smart cropping or background removal, users consume image credits. The number of credits required for each operation is clearly specified, allowing users to understand the costs upfront. Crop.photo offers several subscription plans with varying image credit allowances, enabling users to choose the plan that best fits their needs.</p><h2>Results: Improved speed and precision</h2><p>The automated image editing capabilities of Crop.photo with the integration of Amazon Rekognition has increased speed in editing, with 70% faster image retouching for ecommerce. With a 75% reduction in manual work, the turnaround time for new product images is reduced from 2–3 days to just 1 hour. Similarly, the bulk image editing process has been streamlined, allowing over 100,000 image collections to be processed per day using <a href=\"https://aws.amazon.com/fargate\" target=\"_blank\" rel=\"noopener\">AWS Fargate</a>. Advanced AI-powered image analysis and editing features provide consistent, high-quality images at scale, eliminating the need for manual review and approval of thousands of product images.</p><p>For instance, in the ecommerce industry, this integration facilitates automatic product detection and precise cropping, making sure every image meets specific marketplace and brand standards. In sports, it enables quick identification and cropping of player facial features, including head, eyes, and mouth, adapting to varying backgrounds and maintaining brand consistency.</p><p>The following images are before and after pictures for an ecommerce use case.</p><p>For a <a href=\"https://www.crop.photo/case-studies/bulk-content-resize-beverage\" target=\"_blank\" rel=\"noopener\">famous wine</a> retailer in the United Kingdom, the integration of Amazon Rekognition with Crop.photo streamlined the processing of over 1,700 product images, achieving a 95% reduction in bulk image editing time, a confirmation to the efficiency of AI-powered enhancement.</p><p>Similarly, a top 10 <a href=\"https://www.crop.photo/blog/bulk-on-model-product-images-face-crop\" target=\"_blank\" rel=\"noopener\">global specialty retailer</a> experienced a transformative impact on their ecommerce fashion marketplace workflow. By automating the cropping of over 3,000 on-model product images monthly, they boosted their retouching team’s productivity by over 70%, maintaining compliance with the varied image standards of multiple online marketplaces.</p><p>These case studies illustrate the tangible benefits of integrating Crop.photo with Amazon Rekognition, demonstrating how automation and AI can revolutionize the bulk image editing landscape for ecommerce and sports industries.</p><p>Crop.photo, from AWS Partner Evolphin Software, offers powerful bulk processing tools for automating image cropping, content resizing, and listing image analysis, using advanced AI-driven solutions. Crop.photo is tailored for high-end retailers, ecommerce platforms, and sports organizations. Its integration with Amazon Rekognition aims to streamline the image editing process for clients, providing speed and accuracy in the high-stakes environment of ecommerce and sports. Crop.photo plans additional AI capabilities with <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> generative AI frameworks to adapt to emerging digital imaging trends, so it remains an indispensable tool for its clients.</p><p>To learn more about Evolphin Software and Crop.photo, visit their <a href=\"https://www.crop.photo/\" target=\"_blank\" rel=\"noopener\">website</a>.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/28/Rahul-copy.png\" alt=\"\" width=\"100\" height=\"100\"> founder &amp; CTO of Evolphin Software and Crop.photo, is reshaping how brands produce and manage visual content at scale. Through Crop.photo’s AI-powered tools, global names like Lacoste and Urban Outfitters, as well as ambitious Shopify retailers, are rethinking their creative production workflows. By leveraging cutting-edge Generative AI, he’s enabling brands of all sizes to scale their content creation efficiently while maintaining brand consistency.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/28/vaishnavi-100-1.png\" alt=\"\" width=\"100\" height=\"127\"> is a Solutions Architect specializing in Cloud Security at AWS based in the San Francisco Bay Area. As a trusted technical advisor, Vaishnavi helps customers to design secure, scalable and innovative cloud solutions that drive both business value and technical excellence. Outside of work, Vaishnavi enjoys traveling and exploring different artisan coffee roasters.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/01/16/image-7-1.png\" alt=\"\" width=\"100\" height=\"100\"> is an Account Manager at AWS, who provides guidance to Evolphin Software and other organizations to help accelerate business outcomes leveraging AWS Technologies. John has a degree in Business Administration and Management with a concentration in Finance from Gonzaga University, and enjoys snowboarding in the Sierras in his free time.</p>","contentLength":13507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revolutionizing business processes with Amazon Bedrock and Appian’s generative AI skills","url":"https://aws.amazon.com/blogs/machine-learning/revolutionizing-business-processes-with-amazon-bedrock-and-appians-generative-ai-skills/","date":1739212621,"author":"Sunil Bemarkar","guid":118,"unread":true,"content":"<p><em>This blog post is co-written with Louis Prensky and Philip Kang from Appian.&nbsp;</em></p><p>The digital transformation wave has compelled enterprises to seek innovative solutions to streamline operations, enhance efficiency, and maintain a competitive edge. Recognizing the growing complexity of business processes and the increasing demand for automation, the integration of <a href=\"https://aws.amazon.com/generative-ai/\" target=\"_blank\" rel=\"noopener\">generative AI</a> skills into environments has become essential. This strategic move addresses key challenges such as managing vast amounts of unstructured data, adhering to regulatory compliance, and automating repetitive tasks to boost productivity. Using robust infrastructure and advanced language models, these AI-driven tools enhance decision-making by providing valuable insights, improving operational efficiency by automating routine tasks, and helping with data privacy through built-in detection and management of sensitive information. For enterprises, this means achieving higher levels of operational excellence, significant cost savings, and scalable solutions that adapt to business growth. For customers, it translates to improved service quality, enhanced data protection, and a more dynamic, responsive service, ultimately driving better experiences and satisfaction.</p><p>Appian has led the charge by offering generative AI skills powered by a collaboration with <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> and <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener\">Anthropic’s Claude</a> large language models (LLMs). This partnership allows organizations to:</p><ul><li>Enhance decision making with valuable insights</li><li>Improve operational efficiency by automating tasks</li><li>Help protect data privacy through built-in detection and management of sensitive information</li><li>Maintain compliance with HIPAA and FedRAMP compliant AI skills</li></ul><p>Critically, by placing AI in the context of a wider environment, organizations can operationalize AI in processes that seamlessly integrate with existing software, pass work between digital workers and humans, and help achieve strong security and compliance.</p><p><a href=\"https://www.appian.com/\" target=\"_blank\" rel=\"noopener\">Appian</a>, an&nbsp;<a href=\"https://partners.amazonaws.com/partners/001E000000Rl0w0IAB/Appian\" target=\"_blank\" rel=\"noopener\">AWS Partner</a>&nbsp;with competencies in financial services, healthcare, and life sciences, is a leading provider of&nbsp;<a href=\"https://www.appian.com/low-code-basics/\" target=\"_blank\" rel=\"noopener\">low-code automation software</a> to streamline and optimize complex business processes for enterprises. The Appian AI Process Platform includes everything you need to design, automate, and optimize even the most complex processes, from start to finish. The world’s most innovative organizations trust Appian to improve their workflows, unify data, and optimize operations—resulting in accelerated growth and superior customer experiences.</p><p><a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener\">Amazon Bedrock</a> is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.</p><p>Appian uses the robust infrastructure of Amazon Bedrock and Anthropic’s Claude LLMs to offer fully integrated, pre-built generative AI skills that help developers enhance and automate business processes using low-code development. These use case-driven tools automate common tasks in business processes, making AI-powered applications faster and easier to develop.</p><p>This blog post will cover how Appian AI skills build automation into organizations’ mission-critical processes to improve operational excellence, reduce costs, and build scalable solutions. Additionally, we’ll cover real-world examples of processes such as:</p><ul><li>A mortgage lender that used AI-driven data extraction to reduce mortgage processing times from 16 weeks to 10 weeks.</li><li>A financial services company that achieved a four-fold reduction in data extraction time from trade-related emails.</li><li>A legal institution that used AI to reduce attorney time spent on contract review, enabling them to focus on other, high-value work.</li></ul><h2>Current challenges faced by enterprises</h2><p>Modern enterprises face numerous challenges, including:</p><ul><li><strong>Managing vast amounts of unstructured data: </strong>Enterprises deal with immense volumes of data generated from various sources such as emails, documents, and customer interactions. Organizing, analyzing, and extracting valuable insights from unstructured data can be overwhelming without advanced AI capabilities.</li><li><strong>Help protect data privacy and compliance: </strong>With increasing regulatory requirements around data privacy and protection, organizations must safeguard sensitive information, such as personally identifiable information (PII). Manual processes for data redaction and compliance checks are often error-prone and resource-intensive.</li><li><strong>Streamlining repetitive and time-consuming tasks: </strong>Routine tasks such as data entry, document processing, and content classification consume significant time and effort. Automating these tasks can lead to substantial productivity gains and allow employees to focus on more strategic activities.</li><li><strong>Adapting to rapidly changing market conditions: </strong>In a fast-paced business environment, organizations need to be agile and responsive. This requires real-time data analysis and decision-making capabilities that traditional systems might not provide. AI helps businesses quickly adapt to industry changes and customer demands.</li><li><strong>Enhancing decision-making with accurate data insights: </strong>Making informed decisions requires access to accurate and timely data. However, extracting meaningful insights from large datasets can be challenging without advanced analytical tools. AI-powered solutions can process and analyze data at scale, providing valuable insights that drive better decision-making.</li></ul><h2>Appian AI service architecture</h2><p>The architecture of the generative AI skills integrates both the Amazon Bedrock and <a href=\"https://aws.amazon.com/textract/\" target=\"_blank\" rel=\"noopener\">Amazon Textract</a> scalable infrastructure with Appian’s process management capabilities. This generative AI architecture is designed with&nbsp;<a href=\"https://docs.appian.com/suite/help/24.3/private-ai.html\" target=\"_blank\" rel=\"noopener\">private AI</a>&nbsp;as the foundation and upholds those principles.</p><p>The key components of this architecture include:</p><ol><li><strong>Appian AI Process Platform instances</strong>: The frontend serves as the primary application environment where users interact with the system application to upload documents, initiate workflows, and view processed results.</li><li>: This service functions as an intermediary layer between the Appian instances and AWS AI services (Amazon Textract and Amazon Bedrock). This layer encapsulates the logic required to interact with the AWS AI services to manage API calls, data formatting, and error handling.</li><li>: This AWS service is used to automate the extraction of text and structured data from scanned documents and images and provide the extracted data in a structured format.</li><li>: This AWS service provides advanced AI capabilities using FMs for tasks such as text summarization, sentiment analysis, and natural language understanding. This helps enhance the extracted data with deeper insights and contextual understanding.</li></ol><p>Appian generative AI skills, powered by Amazon Bedrock with Anthropic’s Claude family of LLMs, are designed to jump-start the use of generative AI in your processes. The following figure showcases the diverse capabilities of Appian’s generative AI skills, demonstrating how they enable enterprises to seamlessly automate complex tasks.</p><p>Each new skill provides a pre-populated prompt template tailored to specific tasks, alleviating the need to start from scratch. Businesses can select the desired action and customize the prompt for a perfect fit, enabling the automation of tasks such as:</p><ul><li><strong>Content analysis and processing</strong>: With Appian’s generative AI skills, businesses can automatically generate, summarize, and classify content across various formats. This capability is particularly useful for managing large volumes of customer feedback, generating reports, and creating content summaries, significantly reducing the time and effort required for manual content processing.</li><li>: Organizations generate mountains of data and documents. Extracting this information manually can be both burdensome and error-prone. Appian’s AI skills can perform highly accurate text extraction from PDF files and scanned images and pull relevant data from both structured and unstructured data sources such as invoices, forms, and emails. This speeds up data processing and promotes higher accuracy and consistency.</li><li><strong>PII extraction and redaction</strong>: Identifying and managing PII within large datasets is crucial for data governance and compliance. Appian’s AI skills can automatically identify and extract sensitive information from documents and communication channels. Additionally, Appian supports plugins that can redact this content for further privacy. This assists your compliance efforts without extensive manual intervention.</li><li>: Appian’s AI skills can summarize documents to give users an overview before digging into the details. Whether it’s summarizing research papers, legal documents, or internal reports, AI can generate concise summaries, saving time and making sure that critical information is highlighted for quick review.</li></ul><p>The following figure shows an example of a prompt-builder skill used to extract unstructured data from a bond certificate.</p><p>Each AI skill offers pre-populated prompt templates, allowing you to deploy AI without starting from scratch. Each template caters to specific business needs, making implementation straightforward and efficient. Plus, users can customize these prompts to fit their unique requirements and operational needs.</p><p>In this solution, Appian Cloud seamlessly integrates and customizes Amazon Bedrock and Claude LLMs behind the scenes, abstracting complexity to deliver enterprise-grade AI capabilities tailored to its cloud environment. It provides pre-built, use case specific prompt templates for tasks like text summarization and data extractions, dynamically customized based on user inputs and business context. Using the scalability of the Amazon Bedrock infrastructure, Appian Cloud provides optimal performance and efficient handling of enterprise-scale workflows, all within a fully managed cloud service.</p><p>By addressing these complexities, Appian Cloud empowers businesses to focus solely on using AI to achieve operational excellence and business outcomes without the burdens of technical setup, integration challenges, or ongoing maintenance efforts.</p><p>Appian’s AI skills have proven effective across multiple industries. Here are a few real-world examples:</p><ul><li>: This organization automated the extraction of over 60 data fields from inconsistent document formats, reducing the process timeline from 16 weeks to 10 weeks and achieving 98.33% accuracy. The implementation of Appian’s generative AI skills allowed the mortgage processor to streamline their workflow, significantly cutting down on processing time and improving data accuracy, which led to faster loan approvals and increased customer satisfaction.</li><li>: A financial service company received over 1,000 loosely structured emails about trades. Manually annotating these emails led to significant human errors. With an Appian generative AI skill, the customer revamped the entity tagging process by automatically extracting approximately 40 data fields from unstructured emails. This resulted in a four-fold reduction in extraction time and achieved over 95% accuracy, improving the user experience compared to traditional ML extraction tools. The automated process not only reduced errors but also enhanced the speed and reliability of data extraction, leading to more accurate and timely trading decisions.</li><li>: A legal institution had to review negotiated contracts against the original contracts to determine whether the outlined risks had been resolved. This manual process was error prone and labor intensive. By deploying a generative AI skill, they automated the extraction of changes between contracts to find the differences and whether risks had been resolved. This streamlined the attorney review process and provided insights and reasoning into the differences found. The automated solution significantly reduced the time attorneys spent on contract review, allowing them to focus on more strategic tasks and improving the overall efficiency of the legal department.</li></ul><p>AWS and Appian’s collaboration marks a significant advancement in business process automation. By using the power of Amazon Bedrock and Anthropic’s Claude models, Appian empowers enterprises to optimize and automate processes for greater efficiency and effectiveness. This partnership sets a new standard for AI-driven business solutions, leading to greater growth and enhanced customer experiences. The ability to quickly deploy and customize AI skills allows businesses to stay agile and responsive in a dynamic environment.</p><p><a href=\"https://aws.amazon.com/marketplace/seller-profile?id=94a5a278-5f8c-4a9e-82e4-d3f0dd2ecbfe\" target=\"_blank\" rel=\"noopener\">Appian</a> solutions&nbsp;are available as software as a service (SaaS) offerings in&nbsp;<a href=\"https://aws.amazon.com/marketplace\" target=\"_blank\" rel=\"noopener\">AWS Marketplace</a>. Check out the <a href=\"https://docs.appian.com/suite/help/24.3/ai-skills-intro.html\" target=\"_blank\" rel=\"noopener\">Appian</a> website to learn more about how to use the AI skills.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/03/07/hsSunil.png\" alt=\"Sunil Bemarkar\" width=\"100\" height=\"125\">is a Senior Partner Solutions Architect at Amazon Web Services. He works with various Independent Software Vendors (ISVs) and Strategic customers across industries to accelerate their digital transformation journey and cloud adoption.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/01/28/John-Klacynski-100.jpeg\" alt=\"\" width=\"100\" height=\"100\">is a Principal Customer Solution Manager within the AWS Independent Software Vendor (ISV) team. In this role, he programmatically helps ISV customers adopt AWS technologies and services to reach their business goals more quickly.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/05/louis.jpg\" alt=\"\" width=\"100\" height=\"100\">is a Senior Product Manager at Appian. He is responsible for driving product strategy and feature design for AI Skills within Appian’s Cognitive Automation Group.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/02/01/Appian-Philip-Kang-100x100.jpeg\" alt=\"Philip Kang\" width=\"100\" height=\"100\">is a Principal Solutions Consultant in Partner Technology &amp; Innovation centers with Appian. In this role, he spearheads technical innovation with a focus on AI/ML and cloud solutions.</p>","contentLength":13619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beginner’s Guide to Subqueries in SQL","url":"https://www.kdnuggets.com/beginners-guide-subqueries-sql","date":1739206838,"author":"Nate Rosidi","guid":245,"unread":true,"content":"<article>Subqueries are popular tools for more complex data manipulation in SQL. If you’re a beginner on a quest to understand subqueries, this is the article for you.</article>","contentLength":160,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Rosidi_Beginners_Guide_to_Subqueries_in_SQL_5.png.png","enclosureMime":"","commentsUrl":null},{"title":"Data Science Showdown: Which Tools Will Gain Ground in 2025","url":"https://www.kdnuggets.com/data-science-showdown-tools-gain-ground-2025","date":1739199602,"author":"Iván Palomares Carrascosa","guid":244,"unread":true,"content":"<article>An analysis and discussion of the data science tools expected to gain prominence throughout the present year, and why.</article>","contentLength":118,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/kdn-tools-battle-2025.png","enclosureMime":"","commentsUrl":null},{"title":"Using Gemini 2.0 Pro Locally","url":"https://www.kdnuggets.com/using-gemini-2-pro-locally","date":1739192434,"author":"Abid Ali Awan","guid":243,"unread":true,"content":"<article>Learn the easiest way to use a state-of-the-art Google experimental model locally.</article>","contentLength":82,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/awan_gemini_20_pro_locally_7.png","enclosureMime":"","commentsUrl":null},{"title":"Transformational Power of Marginal Propensity to Reuse Undermined by Siren of One-off AI Projects","url":"https://www.datasciencecentral.com/transformational-power-of-marginal-propensity-to-reuse-undermined-by-siren-of-one-off-ai-projects/","date":1739192355,"author":"Bill Schmarzo","guid":59,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["ai"]}