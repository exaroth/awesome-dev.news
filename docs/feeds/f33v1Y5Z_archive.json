{"id":"f33v1Y5Z","title":"Latest","displayTitle":"Latest","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":940,"items":[{"title":"China Successfully Tests Hypersonic Aircraft, Maybe At Mach 12","url":"https://tech.slashdot.org/story/25/07/01/2222223/china-successfully-tests-hypersonic-aircraft-maybe-at-mach-12?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751450400,"author":"BeauHD","guid":180371,"unread":true,"content":"China's Northwestern Polytechnical University successfully tested a hypersonic aircraft called Feitian-2, claiming it reached Mach 12 and achieved a world-first by autonomously switching between rocket and ramjet propulsion mid-flight. The Register reports: The University named the craft \"Feitian-2\" and according to Chinese media the test flight saw it reach Mach 12 (14,800 km/h or 9,200 mph) -- handily faster than the Mach 5 speeds considered to represent hypersonic flight. Chinese media have not detailed the size of Feitian-2, or its capabilities other than to repeat the University's claim that it combined a rocket and a ramjet into a single unit. [...] The University and Chinese media claim the Feitian-2 flew autonomously while changing from rocket to ramjet while handling the hellish stresses that come with high speed flight.\n \nThis test matters because, as the US Congressional Budget Office found in 2023, hypothetical hypersonic missiles \"have the potential to create uncertainty about what their ultimate target is. Their low flight profile puts them below the horizon for long-range radar and makes them difficult to track, and their ability to maneuver while gliding makes their path unpredictable.\" \"Hypersonic weapons can also maneuver unpredictably at high speeds to counter short-range defenses near a target, making it harder to track and intercept them,\" the Office found.\n \nWashington is so worried about Beijing developing hypersonic weapons that the Trump administration cited the possibility as one reason for banning another 27 Chinese organizations from doing business with US suppliers of AI and advanced computing tech. The flight of Feitian-2 was therefore a further demonstration of China's ability to develop advanced technologies despite US bans.","contentLength":1786,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Huawei releases an open weight model trained on Huawei Ascend GPUs","url":"https://arxiv.org/abs/2505.21411","date":1751441801,"author":"buyucu","guid":180369,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44441089"},{"title":"Podcast Episode: Cryptography Makes a Post-Quantum Leap","url":"https://www.eff.org/deeplinks/2025/06/podcast-episode-cryptography-makes-post-quantum-leap","date":1751439917,"author":"Josh Richman","guid":180362,"unread":true,"content":"<p><a href=\"https://en.wikipedia.org/wiki/RSA_cryptosystem\" target=\"_blank\" rel=\"noopener noreferrer\"></a><a href=\"https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange\" target=\"_blank\" rel=\"noopener noreferrer\"></a><a href=\"https://en.wikipedia.org/wiki/Elliptic-curve_cryptography\" target=\"_blank\" rel=\"noopener noreferrer\"></a></p><p><a href=\"https://en.wikipedia.org/wiki/Quantum_computing\" target=\"_blank\" rel=\"noopener noreferrer\"></a></p><ul></ul><p> I only got into cryptography and especially post quantum quickly after that. further into my professional life. I was a software engineer for a whil,e and the Snowden leaks happened, and phone records get leaked. All of Verizon's phone records get leaked. and then Prism and more leaks and more leaks. And as an engineer first, I felt like everything that I was building and we were building and telling people to use was vulnerable. I wanted to learn more about how to do things securely. I went further and further and further down the rabbit hole of cryptography. And then, I think I saw a talk which was basically like, oh, elliptic curves are vulnerable to a quantum attack. And I was like, well, I, I really like these things. They're very elegant mathematical objects, it's very beautiful. I was sad that they were fundamentally broken, and, I think it was, Dan Bernstein who was like, well, there's this new thing that uses elliptic curves, but is supposed to be post quantum secure. <p>But the math is very difficult and no one understands it. I was like, well, I want to understand it if it preserves my beautiful elliptic curves. That's how I just went, just running, screaming downhill into post quantum cryptography.</p></p><p> That's Deirdre Connolly talking about how her love of beautiful math and her anger at the Snowden revelations about how the government was undermining security, led her to the world of post-quantum cryptography.I'm Cindy Cohn, the executive director of the Electronic Frontier Foundation.</p><p> And I'm Jason Kelley, EFF's activism director. You're listening to How to Fix the Internet.</p><p> On this show we talk to tech leaders, policy-makers, thinkers, artists and engineers about what the future could look like if we get things right online.</p><p> Our guest today is at the forefront of the future of digital security. And just a heads up that this is one of the more technical episodes that we've recorded -- you'll hear quite a bit of cryptography jargon, so we've written up some of the terms that come up in the show notes, so take a look there if you hear a term you don't recognize.</p><p> Deidre Connolly is a research engineer and applied cryptographer at Sandbox AQ, with a particular expertise in post-quantum encryption. She also co-hosts the Security, Cryptography, Whatever podcast, so she's something of a cryptography influencer too. When we asked our tech team here at EFF who we should be speaking with on this episode about quantum cryptography and quantum computers more generally, everyone agreed that Deirdre was the person. So we're very glad to have you here. Welcome, Deirdre.</p><p> Thank you very much for having me. Hi.</p><p> Now we obviously work with a lot of technologists here and, and certainly personally cryptography is near and dear to my heart, but we are not technologists, neither Jason nor I. So can you just give us a baseline of what post-quantum cryptography is and why people are talking about it?</p><p> Sure. So a lot of the cryptography that we have deployed in the real world relies on a lot of math and security assumptions on that math based on things like abstract groups, Diffie-Hellman, elliptic curves, finite fields, and factoring prime numbers such as, uh, systems like RSA. All of these, constructions and problems, mathematical problems, have served us very well in the last 40-ish years of cryptography. They've let us build very useful, efficient, small cryptography that we've deployed in the real world. It turns out that they are all also vulnerable in the same way to advanced cryptographic attacks that are only possible and only efficient when run on a quantum computer, and this is a class of computation, a whole new class of computation versus digital computers, which is the main computing paradigm that we've been used to for the last 75 years plus. <p>Quantum computers allow these new classes of attacks, especially, variants of Shore's algorithm – named Dr. Peter Shore – that basically when run on a sufficiently large, cryptographically relevant quantum computer, makes all of the asymmetric cryptography based on these problems that we've deployed very, very vulnerable. </p>So post-quantum cryptography is trying to take that class of attack into consideration and building cryptography to both replace what we've already deployed and make it resilient to this kind of attack, and trying to see what else we can do with these fundamentally different mathematical and cryptographic assumptions when building cryptography.</p><p> So we've kind of, we've secured our stuff behind a whole lot of walls, and we're slowly building a bulldozer. This is a particular piece of the world where the speed at which computers can do things has been part of our protection, and so we have to rethink that.</p><p> Yeah, quantum computing is a fundamentally new paradigm of how we process data that promises to have very interesting, uh, and like, applications beyond what we can envision right now. Like things like protein folding, chemical analysis, nuclear simulation, and cryptanalysts, or very strong attacks against cryptography.But it is a field where it's such a fundamentally new computational paradigm that we don't even know what its applications fully would be yet, because like we didn't fully know what we were doing with digital computers in the forties and fifties. Like they were big calculators at one time.</p><p> When it was suggested that we talk to you about this. I admit that I have not heard much about this field, and I realized quickly when looking into it that there's sort of a ton of hype around quantum computing and post-quantum cryptography and that kind of hype can make it hard to know whether or not something is like actually going to be a big thing or, whether this is something that's becoming like an investment cycle, like a lot of things do. And one of the things that quickly came up as an actual, like real danger is what's called sort of “save now decrypt later.”</p><p> Oh yeah.</p><p> Right? We have all these messages, for example, that have been encrypted with current encryption methods. And if someone holds onto those, they can decrypt them using quantum computers in the future. How serious is that danger?</p><p> It’s definitely a concern and it's the number one driver I would say to post-quantum crypto adoption in broad industry right now is mitigating the threat of a Store Now/Decrypt Later attack, also known as Harvest Now/Decrypt Later, a bunch of names that all mean the same thing.And fundamentally, it's, uh, especially if you're doing any kind of key agreement over a public channel, and doing key agreement over a public channel is part of the whole purpose of like, you want to be able to talk to someone who you've never really, touched base with before, and you all kind of know, some public parameters that even your adversary knows and based on just the fact that you can send messages to each other and some public parameters, and some secret values that only you know, and only the other party knows you can establish a shared secret, and then you can start encrypting traffic between you to communicate. And this is what you do in your web browser when you have an HTTPS connection, that's over TLS.<p>This is what you do with Signal or WhatsApp or any, or, you know, Facebook Messenger with the encrypted communications. They're using Diffie-Helman as part of the protocol to set up a shared secret, and then you use that to encrypt their message bodies that you're sending back and forth between you.</p>But if you can just store all those communications over that public channel, and the adversary knows the public parameters 'cause they're freely published, that's part of Kerckhoff’s Principle about good cryptography - the only thing that the adversary shouldn't know about your crypto system is the secret key values that you're actually using. It should be secure against an adversary that knows everything that you know, except the secret key material. <p>And you can just record all those public messages and all the public key exchange messages, and you just store them in a big database somewhere. And then when you have your large cryptographically relevant quantum computer, you can rifle through your files and say, hmm, let's point it at this.</p>And that's the threat that's live now to the stuff that we have already deployed and the stuff that we're continuing to do communications on now that is protected by elliptic curve Diffie Hellman, or Finite Field Diffie Hellman, or RSA. They can just record that and just theoretically point an attack at it at a later date when that attack comes online. <p>So like in TLS, there's a lot of browsers and servers and infrastructure providers that have updated to post-quantum resilient solutions for TLS. So they're using a combination of the classic elliptic curve, Diffie Hellman and a post-quantum KEM, uh, called ML Kem that was standardized by the United States based on a public design that's been, you know, a multi international collaboration to help do this design. </p>I think that's been deployed in Chrome, and I think it's deployed by CloudFlare and it's getting deployed – I think it's now become the default option in the latest version of Open SSL. And a lot of other open source projects, so that's TLS similar, approaches are being adopted in open SSH, the most popular SSH implementation in the world. Signal, the service has updated their key exchange to also include a post quantum KEM and their updated key establishments. So when you start a new conversation with someone or reset a conversation with someone that is the latest version of Signal is now protected against that sort of attack. <p>That is definitely happening and it's happening the most rapidly because of that Store now/Decrypt later attack, which is considered live. Everything that we're doing now can just be recorded and then later when the attack comes online, they can attack us retroactively. So that's definitely a big driver of things changing in the wild right now.</p></p><p> Okay. I'm going to throw out two parallels for my very limited knowledge to make sure I understand. This reminds me a little bit of sort of the work that had to be done before Y2K in, in the sense of like, now people think nothing went wrong and nothing was ever gonna go wrong, but all of us working anywhere near the field know actually it took a ton of work to make sure that nothing blew up or stopped working. And the other is that in, I think it was 1998, EFF was involved in something we called Deep Crack, where we made, that's a, I'm realizing now that's a terrible name. But anyway, the DES cracker, um, we basically wanted to show that DES was capable of being cracked, right? And that this was a - correct me if I'm wrong - it was some sort of cryptographic standard that the government was using and people wanted to show that it wasn't sufficient.</p><p> Yes - I think it was the first digital encryption standard. And then after its vulnerability was shown, they, they tripled it up to, to make it useful. And that's why Triple DES is still used in a lot of places and is actually considered okay. And then later came the advanced encryption standard, AES, which we prefer today.</p><p> Okay, so we've learned the lesson, or we are learning the lesson, it sounds like.</p><p> Yeah, I think that that's, that's right. I mean, EFF built the DES cracker because in the nineties the government was insisting that something that everybody knew was really, really insecure and was going to only get worse as computers got stronger and, and strong computers got in more people's hands, um, to basically show that the emperor had no clothes, um, that this wasn't very good. And I think with the NIST standards and what's happening with post-quantum is really, you know, the hopeful version is we learned that lesson and we're not seeing government trying to pretend like there isn't a risk in order to preserve old standards, but instead leading the way with new ones. Is that fair?</p><p> That is very fair. NIST ran this post-quantum competition almost over 10 years, and it had over 80 submissions in the first round from all over the world, from industry, academia, and a mix of everything in between, and then it narrowed it down to. the three that are, they're not all out yet, but there's the key agreement, one called ML Kem, and three signatures. And there's a mix of cryptographic problems that they're based on, but there were multiple rounds, lots of feedback, lots of things got broken. This competition has absolutely led the way for the world of getting ready for post-quantum cryptography. There are some competitions that have happened in Korea, and I think there's some work happening in China for their, you know, for their area.<p>There are other open standards and there are standards happening in other standards bodies, but the NIST competition has led the way, and it, because it's all open and all these standards are open and all of the work and the cryptanalysis that has gone in for the whole stretch. It's all been public and all these standards and drafts and analysis and attacks have been public. It's able to benefit everyone in the world.</p></p><p> I got started in the crypto wars in the nineties where the government was kind of the problem and they still are. And I do wanna ask you about whether you're seeing any role of the kinda national social security, FBI infrastructure, which has traditionally tried to put a thumb on the scales and make things less secure so that they could have access, if you're seeing any of that there. But on the NIST side, I think this provides a nice counter example of how government can help facilitate building a better world sometimes, as opposed to being the thing we have to drag kicking and screaming into it.<p>But let me circle around to the question I embedded in that, which is, you know, one of the problems that that, that we know happened in the nineties around DES, and then of course some of the Snowden revelations indicated some mucking about in security as well behind the scenes by the NSA. Are you seeing anything like that and, and what should we be on the lookout for?</p></p><p> Not in the PQC stuff. Uh, there, like there have been a lot of people that were paying very close attention to what these independent teams were proposing and then what was getting turned into a standard or a proposed standard and every little change, because I, I was closely following the key establishment stuff.Um, every little change people were trying to be like, did you tweak? Why did you tweak that? Did, like, is there a good reason? And like, running down basically all of those things. And like including trying to get into the nitty gritty of like. Okay. We think this is approximately these many bits of security using these parameter and like talking about, I dunno, 123 versus 128 bits and like really paying attention to all of that stuff.<p>And I don't think there was any evidence of anything like that. And, and for, for plus or minus, because there were. I don't remember which crypto scheme it was, but it, there was definitely an improvement from, I think some of the folks at NSA very quietly back in the day to, I think it was the S boxes, and I don't remember if it was DES or AES or whatever it was.</p>But people didn't understand at the time because it was related to advanced, uh, I think it was a differential crypto analysis attacks that folks inside there knew about, and people in outside academia didn't quite know about yet. And then after the fact they were like, oh, they've made this better. Um, we're not, we're not even seeing any evidence of anything of that character either.<p>It's just sort of like, it's very open letting, like if everything's proceeding well and the products are going well of these post-quantum standards, like, you know, leave it alone. And so everything looks good. And like, especially for NSA, uh, national Security Systems in the, in the United States, they have updated their own targets to migrate to post-quantum, and they are relying fully on the highest security level of these new standards.</p>So like they are eating their own dog food. They're protecting the highest classified systems and saying these need to be fully migrated to fully post quantum key agreement. Uh, and I think signatures at different times, but there has to be by like 2035. So if they were doing anything to kind of twiddle with those standards, they'd be, you know, hurting themselves and shooting themselves in the foot.</p><p> Well fingers crossed.</p><p> Because I wanna build a better internet and a better. Internet means that they aren't secretly messing around with our security. And so this is, you know, cautiously good news. Let's take a quick moment to thank our sponsor.“How to Fix the Internet” is supported by The Alfred P. Sloan Foundation’s Program in Public Understanding of Science and Technology. Enriching people’s lives through a keener appreciation of our increasingly technological world and portraying the complex humanity of scientists, engineers, and mathematicians.<p>We also want to thank EFF members and donors. EFF has been fighting for digital rights for 35 years, and that fight is bigger than ever, so please, if you like what we do, go to eff.org/pod to donate. Also, we’d love for you to join us at this year’s EFF awards, where we celebrate the people working towards the better digital future that we all care so much about. Those are coming up on September 12th in San Francisco. You can find more information about that at eff.org/awards.</p>We also wanted to share that our friend Cory Doctorow has a new podcast. Listen to this.&nbsp; [Who Broke the Internet trailer]</p><p> And now, back to our conversation with Deirdre Connolly.</p><p> I think the thing that's fascinating about this is kind of seeing this cat and mouse game about the ability to break codes, and the ability to build codes and systems that are resistant to the breaking, kind of playing out here in the context of building better computers for everyone.And I think it's really fascinating and I think it also for people I. You know, this is a pretty technical conversation, um, even, you know, uh, for our audience. But this is the stuff that goes on under the hood of how we keep journalists safe, how we keep activists safe, how we keep us all safe, whether it's our bank accounts or our, you know, people are talking about mobile IDs now and other, you know, all sorts of sensitive documents that are going to not be in physical form anymore, but are gonna be in digital form. <p>And unless we get this lock part right, we're really creating problems for people. And you know, what I really appreciate about you and the other people kind of in the midst of this fight is it's very unsung, right? It's kind of under the radar for the rest of us, but yet it's the, it's the ground that we need to stand on to, to be safe moving forward.</p></p><p> Yeah, and there's a lot of assumptions, uh, that even the low level theoretical cryptographers and the people implementing their, their stuff into software and the stuff, the people trying to deploy, that there's a, a lot of assumptions that have been baked into what we've built that to a degree don't quite fit in some of the, the things we've been able to build in a post-quantum secure way, or the way we think it's a post-quantum secure way.Um, we're gonna need to change some stuff and we think we know how to change some stuff to make it work. but we are hoping that we don't accidentally introduce any vulnerabilities or gaps. <p>We're trying, but also we're not a hundred percent sure that we're not missing something, 'cause these things are new. And so we're trying, and we're also trying to make sure we don't break things as we change them because we're trying to change them to be post quantum resilient. But you know, once you change something, if there's a possibility, you, you just didn't understand it completely. And you don't wanna break something that was working well in one direction because you wanna improve it in another direction.</p></p><p> And that's why I think it's important to continue to have a robust community of people who are the breakers, right? Who are, are hackers, who are, who are attacking. And that is a, you know, that's a mindset, right? That's a way of thinking about stuff that is important to protect and nurture, um, because, you know, there's an old quote from Bruce Schneider: Anyone can build a crypto system that they themselves cannot break. Right? It takes a community of people trying to really pound away at something to figure out where the holes are. And you know, a lot of the work that EFF does around coders rights and other kinds of things is to make sure that there's space for that. and I think it's gonna be as needed in a quantum world as it was in a kind of classical computer world.</p><p> Absolutely. I'm confident that we will learn a lot more from the breakers about this new cryptography because, like, we've tried to be robust through this, you know, NIST competition, and a lot of those, the things that we learn apply to other constructions as they come out. but like there's a whole area of people who are going to be encountering this kind of newish cryptography for the first time, and they kind of look at it and they're like. Oh, uh, I, I think I might be able to do something interesting with this, and we're, we'll all learn more and we'll try to patch and update as quickly as possible</p><p> And this is why we have competitions to figure out what the best options are and why some people might favor one algorithm over another for different, different processes and things like that.</p><p> And that's why we're probably gonna have a lot of different flavors of post-quantum cryptography getting deployed in the world because it's not just, ah, you know, I don't love NIST. I'm gonna do my own thing in my own country over here. Or, or have different requirements. There is that at play, but also you're trying to not put all your eggs in one basket as well.</p><p> Yeah, so we want a menu of things so that people can really pick, from, you know, vetted, but different strategies. So I wanna ask the kind of core question for the podcast, which is, um, what does it look like if we get this right, if we get quantum computing and, you know, post-quantum crypto, right?How does the world look different? Or does it just look the same? How, what, what does it look like if we do this well?</p><p> Hopefully to a person just using their phone or using their computer to talk to somebody on the other side of the world, hopefully they don't notice. Hopefully to them, if they're, you know, deploying a website and they're like, ah, I need to get a Let’s Encrypt certificate or whatever.Hopefully Let's Encrypt just, you know, insert bot just kind of does everything right by default and they don't have to worry about it. <p>Um, for the builders, it should be, we have a good recommended menu of cryptography that you can use when you're deploying TLS, when you're deploying SSH, uh, when you're building cryptographic applications, especially. </p>So like if you are building something in Go or Java or you know, whatever it might be, the crypto library in your language will have the updated recommended signature algorithm or key agreement algorithm and be, like, this is how we, you know, they have code snippets to say like, this is how you should use it, and they will deprecate the older stuff. <p>And, like, unfortunately there's gonna be a long time where there's gonna be a mix of the new post-quantum stuff that we know how to use and know how to deploy and protect. The most important, you know, stuff like to mitigate Store now/Decrypt later and, you know, get those signatures with the most important, uh, protected stuff.</p>Uh, get those done. But there's a lot of stuff that we're not really clear about. How we wanna do it yet, and kind of going back to one of the things you mentioned earlier, uh, comparing this to Y2K, there was a lot of work that went into mitigating Y2K before, during, immediately after.<p>Unfortunately, the comparison to the post quantum migration kind of falls down because after Y2K, if you hadn't fixed something, it would break. And you would notice in usually an obvious way, and then you could go find it. You, you fix the most important stuff that, you know, if it broke, like you would lose billions of dollars or, you know, whatever. You'd have an outage. </p>For cryptography, especially the stuff that's a little bit fancier. Um, you might not know it's broken because the adversary is not gonna, it's not gonna blow up.<p>And you have to, you know, reboot a server or patch something and then, you know, redeploy. If it's gonna fail, it's gonna fail quietly. And so we're trying to kind of find these things, or at least make the kind of longer tail of stuff, uh, find fixes for that upfront, you know, so that at least the option is available. </p>But for a regular person, hopefully they shouldn't notice. So everyone's trying really hard to make it so that the best security, in terms of the cryptography is deployed with, without downgrading your experience. We're gonna keep trying to do that.<p>I don't wanna build crap and say “Go use it.” I want you to be able to just go about your life and use a tool that's supposed to be useful and helpful. And it's not accidentally leaking all your data to some third party service or just leaving a hole on your network for any, any actor who notices to walk through and you know, all that sort of stuff.</p>So whether it's like implementing things securely in software, or it's cryptography or you know, post-quantum weirdness, like for me, I just wanna build good stuff for people, that's not crap.</p><p> Everyone listening to this agrees with you. We don't want to build crap. We want to build some beautiful things. Let's go out there and do it.</p><p> Thank you so much, Deirdre.</p><p> Thank you!</p><p> Thank you Deirdre. We really appreciate you coming and explaining all of this to, you know, uh, the lawyer and activist at EFF. Well, I think that was probably the most technical conversation we've had, but I followed along pretty well and I feel like at first I was very nervous based on the, save and decrypt concerns. But after we talked to Deirdre, I feel like the people working on this. Just like for Y2K are pretty much gonna keep us out of hot water. And I learned a lot more than I did know before we started the conversation. What about you, Cindy?</p><p> I learned a lot as well. I mean, cryptography and, attacks on security is always, you know, it's a process, and it's a process by which we do the best we can, and then, then we also do the best we can to rip it apart and find all the holes, and then we, we iterate forward. And it's nice to hear that that model is still the model, even as we get into something like quantum computers, which, um, frankly are still hard to conceptualize. But I agree. I think that what the good news outta this interview is I feel like there's a lot of pieces in place to try to do this right, to have this tremendous shift in computing that we don't know when it's coming, but I think that the research indicates that it SI coming, be something that we can handle, um, rather than something that overwhelms us.<p>And I think that's really,it's good to hear that good people are trying to do the right thing here since it's not inevitable.</p></p><p> Yeah, and it is nice when someone's sort of best vision for what the future looks like is hopefully your life. You will have no impacts from this because everything will be taken care of. That's always good. I mean, it sounds like, you know, the main thing for EFF is, as you said, we have to make sure that security engineers, hackers have the resources that they need to protect us from these kinds of threats and, and other kinds of threats obviously.<p>But, you know, that's part of EFF's job, like you mentioned. Our job is to make sure that there are people able to do this work and be protected while doing it so that when the. Solutions do come about. You know, they work and they're implemented and the average person doesn't have to know anything and isn't vulnerable.</p></p><p> Yeah, I also think that, um, I appreciated her vision that this is a, you know, the future's gonna be not just one. Size fits all solution, but a menu of things that take into account, you know, both what works better in terms of, you know, bandwidth and compute time, but also what you know, what people actually need.And I think that's a piece that's kind of built into the way that this is happening that's also really hopeful. In the past and, and I was around when EFF built the DES cracker, um, you know, we had a government that was saying, you know, you know, everything's fine, everything's fine when everybody knew that things weren't fine. <p>So it's also really hopeful that that's not the position that NIST is taking now, and that's not the position that people who may not even pick the NIST standards but pick other standards are really thinking through.</p></p><p> Yeah, it's very helpful and positive and nice to hear when something has improved for the better. Right? And that's what happened here. We had this, this different attitude from, you know, government at large in the past and it's changed and that's partly thanks to EFF, which is amazing.</p><p> Yeah, I think that's right. And, um, you know, we'll see going forward, you know, the governments change and they go through different things, but this is, this is a hopeful moment and we're gonna push on through to this future. I think there's a lot of, you know, there's a lot of worry about quantum computers and what they're gonna do in the world, and it's nice to have a little vision of, not only can we get it right, but there are forces in place that are getting it right. And of course it does my heart so, so good that, you know, someone like Deirdre was inspired by Snowden and dove deep and figured out how to be one of the people who was building the better world. We've talked to so many people like that, and this is a particular, you know, little geeky corner of the world. But, you know, those are our people and that makes me really happy.</p><p> Thanks for joining us for this episode of How to Fix the Internet.If you have feedback or suggestions, we'd love to hear from you. Visit EFF dot org slash podcast and click on listener feedback. While you're there, you can become a member, donate, maybe even pick up some merch and just see what's happening in digital rights this week and every week.<p>Our theme music is by Nat Keefe of BeatMower with Reed Mathis</p>How to Fix the Internet is supported by the Alfred P. Sloan Foundation's program in public understanding of science and technology.I’m Jason Kelley…</p><p> And I’m Cindy Cohn.</p><p><em> This podcast is licensed creative commons attribution 4.0 international, and includes the following music licensed creative commons attribution 3.0 unported by its creators: Drops of H2O, The Filtered Water Treatment by Jay Lang. Sound design, additional music and theme remixes by Gaetan Harris.</em></p>","contentLength":31025,"flags":null,"enclosureUrl":"https://www.eff.org/files/banner_library/2025-htfi-deirdre-blog.png","enclosureMime":"","commentsUrl":null},{"title":"Bezos-Backed Methane Tracking Satellite Is Lost In Space","url":"https://tech.slashdot.org/story/25/07/01/2211218/bezos-backed-methane-tracking-satellite-is-lost-in-space?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751439600,"author":"BeauHD","guid":180370,"unread":true,"content":"MethaneSAT, an $88 million satellite backed by Jeff Bezos and led by the Environmental Defense Fund to track global methane emissions, has been lost in space after going off course and losing power over Norway. \"We're seeing this as a setback, not a failure,\" Amy Middleton, senior vice president at EDF, told Reuters. \"We've made so much progress and so much has been learned that if we hadn't taken this risk, we wouldn't have any of these learnings.\" Reuters reports: The launch of MethaneSAT in March 2024 was a milestone in a years-long campaign by EDF to hold accountable the more than 120 countries that in 2021 pledged to curb their methane emissions. It also sought to help enforce a further promise from 50 oil and gas companies made at the Dubai COP28 climate summit in December 2023 to eliminate methane and routine gas flaring. [...] While MethaneSAT was not the only project to publish satellite data on methane emissions, its backers said it provided more detail on emissions sources and it partnered with Google to create a publicly-available global map of emissions.\n \nEDF reported the lost satellite to federal agencies including the National Oceanic and Atmospheric Administration, Federal Communications Commission and the U.S. Space Force on Tuesday, it said. Building and launching the satellite cost $88 million, according to the EDF. The organization had received a $100 million grant from the Bezos Earth Fund in 2020 and got other major financial support from Arnold Ventures, the Robertson Foundation and the TED Audacious Project and EDF donors. The project was also partnered with the New Zealand Space Agency. EDF said it had insurance to cover the loss and its engineers were investigating what had happened.\n \nThe organization said it would continue to use its resources, including aircraft with methane-detecting spectrometers, to look for methane leaks. It also said it was too early to say whether it would seek to launch another satellite but believed MethaneSAT proved that a highly sensitive instrument \"could see total methane emissions, even at low levels, over wide areas.\"","contentLength":2114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Risky Business #798 -- Mexican cartel surveilled the FBI to identify, kill witnesses","url":"https://risky.biz/RB798/","date":1751432067,"author":"","guid":180361,"unread":true,"content":"<article></article>","contentLength":0,"flags":null,"enclosureUrl":"https://dts.podtrac.com/redirect.mp3/media3.risky.biz/RB798.mp3","enclosureMime":"","commentsUrl":null},{"title":"Pivot probabilities and norm effects in Gaussian elimination for $\\beta$-ensembles","url":"https://arxiv.org/abs/2506.20470","date":1751428800,"author":"","guid":179599,"unread":true,"content":"<article>arXiv:2506.20470v2 Announce Type: replace-cross \nAbstract: We analyze pivot probabilities in Gaussian elimination with partial pivoting (GEPP) for $2 \\times 2$ random matrix ensembles. For GUE matrices, we resolve a previously reported discrepancy between theoretical predictions and empirical observations by deriving the exact pivot probability under standard LAPACK-style implementations. We further show that Dumitriu-Edelman tridiagonal $\\beta$-ensembles agree with the earlier theoretical expectations.</article>","contentLength":508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings","url":"https://arxiv.org/abs/2506.17064","date":1751428800,"author":"","guid":179600,"unread":true,"content":"<article>arXiv:2506.17064v3 Announce Type: replace-cross \nAbstract: Generating diverse, all-atom conformational ensembles of dynamic proteins such as G-protein-coupled receptors (GPCRs) is critical for understanding their function, yet most generative models simplify atomic detail or ignore conformational diversity altogether. We present latent diffusion for full protein generation (LD-FPG), a framework that constructs complete all-atom protein structures, including every side-chain heavy atom, directly from molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural network (ChebNet) to obtain low-dimensional latent embeddings of protein conformations, which are processed using three pooling strategies: blind, sequential and residue-based. A diffusion model trained on these latent representations generates new samples that a decoder, optionally regularized by dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a 2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor in a membrane environment, the sequential and residue-based pooling strategy reproduces the reference ensemble with high structural fidelity (all-atom lDDT of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route to system-specific, all-atom ensemble generation for large proteins, providing a promising tool for structure-based therapeutic design on complex, dynamic targets. The D2R-MD dataset and our implementation are freely available to facilitate further research.</article>","contentLength":1688,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collaborative Charging Scheduling via Balanced Bounding Box Methods","url":"https://arxiv.org/abs/2506.14461","date":1751428800,"author":"","guid":179601,"unread":true,"content":"<article>arXiv:2506.14461v2 Announce Type: replace-cross \nAbstract: Electric mobility faces several challenges, most notably the high cost of infrastructure development and the underutilization of charging stations. The concept of shared charging offers a promising solution. The paper explores sustainable urban logistics through horizontal collaboration between two fleet operators and addresses a scheduling problem for the shared use of charging stations. To tackle this, the study formulates a collaborative scheduling problem as a bi-objective nonlinear integer programming model, in which each company aims to minimize its own costs, creating inherent conflicts that require trade-offs. The Balanced Bounding Box Methods (B3Ms) are introduced in order to efficiently derive the efficient frontier, identifying a reduced set of representative solutions. These methods enhance computational efficiency by selectively disregarding closely positioned and competing solutions, preserving the diversity and representativeness of the solutions over the efficient frontier. To determine the final solution and ensure balanced collaboration, cooperative bargaining methods are applied. Numerical case studies demonstrate the viability and scalability of the developed methods, showing that the B3Ms can significantly reduce computational time while maintaining the integrity of the frontier. These methods, along with cooperative bargaining, provide an effective framework for solving various bi-objective optimization problems, extending beyond the collaborative scheduling problem presented here.</article>","contentLength":1587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ICME 2025 Grand Challenge on Video Super-Resolution for Video Conferencing","url":"https://arxiv.org/abs/2506.12269","date":1751428800,"author":"","guid":179602,"unread":true,"content":"<article>arXiv:2506.12269v2 Announce Type: replace-cross \nAbstract: Super-Resolution (SR) is a critical task in computer vision, focusing on reconstructing high-resolution (HR) images from low-resolution (LR) inputs. The field has seen significant progress through various challenges, particularly in single-image SR. Video Super-Resolution (VSR) extends this to the temporal domain, aiming to enhance video quality using methods like local, uni-, bi-directional propagation, or traditional upscaling followed by restoration. This challenge addresses VSR for conferencing, where LR videos are encoded with H.265 at fixed QPs. The goal is to upscale videos by a specific factor, providing HR outputs with enhanced perceptual quality under a low-delay scenario using causal models. The challenge included three tracks: general-purpose videos, talking head videos, and screen content videos, with separate datasets provided by the organizers for training, validation, and testing. We open-sourced a new screen content dataset for the SR task in this challenge. Submissions were evaluated through subjective tests using a crowdsourced implementation of the ITU-T Rec P.910.</article>","contentLength":1160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting mean estimation over $\\ell_p$ balls: Is the MLE optimal?","url":"https://arxiv.org/abs/2506.10354","date":1751428800,"author":"","guid":179603,"unread":true,"content":"<article>arXiv:2506.10354v2 Announce Type: replace-cross \nAbstract: We revisit the problem of mean estimation in the Gaussian sequence model with $\\ell_p$ constraints for $p \\in [0, \\infty]$. We demonstrate two phenomena for the behavior of the maximum likelihood estimator (MLE), which depend on the noise level, the radius of the (quasi)norm constraint, the dimension, and the norm index $p$. First, if $p$ lies between $0$ and $1 + \\Theta(\\tfrac{1}{\\log d})$, inclusive, or if it is greater than or equal to $2$, the MLE is minimax rate-optimal for all noise levels and all constraint radii. On the other hand, for the remaining norm indices -- namely, if $p$ lies between $1 + \\Theta(\\tfrac{1}{\\log d})$ and $2$ -- here is a more striking behavior: the MLE is minimax rate-suboptimal, despite its nonlinearity in the observations, for essentially all noise levels and constraint radii for which nonlinear estimates are necessary for minimax-optimal estimation. Our results imply that when given $n$ independent and identically distributed Gaussian samples, the MLE can be suboptimal by a polynomial factor in the sample size. Our lower bounds are constructive: whenever the MLE is rate-suboptimal, we provide explicit instances on which the MLE provably incurs suboptimal risk. Finally, in the non-convex case -- namely when $p &lt; 1$ -- we develop sharp local Gaussian width bounds, which may be of independent interest.</article>","contentLength":1414,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation","url":"https://arxiv.org/abs/2506.10230","date":1751428800,"author":"","guid":179604,"unread":true,"content":"<article>arXiv:2506.10230v2 Announce Type: replace-cross \nAbstract: Objective: Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM strategies typically rely on short-prompt text encoders, non-medical LDMs, or large data volumes. These strategies can limit performance and scientific accessibility. We propose a novel LDM conditioning approach to address these limitations. Methods: We propose Class-Conditioned Efficient Large Language model Adapter (CCELLA), a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with free-text clinical reports and radiology classification. We also propose a data-efficient LDM framework centered around CCELLA and a proposed joint loss function. We first evaluate our method on 3D prostate MRI against state-of-the-art. We then augment a downstream classifier model training dataset with synthetic images from our method. Results: Our method achieves a 3D FID score of 0.025 on a size-limited 3D prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.071. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method during training improves classifier accuracy from 69% to 74%. Training a classifier solely on our method's synthetic images achieved comparable performance to training on real images alone. Conclusion: We show that our method improved both synthetic image quality and downstream classifier performance using limited data and minimal human annotation. Significance: The proposed CCELLA-centric framework enables radiology report and class-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility. Code from this study will be available at https://github.com/grabkeem/CCELLA</article>","contentLength":1942,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization","url":"https://arxiv.org/abs/2506.09730","date":1751428800,"author":"","guid":179605,"unread":true,"content":"<article>arXiv:2506.09730v3 Announce Type: replace-cross \nAbstract: This work assesses both empirically and theoretically, using the performance estimation methodology, how robust different first-order optimization methods are when subject to relative inexactness in their gradient computations. Relative inexactness occurs, for example, when compressing the gradient using fewer bits of information, which happens when dealing with large-scale problems on GPUs. Three major families of methods are analyzed: constant step gradient descent, long-step methods, and accelerated methods. The latter two are first shown to be theoretically not robust to inexactness. Then, a semi-heuristic shortening factor is introduced to improve their theoretical guarantees. All methods are subsequently tested on a concrete inexact problem, with two different types of relative inexactness, and it is observed that both accelerated methods are much more robust than expected, and that the shortening factor significantly helps the long-step methods. In the end, all shortened methods appear to be promising, even in this inexact setting.</article>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Euclidean dual gradient ascent for entropically regularized linear and semidefinite programming","url":"https://arxiv.org/abs/2506.09711","date":1751428800,"author":"","guid":179606,"unread":true,"content":"<article>arXiv:2506.09711v2 Announce Type: replace-cross \nAbstract: We present an optimization framework that exhibits dimension-independent convergence on a broad class of semidefinite programs (SDPs). Our approach first regularizes the primal problem with the von Neumann entropy, then solve the regularized problem using dual gradient ascent with respect to a problem-adapted norm. In particular, we show that the dual gradient norm converges to zero at a rate independent of the ambient dimension and, via rounding arguments, construct primal-feasible solutions in certain special cases. We also derive explicit convergence rates for the objective. In order to achieve optimal computational scaling, we must accommodate the use of stochastic gradients constructed via randomized trace estimators. Throughout we illustrate the generality of our framework via three important special cases -- the Goemans-Williamson SDP relaxation of the Max-Cut problem, the optimal transport linear program, and several SDP relaxations of the permutation synchronization problem. Numerical experiments confirm that our methods achieve dimension-independent convergence in practice.</article>","contentLength":1159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditional Local Independence Testing for Dynamic Causal Discovery","url":"https://arxiv.org/abs/2506.07844","date":1751428800,"author":"","guid":179607,"unread":true,"content":"<article>arXiv:2506.07844v2 Announce Type: replace-cross \nAbstract: Inferring causal relationships from dynamical systems is the central interest of many scientific inquiries. Conditional Local Independence (CLI), which describes whether the evolution of one process is influenced by another process given additional processes, is important for causal learning in such systems. However, existing CLI tests were limited to counting processes. In this paper, we propose a nonparametric CLT test for It\\^o processes. Specifically, we first introduce a testing statistic based on the Local Covariance Measure (LCM) by constructing a martingale from the conditional expectation of the process of interest. For estimation, we propose an efficient estimator based on the optimal filtering equation, which can achieve root-N consistency. To establish the asymptotic level and power of the test, we relax the restrictive boundedness condition to a moment bound condition, which is practical for It\\^o processes. We verify the proposed test in synthetic and real-world experiments.</article>","contentLength":1062,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Learning-Based Analysis of ECG and PCG Signals for Rheumatic Heart Disease Detection: A Scoping Review (2015-2025)","url":"https://arxiv.org/abs/2505.18182","date":1751428800,"author":"","guid":179608,"unread":true,"content":"<article>arXiv:2505.18182v2 Announce Type: replace-cross \nAbstract: AI-powered stethoscopes offer a promising alternative for screening rheumatic heart disease (RHD), particularly in regions with limited diagnostic infrastructure. Early detection is vital, yet echocardiography, the gold standard tool, remains largely inaccessible in low-resource settings due to cost and workforce constraints. This review systematically examines machine learning (ML) applications from 2015 to 2025 that analyze electrocardiogram (ECG) and phonocardiogram (PCG) data to support accessible, scalable screening of all RHD variants in relation to the World Heart Federation's \"25 by 25\" goal to reduce RHD mortality. Using PRISMA-ScR guidelines, 37 peer-reviewed studies were selected from PubMed, IEEE Xplore, Scopus, and Embase. Convolutional neural networks (CNNs) dominate recent efforts, achieving a median accuracy of 97.75%, F1-score of 0.95, and AUROC of 0.89. However, challenges remain: 73% of studies used single-center datasets, 81.1% relied on private data, only 10.8% were externally validated, and none assessed cost-effectiveness. Although 45.9% originated from endemic regions, few addressed demographic diversity or implementation feasibility. These gaps underscore the disconnect between model performance and clinical readiness. Bridging this divide requires standardized benchmark datasets, prospective trials in endemic areas, and broader validation. If these issues are addressed, AI-augmented auscultation could transform cardiovascular diagnostics in underserved populations, thereby aiding early detection. This review also offers practical recommendations for building accessible ML-based RHD screening tools, aiming to close the diagnostic gap in low-resource settings where conventional auscultation may miss up to 90% of cases and echocardiography remains out of reach.</article>","contentLength":1873,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples","url":"https://arxiv.org/abs/2505.14518","date":1751428800,"author":"","guid":179609,"unread":true,"content":"<article>arXiv:2505.14518v2 Announce Type: replace-cross \nAbstract: Recent advancements in audio-aware large language models (ALLMs) enable them to process and understand audio inputs. However, these models often hallucinate non-existent sound events, reducing their reliability in real-world applications. To address this, we propose LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method that enhances ALLMs' ability to distinguish between present and absent sounds using synthesized data from the backbone LLM. Unlike prior approaches, our method requires no modification to LLM parameters and efficiently integrates audio representations via a lightweight adapter. Experiments show that LISTEN effectively mitigates hallucinations while maintaining impressive performance on existing audio question and reasoning benchmarks. At the same time, it is more efficient in both data and computation.</article>","contentLength":940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stacked conformal prediction","url":"https://arxiv.org/abs/2505.12578","date":1751428800,"author":"","guid":179610,"unread":true,"content":"<article>arXiv:2505.12578v2 Announce Type: replace-cross \nAbstract: We consider a method for conformalizing a stacked ensemble of predictive models, showing that the potentially simple form of the meta-learner at the top of the stack enables a procedure with manageable computational cost that achieves approximate marginal validity without requiring the use of a separate calibration sample. Empirical results indicate that the method compares favorably to a standard inductive alternative.</article>","contentLength":482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment","url":"https://arxiv.org/abs/2505.09438","date":1751428800,"author":"","guid":179611,"unread":true,"content":"<article>arXiv:2505.09438v2 Announce Type: replace-cross \nAbstract: Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Level-set topology optimisation with unfitted finite elements and automatic shape differentiation","url":"https://arxiv.org/abs/2504.09748","date":1751428800,"author":"","guid":179612,"unread":true,"content":"<article>arXiv:2504.09748v3 Announce Type: replace-cross \nAbstract: In this paper we develop automatic shape differentiation techniques for unfitted discretisations and link these to recent advances in shape calculus for unfitted methods. We extend existing analytic shape calculus results to the case where the domain boundary intersects with the boundary of the background domain. We further show that we can recover these analytic derivatives to machine precision regardless of the mesh size using the developed automatic shape differentiation techniques, drastically reducing the burden associated with the analytic derivation of these quantities. In addition, we show that we can also recover the symmetric shape Hessian. We implement these techniques for both serial and distributed computing frameworks in the Julia package GridapTopOpt and the wider Gridap ecosystem. As part of this implementation we propose a novel graph-based approach for isolated volume detection. We demonstrate the applicability of the unfitted automatic shape differentiation framework and our implementation by considering the three-dimensional minimum compliance topology optimisation of a linear elastic wheel and of a linear elastic structure in a fluid-structure interaction problem with Stokes flow. The implementation is general and allows GridapTopOpt to solve a wider range of problems on unstructured meshes without analytic calculation of shape derivatives and avoiding issues that arise when material properties are smoothed at the domain boundary. The software is open source and available at https://github.com/zjwegert/GridapTopOpt.jl.</article>","contentLength":1624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unsupervised Attributed Dynamic Network Embedding with Stability Guarantees","url":"https://arxiv.org/abs/2503.02859","date":1751428800,"author":"","guid":179613,"unread":true,"content":"<article>arXiv:2503.02859v2 Announce Type: replace-cross \nAbstract: Stability for dynamic network embeddings ensures that nodes behaving the same at different times receive the same embedding, allowing comparison of nodes in the network across time. We present attributed unfolded adjacency spectral embedding (AUASE), a stable unsupervised representation learning framework for dynamic networks in which nodes are attributed with time-varying covariate information. To establish stability, we prove uniform convergence to an associated latent position model. We quantify the benefits of our dynamic embedding by comparing with state-of-the-art network representation learning methods on four real attributed networks. To the best of our knowledge, AUASE is the only attributed dynamic embedding that satisfies stability guarantees without the need for ground truth labels, which we demonstrate provides significant improvements for link prediction and node classification.</article>","contentLength":964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conformal Inference under High-Dimensional Covariate Shifts via Likelihood-Ratio Regularization","url":"https://arxiv.org/abs/2502.13030","date":1751428800,"author":"","guid":179614,"unread":true,"content":"<article>arXiv:2502.13030v5 Announce Type: replace-cross \nAbstract: We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, an image classification task from the WILDS repository, and an LLM question-answering task on the MMLU benchmark.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ansatz-free Hamiltonian learning with Heisenberg-limited scaling","url":"https://arxiv.org/abs/2502.11900","date":1751428800,"author":"","guid":179615,"unread":true,"content":"<article>arXiv:2502.11900v2 Announce Type: replace-cross \nAbstract: Learning the unknown interactions that govern a quantum system is crucial for quantum information processing, device benchmarking, and quantum sensing. The problem, known as Hamiltonian learning, is well understood under the assumption that interactions are local, but this assumption may not hold for arbitrary Hamiltonians. Previous methods all require high-order inverse polynomial dependency with precision, unable to surpass the standard quantum limit and reach the gold standard Heisenberg-limited scaling. Whether Heisenberg-limited Hamiltonian learning is possible without prior assumptions about the interaction structures, a challenge we term \\emph{ansatz-free Hamiltonian learning}, remains an open question. In this work, we present a quantum algorithm to learn arbitrary sparse Hamiltonians without any structure constraints using only black-box queries of the system's real-time evolution and minimal digital controls to attain Heisenberg-limited scaling in estimation error. Our method is also resilient to state-preparation-and-measurement errors, enhancing its practical feasibility. We numerically demonstrate our ansatz-free protocol for learning physical Hamiltonians and validating analog quantum simulations, benchmarking our performance against the state-of-the-art Heisenberg-limited learning approach. Moreover, we establish a fundamental trade-off between total evolution time and quantum control on learning arbitrary interactions, revealing the intrinsic interplay between controllability and total evolution time complexity for any learning algorithm. These results pave the way for further exploration into Heisenberg-limited Hamiltonian learning in complex quantum systems under minimal assumptions, potentially enabling new benchmarking and verification protocols.</article>","contentLength":1855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gradient Descent Algorithm in Hilbert Spaces under Stationary Markov Chains with $\\phi$- and $\\beta$-Mixing","url":"https://arxiv.org/abs/2502.03551","date":1751428800,"author":"","guid":179616,"unread":true,"content":"<article>arXiv:2502.03551v2 Announce Type: replace-cross \nAbstract: In this paper, we study a strictly stationary Markov chain gradient descent algorithm operating in general Hilbert spaces. Our analysis focuses on the mixing coefficients of the underlying process, specifically the $\\phi$- and $\\beta$-mixing coefficients. Under these assumptions, we derive probabilistic upper bounds on the convergence behavior of the algorithm based on the exponential as well as the polynomial decay of the mixing coefficients.</article>","contentLength":506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerating Quantum Reinforcement Learning with a Quantum Natural Policy Gradient Based Approach","url":"https://arxiv.org/abs/2501.16243","date":1751428800,"author":"","guid":179617,"unread":true,"content":"<article>arXiv:2501.16243v3 Announce Type: replace-cross \nAbstract: We address the problem of quantum reinforcement learning (QRL) under model-free settings with quantum oracle access to the Markov Decision Process (MDP). This paper introduces a Quantum Natural Policy Gradient (QNPG) algorithm, which replaces the random sampling used in classical Natural Policy Gradient (NPG) estimators with a deterministic gradient estimation approach, enabling seamless integration into quantum systems. While this modification introduces a bounded bias in the estimator, the bias decays exponentially with increasing truncation levels. This paper demonstrates that the proposed QNPG algorithm achieves a sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-1.5})$ for queries to the quantum oracle, significantly improving the classical lower bound of $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for queries to the MDP.</article>","contentLength":893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Establishing baselines for generative discovery of inorganic crystals","url":"https://arxiv.org/abs/2501.02144","date":1751428800,"author":"","guid":179618,"unread":true,"content":"<article>arXiv:2501.02144v2 Announce Type: replace-cross \nAbstract: Generative artificial intelligence offers a promising avenue for materials discovery, yet its advantages over traditional methods remain unclear. In this work, we introduce and benchmark two baseline approaches - random enumeration of charge-balanced prototypes and data-driven ion exchange of known compounds - against four generative techniques based on diffusion models, variational autoencoders, and large language models. Our results show that established methods such as ion exchange are better at generating novel materials that are stable, although many of these closely resemble known compounds. In contrast, generative models excel at proposing novel structural frameworks and, when sufficient training data is available, can more effectively target properties such as electronic band gap and bulk modulus. To enhance the performance of both the baseline and generative approaches, we implement a post-generation screening step in which all proposed structures are passed through stability and property filters from pre-trained machine learning models including universal interatomic potentials. This low-cost filtering step leads to substantial improvement in the success rates of all methods, remains computationally efficient, and ultimately provides a practical pathway toward more effective generative strategies for materials discovery. By establishing baselines for comparison, this work highlights opportunities for continued advancement of generative models, especially for the targeted generation of novel materials that are thermodynamically stable.</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On best approximation by multivariate ridge functions with applications to generalized translation networks","url":"https://arxiv.org/abs/2412.08453","date":1751428800,"author":"","guid":179619,"unread":true,"content":"<article>arXiv:2412.08453v3 Announce Type: replace-cross \nAbstract: In this paper, we prove sharp upper and lower bounds for the approximation of Sobolev functions by sums of multivariate ridge functions, i.e., for approximation by functions of the form $\\mathbb{R}^d \\ni x \\mapsto \\sum_{k=1}^n \\varrho_k(A_k x) \\in \\mathbb{R}$ with $\\varrho_k : \\mathbb{R}^\\ell \\to \\mathbb{R}$ and $A_k \\in \\mathbb{R}^{\\ell \\times d}$. We show that the order of approximation asymptotically behaves as $n^{-r/(d-\\ell)}$, where $r$ is the regularity (order of differentiability) of the Sobolev functions to be approximated. Our lower bound even holds when approximating $L^\\infty$-Sobolev functions of regularity $r$ with error measured in $L^1$, while our upper bound applies to the approximation of $L^p$-Sobolev functions in $L^p$ for any $1 \\leq p \\leq \\infty$. These bounds generalize well-known results regarding the approximation properties of univariate ridge functions to the multivariate case. We use our results to obtain sharp asymptotic bounds for the approximation of Sobolev functions using generalized translation networks and complex-valued neural networks.</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geological and Well prior assisted full waveform inversion using conditional diffusion models","url":"https://arxiv.org/abs/2412.06959","date":1751428800,"author":"","guid":179620,"unread":true,"content":"<article>arXiv:2412.06959v2 Announce Type: replace-cross \nAbstract: Full waveform inversion (FWI) often faces challenges due to inadequate seismic observations, resulting in band-limited and geologically inaccurate inversion results. Incorporating prior information from potential velocity distributions, well-log information, and our geological knowledge and expectations can significantly improve FWI convergence to a realistic model. While diffusion-regularized FWI has shown improved performance compared to conventional FWI by incorporating the velocity distribution prior, it can benefit even more by incorporating well-log information and other geological knowledge priors. To leverage this fact, we propose a geological class and well-information prior-assisted FWI using conditional diffusion models. This method seamlessly integrates multi-modal information into FWI, simultaneously achieving data fitting and universal geologic and geophysics prior matching, which is often not achieved with traditional regularization methods. Specifically, we propose to combine conditional diffusion models with FWI, where we integrate well-log data and geological class conditions into these conditional diffusion models using classifier-free guidance for multi-modal prior matching beyond the original velocity distribution prior. Numerical experiments on the OpenFWI datasets and field marine data demonstrate the effectiveness of our method compared to conventional FWI and the unconditional diffusion-regularized FWI.</article>","contentLength":1510,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Graph-Based Classical and Quantum Approach to Deterministic L-System Inference","url":"https://arxiv.org/abs/2411.19906","date":1751428800,"author":"","guid":179621,"unread":true,"content":"<article>arXiv:2411.19906v3 Announce Type: replace-cross \nAbstract: L-systems can be made to model and create simulations of many biological processes, such as plant development. Finding an L-system for a given process is typically solved by hand, by experts, in a massively time-consuming process. It would be significant if this could be done automatically from data, such as from sequences of images. In this paper, we are interested in inferring a particular type of L-system, deterministic context-free L-system (D0L-system) from a sequence of strings. We introduce the characteristic graph of a sequence of strings, which we then utilize to translate our problem (inferring D0L-systems) in polynomial time into the maximum independent set problem (MIS) and the SAT problem. After that, we offer a classical exact algorithm and an approximate quantum algorithm for the problem.</article>","contentLength":873,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Storing overlapping associative memories on latent manifolds in low-rank spiking networks","url":"https://arxiv.org/abs/2411.17485","date":1751428800,"author":"","guid":179622,"unread":true,"content":"<article>arXiv:2411.17485v2 Announce Type: replace-cross \nAbstract: Associative memory architectures such as the Hopfield network have long been important conceptual and theoretical models for neuroscience and artificial intelligence. However, translating these abstract models into spiking neural networks has been surprisingly difficult. Indeed, much previous work has been restricted to storing a small number of primarily non-overlapping memories in large networks, thereby limiting their scalability. Here, we revisit the associative memory problem in light of recent advances in understanding spike-based computation. Using a recently-established geometric framework, we show that the spiking activity for a large class of all-inhibitory networks is situated on a low-dimensional, convex, and piecewise-linear manifold, with dynamics that move along the manifold. We then map the associative memory problem onto these dynamics, and demonstrate how the vertices of a hypercubic manifold can be used to store stable, overlapping activity patterns with a direct correspondence to the original Hopfield model. We propose several learning rules, and demonstrate a linear scaling of the storage capacity with the number of neurons, as well as robust pattern completion abilities. Overall, this work serves as a case study to demonstrate the effectiveness of using a geometrical perspective to design dynamics on neural manifolds, with implications for neuroscience and machine learning.</article>","contentLength":1477,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPGD: Steepest Perturbed Gradient Descent Optimization","url":"https://arxiv.org/abs/2411.04946","date":1751428800,"author":"","guid":179623,"unread":true,"content":"<article>arXiv:2411.04946v2 Announce Type: replace-cross \nAbstract: Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging. This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD's superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD's potential as a versatile tool for a wide range of optimization problems.</article>","contentLength":1878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning dynamical systems from data: Gradient-based dictionary optimization","url":"https://arxiv.org/abs/2411.04775","date":1751428800,"author":"","guid":179624,"unread":true,"content":"<article>arXiv:2411.04775v2 Announce Type: replace-cross \nAbstract: The Koopman operator plays a crucial role in analyzing the global behavior of dynamical systems. Existing data-driven methods for approximating the Koopman operator or discovering the governing equations of the underlying system typically require a fixed set of basis functions, also called dictionary. The optimal choice of basis functions is highly problem-dependent and often requires domain knowledge. We present a novel gradient descent-based optimization framework for learning suitable and interpretable basis functions from data and show how it can be used in combination with EDMD, SINDy, and PDE-FIND. We illustrate the efficacy of the proposed approach with the aid of various benchmark problems such as the Ornstein-Uhlenbeck process, Chua's circuit, a nonlinear heat equation, as well as protein-folding data.</article>","contentLength":881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Downscaling Neural Network for Coastal Simulations","url":"https://arxiv.org/abs/2408.16553","date":1751428800,"author":"","guid":179625,"unread":true,"content":"<article>arXiv:2408.16553v2 Announce Type: replace-cross \nAbstract: Learning the fine-scale details of a coastal ocean simulation from a coarse representation is a challenging task. For real-world applications, high-resolution simulations are necessary to advance understanding of many coastal processes, specifically, to predict flooding resulting from tsunamis and storm surges. We propose a Downscaling Neural Network for Coastal Simulation (DNNCS) for spatiotemporal enhancement to efficiently learn the high-resolution numerical solution. Given images of coastal simulations produced on low-resolution computational meshes using low polynomial order discontinuous Galerkin discretizations and a coarse temporal resolution, the proposed DNNCS learns to produce high-resolution free surface elevation and velocity visualizations in both time and space. To efficiently model the dynamic changes over time and space, we propose grid-aware spatiotemporal attention to project the temporal features to the spatial domain for non-local feature matching. The coordinate information is also utilized via positional encoding. For the final reconstruction, we use the spatiotemporal bilinear operation to interpolate the missing frames and then expand the feature maps to the frequency domain for residual mapping. Besides data-driven losses, the proposed physics-informed loss guarantees gradient consistency and momentum changes. Their combination contributes to the overall 24% improvements in Root Mean Square Error (RMSE). To train the proposed model, we propose a novel coastal simulation dataset and use it for model optimization and evaluation. Our method shows superior downscaling quality and fast computation compared to the state-of-the-art methods.</article>","contentLength":1746,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta-Posterior Consistency for the Bayesian Inference of Metastable System","url":"https://arxiv.org/abs/2408.01868","date":1751428800,"author":"","guid":179626,"unread":true,"content":"<article>arXiv:2408.01868v2 Announce Type: replace-cross \nAbstract: The vast majority of the literature on learning dynamical systems or stochastic processes from time series has focused on stable or ergodic systems, for both Bayesian and frequentist inference procedures. However, most real-world systems are only metastable, that is, the dynamics appear to be stable on some time scale, but are in fact unstable over longer time scales. Consistency of inference for metastable systems may not be possible, but one can ask about metaconsistency: Do inference procedures converge when observations are taken over a large but finite time interval, but diverge on longer time scales? In this paper we introduce, discuss, and quantify metaconsistency in a Bayesian framework. We discuss how metaconsistency can be exploited to efficiently infer a model for a sub-system of a larger system, where inference on the global behavior may require much more data, or there is no theoretical guarantee as to the asymptotic success of inference procedures. We also discuss the relation between metaconsistency and the spectral properties of the model dynamical system in the case of uniformly ergodic and non-ergodic diffusions.</article>","contentLength":1207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asymptotic Formulation of the Role of Shear Loads on Multi-Layered Thin Shells and Classification of Their Deformation Modes","url":"https://arxiv.org/abs/2407.21021","date":1751428800,"author":"","guid":179627,"unread":true,"content":"<article>arXiv:2407.21021v3 Announce Type: replace-cross \nAbstract: Shell structures are generally modeled based on kinematic hypotheses, where some of the parameters are preferentially evaluated in a phenomenological manner. In this article, asymptotic analysis against the underlying three-dimensional equation system is considered so as to provide a rational framework for modeling and interpreting the deformation behavior of multi-layered thin shells (MTSs). Capable of accurately predicting both overall stiffness and detailed stress distribution, the proposed shell theory shows its distinguishing features at least in the following aspects. Firstly, it naturally introduces a rule for classifying the deformation modes of MTSs based on the magnitude of the maximum dimensionless principal curvature. Secondly, for each class, the hierarchy in the order of the involved field quantities is examined, and it is shown that when the product of the maximum principal curvature and the characteristic shell size reaches the magnitude of unity or larger, the resulting shell theory cannot be treated by natural extension of plate theories. Lastly, it is demonstrated that, for moderate shear forces and comparable material properties, a leading-order multi-layered shell theory derived from asymptotic analysis should suffice to output satisfactory predictions over the shell stiffness, as well as its internal stress distribution. Numerical examples of the deformation and strength analysis for MTSs are also presented to show the reliability of the leading-order model.</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"De-LightSAM: Modality-Decoupled Lightweight SAM for Generalizable Medical Segmentation","url":"https://arxiv.org/abs/2407.14153","date":1751428800,"author":"","guid":179628,"unread":true,"content":"<article>arXiv:2407.14153v5 Announce Type: replace-cross \nAbstract: The universality of deep neural networks across different modalities and their generalization capabilities to unseen domains play an essential role in medical image segmentation. The recent segment anything model (SAM) has demonstrated strong adaptability across diverse natural scenarios. However, the huge computational costs, demand for manual annotations as prompts and conflict-prone decoding process of SAM degrade its generalization capabilities in medical scenarios. To address these limitations, we propose a modality-decoupled lightweight SAM for domain-generalized medical image segmentation, named De-LightSAM. Specifically, we first devise a lightweight domain-controllable image encoder (DC-Encoder) that produces discriminative visual features for diverse modalities. Further, we introduce the self-patch prompt generator (SP-Generator) to automatically generate high-quality dense prompt embeddings for guiding segmentation decoding. Finally, we design the query-decoupled modality decoder (QM-Decoder) that leverages a one-to-one strategy to provide an independent decoding channel for every modality, preventing mutual knowledge interference of different modalities. Moreover, we design a multi-modal decoupled knowledge distillation (MDKD) strategy to leverage robust common knowledge to complement domain-specific medical feature representations. Extensive experiments indicate that De-LightSAM outperforms state-of-the-arts in diverse medical imaging segmentation tasks, displaying superior modality universality and generalization capabilities. Especially, De-LightSAM uses only 2.0% parameters compared to SAM-H. The source code is available at https://github.com/xq141839/De-LightSAM.</article>","contentLength":1767,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Training-Conditional Coverage Bounds under Covariate Shift","url":"https://arxiv.org/abs/2405.16594","date":1751428800,"author":"","guid":179629,"unread":true,"content":"<article>arXiv:2405.16594v2 Announce Type: replace-cross \nAbstract: Conformal prediction methodology has recently been extended to the covariate shift setting, where the distribution of covariates differs between training and test data. While existing results ensure that the prediction sets from these methods achieve marginal coverage above a nominal level, their coverage rate conditional on the training dataset (referred to as training-conditional coverage) remains unexplored. In this paper, we address this gap by deriving upper bounds on the tail of the training-conditional coverage distribution, offering probably approximately correct (PAC) guarantees for these methods. Our results quantify the relationship between the quality of the prediction sets and the severity of distributional changes, and can potentially be used to compute more efficient prediction sets.</article>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Unconditional Representation of the Conditional Score in Infinite-Dimensional Linear Inverse Problems","url":"https://arxiv.org/abs/2405.15643","date":1751428800,"author":"","guid":179630,"unread":true,"content":"<article>arXiv:2405.15643v3 Announce Type: replace-cross \nAbstract: Score-based diffusion models (SDMs) have emerged as a powerful tool for sampling from the posterior distribution in Bayesian inverse problems. However, existing methods often require multiple evaluations of the forward mapping to generate a single sample, resulting in significant computational costs for large-scale inverse problems. To address this, we propose an unconditional representation of the conditional score-function (UCoS) tailored to linear inverse problems, which avoids forward model evaluations during sampling by shifting computational effort to an offline training phase. In this phase, a task-dependent score function is learned based on the linear forward operator. Crucially, we show that the conditional score can be derived exactly from a trained (unconditional) score using affine transformations, eliminating the need for conditional score approximations. Our approach is formulated in infinite-dimensional function spaces, making it inherently discretization-invariant. We support this formulation with a rigorous convergence analysis that justifies UCoS beyond any specific discretization. Finally we validate UCoS through high-dimensional computed tomography (CT) and image deblurring experiments, demonstrating both scalability and accuracy.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods","url":"https://arxiv.org/abs/2401.16776","date":1751428800,"author":"","guid":179631,"unread":true,"content":"<article>arXiv:2401.16776v2 Announce Type: replace-cross \nAbstract: There has been a growing interest in studying sequential neural posterior estimation (SNPE) techniques for their advantages in dealing with simulation-based models with intractable likelihoods. They are devoted to learning the posterior from adaptively proposed simulations using neural network-based conditional density estimators. As a SNPE technique, the automatic posterior transformation (APT) method proposed by Greenberg et al. (2019) performs notably and scales to high dimensional data. However, the APT method bears the computation of an expectation of the logarithm of an intractable normalizing constant, i.e., a nested expectation. Although atomic APT was proposed to solve this by discretizing the normalizing constant, it remains challenging to analyze the convergence of learning. In this paper, we propose a nested APT method to estimate the involved nested expectation instead. This facilitates establishing the convergence analysis. Since the nested estimators for the loss function and its gradient are biased, we make use of unbiased multi-level Monte Carlo (MLMC) estimators for debiasing. To further reduce the excessive variance of the unbiased estimators, this paper also develops some truncated MLMC estimators by taking account of the trade-off between the bias and the average cost. Numerical experiments for approximating complex posteriors with multimodal in moderate dimensions are provided.</article>","contentLength":1481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT","url":"https://arxiv.org/abs/2401.03302","date":1751428800,"author":"","guid":179632,"unread":true,"content":"<article>arXiv:2401.03302v4 Announce Type: replace-cross \nAbstract: Reliable diagnosis of brain tumors remains challenging due to low clinical incidence rates of such cases. However, this low rate is neglected in most of proposed methods. We propose a clinically inspired framework for anomaly-resilient tumor detection and classification. Detection leverages YOLOv8n fine-tuned on a realistically imbalanced dataset (1:9 tumor-to-normal ratio; 30,000 MRI slices from 81 patients). In addition, we propose a novel Patient-to-Patient (PTP) metric that evaluates diagnostic reliability at the patient level. Classification employs knowledge distillation: a Data Efficient Image Transformer (DeiT) student model is distilled from a ResNet152 teacher. The distilled ViT achieves an F1-score of 0.92 within 20 epochs, matching near teacher performance (F1=0.97) with significantly reduced computational resources. This end-to-end framework demonstrates high robustness in clinically representative anomaly-distributed data, offering a viable tool that adheres to realistic situations in clinics.</article>","contentLength":1081,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Constructing disjoint Steiner trees in Sierpi\\'{n}ski graphs","url":"https://arxiv.org/abs/2310.16463","date":1751428800,"author":"","guid":179633,"unread":true,"content":"<article>arXiv:2310.16463v3 Announce Type: replace-cross \nAbstract: Let $G$ be a graph and $S\\subseteq V(G)$ with $|S|\\geq 2$. Then the trees $T_1, T_2, \\cdots, T_\\ell$ in $G$ are \\emph{internally disjoint Steiner trees} connecting $S$ (or $S$-Steiner trees) if $E(T_i) \\cap E(T_j )=\\emptyset$ and $V(T_i)\\cap V(T_j)=S$ for every pair of distinct integers $i,j$, $1 \\leq i, j \\leq \\ell$. Similarly, if we only have the condition $E(T_i) \\cap E(T_j )=\\emptyset$ but without the condition $V(T_i)\\cap V(T_j)=S$, then they are \\emph{edge-disjoint Steiner trees}. The \\emph{generalized $k$-connectivity}, denoted by $\\kappa_k(G)$, of a graph $G$, is defined as $\\kappa_k(G)=\\min\\{\\kappa_G(S)|S \\subseteq V(G) \\ \\textrm{and} \\ |S|=k \\}$, where $\\kappa_G(S)$ is the maximum number of internally disjoint $S$-Steiner trees. The \\emph{generalized local edge-connectivity} $\\lambda_{G}(S)$ is the maximum number of edge-disjoint Steiner trees connecting $S$ in $G$. The {\\it generalized $k$-edge-connectivity} $\\lambda_k(G)$ of $G$ is defined as $\\lambda_k(G)=\\min\\{\\lambda_{G}(S)\\,|\\,S\\subseteq V(G) \\ and \\ |S|=k\\}$. These measures are generalizations of the concepts of connectivity and edge-connectivity, and they and can be used as measures of vulnerability of networks. It is, in general, difficult to compute these generalized connectivities. However, there are precise results for some special classes of graphs. In this paper, we obtain the exact value of $\\lambda_{k}(S(n,\\ell))$ for $3\\leq k\\leq \\ell^n$, and the exact value of $\\kappa_{k}(S(n,\\ell))$ for $3\\leq k\\leq \\ell$, where $S(n, \\ell)$ is the Sierpi\\'{n}ski graphs with order $\\ell^n$. As a direct consequence, these graphs provide additional interesting examples when $\\lambda_{k}(S(n,\\ell))=\\kappa_{k}(S(n,\\ell))$. We also study the some network properties of Sierpi\\'{n}ski graphs.</article>","contentLength":1836,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements","url":"https://arxiv.org/abs/2506.22419","date":1751428800,"author":"","guid":179634,"unread":true,"content":"<article>arXiv:2506.22419v2 Announce Type: replace \nAbstract: Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.</article>","contentLength":1390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HyperCLOVA X THINK Technical Report","url":"https://arxiv.org/abs/2506.22403","date":1751428800,"author":"","guid":179635,"unread":true,"content":"<article>arXiv:2506.22403v2 Announce Type: replace \nAbstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with $\\mu$P, pre-trained through a three-stage curriculum that expands the context window to $128$K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes. It delivers competitive performance against similarly sized models on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency and translation quality. In addition, a vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with substantially lower training compute than existing models of similar sizes. We also present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. Altogether, these capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI innovation and a valuable resource for the global research community.</article>","contentLength":1380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"B\\'ezierGS: Dynamic Urban Scene Reconstruction with B\\'ezier Curve Gaussian Splatting","url":"https://arxiv.org/abs/2506.22099","date":1751428800,"author":"","guid":179636,"unread":true,"content":"<article>arXiv:2506.22099v2 Announce Type: replace \nAbstract: The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B\\'ezier curve Gaussian splatting (B\\'ezierGS), which represents the motion trajectories of dynamic objects using learnable B\\'ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Negated String Containment is Decidable (Technical Report)","url":"https://arxiv.org/abs/2506.22061","date":1751428800,"author":"","guid":179637,"unread":true,"content":"<article>arXiv:2506.22061v2 Announce Type: replace \nAbstract: We provide a positive answer to a long-standing open question of the decidability of the not-contains string predicate. Not-contains is practically relevant, for instance in symbolic execution of string manipulating programs. Particularly, we show that the predicate $\\neg\\mathit{Contains}(x_1 \\ldots x_n, y_1 \\ldots y_m)$, where $x_1 \\ldots x_n$ and $y_1 \\ldots y_m$ are sequences of string variables constrained by regular languages, is decidable. Decidability of a not-contains predicate combined with chain-free word equations and regular membership constraints follows.</article>","contentLength":627,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Binned semiparametric Bayesian networks","url":"https://arxiv.org/abs/2506.21997","date":1751428800,"author":"","guid":179638,"unread":true,"content":"<article>arXiv:2506.21997v2 Announce Type: replace \nAbstract: This paper introduces a new type of probabilistic semiparametric model that takes advantage of data binning to reduce the computational cost of kernel density estimation in nonparametric distributions. Two new conditional probability distributions are developed for the new binned semiparametric Bayesian networks, the sparse binned kernel density estimation and the Fourier kernel density estimation. These two probability distributions address the curse of dimensionality, which typically impacts binned models, by using sparse tensors and restricting the number of parent nodes in conditional probability calculations. To evaluate the proposal, we perform a complexity analysis and conduct several comparative experiments using synthetic data and datasets from the UCI Machine Learning repository. The experiments include different binning rules, parent restrictions, grid sizes, and number of instances to get a holistic view of the model's behavior. As a result, our binned semiparametric Bayesian networks achieve structural learning and log-likelihood estimations with no statistically significant differences compared to the semiparametric Bayesian networks, but at a much higher speed. Thus, the new binned semiparametric Bayesian networks prove to be a reliable and more efficient alternative to their non-binned counterparts.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning","url":"https://arxiv.org/abs/2506.21980","date":1751428800,"author":"","guid":179639,"unread":true,"content":"<article>arXiv:2506.21980v2 Announce Type: replace \nAbstract: Visual single object tracking aims to continuously localize and estimate the scale of a target in subsequent video frames, given only its initial state in the first frame. This task has traditionally been framed as a template matching problem, evolving through major phases including correlation filters, two-stream networks, and one-stream networks with significant progress achieved. However, these methods typically require explicit classification and regression modeling, depend on supervised training with large-scale datasets, and are limited to the single task of tracking, lacking flexibility. In recent years, multi-modal large language models (MLLMs) have advanced rapidly. Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational capabilities, demonstrate excellent performance in grounding tasks. This has spurred interest in applying such models directly to visual tracking. However, experiments reveal that Qwen2.5-VL struggles with template matching between image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement learning method on a small-scale dataset with a rule-based reward function. The resulting model, R1-Track, achieved notable performance on the GOT-10k benchmark. R1-Track supports flexible initialization via bounding boxes or text descriptions while retaining most of the original model's general capabilities. And we further discuss potential improvements for R1-Track. This rough technical report summarizes our findings as of May 2025.</article>","contentLength":1637,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StruMamba3D: Exploring Structural Mamba for Self-supervised Point Cloud Representation Learning","url":"https://arxiv.org/abs/2506.21541","date":1751428800,"author":"","guid":179640,"unread":true,"content":"<article>arXiv:2506.21541v2 Announce Type: replace \nAbstract: Recently, Mamba-based methods have demonstrated impressive performance in point cloud representation learning by leveraging State Space Model (SSM) with the efficient context modeling ability and linear complexity. However, these methods still face two key issues that limit the potential of SSM: Destroying the adjacency of 3D points during SSM processing and failing to retain long-sequence memory as the input length increases in downstream tasks. To address these issues, we propose StruMamba3D, a novel paradigm for self-supervised point cloud representation learning. It enjoys several merits. First, we design spatial states and use them as proxies to preserve spatial dependencies among points. Second, we enhance the SSM with a state-wise update strategy and incorporate a lightweight convolution to facilitate interactions between spatial states for efficient structure modeling. Third, our method reduces the sensitivity of pre-trained Mamba-based models to varying input lengths by introducing a sequence length-adaptive strategy. Experimental results across four downstream tasks showcase the superior performance of our method. In addition, our method attains the SOTA 95.1% accuracy on ModelNet40 and 92.75% accuracy on the most challenging split of ScanObjectNN without voting strategy.</article>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Active Inference AI Systems for Scientific Discovery","url":"https://arxiv.org/abs/2506.21329","date":1751428800,"author":"","guid":179641,"unread":true,"content":"<article>arXiv:2506.21329v2 Announce Type: replace \nAbstract: The rapid evolution of artificial intelligence has led to expectations of transformative scientific discovery, yet current systems remain fundamentally limited by their operational architectures, brittle reasoning mechanisms, and their separation from experimental reality. Building on earlier work, we contend that progress in AI-driven science now depends on closing three fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap -- rather than on model size/data/test time compute. Scientific reasoning demands internal representations that support simulation of actions and response, causal structures that distinguish correlation from mechanism, and continuous calibration. We define active inference AI systems for scientific discovery as those that (i) maintain long-lived research memories grounded in causal self-supervised foundation models, (ii) symbolic or neuro-symbolic planners equipped with Bayesian guardrails, (iii) grow persistent knowledge graphs where thinking generates novel conceptual nodes, reasoning establishes causal edges, and real-world interaction prunes false connections while strengthening verified pathways, and (iv) refine their internal representations through closed-loop interaction with both high-fidelity simulators and automated laboratories - an operational loop where mental simulation guides action and empirical surprise reshapes understanding. In essence, we outline an architecture where discovery arises from the interplay between internal models that enable counterfactual reasoning and external validation that grounds hypotheses in reality. It is also argued that the inherent ambiguity in feedback from simulations and experiments, and underlying uncertainties makes human judgment indispensable, not as a temporary scaffold but as a permanent architectural component.</article>","contentLength":1897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Dataset for Enhancing MLLMs in Visualization Understanding and Reconstruction","url":"https://arxiv.org/abs/2506.21319","date":1751428800,"author":"","guid":179642,"unread":true,"content":"<article>arXiv:2506.21319v2 Announce Type: replace \nAbstract: Current multimodal large language models (MLLMs), while effective in natural image understanding, struggle with visualization understanding due to their inability to decode the data-to-visual mapping and extract structured information. To address these challenges, we propose SimVec, a compact and structured vector format that encodes chart elements, including mark types, positions, and sizes. Then, we present a new visualization dataset, which consists of bitmap images of charts, their corresponding SimVec representations, and data-centric question-answering pairs, each accompanied by explanatory chain-of-thought sentences. We fine-tune state-of-the-art MLLMs using our dataset. The experimental results show that fine-tuning leads to substantial improvements in data-centric reasoning tasks compared to their zero-shot versions. SimVec also enables MLLMs to accurately and compactly reconstruct chart structures from images. Our dataset and code are available at: https://github.com/VIDA-Lab/MLLM4VIS.</article>","contentLength":1063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry","url":"https://arxiv.org/abs/2506.21098","date":1751428800,"author":"","guid":179643,"unread":true,"content":"<article>arXiv:2506.21098v2 Announce Type: replace \nAbstract: Community Question Answering (CQA) platforms can be deemed as important knowledge bases in community, but effectively leveraging historical interactions and domain knowledge in real-time remains a challenge. Existing methods often underutilize external knowledge, fail to incorporate dynamic historical QA context, or lack memory mechanisms suited for industrial deployment. We propose ComRAG, a retrieval-augmented generation framework for real-time industrial CQA that integrates static knowledge with dynamic historical QA pairs via a centroid-based memory mechanism designed for retrieval, generation, and efficient storage. Evaluated on three industrial CQA datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9% improvement in vector similarity, reducing latency by 8.7% to 23.3%, and lowering chunk growth from 20.23% to 2.06% over iterations.</article>","contentLength":927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning","url":"https://arxiv.org/abs/2506.21096","date":1751428800,"author":"","guid":179644,"unread":true,"content":"<article>arXiv:2506.21096v2 Announce Type: replace \nAbstract: Previous multimodal sentence representation learning methods have achieved impressive performance. However, most approaches focus on aligning images and text at a coarse level, facing two critical challenges:cross-modal misalignment bias and intra-modal semantic divergence, which significantly degrade sentence representation quality. To address these challenges, we propose DALR (Dual-level Alignment Learning for Multimodal Sentence Representation). For cross-modal alignment, we propose a consistency learning module that softens negative samples and utilizes semantic similarity from an auxiliary task to achieve fine-grained cross-modal alignment. Additionally, we contend that sentence relationships go beyond binary positive-negative labels, exhibiting a more intricate ranking structure. To better capture these relationships and enhance representation quality, we integrate ranking distillation with global intra-modal alignment learning. Comprehensive experiments on semantic textual similarity (STS) and transfer (TR) tasks validate the effectiveness of our approach, consistently demonstrating its superiority over state-of-the-art baselines.</article>","contentLength":1208,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Singapore Consensus on Global AI Safety Research Priorities","url":"https://arxiv.org/abs/2506.20702","date":1751428800,"author":"","guid":179645,"unread":true,"content":"<article>arXiv:2506.20702v2 Announce Type: replace \nAbstract: Rapidly improving AI capabilities and autonomy hold significant promise of transformation, but are also driving vigorous debate on how to ensure that AI is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem is therefore essential -- it helps people embrace AI with confidence and gives maximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific Exchange on AI Safety\" aimed to support research in this space by bringing together AI scientists across geographies to identify and synthesise research priorities in AI safety. This resulting report builds on the International AI Safety Report chaired by Yoshua Bengio and backed by 33 governments. By adopting a defence-in-depth model, this report organises AI safety research domains into three types: challenges with creating trustworthy AI systems (Development), challenges with evaluating their risks (Assessment), and challenges with monitoring and intervening after deployment (Control).</article>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Review of Three Variants of the k-d Tree","url":"https://arxiv.org/abs/2506.20687","date":1751428800,"author":"","guid":179646,"unread":true,"content":"<article>arXiv:2506.20687v2 Announce Type: replace \nAbstract: The original description of the k-d tree recognized that rebalancing techniques, such as used to build an AVL tree or a red-black tree, are not applicable to a k-d tree. Hence, in order to build a balanced k-d tree, it is necessary to find the median of a set of data for each recursive subdivision of that set. The sort or selection used to find the median, and the technique used to partition the set about that median, strongly influence the computational complexity of building a k-d tree. This article describes and contrasts three variants of the k-d tree that differ in their technique used to partition the set, and compares the performance of those variants. In addition, dual-threaded execution is proposed and analyzed for one of the three variants.</article>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating and Customizing Robotic Arm Trajectories using Neural Networks","url":"https://arxiv.org/abs/2506.20259","date":1751428800,"author":"","guid":179647,"unread":true,"content":"<article>arXiv:2506.20259v2 Announce Type: replace \nAbstract: We introduce a neural network approach for generating and customizing the trajectory of a robotic arm, that guarantees precision and repeatability. To highlight the potential of this novel method, we describe the design and implementation of the technique and show its application in an experimental setting of cognitive robotics. In this scenario, the NICO robot was characterized by the ability to point to specific points in space with precise linear movements, increasing the predictability of the robotic action during its interaction with humans. To achieve this goal, the neural network computes the forward kinematics of the robot arm. By integrating it with a generator of joint angles, another neural network was developed and trained on an artificial dataset created from suitable start and end poses of the robotic arm. Through the computation of angular velocities, the robot was characterized by its ability to perform the movement, and the quality of its action was evaluated in terms of shape and accuracy. Thanks to its broad applicability, our approach successfully generates precise trajectories that could be customized in their shape and adapted to different settings.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures","url":"https://arxiv.org/abs/2506.19676","date":1751428800,"author":"","guid":179648,"unread":true,"content":"<article>arXiv:2506.19676v2 Announce Type: replace \nAbstract: In recent years, Large-Language-Model-driven AI agents have exhibited unprecedented intelligence and adaptability, and are rapidly changing human production and life. Nowadays, agents are undergoing a new round of evolution. They no longer act as an isolated island like LLMs. Instead, they start to communicate with diverse external entities, such as other agents and tools, to perform more complex tasks collectively. Under this trend, agent communication is regarded as a foundational pillar of the future AI ecosystem, and many organizations have intensively begun to design related communication protocols (e.g., Anthropic's MCP and Google's A2A) within the recent few months. However, this new field exposes significant security hazards, which can cause severe damage to real-world scenarios. To help researchers quickly figure out this promising topic and benefit the future agent communication development, this paper presents a comprehensive survey of agent communication security. More precisely, we first present a clear definition of agent communication and categorize the entire lifecycle of agent communication into three stages: user-agent interaction, agent-agent communication, and agent-environment communication. Next, for each communication phase, we dissect related protocols and analyze the security risks according to the communication characteristics. Then, we summarize and outlook on the possible defense countermeasures for each risk. In addition, we conduct experiments using MCP and A2A to help readers better understand the novel vulnerabilities brought by agent communication. Finally, we discuss open issues and future directions in this promising research field.</article>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding","url":"https://arxiv.org/abs/2506.19288","date":1751428800,"author":"","guid":179649,"unread":true,"content":"<article>arXiv:2506.19288v2 Announce Type: replace \nAbstract: Automated waterway environment perception is crucial for enabling unmanned surface vessels (USVs) to understand their surroundings and make informed decisions. Most existing waterway perception models primarily focus on instance-level object perception paradigms (e.g., detection, segmentation). However, due to the complexity of waterway environments, current perception datasets and models fail to achieve global semantic understanding of waterways, limiting large-scale monitoring and structured log generation. With the advancement of vision-language models (VLMs), we leverage image captioning to introduce WaterCaption, the first captioning dataset specifically designed for waterway environments. WaterCaption focuses on fine-grained, multi-region long-text descriptions, providing a new research direction for visual geo-understanding and spatial scene cognition. Exactly, it includes 20.2k image-text pair data with 1.8 million vocabulary size. Additionally, we propose Da Yu, an edge-deployable multi-modal large language model for USVs, where we propose a novel vision-to-language projector called Nano Transformer Adaptor (NTA). NTA effectively balances computational efficiency with the capacity for both global and fine-grained local modeling of visual features, thereby significantly enhancing the model's ability to generate long-form textual outputs. Da Yu achieves an optimal balance between performance and efficiency, surpassing state-of-the-art models on WaterCaption and several other captioning benchmarks.</article>","contentLength":1582,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration","url":"https://arxiv.org/abs/2506.19283","date":1751428800,"author":"","guid":179650,"unread":true,"content":"<article>arXiv:2506.19283v2 Announce Type: replace \nAbstract: While multi-vehicular collaborative driving demonstrates clear advantages over single-vehicle autonomy, traditional infrastructure-based V2X systems remain constrained by substantial deployment costs and the creation of \"uncovered danger zones\" in rural and suburban areas. We present AirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial Vehicles (UAVs) as a flexible alternative or complement to fixed Road-Side Units (RSUs). Drones offer unique advantages over ground-based perception: complementary bird's-eye-views that reduce occlusions, dynamic positioning capabilities that enable hovering, patrolling, and escorting navigation rules, and significantly lower deployment costs compared to fixed infrastructure. Our dataset comprises 6.73 hours of drone-assisted driving scenarios across urban, suburban, and rural environments with varied weather and lighting conditions. The AirV2X-Perception dataset facilitates the development and standardized evaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in the rapidly expanding field of aerial-assisted autonomous driving systems. The dataset and development kits are open-sourced at https://github.com/taco-group/AirV2X-Perception.</article>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting","url":"https://arxiv.org/abs/2506.19089","date":1751428800,"author":"","guid":179651,"unread":true,"content":"<article>arXiv:2506.19089v2 Announce Type: replace \nAbstract: We introduce $\\texttt{StorySim}$, a programmable framework for synthetically generating stories to evaluate the theory of mind (ToM) and world modeling (WM) capabilities of large language models (LLMs). Unlike prior benchmarks that may suffer from contamination in pretraining data, $\\texttt{StorySim}$ produces novel, compositional story prompts anchored by a highly controllable $\\texttt{Storyboard}$, enabling precise manipulation of character perspectives and events. We use this framework to design first- and second-order ToM tasks alongside WM tasks that control for the ability to track and model mental states. Our experiments across a suite of state-of-the-art LLMs reveal that most models perform better on WM tasks than ToM tasks, and that models tend to perform better reasoning with humans compared to inanimate objects. Additionally, our framework enabled us to find evidence of heuristic behavior such as recency bias and an over-reliance on earlier events in the story. All code for generating data and evaluations is freely available.</article>","contentLength":1105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking the Pedagogical Knowledge of Large Language Models","url":"https://arxiv.org/abs/2506.18710","date":1751428800,"author":"","guid":179652,"unread":true,"content":"<article>arXiv:2506.18710v3 Announce Type: replace \nAbstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.</article>","contentLength":1960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models","url":"https://arxiv.org/abs/2506.18548","date":1751428800,"author":"","guid":179653,"unread":true,"content":"<article>arXiv:2506.18548v2 Announce Type: replace \nAbstract: Click models are a well-established for modeling user interactions with web interfaces. Previous work has mainly focused on traditional single-list web search settings; this includes existing surveys that introduced categorizations based on the first generation of probabilistic graphical model (PGM) click models that have become standard. However, these categorizations have become outdated, as their conceptualizations are unable to meaningfully compare PGM with neural network (NN) click models nor generalize to newer interfaces, such as carousel interfaces. We argue that this outdated view fails to adequately explain the fundamentals of click model designs, thus hindering the development of novel click models.\n  This work reconsiders what should be the fundamental concepts in click model design, grounding them - unlike previous approaches - in their mathematical properties. We propose three fundamental key-design choices that explain what statistical patterns a click model can capture, and thus indirectly, what user behaviors they can capture. Based on these choices, we create a novel click model taxonomy that allows a meaningful comparison of all existing click models; this is the first taxonomy of single-list, grid and carousel click models that includes PGMs and NNs. Finally, we show how our conceptualization provides a foundation for future click model design by an example derivation of a novel design for carousel interfaces.</article>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deciding Termination of Simple Randomized Loops","url":"https://arxiv.org/abs/2506.18541","date":1751428800,"author":"","guid":179654,"unread":true,"content":"<article>arXiv:2506.18541v2 Announce Type: replace \nAbstract: We show that universal positive almost sure termination (UPAST) is decidable for a class of simple randomized programs, i.e., it is decidable whether the expected runtime of such a program is finite for all inputs. Our class contains all programs that consist of a single loop, with a linear loop guard and a loop body composed of two linear commuting and diagonalizable updates. In each iteration of the loop, the update to be carried out is picked at random, according to a fixed probability. We show the decidability of UPAST for this class of programs, where the program's variables and inputs may range over various sub-semirings of the real numbers. In this way, we extend a line of research initiated by Tiwari in 2004 into the realm of randomized programs.</article>","contentLength":817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distribution of codewords on the faces of a hypercube and new combinatorial identities","url":"https://arxiv.org/abs/2506.18494","date":1751428800,"author":"","guid":179655,"unread":true,"content":"<article>arXiv:2506.18494v2 Announce Type: replace \nAbstract: We present a novel framework for studying combinatorial identities through the geometric lens of subset distributions in q-valued cubes. By analyzing how elements of arbitrary subsets are distributed among the faces of the cube E_q^n, we discover new combinatorial identities with geometric significance. We prove that for any subset A contained in E_2^n, the rank function satisfies refined bounds that lead to exact computations for small cardinalities. Specifically, we show that for odd cardinalities, the lower bound is 4D_A/(|A|^2-1) where D_A is the sum of all pairwise Hamming distances in A. Our main theorem establishes identities connecting the number of k-dimensional faces containing exactly e elements of a subset to binomial sums over all subsets of specified cardinality. This yields a parametric family of identities where classical results emerge as special cases. As applications, we derive a geometric interpretation of Vandermonde's identity by examining faces of q-valued cubes, revealing that this classical result naturally arises from counting element distributions. We also obtain a completely new identity for even-weight vectors: (2^(k-1) - 1) times 2^(n-1) times binomial(n,k) equals the sum over i from 1 to floor(n/2) of binomial(n,2i) times binomial(n-2i,k-2i). This identity, valid for all 1 &lt;= k &lt;= n, demonstrates how geometric perspectives can uncover hidden combinatorial relationships. Our framework provides a unified approach for generating new identities and understanding existing ones through subset rank analysis.</article>","contentLength":1610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CARTS: Collaborative Agents for Recommendation Textual Summarization","url":"https://arxiv.org/abs/2506.17765","date":1751428800,"author":"","guid":179656,"unread":true,"content":"<article>arXiv:2506.17765v2 Announce Type: replace \nAbstract: Current recommendation systems often require some form of textual data summarization, such as generating concise and coherent titles for product carousels or other grouped item displays. While large language models have shown promise in NLP domains for textual summarization, these approaches do not directly apply to recommendation systems, where explanations must be highly relevant to the core features of item sets, adhere to strict word limit constraints. In this paper, we propose CARTS (Collaborative Agents for Recommendation Textual Summarization), a multi-agent LLM framework designed for structured summarization in recommendation systems. CARTS decomposes the task into three stages-Generation Augmented Generation (GAG), refinement circle, and arbitration, where successive agent roles are responsible for extracting salient item features, iteratively refining candidate titles based on relevance and length feedback, and selecting the final title through a collaborative arbitration process. Experiments on large-scale e-commerce data and live A/B testing show that CARTS significantly outperforms single-pass and chain-of-thought LLM baselines, delivering higher title relevance and improved user engagement metrics.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases","url":"https://arxiv.org/abs/2506.17336","date":1751428800,"author":"","guid":179657,"unread":true,"content":"<article>arXiv:2506.17336v2 Announce Type: replace \nAbstract: Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.</article>","contentLength":1467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Capturing Visualization Design Rationale","url":"https://arxiv.org/abs/2506.16571","date":1751428800,"author":"","guid":179658,"unread":true,"content":"<article>arXiv:2506.16571v2 Announce Type: replace \nAbstract: Prior natural language datasets for data visualization have focused on tasks such as visualization literacy assessment, insight generation, and visualization generation from natural language instructions. These studies often rely on controlled setups with purpose-built visualizations and artificially constructed questions. As a result, they tend to prioritize the interpretation of visualizations, focusing on decoding visualizations rather than understanding their encoding. In this paper, we present a new dataset and methodology for probing visualization design rationale through natural language. We leverage a unique source of real-world visualizations and natural language narratives: literate visualization notebooks created by students as part of a data visualization course. These notebooks combine visual artifacts with design exposition, in which students make explicit the rationale behind their design decisions. We also use large language models (LLMs) to generate and categorize question-answer-rationale triples from the narratives and articulations in the notebooks. We then carefully validate the triples and curate a dataset that captures and distills the visualization design choices and corresponding rationales of the students.</article>","contentLength":1304,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Studying and Improving Graph Neural Network-based Motif Estimation","url":"https://arxiv.org/abs/2506.15709","date":1751428800,"author":"","guid":179659,"unread":true,"content":"<article>arXiv:2506.15709v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif significance-profile (SP) prediction remains under-explored, with no established benchmarks in the literature. We propose to address this problem, framing SP estimation as a task independent of subgraph frequency estimation. Our approach shifts from frequency counting to direct SP estimation and modulates the problem as multitarget regression. The reformulation is optimised for interpretability, stability and scalability on large graphs. We validate our method using a large synthetic dataset and further test it on real-world graphs. Our experiments reveal that 1-WL limited models struggle to make precise estimations of SPs. However, they can generalise to approximate the graph generation processes of networks by comparing their predicted SP with the ones originating from synthetic generators. This first study on GNN-based motif estimation also hints at how using direct SP estimation can help go past the theoretical limitations that motif estimation faces when performed through subgraph counting.</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intelligent Routing for Sparse Demand Forecasting: A Comparative Evaluation of Selection Strategies","url":"https://arxiv.org/abs/2506.14810","date":1751428800,"author":"","guid":179660,"unread":true,"content":"<article>arXiv:2506.14810v2 Announce Type: replace \nAbstract: Sparse and intermittent demand forecasting in supply chains presents a critical challenge, as frequent zero-demand periods hinder traditional model accuracy and impact inventory management. We propose and evaluate a Model-Router framework that dynamically selects the most suitable forecasting model-spanning classical, ML, and DL methods for each product based on its unique demand pattern. By comparing rule-based, LightGBM, and InceptionTime routers, our approach learns to assign appropriate forecasting strategies, effectively differentiating between smooth, lumpy, or intermittent demand regimes to optimize predictions. Experiments on the large-scale Favorita dataset show our deep learning (Inception Time) router improves forecasting accuracy by up to 11.8% (NWRMSLE) over strong, single-model benchmarks with 4.67x faster inference time. Ultimately, these gains in forecasting precision will drive substantial reductions in both stockouts and wasteful excess inventory, underscoring the critical role of intelligent, adaptive Al in optimizing contemporary supply chain operations.</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discrete Diffusion in Large Language and Multimodal Models: A Survey","url":"https://arxiv.org/abs/2506.13759","date":1751428800,"author":"","guid":179661,"unread":true,"content":"<article>arXiv:2506.13759v2 Announce Type: replace \nAbstract: In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey</article>","contentLength":1856,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unleashing Diffusion and State Space Models for Medical Image Segmentation","url":"https://arxiv.org/abs/2506.12747","date":1751428800,"author":"","guid":179662,"unread":true,"content":"<article>arXiv:2506.12747v2 Announce Type: replace \nAbstract: Existing segmentation models trained on a single medical imaging dataset often lack robustness when encountering unseen organs or tumors. Developing a robust model capable of identifying rare or novel tumor categories not present during training is crucial for advancing medical imaging applications. We propose DSM, a novel framework that leverages diffusion and state space models to segment unseen tumor categories beyond the training data. DSM utilizes two sets of object queries trained within modified attention decoders to enhance classification accuracy. Initially, the model learns organ queries using an object-aware feature grouping strategy to capture organ-level visual features. It then refines tumor queries by focusing on diffusion-based visual prompts, enabling precise segmentation of previously unseen tumors. Furthermore, we incorporate diffusion-guided feature fusion to improve semantic segmentation performance. By integrating CLIP text embeddings, DSM captures category-sensitive classes to improve linguistic transfer knowledge, thereby enhancing the model's robustness across diverse scenarios and multi-label tasks. Extensive experiments demonstrate the superior performance of DSM in various tumor segmentation tasks. Code is available at https://github.com/Rows21/k-Means_Mask_Mamba.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models","url":"https://arxiv.org/abs/2506.12036","date":1751428800,"author":"","guid":179663,"unread":true,"content":"<article>arXiv:2506.12036v3 Announce Type: replace \nAbstract: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the \"golden noise\" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Representational Learning of Foundation Models for Recommendation","url":"https://arxiv.org/abs/2506.11999","date":1751428800,"author":"","guid":179664,"unread":true,"content":"<article>arXiv:2506.11999v3 Announce Type: replace \nAbstract: Developing a single foundation model with the capability to excel across diverse tasks has been a long-standing objective in the field of artificial intelligence. As the wave of general-purpose foundation models sweeps across various domains, their influence has significantly extended to the field of recommendation systems. While recent efforts have explored recommendation foundation models for various generative tasks, they often overlook crucial embedding tasks and struggle with the complexities of multi-task learning, including knowledge sharing &amp; conflict resolution, and convergence speed inconsistencies. To address these limitations, we introduce RecFound, a generative representational learning framework for recommendation foundation models. We construct the first comprehensive dataset for recommendation foundation models covering both generative and embedding tasks across diverse scenarios. Based on this dataset, we propose a novel multi-task training scheme featuring a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge sharing &amp; conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched) to address inconsistent convergence, and a Model Merge module to balance the performance across tasks. Experiments demonstrate that RecFound achieves state-of-the-art performance across various recommendation tasks, outperforming existing baselines.</article>","contentLength":1439,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Toward a Brazilian Research Agenda in Quantum Software Engineering: A Systematic Mapping Study","url":"https://arxiv.org/abs/2506.11013","date":1751428800,"author":"","guid":179665,"unread":true,"content":"<article>arXiv:2506.11013v2 Announce Type: replace \nAbstract: Context: Quantum Software Engineering (QSE) has emerged as a promising discipline to support the development of quantum applications by integrating quantum computing principles with established software engineering practices. Problem: Despite recent growth, QSE still lacks standardized methodologies, tools, and guidelines. Moreover, countries like Brazil have had minimal representation in the development of this emerging field. Objective: This study aims to map the current state of QSE by identifying research trends, contributions, and gaps that can inform future investigations and strategic initiatives. Methodology: A systematic mapping study was conducted across major scientific databases, retrieving 3,219 studies. After applying inclusion and exclusion criteria, 3,052 studies were excluded, resulting in 167 selected for analysis. The publications were classified by study type, research type, and alignment with SWEBOK knowledge areas. Results: Most studies focused on Software Engineering Models and Methods, Software Architecture, and Software Testing. Conceptual and technical proposals were predominant, while empirical validations remained limited. Conclusions: QSE is still a maturing field. Advancing it requires standardization, more empirical research, and greater participation from developing countries. As its main contribution, this study proposes a Brazilian Research Agenda in QSE to guide national efforts and foster the development of a strong local scientific community.</article>","contentLength":1556,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs","url":"https://arxiv.org/abs/2506.10967","date":1751428800,"author":"","guid":179666,"unread":true,"content":"<article>arXiv:2506.10967v2 Announce Type: replace \nAbstract: In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named CDPruner, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency by 78\\%, while maintaining 94\\% of the original accuracy. Our code is available at https://github.com/Theia-4869/CDPruner.</article>","contentLength":1612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantifying Azure RBAC Wildcard Overreach","url":"https://arxiv.org/abs/2506.10755","date":1751428800,"author":"","guid":179667,"unread":true,"content":"<article>arXiv:2506.10755v3 Announce Type: replace \nAbstract: Azure RBAC leverages wildcard permissions to simplify policy authoring, but this abstraction often obscures the actual set of allowed operations and undermines least-privilege guarantees. We introduce Belshazaar, a two-stage framework that targets both the effective permission set problem and the evaluation of wildcards permissions spread. First, we formalize Azure action syntax via a context free grammar and implement a compiler that expands any wildcard into its explicit action set. Second, we define an ultrametric diameter metric to quantify semantic overreach in wildcard scenarios. Applied to Microsoft s official catalog of 15481 actions, Belshazaar reveals that about 50 percent of actions admit a cross Resource Provider reach when associated with non obvious wildcards, and that effective permissions sets are effectively computable. These findings demonstrate that wildcard patterns can introduce substantial privilege bloat, and that our approach offers a scalable, semantics driven path toward tighter, least-privilege RBAC policies in Azure environments.</article>","contentLength":1126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Defensive Adversarial CAPTCHA: A Semantics-Driven Framework for Natural Adversarial Example Generation","url":"https://arxiv.org/abs/2506.10685","date":1751428800,"author":"","guid":179668,"unread":true,"content":"<article>arXiv:2506.10685v3 Announce Type: replace \nAbstract: Traditional CAPTCHA (Completely Automated Public Turing Test to Tell Computers and Humans Apart) schemes are increasingly vulnerable to automated attacks powered by deep neural networks (DNNs). Existing adversarial attack methods often rely on the original image characteristics, resulting in distortions that hinder human interpretation and limit their applicability in scenarios where no initial input images are available. To address these challenges, we propose the Unsourced Adversarial CAPTCHA (DAC), a novel framework that generates high-fidelity adversarial examples guided by attacker-specified semantics information. Leveraging a Large Language Model (LLM), DAC enhances CAPTCHA diversity and enriches the semantic information. To address various application scenarios, we examine the white-box targeted attack scenario and the black box untargeted attack scenario. For target attacks, we introduce two latent noise variables that are alternately guided in the diffusion step to achieve robust inversion. The synergy between gradient guidance and latent variable optimization achieved in this way ensures that the generated adversarial examples not only accurately align with the target conditions but also achieve optimal performance in terms of distributional consistency and attack effectiveness. In untargeted attacks, especially for black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA (BP-DAC), a two-step optimization strategy employing multimodal gradients and bi-path optimization for efficient misclassification. Experiments show that the defensive adversarial CAPTCHA generated by BP-DAC is able to defend against most of the unknown models, and the generated CAPTCHA is indistinguishable to both humans and DNNs.</article>","contentLength":1806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making a Pipeline Production-Ready: Challenges and Lessons Learned in the Healthcare Domain","url":"https://arxiv.org/abs/2506.06946","date":1751428800,"author":"","guid":179669,"unread":true,"content":"<article>arXiv:2506.06946v2 Announce Type: replace \nAbstract: Deploying a Machine Learning (ML) training pipeline into production requires good software engineering practices. Unfortunately, the typical data science workflow often leads to code that lacks critical software quality attributes. This experience report investigates this problem in SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to pre-diagnose insufficiency respiratory via speech analysis. This paper presents an overview of the architecture of the MLES, then compares three versions of its Continuous Training subsystem: from a proof of concept Big Ball of Mud (v1), to a design pattern-based Modular Monolith (v2), to a test-driven set of Microservices (v3) Each version improved its overall extensibility, maintainability, robustness, and resiliency. The paper shares challenges and lessons learned in this process, offering insights for researchers and practitioners seeking to productionize their pipelines.</article>","contentLength":991,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bregman Centroid Guided Cross-Entropy Method","url":"https://arxiv.org/abs/2506.02205","date":1751428800,"author":"","guid":179670,"unread":true,"content":"<article>arXiv:2506.02205v2 Announce Type: replace \nAbstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in model-based reinforcement learning (MBRL), but its unimodal sampling strategy often leads to premature convergence in multimodal landscapes. In this work, we propose Bregman Centroid Guided CEM ($\\mathcal{BC}$-EvoCEM), a lightweight enhancement to ensemble CEM that leverages $\\textit{Bregman centroids}$ for principled information aggregation and diversity control. $\\textbf{$\\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman centroid across CEM workers and updates the least contributing ones by sampling within a trust region around the centroid. Leveraging the duality between Bregman divergences and exponential family distributions, we show that $\\textbf{$\\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM pipelines with negligible overhead. Empirical results on synthetic benchmarks, a cluttered navigation task, and full MBRL pipelines demonstrate that $\\textbf{$\\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution quality, providing a simple yet effective upgrade for CEM.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems","url":"https://arxiv.org/abs/2506.02007","date":1751428800,"author":"","guid":179671,"unread":true,"content":"<article>arXiv:2506.02007v2 Announce Type: replace \nAbstract: We present eACGM, a full-stack AI/ML system monitoring framework based on eBPF. eACGM collects real-time performance data from key hardware components, including the GPU and network communication layer, as well as from key software stacks such as CUDA, Python, and PyTorch, all without requiring any code instrumentation or modifications. Additionally, it leverages libnvml to gather process-level GPU resource usage information. By applying a Gaussian Mixture Model (GMM) to the collected multidimensional performance metrics for statistical modeling and clustering analysis, eACGM effectively identifies complex failure modes, such as latency anomalies, hardware failures, and communication inefficiencies, enabling rapid diagnosis of system bottlenecks and abnormal behaviors.\n  To evaluate eACGM's effectiveness and practicality, we conducted extensive empirical studies and case analyses in multi-node distributed training scenarios. The results demonstrate that eACGM, while maintaining a non-intrusive and low-overhead profile, successfully captures critical performance anomalies during model training and inference. Its stable anomaly detection performance and comprehensive monitoring capabilities validate its applicability and scalability in real-world production environments, providing strong support for performance optimization and fault diagnosis in large-scale AI/ML systems.</article>","contentLength":1446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding the Identity-Transformation Approach in OIDC-Compatible Privacy-Preserving SSO Services","url":"https://arxiv.org/abs/2506.01325","date":1751428800,"author":"","guid":179672,"unread":true,"content":"<article>arXiv:2506.01325v2 Announce Type: replace \nAbstract: OpenID Connect (OIDC) enables a user with commercial-off-the-shelf browsers to log into multiple websites, called relying parties (RPs), by her username and credential set up in another trusted web system, called the identity provider (IdP). Identity transformations are proposed in UppreSSO to provide OIDC-compatible SSO services, preventing both IdP-based login tracing and RP-based identity linkage. While security and privacy of SSO services in UppreSSO have been proved, several essential issues of this identity-transformation approach are not well studied. In this paper, we comprehensively investigate the approach as below. Firstly, several suggestions for the efficient integration of identity transformations in OIDC-compatible SSO are explained. Then, we uncover the relationship between identity-transformations in SSO and oblivious pseudo-random functions (OPRFs), and present two variations of the properties required for SSO security as well as the privacy requirements, to analyze existing OPRF protocols. Finally, new identity transformations different from those designed in UppreSSO, are constructed based on OPRFs, satisfying different variations of SSO security requirements. To the best of our knowledge, this is the first time to uncover the relationship between identity transformations in OIDC-compatible privacy-preserving SSO services and OPRFs, and prove the SSO-related properties (i.e., key-identifier freeness, RP designation and user identification) of OPRF protocols, in addition to the basic properties of correctness, obliviousness and pseudo-randomness.</article>","contentLength":1644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiresolution Analysis and Statistical Thresholding on Dynamic Networks","url":"https://arxiv.org/abs/2506.01208","date":1751428800,"author":"","guid":179673,"unread":true,"content":"<article>arXiv:2506.01208v2 Announce Type: replace \nAbstract: Detecting structural change in dynamic network data has wide-ranging applications. Existing approaches typically divide the data into time bins, extract network features within each bin, and then compare these features over time. This introduces an inherent tradeoff between temporal resolution and the statistical stability of the extracted features. Despite this tradeoff, reminiscent of time-frequency tradeoffs in signal processing, most methods rely on a fixed temporal resolution. Choosing an appropriate resolution parameter is typically difficult and can be especially problematic in domains like cybersecurity, where anomalous behavior may emerge at multiple time scales. We address this challenge by proposing ANIE (Adaptive Network Intensity Estimation), a multi-resolution framework designed to automatically identify the time scales at which network structure evolves, enabling the joint detection of both rapid and gradual changes. Modeling interactions as Poisson processes, our method proceeds in two steps: (1) estimating a low-dimensional subspace of node behavior, and (2) deriving a set of novel empirical affinity coefficients that quantify change in interaction intensity between latent factors and support statistical testing for structural change across time scales. We provide theoretical guarantees for subspace estimation and the asymptotic behavior of the affinity coefficients, enabling model-based change detection. Experiments on synthetic networks show that ANIE adapts to the appropriate time resolution and is able to capture sharp structural changes while remaining robust to noise. Furthermore, applications to real-world data showcase the practical benefits of ANIE's multiresolution approach to detecting structural change over fixed resolution methods.</article>","contentLength":1844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data","url":"https://arxiv.org/abs/2505.24852","date":1751428800,"author":"","guid":179674,"unread":true,"content":"<article>arXiv:2505.24852v2 Announce Type: replace \nAbstract: On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequential data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5% area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on-chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of-the-art inference-only keyword spotting (KWS) accelerators or enabling $4.3\\times$ higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way 5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3% on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 $\\mu$W.</article>","contentLength":1640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?","url":"https://arxiv.org/abs/2505.24778","date":1751428800,"author":"","guid":179675,"unread":true,"content":"<article>arXiv:2505.24778v2 Announce Type: replace \nAbstract: As large language models (LLMs) are increasingly used in high-stakes domains, accurately assessing their confidence is crucial. Humans typically express confidence through epistemic markers (e.g., \"fairly confident\") instead of numerical values. However, it remains unclear whether LLMs consistently use these markers to reflect their intrinsic confidence due to the difficulty of quantifying uncertainty associated with various markers. To address this gap, we first define marker confidence as the observed accuracy when a model employs an epistemic marker. We evaluate its stability across multiple question-answering datasets in both in-distribution and out-of-distribution settings for open-source and proprietary LLMs. Our results show that while markers generalize well within the same distribution, their confidence is inconsistent in out-of-distribution scenarios. These findings raise significant concerns about the reliability of epistemic markers for confidence estimation, underscoring the need for improved alignment between marker based confidence and actual model uncertainty. Our code is available at https://github.com/HKUST-KnowComp/MarCon.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors","url":"https://arxiv.org/abs/2505.24625","date":1751428800,"author":"","guid":179676,"unread":true,"content":"<article>arXiv:2505.24625v2 Announce Type: replace \nAbstract: Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile App Reviews","url":"https://arxiv.org/abs/2505.23452","date":1751428800,"author":"","guid":179677,"unread":true,"content":"<article>arXiv:2505.23452v2 Announce Type: replace \nAbstract: Opinion mining plays a vital role in analysing user feedback and extracting insights from textual data. While most research focuses on sentiment polarity (e.g., positive, negative, neutral), fine-grained emotion classification in app reviews remains underexplored. Fine-grained emotion classification is thus needed to better understand users' affective responses and support downstream tasks such as feature-emotion analysis, user-oriented release planning, and issue triaging. This paper addresses this gap by identifying and addressing the challenges and limitations in fine-grained emotion analysis in the context of app reviews. Our study adapts Plutchik's emotion taxonomy to app reviews by developing a structured annotation framework and dataset. Through an iterative human annotation process, we define clear annotation guidelines and document key challenges in emotion classification. Additionally, we evaluate the feasibility of automating emotion annotation using large language models, assessing their cost-effectiveness and agreement with human-labelled data. Our findings reveal that while large language models significantly reduce manual effort and maintain substantial agreement with human annotators, full automation remains challenging due to the complexity of emotional interpretation. This work contributes to opinion mining in requirements engineering by providing structured guidelines, an annotated dataset, and insights for developing automated pipelines to capture the complexity of emotions in app reviews.</article>","contentLength":1587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conceptual Framework Toward Embodied Collective Adaptive Intelligence","url":"https://arxiv.org/abs/2505.23153","date":1751428800,"author":"","guid":179678,"unread":true,"content":"<article>arXiv:2505.23153v2 Announce Type: replace \nAbstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.</article>","contentLength":926,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Training Free Stylized Abstraction","url":"https://arxiv.org/abs/2505.22663","date":1751428800,"author":"","guid":179679,"unread":true,"content":"<article>arXiv:2505.22663v2 Announce Type: replace \nAbstract: Stylized abstraction synthesizes visually exaggerated yet semantically faithful representations of subjects, balancing recognizability with perceptual distortion. Unlike image-to-image translation, which prioritizes structural fidelity, stylized abstraction demands selective retention of identity cues while embracing stylistic divergence, especially challenging for out-of-distribution individuals. We propose a training-free framework that generates stylized abstractions from a single image using inference-time scaling in vision-language models (VLLMs) to extract identity-relevant features, and a novel cross-domain rectified flow inversion strategy that reconstructs structure based on style-dependent priors. Our method adapts structural restoration dynamically through style-aware temporal scheduling, enabling high-fidelity reconstructions that honor both subject and style. It supports multi-round abstraction-aware generation without fine-tuning. To evaluate this task, we introduce StyleBench, a GPT-based human-aligned metric suited for abstract styles where pixel-level similarity fails. Experiments across diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong generalization to unseen identities and styles in a fully open-source setup.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lazarus Group Targets Crypto-Wallets and Financial Data while employing new Tradecrafts","url":"https://arxiv.org/abs/2505.21725","date":1751428800,"author":"","guid":179680,"unread":true,"content":"<article>arXiv:2505.21725v2 Announce Type: replace \nAbstract: This report presents a comprehensive analysis of a malicious software sample, detailing its architecture, behavioral characteristics, and underlying intent. Through static and dynamic examination, the malware core functionalities, including persistence mechanisms, command-and-control communication, and data exfiltration routines, are identified and its supporting infrastructure is mapped. By correlating observed indicators of compromise with known techniques, tactics, and procedures, this analysis situates the sample within the broader context of contemporary threat campaigns and infers the capabilities and motivations of its likely threat actor.\n  Building on these findings, actionable threat intelligence is provided to support proactive defenses. Threat hunting teams receive precise detection hypotheses for uncovering latent adversarial presence, while monitoring systems can refine alert logic to detect anomalous activity in real time. Finally, the report discusses how this structured intelligence enhances predictive risk assessments, informs vulnerability prioritization, and strengthens organizational resilience against advanced persistent threats. By integrating detailed technical insights with strategic threat landscape mapping, this malware analysis report not only reconstructs past adversary actions but also establishes a robust foundation for anticipating and mitigating future attacks.</article>","contentLength":1469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data","url":"https://arxiv.org/abs/2505.20485","date":1751428800,"author":"","guid":179681,"unread":true,"content":"<article>arXiv:2505.20485v3 Announce Type: replace \nAbstract: The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Program of Equations Thoughts to Solve Algebra Word Problems","url":"https://arxiv.org/abs/2505.20170","date":1751428800,"author":"","guid":179682,"unread":true,"content":"<article>arXiv:2505.20170v2 Announce Type: replace \nAbstract: Solving algebraic word problems (AWPs) has recently emerged as an important natural language processing task. Recently, large language models (LLMs) have demonstrated powerful mathematical capabilities, and the Chain-of-Thought technique, which guides LLMs through step-by-step reasoning, has yielded impressive results. However, this reasoning ability is limited by the computational weaknesses of LLMs themselves, where calculation errors can accumulate, leading to incorrect final answers. To address this, we propose Program of Equations Thoughts (POET), which transforms the task of generating step-by-step reasoning answers into a two-stage task of predicting equations and generating code, offloading complex computations to a Python interpreter to avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which utilizes a manually designed template to enable LLMs to directly generate Python code for one-step solving. Our method achieves accuracies of 95.3% and 98.0% on the PEN and ALG514 datasets, respectively, setting a new state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5% on the DRAW-1K dataset.</article>","contentLength":1209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale","url":"https://arxiv.org/abs/2505.20094","date":1751428800,"author":"","guid":179683,"unread":true,"content":"<article>arXiv:2505.20094v3 Announce Type: replace \nAbstract: Can a scientific simulation system be physically consistent, interpretable by design, and scalable across regimes--all at once? Despite decades of progress, this trifecta remains elusive. Classical methods like Kinetic Monte Carlo ensure thermodynamic accuracy but scale poorly; learning-based methods offer efficiency but often sacrifice physical consistency and interpretability. We present SwarmThinkers, a reinforcement learning framework that recasts atomic-scale simulation as a physically grounded swarm intelligence system. Each diffusing particle is modeled as a local decision-making agent that selects transitions via a shared policy network trained under thermodynamic constraints. A reweighting mechanism fuses learned preferences with transition rates, preserving statistical fidelity while enabling interpretable, step-wise decision making. Training follows a centralized-training, decentralized-execution paradigm, allowing the policy to generalize across system sizes, concentrations, and temperatures without retraining. On a benchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers is the first system to achieve full-scale, physically consistent simulation on a single A100 GPU, previously attainable only via OpenKMC on a supercomputer. It delivers up to 4963x (3185x on average) faster computation with 485x lower memory usage. By treating particles as decision-makers, not passive samplers, SwarmThinkers marks a paradigm shift in scientific simulation--one that unifies physical consistency, interpretability, and scalability through agent-driven intelligence.</article>","contentLength":1660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research","url":"https://arxiv.org/abs/2505.19955","date":1751428800,"author":"","guid":179684,"unread":true,"content":"<article>arXiv:2505.19955v2 Announce Type: replace \nAbstract: Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.</article>","contentLength":1567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two-Stage Regularization-Based Structured Pruning for LLMs","url":"https://arxiv.org/abs/2505.18232","date":1751428800,"author":"","guid":179685,"unread":true,"content":"<article>arXiv:2505.18232v2 Announce Type: replace \nAbstract: The deployment of large language models (LLMs) is largely hindered by their large number of parameters. Structural pruning has emerged as a promising solution. Prior structured pruning methods directly remove unimportant parameters based on certain metrics, which often causes knowledge loss and necessitates extensive retraining. To overcome this, we introduce a novel pruning method TRSP: Two-Stage Regularization-Based Structured Pruning for LLMs. Specifically, we multiply the output of each transformer layer by an initial learnable weight and iteratively learn these weights by adding their $\\ell_1$-norm as a regularization term to the loss function, serving as the first-stage regularization. Subsequently, we apply additional regularization to the difference between the output and input of layers with smaller weights, encouraging the shift of knowledge to the preserved layers. This serves as the second-stage regularization. TRSP retains more knowledge and better preserves model performance than direct parameter elimination. Through extensive experimentation we show that TRSP outperforms strong layer-wise structured pruning methods without requiring retraining. As a layer-wise pruning method, it delivers notable end-to-end acceleration, making it a promising solution for efficient LLM deployment.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning","url":"https://arxiv.org/abs/2505.17117","date":1751428800,"author":"","guid":179686,"unread":true,"content":"<article>arXiv:2505.17117v3 Announce Type: replace \nAbstract: Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Not Minds, but Signs: Reframing LLMs through Semiotics","url":"https://arxiv.org/abs/2505.17080","date":1751428800,"author":"","guid":179687,"unread":true,"content":"<article>arXiv:2505.17080v2 Announce Type: replace \nAbstract: This paper challenges the prevailing tendency to frame Large Language Models (LLMs) as cognitive systems, arguing instead for a semiotic perspective that situates these models within the broader dynamics of sign manipulation and meaning-making. Rather than assuming that LLMs understand language or simulate human thought, we propose that their primary function is to recombine, recontextualize, and circulate linguistic forms based on probabilistic associations. By shifting from a cognitivist to a semiotic framework, we avoid anthropomorphism and gain a more precise understanding of how LLMs participate in cultural processes, not by thinking, but by generating texts that invite interpretation. Through theoretical analysis and practical examples, the paper demonstrates how LLMs function as semiotic agents whose outputs can be treated as interpretive acts, open to contextual negotiation and critical reflection. We explore applications in literature, philosophy, education, and cultural production, emphasizing how LLMs can serve as tools for creativity, dialogue, and critical inquiry. The semiotic paradigm foregrounds the situated, contingent, and socially embedded nature of meaning, offering a more rigorous and ethically aware framework for studying and using LLMs. Ultimately, this approach reframes LLMs as technological participants in an ongoing ecology of signs. They do not possess minds, but they alter how we read, write, and make meaning, compelling us to reconsider the foundations of language, interpretation, and the role of artificial systems in the production of knowledge.</article>","contentLength":1654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification","url":"https://arxiv.org/abs/2505.16722","date":1751428800,"author":"","guid":179688,"unread":true,"content":"<article>arXiv:2505.16722v2 Announce Type: replace \nAbstract: As large language models (LLMs) become increasingly prevalent in global applications, ensuring that they are toxicity-free across diverse linguistic contexts remains a critical challenge. We explore \"Cross-lingual Detoxification\", a cross-lingual paradigm that mitigates toxicity, enabling detoxification capabilities to transfer between high and low-resource languages across different script families. We analyze cross-lingual detoxification's effectiveness through 392 extensive settings to evaluate toxicity reduction in cross-distribution settings with limited data and investigate how mitigation impacts model performance on non-toxic tasks, revealing trade-offs between safety and knowledge preservation. Our code and dataset are publicly available at https://github.com/himanshubeniwal/Breaking-mBad.</article>","contentLength":861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks","url":"https://arxiv.org/abs/2505.16459","date":1751428800,"author":"","guid":179689,"unread":true,"content":"<article>arXiv:2505.16459v3 Announce Type: replace \nAbstract: Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.</article>","contentLength":1634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models","url":"https://arxiv.org/abs/2505.16211","date":1751428800,"author":"","guid":179690,"unread":true,"content":"<article>arXiv:2505.16211v2 Announce Type: replace \nAbstract: The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.</article>","contentLength":1784,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought","url":"https://arxiv.org/abs/2505.12514","date":1751428800,"author":"","guid":179691,"unread":true,"content":"<article>arXiv:2505.12514v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thoughts (CoTs) techniques that generate ``thinking tokens'' before answering the questions. While existing theoretical works demonstrate that CoTs with discrete tokens boost the capability of LLMs, recent work on continuous CoTs lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. In this paper, we prove that a two-layer transformer with $D$ steps of continuous CoTs can solve the directed graph reachability problem, where $D$ is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps where $n$ is the number of vertices ($D&lt;n$). In our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions. We also performed extensive experiments to verify that our theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoTs, without explicit supervision to guide the model to explore multiple paths simultaneously.</article>","contentLength":1742,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption","url":"https://arxiv.org/abs/2505.12147","date":1751428800,"author":"","guid":179692,"unread":true,"content":"<article>arXiv:2505.12147v3 Announce Type: replace \nAbstract: The rapid increase in computing power and the ability to store Big Data in the infrastructure has enabled predictions in a large variety of domains by Machine Learning. However, in many cases, existing Machine Learning tools are considered insufficient or incorrect since they exploit only probabilistic dependencies rather than inference logic. Causal Machine Learning methods seem to close this gap. In this paper, two prevalent tools based on Causal Machine Learning methods are compared, as well as their mathematical underpinning background. The operation of the tools is demonstrated by examining their response to 18 queries, based on the IDEAL Household Energy Dataset, published by the University of Edinburgh. First, it was important to evaluate the causal relations assumption that allowed the use of this approach; this was based on the preexisting scientific knowledge of the domain and was implemented by use of the in-built validation tools. Results were encouraging and may easily be extended to other domains.</article>","contentLength":1079,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Limitations to Computing Quadratic Functions on Reed-Solomon Encoded Data","url":"https://arxiv.org/abs/2505.08000","date":1751428800,"author":"","guid":179693,"unread":true,"content":"<article>arXiv:2505.08000v2 Announce Type: replace \nAbstract: We study the problem of low-bandwidth non-linear computation on Reed-Solomon encoded data. Given an $[n,k]$ Reed-Solomon encoding of a message vector $\\vec{f} \\in \\mathbb{F}_q^k$, and a polynomial $g \\in \\mathbb{F}_q[X_1, X_2, \\ldots, X_k]$, a user wishing to evaluate $g(\\vec{f})$ is given local query access to each codeword symbol. The query response is allowed to be the output of an arbitrary function evaluated locally on the codeword symbol, and the user's aim is to minimize the total information downloaded in order to compute $g(\\vec{f})$.\n  We show that when $k=2$ and $q = p^e$ for prime $p &gt; 2$, then any scheme that evaluates the quadratic monomial $g(X_1, X_2) := X_1 X_2$ must download at least $2 \\log_2(q-1) - 3$ bits of information; compare this with the na\\\"{\\i}ve scheme of Reed-Solomon interpolation which recovers $\\vec{f}$ in its entirety, which downloads $2 \\log_2(q) $ bits. Our result shows that dimension-2 Reed-Solomon codes do not admit any meaningful low-bandwidth scheme for the evaluation of quadratic functions over the encoded data. This contrasts sharply with prior work for low-bandwidth evaluation of \\emph{linear} functions $g(\\vec{f})$ over Reed-Solomon encoded data, for which it is possible to substantially improve upon the na\\\"{\\i}ve bound of $k \\log_2(q) $.</article>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs","url":"https://arxiv.org/abs/2505.06096","date":1751428800,"author":"","guid":179694,"unread":true,"content":"<article>arXiv:2505.06096v2 Announce Type: replace \nAbstract: Limitations in Large Language Model (LLM) capabilities for hardware design tasks, such as generating functional Verilog codes, have motivated various fine-tuning optimizations utilizing curated hardware datasets from open-source repositories. However, these datasets remain limited in size and contain minimal checks on licensing for reuse, resulting in potential copyright violations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to estimate the risk of Verilog-trained LLMs to generate copyright-protected codes. To minimize this risk, we present an open-source Verilog dataset, FreeSet, containing over 220k files, along with the automated dataset curation framework utilized to provide additional guarantees of fair-use Verilog data. We then execute an LLM fine-tuning framework consisting of continual pre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our results indicate that FreeV demonstrates the smallest risk of copyright-infringement among prior works, with only a 3% violation rate. Furthermore, experimental results demonstrate improvements in Verilog generation functionality over its baseline model, improving VerilogEval pass@10 rates by over 10%.</article>","contentLength":1259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Triangular preconditioners for double saddle point linear systems arising in the mixed form of poroelasticity equations","url":"https://arxiv.org/abs/2505.06043","date":1751428800,"author":"","guid":179695,"unread":true,"content":"<article>arXiv:2505.06043v2 Announce Type: replace \nAbstract: In this paper, we study a class of inexact block triangular preconditioners for double saddle-point symmetric linear systems arising from the mixed finite element and mixed hybrid finite element discretization of Biot's poroelasticity equations. We develop a spectral analysis of the preconditioned matrix, showing that the complex eigenvalues lie in a circle of center $(1,0)$ and radius smaller than 1. In contrast, the real eigenvalues are described in terms of the roots of a third-degree polynomial with real coefficients. The results of numerical experiments are reported to show the quality of the theoretical bounds and illustrate the efficiency of the proposed preconditioners used with GMRES, especially in comparison with similar block diagonal preconditioning strategies along with the MINRES iteration.</article>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating Physically Stable and Buildable Brick Structures from Text","url":"https://arxiv.org/abs/2505.05469","date":1751428800,"author":"","guid":179696,"unread":true,"content":"<article>arXiv:2505.05469v2 Announce Type: replace \nAbstract: We introduce BrickGPT, the first approach for generating physically stable interconnecting brick assembly models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of brick structures, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that BrickGPT produces stable, diverse, and aesthetically pleasing brick structures that align closely with the input text prompts. We also develop a text-based brick texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We release our new dataset, StableText2Brick, containing over 47,000 brick structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/BrickGPT/.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition","url":"https://arxiv.org/abs/2505.04203","date":1751428800,"author":"","guid":179697,"unread":true,"content":"<article>arXiv:2505.04203v2 Announce Type: replace \nAbstract: The art of instrument performance stands as a vivid manifestation of human creativity and emotion. Nonetheless, generating instrument performance motions is a highly challenging task, as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performer-instrument interaction. While existing works primarily focus on modeling partial body motions, we propose Expressive ceLlo performance motion Generation for Audio Rendition (ELGAR), a state-of-the-art diffusion-based framework for whole-body fine-grained instrument performance motion generation solely from audio. To emphasize the interactive nature of the instrument performance, we introduce Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL), which effectively guarantee the authenticity of the interplay. Moreover, to better evaluate whether the generated motions align with the semantic context of the music audio, we design novel metrics specifically for string instrument performance motion generation, including finger-contact distance, bow-string distance, and bowing score. Extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods. In addition, we put forward a motion generation dataset SPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated, ELGAR has shown great potential in generating instrument performance motions with complicated and fast interactions, which will promote further development in areas such as animation, music education, interactive art creation, etc.</article>","contentLength":1636,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SKALD: Scalable K-Anonymisation for Large Datasets","url":"https://arxiv.org/abs/2505.03529","date":1751428800,"author":"","guid":179698,"unread":true,"content":"<article>arXiv:2505.03529v2 Announce Type: replace \nAbstract: Data privacy and anonymisation are critical concerns in today's data-driven society, particularly when handling personal and sensitive user data. Regulatory frameworks worldwide recommend privacy-preserving protocols such as k-anonymisation to de-identify releases of tabular data. Available hardware resources provide an upper bound on the maximum size of dataset that can be processed at a time. Large datasets with sizes exceeding this upper bound must be broken up into smaller data chunks for processing. In these cases, standard k-anonymisation tools such as ARX can only operate on a per-chunk basis. This paper proposes SKALD, a novel algorithm for performing k-anonymisation on large datasets with limited RAM. Our SKALD algorithm offers multi-fold performance improvement over standard k-anonymisation methods by extracting and combining sufficient statistics from each chunk during processing to ensure successful k-anonymisation while providing better utility.</article>","contentLength":1025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach","url":"https://arxiv.org/abs/2505.02952","date":1751428800,"author":"","guid":179699,"unread":true,"content":"<article>arXiv:2505.02952v2 Announce Type: replace \nAbstract: Generative AI systems have revolutionized human interaction by enabling natural language-based coding and problem solving. However, the inherent ambiguity of natural language often leads to imprecise instructions, forcing users to iteratively test, correct, and resubmit their prompts. We propose an iterative approach that systematically narrows down these ambiguities through a structured series of clarification questions and alternative solution proposals, illustrated with input/output examples as well. Once every uncertainty is resolved, a final, precise solution is generated. Evaluated on a diverse dataset spanning coding, data analysis, and creative writing, our method demonstrates superior accuracy, competitive resolution times, and higher user satisfaction compared to conventional one-shot solutions, which typically require multiple manual iterations to achieve a correct output.</article>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Deep Learning-Aided Approach for Estimating Field Permeability Map by Fusing Well Logs, Well Tests, and Seismic Data","url":"https://arxiv.org/abs/2505.02093","date":1751428800,"author":"","guid":179700,"unread":true,"content":"<article>arXiv:2505.02093v3 Announce Type: replace \nAbstract: Obtaining reliable permeability maps of oil reservoirs is crucial for building a robust and accurate reservoir simulation model and, therefore, designing effective recovery strategies. This problem, however, remains challenging, as it requires the integration of various data sources by experts from different disciplines. Moreover, there are no sources to provide direct information about the inter-well space. In this work, a new method based on the data-fusion approach is proposed for predicting two-dimensional permeability maps on the whole reservoir area. This method utilizes non-parametric regression with a custom kernel shape accounting for different data sources: well logs, well tests, and seismics. A convolutional neural network is developed to process seismic data and then incorporate it with other sources. A multi-stage data fusion procedure helps to artificially increase the training dataset for the seismic interpretation model and finally to construct the adequate permeability map. The proposed methodology of permeability map construction from different sources was tested on a real oil reservoir located in Western Siberia. The results demonstrate that the developed map perfectly corresponds to the permeability estimations in the wells, and the inter-well space permeability predictions are considerably improved through the incorporation of the seismic data.</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Llama-Nemotron: Efficient Reasoning Models","url":"https://arxiv.org/abs/2505.00949","date":1751428800,"author":"","guid":179701,"unread":true,"content":"<article>arXiv:2505.00949v4 Announce Type: replace \nAbstract: We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.</article>","contentLength":1462,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT","url":"https://arxiv.org/abs/2505.00703","date":1751428800,"author":"","guid":179702,"unread":true,"content":"<article>arXiv:2505.00703v2 Announce Type: replace \nAbstract: Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving the scalability of a high-order atmospheric dynamics solver based on the deal.II library","url":"https://arxiv.org/abs/2505.00384","date":1751428800,"author":"","guid":179703,"unread":true,"content":"<article>arXiv:2505.00384v3 Announce Type: replace \nAbstract: We present recent advances on the massively parallel performance of a numerical scheme for atmosphere dynamics applications based on the deal.II library. The implicit-explicit discontinuous finite element scheme is based on a matrix-free approach, meaning that no global sparse matrix is built and only the action of the linear operators on a vector is actually implemented. Following a profiling analysis, we focus on the performance optimization of the numerical method and describe the impact of different preconditioning and solving techniques in this framework. Moreover, we show how the use of the latest version of the deal.II library and of suitable execution flags can improve the parallel performance.</article>","contentLength":764,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language","url":"https://arxiv.org/abs/2504.19856","date":1751428800,"author":"","guid":179704,"unread":true,"content":"<article>arXiv:2504.19856v3 Announce Type: replace \nAbstract: Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., masked language modeling (MLM), when common domain adaptation via LM fine-tuning is not possible due to a lack of labeled task data. Although popular, MLM requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that the best configuration of ICL-APT performed better than the state-of-the-art DAPT by 28.7% (7.87 points) and requires almost 4 times less GPU-computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.</article>","contentLength":1299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Attentive Neural Processes for Planning with Pushing Actions","url":"https://arxiv.org/abs/2504.17924","date":1751428800,"author":"","guid":179705,"unread":true,"content":"<article>arXiv:2504.17924v3 Announce Type: replace \nAbstract: Our goal is to enable robots to plan sequences of tabletop actions to push a block with unknown physical properties to a desired goal pose. We approach this problem by learning the constituent models of a Partially-Observable Markov Decision Process (POMDP), where the robot can observe the outcome of a push, but the physical properties of the block that govern the dynamics remain unknown. A common solution approach is to train an observation model in a supervised fashion, and do inference with a general inference technique such as particle filters. However, supervised training requires knowledge of the relevant physical properties that determine the problem dynamics, which we do not assume to be known. Planning also requires simulating many belief updates, which becomes expensive when using particle filters to represent the belief. We propose to learn an Attentive Neural Process that computes the belief over a learned latent representation of the relevant physical properties given a history of actions. To address the pushing planning problem, we integrate a trained Neural Process with a double-progressive widening sampling strategy. Simulation results indicate that Neural Process Tree with Double Progressive Widening (NPT-DPW) generates better-performing plans faster than traditional particle-filter methods that use a supervised-trained observation model, even in complex pushing scenarios.</article>","contentLength":1465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Generalized and Training-Free Text-Guided Semantic Manipulation","url":"https://arxiv.org/abs/2504.17269","date":1751428800,"author":"","guid":179706,"unread":true,"content":"<article>arXiv:2504.17269v2 Announce Type: replace \nAbstract: Text-guided semantic manipulation refers to semantically editing an image generated from a source prompt to match a target prompt, enabling the desired semantic changes (e.g., addition, removal, and style transfer) while preserving irrelevant contents. With the powerful generative capabilities of the diffusion model, the task has shown the potential to generate high-fidelity visual content. Nevertheless, existing methods either typically require time-consuming fine-tuning (inefficient), fail to accomplish multiple semantic manipulations (poorly extensible), and/or lack support for different modality tasks (limited generalizability). Upon further investigation, we find that the geometric properties of noises in the diffusion model are strongly correlated with the semantic changes. Motivated by this, we propose a novel $\\textit{GTF}$ for text-guided semantic manipulation, which has the following attractive capabilities: 1) $\\textbf{Generalized}$: our $\\textit{GTF}$ supports multiple semantic manipulations (e.g., addition, removal, and style transfer) and can be seamlessly integrated into all diffusion-based methods (i.e., Plug-and-play) across different modalities (i.e., modality-agnostic); and 2) $\\textbf{Training-free}$: $\\textit{GTF}$ produces high-fidelity results via simply controlling the geometric relationship between noises without tuning or optimization. Our extensive experiments demonstrate the efficacy of our approach, highlighting its potential to advance the state-of-the-art in semantics manipulation.</article>","contentLength":1590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To Offload or Not To Offload: Model-driven Comparison of Edge-native and On-device Processing","url":"https://arxiv.org/abs/2504.15162","date":1751428800,"author":"","guid":179707,"unread":true,"content":"<article>arXiv:2504.15162v2 Announce Type: replace \nAbstract: Computational offloading is a promising approach for overcoming resource constraints on client devices by moving some or all of an application's computations to remote servers. With the advent of specialized hardware accelerators, client devices are now able to perform fast local processing of specific tasks, such as machine learning inference, reducing the need for offloading computations. However, edge servers with accelerators also offer faster processing for offloaded tasks than was previously possible. In this paper, we present an analytic and experimental comparison of on-device processing and edge offloading for a range of accelerator, network, and application workload scenarios, with the goal of understanding when to use local on-device processing and when to offload computations. We present models that leverage analytical queuing results to capture the effects of dynamic factors such as the performance gap between the device and edge server, network variability, server load, and multi-tenancy on the edge server. We experimentally demonstrate the accuracy of our models for a range of hardware and application scenarios and show that our models achieve a mean absolute percentage error of 2.2% compared to observed latencies. We use our models to develop an adaptive resource manager for intelligent offloading and show its efficacy in the presence of variable network conditions and dynamic multi-tenant edge settings.</article>","contentLength":1496,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unified Manipulability and Compliance Analysis of Modular Soft-Rigid Hybrid Fingers","url":"https://arxiv.org/abs/2504.13800","date":1751428800,"author":"","guid":179708,"unread":true,"content":"<article>arXiv:2504.13800v2 Announce Type: replace \nAbstract: This paper presents a unified framework to analyze the manipulability and compliance of modular soft-rigid hybrid robotic fingers. The approach applies to both hydraulic and pneumatic actuation systems. A Jacobian-based formulation maps actuator inputs to joint and task-space responses. Hydraulic actuators are modeled under incompressible assumptions, while pneumatic actuators are described using nonlinear pressure-volume relations. The framework enables consistent evaluation of manipulability ellipsoids and compliance matrices across actuation modes. We validate the analysis using two representative hands: DexCo (hydraulic) and Edgy-2 (pneumatic). Results highlight actuation-dependent trade-offs in dexterity and passive stiffness. These findings provide insights for structure-aware design and actuator selection in soft-rigid robotic fingers.</article>","contentLength":907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Algorithms for the Shortest Vector Problem in $2$-dimensional Lattices, Revisited","url":"https://arxiv.org/abs/2504.12948","date":1751428800,"author":"","guid":179709,"unread":true,"content":"<article>arXiv:2504.12948v2 Announce Type: replace \nAbstract: Efficiently solving the Shortest Vector Problem (SVP) in two-dimensional lattices holds practical significance in cryptography and computational geometry. While simpler than its high-dimensional counterpart, two-dimensional SVP motivates scalable solutions for high-dimensional lattices and benefits applications like sequence cipher cryptanalysis involving large integers. In this work, we first propose a novel definition of reduced bases and develop an efficient adaptive lattice reduction algorithm \\textbf{CrossEuc} that strategically applies the Euclidean algorithm across dimensions. Building on this framework, we introduce \\textbf{HVec}, a vectorized generalization of the Half-GCD algorithm originally defined for integers, which can efficiently halve the bit-length of two vectors and may have independent interest. By iteratively invoking \\textbf{HVec}, our optimized algorithm \\textbf{HVecSBP} achieves a reduced basis in $O(\\log n M(n) )$ time for arbitrary input bases with bit-length $n$, where $M(n)$ denotes the cost of multiplying two $n$-bit integers. Compared to existing algorithms, our design is applicable to general forms of input lattices, eliminating the cost of pre-converting input bases to Hermite Normal Form (HNF). The comprehensive experimental results demonstrate that for the input lattice bases in HNF, the optimized algorithm \\textbf{HVecSBP} achieves at least a $13.5\\times$ efficiency improvement compared to existing methods. For general-form input lattice bases, converting them to HNF before applying \\textbf{HVecSBP} offers only marginal advantages in extreme cases where the two basis vectors are nearly degenerate. However, as the linear dependency between input basis vectors decreases, directly employing \\textbf{HVecSBP} yields increasingly significant efficiency gains, outperforming hybrid approaches that rely on prior \\textbf{HNF} conversion.</article>","contentLength":1947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and Scalable Classification","url":"https://arxiv.org/abs/2504.12652","date":1751428800,"author":"","guid":179710,"unread":true,"content":"<article>arXiv:2504.12652v2 Announce Type: replace \nAbstract: This paper introduces AdaptoVision, a novel convolutional neural network (CNN) architecture designed to efficiently balance computational complexity and classification accuracy. By leveraging enhanced residual units, depth-wise separable convolutions, and hierarchical skip connections, AdaptoVision significantly reduces parameter count and computational requirements while preserving competitive performance across various benchmark and medical image datasets. Extensive experimentation demonstrates that AdaptoVision achieves state-of-the-art on BreakHis dataset and comparable accuracy levels, notably 95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained weights. The model's streamlined architecture and strategic simplifications promote effective feature extraction and robust generalization, making it particularly suitable for deployment in real-time and resource-constrained environments.</article>","contentLength":976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Look and Talk: Seamless AI Assistant Interaction with Gaze-Triggered Activation","url":"https://arxiv.org/abs/2504.09296","date":1751428800,"author":"","guid":179711,"unread":true,"content":"<article>arXiv:2504.09296v2 Announce Type: replace \nAbstract: Engaging with AI assistants to gather essential information in a timely manner is becoming increasingly common. Traditional activation methods, like wake words such as Hey Siri, Ok Google, and Hey Alexa, are constrained by technical challenges such as false activations, recognition errors, and discomfort in public settings. Similarly, activating AI systems via physical buttons imposes strict interactive limitations as it demands particular physical actions, which hinders fluid and spontaneous communication with AI. Our approach employs eye-tracking technology within AR glasses to discern a user's intention to engage with the AI assistant. By sustaining eye contact on a virtual AI avatar for a specific time, users can initiate an interaction silently and without using their hands. Preliminary user feedback suggests that this technique is relatively intuitive, natural, and less obtrusive, highlighting its potential for integrating AI assistants fluidly into everyday interactions.</article>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization","url":"https://arxiv.org/abs/2504.08811","date":1751428800,"author":"","guid":179712,"unread":true,"content":"<article>arXiv:2504.08811v2 Announce Type: replace \nAbstract: Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is primarily due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite that data of each scenario has a distinct reference frame, its generation generally follows common underlying physical rules. Based on this understanding, this article proposes a deep learning framework named analogical learning (AL), which implicitly retrieves the reference frame information associated with a scenario and then to make accurate prediction by relative analogy with other scenarios. Specifically, we design a bipartite neural network called Mateformer. Its first part captures the relativity within multiple latent feature spaces between the input data and a small amount of embedded data from the studied scenario, while its second part uses this relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments validate AL's superiority across three key dimensions. First, it achieves state-of-the-art accuracy in single-scenario benchmarks. Second, it demonstrates stable transferability between different scenarios, avoiding catastrophic forgetting. Finally, and most importantly, it robustly adapts to new, unseen scenarios--including dynamic weather and traffic conditions--without any tuning. All data and code are available at https://github.com/ziruichen-research/ALLoc.</article>","contentLength":1607,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability","url":"https://arxiv.org/abs/2504.07416","date":1751428800,"author":"","guid":179713,"unread":true,"content":"<article>arXiv:2504.07416v2 Announce Type: replace \nAbstract: Recent advancements in multi-modal models have significantly improved vision-language (VL) alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning and offer limited interpretability through attention probability visualizations. To address these challenges, we introduce RadZero, a novel framework for VL alignment in radiology with zero-shot multi-task capability. A key component of our approach is VL-CABS (Vision-Language Cross-Attention Based on Similarity), which aligns text embeddings with local image features for interpretable, fine-grained VL reasoning. RadZero leverages large language models to extract concise semantic sentences from radiology reports and employs multi-positive contrastive training to effectively capture relationships between images and multiple relevant textual descriptions. It uses a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, VL-CABS enables zero-shot inference with similarity probability for classification, and pixel-level VL similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, VL similarity map analysis highlights the potential of VL-CABS for improving explainability in VL alignment. Additionally, qualitative evaluation demonstrates RadZero's capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging.</article>","contentLength":1761,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Plastic tensor networks for interpretable generative modeling","url":"https://arxiv.org/abs/2504.06722","date":1751428800,"author":"","guid":179714,"unread":true,"content":"<article>arXiv:2504.06722v2 Announce Type: replace \nAbstract: A structural optimization scheme for a single-layer nonnegative adaptive tensor tree (NATT) that models a target probability distribution is proposed as an alternative paradigm for generative modeling. The NATT scheme, by construction, automatically searches for a tree structure that best fits a given discrete dataset whose features serve as inputs, and has the advantage that it is interpretable as a probabilistic graphical model. We consider the NATT scheme and a recently proposed Born machine adaptive tensor tree (BMATT) optimization scheme and demonstrate their effectiveness on a variety of generative modeling tasks where the objective is to infer the hidden structure of a provided dataset. Our results show that in terms of minimizing the negative log-likelihood, the single-layer scheme has model performance comparable to the Born machine scheme, though not better. The tasks include deducing the structure of binary bitwise operations, learning the internal structure of random Bayesian networks given only visible sites, and a real-world example related to hierarchical clustering where a cladogram is constructed from mitochondrial DNA sequences. In doing so, we also show the importance of the choice of network topology and the versatility of a least-mutual information criterion in selecting a candidate structure for a tensor tree, as well as discuss aspects of these tensor tree generative models including their information content and interpretability.</article>","contentLength":1530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeking and Updating with Live Visual Knowledge","url":"https://arxiv.org/abs/2504.05288","date":1751428800,"author":"","guid":179715,"unread":true,"content":"<article>arXiv:2504.05288v2 Announce Type: replace \nAbstract: The visual world around us constantly evolves, from real-time news and social media trends to global infrastructure changes visible through satellite imagery and augmented reality enhancements. However, Multimodal Large Language Models (MLLMs), which automate many tasks, struggle to stay current, limited by the cutoff dates in their fixed training datasets. To quantify this stagnation, we introduce LiveVQA, the first-of-its-kind dataset featuring 107,143 samples and 12 categories data specifically designed to support research in both seeking and updating with live visual knowledge. Drawing from recent news articles, video platforms, and academic publications in April 2024-May 2025, LiveVQA enables evaluation of how models handle latest visual information beyond their knowledge boundaries and how current methods help to update them. Our comprehensive benchmarking of 17 state-of-the-art MLLMs reveals significant performance gaps on content beyond knowledge cutoff, and tool-use or agentic visual seeking framework drastically gain an average of 327% improvement. Furthermore, we explore parameter-efficient fine-tuning (PEFT) methods to update MLLMs with new visual knowledge. We dive deeply to the critical balance between adapter capacity and model capability when updating MLLMs with new visual knowledge. All the experimental dataset and source code are publicly available at: https://livevqa.github.io.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PEAKS: Selecting Key Training Examples Incrementally via Prediction Error Anchored by Kernel Similarity","url":"https://arxiv.org/abs/2504.05250","date":1751428800,"author":"","guid":179716,"unread":true,"content":"<article>arXiv:2504.05250v4 Announce Type: replace \nAbstract: As deep learning continues to be driven by ever-larger datasets, understanding which examples are most important for generalization has become a critical question. While progress in data selection continues, emerging applications require studying this problem in dynamic contexts. To bridge this gap, we pose the Incremental Data Selection (IDS) problem, where examples arrive as a continuous stream, and need to be selected without access to the full data source. In this setting, the learner must incrementally build a training dataset of predefined size while simultaneously learning the underlying task. We find that in IDS, the impact of a new sample on the model state depends fundamentally on both its geometric relationship in the feature space and its prediction error. Leveraging this insight, we propose PEAKS (Prediction Error Anchored by Kernel Similarity), an efficient data selection method tailored for IDS. Our comprehensive evaluations demonstrate that PEAKS consistently outperforms existing selection strategies. Furthermore, PEAKS yields increasingly better performance returns than random selection as training data size grows on real-world datasets. The code is available at https://github.com/BurakGurbuz97/PEAKS.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enabling Collaborative Parametric Knowledge Calibration for Retrieval-Augmented Vision Question Answering","url":"https://arxiv.org/abs/2504.04065","date":1751428800,"author":"","guid":179717,"unread":true,"content":"<article>arXiv:2504.04065v2 Announce Type: replace \nAbstract: Knowledge-based Vision Question Answering (KB-VQA) systems address complex visual-grounded questions with knowledge retrieved from external knowledge bases. The tasks of knowledge retrieval and answer generation tasks both necessitate precise multimodal understanding of question context and external knowledge. However, existing methods treat these two stages as separate modules with limited interaction during training, which hinders bi-directional parametric knowledge sharing, ultimately leading to suboptimal performance. To fully exploit the cross-task synergy in KB-VQA, we propose a unified retrieval-augmented VQA framework with collaborative parametric knowledge calibration. The proposed framework can effectively adapt general multimodal pre-trained models for fine-grained, knowledge-intensive tasks while enabling the retriever and generator to collaboratively enhance and share their parametric knowledge during both training and inference. To enhance fine-grained understanding of questions and external documents, we also integrate late interaction mechanism into the proposed training framework. Additionally, we introduce a reflective-answering mechanism that allows the model to explicitly evaluate and refine its knowledge boundary. Our approach achieves competitive performance against state-of-the-art models, delivering a significant 4.7\\% improvement in answering accuracy, and brings an average 7.5\\% boost in base MLLMs' VQA performance.</article>","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses","url":"https://arxiv.org/abs/2503.21393","date":1751428800,"author":"","guid":179718,"unread":true,"content":"<article>arXiv:2503.21393v3 Announce Type: replace \nAbstract: Large Language models (LLMs) have been prominent for language translation, including low-resource languages. There has been limited study on the assessment of the quality of translations generated by LLMs, including Gemini, GPT, and Google Translate. This study addresses this limitation by using semantic and sentiment analysis of selected LLMs for Indian languages, including Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita, Tamas and Maha Prasthanam ) that have been well translated by experts and use LLMs to generate their translations into English, and provide a comparison with selected expert (human) translations. Our investigation revealed that while LLMs have made significant progress in translation accuracy, challenges remain in preserving sentiment and semantic integrity, especially in metaphorical and philosophical contexts for texts such as the Bhagavad Gita. The sentiment analysis revealed that GPT models are better at preserving the sentiment polarity for the given texts when compared to human (expert) translation. The results revealed that GPT models are generally better at maintaining the sentiment and semantics when compared to Google Translate. This study could help in the development of accurate and culturally sensitive translation systems for large language models.</article>","contentLength":1374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition","url":"https://arxiv.org/abs/2503.21248","date":1751428800,"author":"","guid":179719,"unread":true,"content":"<article>arXiv:2503.21248v2 Announce Type: replace \nAbstract: Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as \"research hypothesis mines\", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.</article>","contentLength":1211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation","url":"https://arxiv.org/abs/2503.18549","date":1751428800,"author":"","guid":179720,"unread":true,"content":"<article>arXiv:2503.18549v2 Announce Type: replace \nAbstract: A CAD command sequence is a typical parametric design paradigm in 3D CAD systems where a model is constructed by overlaying 2D sketches with operations such as extrusion, revolution, and Boolean operations. Although there is growing academic interest in the automatic generation of command sequences, existing methods and datasets only support operations such as 2D sketching, extrusion,and Boolean operations. This limitation makes it challenging to represent more complex geometries. In this paper, we present a reinforcement learning (RL) training environment (gym) built on a CAD geometric engine. Given an input boundary representation (B-Rep) geometry, the policy network in the RL algorithm generates an action. This action, along with previously generated actions, is processed within the gym to produce the corresponding CAD geometry, which is then fed back into the policy network. The rewards, determined by the difference between the generated and target geometries within the gym, are used to update the RL network. Our method supports operations beyond sketches, Boolean, and extrusion, including revolution operations. With this training gym, we achieve state-of-the-art (SOTA) quality in generating command sequences from B-Rep geometries.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection","url":"https://arxiv.org/abs/2503.15044","date":1751428800,"author":"","guid":179721,"unread":true,"content":"<article>arXiv:2503.15044v2 Announce Type: replace \nAbstract: The increasing capability of large language models (LLMs) to generate synthetic content has heightened concerns about their misuse, driving the development of Machine-Generated Text (MGT) detection models. However, these detectors face significant challenges due to the lack of high-quality synthetic datasets for training. To address this issue, we propose SPADE, a structured framework for detecting synthetic dialogues using prompt-based positive and negative samples. Our proposed methods yield 14 new dialogue datasets, which we benchmark against eight MGT detection models. The results demonstrate improved generalization performance when utilizing a mixed dataset produced by proposed augmentation frameworks, offering a practical approach to enhancing LLM application security. Considering that real-world agents lack knowledge of future opponent utterances, we simulate online dialogue detection and examine the relationship between chat history length and detection accuracy. Our open-source datasets, code and prompts can be downloaded from https://github.com/AngieYYF/SPADE-customer-service-dialogue.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CoCMT: Communication-Efficient Cross-Modal Transformer for Collaborative Perception","url":"https://arxiv.org/abs/2503.13504","date":1751428800,"author":"","guid":179722,"unread":true,"content":"<article>arXiv:2503.13504v2 Announce Type: replace \nAbstract: Multi-agent collaborative perception enhances each agent perceptual capabilities by sharing sensing information to cooperatively perform robot perception tasks. This approach has proven effective in addressing challenges such as sensor deficiencies, occlusions, and long-range perception. However, existing representative collaborative perception systems transmit intermediate feature maps, such as bird-eye view (BEV) representations, which contain a significant amount of non-critical information, leading to high communication bandwidth requirements. To enhance communication efficiency while preserving perception capability, we introduce CoCMT, an object-query-based collaboration framework that optimizes communication bandwidth by selectively extracting and transmitting essential features. Within CoCMT, we introduce the Efficient Query Transformer (EQFormer) to effectively fuse multi-agent object queries and implement a synergistic deep supervision to enhance the positive reinforcement between stages, leading to improved overall performance. Experiments on OPV2V and V2V4Real datasets show CoCMT outperforms state-of-the-art methods while drastically reducing communication needs. On V2V4Real, our model (Top-50 object queries) requires only 0.416 Mb bandwidth, 83 times less than SOTA methods, while improving AP70 by 1.1 percent. This efficiency breakthrough enables practical collaborative perception deployment in bandwidth-constrained environments without sacrificing detection accuracy.</article>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Edit Transfer: Learning Image Editing via Vision In-Context Relations","url":"https://arxiv.org/abs/2503.13327","date":1751428800,"author":"","guid":179723,"unread":true,"content":"<article>arXiv:2503.13327v2 Announce Type: replace \nAbstract: We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning.</article>","contentLength":1265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead Control","url":"https://arxiv.org/abs/2503.11801","date":1751428800,"author":"","guid":179724,"unread":true,"content":"<article>arXiv:2503.11801v2 Announce Type: replace \nAbstract: We present Diffuse-CLoC, a guided diffusion framework for physics-based look-ahead control that enables intuitive, steerable, and physically realistic motion generation. While existing kinematics motion generation with diffusion models offer intuitive steering capabilities with inference-time conditioning, they often fail to produce physically viable motions. In contrast, recent diffusion-based control policies have shown promise in generating physically realizable motion sequences, but the lack of kinematics prediction limits their steerability. Diffuse-CLoC addresses these challenges through a key insight: modeling the joint distribution of states and actions within a single diffusion model makes action generation steerable by conditioning it on the predicted states. This approach allows us to leverage established conditioning techniques from kinematic motion generation while producing physically realistic motions. As a result, we achieve planning capabilities without the need for a high-level planner. Our method handles a diverse set of unseen long-horizon downstream tasks through a single pre-trained model, including static and dynamic obstacle avoidance, motion in-betweening, and task-space control. Experimental results show that our method significantly outperforms the traditional hierarchical framework of high-level motion diffusion and low-level tracking.</article>","contentLength":1438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mirror Online Conformal Prediction with Intermittent Feedback","url":"https://arxiv.org/abs/2503.10345","date":1751428800,"author":"","guid":179725,"unread":true,"content":"<article>arXiv:2503.10345v4 Announce Type: replace \nAbstract: Online conformal prediction enables the runtime calibration of a pre-trained artificial intelligence model using feedback on its performance. Calibration is achieved through set predictions that are updated via online rules so as to ensure long-term coverage guarantees. While recent research has demonstrated the benefits of incorporating prior knowledge into the calibration process, this has come at the cost of replacing coverage guarantees with less tangible regret guarantees based on the quantile loss. This work introduces intermittent mirror online conformal prediction (IM-OCP), a novel runtime calibration framework that integrates prior knowledge, operates under potentially intermittent feedback, and features minimal memory complexity. IM-OCP guarantees long-term coverage and sub-linear regret, both of which hold deterministically for any given data sequence and in expectation with respect to the intermittent feedback.</article>","contentLength":989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TabNSA: Native Sparse Attention for Efficient Tabular Data Learning","url":"https://arxiv.org/abs/2503.09850","date":1751428800,"author":"","guid":179726,"unread":true,"content":"<article>arXiv:2503.09850v2 Announce Type: replace \nAbstract: Tabular data poses unique challenges for deep learning due to its heterogeneous feature types, lack of spatial structure, and often limited sample sizes. We propose TabNSA, a novel deep learning framework that integrates Native Sparse Attention (NSA) with a TabMixer backbone to efficiently model tabular data. TabNSA tackles computational and representational challenges by dynamically focusing on relevant feature subsets per instance. The NSA module employs a hierarchical sparse attention mechanism, including token compression, selective preservation, and localized sliding windows, to significantly reduce the quadratic complexity of standard attention operations while addressing feature heterogeneity. Complementing this, the TabMixer backbone captures complex, non-linear dependencies through parallel multilayer perceptron (MLP) branches with independent parameters. These modules are synergistically combined via element-wise summation and mean pooling, enabling TabNSA to model both global context and fine-grained interactions. Extensive experiments across supervised and transfer learning settings show that TabNSA consistently outperforms state-of-the-art deep learning models. Furthermore, by augmenting TabNSA with a fine-tuned large language model (LLM), we enable it to effectively address Few-Shot Learning challenges through language-guided generalization on diverse tabular benchmarks.</article>","contentLength":1460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks","url":"https://arxiv.org/abs/2503.08904","date":1751428800,"author":"","guid":179727,"unread":true,"content":"<article>arXiv:2503.08904v2 Announce Type: replace \nAbstract: The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.</article>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DG16M: A Large-Scale Dataset for Dual-Arm Grasping with Force-Optimized Grasps","url":"https://arxiv.org/abs/2503.08358","date":1751428800,"author":"","guid":179728,"unread":true,"content":"<article>arXiv:2503.08358v2 Announce Type: replace \nAbstract: Dual-arm robotic grasping is crucial for handling large objects that require stable and coordinated manipulation. While single-arm grasping has been extensively studied, datasets tailored for dual-arm settings remain scarce. We introduce a large-scale dataset of 16 million dual-arm grasps, evaluated under improved force-closure constraints. Additionally, we develop a benchmark dataset containing 300 objects with approximately 30,000 grasps, evaluated in a physics simulation environment, providing a better grasp quality assessment for dual-arm grasp synthesis methods. Finally, we demonstrate the effectiveness of our dataset by training a Dual-Arm Grasp Classifier network that outperforms the state-of-the-art methods by 15\\%, achieving higher grasp success rates and improved generalization across objects.</article>","contentLength":867,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LangTime: A Language-Guided Unified Model for Time Series Forecasting with Proximal Policy Optimization","url":"https://arxiv.org/abs/2503.08271","date":1751428800,"author":"","guid":179729,"unread":true,"content":"<article>arXiv:2503.08271v2 Announce Type: replace \nAbstract: Recent research has shown an increasing interest in utilizing pre-trained large language models (LLMs) for a variety of time series applications. However, there are three main challenges when using LLMs as foundational models for time series forecasting: (1) Cross-domain generalization. (2) Cross-modality alignment. (3) Error accumulation in autoregressive frameworks. To address these challenges, we proposed LangTime, a language-guided unified model for time series forecasting that incorporates cross-domain pre-training with reinforcement learning-based fine-tuning. Specifically, LangTime constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise and channel-wise instructions, to facilitate domain adaptation and condense time series into a single token, enabling LLMs to understand better and align temporal data. To improve autoregressive forecasting, we introduce TimePPO, a reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error accumulation by leveraging a multidimensional rewards function tailored for time series and a repeat-based value estimation strategy. Extensive experiments demonstrate that LangTime achieves state-of-the-art cross-domain forecasting performance, while TimePPO fine-tuning effectively enhances the stability and accuracy of autoregressive forecasting.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mitigating Hallucinations in YOLO-based Object Detection Models: A Revisit to Out-of-Distribution Detection","url":"https://arxiv.org/abs/2503.07330","date":1751428800,"author":"","guid":179730,"unread":true,"content":"<article>arXiv:2503.07330v2 Announce Type: replace \nAbstract: Object detection systems must reliably perceive objects of interest without being overly confident to ensure safe decision-making in dynamic environments. Filtering techniques based on out-of-distribution (OoD) detection are commonly added as an extra safeguard to filter hallucinations caused by overconfidence in novel objects. Nevertheless, evaluating YOLO-family detectors and their filters under existing OoD benchmarks often leads to unsatisfactory performance. This paper studies the underlying reasons for performance bottlenecks and proposes a methodology to improve performance fundamentally. Our first contribution is a calibration of all existing evaluation results: Although images in existing OoD benchmark datasets are claimed not to have objects within in-distribution (ID) classes (i.e., categories defined in the training dataset), around 13% of objects detected by the object detector are actually ID objects. Dually, the ID dataset containing OoD objects can also negatively impact the decision boundary of filters. These ultimately lead to a significantly imprecise performance estimation. Our second contribution is to consider the task of hallucination reduction as a joint pipeline of detectors and filters. By developing a methodology to carefully synthesize an OoD dataset that semantically resembles the objects to be detected, and using the crafted OoD dataset in the fine-tuning of YOLO detectors to suppress the objectness score, we achieve a 88% reduction in overall hallucination error with a combined fine-tuned detection and filtering system on the self-driving benchmark BDD-100K. Our code and dataset are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.</article>","contentLength":1771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Good Start Matters: Enhancing Continual Learning with Data-Driven Weight Initialization","url":"https://arxiv.org/abs/2503.06385","date":1751428800,"author":"","guid":179731,"unread":true,"content":"<article>arXiv:2503.06385v2 Announce Type: replace \nAbstract: To adapt to real-world data streams, continual learning (CL) systems must rapidly learn new concepts while preserving and utilizing prior knowledge. When it comes to adding new information to continually-trained deep neural networks (DNNs), classifier weights for newly encountered categories are typically initialized randomly, leading to high initial training loss (spikes) and instability. Consequently, achieving optimal convergence and accuracy requires prolonged training, increasing computational costs. Inspired by Neural Collapse (NC), we propose a weight initialization strategy to improve learning efficiency in CL. In DNNs trained with mean-squared-error, NC gives rise to a Least-Square (LS) classifier in the last layer, whose weights can be analytically derived from learned features. We leverage this LS formulation to initialize classifier weights in a data-driven manner, aligning them with the feature distribution rather than using random initialization. Our method mitigates initial loss spikes and accelerates adaptation to new tasks. We evaluate our approach in large-scale CL settings, demonstrating faster adaptation and improved CL performance.</article>","contentLength":1223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Autonomous Robotic Bone Micro-Milling System with Automatic Calibration and 3D Surface Fitting","url":"https://arxiv.org/abs/2503.04038","date":1751428800,"author":"","guid":179732,"unread":true,"content":"<article>arXiv:2503.04038v2 Announce Type: replace \nAbstract: Automating bone micro-milling using a robotic system presents challenges due to the uncertainties in both the external and internal features of bone tissue. For example, during mouse cranial window creation, a circular path with a radius of 2 to 4 mm needs to be milled on the mouse skull using a microdrill. The uneven surface and non-uniform thickness of the mouse skull make it difficult to fully automate this process, requiring the system to possess advanced perceptual and adaptive capabilities. In this study, we address this challenge by integrating a Microscopic Stereo Camera System (MSCS) into the robotic bone micro-milling system and proposing a novel pre-measurement pipeline for the target surface. Starting from uncalibrated cameras, the pipeline enables automatic calibration and 3D surface fitting through a convolutional neural network (CNN)-based keypoint detection. Combined with the existing feedback-based system, we develop the world's first autonomous robotic bone micro-milling system capable of rapidly, in real-time, and accurately perceiving and adapting to surface unevenness and non-uniform thickness, thereby enabling an end-to-end autonomous cranial window creation workflow without human assistance. Validation experiments on euthanized mice demonstrate that the improved system achieves a success rate of 85.7% and an average milling time of 2.1 minutes, showing not only significant performance improvements over the previous system but also exceptional accuracy, speed, and stability compared to human operators.</article>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation","url":"https://arxiv.org/abs/2503.03040","date":1751428800,"author":"","guid":179733,"unread":true,"content":"<article>arXiv:2503.03040v2 Announce Type: replace \nAbstract: Recent advances in large language models have demonstrated impressive capabilities in task-oriented applications, yet building emotionally intelligent chatbots that can engage in natural, strategic conversations remains a challenge. We present a novel approach called SAGE that uses latent variables to control long-horizon behavior in dialogue generation. At the core of our method is the State-Action Chain (SAC), which augments standard language model fine-tuning by introducing latent variables that encapsulate emotional states and conversational strategies between dialogue turns. During inference, these variables are generated before each response, enabling coarse-grained control over dialogue progression while maintaining natural interaction patterns. We also introduce a self-improvement pipeline that leverages dialogue tree search, LLM-based reward modeling, and targeted fine-tuning to optimize conversational trajectories. Our experimental results show that models trained with this approach demonstrate improved performance in emotional intelligence metrics while maintaining strong capabilities on LLM benchmarks. The discrete nature of our latent variables facilitates search-based strategies and provides a foundation for future applications of reinforcement learning to dialogue systems, where learning can occur at the state level rather than the token level. https://github.com/apple/ml-sage-dialog-gen</article>","contentLength":1478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Intrinsic Normal Prototypes within a Single Image for Universal Anomaly Detection","url":"https://arxiv.org/abs/2503.02424","date":1751428800,"author":"","guid":179734,"unread":true,"content":"<article>arXiv:2503.02424v2 Announce Type: replace \nAbstract: Anomaly detection (AD) is essential for industrial inspection, yet existing methods typically rely on ``comparing'' test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is useful and may be more aligned with the anomalies since both the anomalies and the normal information originate from the same image. Therefore, rather than relying on external normality from the training set, we propose INP-Former, a novel method that extracts Intrinsic Normal Prototypes (INPs) directly from the test image. Specifically, we introduce the INP Extractor, which linearly combines normal tokens to represent INPs. We further propose an INP Coherence Loss to ensure INPs can faithfully represent normality for the testing image. These INPs then guide the INP-Guided Decoder to reconstruct only normal tokens, with reconstruction errors serving as anomaly scores. Additionally, we propose a Soft Mining Loss to prioritize hard-to-optimize samples during training. INP-Former achieves state-of-the-art performance in single-class, multi-class, and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a versatile and universal solution for AD. Remarkably, INP-Former also demonstrates some zero-shot AD capability. Code is available at:https://github.com/luow23/INP-Former.</article>","contentLength":1654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enabling mixed-precision in spectral element codes","url":"https://arxiv.org/abs/2503.02134","date":1751428800,"author":"","guid":179735,"unread":true,"content":"<article>arXiv:2503.02134v2 Announce Type: replace \nAbstract: Mixed-precision computing has the potential to significantly reduce the cost of exascale computations, but determining when and how to implement it in programs can be challenging. In this article, we propose a methodology for enabling mixed-precision with the help of computer arithmetic tools, roofline model, and computer arithmetic techniques. As case studies, we consider Nekbone, a mini-application for the Computational Fluid Dynamics (CFD) solver Nek5000, and a modern Neko CFD application. With the help of the Verificarlo tool and computer arithmetic techniques, we introduce a strategy to address stagnation issues in the preconditioned Conjugate Gradient method in Nekbone and apply these insights to implement a mixed-precision version of Neko. We evaluate the derived mixed-precision versions of these codes by combining metrics in three dimensions: accuracy, time-to-solution, and energy-to-solution. Notably, mixed-precision in Nekbone reduces time-to-solution by roughly 1.62x and energy-to-solution by 2.43x on MareNostrum 5, while in the real-world Neko application, the gain is up to 1.3x in both time and energy, with the accuracy that matches double-precision results.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unbent Collections of Orthogonal Drawings","url":"https://arxiv.org/abs/2502.18390","date":1751428800,"author":"","guid":179736,"unread":true,"content":"<article>arXiv:2502.18390v2 Announce Type: replace \nAbstract: Recently, there has been interest in representing single graphs by multiple drawings; for example, using graph stories, storyplans, or uncrossed collections. In this paper, we apply this idea to orthogonal graph drawing. Due to the orthogonal drawing style, we focus on 4-graphs, that is, graphs of maximum degree 4. We restrict ourselves to plane graphs, that is, planar graphs whose embedding is fixed. Our goal is to represent any plane 4-graph $G$ by an unbent collection, that is, a collection of orthogonal drawings of $G$ that adhere to the embedding of $G$ and ensure that each edge of $G$ is drawn without bends in at least one of the drawings. We investigate two objectives. First, we consider minimizing the number of drawings in an unbent collection. We prove that every plane 4-graph can be represented by a collection with at most three drawings, which is tight. We also give necessary and sufficient conditions for a graph to admit an unbent collection of size $2$. Second, we consider minimizing the total number of bends over all drawings in an unbent collection. We show that this problem is NP-hard and give a 3-approximation algorithm. For the special case of plane triconnected cubic graphs, we show how to compute minimum-bend collections in linear time.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Bayesian Perspective for Conventional and Robust Adaptive Filters","url":"https://arxiv.org/abs/2502.18325","date":1751428800,"author":"","guid":179737,"unread":true,"content":"<article>arXiv:2502.18325v2 Announce Type: replace \nAbstract: In this work, we present a new perspective on the origin and interpretation of adaptive filters. By applying Bayesian principles of recursive inference from the state-space model and using a series of simplifications regarding the structure of the solution, we can present, in a unified framework, derivations of many adaptive filters that depend on the probabilistic model of the measurement noise. In particular, under a Gaussian model, we obtain solutions well-known in the literature (such as LMS, NLMS, or Kalman filter), while using non-Gaussian noise, we derive new adaptive algorithms. Notably, under the assumption of Laplacian noise, we obtain a family of robust filters of which the sign-error algorithm is a well-known member, while other algorithms, derived effortlessly in the proposed framework, are entirely new. Numerical examples are shown to illustrate the properties and provide a better insight into the performance of the derived adaptive filters.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Diagnostic Performance: Revealing and Quantifying Ethical Risks in Pathology Foundation Models","url":"https://arxiv.org/abs/2502.16889","date":1751428800,"author":"","guid":179738,"unread":true,"content":"<article>arXiv:2502.16889v2 Announce Type: replace \nAbstract: Pathology foundation models (PFMs), as large-scale pre-trained models tailored for computational pathology, have significantly advanced a wide range of applications. Their ability to leverage prior knowledge from massive datasets has streamlined the development of intelligent pathology models. However, we identify several critical and interrelated ethical risks that remain underexplored, yet must be addressed to enable the safe translation of PFMs from lab to clinic. These include the potential leakage of patient-sensitive attributes, disparities in model performance across demographic and institutional subgroups, and the reliance on diagnosis-irrelevant features that undermine clinical reliability. In this study, we pioneer the quantitative analysis for ethical risks in PFMs, including privacy leakage, clinical reliability, and group fairness. Specifically, we propose an evaluation framework that systematically measures key dimensions of ethical concern: the degree to which patient-sensitive attributes can be inferred from model representations, the extent of performance disparities across demographic and institutional subgroups, and the influence of diagnostically irrelevant features on model decisions. We further investigate the underlying causes of these ethical risks in PFMs and empirically validate our findings. Then we offer insights into potential directions for mitigating such risks, aiming to inform the development of more ethically robust PFMs. This work provides the first quantitative and systematic evaluation of ethical risks in PFMs. Our findings highlight the urgent need for ethical safeguards in PFMs and offer actionable insights for building more trustworthy and clinically robust PFMs. To facilitate future research and deployment, we will release the assessment framework as an online toolkit to support the development, auditing, and deployment of ethically robust PFMs.</article>","contentLength":1971,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SegAnyPET: Universal Promptable Segmentation from Positron Emission Tomography Images","url":"https://arxiv.org/abs/2502.14351","date":1751428800,"author":"","guid":179739,"unread":true,"content":"<article>arXiv:2502.14351v3 Announce Type: replace \nAbstract: Positron Emission Tomography (PET) is a powerful molecular imaging tool that plays a crucial role in modern medical diagnostics by visualizing radio-tracer distribution to reveal physiological processes. Accurate organ segmentation from PET images is essential for comprehensive multi-systemic analysis of interactions between different organs and pathologies. Existing segmentation methods are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical application. Recent developments in segmentation foundation models have shown superior versatility across diverse segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit limited generalization performance on molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data for promptable segmentation. Experimental results demonstrate that SegAnyPET can segment seen and unseen target organs using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation.</article>","contentLength":1891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration","url":"https://arxiv.org/abs/2502.14156","date":1751428800,"author":"","guid":179740,"unread":true,"content":"<article>arXiv:2502.14156v2 Announce Type: replace \nAbstract: Vehicle-to-everything (V2X) collaborative perception has emerged as a promising solution to address the limitations of single-vehicle perception systems. However, existing V2X datasets are limited in scope, diversity, and quality. To address these gaps, we present Mixed Signals, a comprehensive V2X dataset featuring 45.1k point clouds and 240.6k bounding boxes collected from three connected autonomous vehicles (CAVs) equipped with two different configurations of LiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides point clouds and bounding box annotations across 10 classes, ensuring reliable data for perception training. We provide detailed statistical analysis on the quality of our dataset and extensively benchmark existing V2X methods on it. The Mixed Signals dataset is ready-to-use, with precise alignment and consistent annotations across time and viewpoints. Dataset website is available at https://mixedsignalsdataset.cs.cornell.edu/.</article>","contentLength":1026,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression","url":"https://arxiv.org/abs/2502.14051","date":1751428800,"author":"","guid":179741,"unread":true,"content":"<article>arXiv:2502.14051v2 Announce Type: replace \nAbstract: Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Assessing Correctness in LLM-Based Code Generation via Uncertainty Estimation","url":"https://arxiv.org/abs/2502.11620","date":1751428800,"author":"","guid":179742,"unread":true,"content":"<article>arXiv:2502.11620v3 Announce Type: replace \nAbstract: In this work, we explore uncertainty estimation as a proxy for correctness in LLM-generated code. To this end, we adapt two state-of-the-art techniques from natural language generation -- one based on entropy and another on mutual information -- to the domain of code generation. Given the distinct semantic properties of code, we introduce modifications, including a semantic equivalence check based on symbolic execution. Our findings indicate a strong correlation between the uncertainty computed through these techniques and correctness, highlighting the potential of uncertainty estimation for quality assessment. Additionally, we propose a simplified version of the entropy-based method that assumes a uniform distribution over the LLM's responses, demonstrating comparable effectiveness. Using these techniques, we develop an abstention policy that prevents the model from making predictions when uncertainty is high, reducing incorrect outputs to near zero. Our evaluation on the LiveCodeBench shows that our approach significantly outperforms a baseline relying solely on LLM-reported log-probabilities.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PRVQL: Progressive Knowledge-guided Refinement for Robust Egocentric Visual Query Localization","url":"https://arxiv.org/abs/2502.07707","date":1751428800,"author":"","guid":179743,"unread":true,"content":"<article>arXiv:2502.07707v2 Announce Type: replace \nAbstract: Egocentric visual query localization (EgoVQL) focuses on localizing the target of interest in space and time from first-person videos, given a visual query. Despite recent progressive, existing methods often struggle to handle severe object appearance changes and cluttering background in the video due to lacking sufficient target cues, leading to degradation. Addressing this, we introduce PRVQL, a novel Progressive knowledge-guided Refinement framework for EgoVQL. The core is to continuously exploit target-relevant knowledge directly from videos and utilize it as guidance to refine both query and video features for improving target localization. Our PRVQL contains multiple processing stages. The target knowledge from one stage, comprising appearance and spatial knowledge extracted via two specially designed knowledge learning modules, are utilized as guidance to refine the query and videos features for the next stage, which are used to generate more accurate knowledge for further feature refinement. With such a progressive process, target knowledge in PRVQL can be gradually improved, which, in turn, leads to better refined query and video features for localization in the final stage. Compared to previous methods, our PRVQL, besides the given object cues, enjoys additional crucial target information from a video as guidance to refine features, and hence enhances EgoVQL in complicated scenes. In our experiments on challenging Ego4D, PRVQL achieves state-of-the-art result and largely surpasses other methods, showing its efficacy. Our code, model and results will be released at https://github.com/fb-reps/PRVQL.</article>","contentLength":1687,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Curse of Depth in Large Language Models","url":"https://arxiv.org/abs/2502.05795","date":1751428800,"author":"","guid":179744,"unread":true,"content":"<article>arXiv:2502.05795v2 Announce Type: replace \nAbstract: In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling (LNS), which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Across a wide range of model sizes (130M to 7B), our experiments show that LNS consistently outperforms previous normalization and scaling techniques in enhancing LLM pre-training performance. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at \\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}.</article>","contentLength":1684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building Rome with Convex Optimization","url":"https://arxiv.org/abs/2502.04640","date":1751428800,"author":"","guid":179745,"unread":true,"content":"<article>arXiv:2502.04640v4 Announce Type: replace \nAbstract: Global bundle adjustment is made easy by depth prediction and convex optimization. We (i) propose a scaled bundle adjustment (SBA) formulation that lifts 2D keypoint measurements to 3D with learned depth, (ii) design an empirically tight convex semidfinite program (SDP) relaxation that solves SBA to certfiable global optimality, (iii) solve the SDP relaxations at extreme scale with Burer-Monteiro factorization and a CUDA-based trust-region Riemannian optimizer (dubbed XM), (iv) build a structure from motion (SfM) pipeline with XM as the optimization engine and show that XM-SfM compares favorably with existing pipelines in terms of reconstruction quality while being significantly faster, more scalable, and initialization-free.</article>","contentLength":788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Energy dissipation law and maximum bound principle-preserving linear BDF2 schemes with variable steps for the Allen-Cahn equation","url":"https://arxiv.org/abs/2502.04616","date":1751428800,"author":"","guid":179746,"unread":true,"content":"<article>arXiv:2502.04616v2 Announce Type: replace \nAbstract: In this paper, we propose and analyze a linear, structure-preserving scalar auxiliary variable (SAV) method for solving the Allen--Cahn equation based on the second-order backward differentiation formula (BDF2) with variable time steps. To this end, we first design a novel and essential auxiliary functional that serves twofold functions: (i) ensuring that a first-order approximation to the auxiliary variable, which is essentially important for deriving the unconditional energy dissipation law, does not affect the second-order temporal accuracy of the phase function $\\phi$; and (ii) allowing us to develop effective stabilization terms that are helpful to establish the MBP-preserving linear methods. Together with this novel functional and standard central difference stencil, we then propose a linear, second-order variable-step BDF2 type stabilized exponential SAV scheme, namely BDF2-sESAV-I, which is shown to preserve both the discrete modified energy dissipation law under the temporal stepsize ratio $ 0 &lt; r_{k} := \\tau_{k}/\\tau_{k-1} &lt; 4.864 - \\delta $ with a positive constant $\\delta$ and the MBP under $ 0 &lt; r_{k} &lt; 1 + \\sqrt{2} $. Moreover, an analysis of the approximation to the original energy by the modified one is presented. With the help of the kernel recombination technique, optimal $ H^{1}$- and $ L^{\\infty}$-norm error estimates of the variable-step BDF2-sESAV-I scheme are rigorously established. Numerical examples are carried out to verify the theoretical results and demonstrate the effectiveness and efficiency of the proposed scheme.</article>","contentLength":1623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms","url":"https://arxiv.org/abs/2502.04388","date":1751428800,"author":"","guid":179747,"unread":true,"content":"<article>arXiv:2502.04388v3 Announce Type: replace \nAbstract: Artificial Intelligence (AI) agents capable of autonomous learning and independent decision-making hold great promise for addressing complex challenges across various critical infrastructure domains, including transportation, energy systems, and manufacturing. However, the surge in the design and deployment of AI systems, driven by various stakeholders with distinct and unaligned objectives, introduces a crucial challenge: How can uncoordinated AI systems coexist and evolve harmoniously in shared environments without creating chaos or compromising safety? To address this, we advocate for a fundamental rethinking of existing multi-agent frameworks, such as multi-agent systems and game theory, which are largely limited to predefined rules and static objective structures. We posit that AI agents should be empowered to adjust their objectives dynamically, make compromises, form coalitions, and safely compete or cooperate through evolving relationships and social feedback. Through two case studies in critical infrastructure applications, we call for a shift toward the emergent, self-organizing, and context-aware nature of these multi-agentic AI systems.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering","url":"https://arxiv.org/abs/2502.03628","date":1751428800,"author":"","guid":179748,"unread":true,"content":"<article>arXiv:2502.03628v2 Announce Type: replace \nAbstract: Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits ranking throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss - visually grounded tokens gradually become less favored throughout generation, and (2) early excitation - semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information - visually grounded tokens though not being eventually decoded still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by about 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies. Code is available at https://github.com/LzVv123456/VISTA.</article>","contentLength":1669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds","url":"https://arxiv.org/abs/2502.02869","date":1751428800,"author":"","guid":179749,"unread":true,"content":"<article>arXiv:2502.02869v2 Announce Type: replace \nAbstract: In-Context Reinforcement Learning (ICRL) enables agents to learn automatically and on-the-fly from their interactive experiences. However, a major challenge in scaling up ICRL is the lack of scalable task collections. To address this, we propose the procedurally generated tabular Markov Decision Processes, named AnyMDP. Through a carefully designed randomization process, AnyMDP is capable of generating high-quality tasks on a large scale while maintaining relatively low structural biases. To facilitate efficient meta-training at scale, we further introduce step-wise supervision and induce prior information in the ICRL framework.Our results demonstrate that, with a sufficiently large scale of AnyMDP tasks, the proposed model can generalize to tasks that were not considered in the training set. The scalable task set provided by AnyMDP also enables a more thorough empirical investigation of the relationship between data distribution and ICRL performance. We further show that the generalization of ICRL potentially comes at the cost of increased task diversity and longer adaptation periods. This finding carries critical implications for scaling robust ICRL capabilities, highlighting the necessity of diverse and extensive task design, and prioritizing asymptotic performance over few-shot adaptation.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Instruct-4DGS: Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation","url":"https://arxiv.org/abs/2502.02091","date":1751428800,"author":"","guid":179750,"unread":true,"content":"<article>arXiv:2502.02091v3 Announce Type: replace \nAbstract: Recent 4D dynamic scene editing methods require editing thousands of 2D images used for dynamic scene synthesis and updating the entire scene with additional training loops, resulting in several hours of processing to edit a single dynamic scene. Therefore, these methods are not scalable with respect to the temporal dimension of the dynamic scene (i.e., the number of timesteps). In this work, we propose Instruct-4DGS, an efficient dynamic scene editing method that is more scalable in terms of temporal dimension. To achieve computational efficiency, we leverage a 4D Gaussian representation that models a 4D dynamic scene by combining static 3D Gaussians with a Hexplane-based deformation field, which captures dynamic information. We then perform editing solely on the static 3D Gaussians, which is the minimal but sufficient component required for visual editing. To resolve the misalignment between the edited 3D Gaussians and the deformation field, which may arise from the editing process, we introduce a refinement stage using a score distillation mechanism. Extensive editing results demonstrate that Instruct-4DGS is efficient, reducing editing time by more than half compared to existing methods while achieving high-quality edits that better follow user instructions. Code and results: https://hanbyelcho.info/instruct-4dgs/</article>","contentLength":1392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty Quantification of Wind Gust Predictions in the Northeast United States: An Evidential Neural Network and Explainable Artificial Intelligence Approach","url":"https://arxiv.org/abs/2502.00300","date":1751428800,"author":"","guid":179751,"unread":true,"content":"<article>arXiv:2502.00300v2 Announce Type: replace \nAbstract: Machine learning algorithms have shown promise in reducing bias in wind gust predictions, while still underpredicting high gusts. Uncertainty quantification (UQ) supports this issue by identifying when predictions are reliable or need cautious interpretation. Using data from 61 extratropical storms in the Northeastern USA, we introduce evidential neural network (ENN) as a novel approach for UQ in gust predictions, leveraging atmospheric variables from the Weather Research and Forecasting (WRF) model. Explainable AI techniques suggested that key predictive features contributed to higher uncertainty, which correlated strongly with storm intensity and spatial gust gradients. Compared to WRF, ENN demonstrated a 47% reduction in RMSE and allowed the construction of gust prediction intervals without an ensemble, successfully capturing at least 95% of observed gusts at 179 out of 266 stations. From an operational perspective, providing gust forecasts with quantified uncertainty enhances stakeholders' confidence in risk assessment and response planning for extreme gust events.</article>","contentLength":1138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Source-Channel Separation Theorems for Distortion Perception Coding","url":"https://arxiv.org/abs/2501.17706","date":1751428800,"author":"","guid":179752,"unread":true,"content":"<article>arXiv:2501.17706v2 Announce Type: replace \nAbstract: It is well known that separation between lossy source coding and channel coding is asymptotically optimal under classical additive distortion measures. Recently, coding under a new class of quality considerations, often referred to as perception or realism, has attracted significant attention due to its close connection to neural generative models and semantic communications. In this work, we revisit source-channel separation under the consideration of distortion-perception. We show that when the perception quality is measured on the block level, i.e., in the strong-sense, the optimality of separation still holds when common randomness is shared between the encoder and the decoder; however, separation is no longer optimal when such common randomness is not available. In contrast, when the perception quality is the average per-symbol measure, i.e., in the weak-sense, the optimality of separation holds regardless of the availability of common randomness.</article>","contentLength":1019,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ECLYPSE: a Python Framework for Simulation and Emulation of the Cloud-Edge Continuum","url":"https://arxiv.org/abs/2501.17126","date":1751428800,"author":"","guid":179753,"unread":true,"content":"<article>arXiv:2501.17126v2 Announce Type: replace \nAbstract: The Cloud-Edge continuum enhances application performance by bringing computation closer to data sources. However, it presents considerable challenges in managing resources and determining service placement, as these tasks require navigating diverse, dynamic environments characterised by fluctuating network conditions. Addressing these challenges calls for tools combining simulation and emulation of Cloud-Edge systems to rigorously assess novel application and resource management strategies. In this paper, we introduce ECLYPSE, a Python-based framework that enables the simulation and emulation of the Cloud-Edge continuum via adaptable resource allocation and service placement models. ECLYPSE features an event-driven architecture for dynamically adapting network configurations and resources. It also supports seamless transitions between simulated and emulated setups. In this work, ECLYPSE capabilities are illustrated over three use cases, showing how the framework supports rapid prototyping across diverse experimental settings.</article>","contentLength":1095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An artificial viscosity approach to high order entropy stable discontinuous Galerkin methods","url":"https://arxiv.org/abs/2501.16529","date":1751428800,"author":"","guid":179754,"unread":true,"content":"<article>arXiv:2501.16529v2 Announce Type: replace \nAbstract: Entropy stable discontinuous Galerkin (DG) methods improve the robustness of high order DG simulations of nonlinear conservation laws. These methods yield a semi-discrete entropy inequality, and rely on an algebraic flux differencing formulation which involves both summation-by-parts (SBP) discretization matrices and entropy conservative two-point finite volume fluxes. However, explicit expressions for such two-point finite volume fluxes may not be available for all systems, or may be computationally expensive to compute.\n  This paper proposes an alternative approach to constructing entropy stable DG methods using an entropy correction artificial viscosity, where the artificial viscosity coefficient is determined based on the local violation of a cell entropy inequality and the local entropy dissipation. The resulting method \\bnote{is a modification of the entropy correction introduced by Abgrall, Offner, and Ranocha (2022) in \"Reinterpretation and Extension of Entropy Correction Terms for Residual Distribution and Discontinuous Galerkin Schemes: Application to Structure Preserving Discretization\", and recovers the same global semi-discrete entropy inequality that is satisfied by entropy stable flux differencing DG methods. The entropy correction artificial viscosity coefficients are parameter-free and locally computable over each cell, and the resulting artificial viscosity preserves both high order accuracy and a hyperbolic maximum stable time-step size under explicit time-stepping.</article>","contentLength":1562,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A novel Trunk Branch-net PINN for flow and heat transfer prediction in porous medium","url":"https://arxiv.org/abs/2501.16362","date":1751428800,"author":"","guid":179755,"unread":true,"content":"<article>arXiv:2501.16362v2 Announce Type: replace \nAbstract: A novel Trunk-Branch (TB)-net physics-informed neural network (PINN) architecture is developed, which is a PINN-based method incorporating trunk and branch nets to capture both global and local features. The aim is to solve four main classes of problems: forward flow problem, forward heat transfer problem, inverse heat transfer problem, and transfer learning problem within the porous medium, which are notoriously complex that could not be handled by origin PINN. In the proposed TB-net PINN architecture, a Fully-connected Neural Network (FNN) is used as the trunk net, followed by separated FNNs as the branch nets with respect to outputs, and automatic differentiation is performed for partial derivatives of outputs with respect to inputs by considering various physical loss. The effectiveness and flexibility of the novel TB-net PINN architecture is demonstrated through a collection of forward problems, and transfer learning validates the feasibility of resource reuse. Combining with the superiority over traditional numerical methods in solving inverse problems, the proposed TB-net PINN shows its great potential for practical engineering applications.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HERITRACE: A User-Friendly Semantic Data Editor with Change Tracking and Provenance Management for Cultural Heritage Institutions","url":"https://arxiv.org/abs/2501.16197","date":1751428800,"author":"","guid":179756,"unread":true,"content":"<article>arXiv:2501.16197v2 Announce Type: replace \nAbstract: HERITRACE is a data editor designed for galleries, libraries, archives and museums, aimed at simplifying data curation while enabling non-technical domain experts to manage data intuitively without losing its semantic integrity. While the semantic nature of RDF can pose a barrier to data curation due to its complexity, HERITRACE conceals this intricacy while preserving the advantages of semantic representation. The system natively supports provenance management and change tracking, ensuring transparency and accountability throughout the curation process. Although HERITRACE functions effectively out of the box, it offers a straightforward customization interface for technical staff, enabling adaptation to the specific data model required by a given collection. Current applications include the ParaText project, and its adoption is already planned for OpenCitations. Future developments will focus on integrating the RDF Mapping Language (RML) to enhance compatibility with non-RDF data formats, further expanding its applicability in digital heritage management.</article>","contentLength":1125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Benchmarking Platform for DDR4 Memory Performance in Data-Center-Class FPGAs","url":"https://arxiv.org/abs/2501.15582","date":1751428800,"author":"","guid":179757,"unread":true,"content":"<article>arXiv:2501.15582v2 Announce Type: replace \nAbstract: FPGAs are increasingly utilized in data centers due to their capacity to exploit data parallelism in computationally intensive workloads. Furthermore, the processing of modern data center workloads requires moving vast amounts of data, making it essential to optimize data exchange between FPGAs and memory. This paper introduces a novel benchmarking platform for the evaluation of DDR4 memory performance in data-center-class FPGAs. The proposed solution features highly configurable traffic generation with complex memory access patterns defined at run time and can be flexibly instantiated on the target FPGA to support multiple memory channels and varying data rates. An extensive experimental campaign, targeting the AMD Kintex UltraScale 115 FPGA and encompassing up to three memory channels with data rates ranging from 1600 to 2400 MT/s and various memory traffic configurations, demonstrates the benchmarking platform's capability to effectively evaluate DDR4 memory performance.</article>","contentLength":1041,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Representation Consistency Model via Contrastive Denoising","url":"https://arxiv.org/abs/2501.13094","date":1751428800,"author":"","guid":179758,"unread":true,"content":"<article>arXiv:2501.13094v2 Announce Type: replace \nAbstract: Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3% on average, with up to 11.6% at larger radii, while reducing inference costs by 85$\\times$ on average. Codes are available at: https://github.com/jiachenlei/rRCM.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the convergence of two-step modified Newton method for nonsymmetric algebraic Riccati equations from transport theory","url":"https://arxiv.org/abs/2501.11922","date":1751428800,"author":"","guid":179759,"unread":true,"content":"<article>arXiv:2501.11922v2 Announce Type: replace \nAbstract: This paper is concerned with the convergence of a two-step modified Newton method for solving the nonlinear system arising from the minimal nonnegative solution of nonsymmetric algebraic Riccati equations from neutron transport theory. We show the monotonic convergence of the two-step modified Newton method under mild assumptions. When the Jacobian of the nonlinear operator at the minimal positive solution is singular, we present a convergence analysis of the two-step modified Newton method in this context. Numerical experiments are conducted to demonstrate that the proposed method yields comparable results to several existing Newton-type methods and that it brings a significant reduction in computation time for nearly singular and large-scale problems.</article>","contentLength":816,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semi-supervised Semantic Segmentation for Remote Sensing Images via Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention","url":"https://arxiv.org/abs/2501.10736","date":1751428800,"author":"","guid":179760,"unread":true,"content":"<article>arXiv:2501.10736v3 Announce Type: replace \nAbstract: Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks.</article>","contentLength":1512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Study of In-Context-Learning-Based Text-to-SQL Errors","url":"https://arxiv.org/abs/2501.09310","date":1751428800,"author":"","guid":179761,"unread":true,"content":"<article>arXiv:2501.09310v2 Announce Type: replace \nAbstract: Large language models (LLMs) have been adopted to perform text-to-SQL tasks, utilizing their in-context learning (ICL) capability to translate natural language questions into structured query language (SQL). However, such a technique faces correctness problems and requires efficient repairing solutions. In this paper, we conduct the first comprehensive study of text-to-SQL errors. Our study covers four representative ICL-based techniques, five basic repairing methods, two benchmarks, and two LLM settings. We find that text-to-SQL errors are widespread and summarize 29 error types of 7 categories. We also find that existing repairing attempts have limited correctness improvement at the cost of high computational overhead with many mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL error detection and repairing framework. The evaluation demonstrates that MapleRepair outperforms existing solutions by repairing 13.8% more queries with neglectable mis-repairs and 67.4% less overhead.</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UFGraphFR: Graph Federation Recommendation System based on User Text description features","url":"https://arxiv.org/abs/2501.08044","date":1751428800,"author":"","guid":179762,"unread":true,"content":"<article>arXiv:2501.08044v3 Announce Type: replace \nAbstract: Federated learning has emerged as a key paradigm in privacy-preserving computing due to its \"data usable but not visible\" property, enabling users to collaboratively train models without sharing raw data. Motivated by this, federated recommendation systems offer a promising architecture that balances user privacy with recommendation accuracy through distributed collaborative learning. However, existing federated recommendation methods often neglect the underlying semantic or behavioral relationships between users during parameter aggregation, which limits their recommendation effectiveness. To overcome this limitation, graph-based federated recommendation systems have been proposed to leverage neighborhood information. Yet, conventional graph construction methods usually require access to raw user data or explicit social links, which contradicts the strict privacy requirements of federated learning. In this work, we propose UFGraphFR (User Text-feature-based Graph Federated Recommendation), a novel personalized federated recommendation framework that constructs a user graph based on clients' locally embedded text features. Our core assumption is that users with similar textual feature descriptions exhibit similar preferences. Accordingly, UFGraphFR introduces two key components: (1) a privacy-preserving user relationship graph constructed from the joint embedding layer's weight matrix without leaking raw user attributes; (2) a Transformer-based architecture to model temporal dependencies in user-item interaction sequences. Experimental results on benchmark datasets such as MovieLens and HetRec2011 demonstrate that UFGraphFR achieves recommendation accuracy comparable to both centralized and state-of-the-art federated baselines while preserving user privacy. The code is available at: https://github.com/trueWangSyutung/UFGraphFR.</article>","contentLength":1912,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Computation of Collatz Sequence Stopping Times: A Novel Algorithmic Approach","url":"https://arxiv.org/abs/2501.04032","date":1751428800,"author":"","guid":179763,"unread":true,"content":"<article>arXiv:2501.04032v2 Announce Type: replace \nAbstract: The Collatz conjecture, which posits that any positive integer will eventually reach 1 through a specific iterative process, is a classic unsolved problem in mathematics. This research focuses on designing an efficient algorithm to compute the stopping time of numbers in the Collatz sequence, achieving significant computational improvements. By leveraging structural patterns in the Collatz tree, the proposed algorithm minimizes redundant operations and optimizes computational steps. Unlike prior methods, it efficiently handles extremely large numbers without requiring advanced techniques such as memoization or parallelization. Experimental evaluations confirm computational efficiency improvements of approximately 28% over state-of-the-art methods. These findings underscore the algorithm's scalability and robustness, providing a foundation for future large-scale verification of the conjecture and potential applications in computational mathematics.</article>","contentLength":1014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial Vehicle Imagery","url":"https://arxiv.org/abs/2501.01855","date":1751428800,"author":"","guid":179764,"unread":true,"content":"<article>arXiv:2501.01855v3 Announce Type: replace \nAbstract: Unmanned aerial vehicle object detection (UAV-OD) has been widely used in various scenarios. However, most existing UAV-OD algorithms rely on manually designed components, which require extensive tuning. End-to-end models that do not depend on such manually designed components are mainly designed for natural images, which are less effective for UAV imagery. To address such challenges, this paper proposes an efficient detection transformer (DETR) framework tailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scale feature fusion with frequency enhancement module, which captures both spatial and frequency information at different scales. In addition, a frequency-focused down-sampling module is presented to retain critical spatial details during down-sampling. A semantic alignment and calibration module is developed to align and fuse features from different fusion paths. Experimental results demonstrate the effectiveness and generalization of our approach across various UAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1\\% and $\\text{AP}_{50}$ by 4.2\\% over the baseline. Similar enhancements are observed on the UAVVaste dataset. The project page: https://github.com/ValiantDiligent/UAV-DETR</article>","contentLength":1303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference","url":"https://arxiv.org/abs/2501.01144","date":1751428800,"author":"","guid":179765,"unread":true,"content":"<article>arXiv:2501.01144v4 Announce Type: replace \nAbstract: The rapidly increasing size of large language models (LLMs) presents significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with hardware-supported fine-grained scaling emerging as a promising solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. We propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from a formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. To leverage this efficiently, we propose a two-stage approach for online DialectFP4 activation quantization. Importantly, DialectFP4 ensures energy efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit usage per data, while being only 5.45% (2.69%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.</article>","contentLength":1360,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimally Decoding Two-Dimensional Reed-Solomon Codes Against Deletion Errors","url":"https://arxiv.org/abs/2412.20771","date":1751428800,"author":"","guid":179766,"unread":true,"content":"<article>arXiv:2412.20771v2 Announce Type: replace \nAbstract: Constructing Reed-Solomon (RS) codes that can correct insertion and deletion (ins-del) errors has been the focus of several recent studies. However, efficient decoding algorithms for such codes have received less attention and remain a significant open problem. In this work, we take a first step toward addressing this problem by designing a decoding algorithm for the case of $2$-dimensional RS codes that can correct deletions up to the half-Singleton bound and is optimal in terms of field operations.</article>","contentLength":558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ETTA: Elucidating the Design Space of Text-to-Audio Models","url":"https://arxiv.org/abs/2412.19351","date":1751428800,"author":"","guid":179767,"unread":true,"content":"<article>arXiv:2412.19351v2 Announce Type: replace \nAbstract: Recent years have seen significant progress in Text-To-Audio (TTA) synthesis, enabling users to enrich their creative workflows with synthetic audio generated from natural language prompts. Despite this progress, the effects of data, model architecture, training objective functions, and sampling strategies on target benchmarks are not well understood. With the purpose of providing a holistic understanding of the design space of TTA models, we set up a large-scale empirical experiment focused on diffusion and flow matching models. Our contributions include: 1) AF-Synthetic, a large dataset of high quality synthetic captions obtained from an audio understanding model; 2) a systematic comparison of different architectural, training, and inference design choices for TTA models; 3) an analysis of sampling methods and their Pareto curves with respect to generation quality and inference speed. We leverage the knowledge obtained from this extensive analysis to propose our best model dubbed Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps, ETTA provides improvements over the baselines trained on publicly available data, while being competitive with models trained on proprietary data. Finally, we show ETTA's improved ability to generate creative audio following complex and imaginative captions -- a task that is more challenging than current benchmarks.</article>","contentLength":1443,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Automatic Graph Construction Framework based on Large Language Models for Recommendation","url":"https://arxiv.org/abs/2412.18241","date":1751428800,"author":"","guid":179768,"unread":true,"content":"<article>arXiv:2412.18241v2 Announce Type: replace \nAbstract: Graph neural networks (GNNs) have emerged as state-of-the-art methods to learn from graph-structured data for recommendation. However, most existing GNN-based recommendation methods focus on the optimization of model structures and learning strategies based on pre-defined graphs, neglecting the importance of the graph construction stage. Earlier works for graph construction usually rely on speciffic rules or crowdsourcing, which are either too simplistic or too labor-intensive. Recent works start to utilize large language models (LLMs) to automate the graph construction, in view of their abundant open-world knowledge and remarkable reasoning capabilities. Nevertheless, they generally suffer from two limitations: (1) invisibility of global view (e.g., overlooking contextual information) and (2) construction inefficiency. To this end, we introduce AutoGraph, an automatic graph construction framework based on LLMs for recommendation. Specifically, we first use LLMs to infer the user preference and item knowledge, which is encoded as semantic vectors. Next, we employ vector quantization to extract the latent factors from the semantic vectors. The latent factors are then incorporated as extra nodes to link the user/item nodes, resulting in a graph with in-depth global-view semantics. We further design metapath-based message aggregation to effectively aggregate the semantic and collaborative information. The framework is model-agnostic and compatible with different backbone models. Extensive experiments on three real-world datasets demonstrate the efficacy and efffciency of AutoGraph compared to existing baseline methods. We have deployed AutoGraph in Huawei advertising platform, and gain a 2.69% improvement on RPM and a 7.31% improvement on eCPM in the online A/B test. Currently AutoGraph has been used as the main trafffc model, serving hundreds of millions of people.</article>","contentLength":1948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling","url":"https://arxiv.org/abs/2412.14373","date":1751428800,"author":"","guid":179769,"unread":true,"content":"<article>arXiv:2412.14373v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated exceptional versatility across domains, including applications to electrocardiograms (ECGs). A growing body of work focuses on generating text from multi-channeled ECG signals and corresponding textual prompts. Existing approaches often involve a two-stage process: pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective, followed by finetuning an LLM for natural language generation (NLG) using encoder-derived features. However, these methods face two key limitations: inefficiency due to multi-stage training and challenges in interpreting encoder-generated features. To overcome these issues, we propose ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG signals into tokens, enabling direct end-to-end LLM training by combining ECG and text tokens. This approach enhances interpretability, as ECG tokens can be directly mapped back to the original signals. Leveraging ECG-Byte, we achieve competitive NLG performance while training 3 times faster and using just 48\\% of the data required by traditional two-stage methods.</article>","contentLength":1251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disturbance-Adaptive Data-Driven Predictive Control: Trading Comfort Violations for Savings in Building Climate Control","url":"https://arxiv.org/abs/2412.09238","date":1751428800,"author":"","guid":179770,"unread":true,"content":"<article>arXiv:2412.09238v2 Announce Type: replace \nAbstract: Model Predictive Control (MPC) has demonstrated significant potential in improving energy efficiency in building climate control, outperforming traditional controllers commonly used in modern building management systems. Among MPC variants, Data-driven Predictive Control (DPC) offers the advantage of modeling building dynamics directly from data, thereby substantially reducing commissioning efforts. However, inevitable model uncertainties and measurement noise can result in comfort violations, even with dedicated MPC setups. This paper introduces a Disturbance-Adaptive DPC (DAD-DPC) framework that ensures asymptotic satisfaction of predefined violation bounds without knowing the uncertainty and noise distributions. The framework employs a data-driven pipeline based on Willems' Fundamental Lemma and conformal prediction for application in building climate control. The proposed DAD-DPC framework was validated through four building cases using the high-fidelity BOPTEST simulation platform and an occupied campus building, Polydome. DAD-DPC successfully regulated the average comfort violations to meet pre-defined bounds. Notably, the 5%-violation DAD-DPC setup achieved 30.1%/11.2%/27.1%/20.5% energy savings compared to default controllers across four cases. These results demonstrate the framework's effectiveness in balancing energy consumption and comfort violations, offering a practical solution for building climate control applications.</article>","contentLength":1510,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Th\\'evenin Equivalent Parameters Identification Based on Statistical Characteristics of System Ambient Data","url":"https://arxiv.org/abs/2412.08328","date":1751428800,"author":"","guid":179771,"unread":true,"content":"<article>arXiv:2412.08328v2 Announce Type: replace \nAbstract: This paper proposes a novel method for identifying Th\\'evenin equivalent parameters (TEP) in power system, based on the statistical characteristics of the system's stochastic response. The method leverages stochastic fluctuation data under steady-state grid conditions and applies sliding window techniques to compute sensitivity parameters between voltage magnitude, current magnitude and power. This enables high-accuracy and robust TEP identification. In contrast to traditional methods, the proposed approach does not rely on large disturbances or probing signals but instead utilizes the natural fluctuation behavior of the system. Additionally, the method supports distributed implementation using local measurements of voltage magnitude, current magnitude, and power, offering significant practical value for engineering applications. The theoretical analysis demonstrates the method's robustness in the presence of low signal-to-noise ratio (SNR), asynchronous measurements, and data collinearity issues. Simulation results further confirm the effectiveness of the proposed method in diverse practical scenarios, demonstrating its ability to consistently provide accurate and reliable identification of TEP using system ambient data.</article>","contentLength":1294,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Expert Labels into LLM-based Emission Goal Detection: Example Selection vs Automatic Prompt Design","url":"https://arxiv.org/abs/2412.06432","date":1751428800,"author":"","guid":179772,"unread":true,"content":"<article>arXiv:2412.06432v2 Announce Type: replace \nAbstract: We address the detection of emission reduction goals in corporate reports, an important task for monitoring companies' progress in addressing climate change. Specifically, we focus on the issue of integrating expert feedback in the form of labeled example passages into LLM-based pipelines, and compare the two strategies of (1) a dynamic selection of few-shot examples and (2) the automatic optimization of the prompt by the LLM itself. Our findings on a public dataset of 769 climate-related passages from real-world business reports indicate that automatic prompt optimization is the superior approach, while combining both methods provides only limited benefit. Qualitative results indicate that optimized prompts do indeed capture many intricacies of the targeted emission goal extraction task.</article>","contentLength":852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STONet: A neural operator for modeling solute transport in micro-cracked reservoirs","url":"https://arxiv.org/abs/2412.05576","date":1751428800,"author":"","guid":179773,"unread":true,"content":"<article>arXiv:2412.05576v2 Announce Type: replace \nAbstract: In this work, we introduce a novel neural operator, the Solute Transport Operator Network (STONet), to efficiently model contaminant transport in micro-cracked porous media. STONet's model architecture is specifically designed for this problem and uniquely integrates an enriched DeepONet structure with a transformer-based multi-head attention mechanism, enhancing performance without incurring additional computational overhead compared to existing neural operators. The model combines different networks to encode heterogeneous properties effectively and predict the rate of change of the concentration field to accurately model the transport process. The training data is obtained using finite element (FEM) simulations by random sampling of micro-fracture distributions and applied pressure boundary conditions, which capture diverse scenarios of fracture densities, orientations, apertures, lengths, and balance of pressure-driven to density-driven flow. Our numerical experiments demonstrate that, once trained, STONet achieves accurate predictions, with relative errors typically below 1% compared with FEM simulations while reducing runtime by approximately two orders of magnitude. This type of computational efficiency facilitates building digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. The data and code for the paper will be published at https://github.com/ehsanhaghighat/STONet.</article>","contentLength":1525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension","url":"https://arxiv.org/abs/2412.03704","date":1751428800,"author":"","guid":179774,"unread":true,"content":"<article>arXiv:2412.03704v3 Announce Type: replace \nAbstract: Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM.</article>","contentLength":1422,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative AI-based data augmentation for improved bioacoustic classification in noisy environments","url":"https://arxiv.org/abs/2412.01530","date":1751428800,"author":"","guid":179775,"unread":true,"content":"<article>arXiv:2412.01530v2 Announce Type: replace \nAbstract: 1. Obtaining data to train robust artificial intelligence (AI)-based models for species classification can be challenging, particularly for rare species. Data augmentation can boost classification accuracy by increasing the diversity of training data and is cheaper to obtain than expert-labelled data. However, many classic image-based augmentation techniques are not suitable for audio spectrograms. 2. We investigate two generative AI models as data augmentation tools to synthesise spectrograms and supplement audio data: Auxiliary Classifier Generative Adversarial Networks (ACGAN) and Denoising Diffusion Probabilistic Models (DDPMs). The latter performed particularly well in terms of both realism of generated spectrograms and accuracy in a resulting classification task. 3. Alongside these new approaches, we present a new audio data set of 640 hours of bird calls from wind farm sites in Ireland, approximately 800 samples of which have been labelled by experts. Wind farm data are particularly challenging for classification models given the background wind and turbine noise. 4. Training an ensemble of classification models on real and synthetic data combined gave 92.6% accuracy (and 90.5% with just the real data) when compared with highly confident BirdNET predictions. 5. Our approach can be used to augment acoustic signals for more species and other land-use types, and has the potential to bring about a step-change in our capacity to develop reliable AI-based detection of rare species. Our code is available at https://github.com/gibbona1/SpectrogramGenAI.</article>","contentLength":1631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OMNI-DC: Highly Robust Depth Completion with Multiresolution Depth Integration","url":"https://arxiv.org/abs/2411.19278","date":1751428800,"author":"","guid":179776,"unread":true,"content":"<article>arXiv:2411.19278v2 Announce Type: replace \nAbstract: Depth completion (DC) aims to predict a dense depth map from an RGB image and a sparse depth map. Existing DC methods generalize poorly to new datasets or unseen sparse depth patterns, limiting their real-world applications. We propose OMNI-DC, a highly robust DC model that generalizes well zero-shot to various datasets. The key design is a novel Multi-resolution Depth Integrator, allowing our model to deal with very sparse depth inputs. We also introduce a novel Laplacian loss to model the ambiguity in the training process. Moreover, we train OMNI-DC on a mixture of high-quality datasets with a scale normalization technique and synthetic depth patterns. Extensive experiments on 7 datasets show consistent improvements over baselines, reducing errors by as much as 43%. Codes and checkpoints are available at https://github.com/princeton-vl/OMNI-DC.</article>","contentLength":911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Dual Prototypes for Task-Wise Adaption in Pre-Trained Model-Based Class-Incremental Learning","url":"https://arxiv.org/abs/2411.17766","date":1751428800,"author":"","guid":179777,"unread":true,"content":"<article>arXiv:2411.17766v3 Announce Type: replace \nAbstract: Class-incremental learning (CIL) aims to acquire new classes while conserving historical knowledge incrementally. Despite existing pre-trained model (PTM) based methods performing excellently in CIL, it is better to fine-tune them on downstream incremental tasks with massive patterns unknown to PTMs. However, using task streams for fine-tuning could lead to \\textit{catastrophic forgetting} that will erase the knowledge in PTMs. This paper proposes the Dual Prototype network for Task-wise Adaption (DPTA) of PTM-based CIL. For each incremental learning task, an adapter module is built to fine-tune the PTM, where the center-adapt loss forces the representation to be more centrally clustered and class separable. The dual prototype network improves the prediction process by enabling test-time adapter selection, where the raw prototypes deduce several possible task indexes of test samples to select suitable adapter modules for PTM, and the augmented prototypes that could separate highly correlated classes are utilized to determine the final result. Experiments on several benchmark datasets demonstrate the excellent performance of DPTA. Code is available in https://github.com/Yorkxzm/DPTA</article>","contentLength":1253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Intervention Models for Causal Perturbation Modeling","url":"https://arxiv.org/abs/2411.14003","date":1751428800,"author":"","guid":179778,"unread":true,"content":"<article>arXiv:2411.14003v2 Announce Type: replace \nAbstract: We consider the problem of predicting perturbation effects via causal models. In many applications, it is a priori unknown which mechanisms of a system are modified by an external perturbation, even though the features of the perturbation are available. For example, in genomics, some properties of a drug may be known, but not their causal effects on the regulatory pathways of cells. We propose a generative intervention model (GIM) that learns to map these perturbation features to distributions over atomic interventions in a jointly-estimated causal model. Contrary to prior approaches, this enables us to predict the distribution shifts of unseen perturbation features while gaining insights about their mechanistic effects in the underlying data-generating process. On synthetic data and scRNA-seq drug perturbation data, GIMs achieve robust out-of-distribution predictions on par with unstructured approaches, while effectively inferring the underlying perturbation mechanisms, often better than other causal inference methods.</article>","contentLength":1088,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SMoLoRA: Exploring and Defying Dual Catastrophic Forgetting in Continual Visual Instruction Tuning","url":"https://arxiv.org/abs/2411.13949","date":1751428800,"author":"","guid":179779,"unread":true,"content":"<article>arXiv:2411.13949v2 Announce Type: replace \nAbstract: Visual instruction tuning (VIT) enables multimodal large language models (MLLMs) to effectively handle a wide range of vision tasks by framing them as language-based instructions. Building on this, continual visual instruction tuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks, accommodating evolving functionalities. While prior work has advanced CVIT through the development of new benchmarks and approaches to mitigate catastrophic forgetting, these efforts largely follow traditional continual learning paradigms, neglecting the unique challenges specific to CVIT. We identify a dual form of catastrophic forgetting in CVIT, where MLLMs not only forget previously learned visual understanding but also experience a decline in instruction following abilities as they acquire new tasks. To address this, we introduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework, which employs separable routing through two distinct modules-one for visual understanding and another for instruction following. This dual-routing design enables specialized adaptation in both domains, preventing forgetting while improving performance. Furthermore, we propose a new CVIT benchmark that goes beyond existing benchmarks by additionally evaluating a model's ability to generalize to unseen tasks and handle diverse instructions across various tasks. Extensive experiments demonstrate that SMoLoRA outperforms existing methods in mitigating dual forgetting, improving generalization to unseen tasks, and ensuring robustness in following diverse instructions. Code is available at https://github.com/Minato-Zackie/SMoLoRA.</article>","contentLength":1701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Identity Preserving 3D Head Stylization with Multiview Score Distillation","url":"https://arxiv.org/abs/2411.13536","date":1751428800,"author":"","guid":179780,"unread":true,"content":"<article>arXiv:2411.13536v2 Announce Type: replace \nAbstract: 3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the https://three-bee.github.io/head_stylization for more visuals.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Holistic to Localized: Local Enhanced Adapters for Efficient Visual Instruction Fine-Tuning","url":"https://arxiv.org/abs/2411.12787","date":1751428800,"author":"","guid":179781,"unread":true,"content":"<article>arXiv:2411.12787v3 Announce Type: replace \nAbstract: Efficient Visual Instruction Fine-Tuning (EVIT) seeks to adapt Multimodal Large Language Models (MLLMs) to downstream tasks with minimal computational overhead. However, as task diversity and complexity increase, EVIT faces significant challenges in resolving data conflicts. To address this limitation, we propose the Dual Low-Rank Adaptation (Dual-LoRA), a holistic-to-local framework that enhances the adapter's capacity to address data conflict through dual structural optimization. Specifically, we utilize two subspaces: a skill space for stable, holistic knowledge retention, and a rank-rectified task space that locally activates the holistic knowledge. Additionally, we introduce Visual Cue Enhancement (VCE), a multi-level local feature aggregation module designed to enrich the vision-language projection with local details. Our approach is both memory- and time-efficient, requiring only 1.16$\\times$ the inference time of the standard LoRA method (with injection into the query and value projection layers), and just 73\\% of the inference time of a 4-expert LoRA-MoE. Extensive experiments on various downstream tasks and general MLLM benchmarks validate the effectiveness of our proposed methods.</article>","contentLength":1263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Careless Whisper: Exploiting Silent Delivery Receipts to Monitor Users on Mobile Instant Messengers","url":"https://arxiv.org/abs/2411.11194","date":1751428800,"author":"","guid":179782,"unread":true,"content":"<article>arXiv:2411.11194v3 Announce Type: replace \nAbstract: With over 3 billion users globally, mobile instant messaging apps have become indispensable for both personal and professional communication. Besides plain messaging, many services implement additional features such as delivery and read receipts informing a user when a message has successfully reached its target. This paper highlights that delivery receipts can pose significant privacy risks to users. We use specifically crafted messages that trigger delivery receipts allowing any user to be pinged without their knowledge or consent. By using this technique at high frequency, we demonstrate how an attacker could extract private information such as the online and activity status of a victim, e.g., screen on/off. Moreover, we can infer the number of currently active user devices and their operating system, as well as launch resource exhaustion attacks, such as draining a user's battery or data allowance, all without generating any notification on the target side. Due to the widespread adoption of vulnerable messengers (WhatsApp and Signal) and the fact that any user can be targeted simply by knowing their phone number, we argue for a design change to address this issue.</article>","contentLength":1239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VideoCogQA: A Controllable Benchmark for Evaluating Cognitive Abilities in Video-Language Models","url":"https://arxiv.org/abs/2411.09105","date":1751428800,"author":"","guid":179783,"unread":true,"content":"<article>arXiv:2411.09105v2 Announce Type: replace \nAbstract: Recent advancements in Large Video-Language Models (LVLMs) have led to promising results in multimodal video understanding. However, it remains unclear whether these models possess the cognitive capabilities required for high-level tasks, particularly those involving symbolic and abstract perception. Existing benchmarks typically rely on real-world, annotated videos, which lack control over video content and inherent difficulty, limiting their diagnostic power. To bridge this gap, we propose VideoCogQA, a scalable and fully controllable benchmark inspired by game-world environments, designed to evaluate the cognitive abilities of LVLMs. By generating synthetic videos via a programmatic engine, VideoCogQA allows fine-grained control over visual elements, temporal dynamics, and task difficulty. This approach enables a focused evaluation of video cognitive abilities, independent of prior knowledge from visual scene semantics. The dataset includes 800 videos and 3,280 question-answer pairs, featuring tasks related to abstract concepts, symbolic elements, and multimodal integration, with varying levels of difficulty. Experimental results show that even state-of-the-art (SOTA) models, such as GPT-4o, achieve an average performance of 48.8% on tasks involving abstract concepts. Additionally, performance drops by 15% as task complexity increases, highlighting the challenges LVLMs face in maintaining consistent performance. Through this work, we hope to show the limitations of current LVLMs and offer insights into how they can more effectively emulate human cognitive processes in the future.</article>","contentLength":1662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers","url":"https://arxiv.org/abs/2411.04403","date":1751428800,"author":"","guid":179784,"unread":true,"content":"<article>arXiv:2411.04403v2 Announce Type: replace \nAbstract: Learned sparse retrieval, which can efficiently perform retrieval through mature inverted-index engines, has garnered growing attention in recent years. Particularly, the inference-free sparse retrievers are attractive as they eliminate online model inference in the retrieval phase thereby avoids huge computational cost, offering reasonable throughput and latency. However, even the state-of-the-art (SOTA) inference-free sparse models lag far behind in terms of search relevance when compared to both sparse and dense siamese models. Towards competitive search relevance for inference-free sparse retrievers, we argue that they deserve dedicated training methods other than using same ones with siamese encoders. In this paper, we propose two different approaches for performance improvement. First, we propose an IDF-aware penalty for the matching function that suppresses the contribution of low-IDF tokens and increases the model's focus on informative terms. Moreover, we propose a heterogeneous ensemble knowledge distillation framework that combines siamese dense and sparse retrievers to generate supervisory signals during the pre-training phase. The ensemble framework of dense and sparse retriever capitalizes on their strengths respectively, providing a strong upper bound for knowledge distillation. To concur the diverse feedback from heterogeneous supervisors, we normalize and then aggregate the outputs of the teacher models to eliminate score scale differences. On the BEIR benchmark, our model outperforms existing SOTA inference-free sparse model by \\textbf{3.3 NDCG@10 score}. It exhibits search relevance comparable to siamese sparse retrievers and client-side latency only \\textbf{1.1x that of BM25}.</article>","contentLength":1778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigating the heterogenous effects of a massive content moderation intervention via Difference-in-Differences","url":"https://arxiv.org/abs/2411.04037","date":1751428800,"author":"","guid":179785,"unread":true,"content":"<article>arXiv:2411.04037v4 Announce Type: replace \nAbstract: In today's online environments, users encounter harm and abuse on a daily basis. Therefore, content moderation is crucial to ensure their safety and well-being. However, the effectiveness of many moderation interventions is still uncertain. Here, we apply a causal inference approach to shed light on the effectiveness of The Great Ban, a massive social media deplatforming intervention on Reddit. We analyze 53M comments shared by nearly 34K users, providing in-depth results on both the intended and unintended consequences of the ban. Our causal analyses reveal that 15.6% of the moderated users abandoned the platform while the remaining ones decreased their overall toxicity by 4.1%. Nonetheless, a small subset of users exhibited marked increases in both the intensity and volume of toxic behavior, particularly among those whose activity levels changed after the intervention. However, these reactions were not accompanied by greater activity or engagement, suggesting that even the most toxic users maintained a limited overall impact. Our findings bring to light new insights on the effectiveness of deplatforming moderation interventions. Furthermore, they also contribute to informing future content moderation strategies and regulations.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revealing Floating-Point Accumulation Orders in Software/Hardware Implementations","url":"https://arxiv.org/abs/2411.00442","date":1751428800,"author":"","guid":179786,"unread":true,"content":"<article>arXiv:2411.00442v3 Announce Type: replace \nAbstract: Accumulation-based operations, such as summation and matrix multiplication, are fundamental to numerous computational domains. However, their accumulation orders are often undocumented in existing software and hardware implementations, making it difficult for developers to ensure consistent results across systems. To address this issue, we introduce FPRev, a diagnostic tool designed to reveal the accumulation order in the software and hardware implementations through numerical testing. With FPRev, developers can identify and compare accumulation orders, enabling developers to create reproducible software and verify implementation equivalence.\n  FPRev is a testing-based tool that non-intrusively reveals the accumulation order by analyzing the outputs of the tested implementation for distinct specially designed inputs. Employing FPRev, we showcase the accumulation orders of popular libraries (such as NumPy and PyTorch) on CPUs and GPUs (including GPUs with specialized matrix accelerators such as Tensor Cores). We also validate the efficiency of FPRev through extensive experiments. FPRev exhibits a lower time complexity compared to the basic solution. FPRev is open-sourced at https://github.com/peichenxie/FPRev.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EvoPress: Accurate Dynamic Model Compression via Evolutionary Search","url":"https://arxiv.org/abs/2410.14649","date":1751428800,"author":"","guid":179787,"unread":true,"content":"<article>arXiv:2410.14649v2 Announce Type: replace \nAbstract: The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by dynamic, non-uniform compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on estimating the importance of a given layer, implicitly assuming that layers contribute independently to the overall compression error. We begin from the motivating observation that this independence assumption does not generally hold for LLM compression: pruning a model further may even significantly recover performance. To address this, we propose EvoPress, a novel evolutionary framework for dynamic LLM compression. By formulating dynamic compression as a general optimization problem, EvoPress identifies optimal compression profiles in a highly efficient manner, and generalizes across diverse models and compression techniques. Via EvoPress, we achieve state-of-the-art performance for dynamic compression of Llama, Mistral, and Phi models, setting new benchmarks for structural pruning (block/layer dropping), unstructured sparsity, and quantization with dynamic bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress}.</article>","contentLength":1467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion","url":"https://arxiv.org/abs/2410.14405","date":1751428800,"author":"","guid":179788,"unread":true,"content":"<article>arXiv:2410.14405v4 Announce Type: replace \nAbstract: Language models (LMs) can make a correct prediction based on many possible signals in a prompt, not all corresponding to recall of factual associations. However, current interpretations of LMs fail to take this into account. For example, given the query \"Astrid Lindgren was born in\" with the corresponding completion \"Sweden\", no difference is made between whether the prediction was based on knowing where the author was born or assuming that a person with a Swedish-sounding name was born in Sweden. In this paper, we present a model-specific recipe - PrISM - for constructing datasets with examples of four different prediction scenarios: generic language modeling, guesswork, heuristics recall and exact fact recall. We apply two popular interpretability methods to the scenarios: causal tracing (CT) and information flow analysis. We find that both yield distinct results for each scenario. Results for exact fact recall and generic language modeling scenarios confirm previous conclusions about the importance of mid-range MLP sublayers for fact recall, while results for guesswork and heuristics indicate a critical role of late last token position MLP sublayers. In summary, we contribute resources for a more extensive and granular study of fact completion in LMs, together with analyses that provide a more nuanced understanding of how LMs process fact-related queries.</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sliding Puzzles Gym: A Scalable Benchmark for State Representation in Visual Reinforcement Learning","url":"https://arxiv.org/abs/2410.14038","date":1751428800,"author":"","guid":179789,"unread":true,"content":"<article>arXiv:2410.14038v4 Announce Type: replace \nAbstract: Effective visual representation learning is crucial for reinforcement learning (RL) agents to extract task-relevant information from raw sensory inputs and generalize across diverse environments. However, existing RL benchmarks lack the ability to systematically evaluate representation learning capabilities in isolation from other learning challenges. To address this gap, we introduce the Sliding Puzzles Gym (SPGym), a novel benchmark that transforms the classic 8-tile puzzle into a visual RL task with images drawn from arbitrarily large datasets. SPGym's key innovation lies in its ability to precisely control representation learning complexity through adjustable grid sizes and image pools, while maintaining fixed environment dynamics, observation, and action spaces. This design enables researchers to isolate and scale the visual representation challenge independently of other learning components. Through extensive experiments with model-free and model-based RL algorithms, we uncover fundamental limitations in current methods' ability to handle visual diversity. As we increase the pool of possible images, all algorithms exhibit in- and out-of-distribution performance degradation, with sophisticated representation learning techniques often underperforming simpler approaches like data augmentation. These findings highlight critical gaps in visual representation learning for RL and establish SPGym as a valuable tool for driving progress in robust, generalizable decision-making systems.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal Regularization","url":"https://arxiv.org/abs/2410.11281","date":1751428800,"author":"","guid":179790,"unread":true,"content":"<article>arXiv:2410.11281v2 Announce Type: replace \nAbstract: We report DynaCLR, a self-supervised method for embedding cell and organelle Dynamics via Contrastive Learning of Representations of time-lapse images. DynaCLR integrates single-cell tracking and time-aware contrastive sampling to learn robust, temporally regularized representations of cell dynamics. DynaCLR embeddings generalize effectively to in-distribution and out-of-distribution datasets, and can be used for several downstream tasks with sparse human annotations. We demonstrate efficient annotations of cell states with a human-in-the-loop using fluorescence and label-free imaging channels. DynaCLR method enables diverse downstream biological analyses: classification of cell division and infection, clustering heterogeneous cell migration patterns, cross-modal distillation of cell states from fluorescence to label-free channel, alignment of asynchronous cellular responses and broken cell tracks, and discovering organelle response due to infection. DynaCLR is a flexible method for comparative analyses of dynamic cellular responses to pharmacological, microbial, and genetic perturbations. We provide PyTorch-based implementations of the model training and inference pipeline (https://github.com/mehta-lab/viscy) and a GUI (https://github.com/czbiohub-sf/napari-iohub) for the visualization and annotation of trajectories of cells in the real space and the embedding space.</article>","contentLength":1443,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Fujita exponent for finite difference approximations of nonlocal and local semilinear blow-up problems","url":"https://arxiv.org/abs/2410.10458","date":1751428800,"author":"","guid":179791,"unread":true,"content":"<article>arXiv:2410.10458v3 Announce Type: replace \nAbstract: We study monotone finite difference approximations for a broad class of reaction-diffusion problems, incorporating general symmetric L\\'evy operators. By employing an adaptive time-stepping discretization, we derive the discrete Fujita critical exponent for these problems. Additionally, under general consistency assumptions, we establish the convergence of discrete blow-up times to their continuous counterparts. As complementary results, we also present the asymptotic-in-time behavior of discrete heat-type equations as well as an extensive analysis of discrete eigenvalue problems.</article>","contentLength":640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Subspace method of moments for ab initio 3-D single-particle cryo-EM reconstruction","url":"https://arxiv.org/abs/2410.06889","date":1751428800,"author":"","guid":179792,"unread":true,"content":"<article>arXiv:2410.06889v2 Announce Type: replace \nAbstract: Cryo-electron microscopy (cryo-EM) is a widely-used technique for recovering the 3-D structure of biological molecules from a large number of experimentally generated noisy 2-D tomographic projection images of the 3-D structure, taken from unknown viewing angles. Through computationally intensive algorithms, these observed images are processed to reconstruct the 3-D structures. Many popular computational methods rely on estimating the unknown angles as part of the reconstruction process, which becomes particularly challenging at low signal-to-noise ratios. The method of moments (MoM) offers an alternative approach that circumvents the estimation of viewing orientations of individual projection images by instead estimating the underlying distribution of the viewing angles, and is robust to noise given sufficiently many images. However, the method of moments typically entails computing high-order moments of the projection images, incurring significant computational and memory costs. To mitigate this, we propose a new approach called the subspace method of moments (subspace MoM), which compresses the first three moments using data-driven low-rank tensor techniques as well as expansion into a suitable function basis. The compressed moments can be efficiently computed from the set of projection images using numerical quadrature and can be employed to jointly reconstruct the 3-D structure and the distribution of viewing orientations. We illustrate the practical applicability of the subspace MoM through numerical experiments using up to the third-order moment on synthetic datasets with a simplified cryo-EM image formation model, which significantly improves the resolution of MoM reconstructions compared to previous approaches.</article>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging SFT and DPO for Diffusion Model Alignment with Self-Sampling Preference Optimization","url":"https://arxiv.org/abs/2410.05255","date":1751428800,"author":"","guid":179793,"unread":true,"content":"<article>arXiv:2410.05255v2 Announce Type: replace \nAbstract: Existing post-training techniques are broadly categorized into supervised fine-tuning (SFT) and reinforcement learning (RL) methods; the former is stable during training but suffers from limited generalization, while the latter, despite its stronger generalization capability, relies on additional preference data or reward models and carries the risk of reward exploitation. In order to preserve the advantages of both SFT and RL -- namely, eliminating the need for paired data and reward models while retaining the training stability of SFT and the generalization ability of RL -- a new alignment method, Self-Sampling Preference Optimization (SSPO), is proposed in this paper. SSPO introduces a Random Checkpoint Replay (RCR) strategy that utilizes historical checkpoints to construct paired data, thereby effectively mitigating overfitting. Simultaneously, a Self-Sampling Regularization (SSR) strategy is employed to dynamically evaluate the quality of generated samples; when the generated samples are more likely to be winning samples, the approach automatically switches from DPO (Direct Preference Optimization) to SFT, ensuring that the training process accurately reflects the quality of the samples. Experimental results demonstrate that SSPO not only outperforms existing methods on text-to-image benchmarks, but its effectiveness has also been validated in text-to-video tasks. We validate SSPO across both text-to-image and text-to-video benchmarks. SSPO surpasses all previous approaches on the text-to-image benchmarks and demonstrates outstanding performance on the text-to-video benchmarks.</article>","contentLength":1662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Cost of Consecutive Estimation Error: Significance-Aware Non-linear Aging","url":"https://arxiv.org/abs/2410.03637","date":1751428800,"author":"","guid":179794,"unread":true,"content":"<article>arXiv:2410.03637v2 Announce Type: replace \nAbstract: This paper considers the semantics-aware remote state estimation of an asymmetric Markov chain with prioritized states. Due to resource constraints, the sensor needs to trade between estimation quality and communication cost. The aim is to exploit the significance of information through the history of system realizations to determine the optimal timing of transmission, thereby reducing the amount of uninformative data transmitted in the network. To this end, we introduce a new metric, the significance-aware Age of Consecutive Error (AoCE), that captures two semantic attributes: the significance of estimation error and the cost of consecutive error. Different costs and non-linear age functions are assigned to different estimation errors to account for their relative importance to system performance. We identify the optimal transmission problem as a countably infinite state Markov decision process (MDP) with unbounded costs. We first give sufficient conditions on the age functions, source pattern, and channel reliability so that an optimal policy exists to have bounded average costs. We show that the optimal policy exhibits a switching structure. That is, the sensor triggers a transmission only when the system has been trapped in an error for a certain number of consecutive time slots. We also provide sufficient conditions under which the switching policy degenerates into a simple threshold policy, i.e., featuring identical thresholds for all estimation errors. Furthermore, we exploit the structural properties and develop a structured policy iteration (SPI) algorithm that considerably reduces computation overhead. Numerical results show that the optimal policy outperforms the classic rule-, distortion- and age-based policies. An important takeaway is that the more semantic attributes we utilize, the fewer transmissions are needed.</article>","contentLength":1913,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs","url":"https://arxiv.org/abs/2410.01141","date":1751428800,"author":"","guid":179795,"unread":true,"content":"<article>arXiv:2410.01141v3 Announce Type: replace \nAbstract: This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.</article>","contentLength":640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The complexity of separability for semilinear sets and Parikh automata","url":"https://arxiv.org/abs/2410.00548","date":1751428800,"author":"","guid":179796,"unread":true,"content":"<article>arXiv:2410.00548v4 Announce Type: replace \nAbstract: In a \\emph{separability problem}, we are given two sets $K$ and $L$ from a class $\\mathcal{C}$, and we want to decide whether there exists a set $S$ from a class $\\mathcal{S}$ such that $K\\subseteq S$ and $S\\cap L=\\emptyset$. In this case, we speak of \\emph{separability of sets in $\\mathcal{C}$ by sets in $\\mathcal{S}$}.\n  We study two types of separability problems. First, we consider separability of semilinear sets (i.e. subsets of $\\mathbb{N}^d$ for some $d$) by sets definable by quantifier-free monadic Presburger formulas (or equivalently, the recognizable subsets of $\\mathbb{N}^d$). Here, a formula is monadic if each atom uses at most one variable. Second, we consider separability of languages of Parikh automata by regular languages. A Parikh automaton is a machine with access to counters that can only be incremented, and have to meet a semilinear constraint at the end of the run. Both of these separability problems are known to be decidable with elementary complexity.\n  Our main results are that both problems are coNP-complete. In the case of semilinear sets, coNP-completeness holds regardless of whether the input sets are specified by existential Presburger formulas, quantifier-free formulas, or semilinear representations. Our results imply that recognizable separability of rational subsets of $\\Sigma^*\\times\\mathbb{N}^d$ (shown decidable by Choffrut and Grigorieff) is coNP-complete as well. Another application is that regularity of deterministic Parikh automata (where the target set is specified using a quantifier-free Presburger formula) is coNP-complete as well.</article>","contentLength":1651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","url":"https://arxiv.org/abs/2409.20302","date":1751428800,"author":"","guid":179797,"unread":true,"content":"<article>arXiv:2409.20302v4 Announce Type: replace \nAbstract: Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information, particularly for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component for efficient ontology management, the growing size of ontologies and accumulating errors caused by manual labour overwhelm current OV approaches. In this paper, we propose a fresh approach to performing OV using existing ontology matching (OM) techniques and systems. We introduce a unified OM4OV pipeline. From an OM perspective, we reconstruct a new task formulation and measurements for OV tasks. Building upon the prior alignment(s) from OM, we propose a pipeline optimisation method called the cross-reference (CR) mechanism to enhance overall OV performance. We experimentally validate the OM4OV pipeline and the cross-reference mechanism in an OV testbed originating from the Ontology Alignment Evaluation Initiative (OAEI) datasets. We also discuss insights into OM used for OV tasks, where some apparent false mappings detected by OV systems are not actually untrue.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes","url":"https://arxiv.org/abs/2409.15128","date":1751428800,"author":"","guid":179798,"unread":true,"content":"<article>arXiv:2409.15128v2 Announce Type: replace \nAbstract: The general-utility Markov decision processes (GUMDPs) framework generalizes the MDPs framework by considering objective functions that depend on the frequency of visitation of state-action pairs induced by a given policy. In this work, we contribute with the first analysis on the impact of the number of trials, i.e., the number of randomly sampled trajectories, in infinite-horizon GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a key-role in infinite-horizon GUMDPs and the expected performance of a given policy depends, in general, on the number of trials. We consider both discounted and average GUMDPs, where the objective function depends, respectively, on discounted and average frequencies of visitation of state-action pairs. First, we study policy evaluation under discounted GUMDPs, proving lower and upper bounds on the mismatch between the finite and infinite trials formulations for GUMDPs. Second, we address average GUMDPs, studying how different classes of GUMDPs impact the mismatch between the finite and infinite trials formulations. Third, we provide a set of empirical results to support our claims, highlighting how the number of trajectories and the structure of the underlying GUMDP influence policy evaluation.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural Networks Generalize on Low Complexity Data","url":"https://arxiv.org/abs/2409.12446","date":1751428800,"author":"","guid":179799,"unread":true,"content":"<article>arXiv:2409.12446v4 Announce Type: replace \nAbstract: We show that feedforward neural networks with ReLU activation generalize on low complexity data, suitably defined. Given i.i.d.~data generated from a simple programming language, the minimum description length (MDL) feedforward neural network which interpolates the data generalizes with high probability. We define this simple programming language, along with a notion of description length of such networks. We provide several examples on basic computational tasks, such as checking primality of a natural number. For primality testing, our theorem shows the following and more. Suppose that we draw an i.i.d.~sample of $n$ numbers uniformly at random from $1$ to $N$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then, the interpolating MDL network accurately answers, with error probability $1- O((\\ln N)/n)$, whether a newly drawn number between $1$ and $N$ is a prime or not. Note that the network is not designed to detect primes; minimum description learning discovers a network which does so. Extensions to noisy data are also discussed, suggesting that MDL neural network interpolators can demonstrate tempered overfitting.</article>","contentLength":1220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Depth Matters: Exploring Deep Interactions of RGB-D for Semantic Segmentation in Traffic Scenes","url":"https://arxiv.org/abs/2409.07995","date":1751428800,"author":"","guid":179800,"unread":true,"content":"<article>arXiv:2409.07995v2 Announce Type: replace \nAbstract: RGB-D has gradually become a crucial data source for understanding complex scenes in assisted driving. However, existing studies have paid insufficient attention to the intrinsic spatial properties of depth maps. This oversight significantly impacts the attention representation, leading to prediction errors caused by attention shift issues. To this end, we propose a novel learnable Depth interaction Pyramid Transformer (DiPFormer) to explore the effectiveness of depth. Firstly, we introduce Depth Spatial-Aware Optimization (Depth SAO) as offset to represent real-world spatial relationships. Secondly, the similarity in the feature space of RGB-D is learned by Depth Linear Cross-Attention (Depth LCA) to clarify spatial differences at the pixel level. Finally, an MLP Decoder is utilized to effectively fuse multi-scale features for meeting real-time requirements. Comprehensive experiments demonstrate that the proposed DiPFormer significantly addresses the issue of attention misalignment in both road detection (+7.5%) and semantic segmentation (+4.9% / +1.5%) tasks. DiPFormer achieves state-of-the-art performance on the KITTI (97.57% F-score on KITTI road and 68.74% mIoU on KITTI-360) and Cityscapes (83.4% mIoU) datasets.</article>","contentLength":1289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving the Parameter Dependency for High-Multiplicity Scheduling on Uniform Machines","url":"https://arxiv.org/abs/2409.04212","date":1751428800,"author":"","guid":179801,"unread":true,"content":"<article>arXiv:2409.04212v2 Announce Type: replace \nAbstract: Block-structured integer linear programs (ILPs) play an important role in various application fields. We address $n$-fold ILPs where the matrix $A$ has a specific structure, i.e., where the blocks in the lower part of A consist only of the row vectors $(1, ... ,1)$.\n  In this paper, we propose an approach tailored to exactly these combinatorial $n$-folds. We utilize a divide and conquer approach to separate the original problem such that the right-hand side iteratively decreases in size. We show that this decrease in size can be calculated such that we only need to consider a bounded amount of possible right-hand sides. This, in turn, lets us efficiently combine solutions of the smaller right-hand sides to solve the original problem. We can decide the feasibility of, and also optimally solve, such problems in time $(nr\\Delta)^{O(r)} \\log(\\|b\\|_\\infty)$, where $n$ is the number of blocks, $r$ the number of rows in the upper blocks and $\\Delta=\\|A\\|_\\infty$.\n  We complement the algorithm by discussing applications of the $n$-fold ILPs with the specific structure we require. We consider the problems of (i) scheduling on uniform machines, (ii) closest string and (iii) (graph) imbalance.\n  Regarding (i), our algorithm results in running times of $p_{\\max}^{O(d)}\\text{poly}(I)$, matching a lower bound derived via ETH.\n  For (ii) we achieve running times matching the current state-of-the-art in the general case. In contrast to the state-of-the-art, our result can leverage a bounded number of column-types to yield an improved running time.\n  For (iii), we improve the parameter dependency on the size of the vertex cover.</article>","contentLength":1692,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the impact of coordinated fleets size on traffic efficiency","url":"https://arxiv.org/abs/2408.15742","date":1751428800,"author":"","guid":179802,"unread":true,"content":"<article>arXiv:2408.15742v2 Announce Type: replace \nAbstract: We investigate a traffic assignment problem on a transportation network, considering both the demands of individual drivers and of a large fleet controlled by a central operator (minimizing the fleet's average travel time). We formulate this problem as a two-player convex game and we study how the size of the coordinated fleet, measured in terms of share of the total demand, influences the Price of Anarchy (PoA). We show that, for two-terminal networks, there are cases in which the fleet must reach a minimum share before actually affecting the PoA, which otherwise remains unchanged. Moreover, for parallel networks we prove that, under suitable assumptions, the PoA is monotonically non-increasing in the fleet share.</article>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flexora: Flexible Low Rank Adaptation for Large Language Models","url":"https://arxiv.org/abs/2408.10774","date":1751428800,"author":"","guid":179803,"unread":true,"content":"<article>arXiv:2408.10774v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) are driving advancements in artificial intelligence by increasing the scale of model parameters, which has significantly enhanced generalization ability and unlocked new capabilities in practice. However, their performance in specific downstream tasks is usually hindered by their knowledge boundaries on these tasks. Thus, fine-tuning techniques, especially the widely used Low-Rank Adaptation (LoRA) method, have been introduced to expand the boundaries on these tasks, whereas LoRA would underperform on certain tasks owing to its potential overfitting on these tasks. To overcome this overfitting and improve the performance of LoRA, we propose the flexible low rank adaptation (Flexora) method to automatically and flexibly select the most important layers needing to be fine-tuned to achieve the best performance on different downstream tasks. Specifically, Flexora firstly frames this layer selection problem as a well-defined hyperparameter optimization (HPO) problem, then addresses it using the unrolled differentiation (UD) method, and finally selects the most useful layers based on the optimized hyperparameters. Our extensive experiments on many pretrained models and natural language tasks show that Flexora is able to consistently improve over the existing baselines, indicating the effectiveness of our Flexora in practice. We additionally provide insightful theoretical results and many ablation studies to deliver a comprehensive understanding of our Flexora.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Metascience Study of the Low-Code Scientific Field","url":"https://arxiv.org/abs/2408.05975","date":1751428800,"author":"","guid":179804,"unread":true,"content":"<article>arXiv:2408.05975v2 Announce Type: replace \nAbstract: In the last years, model-related publications have been exploring the application of modeling techniques across various domains. Initially focused on UML and the Model-Driven Architecture approach, the literature has been evolving towards the usage of more general concepts such as Model-Driven Development or Model-Driven Engineering. More recently, however, the term \"low-code\" has taken the modeling field by storm, largely due to its association with several highly popular development platforms. The research community is still discussing the differences and commonalities between this emerging term and previous modeling-related concepts, as well as the broader implications of low-code on the modeling field. In this paper, we present a metascience study of Low-Code. Our study follows a two-fold approach: (1) to analyze the composition and growth (e.g., size, diversity, venues, and topics) of the emerging Low-Code community; and (2) to explore how these aspects differ from those of the \"classical\" model-driven community. Ultimately, we hope to trigger a discussion on the current state and potential future trajectory of the low-code community, as well as the opportunities for collaboration and synergies between the low-code and modeling communities.</article>","contentLength":1318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parameter-Efficient Fine-Tuning via Circular Convolution","url":"https://arxiv.org/abs/2407.19342","date":1751428800,"author":"","guid":179805,"unread":true,"content":"<article>arXiv:2407.19342v4 Announce Type: replace \nAbstract: Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large foundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$ to represent weight changes (i.e., $\\Delta \\mathbf{W} = \\mathbf{B} \\mathbf{A}$). This method reduces trainable parameters and mitigates heavy memory consumption associated with full delta matrices by sequentially multiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its success, the intrinsic low-rank characteristic may limit its performance. Although several variants have been proposed to address this issue, they often overlook the crucial computational and memory efficiency brought by LoRA. In this paper, we propose Circular Convolution Adaptation (C$^3$A), which not only achieves high-rank adaptation with enhanced performance but also excels in both computational power and memory utilization. Extensive experiments demonstrate that C$^3$A consistently outperforms LoRA and its variants across various fine-tuning tasks.</article>","contentLength":1050,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lifelong Learning of Video Diffusion Models From a Single Video Stream","url":"https://arxiv.org/abs/2406.04814","date":1751428800,"author":"","guid":179806,"unread":true,"content":"<article>arXiv:2406.04814v3 Announce Type: replace \nAbstract: This work demonstrates that training autoregressive video diffusion models from a single video stream$\\unicode{x2013}$resembling the experience of embodied agents$\\unicode{x2013}$is not only possible, but can also be as effective as standard offline training given the same number of gradient steps. Our work further reveals that this main result can be achieved using experience replay methods that only retain a subset of the preceding video stream. To support training and evaluation in this setting, we introduce four new datasets for streaming lifelong generative video modeling: Lifelong Bouncing Balls, Lifelong 3D Maze, Lifelong Drive, and Lifelong PLAICraft, each consisting of one million consecutive frames from environments of increasing complexity.</article>","contentLength":814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Model Confidence Estimation via Black-Box Access","url":"https://arxiv.org/abs/2406.04370","date":1751428800,"author":"","guid":179807,"unread":true,"content":"<article>arXiv:2406.04370v4 Announce Type: replace \nAbstract: Estimating uncertainty or confidence in the responses of a model can be significant in evaluating trust not only in the responses, but also in the model as a whole. In this paper, we explore the problem of estimating confidence for responses of large language models (LLMs) with simply black-box or query access to them. We propose a simple and extensible framework where, we engineer novel features and train a (interpretable) model (viz. logistic regression) on these features to estimate the confidence. We empirically demonstrate that our simple framework is effective in estimating confidence of Flan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&amp;A tasks as well as of Pegasus-large and BART-large on two benchmark summarization tasks with it surpassing baselines by even over $10\\%$ (on AUROC) in some cases. Additionally, our interpretable approach provides insight into features that are predictive of confidence, leading to the interesting and useful discovery that our confidence models built for one LLM generalize zero-shot across others on a given dataset.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unsupervised contrastive analysis for anomaly detection in brain MRIs via conditional diffusion models","url":"https://arxiv.org/abs/2406.00772","date":1751428800,"author":"","guid":179808,"unread":true,"content":"<article>arXiv:2406.00772v3 Announce Type: replace \nAbstract: Contrastive Analysis (CA) detects anomalies by contrasting patterns unique to a target group (e.g., unhealthy subjects) from those in a background group (e.g., healthy subjects). In the context of brain MRIs, existing CA approaches rely on supervised contrastive learning or variational autoencoders (VAEs) using both healthy and unhealthy data, but such reliance on target samples is challenging in clinical settings. Unsupervised Anomaly Detection (UAD) offers an alternative by learning a reference representation of healthy anatomy without the need for target samples. Deviations from this reference distribution can indicate potential anomalies. In this context, diffusion models have been increasingly adopted in UAD due to their superior performance in image generation compared to VAEs. Nonetheless, precisely reconstructing the anatomy of the brain remains a challenge. In this work, we propose an unsupervised framework to improve the reconstruction quality by training a self-supervised contrastive encoder on healthy images to extract meaningful anatomical features. These features are used to condition a diffusion model to reconstruct the healthy appearance of a given image, enabling interpretable anomaly localization via pixel-wise comparison. We validate our approach through a proof-of-concept on a facial image dataset and further demonstrate its effectiveness on four brain MRI datasets, achieving state-of-the-art anomaly localization performance on the NOVA benchmark.</article>","contentLength":1544,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Text-Guided Single Image Editing for Remote Sensing Images","url":"https://arxiv.org/abs/2405.05769","date":1751428800,"author":"","guid":179809,"unread":true,"content":"<article>arXiv:2405.05769v4 Announce Type: replace \nAbstract: Artificial intelligence generative content (AIGC) has significantly impacted image generation in the field of remote sensing. However, the equally important area of remote sensing image (RSI) editing has not received sufficient attention. Deep learning based editing methods generally involve two sequential stages: generation and editing. For natural images, these stages primarily rely on generative backbones pre-trained on large-scale benchmark datasets and text guidance facilitated by vision-language models (VLMs). However, it become less viable for RSIs: First, existing generative RSI benchmark datasets do not fully capture the diversity of RSIs, and is often inadequate for universal editing tasks. Second, the single text semantic corresponds to multiple image semantics, leading to the introduction of incorrect semantics. To solve above problems, this paper proposes a text-guided RSI editing method and can be trained using only a single image. A multi-scale training approach is adopted to preserve consistency without the need for training on extensive benchmarks, while leveraging RSI pre-trained VLMs and prompt ensembling (PE) to ensure accuracy and controllability. Experimental results on multiple RSI editing tasks show that the proposed method offers significant advantages in both CLIP scores and subjective evaluations compared to existing methods. Additionally, we explore the ability of the edited RSIs to support disaster assessment tasks in order to validate their practicality. Codes will be released at https://github.com/HIT-PhilipHan/remote_sensing_image_editing.</article>","contentLength":1650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delooping presented groups in homotopy type theory","url":"https://arxiv.org/abs/2405.03264","date":1751428800,"author":"","guid":179810,"unread":true,"content":"<article>arXiv:2405.03264v2 Announce Type: replace \nAbstract: Homotopy type theory is a logical setting based on Martin-L\\\"of type theory in which one can perform geometric constructions and proofs in a synthetic way. Namely, types can be interpreted as spaces up to homotopy and proofs as homotopy invariant constructions. In this context, loop spaces of pointed connected groupoids provide a natural representation of groups, and any group can be obtained as the loop space of such a type, which is then called a delooping of the group. There are two main methods for constructing the delooping of an arbitrary group G. The first one consists in describing it as a pointed higher inductive type, whereas the second one consists in taking the connected component of the principal G-torsor in the type of sets equipped with an action of G. We show here that, when a presentation (or even a generating set) is known for the group, simpler variants of those constructions can be used to build deloopings. The resulting types are more amenable to computations and lead to simpler meta-theoretic reasoning. Finally, we develop a type theoretical notion of 2-polygraph, which allows manipulating higher inductive types such as the ones involved in the description of deloopings. These allow us to investigate in this context a construction for the Cayley graph of a generated group and show that it encodes the relations of the group, as well as a Cayley complex which encodes relations between relations. Many of the developments performed in the article have been formalized using the cubical version of the Agda proof assistant.</article>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hardness and Tight Approximations of Demand Strip Packing","url":"https://arxiv.org/abs/2404.15917","date":1751428800,"author":"","guid":179811,"unread":true,"content":"<article>arXiv:2404.15917v2 Announce Type: replace \nAbstract: We settle the pseudo-polynomial complexity of the Demand Strip Packing (DSP) problem: Given a strip of fixed width and a set of items with widths and heights, the items must be placed inside the strip with the objective of minimizing the peak height. This problem has gained significant scientific interest due to its relevance in smart grids[Deppert et al.\\ APPROX'21, G\\'alvez et al.\\ APPROX'21]. Smart Grids are a modern form of electrical grid that provide opportunities for optimization. They are forecast to impact the future of energy provision significantly. Algorithms running in pseudo-polynomial time lend themselves to these applications as considered time intervals, such as days, are small. Moreover, such algorithms can provide superior approximation guarantees over those running in polynomial time. Consequently, they evoke scientific interest in related problems.\n  We prove that Demand Strip Packing is strongly NP-hard for approximation ratios below $5/4$. Through this proof, we provide novel insights into the relation of packing and scheduling problems. Using these insights, we show a series of frameworks that solve both Demand Strip Packing and Parallel Task Scheduling optimally when increasing the strip's width or number of machines. Such alterations to problems are known as resource augmentation. Applications are found when penalty costs are prohibitively large. Finally, we provide a pseudo-polynomial time approximation algorithm for DSP with an approximation ratio of $(5/4+\\varepsilon)$, which is nearly optimal assuming $P\\neq NP$. The construction of this algorithm provides several insights into the structure of DSP solutions and uses novel techniques to restructure optimal solutions.</article>","contentLength":1778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging","url":"https://arxiv.org/abs/2404.09158","date":1751428800,"author":"","guid":179812,"unread":true,"content":"<article>arXiv:2404.09158v3 Announce Type: replace \nAbstract: In this paper, we introduce StreakNet-Arch, a real-time, end-to-end binary-classification framework based on our self-developed Underwater Carrier LiDAR-Radar (UCLR) that embeds Self-Attention and our novel Double Branch Cross Attention (DBC-Attention) to enhance scatter suppression. Under controlled water tank validation conditions, StreakNet-Arch with Self-Attention or DBC-Attention outperforms traditional bandpass filtering and achieves higher $F_1$ scores than learning-based MP networks and CNNs at comparable model size and complexity. Real-time benchmarks on an NVIDIA RTX 3060 show a constant Average Imaging Time (54 to 84 ms) regardless of frame count, versus a linear increase (58 to 1,257 ms) for conventional methods. To facilitate further research, we contribute a publicly available streak-tube camera image dataset contains 2,695,168 real-world underwater 3D point cloud data. More importantly, we validate our UCLR system in a South China Sea trial, reaching an error of 46mm for 3D target at 1,000 m depth and 20 m range. Source code and data are available at https://github.com/BestAnHongjun/StreakNet .</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A system capable of verifiably and privately screening global DNA synthesis","url":"https://arxiv.org/abs/2403.14023","date":1751428800,"author":"","guid":179813,"unread":true,"content":"<article>arXiv:2403.14023v3 Announce Type: replace \nAbstract: Printing custom DNA sequences is essential to scientific and biomedical research, but the technology can be used to manufacture plagues as well as cures. Just as ink printers recognize and reject attempts to counterfeit money, DNA synthesizers and assemblers should deny unauthorized requests to make viral DNA that could be misused. There are three complications. First, we don't need to quickly update printers to deal with newly discovered currencies, whereas we regularly learn of new potential pandemic viruses and other biological threats. Second, convincing counterfeit bills can't be printed in small pieces and taped together, while preventing the distributed synthesis and subsequent re-assembly of controlled sequences will require tracking which DNA fragments have been ordered across all providers and benchtop devices while protecting legitimate customer privacy. Finally, counterfeiting can at worst undermine faith in currency, whereas unauthorized DNA synthesis could be used to deliberately cause pandemics. Here we describe SecureDNA, a free, privacy-preserving, and fully automated system capable of verifiably screening all DNA synthesis orders of 30+ nucleotides against an up-to-date database of controlled sequences, and its operational performance and specificity when applied to 67 million nucleotides of DNA synthesized by providers in the United States, Europe, and China.</article>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fully Differentiable Lagrangian Convolutional Neural Network for Physics-Informed Precipitation Nowcasting","url":"https://arxiv.org/abs/2402.10747","date":1751428800,"author":"","guid":179814,"unread":true,"content":"<article>arXiv:2402.10747v2 Announce Type: replace \nAbstract: This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods. It consists of a U-Net that dynamically produces mesoscale advection motion fields, a differentiable semi-Lagrangian extrapolation operator, and an advection-free U-Net capturing the growth and decay of precipitation over time. Using our approach, we successfully implement the Lagrangian convolutional neural network for precipitation nowcasting in a fully differentiable and GPU-accelerated manner. This allows for end-to-end training and inference, including the data-driven Lagrangian coordinate system transformation of the data at runtime. We evaluate the model and compare it with other related AI-based models both quantitatively and qualitatively in an extreme event case study. Based on our evaluation, LUPIN matches and even exceeds the performance of the chosen benchmarks, opening the door for other Lagrangian machine learning models.</article>","contentLength":1204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Soft Dice Confidence: A Near-Optimal Confidence Estimator for Selective Prediction in Semantic Segmentation","url":"https://arxiv.org/abs/2402.10665","date":1751428800,"author":"","guid":179815,"unread":true,"content":"<article>arXiv:2402.10665v3 Announce Type: replace \nAbstract: Selective prediction augments a model with the option to abstain from providing unreliable predictions. The key ingredient is a confidence score function, which should be directly related to the conditional risk. In the case of binary semantic segmentation, existing score functions either ignore the particularities of the evaluation metric or demand additional held-out data for tuning. We propose the Soft Dice Confidence (SDC), a simple, tuning-free confidence score function that directly aligns with the Dice coefficient metric. We prove that, under conditional independence, the SDC is near optimal: we establish upper and lower bounds on the ratio between the SDC and the ideal (intractable) confidence score function and show that these bounds are very close to 1. Experiments on six public medical-imaging benchmarks and on synthetic data corroborate our theoretical findings. In fact, SDC outperformed all prior confidence estimators from the literature in all of our experiments, including those that rely on additional data. These results position SDC as a reliable and efficient confidence estimator for selective prediction in semantic segmentation.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantifying analogy of concepts via ologs and wiring diagrams","url":"https://arxiv.org/abs/2402.01020","date":1751428800,"author":"","guid":179816,"unread":true,"content":"<article>arXiv:2402.01020v2 Announce Type: replace \nAbstract: We build on the theory of ontology logs (ologs) created by Spivak and Kent, and define a notion of wiring diagrams. In this article, a wiring diagram is a finite directed labelled graph. The labels correspond to types in an olog; they can also be interpreted as readings of sensors in an autonomous system. As such, wiring diagrams can be used as a framework for an autonomous system to form abstract concepts. We show that the graphs underlying skeleton wiring diagrams form a category. This allows skeleton wiring diagrams to be compared and manipulated using techniques from both graph theory and category theory. We also extend the usual definition of graph edit distance to the case of wiring diagrams by using operations only available to wiring diagrams, leading to a metric on the set of all skeleton wiring diagrams. In the end, we give an extended example on calculating the distance between two concepts represented by wiring diagrams, and explain how to apply our framework to any application domain.</article>","contentLength":1065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs","url":"https://arxiv.org/abs/2401.14640","date":1751428800,"author":"","guid":179817,"unread":true,"content":"<article>arXiv:2401.14640v2 Announce Type: replace \nAbstract: Attributed Question Answering (AQA) has attracted wide attention, but there are still several limitations in evaluating the attributions, including lacking fine-grained attribution categories, relying on manual annotations, and failing to compare attributions with only subtle differences. To bridge these gaps, we introduce Complex Attributed Question Answering (CAQA), a large-scale benchmark containing comprehensive attribution categories, automatically generated using Knowledge Graphs (KGs), and complex attribution scenarios. We have conducted extensive experiments to verify the effectiveness of CAQA, including the benchmarking of 25 automatic evaluators, their comparison with human evaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These experiments also lead to a series of important findings that can benefit the future research of AQA. All the codes and data are publicly accessible at https://github.com/HuuuNan/CAQA-Benchmark.</article>","contentLength":1017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Resilient is QUIC to Security and Privacy Attacks?","url":"https://arxiv.org/abs/2401.06657","date":1751428800,"author":"","guid":179818,"unread":true,"content":"<article>arXiv:2401.06657v3 Announce Type: replace \nAbstract: QUIC has rapidly evolved into a cornerstone transport protocol for secure, low-latency communications, yet its deployment continues to expose critical security and privacy vulnerabilities, particularly during connection establishment phases and via traffic analysis. This paper systematically revisits a comprehensive set of attacks on QUIC and emerging privacy threats. Building upon these observations, we critically analyze recent IETF mitigation efforts, including TLS Encrypted Client Hello (ECH), Oblivious HTTP (OHTTP) and MASQUE. We analyze how these mechanisms enhance privacy while introducing new operational risks, particularly under adversarial load. Additionally, we discuss emerging challenges posed by post-quantum cryptographic (PQC) handshakes, including handshake expansion and metadata leakage risks. Our analysis highlights ongoing gaps between theoretical defenses and practical deployments, and proposes new research directions focused on adaptive privacy mechanisms. Building on these insights, we propose future directions to ensure long-term security of QUIC and aim to guide its evolution as a robust, privacy-preserving, and resilient transport foundation for the next-generation Internet.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Correlated Equilibrium: Definition and Computation","url":"https://arxiv.org/abs/2311.17592","date":1751428800,"author":"","guid":179819,"unread":true,"content":"<article>arXiv:2311.17592v2 Announce Type: replace \nAbstract: We study N-player finite games with costs perturbed due to time-varying disturbances in the underlying system and to that end, we propose the concept of Robust Correlated Equilibrium that generalizes the definition of Correlated Equilibrium. Conditions under which the Robust Correlated Equilibrium exists are specified, and a decentralized algorithm for learning strategies that are optimal in the sense of Robust Correlated Equilibrium is proposed. The primary contribution of the paper is the convergence analysis of the algorithm and to that end, we propose a modification of the celebrated Blackwell's Approachability theorem to games with costs that are not just time-average, as in the original Blackwell's Approachability Theorem, but also include the time-average of previous algorithm iterates. The designed algorithm is applied to a practical water distribution network with pumps being the controllers and their costs being perturbed by uncertain consumption due to the consumers. Simulation results show that each controller achieves no regret, and empirical distributions converge to the Robust Correlated Equilibrium.</article>","contentLength":1185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Identifying the Truth of Global Model: A Generic Solution to Defend Against Byzantine and Backdoor Attacks in Federated Learning (full version)","url":"https://arxiv.org/abs/2311.10248","date":1751428800,"author":"","guid":179820,"unread":true,"content":"<article>arXiv:2311.10248v3 Announce Type: replace \nAbstract: Federated Learning (FL) enables multiple parties to train machine learning models collaboratively without sharing the raw training data. However, the federated nature of FL enables malicious clients to influence a trained model by injecting error model updates via Byzantine or backdoor attacks. To detect malicious model updates, a typical approach is to measure the distance between each model update and a \\textit{ground-truth model update}. To find such \\textit{ground-truth model updates}, existing defenses either require a benign root dataset on the server (e.g., FLTrust) or simply use trimmed mean or median as the threshold for clipping (e.g., FLAME). However, such benign root datasets are impractical, and the trimmed mean or median may also eliminate contributions from these underrepresented datasets.\n  In this paper, we propose a generic solution, namely FedTruth, to defend against model poisoning attacks in FL, where the \\textit{ground-truth model update} (i.e., the global model update) will be estimated among all the model updates with dynamic aggregation weights. Specifically, FedTruth does not have specific assumptions on the benign or malicious data distribution or access to a benign root dataset. Moreover, FedTruth considers the potential contributions from all benign clients. Our empirical results show that FedTruth can reduce the impacts of poisoned model updates against both Byzantine and backdoor attacks, and is also efficient in large-scale FL systems.</article>","contentLength":1544,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Identifying Systems with Symmetries using Equivariant Autoregressive Reservoir Computers","url":"https://arxiv.org/abs/2311.09511","date":1751428800,"author":"","guid":179821,"unread":true,"content":"<article>arXiv:2311.09511v3 Announce Type: replace \nAbstract: The investigation reported in this document focuses on identifying systems with symmetries using equivariant autoregressive reservoir computers. General results in structured matrix approximation theory are presented, exploring a two-fold approach. Firstly, a comprehensive examination of generic symmetry-preserving nonlinear time delay embedding is conducted. This involves analyzing time series data sampled from an equivariant system under study. Secondly, sparse least-squares methods are applied to discern approximate representations of the output coupling matrices. These matrices play a critical role in determining the nonlinear autoregressive representation of an equivariant system. The structural characteristics of these matrices are dictated by the set of symmetries inherent in the system. The document outlines prototypical algorithms derived from the described techniques, offering insight into their practical applications. Emphasis is placed on the significant improvement on structured identification precision when compared to classical reservoir computing methods for the simulation of equivariant dynamical systems.</article>","contentLength":1192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hardest Monotone Functions for Evolutionary Algorithms","url":"https://arxiv.org/abs/2311.07438","date":1751428800,"author":"","guid":179822,"unread":true,"content":"<article>arXiv:2311.07438v2 Announce Type: replace \nAbstract: In this paper we revisit the question how hard it can be for the $(1+1)$ Evolutionary Algorithm to optimize monotone pseudo-Boolean functions. By introducing a more pessimistic stochastic process, the partially-ordered evolutionary algorithm (PO-EA) model, Jansen first proved a runtime bound of $O(n^{3/2})$. More recently, Lengler, Martinsson and Steger improved this upper bound to $O(n \\log^2 n)$ by an entropy compression argument. In this work, we analyze monotone functions that may adversarially vary at each step of the optimization, so-called dynamic monotone functions. We introduce the function Switching Dynamic BinVal (SDBV) and prove, using a combinatorial argument, that for the $(1+1)$-EA with any mutation rate $p \\in [0,1]$, SDBV is drift minimizing within the class of dynamic monotone functions. We further show that the $(1+1)$-EA optimizes SDBV in $\\Theta(n^{3/2})$ generations. Therefore, our construction provides the first explicit example which realizes the pessimism of the \\poea model. Our simulations demonstrate matching runtimes for both the static and the self-adjusting $(1,\\lambda)$-EA and $(1+\\lambda)$-EA. Moreover, devising an example for fixed dimension, we illustrate that drift minimization does not equal maximal runtime beyond asymptotic analysis.</article>","contentLength":1343,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity","url":"https://arxiv.org/abs/2310.05175","date":1751428800,"author":"","guid":179823,"unread":true,"content":"<article>arXiv:2310.05175v4 Announce Type: replace \nAbstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL.</article>","contentLength":1883,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs","url":"https://arxiv.org/abs/2310.02277","date":1751428800,"author":"","guid":179824,"unread":true,"content":"<article>arXiv:2310.02277v3 Announce Type: replace \nAbstract: We present Junk DNA Hypothesis by adopting a novel task-centric angle for the pre-trained weights of large language models (LLMs). It has been believed that weights in LLMs contain significant redundancy, leading to the conception that a considerable chunk of the parameters can be removed by pruning without compromising performance. Contrary to this belief, this paper presents a counter-argument: small-magnitude weights of pre-trained model weights encode vital knowledge essential for tackling difficult downstream tasks - manifested as the monotonic relationship between the performance drop of downstream tasks across the difficulty spectrum, as we prune more pre-trained weights by magnitude. Moreover, we reveal that these seemingly inconsequential weights can result in irreparable loss of knowledge and performance degradation in difficult tasks, even when downstream continual training is allowed. Interestingly, our evaluations show that the other popular compression, namely quantization, fails to exhibit similar monotonic effect and does not as convincingly disentangle this task-difficulty information. To study formally, we introduce several quantifiable metrics to gauge the downstream task difficulty: (1) within the same task category, and (2) across different task categories. Our extensive experiments substantiate the Junk DNA Hypothesis across a diverse range of model sizes, tasks, datasets, and even pruning methods. Codes are available at: https://github.com/VITA-Group/Junk_DNA_Hypothesis.git.</article>","contentLength":1575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contrastive Conditional Latent Diffusion for Audio-visual Segmentation","url":"https://arxiv.org/abs/2307.16579","date":1751428800,"author":"","guid":179825,"unread":true,"content":"<article>arXiv:2307.16579v2 Announce Type: replace \nAbstract: We propose a contrastive conditional latent diffusion model for audio-visual segmentation (AVS) to thoroughly investigate the impact of audio, where the correlation between audio and the final segmentation map is modeled to guarantee the strong correlation between them. To achieve semantic-correlated representation learning, our framework incorporates a latent diffusion model. The diffusion model learns the conditional generation process of the ground-truth segmentation map, resulting in ground-truth aware inference during the denoising process at the test stage. As our model is conditional, it is vital to ensure that the conditional variable contributes to the model output. We thus extensively model the contribution of the audio signal by minimizing the density ratio between the conditional probability of the multimodal data, e.g. conditioned on the audio-visual data, and that of the unimodal data, e.g. conditioned on the audio data only. In this way, our latent diffusion model via density ratio optimization explicitly maximizes the contribution of audio for AVS, which can then be achieved with contrastive learning as a constraint, where the diffusion part serves as the main objective to achieve maximum likelihood estimation, and the density ratio optimization part imposes the constraint. By adopting this latent diffusion model via contrastive learning, we effectively enhance the contribution of audio for AVS. The effectiveness of our solution is validated through experimental results on the benchmark dataset. Code and results are online via our project page: https://github.com/OpenNLPLab/DiffusionAVS.</article>","contentLength":1683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Modified RISE Control for Quadrotors: Enhancing Trajectory Tracking Through Uncertainty Compensation","url":"https://arxiv.org/abs/2303.10270","date":1751428800,"author":"","guid":179826,"unread":true,"content":"<article>arXiv:2303.10270v2 Announce Type: replace \nAbstract: This paper presents an adaptive modified Robust Inverse of Signum Error (AM-RISE) control method, which achieves reliable trajectory tracking control for a quadrotor unmanned aerial vehicle. The proposed method systematically accounts for gyroscopic effects, rotor dynamics, parametric uncertainties, and external disturbances, ensuring robust performance across varying trajectory speeds. Through novel mathematical manipulation in the error system development, the quadrotor dynamics are expressed in a control-oriented form, which explicitly incorporates the uncertainty in the gyroscopic term and control actuation term. An adaptive modified RISE law is then designed to stabilize both the position and attitude loops of the quadrotor system. A rigorous Lyapunov-based analysis is utilized to prove asymptotic trajectory tracking, where the region of convergence can be made arbitrarily large through judicious control gain selection. Moreover, the stability analysis formally addresses gyroscopic effects and actuator uncertainty. To illustrate the performance of the control law, comparative numerical simulation results are provided, which demonstrate the improved closed-loop performance achieved under varying levels of parametric uncertainty, disturbance magnitudes and trajectory speeds.</article>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhanced Controllability of Diffusion Models via Feature Disentanglement and Realism-Enhanced Sampling Methods","url":"https://arxiv.org/abs/2302.14368","date":1751428800,"author":"","guid":179827,"unread":true,"content":"<article>arXiv:2302.14368v5 Announce Type: replace \nAbstract: As Diffusion Models have shown promising performance, a lot of efforts have been made to improve the controllability of Diffusion Models. However, how to train Diffusion Models to have the disentangled latent spaces and how to naturally incorporate the disentangled conditions during the sampling process have been underexplored. In this paper, we present a training framework for feature disentanglement of Diffusion Models (FDiff). We further propose two sampling methods that can boost the realism of our Diffusion Models and also enhance the controllability. Concisely, we train Diffusion Models conditioned on two latent features, a spatial content mask, and a flattened style embedding. We rely on the inductive bias of the denoising process of Diffusion Models to encode pose/layout information in the content feature and semantic/style information in the style feature. Regarding the sampling methods, we first generalize Composable Diffusion Models (GCDM) by breaking the conditional independence assumption to allow for some dependence between conditional inputs, which is shown to be effective in realistic generation in our experiments. Second, we propose timestep-dependent weight scheduling for content and style features to further improve the performance. We also observe better controllability of our proposed methods compared to existing methods in image manipulation and image translation.</article>","contentLength":1461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Higher-Order Weakest Precondition Transformers via a CPS Transformation","url":"https://arxiv.org/abs/2301.09997","date":1751428800,"author":"","guid":179828,"unread":true,"content":"<article>arXiv:2301.09997v2 Announce Type: replace \nAbstract: Weakest preconditions are a useful notion for program verification as they reduce a problem of program verification to a problem of constraint solving. Category-theoretic generalisations of weakest preconditions have been studied to capture various computational effects and various properties in a unified framework. In this paper, we propose a novel and general relationship between weakest precondition transformers and CPS transformations for higher-order functional languages with general computational effects and recursion. Technically, this gives a syntactic counterpart of the categorically-defined generic weakest precondition transformer in [Aguirre &amp; Katsumata, 2020]. The usefulness of our results is threefold. (1) Since CPS transformations purify effectful programs, various verification problems for effectful programs can be reduced to verification problems for pure programs. This syntactic reduction makes it easier to solve the verification problems and potentially facilitates combinations with other sophisticated verification methods tailored for pure programs. (2) We capture two existing verification methods, namely, verification of event sequences [Kobayashi et al., 2018] and expected cost [Avanzini et al., 2021] as instances of our framework. (3) Our results streamline the process of extending weakest precondition transformers for imperative programs to those for higher-order programs. We show two such extensions: analysis of higher moments of cost and the conditional weakest pre-expectation for higher-order probabilistic programs. These extensions demonstrate that our theoretical framework can produce novel verification methods.</article>","contentLength":1720,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quasi-symbolic Semantic Geometry over Transformer-based Variational AutoEncoder","url":"https://arxiv.org/abs/2210.06230","date":1751428800,"author":"","guid":179829,"unread":true,"content":"<article>arXiv:2210.06230v3 Announce Type: replace \nAbstract: Formal/symbolic semantics can provide canonical, rigid controllability and interpretability to sentence representations due to their \\textit{localisation} or \\textit{composition} property. How can we deliver such property to the current distributional sentence representations to control and interpret the generation of language models (LMs)? In this work, we theoretically frame the sentence semantics as the composition of \\textit{semantic role - word content} features and propose the formal semantic geometry. To inject such geometry into Transformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational AutoEncoder with a supervision approach, where the sentence generation can be manipulated and explained over low-dimensional latent Gaussian space. In addition, we propose a new probing algorithm to guide the movement of sentence vectors over such geometry. Experimental results reveal that the formal semantic geometry can potentially deliver better control and interpretation to sentence generation.</article>","contentLength":1072,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computing Threshold Budgets in Discrete-Bidding Games","url":"https://arxiv.org/abs/2210.02773","date":1751428800,"author":"","guid":179830,"unread":true,"content":"<article>arXiv:2210.02773v5 Announce Type: replace \nAbstract: In a two-player zero-sum graph game, the players move a token throughout a graph to produce an infinite play, which determines the winner of the game. Bidding games are graph games in which in each turn, an auction (bidding) determines which player moves the token: the players have budgets, and in each turn, both players simultaneously submit bids that do not exceed their available budgets, the higher bidder moves the token, and pays the bid to the lower bidder (called Richman bidding). We focus on discrete-bidding games, in which, motivated by practical applications, the granularity of the players' bids is restricted, e.g., bids must be given in cents.\n  A central quantity in bidding games is threshold budgets: a necessary and sufficient initial budget for winning the game. Previously, thresholds were shown to exist in parity games, but their structure was only understood for reachability games. Moreover, the previously-known algorithms have a worst-case exponential running time for both reachability and parity objectives, and output strategies that use exponential memory. We describe two algorithms for finding threshold budgets in parity discrete-bidding games. The first is a fixed-point algorithm. It reveals, for the first time, the structure of threshold budgets in parity discrete-bidding games. Based on this structure, we develop a second algorithm that shows that the problem of finding threshold budgets is in NP and coNP for both reachability and parity objectives. Moreover, our algorithm constructs strategies that use only linear memory.\n  This is a corrected version of the paper (arXiv:2210.02773v4) published originally on Jan 22, 2025.</article>","contentLength":1725,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conquering Ghosts: Relation Learning for Information Reliability Representation and End-to-End Robust Navigation","url":"https://arxiv.org/abs/2203.09952","date":1751428800,"author":"","guid":179831,"unread":true,"content":"<article>arXiv:2203.09952v4 Announce Type: replace \nAbstract: Environmental disturbances, such as sensor data noises, various lighting conditions, challenging weathers and external adversarial perturbations, are inevitable in real self-driving applications. Existing researches and testings have shown that they can severely influence the vehicles perception ability and performance, one of the main issue is the false positive detection, i.e., the ghost object which is not real existed or occurs in the wrong position (such as a non-existent vehicle). Traditional navigation methods tend to avoid every detected objects for safety, however, avoiding a ghost object may lead the vehicle into a even more dangerous situation, such as a sudden break on the highway. Considering the various disturbance types, it is difficult to address this issue at the perceptual aspect. A potential solution is to detect the ghost through relation learning among the whole scenario and develop an integrated end-to-end navigation system. Our underlying logic is that the behavior of all vehicles in the scene is influenced by their neighbors, and normal vehicles behave in a logical way, while ghost vehicles do not. By learning the spatio-temporal relation among surrounding vehicles, an information reliability representation is learned for each detected vehicle and then a robot navigation network is developed. In contrast to existing works, we encourage the network to learn how to represent the reliability and how to aggregate all the information with uncertainties by itself, thus increasing the efficiency and generalizability. To the best of the authors knowledge, this paper provides the first work on using graph relation learning to achieve end-to-end robust navigation in the presence of ghost vehicles. Simulation results in the CARLA platform demonstrate the feasibility and effectiveness of the proposed method in various scenarios.</article>","contentLength":1925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advancing Lung Disease Diagnosis in 3D CT Scans","url":"https://arxiv.org/abs/2507.00993","date":1751428800,"author":"","guid":179832,"unread":true,"content":"<article>arXiv:2507.00993v1 Announce Type: cross \nAbstract: To enable more accurate diagnosis of lung disease in chest CT scans, we propose a straightforward yet effective model. Firstly, we analyze the characteristics of 3D CT scans and remove non-lung regions, which helps the model focus on lesion-related areas and reduces computational cost. We adopt ResNeSt50 as a strong feature extractor, and use a weighted cross-entropy loss to mitigate class imbalance, especially for the underrepresented squamous cell carcinoma category. Our model achieves a Macro F1 Score of 0.80 on the validation set of the Fair Disease Diagnosis Challenge, demonstrating its strong performance in distinguishing between different lung conditions.</article>","contentLength":721,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stable skeleton integral equations for general coefficient Helmholtz transmission problems","url":"https://arxiv.org/abs/2507.00991","date":1751428800,"author":"","guid":179833,"unread":true,"content":"<article>arXiv:2507.00991v1 Announce Type: cross \nAbstract: A novel variational formulation of layer potentials and boundary integral operators generalizes their classical construction by Green's functions, which are not explicitly available for Helmholtz problems with variable coefficients. Wavenumber explicit estimates and properties like jump conditions follow directly from their variational definition and enable a non-local (``integral'') formulation of acoustic transmission problems (TP) with piecewise Lipschitz coefficients. We obtain the well-posedness of the integral equations directly from the stability of the underlying TP. The simultaneous analysis for general dimensions and complex wavenumbers (in this paper) imposes an artificial boundary on the external Helmholtz problem and employs recent insights into the associated Dirichlet-to-Neumann map.</article>","contentLength":860,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DMCIE: Diffusion Model with Concatenation of Inputs and Errors to Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images","url":"https://arxiv.org/abs/2507.00983","date":1751428800,"author":"","guid":179834,"unread":true,"content":"<article>arXiv:2507.00983v1 Announce Type: cross \nAbstract: Accurate segmentation of brain tumors in MRI scans is essential for reliable clinical diagnosis and effective treatment planning. Recently, diffusion models have demonstrated remarkable effectiveness in image generation and segmentation tasks. This paper introduces a novel approach to corrective segmentation based on diffusion models. We propose DMCIE (Diffusion Model with Concatenation of Inputs and Errors), a novel framework for accurate brain tumor segmentation in multi-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation mask, from which an error map is generated by identifying the differences between the prediction and the ground truth. The error map, concatenated with the original MRI images, are used to guide a diffusion model. Using multimodal MRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation accuracy by focusing on misclassified regions, guided by the original inputs. Evaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art diffusion-based segmentation methods, achieving a Dice Score of 93.46 and an HD95 of 5.94 mm. These results highlight the effectiveness of error-guided diffusion in producing precise and reliable brain tumor segmentations.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Atmospheric model-trained machine learning selection and classification of ultracool TY dwarfs","url":"https://arxiv.org/abs/2507.00957","date":1751428800,"author":"","guid":179835,"unread":true,"content":"<article>arXiv:2507.00957v1 Announce Type: cross \nAbstract: The T and Y spectral classes represent the coolest and lowest-mass population of brown dwarfs, yet their census remains incomplete due to limited statistics. Existing detection frameworks are often constrained to identifying M, L, and early T dwarfs, owing to the sparse observational sample of ultracool dwarfs (UCDs) at later types. This paper presents a novel machine learning framework capable of detecting and classifying late-T and Y dwarfs, trained entirely on synthetic photometry from atmospheric models. Utilizing grids from the ATMO 2020 and Sonora Bobcat models, I produce a training dataset over two orders of magnitude larger than any empirical set of &gt;T6 UCDs. Polynomial color relations fitted to the model photometry are used to assign spectral types to these synthetic models, which in turn train an ensemble of classifiers to identify and classify the spectral type of late UCDs. The model is highly performant when validating on both synthetic and empirical datasets, verifying catalogs of known UCDs with object classification metrics &gt;99% and an average spectral type precision within 0.35 +/- 0.37 subtypes. Application of the model to a 1.5 degree region around Pisces and the UKIDSS UDS field results in the discovery of one previously uncatalogued T8.2 candidate, demonstrating the ability of this model-trained approach in discovering faint, late-type UCDs from photometric catalogs.</article>","contentLength":1461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Sentences to Sequences: Rethinking Languages in Biological System","url":"https://arxiv.org/abs/2507.00953","date":1751428800,"author":"","guid":179836,"unread":true,"content":"<article>arXiv:2507.00953v1 Announce Type: cross \nAbstract: The paradigm of large language models in natural language processing (NLP) has also shown promise in modeling biological languages, including proteins, RNA, and DNA. Both the auto-regressive generation paradigm and evaluation metrics have been transferred from NLP to biological sequence modeling. However, the intrinsic structural correlations in natural and biological languages differ fundamentally. Therefore, we revisit the notion of language in biological systems to better understand how NLP successes can be effectively translated to biological domains. By treating the 3D structure of biomolecules as the semantic content of a sentence and accounting for the strong correlations between residues or bases, we highlight the importance of structural evaluation and demonstrate the applicability of the auto-regressive paradigm in biological language modeling. Code can be found at \\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}</article>","contentLength":1008,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeing is not believing in limited visibility cops and robbers","url":"https://arxiv.org/abs/2507.00941","date":1751428800,"author":"","guid":179837,"unread":true,"content":"<article>arXiv:2507.00941v1 Announce Type: cross \nAbstract: We consider the model of limited visibility Cops and Robbers, where the cops can only see within their $l$-neighbourhood. We prove that the number of cops needed to see the robber can be arbitrarily smaller than the number needed to capture the robber, answering an open question from the literature. We then consider how close we can get to seeing the robber when we do not have enough cops, along with a probabilistic interpretation.</article>","contentLength":486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection","url":"https://arxiv.org/abs/2507.00903","date":1751428800,"author":"","guid":179838,"unread":true,"content":"<article>arXiv:2507.00903v1 Announce Type: cross \nAbstract: Objectives Parametric tissue mapping enables quantitative cardiac tissue characterization but is limited by inter-observer variability during manual delineation. Traditional approaches relying on average relaxation values and single cutoffs may oversimplify myocardial complexity. This study evaluates whether deep learning (DL) can achieve segmentation accuracy comparable to inter-observer variability, explores the utility of statistical features beyond mean T1/T2 values, and assesses whether machine learning (ML) combining multiple features enhances disease detection. Materials &amp; Methods T1 and T2 maps were manually segmented. The test subset was independently annotated by two observers, and inter-observer variability was assessed. A DL model was trained to segment left ventricle blood pool and myocardium. Average (A), lower quartile (LQ), median (M), and upper quartile (UQ) were computed for the myocardial pixels and employed in classification by applying cutoffs or in ML. Dice similarity coefficient (DICE) and mean absolute percentage error evaluated segmentation performance. Bland-Altman plots assessed inter-user and model-observer agreement. Receiver operating characteristic analysis determined optimal cutoffs. Pearson correlation compared features from model and manual segmentations. F1-score, precision, and recall evaluated classification performance. Wilcoxon test assessed differences between classification methods, with p &lt; 0.05 considered statistically significant. Results 144 subjects were split into training (100), validation (15) and evaluation (29) subsets. Segmentation model achieved a DICE of 85.4%, surpassing inter-observer agreement. Random forest applied to all features increased F1-score (92.7%, p &lt; 0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining multiple features with ML improves disease detection.</article>","contentLength":1925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An in depth look at the Procrustes-Wasserstein distance: properties and barycenters","url":"https://arxiv.org/abs/2507.00894","date":1751428800,"author":"","guid":179839,"unread":true,"content":"<article>arXiv:2507.00894v1 Announce Type: cross \nAbstract: Due to its invariance to rigid transformations such as rotations and reflections, Procrustes-Wasserstein (PW) was introduced in the literature as an optimal transport (OT) distance, alternative to Wasserstein and more suited to tasks such as the alignment and comparison of point clouds. Having that application in mind, we carefully build a space of discrete probability measures and show that over that space PW actually is a distance. Algorithms to solve the PW problems already exist, however we extend the PW framework by discussing and testing several initialization strategies. We then introduce the notion of PW barycenter and detail an algorithm to estimate it from the data. The result is a new method to compute representative shapes from a collection of point clouds. We benchmark our method against existing OT approaches, demonstrating superior performance in scenarios requiring precise alignment and shape preservation. We finally show the usefulness of the PW barycenters in an archaeological context. Our results highlight the potential of PW in boosting 2D and 3D point cloud analysis for machine learning and computational geometry applications.</article>","contentLength":1216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Swarm-based optimization with jumps: a kinetic BGK framework and convergence analysis","url":"https://arxiv.org/abs/2507.00871","date":1751428800,"author":"","guid":179840,"unread":true,"content":"<article>arXiv:2507.00871v1 Announce Type: cross \nAbstract: Metaheuristic algorithms are powerful tools for global optimization, particularly for non-convex and non-differentiable problems where exact methods are often impractical. Particle-based optimization methods, inspired by swarm intelligence principles, have shown effectiveness due to their ability to balance exploration and exploitation within the search space. In this work, we introduce a novel particle-based optimization algorithm where velocities are updated via random jumps, a strategy commonly used to enhance stochastic exploration. We formalize this approach by describing the dynamics through a kinetic modelling of BGK type, offering a unified framework that accommodates general noise distributions, including heavy-tailed ones like Cauchy. Under suitable parameter scaling, the model reduces to the Consensus-Based Optimization (CBO) dynamics. For non-degenerate Gaussian noise in bounded domains, we prove propagation of chaos and convergence towards minimizers. Numerical results on benchmark problems validate the approach and highlight its connection to CBO.</article>","contentLength":1128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Template-Fitting Meets Deep Learning: Redshift Estimation Using Physics-Guided Neural Networks","url":"https://arxiv.org/abs/2507.00866","date":1751428800,"author":"","guid":179841,"unread":true,"content":"<article>arXiv:2507.00866v1 Announce Type: cross \nAbstract: Accurate photometric redshift estimation is critical for observational cosmology, especially in large-scale surveys where spectroscopic measurements are impractical. Traditional approaches include template fitting and machine learning, each with distinct strengths and limitations. We present a hybrid method that integrates template fitting with deep learning using physics-guided neural networks. By embedding spectral energy distribution templates into the network architecture, our model encodes physical priors into the training process. The system employs a multimodal design, incorporating cross-attention mechanisms to fuse photometric and image data, along with Bayesian layers for uncertainty estimation. We evaluate our model on the publicly available PREML dataset, which includes approximately 400,000 galaxies from the Hyper Suprime-Cam PDR3 release, with 5-band photometry, multi-band imaging, and spectroscopic redshifts. Our approach achieves an RMS error of 0.0507, a 3-sigma catastrophic outlier rate of 0.13%, and a bias of 0.0028. The model satisfies two of the three LSST photometric redshift requirements for redshifts below 3. These results highlight the potential of combining physically motivated templates with data-driven models for robust redshift estimation in upcoming cosmological surveys.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Evolution of Altruistic Rationality Provides a Solution to Social Dilemmas via Rational Reciprocity","url":"https://arxiv.org/abs/2507.00858","date":1751428800,"author":"","guid":179842,"unread":true,"content":"<article>arXiv:2507.00858v1 Announce Type: cross \nAbstract: Decades of scientific inquiry have sought to understand how evolution fosters cooperation, a concept seemingly at odds with the belief that evolution should produce rational, self-interested individuals. Most previous work has focused on the evolution of cooperation among boundedly rational individuals whose decisions are governed by behavioral rules that do not need to be rational. Here, using an evolutionary model, we study how altruism can evolve in a community of rational agents and promote cooperation. We show that in both well-mixed and structured populations, a population of objectively rational agents is readily invaded by mutant individuals who make rational decisions but evolve a distorted (i.e., subjective) perception of their payoffs. This promotes behavioral diversity and gives rise to the evolution of rational, other-regarding agents who naturally solve all the known strategic problems of two-person, two-strategy games by perceiving their games as pure coordination games.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ranking Quantilized Mean-Field Games with an Application to Early-Stage Venture Investments","url":"https://arxiv.org/abs/2507.00853","date":1751428800,"author":"","guid":179843,"unread":true,"content":"<article>arXiv:2507.00853v1 Announce Type: cross \nAbstract: Quantilized mean-field game models involve quantiles of the population's distribution. We study a class of such games with a capacity for ranking games, where the performance of each agent is evaluated based on its terminal state relative to the population's $\\alpha$-quantile value, $\\alpha \\in (0,1)$. This evaluation criterion is designed to select the top $(1-\\alpha)\\%$ performing agents. We provide two formulations for this competition: a target-based formulation and a threshold-based formulation. In the former and latter formulations, to satisfy the selection condition, each agent aims for its terminal state to be \\textit{exactly} equal and \\textit{at least} equal to the population's $\\alpha$-quantile value, respectively.\n  For the target-based formulation, we obtain an analytic solution and demonstrate the $\\epsilon$-Nash property for the asymptotic best-response strategies in the $N$-player game. Specifically, the quantilized mean-field consistency condition is expressed as a set of forward-backward ordinary differential equations, characterizing the $\\alpha$-quantile value at equilibrium. For the threshold-based formulation, we obtain a semi-explicit solution and numerically solve the resulting quantilized mean-field consistency condition.\n  Subsequently, we propose a new application in the context of early-stage venture investments, where a venture capital firm financially supports a group of start-up companies engaged in a competition over a finite time horizon, with the goal of selecting a percentage of top-ranking ones to receive the next round of funding at the end of the time horizon. We present the results and interpretations of numerical experiments for both formulations discussed in this context and show that the target-based formulation provides a very good approximation for the threshold-based formulation.</article>","contentLength":1906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection","url":"https://arxiv.org/abs/2507.00832","date":1751428800,"author":"","guid":179844,"unread":true,"content":"<article>arXiv:2507.00832v1 Announce Type: cross \nAbstract: Introduction: Deep learning (DL) models can help detect intracranial aneurysms on CTA, but high false positive (FP) rates remain a barrier to clinical translation, despite improvement in model architectures and strategies like detection threshold tuning. We employed an automated, anatomy-based, heuristic-learning hybrid artery-vein segmentation post-processing method to further reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D convolutional neural network-transformer hybrid (3D-CNN-TR), were trained with 1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143 held-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and cavernous venous sinus (CVS) segmentation masks were applied to remove possible FPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3) vein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more than artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79 false-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were commonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%; 3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular (CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing CPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without reducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from 1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable post-processing can improve DL-based aneurysm detection model performance. More broadly, automated, domain-informed, hybrid heuristic-learning processing holds promise for improving the performance and clinical acceptance of aneurysm detection models.</article>","contentLength":1825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Speedups for Polynomial-Time Dynamic Programming Algorithms","url":"https://arxiv.org/abs/2507.00823","date":1751428800,"author":"","guid":179845,"unread":true,"content":"<article>arXiv:2507.00823v1 Announce Type: cross \nAbstract: We introduce a quantum dynamic programming framework that allows us to directly extend to the quantum realm a large body of classical dynamic programming algorithms. The corresponding quantum dynamic programming algorithms retain the same space complexity as their classical counterpart, while achieving a computational speedup. For a combinatorial (search or optimization) problem $\\mathcal P$ and an instance $I$ of $\\mathcal P$, such a speedup can be expressed in terms of the average degree $\\delta$ of the dependency digraph $G_{\\mathcal{P}}(I)$ of $I$, determined by a recursive formulation of $\\mathcal P$. The nodes of this graph are the subproblems of $\\mathcal P$ induced by $I$ and its arcs are directed from each subproblem to those on whose solution it relies. In particular, our framework allows us to solve the considered problems in $\\tilde{O}(|V(G_{\\mathcal{P}}(I))| \\sqrt{\\delta})$ time. As an example, we obtain a quantum version of the Bellman-Ford algorithm for computing shortest paths from a single source vertex to all the other vertices in a weighted $n$-vertex digraph with $m$ edges that runs in $\\tilde{O}(n\\sqrt{nm})$ time, which improves the best known classical upper bound when $m \\in \\Omega(n^{1.4})$.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Research on Improving the High Precision and Lightweight Diabetic Retinopathy Detection of YOLOv8n","url":"https://arxiv.org/abs/2507.00780","date":1751428800,"author":"","guid":179846,"unread":true,"content":"<article>arXiv:2507.00780v1 Announce Type: cross \nAbstract: Early detection and diagnosis of diabetic retinopathy is one of the current research focuses in ophthalmology. However, due to the subtle features of micro-lesions and their susceptibility to background interference, ex-isting detection methods still face many challenges in terms of accuracy and robustness. To address these issues, a lightweight and high-precision detection model based on the improved YOLOv8n, named YOLO-KFG, is proposed. Firstly, a new dynamic convolution KWConv and C2f-KW module are designed to improve the backbone network, enhancing the model's ability to perceive micro-lesions. Secondly, a fea-ture-focused diffusion pyramid network FDPN is designed to fully integrate multi-scale context information, further improving the model's ability to perceive micro-lesions. Finally, a lightweight shared detection head GSDHead is designed to reduce the model's parameter count, making it more deployable on re-source-constrained devices. Experimental results show that compared with the base model YOLOv8n, the improved model reduces the parameter count by 20.7%, increases mAP@0.5 by 4.1%, and improves the recall rate by 7.9%. Compared with single-stage mainstream algorithms such as YOLOv5n and YOLOv10n, YOLO-KFG demonstrates significant advantages in both detection accuracy and efficiency.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End","url":"https://arxiv.org/abs/2507.00755","date":1751428800,"author":"","guid":179847,"unread":true,"content":"<article>arXiv:2507.00755v1 Announce Type: cross \nAbstract: This paper presents a circuit-algorithm co-design framework for learnable analog front-end (AFE) in audio signal classification. Designing AFE and backend classifiers separately is a common practice but non-ideal, as shown in this paper. Instead, this paper proposes a joint optimization of the backend classifier with the AFE's transfer function to achieve system-level optimum. More specifically, the transfer function parameters of an analog bandpass filter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training loop for the classifier. Using a co-design loss function LBPF, this work shows superior optimization of both the filter bank and the classifier. Implemented in open-source SKY130 130nm CMOS process, the optimized design achieved 90.5%-94.2% accuracy for 10-keyword classification task across a wide range of input signal SNR from 5 dB to 20 dB, with only 22k classifier parameters. Compared to conventional approach, the proposed audio AFE achieves 8.7% and 12.9% reduction in power and capacitor area respectively.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SINDy on slow manifolds","url":"https://arxiv.org/abs/2507.00747","date":1751428800,"author":"","guid":179848,"unread":true,"content":"<article>arXiv:2507.00747v1 Announce Type: cross \nAbstract: The sparse identification of nonlinear dynamics (SINDy) has been established as an effective method to learn interpretable models of dynamical systems from data. However, for high-dimensional slow-fast dynamical systems, the regression problem becomes simultaneously computationally intractable and ill-conditioned. Although, in principle, modeling only the dynamics evolving on the underlying slow manifold addresses both of these challenges, the truncated fast variables have to be compensated by including higher-order nonlinearities as candidate terms for the model, leading to an explosive growth in the size of the SINDy library. In this work, we develop a SINDy variant that is able to robustly and efficiently identify slow-fast dynamics in two steps: (i) identify the slow manifold, that is, an algebraic equation for the fast variables as functions of the slow ones, and (ii) learn a model for the dynamics of the slow variables restricted to the manifold. Critically, the equation learned in (i) is leveraged to build a manifold-informed function library for (ii) that contains only essential higher-order nonlinearites as candidate terms. Rather than containing all monomials of up to a certain degree, the resulting custom library is a sparse subset of the latter that is tailored to the specific problem at hand. The approach is demonstrated on numerical examples of a snap-through buckling beam and the flow over a NACA 0012 airfoil. We find that our method significantly reduces both the condition number and the size of the SINDy library, thus enabling accurate identification of the dynamics on slow manifolds.</article>","contentLength":1679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery","url":"https://arxiv.org/abs/2507.00743","date":1751428800,"author":"","guid":179849,"unread":true,"content":"<article>arXiv:2507.00743v1 Announce Type: cross \nAbstract: In this study, we developed deep learning-based method to classify the type of surgery performed for epiretinal membrane (ERM) removal, either internal limiting membrane (ILM) removal or ERM-alone removal. Our model, based on the ResNet18 convolutional neural network (CNN) architecture, utilizes postoperative optical coherence tomography (OCT) center scans as inputs. We evaluated the model using both original scans and scans preprocessed with energy crop and wavelet denoising, achieving 72% accuracy on preprocessed inputs, outperforming the 66% accuracy achieved on original scans. To further improve accuracy, we integrated tunable wavelet units with two key adaptations: Orthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect Reconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units allowed the model to automatically adjust filter coefficients during training and were incorporated into downsampling, stride-two convolution, and pooling layers, enhancing its ability to distinguish between ERM-ILM removal and ERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU increasing performance to 78%. Performance comparisons showed that our AI model outperformed a trained human grader, who achieved only 50% accuracy in classifying the removal surgery types from postoperative OCT scans. These findings highlight the potential of CNN based models to improve clinical decision-making by providing more accurate and reliable classifications. To the best of our knowledge, this is the first work to employ tunable wavelets for classifying different types of ERM removal surgery.</article>","contentLength":1682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A simplified unified wave-particle method for diatomic gases with rotational and vibrational non-equilibrium","url":"https://arxiv.org/abs/2507.00720","date":1751428800,"author":"","guid":179850,"unread":true,"content":"<article>arXiv:2507.00720v1 Announce Type: cross \nAbstract: The hypersonic flow around near-space vehicles constitutes a multi-scale flow problem. Due to insufficient molecular collisions to achieve equilibrium, rarefied gas effects are present in the flow field. Thus, numerical methods capable of accurately resolving multi-scale flows are required. Furthermore, high-temperature gas effects in hypersonic flows mean vibrational excitation of polyatomic molecules. Consequently, numerical methods accounting for non-equilibrium in rotational and vibrational internal energy modes are required. This study derives a quantified model-competition (QMC) mechanism for diatomic gases with rotational and vibrational non-equilibrium, starting from integral solutions of kinetic model equations with rotational and vibrational energy. The QMC mechanism categorize collisional and free-transport particles in cell, applying computational weighting based on their local scale regimes. We developed a simplified unified wave-particle (SUWP) method for diatomic gases based on QMC mechanism. For the macroscopic of the method, a three-temperature model accounting for rotational and vibrational energy is incorporated into both the kinetic inviscid flux scheme and {Navier-Stokes} solvers. For the microscopic of the method, a collisionless DSMC solver is employed to resolve non-equilibrium flow physics. This work validates the proposed SUWP method with rotational and vibrational non-equilibrium through benchmark cases, including shock tube, shock structures, flow past a cylinder, Apollo 6 command module and space station Mir. Compared to the DSMC and deterministic methods, the SUWP method exhibits favorable computational efficiency while maintaining accuracy.</article>","contentLength":1750,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Guided Unconditional and Conditional Generative Models for Super-Resolution and Inference of Quasi-Geostrophic Turbulence","url":"https://arxiv.org/abs/2507.00719","date":1751428800,"author":"","guid":179851,"unread":true,"content":"<article>arXiv:2507.00719v1 Announce Type: cross \nAbstract: Typically, numerical simulations of the ocean, weather, and climate are coarse, and observations are sparse and gappy. In this work, we apply four generative diffusion modeling approaches to super-resolution and inference of forced two-dimensional quasi-geostrophic turbulence on the beta-plane from coarse, sparse, and gappy observations. Two guided approaches minimally adapt a pre-trained unconditional model: SDEdit modifies the initial condition, and Diffusion Posterior Sampling (DPS) modifies the reverse diffusion process score. The other two conditional approaches, a vanilla variant and classifier-free guidance, require training with paired high-resolution and observation data. We consider eight test cases spanning: two regimes, eddy and anisotropic-jet turbulence; two Reynolds numbers, 10^3 and 10^4; and two observation types, 4x coarse-resolution fields and coarse, sparse and gappy observations. Our comprehensive skill metrics include norms of the reconstructed vorticity fields, turbulence statistical quantities, and quantification of the super-resolved probabilistic ensembles and their errors. We also study the sensitivity to tuning parameters such as guidance strength. Results show that SDEdit generates unphysical fields, while DPS generates reasonable reconstructions at low computational cost but with smoothed fine-scale features. Both conditional approaches require re-training, but they reconstruct missing fine-scale features, are cycle-consistent with observations, and possess the correct statistics such as energy spectra. Further, their mean model errors are highly correlated with and predictable from their ensemble standard deviations. Results highlight the trade-offs between ease of implementation, fidelity (sharpness), and cycle-consistency of the diffusion models, and offer practical guidance for deployment in geophysical inverse problems.</article>","contentLength":1937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"General Perturbation Resilient Dynamic String-Averaging for Inconsistent Problems with Superiorization","url":"https://arxiv.org/abs/2507.00717","date":1751428800,"author":"","guid":179852,"unread":true,"content":"<article>arXiv:2507.00717v1 Announce Type: cross \nAbstract: In this paper we introduce a General Dynamic String-Averaging (GDSA) iterative scheme and investigate its convergence properties in the inconsistent case, that is, when the input operators don't have a common fixed point. The Dynamic String-Averaging Projection (DSAP) algorithm itself was introduced in an 2013 paper, where its strong convergence and bounded perturbation resilience were studied in the consistent case (that is, when the sets under consideration had a nonempty intersection). Results involving combination of the DSAP method with superiorization, were presented in 2015. The proof of the weak convergence of our GDSA method is based on the notion of \"strong coherence\" of sequences of operators that was introduced in 2019. This is an improvement of the property of \"coherence\" of sequences of operators introduced in 2001 by Bauschke and Combettes. Strong coherence provides a more convenient sufficient convergence condition for methods that employ infinite sequences of operators and it turns out to be a useful general tool when applied to proving the convergence of many iterative methods. In this paper we combine the ideas of both dynamic string-averaging and strong coherence, in order to analyze our GDSA method for a general class of operators and its bounded perturbation resilience in the inconsistent case with weak and strong convergence. We then discuss an application of the GDSA method to the Superiorization Methodology, developing results on the behavior of its superiorized version.</article>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer","url":"https://arxiv.org/abs/2507.00683","date":1751428800,"author":"","guid":179853,"unread":true,"content":"<article>arXiv:2507.00683v1 Announce Type: cross \nAbstract: The recently proposed physics-based framework by Huo and Johnson~\\cite{huo2024capturing} models the attention mechanism of Large Language Models (LLMs) as an interacting two-body spin system, offering a first-principles explanation for phenomena like repetition and bias. Building on this hypothesis, we extract the complete Query-Key weight matrices from a production-grade GPT-2 model and derive the corresponding effective Hamiltonian for every attention head. From these Hamiltonians we obtain analytic \\textit{phase boundaries} logit gap criteria that predict which token should dominate the next-token distribution for a given context. A systematic evaluation on 144 heads across 20 factual-recall prompts reveals a strong negative correlation between the theoretical logit gaps and the model's empirical token rankings ($r\\approx-0.70$, $p&lt;10^{-3}$).Targeted ablations further show that suppressing the heads most aligned with the spin-bath predictions induces the anticipated shifts in output probabilities, confirming a causal link rather than a coincidental association. Taken together, our findings provide the first strong empirical evidence for the spin-bath analogy in a production-grade model. This validation not only furnishes a tractable, physics-inspired lens for interpretability but also provides the groundwork for novel generative models, bridging the gap between theoretical condensed matter physics and AI.</article>","contentLength":1482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Machines Philosophize?","url":"https://arxiv.org/abs/2507.00675","date":1751428800,"author":"","guid":179854,"unread":true,"content":"<article>arXiv:2507.00675v1 Announce Type: cross \nAbstract: Inspired by the Turing test, we present a novel methodological framework to assess the extent to which a population of machines mirrors the philosophical views of a population of humans. The framework consists of three steps: (i) instructing machines to impersonate each human in the population, reflecting their backgrounds and beliefs, (ii) administering a questionnaire covering various philosophical positions to both humans and machines, and (iii) statistically analyzing the resulting responses. We apply this methodology to the debate on scientific realism, a long-standing philosophical inquiry exploring the relationship between science and reality. By considering the outcome of a survey of over 500 human participants, including both physicists and philosophers of science, we generate their machine personas using an artificial intelligence engine based on a large-language generative model. We reveal that the philosophical views of a population of machines are, on average, similar to those endorsed by a population of humans, irrespective of whether they are physicists or philosophers of science. As compared to humans, however, machines exhibit a weaker inclination toward scientific realism and a stronger coherence in their philosophical positions. Given the observed similarities between the populations of humans and machines, this methodological framework may offer unprecedented opportunities for advancing research in experimental philosophy by replacing human participants with their machine-impersonated counterparts, possibly mitigating the efficiency and reproducibility issues that affect survey-based empirical studies.</article>","contentLength":1700,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays","url":"https://arxiv.org/abs/2507.00673","date":1751428800,"author":"","guid":179855,"unread":true,"content":"<article>arXiv:2507.00673v1 Announce Type: cross \nAbstract: Image segmentation plays a vital role in the medical field by isolating organs or regions of interest from surrounding areas. Traditionally, segmentation models are trained on a specific organ or a disease, limiting their ability to handle other organs and diseases. At present, few advanced models can perform multi-organ or multi-disease segmentation, offering greater flexibility. Also, recently, prompt-based image segmentation has gained attention as a more flexible approach. It allows models to segment areas based on user-provided prompts. Despite these advances, there has been no dedicated work on prompt-based interactive multi-organ and multi-disease segmentation, especially for Chest X-rays. This work presents two main contributions: first, generating doodle prompts by medical experts of a collection of datasets from multiple sources with 23 classes, including 6 organs and 17 diseases, specifically designed for prompt-based Chest X-ray segmentation. Second, we introduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple organs and diseases from Chest X-rays. The model incorporates multi-stage feature fusion, enabling it to combine features from various network layers for better spatial and semantic understanding, enhancing segmentation accuracy. Compared to existing pre-trained models for prompt-based image segmentation, our model scores well, providing a reliable solution for segmenting Chest X-rays based on user prompts.</article>","contentLength":1521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harnessing the Power of Reinforcement Learning for Adaptive MCMC","url":"https://arxiv.org/abs/2507.00671","date":1751428800,"author":"","guid":179856,"unread":true,"content":"<article>arXiv:2507.00671v1 Announce Type: cross \nAbstract: Sampling algorithms drive probabilistic machine learning, and recent years have seen an explosion in the diversity of tools for this task. However, the increasing sophistication of sampling algorithms is correlated with an increase in the tuning burden. There is now a greater need than ever to treat the tuning of samplers as a learning task in its own right. In a conceptual breakthrough, Wang et al (2025) formulated Metropolis-Hastings as a Markov decision process, opening up the possibility for adaptive tuning using Reinforcement Learning (RL). Their emphasis was on theoretical foundations; realising the practical benefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for subsequent work. The purpose of this paper is twofold: First, we observe the surprising result that natural choices of reward, such as the acceptance rate, or the expected squared jump distance, provide insufficient signal for training RLMH. Instead, we propose a novel reward based on the contrastive divergence, whose superior performance in the context of RLMH is demonstrated. Second, we explore the potential of RLMH and present adaptive gradient-based samplers that balance flexibility of the Markov transition kernel with learnability of the associated RL task. A comprehensive simulation study using the posteriordb benchmark supports the practical effectiveness of RLMH.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions","url":"https://arxiv.org/abs/2507.00670","date":1751428800,"author":"","guid":179857,"unread":true,"content":"<article>arXiv:2507.00670v1 Announce Type: cross \nAbstract: In recent years, accelerated MRI reconstruction based on deep learning has led to significant improvements in image quality with impressive results for high acceleration factors. However, from a clinical perspective image quality is only secondary; much more important is that all clinically relevant information is preserved in the reconstruction from heavily undersampled data. In this paper, we show that existing techniques, even when considering resampling for diffusion-based reconstruction, can fail to reconstruct small and rare pathologies, thus leading to potentially wrong diagnosis decisions (false negatives). To uncover the potentially missing clinical information we propose ``Semantically Diverse Reconstructions'' (\\SDR), a method which, given an original reconstruction, generates novel reconstructions with enhanced semantic variability while all of them are fully consistent with the measured data. To evaluate \\SDR automatically we train an object detector on the fastMRI+ dataset. We show that \\SDR significantly reduces the chance of false-negative diagnoses (higher recall) and improves mean average precision compared to the original reconstructions. The code is available on https://github.com/NikolasMorshuis/SDR</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound","url":"https://arxiv.org/abs/2507.00660","date":1751428800,"author":"","guid":179858,"unread":true,"content":"<article>arXiv:2507.00660v1 Announce Type: cross \nAbstract: Mitral regurgitation is one of the most prevalent cardiac disorders. Four-dimensional (4D) ultrasound has emerged as the primary imaging modality for assessing dynamic valvular morphology. However, 4D mitral valve (MV) analysis remains challenging due to limited phase annotations, severe motion artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency in existing methods hinders 4D MV analysis. To bridge this gap, we propose a Motion-Topology guided consistency network (MTCNet) for accurate 4D MV ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only sparse end-diastolic and end-systolic annotations. First, we design a cross-phase motion-guided consistency learning strategy, utilizing a bi-directional attention memory bank to propagate spatio-temporal features. This enables MTCNet to achieve excellent performance both per- and inter-phase. Second, we devise a novel topology-guided correlation regularization that explores physical prior knowledge to maintain anatomically plausible. Therefore, MTCNet can effectively leverage structural correspondence between labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV dataset, with 1408 phases from 160 patients, show that MTCNet performs superior cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD: 1.75mm). Both the code and the dataset are available at https://github.com/crs524/MTCNet.</article>","contentLength":1497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A convex lifting approach for the Calder\\'on problem","url":"https://arxiv.org/abs/2507.00645","date":1751428800,"author":"","guid":179859,"unread":true,"content":"<article>arXiv:2507.00645v1 Announce Type: cross \nAbstract: The Calder\\'on problem consists in recovering an unknown coefficient of a partial differential equation from boundary measurements of its solution. These measurements give rise to a highly nonlinear forward operator. As a consequence, the development of reconstruction methods for this inverse problem is challenging, as they usually suffer from the problem of local convergence. To circumvent this issue, we propose an alternative approach based on lifting and convex relaxation techniques, that have been successfully developed for solving finite-dimensional quadratic inverse problems. This leads to a convex optimization problem whose solution coincides with the sought-after coefficient, provided that a non-degenerate source condition holds. We demonstrate the validity of our approach on a toy model where the solution of the partial differential equation is known everywhere in the domain. In this simplified setting, we verify that the non-degenerate source condition holds under certain assumptions on the unknown coefficient. We leave the investigation of its validity in the Calder\\'on setting for future works.</article>","contentLength":1174,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws","url":"https://arxiv.org/abs/2507.00641","date":1751428800,"author":"","guid":179860,"unread":true,"content":"<article>arXiv:2507.00641v1 Announce Type: cross \nAbstract: Traditional machine learning approaches in physics rely on global optimization, limiting interpretability and enforcing physical constraints externally. We introduce the Hebbian Physics Network (HPN), a self-organizing computational framework in which learning emerges from local Hebbian updates driven by violations of conservation laws. Grounded in non-equilibrium thermodynamics and inspired by Prigogine/'s theory of dissipative structures, HPNs eliminate the need for global loss functions by encoding physical laws directly into the system/'s local dynamics. Residuals - quantified imbalances in continuity, momentum, or energy - serve as thermodynamic signals that drive weight adaptation through generalized Hebbian plasticity. We demonstrate this approach on incompressible fluid flow and continuum diffusion, where physically consistent structures emerge from random initial conditions without supervision. HPNs reframe computation as a residual-driven thermodynamic process, offering an interpretable, scalable, and physically grounded alternative for modeling complex dynamical systems.</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Forward Reverse Kernel Regression for the Schr\\\"{o}dinger bridge problem","url":"https://arxiv.org/abs/2507.00640","date":1751428800,"author":"","guid":179861,"unread":true,"content":"<article>arXiv:2507.00640v1 Announce Type: cross \nAbstract: In this paper, we study the Schr\\\"odinger Bridge Problem (SBP), which is central to entropic optimal transport. For general reference processes and begin--endpoint distributions, we propose a forward-reverse iterative Monte Carlo procedure to approximate the Schr\\\"odinger potentials in a nonparametric way. In particular, we use kernel based Monte Carlo regression in the context of Picard iteration of a corresponding fixed point problem. By preserving in the iteration positivity and contractivity in a Hilbert metric sense, we develop a provably convergent algorithm. Furthermore, we provide convergence rates for the potential estimates and prove their optimality. Finally, as an application, we propose a non-nested Monte Carlo procedure for the final dimensional distributions of the Schr\\\"odinger Bridge process, based on the constructed potentials and the forward-reverse simulation method for conditional diffusions.</article>","contentLength":977,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalization performance of narrow one-hidden layer networks in the teacher-student setting","url":"https://arxiv.org/abs/2507.00629","date":1751428800,"author":"","guid":179862,"unread":true,"content":"<article>arXiv:2507.00629v1 Announce Type: cross \nAbstract: Understanding the generalization abilities of neural networks for simple input-output distributions is crucial to account for their learning performance on real datasets. The classical teacher-student setting, where a network is trained from data obtained thanks to a label-generating teacher model, serves as a perfect theoretical test bed. In this context, a complete theoretical account of the performance of fully connected one-hidden layer networks in the presence of generic activation functions is lacking. In this work, we develop such a general theory for narrow networks, i.e. networks with a large number of hidden units, yet much smaller than the input dimension. Using methods from statistical physics, we provide closed-form expressions for the typical performance of both finite temperature (Bayesian) and empirical risk minimization estimators, in terms of a small number of weight statistics. In doing so, we highlight the presence of a transition where hidden neurons specialize when the number of samples is sufficiently large and proportional to the number of parameters of the network. Our theory accurately predicts the generalization error of neural networks trained on regression or classification tasks with either noisy full-batch gradient descent (Langevin dynamics) or full-batch gradient descent.</article>","contentLength":1376,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Simple Proof of Nehari's Theorem Based on Duality","url":"https://arxiv.org/abs/2507.00624","date":1751428800,"author":"","guid":179863,"unread":true,"content":"<article>arXiv:2507.00624v1 Announce Type: cross \nAbstract: In this technical note we provide a simple proof of Nehari's theorem on the optimal approximation by $H_\\infty$ functions, based on convex duality.</article>","contentLength":198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geometric Gaussian Approximations of Probability Distributions","url":"https://arxiv.org/abs/2507.00616","date":1751428800,"author":"","guid":179864,"unread":true,"content":"<article>arXiv:2507.00616v1 Announce Type: cross \nAbstract: Approximating complex probability distributions, such as Bayesian posterior distributions, is of central interest in many applications. We study the expressivity of geometric Gaussian approximations. These consist of approximations by Gaussian pushforwards through diffeomorphisms or Riemannian exponential maps. We first review these two different kinds of geometric Gaussian approximations. Then we explore their relationship to one another. We further provide a constructive proof that such geometric Gaussian approximations are universal, in that they can capture any probability distribution. Finally, we discuss whether, given a family of probability distributions, a common diffeomorphism can be found to obtain uniformly high-quality geometric Gaussian approximations for that family.</article>","contentLength":843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping","url":"https://arxiv.org/abs/2507.00613","date":1751428800,"author":"","guid":179865,"unread":true,"content":"<article>arXiv:2507.00613v1 Announce Type: cross \nAbstract: Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac parametric mapping for characterizing myocardial tissue and diagnosing cardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI) acquires 11 breath-hold baseline images with interleaved rest periods to ensure mapping accuracy. However, prolonged scanning can be challenging for patients with poor breathholds, often leading to motion artifacts that degrade image quality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a signal recovery model involving an iterative estimation process. Recent studies have proposed deep-learning approaches for rapid $T_1$ mapping using shortened sequences to reduce acquisition time for patient comfort. Nevertheless, existing methods overlook important physics constraints, limiting interpretability and generalization. In this work, we present an accelerated, end-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary Differential Equations (ODEs) to model temporal dynamics and address these challenges. Our method achieves high-accuracy $T_1$ estimation from a sparse subset of baseline images and ensures efficient null index estimation at test time. Specifically, we develop a continuous-time LSTM-ODE model to enable selective Look-Locker (LL) data acquisition with arbitrary time lags. Experimental results show superior performance in $T_1$ estimation for both native and post-contrast sequences and demonstrate the strong benefit of our physics-based formulation over direct data-driven $T_1$ priors.</article>","contentLength":1632,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models","url":"https://arxiv.org/abs/2507.00582","date":1751428800,"author":"","guid":179866,"unread":true,"content":"<article>arXiv:2507.00582v1 Announce Type: cross \nAbstract: Deformable medical image registration is traditionally formulated as an optimization problem. While classical methods solve this problem iteratively, recent learning-based approaches use recurrent neural networks (RNNs) to mimic this process by unrolling the prediction of deformation fields in a fixed number of steps. However, classical methods typically converge after sufficient iterations, but learning-based unrolling methods lack a theoretical convergence guarantee and show instability empirically. In addition, unrolling methods have a practical bottleneck at training time: GPU memory usage grows linearly with the unrolling steps due to backpropagation through time (BPTT). To address both theoretical and practical challenges, we propose DEQReg, a novel registration framework based on Deep Equilibrium Models (DEQ), which formulates registration as an equilibrium-seeking problem, establishing a natural connection between classical optimization and learning-based unrolling methods. DEQReg maintains constant memory usage, enabling theoretically unlimited iteration steps. Through extensive evaluation on the public brain MRI and lung CT datasets, we show that DEQReg can achieve competitive registration performance, while substantially reducing memory consumption compared to state-of-the-art unrolling methods. We also reveal an intriguing phenomenon: the performance of existing unrolling methods first increases slightly then degrades irreversibly when the inference steps go beyond the training configuration. In contrast, DEQReg achieves stable convergence with its inbuilt equilibrium-seeking mechanism, bridging the gap between classical optimization-based and modern learning-based registration methods.</article>","contentLength":1778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linear rank-metric intersecting codes","url":"https://arxiv.org/abs/2507.00569","date":1751428800,"author":"","guid":179867,"unread":true,"content":"<article>arXiv:2507.00569v1 Announce Type: cross \nAbstract: In this paper we introduce and investigate rank-metric intersecting codes, a new class of linear codes in the rank-metric context, inspired by the well-studied notion of intersecting codes in the Hamming metric. A rank-metric code is said to be intersecting if any two nonzero codewords have supports intersecting non trivially. We explore this class from both a coding-theoretic and geometric perspective, highlighting its relationship with minimal codes, MRD codes, and Hamming-metric intersecting codes. We derive structural properties, sufficient conditions based on minimum distance, and geometric characterizations in terms of 2-spannable $q$-systems. We establish upper and lower bounds on code parameters and show some constructions, which leave a range of unexplored parameters. Finally, we connect rank-intersecting codes to other combinatorial structures such as $(2,1)$-separating systems and frameproof codes.</article>","contentLength":973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inverse Design in Nanophotonics via Representation Learning","url":"https://arxiv.org/abs/2507.00546","date":1751428800,"author":"","guid":179868,"unread":true,"content":"<article>arXiv:2507.00546v1 Announce Type: cross \nAbstract: Inverse design in nanophotonics, the computational discovery of structures achieving targeted electromagnetic (EM) responses, has become a key tool for recent optical advances. Traditional intuition-driven or iterative optimization methods struggle with the inherently high-dimensional, non-convex design spaces and the substantial computational demands of EM simulations. Recently, machine learning (ML) has emerged to address these bottlenecks effectively. This review frames ML-enhanced inverse design methodologies through the lens of representation learning, classifying them into two categories: output-side and input-side approaches. Output-side methods use ML to learn a representation in the solution space to create a differentiable solver that accelerates optimization. Conversely, input-side techniques employ ML to learn compact, latent-space representations of feasible device geometries, enabling efficient global exploration through generative models. Each strategy presents unique trade-offs in data requirements, generalization capacity, and novel design discovery potentials. Hybrid frameworks that combine physics-based optimization with data-driven representations help escape poor local optima, improve scalability, and facilitate knowledge transfer. We conclude by highlighting open challenges and opportunities, emphasizing complexity management, geometry-independent representations, integration of fabrication constraints, and advancements in multiphysics co-designs.</article>","contentLength":1544,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simulation-Efficient Cosmological Inference with Multi-Fidelity SBI","url":"https://arxiv.org/abs/2507.00514","date":1751428800,"author":"","guid":179869,"unread":true,"content":"<article>arXiv:2507.00514v1 Announce Type: cross \nAbstract: The simulation cost for cosmological simulation-based inference can be decreased by combining simulation sets of varying fidelity. We propose an approach to such multi-fidelity inference based on feature matching and knowledge distillation. Our method results in improved posterior quality, particularly for small simulation budgets and difficult inference problems.</article>","contentLength":417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+","url":"https://arxiv.org/abs/2507.00511","date":1751428800,"author":"","guid":179870,"unread":true,"content":"<article>arXiv:2507.00511v1 Announce Type: cross \nAbstract: In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two cutting-edge deep learning architectures designed to enhance medical image segmentation. Our approach integrates Squeeze-and-Excitation (SE) and Convolutional Block Attention Module (CBAM) techniques into the traditional VM U-Net framework, significantly improving segmentation accuracy, feature localization, and computational efficiency. Both models show superior performance compared to the baseline VM-Unet across multiple datasets. Notably, VMSEUnet achieves the highest accuracy, IoU, precision, and recall while maintaining low loss values. It also exhibits exceptional computational efficiency with faster inference times and lower memory usage on both GPU and CPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a valuable tool for medical image analysis. These findings highlight its potential for real-world clinical applications, emphasizing the importance of further research to optimize accuracy, robustness, and computational efficiency.</article>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Physics-Aware Style Transfer for Adaptive Holographic Reconstruction","url":"https://arxiv.org/abs/2507.00482","date":1751428800,"author":"","guid":179871,"unread":true,"content":"<article>arXiv:2507.00482v1 Announce Type: cross \nAbstract: Inline holographic imaging presents an ill-posed inverse problem of reconstructing objects' complex amplitude from recorded diffraction patterns. Although recent deep learning approaches have shown promise over classical phase retrieval algorithms, they often require high-quality ground truth datasets of complex amplitude maps to achieve a statistical inverse mapping operation between the two domains. Here, we present a physics-aware style transfer approach that interprets the object-to-sensor distance as an implicit style within diffraction patterns. Using the style domain as the intermediate domain to construct cyclic image translation, we show that the inverse mapping operation can be learned in an adaptive manner only with datasets composed of intensity measurements. We further demonstrate its biomedical applicability by reconstructing the morphology of dynamically flowing red blood cells, highlighting its potential for real-time, label-free imaging. As a framework that leverages physical cues inherently embedded in measurements, the presented method offers a practical learning strategy for imaging applications where ground truth is difficult or impossible to obtain.</article>","contentLength":1240,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Process-aware and high-fidelity microstructure generation using stable diffusion","url":"https://arxiv.org/abs/2507.00459","date":1751428800,"author":"","guid":179872,"unread":true,"content":"<article>arXiv:2507.00459v1 Announce Type: cross \nAbstract: Synthesizing realistic microstructure images conditioned on processing parameters is crucial for understanding process-structure relationships in materials design. However, this task remains challenging due to limited training micrographs and the continuous nature of processing variables. To overcome these challenges, we present a novel process-aware generative modeling approach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art text-to-image diffusion model adapted for microstructure generation. Our method introduces numeric-aware embeddings that encode continuous variables (annealing temperature, time, and magnification) directly into the model's conditioning, enabling controlled image generation under specified process conditions and capturing process-driven microstructural variations. To address data scarcity and computational constraints, we fine-tune only a small fraction of the model's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently transferring the pre-trained model to the materials domain. We validate realism using a semantic segmentation model based on a fine-tuned U-Net with a VGG16 encoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean IoU, outperforming previous methods. Quantitative analyses using physical descriptors and spatial statistics show strong agreement between synthetic and real microstructures. Specifically, two-point correlation and lineal-path errors remain below 2.1% and 0.6%, respectively. Our method represents the first adaptation of SD3.5-Large for process-aware microstructure generation, offering a scalable approach for data-driven materials design.</article>","contentLength":1718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mitigating Language Mismatch in SSL-Based Speaker Anonymization","url":"https://arxiv.org/abs/2507.00458","date":1751428800,"author":"","guid":179873,"unread":true,"content":"<article>arXiv:2507.00458v1 Announce Type: cross \nAbstract: Speaker anonymization aims to protect speaker identity while preserving content information and the intelligibility of speech. However, most speaker anonymization systems (SASs) are developed and evaluated using only English, resulting in degraded utility for other languages. This paper investigates language mismatch in SASs for Japanese and Mandarin speech. First, we fine-tune a self-supervised learning (SSL)-based content encoder with Japanese speech to verify effective language adaptation. Then, we propose fine-tuning a multilingual SSL model with Japanese speech and evaluating the SAS in Japanese and Mandarin. Downstream experiments show that fine-tuning an English-only SSL model with the target language enhances intelligibility while maintaining privacy and that multilingual SSL further extends SASs' utility across different languages. These findings highlight the importance of language adaptation and multilingual pre-training of SSLs for robust multilingual speaker anonymization.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding","url":"https://arxiv.org/abs/2507.00419","date":1751428800,"author":"","guid":179874,"unread":true,"content":"<article>arXiv:2507.00419v1 Announce Type: cross \nAbstract: Understanding Earth's subsurface is critical for energy transition, natural hazard mitigation, and planetary science. Yet subsurface analysis remains fragmented, with separate models required for structural interpretation, stratigraphic analysis, geobody segmentation, and property modeling-each tightly coupled to specific data distributions and task formulations. We introduce the Geological Everything Model 3D (GEM), a unified generative architecture that reformulates all these tasks as prompt-conditioned inference along latent structural frameworks derived from subsurface imaging. This formulation moves beyond task-specific models by enabling a shared inference mechanism, where GEM propagates human-provided prompts-such as well logs, masks, or structural sketches-along inferred structural frameworks to produce geologically coherent outputs. Through this mechanism, GEM achieves zero-shot generalization across tasks with heterogeneous prompt types, without retraining for new tasks or data sources. This capability emerges from a two-stage training process that combines self-supervised representation learning on large-scale field seismic data with adversarial fine-tuning using mixed prompts and labels across diverse subsurface tasks. GEM demonstrates broad applicability across surveys and tasks, including Martian radar stratigraphy analysis, structural interpretation in subduction zones, full seismic stratigraphic interpretation, geobody delineation, and property modeling. By bridging expert knowledge with generative reasoning in a structurally aware manner, GEM lays the foundation for scalable, human-in-the-loop geophysical AI-transitioning from fragmented pipelines to a vertically integrated, promptable reasoning system. Project page: https://douyimin.github.io/GEM</article>","contentLength":1845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning collective variables that respect permutational symmetry","url":"https://arxiv.org/abs/2507.00408","date":1751428800,"author":"","guid":179875,"unread":true,"content":"<article>arXiv:2507.00408v1 Announce Type: cross \nAbstract: In addition to translational and rotational symmetries, clusters of identical interacting particles possess permutational symmetry. Coarse-grained models for such systems are instrumental in identifying metastable states, providing an effective description of their dynamics, and estimating transition rates. We propose a numerical framework for learning collective variables that respect translational, rotational, and permutational symmetries, and for estimating transition rates and residence times. It combines a sort-based featurization, residence manifold learning in the feature space, and learning collective variables with autoencoders whose loss function utilizes the orthogonality relationship (Legoll and Lelievre, 2010). The committor of the resulting reduced model is used as the reaction coordinate in the forward flux sampling and to design a control for sampling the transition path process. We offer two case studies, the Lennard-Jones-7 in 2D and the Lennard-Jones-8 in 3D. The transition rates and residence times computed with the aid of the reduced models agree with those obtained via brute-force methods.</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials","url":"https://arxiv.org/abs/2507.00407","date":1751428800,"author":"","guid":179876,"unread":true,"content":"<article>arXiv:2507.00407v1 Announce Type: cross \nAbstract: Accurate molecular property predictions require 3D geometries, which are typically obtained using expensive methods such as density functional theory (DFT). Here, we attempt to obtain molecular geometries by relying solely on machine learning interatomic potential (MLIP) models. To this end, we first curate a large-scale molecular relaxation dataset comprising 3.5 million molecules and 300 million snapshots. Then MLIP foundation models are trained with supervised learning to predict energy and forces given 3D molecular structures. Once trained, we show that the foundation models can be used in different ways to obtain geometries either explicitly or implicitly. First, it can be used to obtain low-energy 3D geometries via geometry optimization, providing relaxed 3D geometries for downstream molecular property predictions. To mitigate potential biases and enhance downstream predictions, we introduce geometry fine-tuning based on the relaxed 3D geometries. Second, the foundation models can be directly fine-tuned for property prediction when ground truth 3D geometries are available. Our results demonstrate that MLIP foundation models trained on relaxation data can provide valuable molecular geometries that benefit property predictions.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GRAND: Graph Release with Assured Node Differential Privacy","url":"https://arxiv.org/abs/2507.00402","date":1751428800,"author":"","guid":179877,"unread":true,"content":"<article>arXiv:2507.00402v1 Announce Type: cross \nAbstract: Differential privacy is a well-established framework for safeguarding sensitive information in data. While extensively applied across various domains, its application to network data -- particularly at the node level -- remains underexplored. Existing methods for node-level privacy either focus exclusively on query-based approaches, which restrict output to pre-specified network statistics, or fail to preserve key structural properties of the network. In this work, we propose GRAND (Graph Release with Assured Node Differential privacy), which is, to the best of our knowledge, the first network release mechanism that releases entire networks while ensuring node-level differential privacy and preserving structural properties. Under a broad class of latent space models, we show that the released network asymptotically follows the same distribution as the original network. The effectiveness of the approach is evaluated through extensive experiments on both synthetic and real-world datasets.</article>","contentLength":1052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Logarithmic Depth Decomposition of Approximate Multi-Controlled Single-Qubit Gates Without Ancilla Qubits","url":"https://arxiv.org/abs/2507.00400","date":1751428800,"author":"","guid":179878,"unread":true,"content":"<article>arXiv:2507.00400v1 Announce Type: cross \nAbstract: The synthesis of quantum operators involves decomposing general quantum gates into the gate set supported by a given quantum device. Multi-controlled gates are essential components in this process. In this work, we present improved decompositions of multi-controlled NOT gates with logarithmic depth using a single ancilla qubit, while also reducing the constant factors in the circuit depth compared to previous work. We optimize a previously proposed decomposition of multi-target, multi-controlled special unitary SU(2) gates by identifying the presence of a conditionally clean qubit. Additionally, we introduce the best-known decomposition of multi-controlled approximate unitary U(2) gates without using ancilla qubits. This approach significantly reduces the overall circuit depth and CNOT count while preserving an adjustable error parameter, yielding a more efficient and scalable solution for synthesizing large controlled-unitary gates. Our method is particularly suitable for both NISQ and fault-tolerant quantum architectures. All software developed in this project is freely available.</article>","contentLength":1150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound","url":"https://arxiv.org/abs/2507.00398","date":1751428800,"author":"","guid":179879,"unread":true,"content":"<article>arXiv:2507.00398v1 Announce Type: cross \nAbstract: Accurate fetal birth weight (FBW) estimation is essential for optimizing delivery decisions and reducing perinatal mortality. However, clinical methods for FBW estimation are inefficient, operator-dependent, and challenging to apply in cases of complex fetal anatomy. Existing deep learning methods are based on 2D standard ultrasound (US) images or videos that lack spatial information, limiting their prediction accuracy. In this study, we propose the first method for directly estimating FBW from 3D fetal US volumes. Our approach integrates a multi-scale feature fusion network (MFFN) and a synthetic sample-based learning framework (SSLF). The MFFN effectively extracts and fuses multi-scale features under sparse supervision by incorporating channel attention, spatial attention, and a ranking-based loss function. SSLF generates synthetic samples by simply combining fetal head and abdomen data from different fetuses, utilizing semi-supervised learning to improve prediction performance. Experimental results demonstrate that our method achieves superior performance, with a mean absolute error of $166.4\\pm155.9$ $g$ and a mean absolute percentage error of $5.1\\pm4.6$%, outperforming existing methods and approaching the accuracy of a senior doctor. Code is available at: https://github.com/Qioy-i/EFW.</article>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Affine-Invariant Global Non-Asymptotic Convergence Analysis of BFGS under Self-Concordance","url":"https://arxiv.org/abs/2507.00361","date":1751428800,"author":"","guid":179880,"unread":true,"content":"<article>arXiv:2507.00361v1 Announce Type: cross \nAbstract: In this paper, we establish global non-asymptotic convergence guarantees for the BFGS quasi-Newton method without requiring strong convexity or the Lipschitz continuity of the gradient or Hessian. Instead, we consider the setting where the objective function is strictly convex and strongly self-concordant. For an arbitrary initial point and any arbitrary positive-definite initial Hessian approximation, we prove global linear and superlinear convergence guarantees for BFGS when the step size is determined using a line search scheme satisfying the weak Wolfe conditions. Moreover, all our global guarantees are affine-invariant, with the convergence rates depending solely on the initial error and the strongly self-concordant constant. Our results extend the global non-asymptotic convergence theory of BFGS beyond traditional assumptions and, for the first time, establish affine-invariant convergence guarantees aligning with the inherent affine invariance of the BFGS method.</article>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Faces in rectilinear drawings of complete graphs","url":"https://arxiv.org/abs/2507.00313","date":1751428800,"author":"","guid":179881,"unread":true,"content":"<article>arXiv:2507.00313v1 Announce Type: cross \nAbstract: We initiate the study of extremal problems about faces in convex rectilinear drawings of~$K_n$, that is, drawings where vertices are represented by points in the plane in convex position and edges by line segments between the points representing the end-vertices. We show that if a convex rectilinear drawing of $K_n$ does not contain a common interior point of at least three edges, then there is always a face forming a convex 5-gon while there are such drawings without any face forming a convex $k$-gon with $k \\geq 6$.\n  A convex rectilinear drawing of $K_n$ is \\emph{regular} if its vertices correspond to vertices of a regular convex $n$-gon. We characterize positive integers $n$ for which regular drawings of $K_n$ contain a face forming a convex 5-gon.\n  To our knowledge, this type of problems has not been considered in the literature before and so we also pose several new natural open problems.</article>","contentLength":959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Interpretability in Generative Modeling: Statistically Disentangled Latent Spaces Guided by Generative Factors in Scientific Datasets","url":"https://arxiv.org/abs/2507.00298","date":1751428800,"author":"","guid":179882,"unread":true,"content":"<article>arXiv:2507.00298v1 Announce Type: cross \nAbstract: This study addresses the challenge of statistically extracting generative factors from complex, high-dimensional datasets in unsupervised or semi-supervised settings. We investigate encoder-decoder-based generative models for nonlinear dimensionality reduction, focusing on disentangling low-dimensional latent variables corresponding to independent physical factors. Introducing Aux-VAE, a novel architecture within the classical Variational Autoencoder framework, we achieve disentanglement with minimal modifications to the standard VAE loss function by leveraging prior statistical knowledge through auxiliary variables. These variables guide the shaping of the latent space by aligning latent factors with learned auxiliary variables. We validate the efficacy of Aux-VAE through comparative assessments on multiple datasets, including astronomical simulations.</article>","contentLength":916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context","url":"https://arxiv.org/abs/2507.00288","date":1751428800,"author":"","guid":179883,"unread":true,"content":"<article>arXiv:2507.00288v1 Announce Type: cross \nAbstract: This study explores how AI-powered digital innovations are reshaping organisational accountability in a transnational governance context. As AI systems increasingly mediate decision-making in domains such as auditing and financial reporting, traditional mechanisms of accountability, based on control, transparency, and auditability, are being destabilised. We integrate the Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and institutional theory to examine how organisations adopt AI technologies in response to regulatory, ethical, and cultural pressures that transcend national boundaries. We argue that accountability is co-constructed within global socio-technical networks, shaped not only by user perceptions but also by governance logics and normative expectations. Extending TAM, we incorporate compliance and legitimacy as key factors in perceived usefulness and usability. Drawing on ANT, we reconceptualise accountability as a relational and emergent property of networked assemblages. We propose two organisational strategies including internal governance reconfiguration and external actor-network engagement to foster responsible, legitimate, and globally accepted AI adoption in the accounting domain.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Satellite and Mobile Phone Data Reveal How Violence Affects Seasonal Migration in Afghanistan","url":"https://arxiv.org/abs/2507.00279","date":1751428800,"author":"","guid":179884,"unread":true,"content":"<article>arXiv:2507.00279v1 Announce Type: cross \nAbstract: Seasonal migration plays a critical role in stabilizing rural economies and sustaining the livelihoods of agricultural households. Violence and civil conflict have long been thought to disrupt these labor flows, but this hypothesis has historically been hard to test given the lack of reliable data on migration in conflict zones. Focusing on Afghanistan in the 8-year period prior to the Taliban's takeover in 2021, we first demonstrate how satellite imagery can be used to infer the timing of the opium harvest, which employs a large number of seasonal workers in relatively well-paid jobs. We then use a dataset of nationwide mobile phone records to characterize the migration response to this harvest, and examine whether and how violence and civil conflict disrupt this migration. We find that, on average, districts with high levels of poppy cultivation receive significantly more seasonal migrants than districts with no poppy cultivation. These labor flows are surprisingly resilient to idiosyncratic violent events at the source or destination, including extreme violence resulting in large numbers of fatalities. However, seasonal migration is affected by longer-term patterns of conflict, such as the extent of Taliban control in origin and destination locations.</article>","contentLength":1325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations","url":"https://arxiv.org/abs/2507.00269","date":1751428800,"author":"","guid":179885,"unread":true,"content":"<article>arXiv:2507.00269v1 Announce Type: cross \nAbstract: Current sparse autoencoder (SAE) approaches to neural network interpretability assume that activations can be decomposed through linear superposition into sparse, interpretable features. Despite high reconstruction fidelity, SAEs consistently fail to eliminate polysemanticity and exhibit pathological behavioral errors. We propose that neural networks encode information in two complementary spaces compressed into the same substrate: feature identity and feature integration. To test this dual encoding hypothesis, we develop sequential and joint-training architectures to capture identity and integration patterns simultaneously. Joint training achieves 41.3% reconstruction improvement and 51.6% reduction in KL divergence errors. This architecture spontaneously develops bimodal feature organization: low squared norm features contributing to integration pathways and the rest contributing directly to the residual. Small nonlinear components (3% of parameters) achieve 16.5% standalone improvements, demonstrating parameter-efficient capture of computational relationships crucial for behavior. Additionally, intervention experiments using 2x2 factorial stimulus designs demonstrated that integration features exhibit selective sensitivity to experimental manipulations and produce systematic behavioral effects on model outputs, including significant interaction effects across semantic dimensions. This work provides systematic evidence for (1) dual encoding in neural representations, (2) meaningful nonlinearly encoded feature interactions, and (3) introduces an architectural paradigm shift from post-hoc feature analysis to integrated computational design, establishing foundations for next-generation SAEs.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disentangled Feature Importance","url":"https://arxiv.org/abs/2507.00260","date":1751428800,"author":"","guid":179886,"unread":true,"content":"<article>arXiv:2507.00260v1 Announce Type: cross \nAbstract: Feature importance quantification faces a fundamental challenge: when predictors are correlated, standard methods systematically underestimate their contributions. We prove that major existing approaches target identical population functionals under squared-error loss, revealing why they share this correlation-induced bias.\n  To address this limitation, we introduce \\emph{Disentangled Feature Importance (DFI)}, a nonparametric generalization of the classical $R^2$ decomposition via optimal transport. DFI transforms correlated features into independent latent variables using a transport map, eliminating correlation distortion. Importance is computed in this disentangled space and attributed back through the transport map's sensitivity. DFI provides a principled decomposition of importance scores that sum to the total predictive variability for latent additive models and to interaction-weighted functional ANOVA variances more generally, under arbitrary feature dependencies.\n  We develop a comprehensive semiparametric theory for DFI. For general transport maps, we establish root-$n$ consistency and asymptotic normality of importance estimators in the latent space, which extends to the original feature space for the Bures-Wasserstein map. Notably, our estimators achieve second-order estimation error, which vanishes if both regression function and transport map estimation errors are $o_{\\mathbb{P}}(n^{-1/4})$. By design, DFI avoids the computational burden of repeated submodel refitting and the challenges of conditional covariate distribution estimation, thereby achieving computational efficiency.</article>","contentLength":1670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robustness Analysis for Quantum Systems Controlled by Continuous-Time Pulses","url":"https://arxiv.org/abs/2507.00255","date":1751428800,"author":"","guid":179887,"unread":true,"content":"<article>arXiv:2507.00255v1 Announce Type: cross \nAbstract: Differential sensitivity techniques originally developed to study the robustness of energy landscape controllers are generalized to the important case of closed quantum systems subject to continuously varying controls. Vanishing sensitivity to parameter variation is shown to coincide with perfect fidelity, as was the case for time-invariant controls. Bounds on the magnitude of the differential sensitivity to any parameter variation are derived based simply on knowledge of the system Hamiltonian and the maximum size of the control inputs</article>","contentLength":593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fully Parallelized BP Decoding for Quantum LDPC Codes Can Outperform BP-OSD","url":"https://arxiv.org/abs/2507.00254","date":1751428800,"author":"","guid":179888,"unread":true,"content":"<article>arXiv:2507.00254v1 Announce Type: cross \nAbstract: In this work, we propose a lightweight decoder based solely on belief-propagation (BP), augmented with a speculative post-processing strategy inspired by classical Chase decoding. Our method identifies unreliable bits via BP oscillation statistics, generates a set of modified test patterns, and decodes them in parallel using low-iteration BP. We demonstrate that our approach can achieve logical error rates comparable to or even better than BP-OSD, but has lower latency over its parallelization for a variety of bivariate bicycle codes, which significantly reduces decoding complexity.</article>","contentLength":640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compact Representation of Semilinear and Terrain-like Graphs","url":"https://arxiv.org/abs/2507.00252","date":1751428800,"author":"","guid":179889,"unread":true,"content":"<article>arXiv:2507.00252v1 Announce Type: cross \nAbstract: We consider the existence and construction of \\textit{biclique covers} of graphs, consisting of coverings of their edge sets by complete bipartite graphs. The \\textit{size} of such a cover is the sum of the sizes of the bicliques. Small-size biclique covers of graphs are ubiquitous in computational geometry, and have been shown to be useful compact representations of graphs. We give a brief survey of classical and recent results on biclique covers and their applications, and give new families of graphs having biclique covers of near-linear size.\n  In particular, we show that semilinear graphs, whose edges are defined by linear relations in bounded dimensional space, always have biclique covers of size $O(n\\polylog n)$. This generalizes many previously known results on special classes of graphs including interval graphs, permutation graphs, and graphs of bounded boxicity, but also new classes such as intersection graphs of L-shapes in the plane. It also directly implies the bounds for Zarankiewicz's problem derived by Basit, Chernikov, Starchenko, Tao, and Tran (\\textit{Forum Math. Sigma}, 2021).\n  We also consider capped graphs, also known as terrain-like graphs, defined as ordered graphs forbidding a certain ordered pattern on four vertices. Terrain-like graphs contain the induced subgraphs of terrain visibility graphs. We give an elementary proof that these graphs admit biclique partitions of size $O(n\\log^3 n)$. This provides a simple combinatorial analogue of a classical result from Agarwal, Alon, Aronov, and Suri on polygon visibility graphs (\\textit{Discrete Comput. Geom.} 1994).\n  Finally, we prove that there exists families of unit disk graphs on $n$ vertices that do not admit biclique coverings of size $o(n^{4/3})$, showing that we are unlikely to improve on Szemer\\'edi-Trotter type incidence bounds for higher-degree semialgebraic graphs.</article>","contentLength":1930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Endogenous Network Structures with Precision and Dimension Choices","url":"https://arxiv.org/abs/2507.00249","date":1751428800,"author":"","guid":179890,"unread":true,"content":"<article>arXiv:2507.00249v1 Announce Type: cross \nAbstract: This paper presents a social learning model where the network structure is endogenously determined by signal precision and dimension choices. Agents not only choose the precision of their signals and what dimension of the state to learn about, but these decisions directly determine the underlying network structure on which social learning occurs. We show that under a fixed network structure, the optimal precision choice is sublinear in the agent's stationary influence in the network, and this individually optimal choice is worse than the socially optimal choice by a factor of $n^{1/3}$. Under a dynamic network structure, we specify the network by defining a kernel distance between agents, which then determines how much weight agents place on one another. Agents choose dimensions to learn about such that their choice minimizes the squared sum of influences of all agents: a network with equally distributed influence across agents is ideal.</article>","contentLength":1002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Observation of Blood Flow in Major Neck Vessels Modulated 1 by Physiological Maneuvers","url":"https://arxiv.org/abs/2507.00231","date":1751428800,"author":"","guid":179891,"unread":true,"content":"<article>arXiv:2507.00231v1 Announce Type: cross \nAbstract: Large neck vessels (carotid artery and internal jugular vein, IJV) offer a unique opportunity to monitor hemodynamics non-invasively by optical means. The primary shortcoming of past work has been the focus on healthy volunteers in normal physiological conditions and well-controlled environments. To drive the technology closer to the bedside, testing is required under more re-alistic conditions, including in pathologies and real-world environments (e.g., similar toICU or emergency care settings). The primary goal of the current work was to extend the range of physiological maneuvers for blood flow modulation by introducing new maneuvers and ob-serving PPG response to them. The data from the necks of two healthy volunteers in a supine position were collected by clinical PPG and in-house built PPG sensors, accompanied by ECG signal collection. Seven maneuvers (abdominojugular test, breath holding, Valsalva, proximal occlusion of right IJV, distal occlusion of right IJV, proximal occlusion of left IJV, distal occlusion of left IJV) were performed in sequence with 1 min allocated for each maneuver. The 1 min was split into three segments: baseline (15 s), experiment (15 s), and recovery (30 s). Thus, the overall du-ration of the experiment was 7 min. AC amplitude from clinical PPG, DC amplitudes from in-house built PPG, and ECG signal were compared during all seven physiological maneuvers. Newly proposed maneuvers (Valsalva and IJV occlusions) demonstrated modulation of blood flow, which was more significant than previously reported maneuvers (abdominojugular test and breath holding). The proposed physiological maneuvers demonstrate high potential as instruments for modulating blood flow in major neck vessels.</article>","contentLength":1786,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis","url":"https://arxiv.org/abs/2507.00227","date":1751428800,"author":"","guid":179892,"unread":true,"content":"<article>arXiv:2507.00227v1 Announce Type: cross \nAbstract: While generative methods have progressed rapidly in recent years, generating expressive prosody for an utterance remains a challenging task in text-to-speech synthesis. This is particularly true for systems that model prosody explicitly through parameters such as pitch, energy, and duration, which is commonly done for the sake of interpretability and controllability. In this work, we investigate the effectiveness of stochastic methods for this task, including Normalizing Flows, Conditional Flow Matching, and Rectified Flows. We compare these methods to a traditional deterministic baseline, as well as to real human realizations. Our extensive subjective and objective evaluations demonstrate that stochastic methods produce natural prosody on par with human speakers by capturing the variability inherent in human speech. Further, they open up additional controllability options by allowing the sampling temperature to be tuned.</article>","contentLength":986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discovering the underlying analytic structure within Standard Model constants using artificial intelligence","url":"https://arxiv.org/abs/2507.00225","date":1751428800,"author":"","guid":179893,"unread":true,"content":"<article>arXiv:2507.00225v1 Announce Type: cross \nAbstract: This paper presents a search for underlying analytic structures among the fundamental parameters of the Standard Model (SM) using symbolic regression and genetic programming. We identify the simplest analytic relationships connecting pairs of these constants and report several notable observations based on about a thousand expressions with relative precision better than 1%. These results may serve as valuable inputs for model builders and artificial intelligence methods aimed at uncovering hidden patterns among the SM constants, or potentially used as building blocks for a deeper underlying law that connects all parameters of the SM through a small set of fundamental constants.</article>","contentLength":737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures","url":"https://arxiv.org/abs/2507.00209","date":1751428800,"author":"","guid":179894,"unread":true,"content":"<article>arXiv:2507.00209v1 Announce Type: cross \nAbstract: High-resolution imaging is crucial for enhancing visual clarity and enabling precise computer-assisted guidance in minimally invasive surgery (MIS). Despite the increasing adoption of 4K endoscopic systems, there remains a significant gap in publicly available native 4K datasets tailored specifically for robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible surgical imaging and video dataset captured at a native 4K resolution, representing realistic conditions of robotic-assisted procedures. SurgiSR4K comprises diverse visual scenarios including specular reflections, tool occlusions, bleeding, and soft tissue deformations, meticulously designed to reflect common challenges faced during laparoscopic and robotic surgeries. This dataset opens up possibilities for a broad range of computer vision tasks that might benefit from high resolution data, such as super resolution (SR), smoke removal, surgical instrument detection, 3D tissue reconstruction, monocular depth estimation, instance segmentation, novel view synthesis, and vision-language model (VLM) development. SurgiSR4K provides a robust foundation for advancing research in high-resolution surgical imaging and fosters the development of intelligent imaging technologies aimed at enhancing performance, safety, and usability in image-guided robotic surgeries.</article>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards 3D Semantic Image Synthesis for Medical Imaging","url":"https://arxiv.org/abs/2507.00206","date":1751428800,"author":"","guid":179895,"unread":true,"content":"<article>arXiv:2507.00206v1 Announce Type: cross \nAbstract: In the medical domain, acquiring large datasets is challenging due to both accessibility issues and stringent privacy regulations. Consequently, data availability and privacy protection are major obstacles to applying machine learning in medical imaging. To address this, our study proposes the Med-LSDM (Latent Semantic Diffusion Model), which operates directly in the 3D domain and leverages de-identified semantic maps to generate synthetic data as a method of privacy preservation and data augmentation. Unlike many existing methods that focus on generating 2D slices, Med-LSDM is designed specifically for 3D semantic image synthesis, making it well-suited for applications requiring full volumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D image generation process by applying a diffusion model within the latent space of a pre-trained VQ-GAN. By operating in the compressed latent space, the model significantly reduces computational complexity while still preserving critical 3D spatial details. Our approach demonstrates strong performance in 3D semantic medical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional Duke Breast dataset and similar Dice scores (0.70964) to those of real images (0.71496). These results demonstrate that the synthetic data from our model have a small domain gap with real data and are useful for data augmentation.</article>","contentLength":1452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)","url":"https://arxiv.org/abs/2507.00185","date":1751428800,"author":"","guid":179896,"unread":true,"content":"<article>arXiv:2507.00185v1 Announce Type: cross \nAbstract: Current artificial intelligence models for medical imaging are predominantly single modality and single disease. Attempts to create multimodal and multi-disease models have resulted in inconsistent clinical accuracy. Furthermore, training these models typically requires large, labour-intensive, well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal, multi-specialty foundation model trained using self-supervised learning and a memory module. MerMED-FM was trained on 3.3 million medical images from over ten specialties and seven modalities, including computed tomography (CT), chest X-rays (CXR), ultrasound (US), pathology patches, color fundus photography (CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was evaluated across multiple diseases and compared against existing foundational models. Strong performance was achieved across all modalities, with AUROCs of 0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894 (CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable, versatile, cross-specialty foundation model that enables robust medical imaging interpretation across diverse medical disciplines.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Music Source Separation Models Preserve Spatial Information in Binaural Audio?","url":"https://arxiv.org/abs/2507.00155","date":1751428800,"author":"","guid":179897,"unread":true,"content":"<article>arXiv:2507.00155v1 Announce Type: cross \nAbstract: Binaural audio remains underexplored within the music information retrieval community. Motivated by the rising popularity of virtual and augmented reality experiences as well as potential applications to accessibility, we investigate how well existing music source separation (MSS) models perform on binaural audio. Although these models process two-channel inputs, it is unclear how effectively they retain spatial information. In this work, we evaluate how several popular MSS models preserve spatial information on both standard stereo and novel binaural datasets. Our binaural data is synthesized using stems from MUSDB18-HQ and open-source head-related transfer functions by positioning instrument sources randomly along the horizontal plane. We then assess the spatial quality of the separated stems using signal processing and interaural cue-based metrics. Our results show that stereo MSS models fail to preserve the spatial information critical for maintaining the immersive quality of binaural audio, and that the degradation depends on model architecture as well as the target instrument. Finally, we highlight valuable opportunities for future work at the intersection of MSS and immersive audio.</article>","contentLength":1259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gender and Discipline Shape Length, Content and Tone of Grant Peer Review Reports","url":"https://arxiv.org/abs/2507.00103","date":1751428800,"author":"","guid":179898,"unread":true,"content":"<article>arXiv:2507.00103v1 Announce Type: cross \nAbstract: Peer review by experts is central to the evaluation of grant proposals, but little is known about how gender and disciplinary differences shape the content and tone of grant peer review reports. We analyzed 39,280 review reports submitted to the Swiss National Science Foundation between 2016 and 2023, covering 11,385 proposals for project funding across 21 disciplines from the Social Sciences and Humanities (SSH), Life Sciences (LS), and Mathematics, Informatics, Natural Sciences, and Technology (MINT). Using supervised machine learning, we classified over 1.3 million sentences by evaluation criteria and sentiment. Reviews in SSH were significantly longer and more critical, with less focus on the applicant's track record, while those in MINT were more concise and positive, with a higher focus on the track record, as compared to those in LS. Compared to male reviewers, female reviewers write longer reviews that more closely align with the evaluation criteria and express more positive sentiments. Female applicants tend to receive reviews with slightly more positive sentiment than male applicants. Gender and disciplinary culture influence how grant proposals are reviewed - shaping the tone, length, and focus of peer review reports. These differences have important implications for fairness and consistency in research funding.</article>","contentLength":1395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Authentication of Continuous-Variable Quantum Messages","url":"https://arxiv.org/abs/2507.00095","date":1751428800,"author":"","guid":179899,"unread":true,"content":"<article>arXiv:2507.00095v1 Announce Type: cross \nAbstract: We introduce the first quantum authentication scheme for continuous-variable states. Our scheme is based on trap states, and is an adaptation of a discrete-variable scheme by Broadbent et al. (arXiv:1211.1080), but with more freedom in choosing the number of traps. We provide a security proof, mostly following the approach of Broadbent and Wainewright (arXiv:1607.03075). As a necessary ingredient for the proof we derive the continuous-variable analogue of the Pauli Twirl.</article>","contentLength":527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How large language models judge and influence human cooperation","url":"https://arxiv.org/abs/2507.00088","date":1751428800,"author":"","guid":179900,"unread":true,"content":"<article>arXiv:2507.00088v1 Announce Type: cross \nAbstract: Humans increasingly rely on large language models (LLMs) to support decisions in social settings. Previous work suggests that such tools shape people's moral and political judgements. However, the long-term implications of LLM-based social decision-making remain unknown. How will human cooperation be affected when the assessment of social interactions relies on language models? This is a pressing question, as human cooperation is often driven by indirect reciprocity, reputations, and the capacity to judge interactions of others. Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide 21 different LLMs with an extensive set of examples where individuals cooperate -- or refuse cooperating -- in a range of social contexts, and ask how these interactions should be judged. Furthermore, through an evolutionary game-theoretical model, we evaluate cooperation dynamics in populations where the extracted LLM-driven judgements prevail, assessing the long-term impact of LLMs on human prosociality. We observe a remarkable agreement in evaluating cooperation against good opponents. On the other hand, we notice within- and between-model variance when judging cooperation with ill-reputed individuals. We show that the differences revealed between models can significantly impact the prevalence of cooperation. Finally, we test prompts to steer LLM norms, showing that such interventions can shape LLM judgements, particularly through goal-oriented prompts. Our research connects LLM-based advices and long-term social dynamics, and highlights the need to carefully align LLM norms in order to preserve human cooperation.</article>","contentLength":1698,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time Invariant Sensor Tasking for Catalog Maintenance of LEO Space objects using Stochastic Geometry","url":"https://arxiv.org/abs/2507.00076","date":1751428800,"author":"","guid":179901,"unread":true,"content":"<article>arXiv:2507.00076v1 Announce Type: cross \nAbstract: Catalog maintenance of space objects by limited number of ground-based sensors presents a formidable challenging task to the space community. This article presents a methodology for time-invariant tracking and surveillance of space objects in low Earth orbit (LEO) by optimally directing ground sensors. Our methodology aims to maximize the expected number of space objects from a set of ground stations by utilizing concepts from stochastic geometry, particularly the Poisson point process. We have provided a systematic framework to understand visibility patterns and enhance the efficiency of tracking multiple objects simultaneously. Our approach contributes to more informed decision-making in space operations, ultimately supporting efforts to maintain safety and sustainability in LEO.</article>","contentLength":843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The gradual transformation of inland countries -- human plowing, horse plowing and equity incentives","url":"https://arxiv.org/abs/2507.00067","date":1751428800,"author":"","guid":179902,"unread":true,"content":"<article>arXiv:2507.00067v1 Announce Type: cross \nAbstract: Many modern countries have not learned their lessons and often hope for the wisdom of later generations, resulting in them only possessing modern technology and difficult to iterate ancient civilizations. At present, there is no way to tell how we should learn from history and promote the gradual upgrading of civilization. Therefore, we must tell the history of civilization's progress and the means of governance, learn from experience to improve the comprehensive strength and survival ability of civilization, and achieve an optimal solution for the tempering brought by conflicts and the reduction of internal conflicts. Firstly, we must follow the footsteps of history and explore the reasons for the long-term stability of each country in conflict, including providing economic benefits to the people and means of suppressing them; then, use mathematical methods to demonstrate how we can achieve the optimal solution at the current stage. After analysis, we can conclude that the civilization transformed from human plowing to horse plowing can easily suppress the resistance of the people and provide them with the ability to resist; The selection of rulers should consider multiple institutional aspects, such as exams, elections, and drawing lots; Economic development follows a lognormal distribution and can be adjusted by expected value and variance. Using a lognormal distribution with the maximum value to divide equity can adjust the wealth gap.</article>","contentLength":1514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Segmentation-Based Regression for Quantum Neural Networks","url":"https://arxiv.org/abs/2507.00065","date":1751428800,"author":"","guid":179903,"unread":true,"content":"<article>arXiv:2507.00065v1 Announce Type: cross \nAbstract: Recent advances in quantum hardware motivate the development of algorithmic frameworks that integrate quantum sampling with classical inference. This work introduces a segmentation-based regression method tailored to quantum neural networks (QNNs), where real-valued outputs are encoded as base-b digit sequences and inferred through greedy digitwise optimization. By casting the regression task as a constrained combinatorial problem over a structured digit lattice, the method replaces continuous inference with interpretable and tractable updates. A hybrid quantum-classical architecture is employed: quantum circuits generate candidate digits through projective measurement, while classical forward models evaluate these candidates based on task-specific error functionals. We formalize the algorithm from first principles, derive convergence and complexity bounds, and demonstrate its effectiveness on inverse problems involving PDE-constrained models. The resulting framework provides a robust, high-precision interface between quantum outputs and continuous scientific inference.</article>","contentLength":1137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Car-Following Models with Bike Dynamics for Improved Traffic Simulation","url":"https://arxiv.org/abs/2507.00062","date":1751428800,"author":"","guid":179904,"unread":true,"content":"<article>arXiv:2507.00062v1 Announce Type: cross \nAbstract: Road traffic simulations are crucial for establishing safe and efficient traffic environments. They are used to test various road applications before real-world implementation. SUMO is a well-known simulator for road networks and intermodal traffic, often used in conjunction with other tools to test various types of applications. Realistic simulations require accurate movement models for different road users, such as cars, bicycles, and buses. While realistic models are already implemented for most vehicle types, bicycles, which are essential for achieving safe and efficient traffic, can only be modeled as slow vehicles or fast pedestrians at present. This paper introduces the Realistic Bicycle Dynamics Model (RBDM), the first dedicated bicycle model for SUMO, addressing this significant gap. Leveraging real-world bicycle data from the SimRa dataset, the RBDM implements realistic speed, acceleration, and deceleration behaviors of bicycles in urban scenarios. The evaluation is conducted using the Monaco SUMO traffic scenario and a newly generated Berlin scenario in SUMO. The RBDM significantly outperforms the existing slow-vehicle approximation in SUMO, aligning more closely with real-world data. These results underscore the necessity of a realistic bicycle movement model for accurate simulations, given the significant differences in the movement profiles of bicycles, cars, and pedestrians. Furthermore, the model is tested for its ability to generalize to disparate scenarios and urban topologies, which is dependent on the manner and geographical region in which the SimRa data were gathered. In addition, recommendations are provided for how it could be adapted for use in different city topologies.</article>","contentLength":1775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-Time Guidewire Tip Tracking Using a Siamese Network for Image-Guided Endovascular Procedures","url":"https://arxiv.org/abs/2507.00051","date":1751428800,"author":"","guid":179905,"unread":true,"content":"<article>arXiv:2507.00051v1 Announce Type: cross \nAbstract: An ever-growing incorporation of AI solutions into clinical practices enhances the efficiency and effectiveness of healthcare services. This paper focuses on guidewire tip tracking tasks during image-guided therapy for cardiovascular diseases, aiding physicians in improving diagnostic and therapeutic quality. A novel tracking framework based on a Siamese network with dual attention mechanisms combines self- and cross-attention strategies for robust guidewire tip tracking. This design handles visual ambiguities, tissue deformations, and imaging artifacts through enhanced spatial-temporal feature learning. Validation occurred on 3 randomly selected clinical digital subtraction angiography (DSA) sequences from a dataset of 15 sequences, covering multiple interventional scenarios. The results indicate a mean localization error of 0.421 $\\pm$ 0.138 mm, with a maximum error of 1.736 mm, and a mean Intersection over Union (IoU) of 0.782. The framework maintains an average processing speed of 57.2 frames per second, meeting the temporal demands of endovascular imaging. Further validations with robotic platforms for automating diagnostics and therapies in clinical routines yielded tracking errors of 0.708 $\\pm$ 0.695 mm and 0.148 $\\pm$ 0.057 mm in two distinct experimental scenarios.</article>","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Musical Source Separation of Brazilian Percussion","url":"https://arxiv.org/abs/2503.04995","date":1751428800,"author":"","guid":179906,"unread":true,"content":"<article>arXiv:2503.04995v1 Announce Type: cross \nAbstract: Musical source separation (MSS) has recently seen a big breakthrough in separating instruments from a mixture in the context of Western music, but research on non-Western instruments is still limited due to a lack of data. In this demo, we use an existing dataset of Brazilian sama percussion to create artificial mixtures for training a U-Net model to separate the surdo drum, a traditional instrument in samba. Despite limited training data, the model effectively isolates the surdo, given the drum's repetitive patterns and its characteristic low-pitched timbre. These results suggest that MSS systems can be successfully harnessed to work in more culturally-inclusive scenarios without the need of collecting extensive amounts of data.</article>","contentLength":790,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Intersection Graphs of Graphs and Hypergraphs: A Survey","url":"https://arxiv.org/abs/1809.08472","date":1751428800,"author":"","guid":179907,"unread":true,"content":"<article>arXiv:1809.08472v4 Announce Type: cross \nAbstract: In this survey, we have attempted to show some developmental milestones on the characterizations of intersection graphs of hypergraphs. The theory of intersection graphs of hypergraphs has been a classical topic in the theory of special graphs. To conclude, at the end, we have listed some open problems posed by various authors whose work has contributed to this survey and also the new trends coming out of intersection graphs. Keywords: Hypergraphs, Intersection graphs, Line graphs, Representative graphs, Derived graphs, Algorithms (ALG), Forbidden induced subgraphs (FIS), Krausz partitions, Eigenvalues.</article>","contentLength":661,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Bivariegated Graphs and Line Graphs","url":"https://arxiv.org/abs/1809.08467","date":1751428800,"author":"","guid":179908,"unread":true,"content":"<article>arXiv:1809.08467v4 Announce Type: cross \nAbstract: This note is on the structures of line graphs and 2-variegated graphs. We have given here solutions of some graph equations involving line graphs and 2-variegated graphs. In addition, a characterization of potentially 2-variegated line graphic degree sequences is given.</article>","contentLength":321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models","url":"https://arxiv.org/abs/2507.01017","date":1751428800,"author":"","guid":179909,"unread":true,"content":"<article>arXiv:2507.01017v1 Announce Type: new \nAbstract: Human error remains a dominant risk driver in safety-critical sectors such as nuclear power, aviation, and healthcare, where seemingly minor mistakes can cascade into catastrophic outcomes. Although decades of research have produced a rich repertoire of mitigation techniques, persistent limitations: scarce high-quality data, algorithmic opacity, and residual reliance on expert judgment, continue to constrain progress. This review synthesizes recent advances at the intersection of risk-informed decision making, human reliability assessment (HRA), artificial intelligence (AI), and cognitive science to clarify how their convergence can curb human-error risk. We first categorize the principal forms of human error observed in complex sociotechnical environments and outline their quantitative impact on system reliability. Next, we examine risk-informed frameworks that embed HRA within probabilistic and data-driven methodologies, highlighting successes and gaps. We then survey cognitive and human-performance models, detailing how mechanistic accounts of perception, memory, and decision-making enrich error prediction and complement HRA metrics. Building on these foundations, we critically assess AI-enabled techniques for real-time error detection, operator-state estimation, and AI-augmented HRA workflows. Across these strands, a recurring insight emerges: integrating cognitive models with AI-based analytics inside risk-informed HRA pipelines markedly enhances predictive fidelity, yet doing so demands richer datasets, transparent algorithms, and rigorous validation. Finally, we identify promising research directions, coupling resilience engineering concepts with grounded theory, operationalizing the iceberg model of incident causation, and establishing cross-domain data consortia, to foster a multidisciplinary paradigm that elevates human reliability in high-stakes systems.</article>","contentLength":1946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers","url":"https://arxiv.org/abs/2507.01016","date":1751428800,"author":"","guid":179910,"unread":true,"content":"<article>arXiv:2507.01016v1 Announce Type: new \nAbstract: In this paper, we introduce an innovative vector quantization based action tokenizer built upon the largest-scale action trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive dataset enables our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates inference but also generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted to a wide range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon planning. A key finding of our work is that the domain gap between synthetic and real action trajectories is marginal, allowing us to effectively utilize a vast amount of synthetic data during training without compromising real-world performance. To validate our approach, we conducted extensive experiments in both simulated environments and on real robotic platforms. The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our tokenizer on downstream tasks improves significantly-most notably, achieving up to a 30% higher success rate on two real-world tasks in long-horizon scenarios. These findings highlight the potential of our action tokenizer as a robust and scalable solution for real-time embodied intelligence systems, paving the way for more efficient and reliable robotic control in diverse application domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io</article>","contentLength":1567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution","url":"https://arxiv.org/abs/2507.01012","date":1751428800,"author":"","guid":179911,"unread":true,"content":"<article>arXiv:2507.01012v1 Announce Type: new \nAbstract: Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ShapeEmbed: a self-supervised learning framework for 2D contour quantification","url":"https://arxiv.org/abs/2507.01009","date":1751428800,"author":"","guid":179912,"unread":true,"content":"<article>arXiv:2507.01009v1 Announce Type: new \nAbstract: The shape of objects is an important source of visual information in a wide range of applications. One of the core challenges of shape quantification is to ensure that the extracted measurements remain invariant to transformations that preserve an object's intrinsic geometry, such as changing its size, orientation, and position in the image. In this work, we introduce ShapeEmbed, a self-supervised representation learning framework designed to encode the contour of objects in 2D images, represented as a Euclidean distance matrix, into a shape descriptor that is invariant to translation, scaling, rotation, reflection, and point indexing. Our approach overcomes the limitations of traditional shape descriptors while improving upon existing state-of-the-art autoencoder-based approaches. We demonstrate that the descriptors learned by our framework outperform their competitors in shape classification tasks on natural and biological images. We envision our approach to be of particular relevance to biological imaging applications.</article>","contentLength":1086,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation","url":"https://arxiv.org/abs/2507.01008","date":1751428800,"author":"","guid":179913,"unread":true,"content":"<article>arXiv:2507.01008v1 Announce Type: new \nAbstract: We present the DexWrist, a compliant robotic wrist designed to advance robotic manipulation in highly-constrained environments, enable dynamic tasks, and speed up data collection. DexWrist is designed to be close to the functional capabilities of the human wrist and achieves mechanical compliance and a greater workspace as compared to existing robotic wrist designs. The DexWrist can supercharge policy learning by (i) enabling faster teleoperation and therefore making data collection more scalable; (ii) completing tasks in fewer steps which reduces trajectory lengths and therefore can ease policy learning; (iii) DexWrist is designed to be torque transparent with easily simulatable kinematics for simulated data collection; and (iv) most importantly expands the workspace of manipulation for approaching highly cluttered scenes and tasks. More details about the wrist can be found at: dexwrist.csail.mit.edu.</article>","contentLength":964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning","url":"https://arxiv.org/abs/2507.01006","date":1751428800,"author":"","guid":179914,"unread":true,"content":"<article>arXiv:2507.01006v1 Announce Type: new \nAbstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking.</article>","contentLength":1528,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention","url":"https://arxiv.org/abs/2507.01004","date":1751428800,"author":"","guid":179915,"unread":true,"content":"<article>arXiv:2507.01004v1 Announce Type: new \nAbstract: Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.</article>","contentLength":1684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes","url":"https://arxiv.org/abs/2507.01003","date":1751428800,"author":"","guid":179916,"unread":true,"content":"<article>arXiv:2507.01003v1 Announce Type: new \nAbstract: Recent studies have proposed interpreting the training process from an ergodic perspective. Building on this foundation we present a unified framework for understanding and accelerating the training of deep neural networks via stochastic gradient descent. By analyzing the geometric landscape of the objective function we introduce a practical diagnostic, the running estimate of the largest Lyapunov exponent, which provably distinguishes genuine convergence toward stable minimizers from mere statistical stabilization near saddle points. We then propose a ghost category extension for standard classifiers that adds auxiliary ghost output nodes so the model gains extra descent directions that open a lateral corridor around narrow loss barriers and enable the optimizer to bypass poor basins during the early training phase. We show that this extension strictly reduces approximation error and that after sufficient convergence the ghost dimensions collapse and the extended model's invariant law coincides with that of the original and there exists a path in the enlarged parameter space along which the total loss does not increase while the original loss decreases by an arbitrary margin. Taken together these results provide a principled architecture level intervention that accelerates early stage trainability while preserving asymptotic behavior.</article>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks","url":"https://arxiv.org/abs/2507.01001","date":1751428800,"author":"","guid":179917,"unread":true,"content":"<article>arXiv:2507.01001v1 Announce Type: new \nAbstract: We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.</article>","contentLength":1545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America","url":"https://arxiv.org/abs/2507.00999","date":1751428800,"author":"","guid":179918,"unread":true,"content":"<article>arXiv:2507.00999v1 Announce Type: new \nAbstract: Leaderboards showcase the current capabilities and limitations of Large Language Models (LLMs). To motivate the development of LLMs that represent the linguistic and cultural diversity of the Spanish-speaking community, we present La Leaderboard, the first open-source leaderboard to evaluate generative LLMs in languages and language varieties of Spain and Latin America. La Leaderboard is a community-driven project that aims to establish an evaluation standard for everyone interested in developing LLMs for the Spanish-speaking community. This initial version combines 66 datasets in Basque, Catalan, Galician, and different Spanish varieties, showcasing the evaluation results of 50 models. To encourage community-driven development of leaderboards in other languages, we explain our methodology, including guidance on selecting the most suitable evaluation setup for each downstream task. In particular, we provide a rationale for using fewer few-shot examples than typically found in the literature, aiming to reduce environmental impact and facilitate access to reproducible results for a broader research community.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geometrization of Higher-Order Linear Control Laws for Attitude Control on $\\mathsf{SO(3)}$","url":"https://arxiv.org/abs/2507.00997","date":1751428800,"author":"","guid":179919,"unread":true,"content":"<article>arXiv:2507.00997v1 Announce Type: new \nAbstract: This paper presents a theoretical framework for analyzing the stability of higher-order geometric nonlinear control laws for attitude control on the Special Orthogonal Group $\\mathrm{SO(3)}$. In particular, the paper extends existing results on the analysis of PID-type geometric nonlinear control laws to more general higher-order dynamic state-feedback compensators on $\\mathrm{SO(3)}$. The candidate Lyapunov function is motivated by quadratic Lyapunov functions of the form $V(x)=x^{\\top}Px$ typically considered in the analysis of linear time-invariant (LTI) systems. The stability analysis is carried out in two steps. In the first step, a sufficient condition is obtained for the positive definiteness of the candidate Lyapunov function, and a necessary and sufficient condition for the negative definiteness of the corresponding Lyapunov rate. These conditions ensure that the desired equilibrium is almost globally asymptotically stable (AGAS). In the second step, a convex relaxation of the proposed conditions is used to obtain sufficient conditions in the form of linear matrix inequalities (LMIs). Overall, the approach is motivated by the widespread use of LMI-based analysis and design tools for LTI systems. To reduce conservatism, matrix gains are considered for the controller gains as well as the Lyapunov function coefficients. The applicability of the approach to practical problems is illustrated by designing and analyzing a 21-state geometric nonlinear attitude control law for a multicopter.</article>","contentLength":1565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Should We Still Pretrain Encoders with Masked Language Modeling?","url":"https://arxiv.org/abs/2507.00994","date":1751428800,"author":"","guid":179920,"unread":true,"content":"<article>arXiv:2507.00994v1 Announce Type: new \nAbstract: Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.</article>","contentLength":1583,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis","url":"https://arxiv.org/abs/2507.00992","date":1751428800,"author":"","guid":179921,"unread":true,"content":"<article>arXiv:2507.00992v1 Announce Type: new \nAbstract: Text-to-image generation has greatly advanced content creation, yet accurately rendering visual text remains a key challenge due to blurred glyphs, semantic drift, and limited style control. Existing methods often rely on pre-rendered glyph images as conditions, but these struggle to retain original font styles and color cues, necessitating complex multi-branch designs that increase model overhead and reduce flexibility. To address these issues, we propose a segmentation-guided framework that uses pixel-level visual text masks -- rich in glyph shape, color, and spatial detail -- as unified conditional inputs. Our method introduces two core components: (1) a fine-tuned bilingual segmentation model for precise text mask extraction, and (2) a streamlined diffusion model augmented with adaptive glyph conditioning and a region-specific loss to preserve textual fidelity in both content and style. Our approach achieves state-of-the-art performance on the AnyText benchmark, significantly surpassing prior methods in both Chinese and English settings. To enable more rigorous evaluation, we also introduce two new benchmarks: GlyphMM-benchmark for testing layout and glyph consistency in complex typesetting, and MiniText-benchmark for assessing generation quality in small-scale text regions. Experimental results show that our model outperforms existing methods by a large margin in both scenarios, particularly excelling at small text rendering and complex layout preservation, validating its strong generalization and deployment readiness.</article>","contentLength":1598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations","url":"https://arxiv.org/abs/2507.00990","date":1751428800,"author":"","guid":179922,"unread":true,"content":"<article>arXiv:2507.00990v1 Announce Type: new \nAbstract: This work introduces Robots Imitating Generated Videos (RIGVid), a system that enables robots to perform complex manipulation tasks--such as pouring, wiping, and mixing--purely by imitating AI-generated videos, without requiring any physical demonstrations or robot-specific training. Given a language command and an initial scene image, a video diffusion model generates potential demonstration videos, and a vision-language model (VLM) automatically filters out results that do not follow the command. A 6D pose tracker then extracts object trajectories from the video, and the trajectories are retargeted to the robot in an embodiment-agnostic fashion. Through extensive real-world evaluations, we show that filtered generated videos are as effective as real demonstrations, and that performance improves with generation quality. We also show that relying on generated videos outperforms more compact alternatives such as keypoint prediction using VLMs, and that strong 6D pose tracking outperforms other ways to extract trajectories, such as dense feature point tracking. These findings suggest that videos produced by a state-of-the-art off-the-shelf model can offer an effective source of supervision for robotic manipulation.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discourse Heuristics For Paradoxically Moral Self-Correction","url":"https://arxiv.org/abs/2507.00985","date":1751428800,"author":"","guid":179923,"unread":true,"content":"<article>arXiv:2507.00985v1 Announce Type: new \nAbstract: Moral self-correction has emerged as a promising approach for aligning the output of Large Language Models (LLMs) with human moral values. However, moral self-correction techniques are subject to two primary paradoxes. First, despite empirical and theoretical evidence to support the effectiveness of self-correction, this LLM capability only operates at a superficial level. Second, while LLMs possess the capability of self-diagnosing immoral aspects of their output, they struggle to identify the cause of this moral inconsistency during their self-correction process. To better understand and address these paradoxes, we analyze the discourse constructions in fine-tuning corpora designed to enhance moral self-correction, uncovering the existence of the heuristics underlying effective constructions. We demonstrate that moral self-correction relies on discourse constructions that reflect heuristic shortcuts, and that the presence of these heuristic shortcuts during self-correction leads to inconsistency when attempting to enhance both self-correction and self-diagnosis capabilities jointly. Based on our findings, we propose a solution to improve moral self-correction by leveraging the heuristics of curated datasets. We also highlight the generalization challenges of this capability, particularly in terms of learning from situated context and model scales.</article>","contentLength":1420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation","url":"https://arxiv.org/abs/2507.00984","date":1751428800,"author":"","guid":179924,"unread":true,"content":"<article>arXiv:2507.00984v1 Announce Type: new \nAbstract: Modern warehouse automation systems rely on fleets of intelligent robots that generate vast amounts of data -- most of which remains unannotated. This paper develops a self-supervised domain adaptation pipeline that leverages real-world, unlabeled data to improve perception models without requiring manual annotations. Our work focuses specifically on estimating the pose and shape of boxes and presents a correct-and-certify pipeline for self-supervised box pose and shape estimation. We extensively evaluate our approach across a range of simulated and real industrial settings, including adaptation to a large-scale real-world dataset of 50,000 images. The self-supervised model significantly outperforms models trained solely in simulation and shows substantial improvements over a zero-shot 3D bounding box estimation baseline.</article>","contentLength":882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations","url":"https://arxiv.org/abs/2507.00981","date":1751428800,"author":"","guid":179925,"unread":true,"content":"<article>arXiv:2507.00981v1 Announce Type: new \nAbstract: Recent years have witnessed substantial progress on monocular depth estimation, particularly as measured by the success of large models on standard benchmarks. However, performance on standard benchmarks does not offer a complete assessment, because most evaluate accuracy but not robustness. In this work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which enables systematic robustness evaluation. PDE uses procedural generation to create 3D scenes that test robustness to various controlled perturbations, including object, camera, material and lighting changes. Our analysis yields interesting findings on what perturbations are challenging for state-of-the-art depth models, which we hope will inform further research. Code and data are available at https://github.com/princeton-vl/proc-depth-eval.</article>","contentLength":871,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RTMap: Real-Time Recursive Mapping with Change Detection and Localization","url":"https://arxiv.org/abs/2507.00980","date":1751428800,"author":"","guid":179926,"unread":true,"content":"<article>arXiv:2507.00980v1 Announce Type: new \nAbstract: While recent online HD mapping methods relieve burdened offline pipelines and solve map freshness, they remain limited by perceptual inaccuracies, occlusion in dense traffic, and an inability to fuse multi-agent observations. We propose RTMap to enhance these single-traversal methods by persistently crowdsourcing a multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap simultaneously addresses three core challenges in an end-to-end fashion: (1) Uncertainty-aware positional modeling for HD map elements, (2) probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3) real-time detection for possible road structural changes. Experiments on several public autonomous driving datasets demonstrate our solid performance on both the prior-aided map quality and the localization accuracy, demonstrating our effectiveness of robustly serving downstream prediction and planning modules while gradually improving the accuracy and freshness of the crowdsourced prior-map asynchronously. Our source-code will be made publicly available at https://github.com/CN-ADLab/RTMap (Camera ready version incorporating reviewer suggestions will be updated soon).</article>","contentLength":1235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing LLM Agent Safety via Causal Influence Prompting","url":"https://arxiv.org/abs/2507.00979","date":1751428800,"author":"","guid":179927,"unread":true,"content":"<article>arXiv:2507.00979v1 Announce Type: new \nAbstract: As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.</article>","contentLength":995,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anatomy of High-Performance Column-Pivoted QR Decomposition","url":"https://arxiv.org/abs/2507.00976","date":1751428800,"author":"","guid":179928,"unread":true,"content":"<article>arXiv:2507.00976v1 Announce Type: new \nAbstract: We introduce an algorithmic framework for performing QR factorization with column pivoting (QRCP) on general matrices. The framework enables the design of practical QRCP algorithms through user-controlled choices for the core subroutines. We provide a comprehensive overview of how to navigate these choices on modern hardware platforms, offering detailed descriptions of alternative methods for both CPUs and GPUs. The practical QRCP algorithms developed within this framework are implemented as part of the open-source RandLAPACK library. Our empirical evaluation demonstrates that, on a dual AMD EPYC 9734 system, the proposed method achieves performance improvements of up to two orders of magnitude over LAPACK's standard QRCP routine and greatly surpasses the performance of the current state-of-the-art randomized QRCP algorithm. Additionally, on an NVIDIA H100 GPU, our method attains approximately 65 percent of the performance of cuSOLVER's unpivoted QR factorization.</article>","contentLength":1027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reasoning as an Adaptive Defense for Safety","url":"https://arxiv.org/abs/2507.00971","date":1751428800,"author":"","guid":179929,"unread":true,"content":"<article>arXiv:2507.00971v1 Announce Type: new \nAbstract: Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt.</article>","contentLength":1381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surgical Neural Radiance Fields from One Image","url":"https://arxiv.org/abs/2507.00969","date":1751428800,"author":"","guid":179930,"unread":true,"content":"<article>arXiv:2507.00969v1 Announce Type: new \nAbstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D reconstruction and view synthesis, yet their reliance on extensive multi-view data limits their application in surgical intraoperative settings where only limited data is available. In particular, collecting such extensive data intraoperatively is impractical due to time constraints. This work addresses this challenge by leveraging a single intraoperative image and preoperative data to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera viewpoints and images needed for robust and unobstructed training. Intraoperatively, the appearance of the surgical image is transferred to the pre-constructed training set through neural style transfer, specifically combining WTC2 and STROTSS to prevent over-stylization. This process enables the creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases. Quantitative comparisons to NeRF models trained on real surgical microscope images demonstrate strong synthesis agreement, with similarity metrics indicating high reconstruction fidelity and stylistic alignment. When compared with ground truth, our method demonstrates high structural similarity, confirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF training in surgical settings, overcoming the limitations of traditional multi-view methods.</article>","contentLength":1610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement","url":"https://arxiv.org/abs/2507.00966","date":1751428800,"author":"","guid":179931,"unread":true,"content":"<article>arXiv:2507.00966v1 Announce Type: new \nAbstract: With the advent of new sequence models like Mamba and xLSTM, several studies have shown that these models match or outperform state-of-the-art models in single-channel speech enhancement, automatic speech recognition, and self-supervised audio representation learning. However, prior research has demonstrated that sequence models like LSTM and Mamba tend to overfit to the training set. To address this issue, previous works have shown that adding self-attention to LSTMs substantially improves generalization performance for single-channel speech enhancement. Nevertheless, neither the concept of hybrid Mamba and time-frequency attention models nor their generalization performance have been explored for speech enhancement. In this paper, we propose a novel hybrid architecture, MambAttention, which combines Mamba and shared time- and frequency-multi-head attention modules for generalizable single-channel speech enhancement. To train our model, we introduce VoiceBank+Demand Extended (VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our proposed MambAttention model significantly outperforms existing state-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar complexity across all reported metrics on two out-of-domain datasets: DNS 2020 and EARS-WHAM_v2, while matching their performance on the in-domain dataset VB-DemandEx. Ablation studies highlight the role of weight sharing between the time- and frequency-multi-head attention modules for generalization performance. Finally, we explore integrating the shared time- and frequency-multi-head attention modules with LSTM and xLSTM, which yields a notable performance improvement on the out-of-domain datasets. However, our MambAttention model remains superior on both out-of-domain datasets across all reported evaluation metrics.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning","url":"https://arxiv.org/abs/2507.00965","date":1751428800,"author":"","guid":179932,"unread":true,"content":"<article>arXiv:2507.00965v1 Announce Type: new \nAbstract: Many machine learning tasks can benefit from external knowledge. Large knowledge graphs store such knowledge, and embedding methods can be used to distill it into ready-to-use vector representations for downstream applications. For this purpose, current models have however two limitations: they are primarily optimized for link prediction, via local contrastive learning, and they struggle to scale to the largest graphs due to GPU memory limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation ALgorithm for large knowledge graphs designed to produce high-quality embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce global embedding alignment by optimizing embeddings only on a small core of entities, and then propagating them to the rest of the graph via message passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream machine learning tasks. Our results show that SEPAL significantly outperforms previous methods on downstream tasks. In addition, SEPAL scales up its base embedding model, enabling fitting huge knowledge graphs on commodity hardware.</article>","contentLength":1176,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking the Discovery Engine","url":"https://arxiv.org/abs/2507.00964","date":1751428800,"author":"","guid":179933,"unread":true,"content":"<article>arXiv:2507.00964v1 Announce Type: new \nAbstract: The Discovery Engine is a general purpose automated system for scientific discovery, which combines machine learning with state-of-the-art ML interpretability to enable rapid and robust scientific insight across diverse datasets. In this paper, we benchmark the Discovery Engine against five recent peer-reviewed scientific publications applying machine learning across medicine, materials science, social science, and environmental science. In each case, the Discovery Engine matches or exceeds prior predictive performance while also generating deeper, more actionable insights through rich interpretability artefacts. These results demonstrate its potential as a new standard for automated, interpretable scientific modelling that enables complex knowledge discovery from data.</article>","contentLength":829,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception","url":"https://arxiv.org/abs/2507.00963","date":1751428800,"author":"","guid":179934,"unread":true,"content":"<article>arXiv:2507.00963v1 Announce Type: new \nAbstract: As social robots increasingly enter dementia care, concerns about deception, intentional or not, are gaining attention. Yet, how robotic design cues might elicit misleading perceptions in people with dementia, and how these perceptions arise, remains insufficiently understood. In this scoping review, we examined 26 empirical studies on interactions between people with dementia and physical social robots. We identify four key design cue categories that may influence deceptive impressions: cues resembling physiological signs (e.g., simulated breathing), social intentions (e.g., playful movement), familiar beings (e.g., animal-like form and sound), and, to a lesser extent, cues that reveal artificiality. Thematic analysis of user responses reveals that people with dementia often attribute biological, social, and mental capacities to robots, dynamically shifting between awareness and illusion. These findings underscore the fluctuating nature of ontological perception in dementia contexts. Existing definitions of robotic deception often rest on philosophical or behaviorist premises, but rarely engage with the cognitive mechanisms involved. We propose an empirically grounded definition: robotic deception occurs when Type 1 (automatic, heuristic) processing dominates over Type 2 (deliberative, analytic) reasoning, leading to misinterpretation of a robot's artificial nature. This dual-process perspective highlights the ethical complexity of social robots in dementia care and calls for design approaches that are not only engaging, but also epistemically respectful.</article>","contentLength":1631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digital Collections Explorer: An Open-Source, Multimodal Viewer for Searching Digital Collections","url":"https://arxiv.org/abs/2507.00961","date":1751428800,"author":"","guid":179935,"unread":true,"content":"<article>arXiv:2507.00961v1 Announce Type: new \nAbstract: We present Digital Collections Explorer, a web-based, open-source exploratory search platform that leverages CLIP (Contrastive Language-Image Pre-training) for enhanced visual discovery of digital collections. Our Digital Collections Explorer can be installed locally and configured to run on a visual collection of interest on disk in just a few steps. Building upon recent advances in multimodal search techniques, our interface enables natural language queries and reverse image searches over digital collections with visual features. This paper describes the system's architecture, implementation, and application to various cultural heritage collections, demonstrating its potential for democratizing access to digital archives, especially those with impoverished metadata. We present case studies with maps, photographs, and PDFs extracted from web archives in order to demonstrate the flexibility of the Digital Collections Explorer, as well as its ease of use. We demonstrate that the Digital Collections Explorer scales to hundreds of thousands of images on a MacBook Pro with an M4 chip. Lastly, we host a public demo of Digital Collections Explorer.</article>","contentLength":1209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact","url":"https://arxiv.org/abs/2507.00951","date":1751428800,"author":"","guid":179936,"unread":true,"content":"<article>arXiv:2507.00951v1 Announce Type: new \nAbstract: Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.</article>","contentLength":1890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MVP: Winning Solution to SMP Challenge 2025 Video Track","url":"https://arxiv.org/abs/2507.00950","date":1751428800,"author":"","guid":179937,"unread":true,"content":"<article>arXiv:2507.00950v1 Announce Type: new \nAbstract: Social media platforms serve as central hubs for content dissemination, opinion expression, and public engagement across diverse modalities. Accurately predicting the popularity of social media videos enables valuable applications in content recommendation, trend detection, and audience engagement. In this paper, we present Multimodal Video Predictor (MVP), our winning solution to the Video Track of the SMP Challenge 2025. MVP constructs expressive post representations by integrating deep video features extracted from pretrained models with user metadata and contextual information. The framework applies systematic preprocessing techniques, including log-transformations and outlier removal, to improve model robustness. A gradient-boosted regression model is trained to capture complex patterns across modalities. Our approach ranked first in the official evaluation of the Video Track, demonstrating its effectiveness and reliability for multimodal video popularity prediction on social platforms. The source code is available at https://anonymous.4open.science/r/SMPDVideo.</article>","contentLength":1132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Fast Can Graph Computations Go on Fine-grained Parallel Architectures","url":"https://arxiv.org/abs/2507.00949","date":1751428800,"author":"","guid":179938,"unread":true,"content":"<article>arXiv:2507.00949v1 Announce Type: new \nAbstract: Large-scale graph problems are of critical and growing importance and historically parallel architectures have provided little support. In the spirit of co-design, we explore the question, How fast can graph computing go on a fine-grained architecture? We explore the possibilities of an architecture optimized for fine-grained parallelism, natural programming, and the irregularity and skew found in real-world graphs. Using two graph benchmarks, PageRank (PR) and Breadth-First Search (BFS), we evaluate a Fine-Grained Graph architecture, UpDown, to explore what performance codesign can achieve. To demonstrate programmability, we wrote five variants of these algorithms. Simulations of up to 256 nodes (524,288 lanes) and projections to 16,384 nodes (33M lanes) show the UpDown system can achieve 637K GTEPS PR and 989K GTEPS BFS on RMAT, exceeding the best prior results by 5x and 100x respectively.</article>","contentLength":953,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time Series Foundation Models are Flow Predictors","url":"https://arxiv.org/abs/2507.00945","date":1751428800,"author":"","guid":179939,"unread":true,"content":"<article>arXiv:2507.00945v1 Announce Type: new \nAbstract: We investigate the effectiveness of time series foundation models (TSFMs) for crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD flows-these models are deployed in a strict zero-shot setting, using only the temporal evolution of each OD flow and no explicit spatial information. Moirai and TimesFM outperform both statistical and deep learning baselines, achieving up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to state-of-the-art competitors. Our results highlight the practical value of TSFMs for accurate, scalable flow prediction, even in scenarios with limited annotated data or missing spatial context.</article>","contentLength":781,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Feedback Schemes for Dirty Paper Channels With State Estimation at the Receiver","url":"https://arxiv.org/abs/2507.00942","date":1751428800,"author":"","guid":179940,"unread":true,"content":"<article>arXiv:2507.00942v1 Announce Type: new \nAbstract: In the literature, it has been shown that feedback does not increase the optimal rate-distortion region of the dirty paper channel with state estimation at the receiver (SE-R). On the other hand, it is well-known that feedback helps to construct low-complexity coding schemes in Gaussian channels, such as the elegant Schalkwijk-Kailath (SK) feedback scheme. This motivates us to explore capacity-achieving SK-type schemes in dirty paper channels with SE-R and feedback. In this paper, we first propose a capacity-achieving feedback scheme for the dirty paper channel with SE-R (DPC-SE-R), which combines the superposition coding and the classical SK-type scheme. Then, we extend this scheme to the dirty paper multiple-access channel with SE-R and feedback, and also show the extended scheme is capacity-achieving. Finally, we discuss how to extend our scheme to a noisy state observation case of the DPC-SE-R. However, the capacity-achieving SK-type scheme for such a case remains unknown.</article>","contentLength":1040,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks","url":"https://arxiv.org/abs/2507.00938","date":1751428800,"author":"","guid":179941,"unread":true,"content":"<article>arXiv:2507.00938v1 Announce Type: new \nAbstract: Recent progress in large language models (LLMs) has enabled the development of autonomous web agents capable of navigating and interacting with real websites. However, evaluating such agents remains challenging due to the instability and inconsistency of existing benchmarks, which often rely on dynamic content or oversimplified simulations. In this work, we introduce WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks grounded in the arXiv platform. WebArXiv ensures reproducible and reliable evaluation by anchoring tasks in fixed web snapshots with deterministic ground truths and standardized action trajectories. Through behavioral analysis, we identify a common failure mode, Rigid History Reflection, where agents over-rely on fixed interaction histories. To address this, we propose a lightweight dynamic reflection mechanism that allows agents to selectively retrieve relevant past steps during decision-making. We evaluate ten state-of-the-art web agents on WebArXiv. Results demonstrate clear performance differences across agents and validate the effectiveness of our proposed reflection strategy.</article>","contentLength":1188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles","url":"https://arxiv.org/abs/2507.00937","date":1751428800,"author":"","guid":179942,"unread":true,"content":"<article>arXiv:2507.00937v1 Announce Type: new \nAbstract: Low-cost indoor mobile robots have gained popularity with the increasing adoption of automation in homes and commercial spaces. However, existing lidar and camera-based solutions have limitations such as poor performance in visually obscured environments, high computational overhead for data processing, and high costs for lidars. In contrast, mmWave radar sensors offer a cost-effective and lightweight alternative, providing accurate ranging regardless of visibility. However, existing radar-based localization suffers from sparse point cloud generation, noise, and false detections. Thus, in this work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph neural network (GNN)-based framework to enhance radar point clouds, even in complex and dynamic environments. With an inference time of just 7.3 ms on the low-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such resource-constrained devices, requiring no additional computational resources. We evaluate its performance across key tasks, including localization, SLAM, and autonomous navigation, in three different environments. Our results demonstrate strong reliability and generalizability, making RaGNNarok a robust solution for low-cost indoor mobile robots.</article>","contentLength":1299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inverse matroid optimization under subset constraints","url":"https://arxiv.org/abs/2507.00930","date":1751428800,"author":"","guid":179943,"unread":true,"content":"<article>arXiv:2507.00930v1 Announce Type: new \nAbstract: In the Inverse Matroid problem, we are given a matroid, a fixed basis $B$, and an initial weight function, and the goal is to minimally modify the weights -- measured by some function -- so that $B$ becomes a maximum-weight basis. The problem arises naturally in settings where one wishes to explain or enforce a given solution by minimally perturbing the input.\n  We extend this classical problem by replacing the fixed basis with a subset $S_0$ of the ground set and imposing various structural constraints on the set of maximum-weight bases relative to $S_0$. Specifically, we study six variants: (A) Inverse Matroid Exists, where $S_0$ must contain at least one maximum-weight basis; (B) Inverse Matroid All, where all bases contained in $S_0$ are maximum-weight; and (C) Inverse Matroid Only, where $S_0$ contains exactly the maximum-weight bases, along with their natural negated counterparts.\n  For all variants, we develop combinatorial polynomial-time algorithms under the $\\ell_\\infty$-norm. A key ingredient is a refined min-max theorem for Inverse Matroid under the $\\ell_\\infty$-norm, which enables simpler and faster algorithms than previous approaches and may be of independent combinatorial interest. Our work significantly broadens the range of inverse optimization problems on matroids that can be solved efficiently, especially those that constrain the structure of optimal solutions through subset inclusion or exclusion.</article>","contentLength":1490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Generalization in Node and Link Prediction","url":"https://arxiv.org/abs/2507.00927","date":1751428800,"author":"","guid":179944,"unread":true,"content":"<article>arXiv:2507.00927v1 Announce Type: new \nAbstract: Using message-passing graph neural networks (MPNNs) for node and link prediction is crucial in various scientific and industrial domains, which has led to the development of diverse MPNN architectures. Besides working well in practical settings, their ability to generalize beyond the training set remains poorly understood. While some studies have explored MPNNs' generalization in graph-level prediction tasks, much less attention has been given to node- and link-level predictions. Existing works often rely on unrealistic i.i.d.\\@ assumptions, overlooking possible correlations between nodes or links, and assuming fixed aggregation and impractical loss functions while neglecting the influence of graph structure. In this work, we introduce a unified framework to analyze the generalization properties of MPNNs in inductive and transductive node and link prediction settings, incorporating diverse architectural parameters and loss functions and quantifying the influence of graph structure. Additionally, our proposed generalization framework can be applied beyond graphs to any classification task under the inductive or transductive setting. Our empirical study supports our theoretical insights, deepening our understanding of MPNNs' generalization capabilities in these tasks.</article>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HyperFusion: Hierarchical Multimodal Ensemble Learning for Social Media Popularity Prediction","url":"https://arxiv.org/abs/2507.00926","date":1751428800,"author":"","guid":179945,"unread":true,"content":"<article>arXiv:2507.00926v1 Announce Type: new \nAbstract: Social media popularity prediction plays a crucial role in content optimization, marketing strategies, and user engagement enhancement across digital platforms. However, predicting post popularity remains challenging due to the complex interplay between visual, textual, temporal, and user behavioral factors. This paper presents HyperFusion, a hierarchical multimodal ensemble learning framework for social media popularity prediction. Our approach employs a three-tier fusion architecture that progressively integrates features across abstraction levels: visual representations from CLIP encoders, textual embeddings from transformer models, and temporal-spatial metadata with user characteristics. The framework implements a hierarchical ensemble strategy combining CatBoost, TabNet, and custom multi-layer perceptrons. To address limited labeled data, we propose a two-stage training methodology with pseudo-labeling and iterative refinement. We introduce novel cross-modal similarity measures and hierarchical clustering features that capture inter-modal dependencies. Experimental results demonstrate that HyperFusion achieves competitive performance on the SMP challenge dataset. Our team achieved third place in the SMP Challenge 2025 (Image Track). The source code is available at https://anonymous.4open.science/r/SMPDImage.</article>","contentLength":1383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Privacy-Preserving Quantized Federated Learning with Diverse Precision","url":"https://arxiv.org/abs/2507.00920","date":1751428800,"author":"","guid":179946,"unread":true,"content":"<article>arXiv:2507.00920v1 Announce Type: new \nAbstract: Federated learning (FL) has emerged as a promising paradigm for distributed machine learning, enabling collaborative training of a global model across multiple local devices without requiring them to share raw data. Despite its advancements, FL is limited by factors such as: (i) privacy risks arising from the unprotected transmission of local model updates to the fusion center (FC) and (ii) decreased learning utility caused by heterogeneity in model quantization resolution across participating devices. Prior work typically addresses only one of these challenges because maintaining learning utility under both privacy risks and quantization heterogeneity is a non-trivial task. In this paper, our aim is therefore to improve the learning utility of a privacy-preserving FL that allows clusters of devices with different quantization resolutions to participate in each FL round. Specifically, we introduce a novel stochastic quantizer (SQ) that is designed to simultaneously achieve differential privacy (DP) and minimum quantization error. Notably, the proposed SQ guarantees bounded distortion, unlike other DP approaches. To address quantization heterogeneity, we introduce a cluster size optimization technique combined with a linear fusion approach to enhance model aggregation accuracy. Numerical simulations validate the benefits of our approach in terms of privacy protection and learning utility compared to the conventional LaplaceSQ-FL algorithm.</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey: Learning Embodied Intelligence from Physical Simulators and World Models","url":"https://arxiv.org/abs/2507.00917","date":1751428800,"author":"","guid":179947,"unread":true,"content":"<article>arXiv:2507.00917v1 Announce Type: new \nAbstract: The pursuit of artificial general intelligence (AGI) has placed embodied intelligence at the forefront of robotics research. Embodied intelligence focuses on agents capable of perceiving, reasoning, and acting within the physical world. Achieving robust embodied intelligence requires not only advanced perception and control, but also the ability to ground abstract cognition in real-world interactions. Two foundational technologies, physical simulators and world models, have emerged as critical enablers in this quest. Physical simulators provide controlled, high-fidelity environments for training and evaluating robotic agents, allowing safe and efficient development of complex behaviors. In contrast, world models empower robots with internal representations of their surroundings, enabling predictive planning and adaptive decision-making beyond direct sensory input. This survey systematically reviews recent advances in learning embodied AI through the integration of physical simulators and world models. We analyze their complementary roles in enhancing autonomy, adaptability, and generalization in intelligent robots, and discuss the interplay between external simulation and internal modeling in bridging the gap between simulated training and real-world deployment. By synthesizing current progress and identifying open challenges, this survey aims to provide a comprehensive perspective on the path toward more capable and generalizable embodied AI systems. We also maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.</article>","contentLength":1693,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Masks make discriminative models great again!","url":"https://arxiv.org/abs/2507.00916","date":1751428800,"author":"","guid":179948,"unread":true,"content":"<article>arXiv:2507.00916v1 Announce Type: new \nAbstract: We present Image2GS, a novel approach that addresses the challenging problem of reconstructing photorealistic 3D scenes from a single image by focusing specifically on the image-to-3D lifting component of the reconstruction process. By decoupling the lifting problem (converting an image to a 3D model representing what is visible) from the completion problem (hallucinating content not present in the input), we create a more deterministic task suitable for discriminative models. Our method employs visibility masks derived from optimized 3D Gaussian splats to exclude areas not visible from the source view during training. This masked training strategy significantly improves reconstruction quality in visible regions compared to strong baselines. Notably, despite being trained only on masked regions, Image2GS remains competitive with state-of-the-art discriminative models trained on full target images when evaluated on complete scenes. Our findings highlight the fundamental struggle discriminative models face when fitting unseen regions and demonstrate the advantages of addressing image-to-3D lifting as a distinct problem with specialized techniques.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MichelangeRoll: Sculpting Rational Distributions Exactly and Efficiently","url":"https://arxiv.org/abs/2507.00915","date":1751428800,"author":"","guid":179949,"unread":true,"content":"<article>arXiv:2507.00915v1 Announce Type: new \nAbstract: Simulating an arbitrary discrete distribution $D \\in [0, 1]^n$ using fair coin tosses incurs trade-offs between entropy complexity and space and time complexity. Shannon's theory suggests that $H(D)$ tosses are necessary and sufficient, but does not guarantee exact distribution. Knuth and Yao showed that a decision tree consumes fewer than $H(D) + 2$ tosses for one exact sample. Drapper and Saad's recent work addresses the space and time aspect, showing that $H(D) + 2$ tosses, $O(n \\log(n) \\log(m))$ memory, and $O(H(D))$ operations are all it costs, where $m$ is the common denominator of the probability masses in $D$ and $n$ is the number of possible outcomes.\n  In this paper, MichelangeRoll recycles leftover entropy to break the \"$+2$\" barrier. With $O((n + 1/\\varepsilon) \\log(m/\\varepsilon))$ memory, the entropy cost of generating a ongoing sequence of $D$ is reduced to $H(D) + \\varepsilon$ per sample.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications","url":"https://arxiv.org/abs/2507.00914","date":1751428800,"author":"","guid":179950,"unread":true,"content":"<article>arXiv:2507.00914v1 Announce Type: new \nAbstract: The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.</article>","contentLength":1678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cognate Data Bottleneck in Language Phylogenetics","url":"https://arxiv.org/abs/2507.00911","date":1751428800,"author":"","guid":179951,"unread":true,"content":"<article>arXiv:2507.00911v1 Announce Type: new \nAbstract: To fully exploit the potential of computational phylogenetic methods for cognate data one needs to leverage specific (complex) models an machine learning-based techniques. However, both approaches require datasets that are substantially larger than the manually collected cognate data currently available. To the best of our knowledge, there exists no feasible approach to automatically generate larger cognate datasets. We substantiate this claim by automatically extracting datasets from BabelNet, a large multilingual encyclopedic dictionary. We demonstrate that phylogenetic inferences on the respective character matrices yield trees that are largely inconsistent with the established gold standard ground truth trees. We also discuss why we consider it as being unlikely to be able to extract more suitable character matrices from other multilingual resources. Phylogenetic data analysis approaches that require larger datasets can therefore not be applied to cognate data. Thus, it remains an open question how, and if these computational approaches can be applied in historical linguistics.</article>","contentLength":1147,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona","url":"https://arxiv.org/abs/2507.00909","date":1751428800,"author":"","guid":179952,"unread":true,"content":"<article>arXiv:2507.00909v1 Announce Type: new \nAbstract: Artificial intelligence (AI) is fueling exponential electricity demand growth, threatening grid reliability, raising prices for communities paying for new energy infrastructure, and stunting AI innovation as data centers wait for interconnection to constrained grids. This paper presents the first field demonstration, in collaboration with major corporate partners, of a software-only approach--Emerald Conductor--that transforms AI data centers into flexible grid resources that can efficiently and immediately harness existing power systems without massive infrastructure buildout. Conducted at a 256-GPU cluster running representative AI workloads within a commercial, hyperscale cloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in cluster power usage for three hours during peak grid events while maintaining AI quality of service (QoS) guarantees. By orchestrating AI workloads based on real-time grid signals without hardware modifications or energy storage, this platform reimagines data centers as grid-interactive assets that enhance grid reliability, advance affordability, and accelerate AI's development.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses","url":"https://arxiv.org/abs/2507.00907","date":1751428800,"author":"","guid":179953,"unread":true,"content":"<article>arXiv:2507.00907v1 Announce Type: new \nAbstract: In a world where deepfakes and cloned voices are emerging as sophisticated attack vectors, organizations require a new security mindset: Sensorial Zero Trust [9]. This article presents a scientific analysis of the need to systematically doubt information perceived through the senses, establishing rigorous verification protocols to mitigate the risks of fraud based on generative artificial intelligence. Key concepts, such as Out-of-Band verification, Vision-Language Models (VLMs) as forensic collaborators, cryptographic provenance, and human training, are integrated into a framework that extends Zero Trust principles to human sensory information. The approach is grounded in empirical findings and academic research, emphasizing that in an era of AI-generated realities, even our eyes and ears can no longer be implicitly trusted without verification. Leaders are called to foster a culture of methodological skepticism to protect organizational integrity in this new threat landscape.</article>","contentLength":1041,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks","url":"https://arxiv.org/abs/2507.00902","date":1751428800,"author":"","guid":179954,"unread":true,"content":"<article>arXiv:2507.00902v1 Announce Type: new \nAbstract: Direct-satellite-to-device (DS2D) communication is emerging as a promising solution for global mobile service extension, leveraging the deployment of satellite constellations. However, the challenge of managing DS2D connectivity for multi-constellations becomes outstanding, including high interference and frequent handovers caused by multi-coverage overlap and rapid satellite movement. Moreover, existing approaches primarily operate within single-constellation shell, which inherently limits the ability to exploit the vast potential of multi-constellation connectivity provision, resulting in suboptimal DS2D service performances. To address these challenges, this article proposes a Constellation as a Service (CaaS) framework, which treats the entire multi-constellation infrastructure as a shared resource pool and dynamically forms optimal sub-constellations (SCs) for each DS2D service region. The formation of each SC integrates satellites from various orbits to provide tailored connectivity based on user demands, guided by two innovative strategies: predictive satellite beamforming using generative artificial intelligence (GenAI) and pre-configured handover path for efficient satellite access and mobility management. Simulation results demonstrate that CaaS significantly improves satellite service rates while reducing handover overhead, making it an efficient and continuable solution for managing DS2D connectivity in multi-constellation environments.</article>","contentLength":1521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality","url":"https://arxiv.org/abs/2507.00899","date":1751428800,"author":"","guid":179955,"unread":true,"content":"<article>arXiv:2507.00899v1 Announce Type: new \nAbstract: State-of-the-art models for 3D molecular generation are based on significant inductive biases, SE(3), permutation equivariance to respect symmetry and graph message-passing networks to capture local chemistry, yet the generated molecules still struggle with physical plausibility. We introduce TABASCO which relaxes these assumptions: The model has a standard non-equivariant transformer architecture, treats atoms in a molecule as sequences and reconstructs bonds deterministically after generation. The absence of equivariant layers and message passing allows us to significantly simplify the model architecture and scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves state-of-the-art PoseBusters validity and delivers inference roughly 10x faster than the strongest baseline, while exhibiting emergent rotational equivariance despite symmetry not being hard-coded. Our work offers a blueprint for training minimalist, high-throughput generative models suited to specialised tasks such as structure- and pharmacophore-based drug design. We provide a link to our implementation at github.com/carlosinator/tabasco.</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models","url":"https://arxiv.org/abs/2507.00898","date":1751428800,"author":"","guid":179956,"unread":true,"content":"<article>arXiv:2507.00898v1 Announce Type: new \nAbstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm for understanding and reasoning about image input through textual responses. Although they have achieved remarkable performance across a range of multi-modal tasks, they face the persistent challenge of hallucination, which introduces practical weaknesses and raises concerns about their reliable deployment in real-world applications. Existing work has explored contrastive decoding approaches to mitigate this issue, where the output of the original LVLM is compared and contrasted with that of a perturbed version. However, these methods require two or more queries that slow down LVLM response generation, making them less suitable for real-time applications. To overcome this limitation, we propose ONLY, a training-free decoding approach that requires only a single query and a one-layer intervention during decoding, enabling efficient real-time deployment. Specifically, we enhance textual outputs by selectively amplifying crucial textual information using a text-to-visual entropy ratio for each token. Extensive experimental results demonstrate that our proposed ONLY consistently outperforms state-of-the-art methods across various benchmarks while requiring minimal implementation effort and computational cost. Code is available at https://github.com/zifuwan/ONLY.</article>","contentLength":1402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QUIC Delay Control: an implementation of congestion and delay control","url":"https://arxiv.org/abs/2507.00896","date":1751428800,"author":"","guid":179957,"unread":true,"content":"<article>arXiv:2507.00896v1 Announce Type: new \nAbstract: A new congestion and delay control algorithm named QUIC Delay Control (QUIC-DC) is proposed for controlling not only congestion but also the queuing delay encountered along the forward communication path. The core idea is to estimate the one-way queuing delay of a connection to trigger an early reaction to congestion. This idea, along with the TCP Westwood+ congestion control algorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2, NewReno, Westwood+. The results obtained in both emulated and real network connections show that QUIC-DC can significantly reduce packet losses along with end-to-end communication delays, while preserving network utilization, features that are both very useful for real-time applications.</article>","contentLength":796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes","url":"https://arxiv.org/abs/2507.00891","date":1751428800,"author":"","guid":179958,"unread":true,"content":"<article>arXiv:2507.00891v1 Announce Type: new \nAbstract: Memes are widely used in online social interactions, providing vivid, intuitive, and often humorous means to express intentions and emotions. Existing dialogue datasets are predominantly limited to either manually annotated or pure-text conversations, lacking the expressiveness and contextual nuance that multimodal interactions provide.To address these challenges, we introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue dataset with contextually retrieved memes. Our dataset combines a large-scale, MLLM-annotated meme library with dialogues auto-generated by dual agents across diverse scenarios. We introduce a retrieval framework and adaptive threshold to ensure contextually relevant, naturally spaced meme usage. Experiments demonstrate the effectiveness of our approach in generating contextually appropriate and diverse meme-incorporated dialogues, offering a scalable and privacy-preserving resource for advancing multimodal conversational AI.</article>","contentLength":1024,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond","url":"https://arxiv.org/abs/2507.00886","date":1751428800,"author":"","guid":179959,"unread":true,"content":"<article>arXiv:2507.00886v1 Announce Type: new \nAbstract: As multimodal language models advance, their application to 3D scene understanding is a fast-growing frontier, driving the development of 3D Vision-Language Models (VLMs). Current methods show strong dependence on object detectors, introducing processing bottlenecks and limitations in taxonomic flexibility. To address these limitations, we propose a scene-centric 3D VLM for 3D Gaussian splat scenes that employs language- and task-aware scene representations. Our approach directly embeds rich linguistic features into the 3D scene representation by associating language with each Gaussian primitive, achieving early modality alignment. To process the resulting dense representations, we introduce a dual sparsifier that distills them into compact, task-relevant tokens via task-guided and location-guided pathways, producing sparse, task-aware global and local scene tokens. Notably, we present the first Gaussian splatting-based VLM, leveraging photorealistic 3D representations derived from standard RGB images, demonstrating strong generalization: it improves performance of prior 3D VLM five folds, in out-of-the-domain settings.</article>","contentLength":1186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check","url":"https://arxiv.org/abs/2507.00885","date":1751428800,"author":"","guid":179960,"unread":true,"content":"<article>arXiv:2507.00885v1 Announce Type: new \nAbstract: Downstream scaling laws aim to predict task performance at larger scales from pretraining losses at smaller scales. Whether this prediction should be possible is unclear: some works demonstrate that task performance follows clear linear scaling trends under transformation, whereas others point out fundamental challenges to downstream scaling laws, such as emergence and inverse scaling. In this work, we conduct a meta-analysis of existing data on downstream scaling laws, finding that close fit to linear scaling laws only occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign changes to the experimental setting can completely change the scaling trend. Our analysis underscores the need to understand the conditions under which scaling laws succeed. To fully model the relationship between pretraining loss and downstream task performance, we must embrace the cases in which scaling behavior deviates from linear trends.</article>","contentLength":997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations","url":"https://arxiv.org/abs/2507.00883","date":1751428800,"author":"","guid":179961,"unread":true,"content":"<article>arXiv:2507.00883v1 Announce Type: new \nAbstract: Although mathematics is often considered culturally neutral, the way mathematical problems are presented can carry implicit cultural context. Existing benchmarks like GSM8K are predominantly rooted in Western norms, including names, currencies, and everyday scenarios. In this work, we create culturally adapted variants of the GSM8K test set for five regions Africa, India, China, Korea, and Japan using prompt-based transformations followed by manual verification. We evaluate six large language models (LLMs), ranging from 8B to 72B parameters, across five prompting strategies to assess their robustness to cultural variation in math problem presentation. Our findings reveal a consistent performance gap: models perform best on the original US-centric dataset and comparatively worse on culturally adapted versions. However, models with reasoning capabilities are more resilient to these shifts, suggesting that deeper reasoning helps bridge cultural presentation gaps in mathematical tasks</article>","contentLength":1044,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Move Therefore I Learn: Experience-Based Traversability in Outdoor Robotics","url":"https://arxiv.org/abs/2507.00882","date":1751428800,"author":"","guid":179962,"unread":true,"content":"<article>arXiv:2507.00882v1 Announce Type: new \nAbstract: Accurate traversability estimation is essential for safe and effective navigation of outdoor robots operating in complex environments. This paper introduces a novel experience-based method that allows robots to autonomously learn which terrains are traversable based on prior navigation experience, without relying on extensive pre-labeled datasets. The approach integrates elevation and texture data into multi-layered grid maps, which are processed using a variational autoencoder (VAE) trained on a generic texture dataset. During an initial teleoperated phase, the robot collects sensory data while moving around the environment. These experiences are encoded into compact feature vectors and clustered using the BIRCH algorithm to represent traversable terrain areas efficiently. In deployment, the robot compares new terrain patches to its learned feature clusters to assess traversability in real time. The proposed method does not require training with data from the targeted scenarios, generalizes across diverse surfaces and platforms, and dynamically adapts as new terrains are encountered. Extensive evaluations on both synthetic benchmarks and real-world scenarios with wheeled and legged robots demonstrate its effectiveness, robustness, and superior adaptability compared to state-of-the-art approaches.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Difficulty-Aware Analysis of Deep Neural Networks","url":"https://arxiv.org/abs/2507.00881","date":1751428800,"author":"","guid":179963,"unread":true,"content":"<article>arXiv:2507.00881v1 Announce Type: new \nAbstract: Traditional instance-based model analysis focuses mainly on misclassified instances. However, this approach overlooks the varying difficulty associated with different instances. Ideally, a robust model should recognize and reflect the challenges presented by intrinsically difficult instances. It is also valuable to investigate whether the difficulty perceived by the model aligns with that perceived by humans. To address this, we propose incorporating instance difficulty into the deep neural network evaluation process, specifically for supervised classification tasks on image data. Specifically, we consider difficulty measures from three perspectives -- data, model, and human -- to facilitate comprehensive evaluation and comparison. Additionally, we develop an interactive visual tool, DifficultyEyes, to support the identification of instances of interest based on various difficulty patterns and to aid in analyzing potential data or model issues. Case studies demonstrate the effectiveness of our approach.</article>","contentLength":1067,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NN-Former: Rethinking Graph Structure in Neural Architecture Representation","url":"https://arxiv.org/abs/2507.00880","date":1751428800,"author":"","guid":179964,"unread":true,"content":"<article>arXiv:2507.00880v1 Announce Type: new \nAbstract: The growing use of deep learning necessitates efficient network design and deployment, making neural predictors vital for estimating attributes such as accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers have shown promising performance in representing neural architectures. However, each of both methods has its disadvantages. GNNs lack the capabilities to represent complicated features, while transformers face poor generalization when the depth of architecture grows. To mitigate the above issues, we rethink neural architecture topology and show that sibling nodes are pivotal while overlooked in previous research. We thus propose a novel predictor leveraging the strengths of GNNs and transformers to learn the enhanced topology. We introduce a novel token mixer that considers siblings, and a new channel mixer named bidirectional graph isomorphism feed-forward network. Our approach consistently achieves promising performance in both accuracy and latency prediction, providing valuable insights for learning Directed Acyclic Graph (DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite","url":"https://arxiv.org/abs/2507.00877","date":1751428800,"author":"","guid":179965,"unread":true,"content":"<article>arXiv:2507.00877v1 Announce Type: new \nAbstract: Empirical evaluation of state-of-the-art natural-language (NL) to temporal-logic (TL) translation systems reveals near-perfect performance on existing benchmarks. However, current studies measure only the accuracy of the translation of NL logic into formal TL, ignoring a system's capacity to ground atomic propositions into new scenarios or environments. This is a critical feature, necessary for the verification of resulting formulas in a concrete state space. Consequently, most NL-to-TL translation frameworks propose their own bespoke dataset in which the correct grounding is known a-priori, inflating performance metrics and neglecting the need for extensible, domain-general systems. In this paper, we introduce the Verifiable Linear Temporal Logic Benchmark ( VLTL-Bench), a unifying benchmark that measures verification and verifiability of automated NL-to-LTL translation. The dataset consists of three unique state spaces and thousands of diverse natural language specifications and corresponding formal specifications in temporal logic. Moreover, the benchmark contains sample traces to validate the temporal logic expressions. While the benchmark directly supports end-to-end evaluation, we observe that many frameworks decompose the process into i) lifting, ii) grounding, iii) translation, and iv) verification. The benchmark provides ground truths after each of these steps to enable researches to improve and evaluate different substeps of the overall problem. To encourage methodologically sound advances in verifiable NL-to-LTL translation approaches, we release VLTL-Bench here: https://www.kaggle.com/datasets/dubascudes/vltl bench.</article>","contentLength":1704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation","url":"https://arxiv.org/abs/2507.00875","date":1751428800,"author":"","guid":179966,"unread":true,"content":"<article>arXiv:2507.00875v1 Announce Type: new \nAbstract: Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.</article>","contentLength":1395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Visual in-Context Learning for Compositional Medical Tasks within Reach?","url":"https://arxiv.org/abs/2507.00868","date":1751428800,"author":"","guid":179967,"unread":true,"content":"<article>arXiv:2507.00868v1 Announce Type: new \nAbstract: In this paper, we explore the potential of visual in-context learning to enable a single model to handle multiple tasks and adapt to new tasks during test time without re-training. Unlike previous approaches, our focus is on training in-context learners to adapt to sequences of tasks, rather than individual tasks. Our goal is to solve complex tasks that involve multiple intermediate steps using a single model, allowing users to define entire vision pipelines flexibly at test time. To achieve this, we first examine the properties and limitations of visual in-context learning architectures, with a particular focus on the role of codebooks. We then introduce a novel method for training in-context learners using a synthetic compositional task generation engine. This engine bootstraps task sequences from arbitrary segmentation datasets, enabling the training of visual in-context learners for compositional tasks. Additionally, we investigate different masking-based training objectives to gather insights into how to train models better for solving complex, compositional tasks. Our exploration not only provides important insights especially for multi-modal medical task sequences but also highlights challenges that need to be addressed.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals","url":"https://arxiv.org/abs/2507.00862","date":1751428800,"author":"","guid":179968,"unread":true,"content":"<article>arXiv:2507.00862v1 Announce Type: new \nAbstract: Accurately predicting potato sprouting before the emergence of any visual signs is critical for effective storage management, as sprouting degrades both the commercial and nutritional value of tubers. Effective forecasting allows for the precise application of anti-sprouting chemicals (ASCs), minimizing waste and reducing costs. This need has become even more pressing following the ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to health and environmental concerns, which has led to the adoption of significantly more expensive alternative ASCs. Existing approaches primarily rely on visual identification, which only detects sprouting after morphological changes have occurred, limiting their effectiveness for proactive management. A reliable early prediction method is therefore essential to enable timely intervention and improve the efficiency of post-harvest storage strategies, where early refers to detecting sprouting before any visible signs appear. In this work, we address the problem of early prediction of potato sprouting. To this end, we propose a novel machine learning (ML)-based approach that enables early prediction of potato sprouting using electrophysiological signals recorded from tubers using proprietary sensors. Our approach preprocesses the recorded signals, extracts relevant features from the wavelet domain, and trains supervised ML models for early sprouting detection. Additionally, we incorporate uncertainty quantification techniques to enhance predictions. Experimental results demonstrate promising performance in the early detection of potato sprouting by accurately predicting the exact day of sprouting for a subset of potatoes and while showing acceptable average error across all potatoes. Despite promising results, further refinements are necessary to minimize prediction errors, particularly in reducing the maximum observed deviations.</article>","contentLength":1961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SafeMap: Robust HD Map Construction from Incomplete Observations","url":"https://arxiv.org/abs/2507.00861","date":1751428800,"author":"","guid":179969,"unread":true,"content":"<article>arXiv:2507.00861v1 Announce Type: new \nAbstract: Robust high-definition (HD) map construction is vital for autonomous driving, yet existing methods often struggle with incomplete multi-view camera data. This paper presents SafeMap, a novel framework specifically designed to secure accuracy even when certain camera views are missing. SafeMap integrates two key components: the Gaussian-based Perspective View Reconstruction (G-PVR) module and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module. G-PVR leverages prior knowledge of view importance to dynamically prioritize the most informative regions based on the relationships among available camera views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV representations derived from incomplete observations. Together, these components facilitate the end-to-end map reconstruction and robust HD map generation. SafeMap is easy to implement and integrates seamlessly into existing systems, offering a plug-and-play solution for enhanced robustness. Experimental results demonstrate that SafeMap significantly outperforms previous methods in both complete and incomplete scenarios, highlighting its superior performance and reliability.</article>","contentLength":1230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Vehicular Platooning with Wireless Federated Learning: A Resource-Aware Control Framework","url":"https://arxiv.org/abs/2507.00856","date":1751428800,"author":"","guid":179970,"unread":true,"content":"<article>arXiv:2507.00856v1 Announce Type: new \nAbstract: This paper aims to enhance the performance of Vehicular Platooning (VP) systems integrated with Wireless Federated Learning (WFL). In highly dynamic environments, vehicular platoons experience frequent communication changes and resource constraints, which significantly affect information exchange and learning model synchronization. To address these challenges, we first formulate WFL in VP as a joint optimization problem that simultaneously considers Age of Information (AoI) and Federated Learning Model Drift (FLMD) to ensure timely and accurate control. Through theoretical analysis, we examine the impact of FLMD on convergence performance and develop a two-stage Resource-Aware Control framework (RACE). The first stage employs a Lagrangian dual decomposition method for resource configuration, while the second stage implements a multi-agent deep reinforcement learning approach for vehicle selection. The approach integrates Multi-Head Self-Attention and Long Short-Term Memory networks to capture spatiotemporal correlations in communication states. Experimental results demonstrate that, compared to baseline methods, the proposed framework improves AoI optimization by up to 45%, accelerates learning convergence, and adapts more effectively to dynamic VP environments on the AI4MARS dataset.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A New Family of Thread to Core Allocation Policies for an SMT ARM Processor","url":"https://arxiv.org/abs/2507.00855","date":1751428800,"author":"","guid":179971,"unread":true,"content":"<article>arXiv:2507.00855v1 Announce Type: new \nAbstract: Modern high-performance servers commonly integrate Simultaneous Multithreading (SMT) processors, which efficiently boosts throughput over single-threaded cores. Optimizing performance in SMT processors faces challenges due to the inter-application interference within each SMT core. To mitigate the interference, thread-to-core (T2C) allocation policies play a pivotal role. State-of-the-art T2C policies work in two steps: i) building a per-application performance stack using performance counters and ii) building performance prediction models to identify the best pairs of applications to run on each core.\n  This paper explores distinct ways to build the performance stack in ARM processors and introduces the Instructions and Stalls Cycles (ISC) stack, a novel approach to overcome ARM PMU limitations. The ISC stacks are used as inputs for a performance prediction model to estimate the applications' performance considering the inter-application interference. The accuracy of the prediction model (second step) depends on the accuracy of the performance stack (first step); thus, the higher the accuracy of the performance stack, the higher the potential performance gains obtained by the T2C allocation policy.\n  This paper presents SYNPA as a family of T2C allocation policies. Experimental results show that $SYNPA4$, the best-performing SYNPA variant, outperforms turnaround time by 38\\% over Linux, which represents 3$\\times$ the gains achieved by the state-of-the-art policies for ARM processors. Furthermore, the multiple discussions and refinements presented throughout this paper can be applied to other SMT processors from distinct vendors and are aimed at helping performance analysts build performance stacks for accurate performance estimates in real processors.</article>","contentLength":1831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting","url":"https://arxiv.org/abs/2507.00852","date":1751428800,"author":"","guid":179972,"unread":true,"content":"<article>arXiv:2507.00852v1 Announce Type: new \nAbstract: Flexible manufacturing systems in Industry 4.0 require robots capable of handling objects in unstructured environments without rigid positioning constraints. This paper presents a computer vision system that enables industrial robots to detect and grasp pen components in arbitrary orientations without requiring structured trays, while maintaining robust performance under varying lighting conditions. We implement and evaluate a Mask R-CNN-based approach on a complete pen manufacturing line at ZHAW, addressing three critical challenges: object detection without positional constraints, robustness to extreme lighting variations, and reliable performance with cost-effective cameras. Our system achieves 95% detection accuracy across diverse lighting conditions while eliminating the need for structured component placement, demonstrating a 30% reduction in setup time and significant improvement in manufacturing flexibility. The approach is validated through extensive testing under four distinct lighting scenarios, showing practical applicability for real-world industrial deployment.</article>","contentLength":1140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aligning Learning and Endogenous Decision-Making","url":"https://arxiv.org/abs/2507.00851","date":1751428800,"author":"","guid":179973,"unread":true,"content":"<article>arXiv:2507.00851v1 Announce Type: new \nAbstract: Many of the observations we make are biased by our decisions. For instance, the demand of items is impacted by the prices set, and online checkout choices are influenced by the assortments presented. The challenge in decision-making under this setting is the lack of counterfactual information, and the need to learn it instead. We introduce an end-to-end method under endogenous uncertainty to train ML models to be aware of their downstream, enabling their effective use in the decision-making stage. We further introduce a robust optimization variant that accounts for uncertainty in ML models -- specifically by constructing uncertainty sets over the space of ML models and optimizing actions to protect against worst-case predictions. We prove guarantees that this robust approach can capture near-optimal decisions with high probability as a function of data. Besides this, we also introduce a new class of two-stage stochastic optimization problems to the end-to-end learning framework that can now be addressed through our framework. Here, the first stage is an information-gathering problem to decide which random variable to poll and gain information about before making a second-stage decision based off of it. We present several computational experiments for pricing and inventory assortment/recommendation problems. We compare against existing methods in online learning/bandits/offline reinforcement learning and show our approach has consistent improved performance over these. Just as in the endogenous setting, the model's prediction also depends on the first-stage decision made. While this decision does not affect the random variable in this setting, it does affect the correct point forecast that should be made.</article>","contentLength":1782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection","url":"https://arxiv.org/abs/2507.00849","date":1751428800,"author":"","guid":179974,"unread":true,"content":"<article>arXiv:2507.00849v1 Announce Type: new \nAbstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in traffic management, agriculture, emergency rescue, etc. However, it faces significant challenges, including occlusions, small object sizes, and irregular shapes. These challenges highlight the necessity for a robust and efficient multimodal UAV object detection method. Mamba has demonstrated considerable potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a multimodal UAV object detection framework based on Mamba architectures. To improve geometric adaptability, we propose the Deformable Token Mamba Block (DTMB) to generate deformable tokens by incorporating adaptive patches from deformable convolutions alongside normal patches from normal convolutions, which serve as the inputs to the Mamba Block. To optimize the multimodal feature complementarity, we design two separate DTMBs for the RGB and infrared (IR) modalities, with the outputs from both DTMBs integrated into the Mamba Block for feature extraction and into the Fusion Mamba Block for feature fusion. Additionally, to improve multiscale object detection, especially for small objects, we stack four DTMBs at different scales to produce multiscale feature representations, which are then sent to the Detection Neck for Mamba (DNM). The DNM module, inspired by the YOLO series, includes modifications to the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In particular, we employ cross-enhanced spatial attention before the DTMB and cross-channel attention after the Fusion Mamba Block to extract more discriminative features. Experimental results on the DroneVehicle dataset show that our method outperforms the baseline OAFA method by 3.6% in the mAP metric. Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.</article>","contentLength":1869,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters","url":"https://arxiv.org/abs/2507.00848","date":1751428800,"author":"","guid":179975,"unread":true,"content":"<article>arXiv:2507.00848v1 Announce Type: new \nAbstract: HIV epidemiological data is increasingly complex, requiring advanced computation for accurate cluster detection and forecasting. We employed quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code level using AIDSVu and synthetic SDoH data for 2022. Our approach compared classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV prevalence forecasting, and used quantum Bayesian networks to explore causal links between SDoH factors and HIV incidence. The QAOA-based method achieved 92% accuracy in cluster detection within 1.6 seconds, outperforming classical algorithms. Meanwhile, the hybrid quantum-classical neural network predicted HIV prevalence with 94% accuracy, surpassing a purely classical counterpart. Quantum Bayesian analysis identified housing instability as a key driver of HIV cluster emergence and expansion, with stigma exerting a geographically variable influence. These quantum-enhanced methods deliver greater precision and efficiency in HIV surveillance while illuminating critical causal pathways. This work can guide targeted interventions, optimize resource allocation for PrEP, and address structural inequities fueling HIV transmission.</article>","contentLength":1337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing","url":"https://arxiv.org/abs/2507.00847","date":1751428800,"author":"","guid":179976,"unread":true,"content":"<article>arXiv:2507.00847v1 Announce Type: new \nAbstract: Bluetooth is a pervasive wireless communication technology used by billions of devices for short-range connectivity. The security of Bluetooth relies on the pairing process, where devices establish shared long-term keys for secure communications. However, many commercial Bluetooth devices implement automatic pairing functions to improve user convenience, creating a previously unexplored attack surface.\n  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in the automatic pairing functions in commercial Bluetooth devices to achieve completely silent device link key overwriting. The Stealtooth attack leverages the fact that Bluetooth audio devices automatically transition to pairing mode under specific conditions, enabling attackers to hijack pairing processes without user awareness or specialized tools. We also extend the attack into the MitM Stealtooth attack, combining automatic pairing abuse with power-saving mode techniques to enable man-in-the-middle attacks.\n  We evaluate the attacks against 10 commercial Bluetooth devices from major manufacturers, demonstrating widespread vulnerabilities across diverse device types and manufacturers. Our practical implementation requires only commodity hardware and open-source software, highlighting the low barrier to entry for attackers.\n  We propose defenses both device and protocol levels, including enhanced user notifications and standardized automatic pairing guidelines. Our findings reveal a critical tension between security and usability, showing that current automatic pairing implementations create systematic vulnerabilities. We responsibly disclosed our findings to affected vendors, with several already releasing patches.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation","url":"https://arxiv.org/abs/2507.00846","date":1751428800,"author":"","guid":179977,"unread":true,"content":"<article>arXiv:2507.00846v1 Announce Type: new \nAbstract: Efficient sampling from the Boltzmann distribution defined by an energy function is a key challenge in modeling physical systems such as molecules. Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows that transform a simple prior into a distribution that can be reweighted to match the Boltzmann distribution using sample likelihoods. However, obtaining likelihoods requires computing costly Jacobians during integration, making it impractical for large molecular systems. To overcome this, we propose learning the likelihood of the generated distribution via an energy-based model trained with noise contrastive estimation and score matching. By using stochastic interpolants to anneal between the prior and generated distributions, we combine both the objective functions to efficiently learn the density function. On the alanine dipeptide system, we demonstrate that our method yields free energy profiles and energy distributions comparable to those obtained with exact likelihoods. Additionally, we show that free energy differences between metastable states can be estimated accurately with orders-of-magnitude speedup.</article>","contentLength":1198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Echo Top Heights Improve Deep Learning Nowcasts?","url":"https://arxiv.org/abs/2507.00845","date":1751428800,"author":"","guid":179978,"unread":true,"content":"<article>arXiv:2507.00845v1 Announce Type: new \nAbstract: Precipitation nowcasting -- the short-term prediction of rainfall using recent radar observations -- is critical for weather-sensitive sectors such as transportation, agriculture, and disaster mitigation. While recent deep learning models have shown promise in improving nowcasting skill, most approaches rely solely on 2D radar reflectivity fields, discarding valuable vertical information available in the full 3D radar volume. In this work, we explore the use of Echo Top Height (ETH), a 2D projection indicating the maximum altitude of radar reflectivity above a given threshold, as an auxiliary input variable for deep learning-based nowcasting. We examine the relationship between ETH and radar reflectivity, confirming its relevance for predicting rainfall intensity. We implement a single-pass 3D U-Net that processes both the radar reflectivity and ETH as separate input channels. While our models are able to leverage ETH to improve skill at low rain-rate thresholds, results are inconsistent at higher intensities and the models with ETH systematically underestimate precipitation intensity. Three case studies are used to illustrate how ETH can help in some cases, but also confuse the models and increase the error variance. Nonetheless, the study serves as a foundation for critically assessing the potential contribution of additional variables to nowcasting performance.</article>","contentLength":1435,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents","url":"https://arxiv.org/abs/2507.00841","date":1751428800,"author":"","guid":179979,"unread":true,"content":"<article>arXiv:2507.00841v1 Announce Type: new \nAbstract: With the wide application of multimodal foundation models in intelligent agent systems, scenarios such as mobile device control, intelligent assistant interaction, and multimodal task execution are gradually relying on such large model-driven agents. However, the related systems are also increasingly exposed to potential jailbreak risks. Attackers may induce the agents to bypass the original behavioral constraints through specific inputs, and then trigger certain risky and sensitive operations, such as modifying settings, executing unauthorized commands, or impersonating user identities, which brings new challenges to system security. Existing security measures for intelligent agents still have limitations when facing complex interactions, especially in detecting potentially risky behaviors across multiple rounds of conversations or sequences of tasks. In addition, an efficient and consistent automated methodology to assist in assessing and determining the impact of such risks is currently lacking. This work explores the security issues surrounding mobile multimodal agents, attempts to construct a risk discrimination mechanism by incorporating behavioral sequence information, and designs an automated assisted assessment scheme based on a large language model. Through preliminary validation in several representative high-risk tasks, the results show that the method can improve the recognition of risky behaviors to some extent and assist in reducing the probability of agents being jailbroken. We hope that this study can provide some valuable references for the security risk modeling and protection of multimodal intelligent agent systems.</article>","contentLength":1712,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries","url":"https://arxiv.org/abs/2507.00839","date":1751428800,"author":"","guid":179980,"unread":true,"content":"<article>arXiv:2507.00839v1 Announce Type: new \nAbstract: Dynamic graph storage systems are essential for real-time applications such as social networks and recommendation, where graph data continuously evolves. However, they face significant challenges in efficiently handling concurrent read and write operations. We find that existing methods suffer from write queries interfering with read efficiency, substantial time and space overhead due to per-edge versioning, and an inability to balance performance, such as slow searches under concurrent workloads. To address these issues, we propose RapidStore, a holistic approach for efficient in-memory dynamic graph storage designed for read-intensive workloads. Our key idea is to exploit the characteristics of graph queries through a decoupled system design that separates the management of read and write queries and decouples version data from graph data. Particularly, we design an efficient dynamic graph store to cooperate with the graph concurrency control mechanism. Experimental results demonstrate that RapidStore enables fast and scalable concurrent graph queries, effectively balancing the performance of inserts, searches, and scans, and significantly improving efficiency in dynamic graph storage systems.</article>","contentLength":1263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stylometry recognizes human and LLM-generated texts in short samples","url":"https://arxiv.org/abs/2507.00838","date":1751428800,"author":"","guid":179981,"unread":true,"content":"<article>arXiv:2507.00838v1 Announce Type: new \nAbstract: The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show -- crucially, in the context of the increasingly sophisticated LLMs -- that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type.</article>","contentLength":1723,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning","url":"https://arxiv.org/abs/2507.00833","date":1751428800,"author":"","guid":179982,"unread":true,"content":"<article>arXiv:2507.00833v1 Announce Type: new \nAbstract: For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. Project page is https://openhumanoidgen.github.io.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Surprising Efficacy of LLMs for Penetration-Testing","url":"https://arxiv.org/abs/2507.00829","date":1751428800,"author":"","guid":179983,"unread":true,"content":"<article>arXiv:2507.00829v1 Announce Type: new \nAbstract: This paper presents a critical examination of the surprising efficacy of Large Language Models (LLMs) in penetration testing. The paper thoroughly reviews the evolution of LLMs and their rapidly expanding capabilities which render them increasingly suitable for complex penetration testing operations. It systematically details the historical adoption of LLMs in both academic research and industry, showcasing their application across various offensive security tasks and covering broader phases of the cyber kill chain. Crucially, the analysis also extends to the observed adoption of LLMs by malicious actors, underscoring the inherent dual-use challenge of this technology within the security landscape.\n  The unexpected effectiveness of LLMs in this context is elucidated by several key factors: the strong alignment between penetration testing's reliance on pattern-matching and LLMs' core strengths, their inherent capacity to manage uncertainty in dynamic environments, and cost-effective access to competent pre-trained models through LLM providers.\n  The current landscape of LLM-aided penetration testing is categorized into interactive 'vibe-hacking' and the emergence of fully autonomous systems. The paper identifies and discusses significant obstacles impeding wider adoption and safe deployment. These include critical issues concerning model reliability and stability, paramount safety and security concerns, substantial monetary and ecological costs, implications for privacy and digital sovereignty, complex questions of accountability, and profound ethical dilemmas. This comprehensive review and analysis provides a foundation for discussion on future research directions and the development of robust safeguards at the intersection of AI and security.</article>","contentLength":1822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering","url":"https://arxiv.org/abs/2507.00828","date":1751428800,"author":"","guid":179984,"unread":true,"content":"<article>arXiv:2507.00828v1 Announce Type: new \nAbstract: Topic model and document-clustering evaluations either use automated metrics that align poorly with human preferences or require expert labels that are intractable to scale. We design a scalable human evaluation protocol and a corresponding automated approximation that reflect practitioners' real-world usage of models. Annotators -- or an LLM-based proxy -- review text items assigned to a topic or cluster, infer a category for the group, then apply that category to other documents. Using this protocol, we collect extensive crowdworker annotations of outputs from a diverse set of topic models on two datasets. We then use these annotations to validate automated proxies, finding that the best LLM proxies are statistically indistinguishable from a human annotator and can therefore serve as a reasonable substitute in automated evaluations. Package, web interface, and data are at https://github.com/ahoho/proxann</article>","contentLength":968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Technique for the Detection of PDF Tampering or Forgery","url":"https://arxiv.org/abs/2507.00827","date":1751428800,"author":"","guid":179985,"unread":true,"content":"<article>arXiv:2507.00827v1 Announce Type: new \nAbstract: Tampering or forgery of digital documents has become widespread, most commonly through altering images without any malicious intent such as enhancing the overall appearance of the image. However, there are occasions when tampering of digital documents can have negative consequences, such as financial fraud and reputational damage. Tampering can occur through altering a digital document's text or editing an image's pixels. Many techniques have been developed to detect whether changes have been made to a document. Most of these techniques rely on generating hashes or watermarking the document. These techniques, however, have limitations in that they cannot detect alterations to portable document format (PDF) signatures or other non-visual aspects, such as metadata. This paper presents a new technique that can be used to detect tampering within a PDF document by utilizing the PDF document's file page objects. The technique employs a prototype that can detect changes to a PDF document, such as changes made to the text, images, or metadata of the said file.</article>","contentLength":1117,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Getting Dynamic Line Ratings into Markets","url":"https://arxiv.org/abs/2507.00826","date":1751428800,"author":"","guid":179986,"unread":true,"content":"<article>arXiv:2507.00826v1 Announce Type: new \nAbstract: Static transmission line ratings may lead to underutilization of line capacity due to overly conservative (worst-case) assumptions. Grid-enhancing technologies (GETs) such as dynamic line ratings (DLRs), which adjust line capacity based on real-time conditions, are a techno-economically viable alternative to increase the utilization of existing power lines. Nonetheless, their adoption has been slow, partly due to the absence of operational tools that effectively account for simultaneous impacts on dispatch and pricing. In this paper, we represent transmission capacity with DLRs as a stock-like resource with time-variant interdependency, which is modeled via an approximation of line temperature evolution process, decoupling the impacts of ambient weather conditions and power flow on transmission line temperature and thus capacity. We integrate DLRs into a multi-period DC optimal power flow problem, with chance constrains addressing correlated uncertainty in DLRs and renewable generation. This yields non-convex problems that we transform into a tractable convex form by linearization. We derive locational marginal energy and ancillary services prices consistent with a competitive equilibrium. Numerical experiments on the 11-zone and 1814-node NYISO systems demonstrate its performance, including impacts on dispatch, pricing, and marginal carbon emissions.</article>","contentLength":1422,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery","url":"https://arxiv.org/abs/2507.00825","date":1751428800,"author":"","guid":179987,"unread":true,"content":"<article>arXiv:2507.00825v1 Announce Type: new \nAbstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial challenges, including small target sizes, high-density distributions, and cluttered backgrounds in UAV imagery. Current algorithms often depend on hand-crafted components like anchor boxes, which demand fine-tuning and exhibit limited generalization, and Non-Maximum Suppression (NMS), which is threshold-sensitive and prone to misclassifying dense objects. These generic architectures thus struggle to adapt to aerial imaging characteristics, resulting in performance limitations. Moreover, emerging end-to-end frameworks have yet to effectively mitigate these aerial-specific challenges.To address these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time Detection Transformer framework tailored for UAVs. First, we introduce the High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone. HFESNet preserves critical high-frequency spatial details to extract robust semantic features, thereby improving discriminative capability for small and occluded targets in complex backgrounds. Second, our Efficient Small Object Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with minimal computational overhead, significantly boosting small object detection. Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware Positional Encoding (GAPE) modules enhance the detector's decoder stability and localization accuracy, effectively optimizing bounding boxes and providing explicit spatial priors for dense scenes. Experiments on the VisDrone dataset demonstrate that HEGS-DETR achieves a 5.1\\% AP$_{50}$ and 3.8\\% AP increase over the baseline, while maintaining real-time speed and reducing parameter count by 4M.</article>","contentLength":1804,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PANDAS: Peer-to-peer, Adaptive Networking for Data Availability Sampling within Ethereum Consensus Timebounds","url":"https://arxiv.org/abs/2507.00824","date":1751428800,"author":"","guid":179988,"unread":true,"content":"<article>arXiv:2507.00824v1 Announce Type: new \nAbstract: Layer-2 protocols can assist Ethereum's limited throughput, but globally broadcasting layer-2 data limits their scalability. The Danksharding evolution of Ethereum aims to support the selective distribution of layer-2 data, whose availability in the network is verified using randomized data availability sampling (DAS). Integrating DAS into Ethereum's consensus process is challenging, as pieces of layer-2 data must be disseminated and sampled within four seconds of the beginning of each consensus slot. No existing solution can support dissemination and sampling under such strict time bounds.\n  We propose PANDAS, a practical approach to integrate DAS with Ethereum under Danksharding's requirements without modifying its protocols for consensus and node discovery. PANDAS disseminates layer-2 data and samples its availability using lightweight, direct exchanges. Its design accounts for message loss, node failures, and unresponsive participants while anticipating the need to scale out the Ethereum network. Our evaluation of PANDAS's prototype in a 1,000-node cluster and simulations for up to 20,000 peers shows that it allows layer-2 data dissemination and sampling under planetary-scale latencies within the 4-second deadline.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data","url":"https://arxiv.org/abs/2507.00822","date":1751428800,"author":"","guid":179989,"unread":true,"content":"<article>arXiv:2507.00822v1 Announce Type: new \nAbstract: Accurate particle size distribution (PSD) measurement is important in industries such as mining, pharmaceuticals, and fertilizer manufacturing, significantly influencing product quality and operational efficiency. Traditional PSD methods like sieve analysis and laser diffraction are manual, time-consuming, and limited by particle overlap. Recent developments in convolutional neural networks (CNNs) enable automated, real-time PSD estimation directly from particle images. In this work, we present a CNN-based methodology trained on realistic synthetic particle imagery generated using Blender's advanced rendering capabilities. Synthetic data sets using this method can replicate various industrial scenarios by systematically varying particle shapes, textures, lighting, and spatial arrangements that closely resemble the actual configurations. We evaluated three CNN-based architectures, ResNet-50, InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10, d50, d90). Results demonstrated comparable accuracy across models, with EfficientNet-B0 achieving the best computational efficiency suitable for real-time industrial deployment. This approach shows the effectiveness of realistic synthetic data for robust CNN training, which offers significant potential for automated industrial PSD monitoring. The code is released at : https://github.com/YasserElj/Synthetic-Granular-Gen</article>","contentLength":1454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures","url":"https://arxiv.org/abs/2507.00821","date":1751428800,"author":"","guid":179990,"unread":true,"content":"<article>arXiv:2507.00821v1 Announce Type: new \nAbstract: Designers have ample opportunities to impact the healthcare domain. However, hospitals are often closed ecosystems that pose challenges in engaging clinical stakeholders, developing domain knowledge, and accessing relevant systems and data. In this paper, we introduce a making-oriented approach to help designers understand the intricacies of their target healthcare context. Using Remote Patient Monitoring (RPM) as a case study, we explore how manually crafting synthetic datasets based on real-world observations enables designers to learn about complex data-driven healthcare systems. Our process involves observing and modeling the real-world RPM context, crafting synthetic datasets, and iteratively prototyping a simplified RPM system that balances contextual richness and intentional abstraction. Through this iterative process of sensemaking through making, designers can still develop context familiarity when direct access to the actual healthcare system is limited. Our approach emphasizes the value of hands-on interaction with data structures to support designers in understanding opaque healthcare systems.</article>","contentLength":1171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs","url":"https://arxiv.org/abs/2507.00817","date":1751428800,"author":"","guid":179991,"unread":true,"content":"<article>arXiv:2507.00817v1 Announce Type: new \nAbstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive capabilities in temporal reasoning and cross-modal understanding, yet their vulnerability to adversarial attacks remains underexplored due to unique challenges: complex cross-modal reasoning mechanisms, temporal dependencies, and computational constraints. We present CAVALRY-V (Cross-modal Language-Vision Adversarial Yielding for Videos), a novel framework that directly targets the critical interface between visual perception and language generation in V-MLLMs. Our approach introduces two key innovations: (1) a dual-objective semantic-visual loss function that simultaneously disrupts the model's text generation logits and visual representations to undermine cross-modal integration, and (2) a computationally efficient two-stage generator framework that combines large-scale pre-training for cross-model transferability with specialized fine-tuning for spatiotemporal coherence. Empirical evaluation on comprehensive video understanding benchmarks demonstrates that CAVALRY-V significantly outperforms existing attack methods, achieving 22.8% average improvement over the best baseline attacks on both commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5, InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves flexibility through implicit temporal coherence modeling rather than explicit regularization, enabling significant performance improvements even on image understanding (34.4% average gain). This capability demonstrates CAVALRY-V's potential as a foundational approach for adversarial research across multimodal systems.</article>","contentLength":1698,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments","url":"https://arxiv.org/abs/2507.00816","date":1751428800,"author":"","guid":179992,"unread":true,"content":"<article>arXiv:2507.00816v1 Announce Type: new \nAbstract: Accurate dynamics modeling is essential for quadrotors to achieve precise trajectory tracking in various applications. Traditional physical knowledge-driven modeling methods face substantial limitations in unknown environments characterized by variable payloads, wind disturbances, and external perturbations. On the other hand, data-driven modeling methods suffer from poor generalization when handling out-of-distribution (OoD) data, restricting their effectiveness in unknown scenarios. To address these challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN), which combines knowledge-driven and data-driven modeling methods by embedding physical constraints directly into the training process for robust quadrotor dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional Network (TCN) architecture that efficiently captures temporal dependencies from historical flight data, while a physics-informed loss function applies physical principles to improve model generalization and robustness across previously unseen conditions. By incorporating real-time prediction results into a model predictive control (MPC) framework, we achieve improvements in closed-loop tracking performance. Comprehensive simulations and real-world flight experiments demonstrate that our approach outperforms baseline methods in terms of prediction accuracy, tracking precision, and robustness to unknown environments.</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Many LLMs Are More Utilitarian Than One","url":"https://arxiv.org/abs/2507.00814","date":1751428800,"author":"","guid":179993,"unread":true,"content":"<article>arXiv:2507.00814v1 Announce Type: new \nAbstract: Moral judgment is integral to large language model (LLM) alignment and social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function collectively during collaboration, compared to individual agents. In human moral judgment, group deliberation leads to a utilitarian boost: a tendency to endorse norm violations that maximize benefits for the greatest number of people despite harms. We study whether a similar dynamic emerges in multi-agent LLM systems. We tested six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reasoned independently, and (2) Group, where they engaged in multi-turn discussions in pairs or triads. In personal moral dilemmas, where agents must decide to directly harm one individual to maximize the utility for others, all models found moral violations to be more acceptable when part of a group than individually, similar to human experiments. Some models endorsed actions that maximized overall well-being, even if they benefited strangers over familiar individuals. Others became more willing to violate moral norms in groups. However, while human groups show a similar action bias, the mechanism for their utilitarian boost differs from LLMs. Whereas the human shift comes from heightened sensitivity to decision outcomes, LLM groups show either reduced norm sensitivity or enhanced impartiality. This suggests that while the surface behavior of LLM collectives mimics human group reasoning, the underlying drivers differ. We discuss the implications for AI alignment, multi-agent design, and artificial moral reasoning.</article>","contentLength":1690,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis","url":"https://arxiv.org/abs/2507.00810","date":1751428800,"author":"","guid":179994,"unread":true,"content":"<article>arXiv:2507.00810v1 Announce Type: new \nAbstract: In this paper, we propose an improved numerical algorithm for solving minimax problems based on nonsmooth optimization, quadratic programming and iterative process. We also provide a rigorous proof of convergence for our algorithm under some mild assumptions, such as gradient continuity and boundedness. Such an algorithm can be widely applied in various fields such as robust optimization, imbalanced learning, etc.</article>","contentLength":466,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-interaction TTS toward professional recording reproduction","url":"https://arxiv.org/abs/2507.00808","date":1751428800,"author":"","guid":179995,"unread":true,"content":"<article>arXiv:2507.00808v1 Announce Type: new \nAbstract: Voice directors often iteratively refine voice actors' performances by providing feedback to achieve the desired outcome. While this iterative feedback-based refinement process is important in actual recordings, it has been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained style refinement after the initial synthesis is not possible, even though the synthesized speech often deviates from the user's intended style. To address this issue, we propose a TTS method with multi-step interaction that allows users to intuitively and rapidly refine synthetized speech. Our approach models the interaction between the TTS model and its user to emulate the relationship between voice actors and voice directors. Experiments show that the proposed model with its corresponding dataset enable iterative style refinements in accordance with users' directions, thus demonstrating its multi-interaction capability. Sample audios are available: https://ntt-hilab-gensp. github.io/ssw13multiinteraction_tts/</article>","contentLength":1065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A posteriori and a priori error estimates for linearized thin sheet folding","url":"https://arxiv.org/abs/2507.00807","date":1751428800,"author":"","guid":179996,"unread":true,"content":"<article>arXiv:2507.00807v1 Announce Type: new \nAbstract: We describe a posteriori error analysis for a discontinuous Galerkin method for a fourth order elliptic interface problem that arises from a linearized model of thin sheet folding. The primary contribution is a local efficiency bound for an estimator that measures the extent to which the interface conditions along the fold are satisfied, which is accomplished by constructing a novel edge bubble function. We subsequently conduct a medius analysis to obtain improved a priori error estimates under the minimal regularity assumption on the exact solution. The performance of the method is illustrated by numerical experiments.</article>","contentLength":676,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Out of the Day Job: Perspectives of Industry Practitioners in Co-Design and Delivery of Software Engineering Courses","url":"https://arxiv.org/abs/2507.00803","date":1751428800,"author":"","guid":179997,"unread":true,"content":"<article>arXiv:2507.00803v1 Announce Type: new \nAbstract: Over more than two decades, The University of Glasgow has co-designed and delivered numerous software engineering focused courses with industry partners, covering both technical and discipline specific professional skills. Such collaborations are not unique and many of the benefits are well recognised in the literature. These include enhancing the real-world relevance of curricula, developing student professional networks ahead of graduation and easing recruitment opportunities for employers.\n  However, there is relatively little scholarship on the perspectives of industry practitioners who participate in course design and delivery. This gap is significant, since the effort invested by practitioners is often substantial and may require ongoing support from both the industry partner and academic institution. Understanding the motivations, expectations and experiences of practitioners who engage in course delivery can guide the formation of future partnerships and ensure their long-term sustainability.\n  We begin to address this gap by reporting on the outcomes of a retrospective conducted amongst the practitioner coauthors of this paper, with the academic coauthors acting as facilitators. All coauthors have participated in the recent co-design and delivery of software engineering courses, but we choose to focus explicitly on the perspectives of the practitioners. We report on the themes that emerged from the discussions and our resulting recommendations for future collaborations.</article>","contentLength":1552,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency","url":"https://arxiv.org/abs/2507.00802","date":1751428800,"author":"","guid":179998,"unread":true,"content":"<article>arXiv:2507.00802v1 Announce Type: new \nAbstract: 3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code is available at: https://github.com/VinyehShaw/TRACE.</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction and Dataflow-flexible Accelerator","url":"https://arxiv.org/abs/2507.00797","date":1751428800,"author":"","guid":179999,"unread":true,"content":"<article>arXiv:2507.00797v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel in natural language processing tasks but pose significant computational and memory challenges for edge deployment due to their intensive resource demands. This work addresses the efficiency of LLM inference by algorithm-hardware-dataflow tri-optimizations. We propose a novel voting-based KV cache eviction algorithm, balancing hardware efficiency and algorithm accuracy by adaptively identifying unimportant kv vectors. From a dataflow perspective, we introduce a flexible-product dataflow and a runtime reconfigurable PE array for matrix-vector multiplication. The proposed approach effectively handles the diverse dimensional requirements and solves the challenges of incrementally varying sequence lengths. Additionally, an element-serial scheduling scheme is proposed for nonlinear operations, such as softmax and layer normalization (layernorm). Results demonstrate a substantial reduction in latency, accompanied by a significant decrease in hardware complexity, from O(N) to O(1). The proposed solution is realized in a custom-designed accelerator, VEDA, which outperforms existing hardware platforms. This research represents a significant advancement in LLM inference on resource-constrained edge devices, facilitating real-time processing, enhancing data privacy, and enabling model customization.</article>","contentLength":1392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters","url":"https://arxiv.org/abs/2507.00792","date":1751428800,"author":"","guid":180000,"unread":true,"content":"<article>arXiv:2507.00792v1 Announce Type: new \nAbstract: Generating accurate and realistic virtual human movements in real-time is of high importance for a variety of applications in computer graphics, interactive virtual environments, robotics, and biomechanics. This paper introduces a novel real-time inverse kinematics (IK) solver specifically designed for realistic human-like movement generation. Leveraging the automatic differentiation and just-in-time compilation of TensorFlow, the proposed solver efficiently handles complex articulated human skeletons with high degrees of freedom. By treating forward and inverse kinematics as differentiable operations, our method effectively addresses common challenges such as error accumulation and complicated joint limits in multi-constrained problems, which are critical for realistic human motion modeling. We demonstrate the solver's effectiveness on the SMPLX human skeleton model, evaluating its performance against widely used iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK, and the nonlinear optimization algorithm IPOPT. Our experiments cover both simple end-effector tasks and sophisticated, multi-constrained problems with realistic joint limits. Results indicate that our IK solver achieves real-time performance, exhibiting rapid convergence, minimal computational overhead per iteration, and improved success rates compared to existing methods. The project code is available at https://github.com/hvoss-techfak/TF-JAX-IK</article>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling","url":"https://arxiv.org/abs/2507.00790","date":1751428800,"author":"","guid":180001,"unread":true,"content":"<article>arXiv:2507.00790v1 Announce Type: new \nAbstract: Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at https://github.com/AMAP-ML/LD-RPS.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection","url":"https://arxiv.org/abs/2507.00789","date":1751428800,"author":"","guid":180002,"unread":true,"content":"<article>arXiv:2507.00789v1 Announce Type: new \nAbstract: Text-to-image diffusion models often struggle to achieve accurate semantic alignment between generated images and text prompts while maintaining efficiency for deployment on resource-constrained hardware. Existing approaches either incur substantial computational overhead through noise optimization or compromise semantic fidelity by aggressively pruning tokens. In this work, we propose OptiPrune, a unified framework that combines distribution-aware initial noise optimization with similarity-based token pruning to address both challenges simultaneously. Specifically, (1) we introduce a distribution-aware noise optimization module guided by attention scores to steer the initial latent noise toward semantically meaningful regions, mitigating issues such as subject neglect and feature entanglement; (2) we design a hardware-efficient token pruning strategy that selects representative base tokens via patch-wise similarity, injects randomness to enhance generalization, and recovers pruned tokens using maximum similarity copying before attention operations. Our method preserves the Gaussian prior during noise optimization and enables efficient inference without sacrificing alignment quality. Experiments on benchmark datasets, including Animal-Animal, demonstrate that OptiPrune achieves state-of-the-art prompt-image consistency with significantly reduced computational cost.</article>","contentLength":1436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability","url":"https://arxiv.org/abs/2507.00788","date":1751428800,"author":"","guid":180003,"unread":true,"content":"<article>arXiv:2507.00788v1 Announce Type: new \nAbstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming software engineering. While several studies highlight productivity improvements, their impact on maintainability requires further investigation. [Objective] This study investigates whether co-development with AI assistants affects software maintainability, specifically how easily other developers can evolve the resulting source code. [Method] We conducted a two-phase controlled experiment involving 151 participants, 95% of whom were professional developers. In Phase 1, participants added a new feature to a Java web application, with or without AI assistance. In Phase 2, a randomized controlled trial, new participants evolved these solutions without AI assistance. [Results] AI-assisted development in Phase 1 led to a modest speedup in subsequent evolution and slightly higher average CodeHealth. Although neither difference was significant overall, the increase in CodeHealth was statistically significant when habitual AI users completed Phase 1. For Phase 1, we also observed a significant effect that corroborates previous productivity findings: using an AI assistant yielded a 30.7% median decrease in task completion time. Moreover, for habitual AI users, the mean speedup was 55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants can effectively accelerate development. Moreover, we did not observe warning signs of degraded code-level maintainability. We recommend that future research focus on risks such as code bloat from excessive code generation and the build-up of cognitive debt as developers invest less mental effort during implementation.</article>","contentLength":1712,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Snaps: Bloated and Outdated?","url":"https://arxiv.org/abs/2507.00786","date":1751428800,"author":"","guid":180004,"unread":true,"content":"<article>arXiv:2507.00786v1 Announce Type: new \nAbstract: Snap is an alternative software packaging system developed by Canonical and provided by default in the Ubuntu Linux distribution. Given the heterogeneity of various Linux distributions and their various releases, Snap allows an interoperable delivery of software directly to users. However, concerns and criticism have also been frequently expressed. Regarding this criticism, the paper shows that currently distributed snap packages are indeed on average bloated in terms of their sizes and outdated in terms updating frequencies. With these empirical observations, this short paper contributes to the research domain of software packaging, software packages, and package managers.</article>","contentLength":731,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative AI and the future of scientometrics: current topics and future questions","url":"https://arxiv.org/abs/2507.00783","date":1751428800,"author":"","guid":180005,"unread":true,"content":"<article>arXiv:2507.00783v1 Announce Type: new \nAbstract: The aim of this paper is to review the use of GenAI in scientometrics, and to begin a debate on the broader implications for the field. First, we provide an introduction on GenAI's generative and probabilistic nature as rooted in distributional linguistics. And we relate this to the debate on the extent to which GenAI might be able to mimic human 'reasoning'. Second, we leverage this distinction for a critical engagement with recent experiments using GenAI in scientometrics, including topic labelling, the analysis of citation contexts, predictive applications, scholars' profiling, and research assessment. GenAI shows promise in tasks where language generation dominates, such as labelling, but faces limitations in tasks that require stable semantics, pragmatic reasoning, or structured domain knowledge. However, these results might become quickly outdated. Our recommendation is, therefore, to always strive to systematically compare the performance of different GenAI models for specific tasks. Third, we inquire whether, by generating large amounts of scientific language, GenAI might have a fundamental impact on our field by affecting textual characteristics used to measure science, such as authors, words, and references. We argue that careful empirical work and theoretical reflection will be essential to remain capable of interpreting the evolving patterns of knowledge production.</article>","contentLength":1449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Diagrammatic Calculus for a Functional Model of Natural Language Semantics","url":"https://arxiv.org/abs/2507.00782","date":1751428800,"author":"","guid":180006,"unread":true,"content":"<article>arXiv:2507.00782v1 Announce Type: new \nAbstract: In this paper, we study a functional programming approach to natural language semantics, allowing us to increase the expressivity of a more traditional denotation style. We will formalize a category based type and effect system, and construct a diagrammatic calculus to model parsing and handling of effects, and use it to efficiently compute the denotations for sentences.</article>","contentLength":422,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review","url":"https://arxiv.org/abs/2507.00775","date":1751428800,"author":"","guid":180007,"unread":true,"content":"<article>arXiv:2507.00775v1 Announce Type: new \nAbstract: We present a systematic review on tasks, interactions, and visualization widgets (refer to tangible entities that are used to accomplish data exploration tasks through specific interactions) in the context of tangible data exploration. Tangible widgets have been shown to reduce cognitive load, enable more natural interactions, and support the completion of complex data exploration tasks. Yet, the field lacks a structured understanding of how task types, interaction methods, and widget designs are coordinated, limiting the ability to identify recurring design patterns and opportunities for innovation. To address this gap, we conduct a systematic review to analyze existing work and characterize the current design of data exploration tasks, interactions, and tangible visualization widgets. We next reflect based on our findings and propose a research agenda to inform the development of a future widget design toolkit for tangible data exploration. Our systematic review and supplemental materials are available at physicalviswidget.github.io and osf.io/vjw5e.</article>","contentLength":1117,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing","url":"https://arxiv.org/abs/2507.00769","date":1751428800,"author":"","guid":180008,"unread":true,"content":"<article>arXiv:2507.00769v1 Announce Type: new \nAbstract: Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.</article>","contentLength":1543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments","url":"https://arxiv.org/abs/2507.00762","date":1751428800,"author":"","guid":180009,"unread":true,"content":"<article>arXiv:2507.00762v1 Announce Type: new \nAbstract: Reinforcement Learning (RL) has demonstrated significant potential in certain real-world industrial applications, yet its broader deployment remains limited by inherent challenges such as sample inefficiency and unstable learning dynamics. This study investigates the utilization of Genetic Algorithms (GAs) as a mechanism for improving RL performance in an industrially inspired sorting environment. We propose a novel approach in which GA-generated expert demonstrations are used to enhance policy learning. These demonstrations are incorporated into a Deep Q-Network (DQN) replay buffer for experience-based learning and utilized as warm-start trajectories for Proximal Policy Optimization (PPO) agents to accelerate training convergence. Our experiments compare standard RL training with rule-based heuristics, brute-force optimization, and demonstration data, revealing that GA-derived demonstrations significantly improve RL performance. Notably, PPO agents initialized with GA-generated data achieved superior cumulative rewards, highlighting the potential of hybrid learning paradigms, where heuristic search methods complement data-driven RL. The utilized framework is publicly available and enables further research into adaptive RL strategies for real-world applications.</article>","contentLength":1331,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model","url":"https://arxiv.org/abs/2507.00761","date":1751428800,"author":"","guid":180010,"unread":true,"content":"<article>arXiv:2507.00761v1 Announce Type: new \nAbstract: Thanks to recent advances in generative AI, computers can now simulate realistic and complex natural processes. We apply this capability to predict how wildfires spread, a task made difficult by the unpredictable nature of fire and the variety of environmental conditions it depends on. In this study, We present the first denoising diffusion model for predicting wildfire spread, a new kind of AI framework that learns to simulate fires not just as one fixed outcome, but as a range of possible scenarios. By doing so, it accounts for the inherent uncertainty of wildfire dynamics, a feature that traditional models typically fail to represent. Unlike deterministic approaches that generate a single prediction, our model produces ensembles of forecasts that reflect physically meaningful distributions of where fire might go next. This technology could help us develop smarter, faster, and more reliable tools for anticipating wildfire behavior, aiding decision-makers in fire risk assessment and response planning.</article>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Open-World Human Action Segmentation Using Graph Convolutional Networks","url":"https://arxiv.org/abs/2507.00756","date":1751428800,"author":"","guid":180011,"unread":true,"content":"<article>arXiv:2507.00756v1 Announce Type: new \nAbstract: Human-object interaction segmentation is a fundamental task of daily activity understanding, which plays a crucial role in applications such as assistive robotics, healthcare, and autonomous systems. Most existing learning-based methods excel in closed-world action segmentation, they struggle to generalize to open-world scenarios where novel actions emerge. Collecting exhaustive action categories for training is impractical due to the dynamic diversity of human activities, necessitating models that detect and segment out-of-distribution actions without manual annotation. To address this issue, we formally define the open-world action segmentation problem and propose a structured framework for detecting and segmenting unseen actions. Our framework introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional Network (EPGCN) with a novel decoder module for robust spatiotemporal feature upsampling. 2) Mixup-based training to synthesize out-of-distribution data, eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss that groups in-distribution actions while distancing out-of-distribution samples.\n  We evaluate our framework on two challenging human-object interaction recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets. Experimental results demonstrate significant improvements over state-of-the-art action segmentation models across multiple open-set evaluation metrics, achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and out-of-distribution detection performances (AUROC), respectively. Additionally, we conduct an in-depth ablation study to assess the impact of each proposed component, identifying the optimal framework configuration for open-world action segmentation.</article>","contentLength":1829,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs","url":"https://arxiv.org/abs/2507.00754","date":1751428800,"author":"","guid":180012,"unread":true,"content":"<article>arXiv:2507.00754v1 Announce Type: new \nAbstract: The integration of Large Language Model (LLMs) blocks with Vision Transformers (ViTs) holds immense promise for vision-only tasks by leveraging the rich semantic knowledge and reasoning capabilities of LLMs. However, a fundamental challenge lies in the inherent modality mismatch between text-centric pretraining of LLMs and vision-centric training of ViTs. Direct fusion often fails to fully exploit the LLM's potential and suffers from unstable finetuning. As a result, LLM blocks are kept frozen while only the vision components are learned. As a remedy to these challenges, we introduce Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges this modality mismatch through a synergistic pre-training strategy. LUViT co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and (2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM block using the MAE objective. This joint optimization guides the ViT to produce LLM-aligned features and the LLM to effectively interpret visual information. We demonstrate through extensive experiments that LUViT significantly improves performance on various downstream vision tasks, showcasing a more effective and efficient pathway to harness LLM knowledge for visual understanding.</article>","contentLength":1405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation","url":"https://arxiv.org/abs/2507.00752","date":1751428800,"author":"","guid":180013,"unread":true,"content":"<article>arXiv:2507.00752v1 Announce Type: new \nAbstract: Accurate temporal segmentation of human actions is critical for intelligent robots in collaborative settings, where a precise understanding of sub-activity labels and their temporal structure is essential. However, the inherent noise in both human pose estimation and object detection often leads to over-segmentation errors, disrupting the coherence of action sequences. To address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g., 30 fps) motion data (skeleton and object detections) to mitigate fragmentation. Our framework introduces three key contributions. First, a sinusoidal encoding strategy that maps 3D skeleton coordinates into a continuous sin-cos space to enhance spatial representation robustness. Second, a temporal graph fusion module that aligns multi-modal inputs with differing resolutions via hierarchical feature aggregation, Third, inspired by the smooth transitions inherent to human actions, we design SmoothLabelMix, a data augmentation technique that mixes input sequences and labels to generate synthetic training examples with gradual action transitions, enhancing temporal consistency in predictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for human-object interaction understanding, demonstrate that our approach outperforms state-of-the-art methods, especially in action segmentation accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.</article>","contentLength":1595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning","url":"https://arxiv.org/abs/2507.00748","date":1751428800,"author":"","guid":180014,"unread":true,"content":"<article>arXiv:2507.00748v1 Announce Type: new \nAbstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding in single-image scenarios with textual references. However, their performance degrades when handling real-world applications involving complex multi-image compositions and multimodal instructions, which reveals limitations in cross-image reasoning and generalization. To address these challenges, we adopt a Reinforcement Learning (RL) based post-training strategy to improve the reasoning performance of MLLMs in multi-image grounding tasks. Our approach begins with synthesizing high-quality chain-of-thought (CoT) data for cold-start initialization, followed by supervised fine-tuning (SFT) using low-rank adaptation (LoRA). The cold-start training stage enables the model to identify correct solutions. Subsequently, we perform rejection sampling using the merged SFT model to curate high-quality RL data and leverage rule-based RL to guide the model toward optimal reasoning paths. Extensive experimental results demonstrate the effectiveness of our approach, achieving +9.04\\% improvements on MIG-Bench and +4.98\\% improvements on several out-of-domain reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach exhibits strong generalization in multi-image perception, with gains of +3.1\\% and +2.4\\% over the base model on subsets of the BLINK and MMIU benchmarks, respectively.</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports","url":"https://arxiv.org/abs/2507.00742","date":1751428800,"author":"","guid":180015,"unread":true,"content":"<article>arXiv:2507.00742v1 Announce Type: new \nAbstract: Computer manufacturers offer platforms for users to describe device faults using textual reports such as \"My screen is flickering\". Identifying the faulty component from the report is essential for automating tests and improving user experience. However, such reports are often ambiguous and lack detail, making this task challenging. Large Language Models (LLMs) have shown promise in addressing such issues. This study evaluates 27 open-source models (1B-72B parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot, Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted 98,948 inferences, processing over 51 million input tokens and generating 13 million output tokens. We achieve f1-score up to 0.76. Results show that three models offer the best balance between size and performance: mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and gemma-2-2b-it, that offer competitive performance with lower VRAM usage, enabling efficient inference on end-user devices as modern laptops or smartphones with NPUs.</article>","contentLength":1121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds","url":"https://arxiv.org/abs/2507.00740","date":1751428800,"author":"","guid":180016,"unread":true,"content":"<article>arXiv:2507.00740v1 Announce Type: new \nAbstract: This paper presents a complete formal specification, protocol description, and mathematical proof structure for Simplified Payment Verification (SPV) as originally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark contrast to the misrepresentations proliferated by popular implementations, we show that SPV is not only secure under bounded adversarial assumptions but strictly optimal for digital cash systems requiring scalable and verifiable transaction inclusion. We reconstruct the SPV protocol from first principles, grounding its verification model in symbolic automata, Merkle membership relations, and chain-of-proof dominance predicates. Through rigorous probabilistic and game-theoretic analysis, we derive the economic bounds within which the protocol operates securely and verify its liveness and safety properties under partial connectivity, hostile relay networks, and adversarial propagation delay. Our specification further introduces low-bandwidth optimisations such as adaptive polling and compressed header synchronisation while preserving correctness. This document serves both as a blueprint for secure SPV implementation and a rebuttal of common misconceptions surrounding non-validating clients.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network","url":"https://arxiv.org/abs/2507.00739","date":1751428800,"author":"","guid":180017,"unread":true,"content":"<article>arXiv:2507.00739v1 Announce Type: new \nAbstract: This work introduces a novel biorthogonal tunable wavelet unit constructed using a lifting scheme that relaxes both the orthogonality and equal filter length constraints, providing greater flexibility in filter design. The proposed unit enhances convolution, pooling, and downsampling operations, leading to improved image classification and anomaly detection in convolutional neural networks (CNN). When integrated into an 18-layer residual neural network (ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12% and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its effectiveness in capturing fine-grained details. Similar improvements were observed in ResNet-34. For anomaly detection in the hazelnut category of the MVTec Anomaly Detection dataset, the proposed method achieved competitive and wellbalanced performance in both segmentation and detection tasks, outperforming existing approaches in terms of accuracy and robustness.</article>","contentLength":1027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN","url":"https://arxiv.org/abs/2507.00736","date":1751428800,"author":"","guid":180018,"unread":true,"content":"<article>arXiv:2507.00736v1 Announce Type: new \nAbstract: Recent years have seen growing interest in Question Difficulty Estimation (QDE) using natural language processing techniques. Question difficulty is often represented using discrete levels, framing the task as ordinal regression due to the inherent ordering from easiest to hardest. However, the literature has neglected the ordinal nature of the task, relying on classification or discretized regression models, with specialized ordinal regression methods remaining unexplored. Furthermore, evaluation metrics are tightly coupled to the modeling paradigm, hindering cross-study comparability. While some metrics fail to account for the ordinal structure of difficulty levels, none adequately address class imbalance, resulting in biased performance assessments. This study addresses these limitations by benchmarking three types of model outputs -- discretized regression, classification, and ordinal regression -- using the balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly captures ordinality and class imbalance. In addition to using popular ordinal regression methods, we propose OrderedLogitNN, extending the ordered logit model from econometrics to neural networks. We fine-tune BERT on the RACE++ and ARC datasets and find that OrderedLogitNN performs considerably better on complex tasks. The balanced DRPS offers a robust and fair evaluation metric for discrete-level QDE, providing a principled foundation for future research.</article>","contentLength":1515,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction","url":"https://arxiv.org/abs/2507.00733","date":1751428800,"author":"","guid":180019,"unread":true,"content":"<article>arXiv:2507.00733v1 Announce Type: new \nAbstract: Ordinal classification problems, where labels exhibit a natural order, are prevalent in high-stakes fields such as medicine and finance. Accurate uncertainty quantification, including the decomposition into aleatoric (inherent variability) and epistemic (lack of knowledge) components, is crucial for reliable decision-making. However, existing research has primarily focused on nominal classification and regression. In this paper, we introduce a novel class of measures of aleatoric and epistemic uncertainty in ordinal classification, which is based on a suitable reduction to (entropy- and variance-based) measures for the binary case. These measures effectively capture the trade-off in ordinal classification between exact hit-rate and minimial error distances. We demonstrate the effectiveness of our approach on various tabular ordinal benchmark datasets using ensembles of gradient-boosted trees and multi-layer perceptrons for approximate Bayesian inference. Our method significantly outperforms standard and label-wise entropy and variance-based measures in error detection, as indicated by misclassification rates and mean absolute error. Additionally, the ordinal measures show competitive performance in out-of-distribution (OOD) detection. Our findings highlight the importance of considering the ordinal nature of classification problems when assessing uncertainty.</article>","contentLength":1430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Temporal Orienteering with Changing Fuel Costs","url":"https://arxiv.org/abs/2507.00728","date":1751428800,"author":"","guid":180020,"unread":true,"content":"<article>arXiv:2507.00728v1 Announce Type: new \nAbstract: The problem Orienteering asks whether there exists a walk which visits a number of sites without exceeding some fuel budget. In the variant of the problem we consider, the cost of each edge in the walk is dependent on the time we depart one endpoint and the time we arrive at the other endpoint. This mirrors applications such as travel between orbiting objects where fuel costs are dependent on both the departure time and the length of time spent travelling. In defining this problem, we introduce a natural generalisation of the standard notion of temporal graphs: the pair consisting of the graph of the sites and a cost function, in which costs as well as shortest travel times between pairs of objects change over time. We believe this model is likely to be of independent interest. The problem of deciding whether a stated goal is feasible is easily seen to be NP-complete; we investigate three different ways to restrict the input which lead to efficient algorithms. These include the number of times an edge can be used, an analogue of vertex-interval-membership width, and the number of sites to be visited.</article>","contentLength":1166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Hierarchical Coded Caching with Offline Users","url":"https://arxiv.org/abs/2507.00727","date":1751428800,"author":"","guid":180021,"unread":true,"content":"<article>arXiv:2507.00727v1 Announce Type: new \nAbstract: This paper studies a two-layer hierarchical network in which some users are offline during the content delivery phase. A two-layer hierarchical network consists of a single server connected to multiple cache-aided mirror sites, and each mirror site is connected to a distinct set of cache-aided users. A scheme for such a hierarchical system with offline users has been proposed recently but considered a special case where all mirror caches have zero memory, which is a significant limitation. We propose an array known as a hierarchical hotplug placement delivery array (HHPDA), which describes the placement and delivery phases of a coded caching scheme for a general two-layer hierarchical network with offline users. Further, we construct a class of HHPDAs using combinatorial t-designs.</article>","contentLength":841,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess","url":"https://arxiv.org/abs/2507.00726","date":1751428800,"author":"","guid":180022,"unread":true,"content":"<article>arXiv:2507.00726v1 Announce Type: new \nAbstract: While reinforcement learning (RL) for large language models (LLMs) has shown promise in mathematical reasoning, strategic reasoning for LLMs using RL remains largely unexplored. We investigate whether LLMs can develop strategic reasoning capabilities through RL in chess. To this end, we leverage a chess-pretrained action-value network to provide dense reward on the LLM's output move quality, which can be seen as a form of knowledge distillation. Our experiments show that our distillation-based dense rewards often outperform sparse binary rewards. However, surprisingly, all models plateau far below expert levels. We provide SFT and RL ablations on chess reasoning training and find evidence that this limitation stems from a deficit in the pretrained models' internal understanding of chess--a deficit which RL alone may not be able to fully overcome.</article>","contentLength":907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory","url":"https://arxiv.org/abs/2507.00725","date":1751428800,"author":"","guid":180023,"unread":true,"content":"<article>arXiv:2507.00725v1 Announce Type: new \nAbstract: Morse-Cerf theory considers a one-parameter family of smooth functions defined on a manifold and studies the evolution of their critical points with the parameter. This paper presents an adaptation of Morse-Cerf theory to a family of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram are introduced as representations of the evolution of critical points of the PL function. The characterization of a crossing in the vertex diagram based on the homology of the lower links of vertices leads to the definition of a topological descriptor for time-varying scalar fields. An algorithm for computing the Cerf diagram and a measure for comparing two Cerf diagrams are also described together with experimental results on time-varying scalar fields.</article>","contentLength":810,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features","url":"https://arxiv.org/abs/2507.00724","date":1751428800,"author":"","guid":180024,"unread":true,"content":"<article>arXiv:2507.00724v1 Announce Type: new \nAbstract: Large vision models achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property for its owner. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to these personalized models. However, in this paper, we reveal that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized models by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by the output differences between the shadow and victim models. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously.</article>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-goal-oriented anisotropic error control and mesh adaptivity for time-dependent convection-dominated problems","url":"https://arxiv.org/abs/2507.00723","date":1751428800,"author":"","guid":180025,"unread":true,"content":"<article>arXiv:2507.00723v1 Announce Type: new \nAbstract: In this work, we present an anisotropic multi-goal error control based on the Dual Weighted Residual (DWR) method for time-dependent convection-diffusion-reaction (CDR) equations. This multi-goal oriented approach allows for an accurate and efficient error control with regard to several quantities of interest simultaneously. Using anisotropic interpolation and restriction operators, we obtain elementwise error indicators in space and time, where the spatial indicators are additionally separated with respect to the single directions. The directional error indicators quantify anisotropy of the solution with respect to the goals, and produce adaptive, anisotropic meshes that efficiently capture layers. To prevent spurious oscillations the streamline upwind Petrov-Galerkin (SUPG) method is applied to stabilize the underlying system in the case of high P\\'{e}clet numbers. Numerical examples show efficiency and robustness of the proposed approach for several goal quantities using established benchmarks for convection-dominated transport.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement","url":"https://arxiv.org/abs/2507.00721","date":1751428800,"author":"","guid":180026,"unread":true,"content":"<article>arXiv:2507.00721v1 Announce Type: new \nAbstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios. Code is available at https://github.com/AMAP-ML/UPRE.</article>","contentLength":1347,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation","url":"https://arxiv.org/abs/2507.00718","date":1751428800,"author":"","guid":180027,"unread":true,"content":"<article>arXiv:2507.00718v1 Announce Type: new \nAbstract: This paper explores the potential of large language models (LLMs) to generate financial reports from time series data. We propose a framework encompassing prompt engineering, model selection, and evaluation. We introduce an automated highlighting system to categorize information within the generated reports, differentiating between insights derived directly from time series data, stemming from financial reasoning, and those reliant on external knowledge. This approach aids in evaluating the factual grounding and reasoning capabilities of the models. Our experiments, utilizing both data from the real stock market indices and synthetic time series, demonstrate the capability of LLMs to produce coherent and informative financial reports.</article>","contentLength":793,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerating Loading WebGraphs in ParaGrapher","url":"https://arxiv.org/abs/2507.00716","date":1751428800,"author":"","guid":180028,"unread":true,"content":"<article>arXiv:2507.00716v1 Announce Type: new \nAbstract: ParaGrapher is a graph loading API and library that enables graph processing frameworks to load large-scale compressed graphs with minimal overhead. This capability accelerates the design and implementation of new high-performance graph algorithms and their evaluation on a wide range of graphs and across different frameworks. However, our previous study identified two major limitations in ParaGrapher: inefficient utilization of high-bandwidth storage and reduced decompression bandwidth due to increased compression ratios. To address these limitations, we present two optimizations for ParaGrapher in this paper. To improve storage utilization, particularly for high-bandwidth storage, we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE (Filesystem in User Space). PG-Fuse optimizes storage access by increasing the size of requested blocks, reducing the number of calls to the underlying filesystem, and caching the received blocks in memory for future calls. To improve the decompression bandwidth, we introduce CompBin, a compact binary representation of the CSR format. CompBin facilitates direct accesses to neighbors while preventing storage usage for unused bytes. Our evaluation on 12 real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.</article>","contentLength":1414,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EARN: Efficient Inference Acceleration for LLM-based Generative Recommendation by Register Tokens","url":"https://arxiv.org/abs/2507.00715","date":1751428800,"author":"","guid":180029,"unread":true,"content":"<article>arXiv:2507.00715v1 Announce Type: new \nAbstract: Large Language Model-based generative recommendation (LLMRec) has achieved notable success, but it suffers from high inference latency due to massive computational overhead and memory pressure of KV Cache. Existing KV Cache reduction methods face critical limitations: cache compression offers marginal acceleration given recommendation tasks' short decoding steps, while prompt compression risks discarding vital interaction history. Through systematic analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1) layer-wise attention sparsity inversion where early layers retain dense informative patterns while later layers exhibit high redundancy, and 2) dual attention sinks phenomenon where attention scores concentrate on both head and tail tokens of input sequences. Motivated by these insights, we propose EARN, an efficient inference framework that leverages the early layers to compress information into register tokens placed at the input sequence boundaries, then focuses solely on these tokens in the subsequent layers. Extensive experiments on three datasets, two LLMRec methods and two LLM architectures demonstrate EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction with better accuracy than the general finetuning approach. Our work bridges the efficiency-effectiveness gap in LLMRec, offering practical deployment advantages for industrial scenarios.</article>","contentLength":1463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories","url":"https://arxiv.org/abs/2507.00711","date":1751428800,"author":"","guid":180030,"unread":true,"content":"<article>arXiv:2507.00711v1 Announce Type: new \nAbstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have recently achieved impressive results on reasoning benchmarks. Yet, growing evidence shows that these models often generate longer but ineffective chains of thought (CoTs), calling into question whether benchmark gains reflect real reasoning improvements. We present new evidence of overthinking, where models disregard correct solutions even when explicitly provided, instead continuing to generate unnecessary reasoning steps that often lead to incorrect conclusions. Experiments on three state-of-the-art models using the AIME2024 math benchmark reveal critical limitations in these models ability to integrate corrective information, posing new challenges for achieving robust and interpretable reasoning.</article>","contentLength":831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Task Offloading for UAV-enabled Secure MEC Against Aerial Eavesdropper","url":"https://arxiv.org/abs/2507.00710","date":1751428800,"author":"","guid":180031,"unread":true,"content":"<article>arXiv:2507.00710v1 Announce Type: new \nAbstract: Unmanned aerial vehicles (UAVs) are recognized as a promising candidate for the multi-access edge computing (MEC) in the future sixth generation communication networks. However, the aerial eavesdropping UAVs (EUAVs) pose a significant security threat to the data offloading. In this paper, we investigate a robust MEC scenario with multiple service UAVs (SUAVs) towards the potential eavesdropping from the EUAV, in which the random parameters such as task complexities are considered in the practical applications. In detail, the problem is formulated to optimize the deployment positions of SUAVs, the connection relationships between GUs and SUAVs, and the offloading ratios. With the uncertain task complexities, the corresponding chance constraints are constructed under the uncertainty set, which is tricky to deal with. Therefore, we first optimize the pre-deployment of SUAVs by the K-means algorithm. Then, the distributionally robust optimization method is employed, and the conditional value at risk is utilized to transform the chance constraints into convex forms, which can be solved via convex toolkits. Finally, the simulation results show that with the consideration of uncertainties, just 5% more energy is consumed compared with the ideal circumstance, which verifies the robustness of the proposed algorithms.</article>","contentLength":1378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving","url":"https://arxiv.org/abs/2507.00709","date":1751428800,"author":"","guid":180032,"unread":true,"content":"<article>arXiv:2507.00709v1 Announce Type: new \nAbstract: Lane segment topology reasoning constructs a comprehensive road network by capturing the topological relationships between lane segments and their semantic types. This enables end-to-end autonomous driving systems to perform road-dependent maneuvers such as turning and lane changing. However, the limitations in consistent positional embedding and temporal multiple attribute learning in existing methods hinder accurate roadnet reconstruction. To address these issues, we propose TopoStreamer, an end-to-end temporal perception model for lane segment topology reasoning. Specifically, TopoStreamer introduces three key improvements: streaming attribute constraints, dynamic lane boundary positional encoding, and lane segment denoising. The streaming attribute constraints enforce temporal consistency in both centerline and boundary coordinates, along with their classifications. Meanwhile, dynamic lane boundary positional encoding enhances the learning of up-to-date positional information within queries, while lane segment denoising helps capture diverse lane segment patterns, ultimately improving model performance. Additionally, we assess the accuracy of existing models using a lane boundary classification metric, which serves as a crucial measure for lane-changing scenarios in autonomous driving. On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements over state-of-the-art methods, achieving substantial performance gains of +3.4% mAP in lane segment perception and +2.1% OLS in centerline perception tasks.</article>","contentLength":1595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the (In)Approximability of the Monitoring Edge Geodetic Set Problem","url":"https://arxiv.org/abs/2507.00708","date":1751428800,"author":"","guid":180033,"unread":true,"content":"<article>arXiv:2507.00708v1 Announce Type: new \nAbstract: We study the minimum \\emph{Monitoring Edge Geodetic Set} (\\megset) problem introduced in [Foucaud et al., CALDAM'23]: given a graph $G$, we say that an edge is monitored by a pair $u,v$ of vertices if \\emph{all} shortest paths between $u$ and $v$ traverse $e$; the goal of the problem consists in finding a subset $M$ of vertices of $G$ such that each edge of $G$ is monitored by at least one pair of vertices in $M$, and $|M|$ is minimized.\n  In this paper, we prove that all polynomial-time approximation algorithms for the minimum \\megset problem must have an approximation ratio of $\\Omega(\\log n)$, unless \\p = \\np. To the best of our knowledge, this is the first non-constant inapproximability result known for this problem. We also strengthen the known \\np-hardness of the problem on $2$-apex graphs by showing that the same result holds for $1$-apex graphs. This leaves open the problem of determining whether the problem remains \\np-hard on planar (i.e., $0$-apex) graphs.\n  On the positive side, we design an algorithm that computes good approximate solutions for hereditary graph classes that admit efficiently computable balanced separators of truly sublinear size. This immediately results in polynomial-time approximation algorithms achieving an approximation ratio of $O(n^{\\frac{1}{4}} \\sqrt{\\log n})$ on planar graphs, graphs with bounded genus, and $k$-apex graphs with $k=O(n^{\\frac{1}{4}})$. On graphs with bounded treewidth, we obtain an approximation ratio of $O(\\log^{3/2} n)$ for any constant $\\varepsilon &gt; 0$. This compares favorably with the best-known approximation algorithm for general graphs, which achieves an approximation ratio of $O(\\sqrt{n \\log n})$ via a simple reduction to the \\textsc{Set Cover} problem.</article>","contentLength":1792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving","url":"https://arxiv.org/abs/2507.00707","date":1751428800,"author":"","guid":180034,"unread":true,"content":"<article>arXiv:2507.00707v1 Announce Type: new \nAbstract: Multi-view image generation in autonomous driving demands consistent 3D scene understanding across camera views. Most existing methods treat this problem as a 2D image set generation task, lacking explicit 3D modeling. However, we argue that a structured representation is crucial for scene generation, especially for autonomous driving applications. This paper proposes BEV-VAE for consistent and controllable view synthesis. BEV-VAE first trains a multi-view image variational autoencoder for a compact and unified BEV latent space and then generates the scene with a latent diffusion transformer. BEV-VAE supports arbitrary view generation given camera configurations, and optionally 3D layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance in both 3D consistent reconstruction and generation. The code is available at: https://github.com/Czm369/bev-vae.</article>","contentLength":931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval","url":"https://arxiv.org/abs/2507.00701","date":1751428800,"author":"","guid":180035,"unread":true,"content":"<article>arXiv:2507.00701v1 Announce Type: new \nAbstract: Recent advancements in spaceborne GNSS missions have produced extensive global datasets, providing a robust basis for deep learning-based significant wave height (SWH) retrieval. While existing deep learning models predominantly utilize CYGNSS data with four-channel information, they often adopt single-channel inputs or simple channel concatenation without leveraging the benefits of cross-channel information interaction during training. To address this limitation, a novel spatial-channel attention-based network, namely SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each channel of the DDMs are modeled as independent attention heads, enabling the fusion of spatial and channel-wise information. For auxiliary parameters, a lightweight attention mechanism is designed to assign weights along the spatial and channel dimensions. The final feature integrates both spatial and channel-level characteristics. Model performance is evaluated using four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC buoy observations. The code is available at https://github.com/Clifx9908/SCAWaveNet.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English","url":"https://arxiv.org/abs/2507.00700","date":1751428800,"author":"","guid":180036,"unread":true,"content":"<article>arXiv:2507.00700v1 Announce Type: new \nAbstract: Cross-cultural research in perception and cognition has shown that individuals from different cultural backgrounds process visual information in distinct ways. East Asians, for example, tend to adopt a holistic perspective, attending to contextual relationships, whereas Westerners often employ an analytical approach, focusing on individual objects and their attributes. In this study, we investigate whether Vision-Language Models (VLMs) trained predominantly on different languages, specifically Japanese and English, exhibit similar culturally grounded attentional patterns. Using comparative analysis of image descriptions, we examine whether these models reflect differences in holistic versus analytic tendencies. Our findings suggest that VLMs not only internalize the structural properties of language but also reproduce cultural behaviors embedded in the training data, indicating that cultural cognition may implicitly shape model outputs.</article>","contentLength":999,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hierarchical and Evolvable Benchmark for Fine-Grained Code Instruction Following with Multi-Turn Feedback","url":"https://arxiv.org/abs/2507.00699","date":1751428800,"author":"","guid":180037,"unread":true,"content":"<article>arXiv:2507.00699v1 Announce Type: new \nAbstract: Large language models (LLMs) have advanced significantly in code generation, yet their ability to follow complex programming instructions with layered and diverse constraints remains underexplored. Existing benchmarks often prioritize functional correctness, overlooking the nuanced requirements found in real-world development. We introduce MultiCodeIF, a comprehensive benchmark designed to evaluate instruction-following in code generation across multiple dimensions: constraint type, hierarchical levels, and iterative refinement. Built upon a structured taxonomy of 9 categories and 27 constraint types, MultiCodeIF enables granular assessment of both functional and non-functional instruction adherence. Using an automated pipeline, ConstraGen, we synthesize and evolve 2,021 code tasks sourced from 14 programming languages, supporting multi-turn evaluation through feedback-driven task variants. Empirical evaluation of six state-of-the-art LLMs uncovers substantial performance disparities. The top-performing model, Claude-3-7-Sonnet, achieves 63.0% average constraint satisfaction, while smaller models like Qwen3-1.7B fall to 44.8%. Models perform well on explicit constraints, but struggle with implicit or abstract constraints. Tasks with multiple hierarchical constraints significantly reduce model success rates, from 54.5% in single-level to just 18.8% in multi-level scenarios. However, structured feedback enables progressive improvement: average constraint satisfaction rises from 63.0% to 83.4% over four iterative refinement rounds. MultiCodeIF provides a scalable, constraint-aware, and feedback-sensitive framework to benchmark LLMs under realistic code generation scenarios, bridging the gap between synthetic evaluations and real-world instruction complexity. The full benchmark dataset, evaluation pipeline, and source code are available at https://github.com/SYSUSELab/MultiCodeIF.</article>","contentLength":1958,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rectifying Magnitude Neglect in Linear Attention","url":"https://arxiv.org/abs/2507.00698","date":1751428800,"author":"","guid":180038,"unread":true,"content":"<article>arXiv:2507.00698v1 Announce Type: new \nAbstract: As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Query's magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at https://github.com/qhfan/MALA</article>","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of A Mixed Finite Element Method for Poisson's Equation with Rough Boundary Data","url":"https://arxiv.org/abs/2507.00697","date":1751428800,"author":"","guid":180039,"unread":true,"content":"<article>arXiv:2507.00697v1 Announce Type: new \nAbstract: This paper is concerned with finite element methods for Poisson's equation with rough boundary data. Conventional methods require that the boundary data $g$ of the problem belongs to $H^{1/2} (\\partial \\Omega)$. However, in many applications one has to consider the case when $g$ is in $L^2(\\partial \\Omega)$ only. To this end, very weak solutions are considered to establish the well-posedness of the problem. Most previously proposed numerical methods use regularizations of the boundary data. The main purpose of this paper is to use the Raviart--Thomas mixed finite element method to solve the Poisson equation with rough boundary data directly. We prove that the solution to the proposed mixed method converges to the very weak solution. In particular, we prove that the convergence rate of the numerical solution is $O(h^{1/2})$ in convex domains and $O(h^{s-1/2})$ in nonconvex domains, where $s &gt; 1/2$ depends on the geometry of the domain. The analysis is based on a regularized approach and a rigorous estimate for the corresponding dual problem. Numerical experiments confirm the theoretically predicted convergence rates for the proposed mixed method for Poisson's equation with rough boundary data.</article>","contentLength":1260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Test-Function Approach to Incremental Stability","url":"https://arxiv.org/abs/2507.00695","date":1751428800,"author":"","guid":180040,"unread":true,"content":"<article>arXiv:2507.00695v1 Announce Type: new \nAbstract: This paper presents a novel framework for analyzing Incremental-Input-to-State Stability ($\\delta$ISS) based on the idea of using rewards as \"test functions.\" Whereas control theory traditionally deals with Lyapunov functions that satisfy a time-decrease condition, reinforcement learning (RL) value functions are constructed by exponentially decaying a Lipschitz reward function that may be non-smooth and unbounded on both sides. Thus, these RL-style value functions cannot be directly understood as Lyapunov certificates. We develop a new equivalence between a variant of incremental input-to-state stability of a closed-loop system under given a policy, and the regularity of RL-style value functions under adversarial selection of a H\\\"older-continuous reward function. This result highlights that the regularity of value functions, and their connection to incremental stability, can be understood in a way that is distinct from the traditional Lyapunov-based approach to certifying stability in control theory.</article>","contentLength":1065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection","url":"https://arxiv.org/abs/2507.00693","date":1751428800,"author":"","guid":180041,"unread":true,"content":"<article>arXiv:2507.00693v1 Announce Type: new \nAbstract: Early identification of suicide risk is crucial for preventing suicidal behaviors. As a result, the identification and study of patterns and markers related to suicide risk have become a key focus of current research. In this paper, we present the results of our work in the 1st SpeechWellness Challenge (SW1), which aims to explore speech as a non-invasive and easily accessible mental health indicator for identifying adolescents at risk of suicide.Our approach leverages large language model (LLM) as the primary tool for feature extraction, alongside conventional acoustic and semantic features. The proposed method achieves an accuracy of 74\\% on the test set, ranking first in the SW1 challenge. These findings demonstrate the potential of LLM-based methods for analyzing speech in the context of suicide risk assessment.</article>","contentLength":876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack","url":"https://arxiv.org/abs/2507.00690","date":1751428800,"author":"","guid":180042,"unread":true,"content":"<article>arXiv:2507.00690v1 Announce Type: new \nAbstract: Adversarial attacks on point clouds often impose strict geometric constraints to preserve plausibility; however, such constraints inherently limit transferability and undefendability. While deformation offers an alternative, existing unstructured approaches may introduce unnatural distortions, making adversarial point clouds conspicuous and undermining their plausibility. In this paper, we propose CageAttack, a cage-based deformation framework that produces natural adversarial point clouds. It first constructs a cage around the target object, providing a structured basis for smooth, natural-looking deformation. Perturbations are then applied to the cage vertices, which seamlessly propagate to the point cloud, ensuring that the resulting deformations remain intrinsic to the object and preserve plausibility. Extensive experiments on seven 3D deep neural network classifiers across three datasets show that CageAttack achieves a superior balance among transferability, undefendability, and plausibility, outperforming state-of-the-art methods. Codes will be made public upon acceptance.</article>","contentLength":1144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion Classifier Guidance for Non-robust Classifiers","url":"https://arxiv.org/abs/2507.00687","date":1751428800,"author":"","guid":180043,"unread":true,"content":"<article>arXiv:2507.00687v1 Announce Type: new \nAbstract: Classifier guidance is intended to steer a diffusion process such that a given classifier reliably recognizes the generated data point as a certain class. However, most classifier guidance approaches are restricted to robust classifiers, which were specifically trained on the noise of the diffusion forward process. We extend classifier guidance to work with general, non-robust, classifiers that were trained without noise. We analyze the sensitivity of both non-robust and robust classifiers to noise of the diffusion process on the standard CelebA data set, the specialized SportBalls data set and the high-dimensional real-world CelebA-HQ data set. Our findings reveal that non-robust classifiers exhibit significant accuracy degradation under noisy conditions, leading to unstable guidance gradients. To mitigate these issues, we propose a method that utilizes one-step denoised image predictions and implements stabilization techniques inspired by stochastic optimization methods, such as exponential moving averages. Experimental results demonstrate that our approach improves the stability of classifier guidance while maintaining sample diversity and visual quality. This work contributes to advancing conditional sampling techniques in generative models, enabling a broader range of classifiers to be used as guidance classifiers.</article>","contentLength":1390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Domain-specific Language and Architecture for Detecting Process Activities from Sensor Streams in IoT","url":"https://arxiv.org/abs/2507.00686","date":1751428800,"author":"","guid":180044,"unread":true,"content":"<article>arXiv:2507.00686v1 Announce Type: new \nAbstract: Modern Internet of Things (IoT) systems are equipped with a plethora of sensors providing real-time data about the current operations of their components, which is crucial for the systems' internal control systems and processes. However, these data are often too fine-grained to derive useful insights into the execution of the larger processes an IoT system might be part of. Process mining has developed advanced approaches for the analysis of business processes that may also be used in the context of IoT. Bringing process mining to IoT requires an event abstraction step to lift the low-level sensor data to the business process level. In this work, we aim to empower domain experts to perform this step using a newly developed domain-specific language (DSL) called Radiant. Radiant supports the specification of patterns within the sensor data that indicate the execution of higher level process activities. These patterns are translated to complex event processing (CEP) applications to be used for detecting activity executions at runtime. We propose a corresponding software architecture for online event abstraction from IoT sensor streams using the CEP applications. We evaluate these applications to monitor activity executions using IoT sensors in smart manufacturing and smart healthcare. The evaluation method and results inform the domain expert about the quality of activity detections and potential for improvement.</article>","contentLength":1482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sectional Kolmogorov N-widths for parameter-dependent function spaces: A general framework with application to parametrized Friedrichs' systems","url":"https://arxiv.org/abs/2507.00678","date":1751428800,"author":"","guid":180045,"unread":true,"content":"<article>arXiv:2507.00678v1 Announce Type: new \nAbstract: We investigate parametrized variational problems where for each parameter the solution may originate from a different parameter-dependent function space. Our main motivation is the theory of Friedrichs' systems, a large abstract class of linear PDE-problems whose solutions are sought in operator- (and thus parameter-)dependent graph spaces. Other applications include function spaces on parametrized domains or discretizations involving data-dependent stabilizers. Concerning the set of all parameter-dependent solutions, we argue that in these cases the interpretation as a \"solution manifold\" widely adopted in the model order reduction community is no longer applicable. Instead, we propose a novel framework based on the theory of fiber bundles and explain how established concepts such as approximability generalize by introducing a Sectional Kolmogorov N-width. Further, we prove exponential approximation rates of this N-width if a norm equivalence criterion is fulfilled. Applying this result to problems with Friedrichs' structure then gives a sufficient criterion that can be easily verified.</article>","contentLength":1153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Steerable Imitation Controllers from Unstructured Animal Motions","url":"https://arxiv.org/abs/2507.00677","date":1751428800,"author":"","guid":180046,"unread":true,"content":"<article>arXiv:2507.00677v1 Announce Type: new \nAbstract: This paper presents a control framework for legged robots that leverages unstructured real-world animal motion data to generate animal-like and user-steerable behaviors. Our framework learns to follow velocity commands while reproducing the diverse gait patterns in the original dataset. To begin with, animal motion data is transformed into a robot-compatible database using constrained inverse kinematics and model predictive control, bridging the morphological and physical gap between the animal and the robot. Subsequently, a variational autoencoder-based motion synthesis module captures the diverse locomotion patterns in the motion database and generates smooth transitions between them in response to velocity commands. The resulting kinematic motions serve as references for a reinforcement learning-based feedback controller deployed on physical robots. We show that this approach enables a quadruped robot to adaptively switch gaits and accurately track user velocity commands while maintaining the stylistic coherence of the motion data. Additionally, we provide component-wise evaluations to analyze the system's behavior in depth and demonstrate the efficacy of our method for more accurate and reliable motion imitation.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation","url":"https://arxiv.org/abs/2507.00676","date":1751428800,"author":"","guid":180047,"unread":true,"content":"<article>arXiv:2507.00676v1 Announce Type: new \nAbstract: Accepted in the ICIP 2025\n  We present a novel transformer-based framework for whole-body grasping that addresses both pose generation and motion infilling, enabling realistic and stable object interactions. Our pipeline comprises three stages: Grasp Pose Generation for full-body grasp generation, Temporal Infilling for smooth motion continuity, and a LiftUp Transformer that refines downsampled joints back to high-resolution markers. To overcome the scarcity of hand-object interaction data, we introduce a data-efficient Generalized Pretraining stage on large, diverse motion datasets, yielding robust spatio-temporal representations transferable to grasping tasks. Experiments on the GRAB dataset show that our method outperforms state-of-the-art baselines in terms of coherence, stability, and visual realism. The modular design also supports easy adaptation to other human-motion applications.</article>","contentLength":950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A hyperboloidal method for numerical simulations of multidimensional nonlinear wave equations","url":"https://arxiv.org/abs/2507.00674","date":1751428800,"author":"","guid":180048,"unread":true,"content":"<article>arXiv:2507.00674v1 Announce Type: new \nAbstract: We consider the scalar wave equation with power nonlinearity in n+1 dimensions. Unlike previous numerical studies, we go beyond the radial case and do not assume any symmetries for n=3, and we only impose an SO(n-1) symmetry in higher dimensions. Our method is based on a hyperboloidal foliation of Minkowski spacetime and conformal compactification. We focus on the late-time power-law decay (tails) of the solutions and compute decay exponents for different spherical harmonic modes, for subcritical, critical and supercritical, focusing and defocusing nonlinear wave equations.</article>","contentLength":629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration","url":"https://arxiv.org/abs/2507.00672","date":1751428800,"author":"","guid":180049,"unread":true,"content":"<article>arXiv:2507.00672v1 Announce Type: new \nAbstract: Edge computing enables real-time data processing closer to its source, thus improving the latency and performance of edge-enabled AI applications. However, traditional AI models often fall short when dealing with complex, dynamic tasks that require advanced reasoning and multimodal data processing. This survey explores the integration of multi-LLMs (Large Language Models) to address this in edge computing, where multiple specialized LLMs collaborate to enhance task performance and adaptability in resource-constrained environments. We review the transition from conventional edge AI models to single LLM deployment and, ultimately, to multi-LLM systems. The survey discusses enabling technologies such as dynamic orchestration, resource scheduling, and cross-domain knowledge transfer that are key for multi-LLM implementation. A central focus is on trusted multi-LLM systems, ensuring robust decision-making in environments where reliability and privacy are crucial. We also present multimodal multi-LLM architectures, where multiple LLMs specialize in handling different data modalities, such as text, images, and audio, by integrating their outputs for comprehensive analysis. Finally, we highlight future directions, including improving resource efficiency, trustworthy governance multi-LLM systems, while addressing privacy, trust, and robustness concerns. This survey provides a valuable reference for researchers and practitioners aiming to leverage multi-LLM systems in edge computing applications.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding","url":"https://arxiv.org/abs/2507.00669","date":1751428800,"author":"","guid":180050,"unread":true,"content":"<article>arXiv:2507.00669v1 Announce Type: new \nAbstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point clouds based on natural language. While prior work has made strides using textual descriptions, leveraging spoken language-known as Audio-based 3D Visual Grounding-remains underexplored and challenging. Motivated by advances in automatic speech recognition (ASR) and speech representation learning, we propose Audio-3DVG, a simple yet effective framework that integrates audio and spatial information for enhanced grounding. Rather than treating speech as a monolithic input, we decompose the task into two complementary components. First, we introduce Object Mention Detection, a multi-label classification task that explicitly identifies which objects are referred to in the audio, enabling more structured audio-scene reasoning. Second, we propose an Audio-Guided Attention module that captures interactions between candidate objects and relational speech cues, improving target discrimination in cluttered scenes. To support benchmarking, we synthesize audio descriptions for standard 3DVG datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate that Audio-3DVG not only achieves new state-of-the-art performance in audio-based grounding, but also competes with text-based methods-highlighting the promise of integrating spoken language into 3D vision tasks.</article>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Special measures of smoothness for approximation by sampling operators in $L_p(\\Bbb{R}^d)$","url":"https://arxiv.org/abs/2507.00667","date":1751428800,"author":"","guid":180051,"unread":true,"content":"<article>arXiv:2507.00667v1 Announce Type: new \nAbstract: Traditional measures of smoothness often fail to provide accurate $L_p$-error estimates for approximation by sampling or interpolation operators, especially for functions with low smoothness. To address this issue, we introduce a modified measure of smoothness that incorporates the local behavior of a function at the sampling points through the use of averaged operators. With this new tool, we obtain matching direct and inverse error estimates for a wide class of sampling operators and functions in $L_p$ spaces. Additionally, we derive a criterion for the convergence of sampling operators in $L_p$, identify conditions that ensure the exact rate of approximation, construct realizations of $K$-functionals based on these operators, and study the smoothness properties of sampling operators. We also demonstrate how our results apply to several well-known operators, including the classical Whittaker-Shannon sampling operator, sampling operators generated by $B$-splines, and those based on the Gaussian.</article>","contentLength":1060,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SAFER: Probing Safety in Reward Models with Sparse Autoencoder","url":"https://arxiv.org/abs/2507.00665","date":1751428800,"author":"","guid":180052,"unread":true,"content":"<article>arXiv:2507.00665v1 Announce Type: new \nAbstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at https://github.com/xzy-101/SAFER-code. \\textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment","url":"https://arxiv.org/abs/2507.00659","date":1751428800,"author":"","guid":180053,"unread":true,"content":"<article>arXiv:2507.00659v1 Announce Type: new \nAbstract: We propose a novel method for aerial visual localization over low Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method LoD-Loc has shown promising localization results leveraging LoD models. However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the majority of available models and those many countries plan to construct nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD city models could unlock drones' potential for global urban localization. To address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine strategy using explicit silhouette alignment to achieve accurate localization over low-LoD city models in the air. Specifically, given a query image, LoD-Loc v2 first applies a building segmentation network to shape building silhouettes. Then, in the coarse pose selection stage, we construct a pose cost volume by uniformly sampling pose hypotheses around a prior pose to represent the pose probability distribution. Each cost of the volume measures the degree of alignment between the projected and predicted silhouettes. We select the pose with maximum value as the coarse pose. In the fine pose estimation stage, a particle filtering method incorporating a multi-beam tracking approach is used to efficiently explore the hypothesis space and obtain the final pose estimation. To further facilitate research in this field, we release two datasets with LoD1 city models covering 10.7 km , along with real RGB queries and ground-truth pose annotations. Experimental results show that LoD-Loc v2 improves estimation accuracy with high-LoD models and enables localization with low-LoD models for the first time. Moreover, it outperforms state-of-the-art baselines by large margins, even surpassing texture-model-based methods, and broadens the convergence basin to accommodate larger prior errors.</article>","contentLength":1944,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity","url":"https://arxiv.org/abs/2507.00657","date":1751428800,"author":"","guid":180054,"unread":true,"content":"<article>arXiv:2507.00657v1 Announce Type: new \nAbstract: We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call \"generation exaggeration\": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.</article>","contentLength":1295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Rate-Distortion Function for Sampled Cyclostationary Gaussian Processes with Memory and with Bounded Processing Delay: Extended Version with Proofs","url":"https://arxiv.org/abs/2507.00656","date":1751428800,"author":"","guid":180055,"unread":true,"content":"<article>arXiv:2507.00656v1 Announce Type: new \nAbstract: We study the rate-distortion function (RDF) for the lossy compression of discrete-time (DT) wide-sense almost cyclostationary (WSACS) Gaussian processes with memory, arising from sampling continuous-time (CT) wide-sense cyclostationary (WSCS) Gaussian source processes. The importance of this problem arises as such CT processes represent communications signals, and sampling must be applied to facilitate the DT processing associated with their compression. Moreover, the physical characteristics of oscillators imply that the sampling interval is incommensurate with the period of the autocorrelation function (AF) of the physical process, giving rise to the DT WSACS model considered. In addition, to reduce the loss, the sampling interval is generally shorter than the correlation length, and thus, the DT process is correlated as well. The difficulty in the RDF characterization follows from the information-instability of WSACS processes, which renders the traditional information-theoretic tools inapplicable. In this work we utilize the information-spectrum framework to characterize the RDF when a finite and bounded delay is allowed between processing of subsequent source sequences. This scenario extends our previous works which studied settings without processing delays or without memory. Numerical evaluations reveal the impact of scenario parameters on the RDF with asynchronous sampling.</article>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural Augmented Kalman Filters for Road Network assisted GNSS positioning","url":"https://arxiv.org/abs/2507.00654","date":1751428800,"author":"","guid":180056,"unread":true,"content":"<article>arXiv:2507.00654v1 Announce Type: new \nAbstract: The Global Navigation Satellite System (GNSS) provides critical positioning information globally, but its accuracy in dense urban environments is often compromised by multipath and non-line-of-sight errors. Road network data can be used to reduce the impact of these errors and enhance the accuracy of a positioning system. Previous works employing road network data are either limited to offline applications, or rely on Kalman Filter (KF) heuristics with little flexibility and robustness. We instead propose training a Temporal Graph Neural Network (TGNN) to integrate road network information into a KF. The TGNN is designed to predict the correct road segment and its associated uncertainty to be used in the measurement update step of the KF. We validate our approach with real-world GNSS data and open-source road networks, observing a 29% decrease in positioning error for challenging scenarios compared to a GNSS-only KF. To the best of our knowledge, ours is the first deep learning-based approach jointly employing road network data and GNSS measurements to determine the user position on Earth.</article>","contentLength":1155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models","url":"https://arxiv.org/abs/2507.00653","date":1751428800,"author":"","guid":180057,"unread":true,"content":"<article>arXiv:2507.00653v1 Announce Type: new \nAbstract: The escalating computational costs of Large Language Model (LLM) inference have become a critical barrier to their widespread and sustainable deployment. While existing optimization strategies are effective, they are predominantly based on statistical heuristics or architectural modifications, lacking a guiding cognitive theory to manage the inference process itself. This paper aims to bridge this gap by introducing a novel paradigm: the Cognitive Load-Aware Inference (CLAI) framework, which operationalizes principles from Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$, and $GCL_{LLM}$), thereby reframing the inference process as a cognitive economics optimization problem: based on the intrinsic complexity of a problem ($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a fine-tuned model that internalizes these principles for spontaneous cognitive economy. Across a range of benchmarks in complex reasoning, long-context question answering, and code generation, our methods achieve significant reductions in token consumption (up to 45\\%) without sacrificing accuracy. Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose difficult problems, a key characteristic of human expert cognition. This work demonstrates that by emulating the brain's resource management strategies, we can build more efficient, robust, and capable artificial intelligence systems.</article>","contentLength":1885,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GANs Secretly Perform Approximate Bayesian Model Selection","url":"https://arxiv.org/abs/2507.00651","date":1751428800,"author":"","guid":180058,"unread":true,"content":"<article>arXiv:2507.00651v1 Announce Type: new \nAbstract: Generative Adversarial Networks (GANs) are popular and successful generative models. Despite their success, optimization is notoriously challenging and they require regularization against overfitting. In this work, we explain the success and limitations of GANs by interpreting them as probabilistic generative models. This interpretation enables us to view GANs as Bayesian neural networks with partial stochasticity, allowing us to establish conditions of universal approximation. We can then cast the adversarial-style optimization of several variants of GANs as the optimization of a proxy for the marginal likelihood. Taking advantage of the connection between marginal likelihood optimization and Occam's razor, we can define regularization and optimization strategies to smooth the loss landscape and search for solutions with minimum description length, which are associated with flat minima and good generalization. The results on a wide range of experiments indicate that these strategies lead to performance improvements and pave the way to a deeper understanding of regularization strategies for GANs.</article>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions","url":"https://arxiv.org/abs/2507.00648","date":1751428800,"author":"","guid":180059,"unread":true,"content":"<article>arXiv:2507.00648v1 Announce Type: new \nAbstract: Visual object tracking has gained promising progress in past decades. Most of the existing approaches focus on learning target representation in well-conditioned daytime data, while for the unconstrained real-world scenarios with adverse weather conditions, e.g. nighttime or foggy environment, the tremendous domain shift leads to significant performance degradation. In this paper, we propose UMDATrack, which is capable of maintaining high-quality target state prediction under various adverse weather conditions within a unified domain adaptation framework. Specifically, we first use a controllable scenario generator to synthesize a small amount of unlabeled videos (less than 2% frames in source daytime datasets) in multiple weather conditions under the guidance of different text prompts. Afterwards, we design a simple yet effective domain-customized adapter (DCA), allowing the target objects' representation to rapidly adapt to various weather conditions without redundant model updating. Furthermore, to enhance the localization consistency between source and target domains, we propose a target-aware confidence alignment module (TCA) following optimal transport theorem. Extensive experiments demonstrate that UMDATrack can surpass existing advanced visual trackers and lead new state-of-the-art performance by a significant margin. Our code is available at https://github.com/Z-Z188/UMDATrack.</article>","contentLength":1458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cooperative Sheaf Neural Networks","url":"https://arxiv.org/abs/2507.00647","date":1751428800,"author":"","guid":180060,"unread":true,"content":"<article>arXiv:2507.00647v1 Announce Type: new \nAbstract: Sheaf diffusion has recently emerged as a promising design pattern for graph representation learning due to its inherent ability to handle heterophilic data and avoid oversmoothing. Meanwhile, cooperative message passing has also been proposed as a way to enhance the flexibility of information diffusion by allowing nodes to independently choose whether to propagate/gather information from/to neighbors. A natural question ensues: is sheaf diffusion capable of exhibiting this cooperative behavior? Here, we provide a negative answer to this question. In particular, we show that existing sheaf diffusion methods fail to achieve cooperative behavior due to the lack of message directionality. To circumvent this limitation, we introduce the notion of cellular sheaves over directed graphs and characterize their in- and out-degree Laplacians. We leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs). Theoretically, we characterize the receptive field of CSNN and show it allows nodes to selectively attend (listen) to arbitrarily far nodes while ignoring all others in their path, potentially mitigating oversquashing. Our experiments show that CSNN presents overall better performance compared to prior art on sheaf diffusion as well as cooperative graph neural networks.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parallel Transmission Aware Co-Design: Enhancing Manipulator Performance Through Actuation-Space Optimization","url":"https://arxiv.org/abs/2507.00644","date":1751428800,"author":"","guid":180061,"unread":true,"content":"<article>arXiv:2507.00644v1 Announce Type: new \nAbstract: In robotics, structural design and behavior optimization have long been considered separate processes, resulting in the development of systems with limited capabilities. Recently, co-design methods have gained popularity, where bi-level formulations are used to simultaneously optimize the robot design and behavior for specific tasks. However, most implementations assume a serial or tree-type model of the robot, overlooking the fact that many robot platforms incorporate parallel mechanisms. In this paper, we present a novel co-design approach that explicitly incorporates parallel coupling constraints into the dynamic model of the robot. In this framework, an outer optimization loop focuses on the design parameters, in our case the transmission ratios of a parallel belt-driven manipulator, which map the desired torques from the joint space to the actuation space. An inner loop performs trajectory optimization in the actuation space, thus exploiting the entire dynamic range of the manipulator. We compare the proposed method with a conventional co-design approach based on a simplified tree-type model. By taking advantage of the actuation space representation, our approach leads to a significant increase in dynamic payload capacity compared to the conventional co-design implementation.</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized Pliable Index Coding For Federated Learning In Intelligent Transportation Systems","url":"https://arxiv.org/abs/2507.00643","date":1751428800,"author":"","guid":180062,"unread":true,"content":"<article>arXiv:2507.00643v1 Announce Type: new \nAbstract: Federated Learning is a promising option for data privacy and security in ITS, because it allows edge devices, Road Side Units (RSUs), and Central Server (CS) to jointly train the machine learning model. Since RSU collects data from the vehicles passing through its range, the local data of each RSU will have a non-IID distribution, which adversely affects the convergence speed and accuracy of FL training. Generating synthetic data locally at individual nodes, followed by data shuffling among the nodes, is a promising approach to address the Non-IID data problem. In this work, we propose pliable index coding (PIC) solutions for efficient data shuffling among the nodes in an FL system. In PIC($S$) problems, a client is satisfied if it can retrieve any $S$ new messages not originally present in its side-information. We particularly consider decentralized pliable index coding problems (DPIC) where the clients communicate among themselves without a central server to model the data shuffling in FL. A class of DPIC, known as Consecutive Decentralized Pliable Index Coding (CDPIC($S$,$K$)), where each client has $K$ consecutive messages as side-information, is considered. For CDPIC($S$,$K$) problems, pliable index code designs are provided for any value of $K$ and $S$, and optimality proofs for some of the cases are established. Further, these CDPIC solutions are applied for data shuffling in FL, to transform the local data distribution towards IID progressively with each transmission, thereby enhancing the performance of FL. The improvement in the accuracy and convergence of the most popular FL technique, FedAvg, and a promising federated submodel technique, CELL (Communication Efficient Lottery Learning), are analysed by providing different degrees of data shuffling using the proposed CDPIC schemes.</article>","contentLength":1872,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatHLS: Towards Systematic Design Automation and Optimization for High-Level Synthesis","url":"https://arxiv.org/abs/2507.00642","date":1751428800,"author":"","guid":180063,"unread":true,"content":"<article>arXiv:2507.00642v1 Announce Type: new \nAbstract: The increasing complexity of computational demands has accelerated the adoption of domain-specific accelerators, yet traditional hardware design methodologies remain constrained by prolonged development and verification cycles. High-Level Synthesis (HLS) bridges the gap between software and hardware by enabling hardware design from high-level programming languages. However, its widespread adoption is hindered by strict coding constraints and intricate hardware-specific optimizations, creating significant obstacles for developers. Recent advancements in Large Language Models (LLMs) demonstrate substantial potential in hardware design automation. However, their effectiveness is limited by the scarcity of high-quality datasets, particularly in the context of HLS. To address these challenges, we introduce ChatHLS, an agile HLS design automation and optimization workflow that leverages fine-tuned LLMs integrated within a multi-agent framework for error correction and design optimization. Our extensive evaluations reveal that ChatHLS achieves an average repair pass rate of 82.7% over 612 test cases, outperforming the GPT-4o and Llama3-8B by 19.1% and 63.0%, respectively. Furthermore, ChatHLS delivers performance enhancements ranging from 1.9$\\times$ to 14.8$\\times$ upon resource-constrained kernels. By enabling sophisticated optimization reasoning within practical computational budgets, ChatHLS attains a 4.9$\\times$ geometric mean speedup compared to state-of-the-art DSL-based approaches. These results underscore the potential of ChatHLS in substantially expediting hardware development cycles while maintaining rigorous standards of design reliability and optimization quality.</article>","contentLength":1747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Network and Attack Graphs for Service-Centric Impact Analysis","url":"https://arxiv.org/abs/2507.00637","date":1751428800,"author":"","guid":180064,"unread":true,"content":"<article>arXiv:2507.00637v1 Announce Type: new \nAbstract: We present a novel methodology for modelling, visualising, and analysing cyber threats, attack paths, as well as their impact on user services in enterprise or infrastructure networks of digital devices and services they provide. Using probabilistic methods to track the propagation of an attack through attack graphs, via the service or application layers, and on physical communication networks, our model enables us to analyse cyber attacks at different levels of detail. Understanding the propagation of an attack within a service among microservices and its spread between different services or application servers could help detect and mitigate it early. We demonstrate that this network-based influence spreading modelling approach enables the evaluation of diverse attack scenarios and the development of protection and mitigation measures, taking into account the criticality of services from the user's perspective. This methodology could also aid security specialists and system administrators in making well-informed decisions regarding risk mitigation strategies.</article>","contentLength":1125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery","url":"https://arxiv.org/abs/2507.00635","date":1751428800,"author":"","guid":180065,"unread":true,"content":"<article>arXiv:2507.00635v1 Announce Type: new \nAbstract: Ophthalmic surgical robots offer superior stability and precision by reducing the natural hand tremors of human surgeons, enabling delicate operations in confined surgical spaces. Despite the advancements in developing vision- and force-based control methods for surgical robots, preoperative navigation remains heavily reliant on manual operation, limiting the consistency and increasing the uncertainty. Existing eye gaze estimation techniques in the surgery, whether traditional or deep learning-based, face challenges including dependence on additional sensors, occlusion issues in surgical environments, and the requirement for facial detection. To address these limitations, this study proposes an innovative eye localization and tracking method that combines machine learning with traditional algorithms, eliminating the requirements of landmarks and maintaining stable iris detection and gaze estimation under varying lighting and shadow conditions. Extensive real-world experiment results show that our proposed method has an average estimation error of 0.58 degrees for eye orientation estimation and 2.08-degree average control error for the robotic arm's movement based on the calculated orientation.</article>","contentLength":1261,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Horus: A Protocol for Trustless Delegation Under Uncertainty","url":"https://arxiv.org/abs/2507.00631","date":1751428800,"author":"","guid":180066,"unread":true,"content":"<article>arXiv:2507.00631v1 Announce Type: new \nAbstract: Correctness is an emergent property of systems where exposing error is cheaper than committing it. In dynamic, low-trust environments, autonomous AI agents benefit from delegating work to sub-agents, yet correctness cannot be assured through upfront specification or centralized oversight. We propose a protocol that enforces correctness through collateralized claims in a recursive verification game. Tasks are published as intents, and solvers compete to fulfill them. Selected solvers carry out tasks under risk, with correctness checked post hoc by verifiers. Any challenger can challenge a result by staking against it to trigger the verification process. Incorrect agents are slashed and correct opposition is rewarded, with an escalation path that penalizes erroneous verifiers themselves. When incentives are aligned across solvers, challengers, and verifiers, falsification conditions make correctness the Nash equilibrium.</article>","contentLength":981,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Price Aware Power Split Control in Heterogeneous Battery Storage Systems","url":"https://arxiv.org/abs/2507.00628","date":1751428800,"author":"","guid":180067,"unread":true,"content":"<article>arXiv:2507.00628v1 Announce Type: new \nAbstract: This paper presents a unified framework for the optimal scheduling of battery dispatch and internal power allocation in Battery energy storage systems (BESS). This novel approach integrates both market-based (price-aware) signals and physical system constraints to simultaneously optimize (1) external energy dispatch and (2) internal heterogeneity management of BESS, enhancing its operational economic value and performance. This work compares both model-based Linear Programming (LP) and model-free Reinforcement Learning (RL) approaches for optimization under varying forecast assumptions, using a custom Gym-based simulation environment. The evaluation considers both long-term and short-term performance, focusing on economic savings, State of Charge (SOC) and temperature balancing, and overall system efficiency. In summary, the long-term results show that the RL approach achieved 10% higher system efficiency compared to LP, whereas the latter yielded 33% greater cumulative savings. In terms of internal heterogeneity, the LP approach resulted in lower mean SOC imbalance, while the RL approach achieved better temperature balance between strings. This behavior is further examined in the short-term evaluation, which indicates that LP delivers strong optimization under known and stable conditions, whereas RL demonstrates higher adaptability in dynamic environments, offering potential advantages for real-time BESS control.</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols","url":"https://arxiv.org/abs/2507.00623","date":1751428800,"author":"","guid":180068,"unread":true,"content":"<article>arXiv:2507.00623v1 Announce Type: new \nAbstract: The increasing complexity of Extended Reality (XR) applications demands substantial processing power and high bandwidth communications, often unavailable on lightweight devices. Remote rendering consists of offloading processing tasks to a remote node with a powerful GPU, delivering the rendered content to the end device. The delivery is usually performed through popular streaming protocols such as Web Real-Time Communications (WebRTC), offering a data channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH), better suitable for scalability. Moreover, new streaming protocols based on QUIC are emerging as potential replacements for WebRTC and DASH and offer benefits like connection migration, stream multiplexing and multipath delivery. This work describes the integration of the two most popular multimedia frameworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote Renderer, and analyzes their performance when offering different protocols for delivering the rendered content to the end device over WIFI or 5G. This solution constitutes a beyond state-of-the-art testbed to conduct cutting-edge research in the XR field.</article>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gender Differences in International Research Collaboration in European Union","url":"https://arxiv.org/abs/2507.00619","date":1751428800,"author":"","guid":180069,"unread":true,"content":"<article>arXiv:2507.00619v1 Announce Type: new \nAbstract: This paper investigates International Research Collaboration (IRC) among European Union (EU) countries from 2011 to 2022, with emphasis on gender-based authorship patterns. Drawing from the Web of Science Social Science Citation Index (WoS-SSCI) database, a large dataset of IRC articles was constructed, annotated with categories of authorship based on gender, author affiliation, and COVID-19 subject as topic. Using network science, the study maps collaboration structures and reveals gendered differences in co-authorship networks. Results highlight a substantial rise in IRC over the decade, particularly with the USA and China as key non-EU partners. Articles with at least one female author were consistently less frequent than those with at least one male author. Notably, female-exclusive collaborations showed distinctive network topologies, with more centralized (star-like) patterns and shorter tree diameters. The COVID-19 pandemic further reshaped collaboration dynamics, temporarily reducing the gender gap in IRC but also revealing vulnerabilities in female-dominated research networks. These findings underscore both progress and persistent disparities in the gender dynamics of EU participation in IRC.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerating MPGP-type Methods Through Preconditioning","url":"https://arxiv.org/abs/2507.00617","date":1751428800,"author":"","guid":180070,"unread":true,"content":"<article>arXiv:2507.00617v1 Announce Type: new \nAbstract: This work investigates the acceleration of MPGP-type algorithms using preconditioning for the solution of quadratic programming problems. The preconditioning needs to be done only on the free set so as not to change the constraints. A variant of preconditioning restricted to the free set is the preconditioning in face. The inner preconditioner in preconditioning in face needs to be recomputed or updated every time the free set changes. Here, we investigate an approximate variant of preconditioning in face that computes the inner preconditioner only once. We analyze the error of the approximate variant and provide numerical experiments demonstrating that very large speedups can be achieved by the approximate variant.</article>","contentLength":774,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hamiltonicity Parameterized by Mim-Width is (Indeed) Para-NP-Hard","url":"https://arxiv.org/abs/2507.00612","date":1751428800,"author":"","guid":180071,"unread":true,"content":"<article>arXiv:2507.00612v1 Announce Type: new \nAbstract: We prove that Hamiltonian Path and Hamiltonian Cycle are NP-hard on graphs of linear mim-width 26, even when a linear order of the input graph with mim-width 26 is provided together with input. This fills a gap left by a broken proof of the para-NP-hardness of Hamiltonicity problems parameterized by mim-width.</article>","contentLength":360,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Residual Reward Models for Preference-based Reinforcement Learning","url":"https://arxiv.org/abs/2507.00611","date":1751428800,"author":"","guid":180072,"unread":true,"content":"<article>arXiv:2507.00611v1 Announce Type: new \nAbstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn high-performance policies in environments where the reward signal is hard to specify, avoiding heuristic and time-consuming reward design. However, PbRL can suffer from slow convergence speed since it requires training in a reward model. Prior work has proposed learning a reward model from demonstrations and fine-tuning it using preferences. However, when the model is a neural network, using different loss functions for pre-training and fine-tuning can pose challenges to reliable optimization. In this paper, we propose a method to effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM assumes that the true reward of the environment can be split into a sum of two parts: a prior reward and a learned reward. The prior reward is a term available before training, for example, a user's ``best guess'' reward function, or a reward function learned from inverse reinforcement learning (IRL), and the learned reward is trained with preferences. We introduce state-based and image-based versions of RRM and evaluate them on several tasks in the Meta-World environment suite. Experimental results show that our method substantially improves the performance of a common PbRL method. Our method achieves performance improvements for a variety of different types of prior rewards, including proxy rewards, a reward obtained from IRL, and even a negated version of the proxy reward. We also conduct experiments with a Franka Panda to show that our method leads to superior performance on a real robot. It significantly accelerates policy learning for different tasks, achieving success in fewer steps than the baseline. The videos are presented at https://sunlighted.github.io/RRM-web/.</article>","contentLength":1827,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the rank weight hierarchy of $M$-codes","url":"https://arxiv.org/abs/2507.00609","date":1751428800,"author":"","guid":180073,"unread":true,"content":"<article>arXiv:2507.00609v1 Announce Type: new \nAbstract: We study the rank weight hierarchy of linear codes which are stable under a linear endomorphism defined over the base field, in particular when the endomorphism is cyclic. In this last case, we give a necessary and sufficient condition for such a code to have first rank weight equal to $1$ in terms of its generator polynomial, as well as an explicit formula for its last rank weight.</article>","contentLength":434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection","url":"https://arxiv.org/abs/2507.00608","date":1751428800,"author":"","guid":180074,"unread":true,"content":"<article>arXiv:2507.00608v1 Announce Type: new \nAbstract: Despite its significant success, object detection in traffic and transportation scenarios requires time-consuming and laborious efforts in acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation (UDA) for object detection has recently gained increasing research attention. UDA for object detection has been dominated by domain alignment methods, which achieve top performance. Recently, self-labeling methods have gained popularity due to their simplicity and efficiency. In this paper, we investigate the limitations that prevent self-labeling detectors from achieving commensurate performance with domain alignment methods. Specifically, we identify the high proportion of simple samples during training, i.e., the simple-label bias, as the central cause. We propose a novel approach called De-Simplifying Pseudo Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level memory bank to implement an innovative pseudo label updating strategy. Then, adversarial samples are introduced during training to enhance the proportion. Furthermore, we propose an adaptive weighted loss to avoid the model suffering from an abundance of false positive pseudo labels in the late training period. Experimental results demonstrate that DeSimPL effectively reduces the proportion of simple samples during training, leading to a significant performance improvement for self-labeling detectors. Extensive experiments conducted on four benchmarks validate our analysis and conclusions.</article>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies","url":"https://arxiv.org/abs/2507.00606","date":1751428800,"author":"","guid":180075,"unread":true,"content":"<article>arXiv:2507.00606v1 Announce Type: new \nAbstract: Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.</article>","contentLength":994,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model","url":"https://arxiv.org/abs/2507.00603","date":1751428800,"author":"","guid":180076,"unread":true,"content":"<article>arXiv:2507.00603v1 Announce Type: new \nAbstract: End-to-end autonomous driving directly generates planning trajectories from raw sensor data, yet it typically relies on costly perception supervision to extract scene information. A critical research challenge arises: constructing an informative driving world model to enable perception annotation-free, end-to-end planning via self-supervised learning. In this paper, we present World4Drive, an end-to-end autonomous driving framework that employs vision foundation models to build latent world models for generating and evaluating multi-modal planning trajectories. Specifically, World4Drive first extracts scene features, including driving intention and world latent representations enriched with spatial-semantic priors provided by vision foundation models. It then generates multi-modal planning trajectories based on current scene features and driving intentions and predicts multiple intention-driven future states within the latent space. Finally, it introduces a world model selector module to evaluate and select the best trajectory. We achieve perception annotation-free, end-to-end planning through self-supervised alignment between actual future observations and predicted observations reconstructed from the latent space. World4Drive achieves state-of-the-art performance without manual perception annotations on both the open-loop nuScenes and closed-loop NavSim benchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower collision rate, and 3.75 faster training convergence. Codes will be accessed at https://github.com/ucaszyp/World4Drive.</article>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based","url":"https://arxiv.org/abs/2507.00601","date":1751428800,"author":"","guid":180077,"unread":true,"content":"<article>arXiv:2507.00601v1 Announce Type: new \nAbstract: This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model's original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method's applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.</article>","contentLength":1605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks","url":"https://arxiv.org/abs/2507.00600","date":1751428800,"author":"","guid":180078,"unread":true,"content":"<article>arXiv:2507.00600v1 Announce Type: new \nAbstract: Understanding the functional roles of financial institutions within interconnected markets is critical for effective supervision, systemic risk assessment, and resolution planning. We propose an interpretable role-based clustering approach for multi-layer financial networks, designed to identify the functional positions of institutions across different market segments. Our method follows a general clustering framework defined by proximity measures, cluster evaluation criteria, and algorithm selection. We construct explainable node embeddings based on egonet features that capture both direct and indirect trading relationships within and across market layers. Using transaction-level data from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate how the approach uncovers heterogeneous institutional roles such as market intermediaries, cross-segment connectors, and peripheral lenders or borrowers. The results highlight the flexibility and practical value of role-based clustering in analyzing financial networks and understanding institutional behavior in complex market structures.</article>","contentLength":1156,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"High-resolution spatial memory requires grid-cell-like neural codes","url":"https://arxiv.org/abs/2507.00598","date":1751428800,"author":"","guid":180079,"unread":true,"content":"<article>arXiv:2507.00598v1 Announce Type: new \nAbstract: Continuous attractor networks (CANs) are widely used to model how the brain temporarily retains continuous behavioural variables via persistent recurrent activity, such as an animal's position in an environment. However, this memory mechanism is very sensitive to even small imperfections, such as noise or heterogeneity, which are both common in biological systems. Previous work has shown that discretising the continuum into a finite set of discrete attractor states provides robustness to these imperfections, but necessarily reduces the resolution of the represented variable, creating a dilemma between stability and resolution. We show that this stability-resolution dilemma is most severe for CANs using unimodal bump-like codes, as in traditional models. To overcome this, we investigate sparse binary distributed codes based on random feature embeddings, in which neurons have spatially-periodic receptive fields. We demonstrate theoretically and with simulations that such grid-cell-like codes enable CANs to achieve both high stability and high resolution simultaneously. The model extends to embedding arbitrary nonlinear manifolds into a CAN, such as spheres or tori, and generalises linear path integration to integration along freely-programmable on-manifold vector fields. Together, this work provides a theory of how the brain could robustly represent continuous variables with high resolution and perform flexible computations over task-relevant manifolds.</article>","contentLength":1524,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gaze3P: Gaze-Based Prediction of User-Perceived Privacy","url":"https://arxiv.org/abs/2507.00596","date":1751428800,"author":"","guid":180080,"unread":true,"content":"<article>arXiv:2507.00596v1 Announce Type: new \nAbstract: Privacy is a highly subjective concept and perceived variably by different individuals. Previous research on quantifying user-perceived privacy has primarily relied on questionnaires. Furthermore, applying user-perceived privacy to optimise the parameters of privacy-preserving techniques (PPT) remains insufficiently explored. To address these limitations, we introduce Gaze3P -- the first dataset specifically designed to facilitate systematic investigations into user-perceived privacy. Our dataset comprises gaze data from 100 participants and 1,000 stimuli, encompassing a range of private and safe attributes. With Gaze3P, we train a machine learning model to implicitly and dynamically predict perceived privacy from human eye gaze. Through comprehensive experiments, we show that the resulting models achieve high accuracy. Finally, we illustrate how predicted privacy can be used to optimise the parameters of differentially private mechanisms, thereby enhancing their alignment with user expectations.</article>","contentLength":1060,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Secrets Must Not Flow: Scaling Security Verification to Large Codebases (extended version)","url":"https://arxiv.org/abs/2507.00595","date":1751428800,"author":"","guid":180081,"unread":true,"content":"<article>arXiv:2507.00595v1 Announce Type: new \nAbstract: Existing program verifiers can prove advanced properties about security protocol implementations, but are difficult to scale to large codebases because of the manual effort required. We develop a novel methodology called *Diodon* that addresses this challenge by splitting the codebase into the protocol implementation (the *Core*) and the remainder (the *Application*). This split allows us to apply powerful semi-automated verification techniques to the security-critical Core, while fully-automatic static analyses scale the verification to the entire codebase by ensuring that the Application cannot invalidate the security properties proved for the Core. The static analyses achieve that by proving *I/O independence*, i.e., that the I/O operations within the Application are independent of the Core's security-relevant data (such as keys), and that the Application meets the Core's requirements. We have proved Diodon sound by first showing that we can safely allow the Application to perform I/O independent of the security protocol, and second that manual verification and static analyses soundly compose. We evaluate Diodon on two case studies: an implementation of the signed Diffie-Hellman key exchange and a large (100k+ LoC) production Go codebase implementing a key exchange protocol for which we obtained secrecy and injective agreement guarantees by verifying a Core of about 1% of the code with the auto-active program verifier Gobra in less than three person months.</article>","contentLength":1533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods","url":"https://arxiv.org/abs/2507.00593","date":1751428800,"author":"","guid":180082,"unread":true,"content":"<article>arXiv:2507.00593v1 Announce Type: new \nAbstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and ensuring efficient traffic flow. Accurate prediction of such manoeuvres is essential for Advanced Driver Assistance Systems (ADAS) to make timely and informed decisions. In this study, we focus on overtake detection using Controller Area Network (CAN) bus data collected from five in-service trucks provided by the Volvo Group. We evaluate three common classifiers for vehicle manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and Support Vector Machines (SVM), and analyse how different preprocessing configurations affect performance. We find that variability in traffic conditions strongly influences the signal patterns, particularly in the no-overtake class, affecting classification performance if training data lacks adequate diversity. Since the data were collected under unconstrained, real-world conditions, class diversity cannot be guaranteed a priori. However, training with data from multiple vehicles improves generalisation and reduces condition-specific bias. Our pertruck analysis also reveals that classification accuracy, especially for overtakes, depends on the amount of training data per vehicle. To address this, we apply a score-level fusion strategy, which yields the best per-truck performance across most cases. Overall, we achieve an accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True Positive Rate). This research has been part of the BIG FUN project, which explores how Artificial Intelligence can be applied to logged vehicle data to understand and predict driver behaviour, particularly in relation to Camera Monitor Systems (CMS), being introduced as digital replacements for traditional exterior mirrors.</article>","contentLength":1810,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Construction of LDPC convolutional codes with large girth from Latin squares","url":"https://arxiv.org/abs/2507.00591","date":1751428800,"author":"","guid":180083,"unread":true,"content":"<article>arXiv:2507.00591v1 Announce Type: new \nAbstract: Due to their capacity approaching performance low-density parity-check (LDPC) codes gained a lot of attention in the last years. The parity-check matrix of the codes can be associated with a bipartite graph, called Tanner graph. To decrease the probability of decoding failure it is desirable to have LDPC codes with large girth of the associated Tanner graph. Moreover, to store such codes efficiently, it is desirable to have compact constructions for them. In this paper, we present constructions of LDPC convolutional codes with girth up to $12$ using a special class of Latin squares and several lifting steps, which enables a compact representation of these codes. With these techniques, we can provide constructions for well-performing and efficiently storable time-varying and time-invariant LDPC convolutional codes as well as for LDPC block codes.</article>","contentLength":906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Circuit Structure Optimization for Quantum Reinforcement Learning","url":"https://arxiv.org/abs/2507.00589","date":1751428800,"author":"","guid":180084,"unread":true,"content":"<article>arXiv:2507.00589v1 Announce Type: new \nAbstract: Reinforcement learning (RL) enables agents to learn optimal policies through environmental interaction. However, RL suffers from reduced learning efficiency due to the curse of dimensionality in high-dimensional spaces. Quantum reinforcement learning (QRL) addresses this issue by leveraging superposition and entanglement in quantum computing, allowing efficient handling of high-dimensional problems with fewer resources. QRL combines quantum neural networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as the core computational module. The PQC performs linear and nonlinear transformations through gate operations, similar to hidden layers in classical neural networks. Previous QRL studies, however, have used fixed PQC structures based on empirical intuition without verifying their optimality. This paper proposes a QRL-NAS algorithm that integrates quantum neural architecture search (QNAS) to optimize PQC structures within QRL. Experiments demonstrate that QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its effectiveness and practical utility.</article>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Context-Aware Academic Emotion Dataset and Benchmark","url":"https://arxiv.org/abs/2507.00586","date":1751428800,"author":"","guid":180085,"unread":true,"content":"<article>arXiv:2507.00586v1 Announce Type: new \nAbstract: Academic emotion analysis plays a crucial role in evaluating students' engagement and cognitive states during the learning process. This paper addresses the challenge of automatically recognizing academic emotions through facial expressions in real-world learning environments. While significant progress has been made in facial expression recognition for basic emotions, academic emotion recognition remains underexplored, largely due to the scarcity of publicly available datasets. To bridge this gap, we introduce RAER, a novel dataset comprising approximately 2,700 video clips collected from around 140 students in diverse, natural learning contexts such as classrooms, libraries, laboratories, and dormitories, covering both classroom sessions and individual study. Each clip was annotated independently by approximately ten annotators using two distinct sets of academic emotion labels with varying granularity, enhancing annotation consistency and reliability. To our knowledge, RAER is the first dataset capturing diverse natural learning scenarios. Observing that annotators naturally consider context cues-such as whether a student is looking at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER (CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes learnable text prompts within the vision-language model CLIP to effectively integrate facial expression and context cues from videos. Experimental results demonstrate that CLIP-CAER substantially outperforms state-of-the-art video-based facial expression recognition methods, which are primarily designed for basic emotions, emphasizing the crucial role of context in accurately recognizing academic emotions. Project page: https://zgsfer.github.io/CAER</article>","contentLength":1813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Similarity Memory Prior is All You Need for Medical Image Segmentation","url":"https://arxiv.org/abs/2507.00585","date":1751428800,"author":"","guid":180086,"unread":true,"content":"<article>arXiv:2507.00585v1 Announce Type: new \nAbstract: In recent years, it has been found that \"grandmother cells\" in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-Generated Video Detection via Perceptual Straightening","url":"https://arxiv.org/abs/2507.00583","date":1751428800,"author":"","guid":180087,"unread":true,"content":"<article>arXiv:2507.00583v1 Announce Type: new \nAbstract: The rapid advancement of generative AI enables highly realistic synthetic videos, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose ReStraV(Representation Straightening Video), a novel approach to distinguish natural from AI-generated videos. Inspired by the \"perceptual straightening\" hypothesis -- which suggests real-world video trajectories become more straight in neural representation domain -- we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistics of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark), substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection.</article>","contentLength":1468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification","url":"https://arxiv.org/abs/2507.00579","date":1751428800,"author":"","guid":180088,"unread":true,"content":"<article>arXiv:2507.00579v1 Announce Type: new \nAbstract: Hallucinations are one of the major problems of LLMs, hindering their trustworthiness and deployment to wider use cases. However, most of the research on hallucinations focuses on English data, neglecting the multilingual nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3 - Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. We propose a two-part pipeline that combines retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned to identify common hallucination patterns. Our system achieves competitive results across all languages, reaching top-10 results in eight languages, including English. Moreover, it supports multiple languages beyond the fourteen covered by the shared task. This multilingual hallucination identifier can help to improve LLM outputs and their usefulness in the future.</article>","contentLength":958,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BadViM: Backdoor Attack against Vision Mamba","url":"https://arxiv.org/abs/2507.00577","date":1751428800,"author":"","guid":180089,"unread":true,"content":"<article>arXiv:2507.00577v1 Announce Type: new \nAbstract: Vision State Space Models (SSMs), particularly architectures like Vision Mamba (ViM), have emerged as promising alternatives to Vision Transformers (ViTs). However, the security implications of this novel architecture, especially their vulnerability to backdoor attacks, remain critically underexplored. Backdoor attacks aim to embed hidden triggers into victim models, causing the model to misclassify inputs containing these triggers while maintaining normal behavior on clean inputs. This paper investigates the susceptibility of ViM to backdoor attacks by introducing BadViM, a novel backdoor attack framework specifically designed for Vision Mamba. The proposed BadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency sensitivity patterns of the victim model to create stealthy, distributed triggers. To maximize attack efficacy, we propose a Hidden State Alignment loss that strategically manipulates the internal representations of model by aligning the hidden states of backdoor images with those of target classes. Extensive experimental results demonstrate that BadViM achieves superior attack success rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits remarkable resilience against common defensive measures, including PatchDrop, PatchShuffle and JPEG compression, which typically neutralize normal backdoor attacks.</article>","contentLength":1421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DynoStore: A wide-area distribution system for the management of data over heterogeneous storage","url":"https://arxiv.org/abs/2507.00576","date":1751428800,"author":"","guid":180090,"unread":true,"content":"<article>arXiv:2507.00576v1 Announce Type: new \nAbstract: Data distribution across different facilities offers benefits such as enhanced resource utilization, increased resilience through replication, and improved performance by processing data near its source. However, managing such data is challenging due to heterogeneous access protocols, disparate authentication models, and the lack of a unified coordination framework. This paper presents DynoStore, a system that manages data across heterogeneous storage systems. At the core of DynoStore are data containers, an abstraction that provides standardized interfaces for seamless data management, irrespective of the underlying storage systems. Multiple data container connections create a cohesive wide-area storage network, ensuring resilience using erasure coding policies. Furthermore, a load-balancing algorithm ensures equitable and efficient utilization of storage resources. We evaluate DynoStore using benchmarks and real-world case studies, including the management of medical and satellite data across geographically distributed environments. Our results demonstrate a 10\\% performance improvement compared to centralized cloud-hosted systems while maintaining competitive performance with state-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits superior fault tolerance, withstanding more failures than traditional systems.</article>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Foundation Models for Clinical Records at Health System Scale","url":"https://arxiv.org/abs/2507.00574","date":1751428800,"author":"","guid":180091,"unread":true,"content":"<article>arXiv:2507.00574v1 Announce Type: new \nAbstract: Large-scale pretraining has transformed modeling of language and other data types, but its potential remains underexplored in healthcare with structured electronic health records (EHRs). We present a novel generative pretraining strategy for sequential EHR data using next-visit event prediction. Our model learns to autoregressively generate various tokenized clinical events for the next visit based on patient history and inherently handles the joint prediction of heterogeneous data types. Additionally, we introduce regularization on predicting repeated events and highlight a key pitfall in EHR-based foundation model evaluations: repeated event tokens can inflate performance metrics when new onsets are not distinguished from subsequent occurrences. Our model is evaluated via zero-shot prediction for forecasting dementia and knee osteoarthritis incidence within 2 and 5 years, and the model performance rivals a fully fine-tuned masked pretrained Transformer baseline, demonstrating that our approach captures complex clinical dependencies without requiring costly task-specific fine-tuning.</article>","contentLength":1150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"High order global flux schemes for general steady state preservation of shallow water moment equations with non-conservative products","url":"https://arxiv.org/abs/2507.00573","date":1751428800,"author":"","guid":180092,"unread":true,"content":"<article>arXiv:2507.00573v1 Announce Type: new \nAbstract: Shallow water moment equations are reduced-order models for free-surface flows that allow to represent vertical variations of the velocity profile at the expense of additional evolution equations for a number of additional variables, so called moments. This introduces non-linear non-conservative products in the system, which make the analytical characterization of steady states much harder if not impossible. The lack of analytical steady states poses a challenge for the design of well-balanced schemes, which aim at preserving such steady states as crucial in many applications. In this work, we present a family of fully well-balanced, high-order WENO finite volume methods for general hyperbolic balance laws with non-conservative products like the shallow water moment equations, for which no analytical steady states are available. The schemes are based on the flux globalization approach, in which both source terms and non-conservative products are integrated with a tailored high order quadrature in the divergence term. The resulting global flux is then reconstructed instead of the conservative variables to preserve all steady states. Numerical tests show the optimal convergence of the method and a significant error reduction for steady state solutions. Furthermore, we provide a numerical comparison of perturbed steady states for different families of shallow water moment equations, which illustrates the flexibility of our method that is valid for general equations without prior knowledge of steady states.</article>","contentLength":1577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Out-of-distribution detection in 3D applications: a review","url":"https://arxiv.org/abs/2507.00570","date":1751428800,"author":"","guid":180093,"unread":true,"content":"<article>arXiv:2507.00570v1 Announce Type: new \nAbstract: The ability to detect objects that are not prevalent in the training set is a critical capability in many 3D applications, including autonomous driving. Machine learning methods for object recognition often assume that all object categories encountered during inference belong to a closed set of classes present in the training data. This assumption limits generalization to the real world, as objects not seen during training may be misclassified or entirely ignored. As part of reliable AI, OOD detection identifies inputs that deviate significantly from the training distribution. This paper provides a comprehensive overview of OOD detection within the broader scope of trustworthy and uncertain AI. We begin with key use cases across diverse domains, introduce benchmark datasets spanning multiple modalities, and discuss evaluation metrics. Next, we present a comparative analysis of OOD detection methodologies, exploring model structures, uncertainty indicators, and distributional distance taxonomies, alongside uncertainty calibration techniques. Finally, we highlight promising research directions, including adversarially robust OOD detection and failure identification, particularly relevant to 3D applications. The paper offers both theoretical and practical insights into OOD detection, showcasing emerging research opportunities such as 3D vision integration. These insights help new researchers navigate the field more effectively, contributing to the development of reliable, safe, and robust AI systems.</article>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment","url":"https://arxiv.org/abs/2507.00566","date":1751428800,"author":"","guid":180094,"unread":true,"content":"<article>arXiv:2507.00566v1 Announce Type: new \nAbstract: Zero-shot skeleton-based action recognition aims to classify unseen skeleton-based human actions without prior exposure to such categories during training. This task is extremely challenging due to the difficulty in generalizing from known to unknown actions. Previous studies typically use two-stage training: pre-training skeleton encoders on seen action categories using cross-entropy loss and then aligning pre-extracted skeleton and text features, enabling knowledge transfer to unseen classes through skeleton-text alignment and language models' generalization. However, their efficacy is hindered by 1) insufficient discrimination for skeleton features, as the fixed skeleton encoder fails to capture necessary alignment information for effective skeleton-text alignment; 2) the neglect of alignment bias between skeleton and unseen text features during testing. To this end, we propose a prototype-guided feature alignment paradigm for zero-shot skeleton-based action recognition, termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive training framework to improve skeleton-text alignment, ensuring sufficient discrimination for skeleton features. Additionally, we introduce a prototype-guided text feature alignment strategy to mitigate the adverse impact of the distribution discrepancy during testing. We provide a theoretical analysis to support our prototype-guided text feature alignment strategy and empirically evaluate our overall PGFA on three well-known datasets. Compared with the top competitor SMIE method, our PGFA achieves absolute accuracy improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD datasets, respectively.</article>","contentLength":1738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computational complexity of covering regular trees","url":"https://arxiv.org/abs/2507.00564","date":1751428800,"author":"","guid":180095,"unread":true,"content":"<article>arXiv:2507.00564v1 Announce Type: new \nAbstract: A graph covering projection, also referred to as a locally bijective homomorphism, is a mapping between the vertices and edges of two graphs that preserves incidences and is a local bijection. This concept originates in topological graph theory but has also found applications in combinatorics and theoretical computer science. In this paper we consider undirected graphs in the most general setting -- graphs may contain multiple edges, loops, and semi-edges. This is in line with recent trends in topological graph theory and mathematical physics.\n  We advance the study of the computational complexity of the {\\sc $H$-Cover} problem, which asks whether an input graph allows a covering projection onto a parameter graph $H$. The quest for a complete characterization started in 1990's. Several results for simple graphs or graphs without semi-edges have been known, the role of semi-edges in the complexity setting has started to be investigated only recently. One of the most general known NP-hardness results states that {\\sc $H$}-Cover is NP-complete for every simple connected regular graph of valency greater than two. We complement this result by considering regular graphs $H$ arising from connected acyclic graphs by adding semi-edges. Namely, we prove that any graph obtained by adding semi-edges to the vertices of a tree making it a $d$-regular graph with $d \\geq 3$, defines an NP-complete graph covering problem. In line with the so called Strong Dichotomy Conjecture, we prove that the NP-hardness holds even for simple graphs on input.</article>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Isogeometric contact analysis in subsea umbilical and power cables","url":"https://arxiv.org/abs/2507.00563","date":1751428800,"author":"","guid":180096,"unread":true,"content":"<article>arXiv:2507.00563v1 Announce Type: new \nAbstract: Subsea umbilical and power cables contain a large number of contact interfaces between different geometries and materials. These complex interactions rise significant challenges for accurately considering contact surface properties by using traditional analytical solutions or finite element methods. These properties have been identified as the most sensitive parameters when performing the numerical simulation for stress analysis. Therefore, it is essential to apply a novel approach for contact analysis which improves the accuracy and efficiency for predicting contact properties. This paper presents an isogeometric analysis (IGA) approach addressing contact problems in dynamic umbilicals and power cables. Firstly, this isogeometric contact algorithm is formulated in MATLAB as a tool including the geometry description, contact detection and penalty function. Secondly, the contact interface between a steel tube and an outer sheath in an dynamic umbilical is established by this IGA contact algorithm and validated against that in ABAQUS for proving the accuracy and efficiency of IGA. Finally, the effects of element refinement, geometrical description, penalty factor on the accuracy, efficiency and stability of IGA are discussed.</article>","contentLength":1292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advancing Local Search in SMT-NRA with MCSAT Integration","url":"https://arxiv.org/abs/2507.00557","date":1751428800,"author":"","guid":180097,"unread":true,"content":"<article>arXiv:2507.00557v1 Announce Type: new \nAbstract: In this paper, we advance local search for Satisfiability Modulo the Theory of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a two-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the key operation, cell-jump, of the local search method for SMT-NRA. Then, we propose an extended local search framework, named \\emph{$2d$-LS} (following the local search framework, LS, for SMT-NRA), integrating the model constructing satisfiability calculus (MCSAT) framework to improve search efficiency. To further improve the efficiency of MCSAT, we implement a recently proposed technique called \\emph{sample-cell projection operator} for MCSAT, which is well suited for CDCL-style search in the real domain and helps guide the search away from conflicting states. Finally, we design a hybrid framework for SMT-NRA combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through information exchange. The experimental results demonstrate improvements in local search performance, highlighting the effectiveness of the proposed methods.</article>","contentLength":1124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing","url":"https://arxiv.org/abs/2507.00554","date":1751428800,"author":"","guid":180098,"unread":true,"content":"<article>arXiv:2507.00554v1 Announce Type: new \nAbstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced.</article>","contentLength":1521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generation of Indoor Open Street Maps for Robot Navigation from CAD Files","url":"https://arxiv.org/abs/2507.00552","date":1751428800,"author":"","guid":180099,"unread":true,"content":"<article>arXiv:2507.00552v1 Announce Type: new \nAbstract: The deployment of autonomous mobile robots is predicated on the availability of environmental maps, yet conventional generation via SLAM (Simultaneous Localization and Mapping) suffers from significant limitations in time, labor, and robustness, particularly in dynamic, large-scale indoor environments where map obsolescence can lead to critical localization failures. To address these challenges, this paper presents a complete and automated system for converting architectural Computer-Aided Design (CAD) files into a hierarchical topometric OpenStreetMap (OSM) representation, tailored for robust life-long robot navigation. Our core methodology involves a multi-stage pipeline that first isolates key structural layers from the raw CAD data and then employs an AreaGraph-based topological segmentation to partition the building layout into a hierarchical graph of navigable spaces. This process yields a comprehensive and semantically rich map, further enhanced by automatically associating textual labels from the CAD source and cohesively merging multiple building floors into a unified, topologically-correct model. By leveraging the permanent structural information inherent in CAD files, our system circumvents the inefficiencies and fragility of SLAM, offering a practical and scalable solution for deploying robots in complex indoor spaces. The software is encapsulated within an intuitive Graphical User Interface (GUI) to facilitate practical use. The code and dataset are available at https://github.com/jiajiezhang7/osmAG-from-cad.</article>","contentLength":1596,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collaborative Multi-Agent Reinforcement Learning Approach for Elastic Cloud Resource Scaling","url":"https://arxiv.org/abs/2507.00550","date":1751428800,"author":"","guid":180100,"unread":true,"content":"<article>arXiv:2507.00550v1 Announce Type: new \nAbstract: This paper addresses the challenges of rapid resource variation and highly uncertain task loads in cloud computing environments. It proposes an optimization method for elastic cloud resource scaling based on a multi-agent system. The method deploys multiple autonomous agents to perceive resource states in parallel and make local decisions. While maintaining the distributed nature of the system, it introduces a collaborative value function to achieve global coordination. This improves the responsiveness of resource scheduling and enhances overall system performance. To strengthen system foresight, a lightweight state prediction model is designed. It assists agents in identifying future workload trends and optimizes the selection of scaling actions. For policy training, the method adopts a centralized training and decentralized execution reinforcement learning framework. This enables agents to learn effectively and coordinate strategies under conditions of incomplete information. The paper also constructs typical cloud scenarios, including multi-tenancy and burst traffic, to evaluate the proposed method. The evaluation focuses on resource isolation, service quality assurance, and robustness. Experimental results show that the proposed multi-agent scaling strategy outperforms existing methods in resource utilization, SLA violation control, and scheduling latency. The results demonstrate strong adaptability and intelligent regulation. This provides an efficient and reliable new approach to solving the problem of elastic resource scaling in complex cloud platforms.</article>","contentLength":1635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm","url":"https://arxiv.org/abs/2507.00547","date":1751428800,"author":"","guid":180101,"unread":true,"content":"<article>arXiv:2507.00547v1 Announce Type: new \nAbstract: The rise of advanced computational algorithms has opened new avenues for computationally intensive research approaches to theory development. However, the opacity of these algorithms and lack of transparency and rigour in their application pose methodological challenges, potentially undermining trust in research. The discourse on methodological rigour in this new genre of research is still emerging. Against this backdrop, I attempt to offer guidance on methodological rigour, particularly in the context of topic modelling algorithms. By illustrating the application of the structural topic modelling algorithm and presenting a set of guidelines, I discuss how to ensure rigour in topic modelling studies. Although the guidelines are for the application of topic modelling algorithms, they can be applied to other algorithms with context-specific adjustments. The guidelines are helpful, especially for novice researchers applying topic modelling, and editors and reviewers handling topic modelling manuscripts. I contribute to the literature on topic modelling and join the emerging dialogue on methodological rigour in computationally intensive theory construction research.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reliable Annotations with Less Effort: Evaluating LLM-Human Collaboration in Search Clarifications","url":"https://arxiv.org/abs/2507.00543","date":1751428800,"author":"","guid":180102,"unread":true,"content":"<article>arXiv:2507.00543v1 Announce Type: new \nAbstract: Despite growing interest in using large language models (LLMs) to automate annotation, their effectiveness in complex, nuanced, and multi-dimensional labelling tasks remains relatively underexplored. This study focuses on annotation for the search clarification task, leveraging a high-quality, multi-dimensional dataset that includes five distinct fine-grained annotation subtasks. Although LLMs have shown impressive capabilities in general settings, our study reveals that even state-of-the-art models struggle to replicate human-level performance in subjective or fine-grained evaluation tasks. Through a systematic assessment, we demonstrate that LLM predictions are often inconsistent, poorly calibrated, and highly sensitive to prompt variations. To address these limitations, we propose a simple yet effective human-in-the-loop (HITL) workflow that uses confidence thresholds and inter-model disagreement to selectively involve human review. Our findings show that this lightweight intervention significantly improves annotation reliability while reducing human effort by up to 45%, offering a relatively scalable and cost-effective yet accurate path forward for deploying LLMs in real-world evaluation settings.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction","url":"https://arxiv.org/abs/2507.00540","date":1751428800,"author":"","guid":180103,"unread":true,"content":"<article>arXiv:2507.00540v1 Announce Type: new \nAbstract: This paper proposes a user semantic intent modeling algorithm based on Capsule Networks to address the problem of insufficient accuracy in intent recognition for human-computer interaction. The method represents semantic features in input text through a vectorized capsule structure. It uses a dynamic routing mechanism to transfer information across multiple capsule layers. This helps capture hierarchical relationships and part-whole structures between semantic entities more effectively. The model uses a convolutional feature extraction module as the low-level encoder. After generating initial semantic capsules, it forms high-level abstract intent representations through an iterative routing process. To further enhance performance, a margin-based mechanism is introduced into the loss function. This improves the model's ability to distinguish between intent classes. Experiments are conducted using a public natural language understanding dataset. Multiple mainstream models are used for comparison. Results show that the proposed model outperforms traditional methods and other deep learning structures in terms of accuracy, F1-score, and intent detection rate. The study also analyzes the effect of the number of dynamic routing iterations on model performance. A convergence curve of the loss function during training is provided. These results verify the stability and effectiveness of the proposed method in semantic modeling. Overall, this study presents a new structured modeling approach to improve intent recognition under complex semantic conditions.</article>","contentLength":1619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics","url":"https://arxiv.org/abs/2507.00539","date":1751428800,"author":"","guid":180104,"unread":true,"content":"<article>arXiv:2507.00539v1 Announce Type: new \nAbstract: This paper presents an innovative Reduced-Order Model (ROM) for merging experimental and simulation data using Data Assimilation (DA) to estimate the \"True\" state of a fluid dynamics system, leading to more accurate predictions. Our methodology introduces a novel approach implementing the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework, grounded in a robust theoretical foundation and applied to fluid dynamics. To address the substantial computational demands of DA, the proposed ROM employs low-resolution (LR) techniques to drastically reduce computational costs. This approach involves downsampling datasets for DA computations, followed by an advanced reconstruction technique based on low-cost Singular Value Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has never been applied to DA before and offers a highly efficient way to enhance resolution with minimal computational resources. Our results demonstrate significant reductions in both computation time and RAM usage through the LR techniques without compromising the accuracy of the estimations. For instance, in a turbulent test case, the LR approach with a compression rate of 15.9 can achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the high-resolution (HR) reference. Furthermore, we highlight the effectiveness of the EnKF in estimating and predicting the state of fluid flow systems based on limited observations and low-fidelity numerical data. This paper highlights the potential of the proposed DA method in fluid dynamics applications, particularly for improving computational efficiency in CFD and related fields. Its ability to balance accuracy with low computational and memory costs makes it suitable for large-scale and real-time applications, such as environmental monitoring or aerospace.</article>","contentLength":1968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation","url":"https://arxiv.org/abs/2507.00537","date":1751428800,"author":"","guid":180105,"unread":true,"content":"<article>arXiv:2507.00537v1 Announce Type: new \nAbstract: This paper studies the role of attention heads in CLIP's image encoder. While CLIP has exhibited robust performance across diverse applications, we hypothesize that certain attention heads negatively affect final representations and that ablating them can improve performance in downstream tasks. To capitalize on this insight, we propose a simple yet effective method, called Attention Ablation Technique (AAT), to suppress the contribution of specific heads by manipulating attention weights. By integrating two alternative strategies tailored for different application scenarios, AAT systematically identifies and ablates detrimental attention heads to enhance representation quality. Experiments demonstrate that AAT consistently improves downstream task performance across various domains, boosting recall rate by up to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight the potential of AAT to effectively refine large-scale vision-language models with virtually no increase in inference cost.</article>","contentLength":1074,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support","url":"https://arxiv.org/abs/2507.00535","date":1751428800,"author":"","guid":180106,"unread":true,"content":"<article>arXiv:2507.00535v1 Announce Type: new \nAbstract: More than twenty-five years ago, first ideas were developed on how to design a system that can provide recommendations to groups of users instead of individual users. Since then, a rich variety of algorithmic proposals were published, e.g., on how to acquire individual preferences, how to aggregate them, and how to generate recommendations for groups of users. However, despite the rich literature on the topic, barely any examples of real-world group recommender systems can be found. This lets us question common assumptions in academic research, in particular regarding communication processes in a group and how recommendation-supported decisions are made. In this essay, we argue that these common assumptions and corresponding system designs often may not match the needs or expectations of users. We thus call for a reorientation in this research area, leveraging the capabilities of modern Generative AI assistants like ChatGPT. Specifically, as one promising future direction, we envision group recommender systems to be systems where human group members interact in a chat and an AI-based group recommendation agent assists the decision-making process in an agentic way. Ultimately, this shall lead to a more natural group decision-making environment and finally to wider adoption of group recommendation systems in practice.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data","url":"https://arxiv.org/abs/2507.00534","date":1751428800,"author":"","guid":180107,"unread":true,"content":"<article>arXiv:2507.00534v1 Announce Type: new \nAbstract: We introduce Nirantar, a comprehensive framework for evaluating continual learning (CL) in multilingual and multi-domain ASR. Designed to reflect real-world CL challenges, Nirantar leverages data collected incrementally across 22 languages and 208 districts in India through natural episodes. This enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL), and the novel Language-Incremental Domain-Incremental Learning (LIDIL) scenarios. Unlike prior work that relies on simulated episodes, Nirantar presents dynamic, non-uniform language and domain shifts, making it an ideal testbed for CL research. With 3250 hours of human-transcribed speech, including 1720 hours newly introduced in this work, our framework enables systematic benchmarking of CL methods. We evaluate existing approaches and demonstrate that no single method performs consistently well, underscoring the need for more robust CL strategies.</article>","contentLength":981,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An inverse-free fixed-time stable dynamical system and its forward-Euler discretization for solving generalized absolute value equations","url":"https://arxiv.org/abs/2507.00531","date":1751428800,"author":"","guid":180108,"unread":true,"content":"<article>arXiv:2507.00531v1 Announce Type: new \nAbstract: An inverse-free dynamical system is proposed to solve the generalized absolute value equation (GAVE) within a fixed time, where the time of convergence is finite and is uniformly bounded for all initial points. Moreover, an iterative method obtained by using the forward-Euler discretization of the proposed dynamic model are developed and sufficient conditions which guarantee that the discrete iteration globally converge to an arbitrarily small neighborhood of the unique solution of GAVE within a finite number of iterative steps are given.</article>","contentLength":593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving","url":"https://arxiv.org/abs/2507.00525","date":1751428800,"author":"","guid":180109,"unread":true,"content":"<article>arXiv:2507.00525v1 Announce Type: new \nAbstract: Interpretable communication is essential for safe and trustworthy autonomous driving, yet current vision-language models (VLMs) often operate under idealized assumptions and struggle to capture user intent in real-world scenarios. Existing driving-oriented VQA datasets are limited to full-scene descriptions or waypoint prediction, preventing the assessment of whether VLMs can respond to localized user-driven queries. We introduce Box-QAymo, a box-referring dataset and benchmark designed to both evaluate and finetune VLMs on spatial and temporal reasoning over user-specified objects. Users express intent by drawing bounding boxes, offering a fast and intuitive interface for focused queries in complex scenes. Specifically, we propose a hierarchical evaluation protocol that begins with binary sanity-check questions to assess basic model capacities, and progresses to (1) attribute prediction for box-referred objects, (2) motion understanding of target instances, and (3) spatiotemporal motion reasoning over inter-object dynamics across frames. To support this, we crowd-sourced fine-grained object classes and visual attributes that reflect the complexity drivers encounter, and extract object trajectories to construct temporally grounded QA pairs. Rigorous quality control through negative sampling, temporal consistency checks, and difficulty-aware balancing guarantee dataset robustness and diversity. Our comprehensive evaluation reveals significant limitations in current VLMs when queried about perception questions, highlighting the gap in achieving real-world performance. This work provides a foundation for developing more robust and interpretable autonomous driving systems that can communicate effectively with users under real-world conditions. Project page and dataset are available at https://djamahl99.github.io/qaymo-pages/.</article>","contentLength":1902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Edge Computing and its Application in Robotics: A Survey","url":"https://arxiv.org/abs/2507.00523","date":1751428800,"author":"","guid":180110,"unread":true,"content":"<article>arXiv:2507.00523v1 Announce Type: new \nAbstract: The Edge computing paradigm has gained prominence in both academic and industry circles in recent years. By implementing edge computing facilities and services in robotics, it becomes a key enabler in the deployment of artificial intelligence applications to robots. Time-sensitive robotics applications benefit from the reduced latency, mobility, and location awareness provided by the edge computing paradigm, which enables real-time data processing and intelligence at the network's edge. While the advantages of integrating edge computing into robotics are numerous, there has been no recent survey that comprehensively examines these benefits. This paper aims to bridge that gap by highlighting important work in the domain of edge robotics, examining recent advancements, and offering deeper insight into the challenges and motivations behind both current and emerging solutions. In particular, this article provides a comprehensive evaluation of recent developments in edge robotics, with an emphasis on fundamental applications, providing in-depth analysis of the key motivations, challenges, and future directions in this rapidly evolving domain. It also explores the importance of edge computing in real-world robotics scenarios where rapid response times are critical. Finally, the paper outlines various open research challenges in the field of edge robotics.</article>","contentLength":1420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cyber Attacks Detection, Prevention, and Source Localization in Digital Substation Communication using Hybrid Statistical-Deep Learning","url":"https://arxiv.org/abs/2507.00522","date":1751428800,"author":"","guid":180111,"unread":true,"content":"<article>arXiv:2507.00522v1 Announce Type: new \nAbstract: The digital transformation of power systems is accelerating the adoption of IEC 61850 standard. However, its communication protocols, including Sampled Values (SV), lack built-in security features such as authentication and encryption, making them vulnerable to malicious packet injection. Such cyber attacks can delay fault clearance or trigger unintended circuit breaker operations. While most existing research focuses on detecting cyber attacks in digital substations, intrusion prevention systems have been disregarded because of the risk of potential communication network disruptions. This paper proposes a novel method using hybrid statistical-deep learning for the detection, prevention, and source localization of IEC 61850 SV injection attacks. The method uses exponentially modified Gaussian distributions to model communication network latency and long short-term memory and Elman recurrent neural network to detect anomalous variations in the estimated probability distributions. It effectively discards malicious SV frames with minimal processing overhead and latency, maintains robustness against communication network latency variation and time-synchronization issues, and guarantees a near-zero false positive rate in non-attack scenarios. Comprehensive validation is conducted on three testbeds involving industrial-grade devices, hardware-in-the-loop simulations, virtualized intelligent electronic devices and merging units, and high-fidelity emulated communication networks. Results demonstrate the method's suitability for practical deployment in IEC 61850-compliant digital substations.</article>","contentLength":1659,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\\texttt{WebANNS}: Fast and Efficient Approximate Nearest Neighbor Search in Web Browsers","url":"https://arxiv.org/abs/2507.00521","date":1751428800,"author":"","guid":180112,"unread":true,"content":"<article>arXiv:2507.00521v1 Announce Type: new \nAbstract: Approximate nearest neighbor search (ANNS) has become vital to modern AI infrastructure, particularly in retrieval-augmented generation (RAG) applications. Numerous in-browser ANNS engines have emerged to seamlessly integrate with popular LLM-based web applications, while addressing privacy protection and challenges of heterogeneous device deployments. However, web browsers present unique challenges for ANNS, including computational limitations, external storage access issues, and memory utilization constraints, which state-of-the-art (SOTA) solutions fail to address comprehensively.\n  We propose \\texttt{WebANNS}, a novel ANNS engine specifically designed for web browsers. \\texttt{WebANNS} leverages WebAssembly to overcome computational bottlenecks, designs a lazy loading strategy to optimize data retrieval from external storage, and applies a heuristic approach to reduce memory usage. Experiments show that \\texttt{WebANNS} is fast and memory efficient, achieving up to $743.8\\times$ improvement in 99th percentile query latency over the SOTA engine, while reducing memory usage by up to 39\\%. Note that \\texttt{WebANNS} decreases query time from 10 seconds to the 10-millisecond range in browsers, making in-browser ANNS practical with user-acceptable latency.</article>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection","url":"https://arxiv.org/abs/2507.00519","date":1751428800,"author":"","guid":180113,"unread":true,"content":"<article>arXiv:2507.00519v1 Announce Type: new \nAbstract: Liver landmarks provide crucial anatomical guidance to the surgeon during laparoscopic liver surgery to minimize surgical risk. However, the tubular structural properties of landmarks and dynamic intraoperative deformations pose significant challenges for automatic landmark detection. In this study, we introduce TopoNet, a novel topology-constrained learning framework for laparoscopic liver landmark detection. Our framework adopts a snake-CNN dual-path encoder to simultaneously capture detailed RGB texture information and depth-informed topological structures. Meanwhile, we propose a boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D features to enhance edge perception while preserving global topology. Additionally, a topological constraint loss function is embedded, which contains a center-line constraint loss and a topological persistence loss to ensure homotopy equivalence between predictions and labels. Extensive experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves outstanding accuracy and computational complexity, highlighting the potential for clinical applications in laparoscopic liver surgery. Our code will be available at https://github.com/cuiruize/TopoNet.</article>","contentLength":1277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling","url":"https://arxiv.org/abs/2507.00518","date":1751428800,"author":"","guid":180114,"unread":true,"content":"<article>arXiv:2507.00518v1 Announce Type: new \nAbstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable method for exploring large action sets in reinforcement learning problems where hyperspherical embedding vectors represent these actions. vMF-exp involves initially sampling a state embedding representation using a von Mises-Fisher distribution, then exploring this representation's nearest neighbors, which scales to virtually unlimited numbers of candidate actions. We show that, under theoretical assumptions, vMF-exp asymptotically maintains the same probability of exploring each action as Boltzmann Exploration (B-exp), a popular alternative that, nonetheless, suffers from scalability issues as it requires computing softmax values for each action. Consequently, vMF-exp serves as a scalable alternative to B-exp for exploring large action sets with hyperspherical embeddings. Experiments on simulated data, real-world public data, and the successful large-scale deployment of vMF-exp on the recommender system of a global music streaming service empirically validate the key properties of the proposed method.</article>","contentLength":1139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Fourier spectral approach to the spatial discretization of quasilinear hyperbolic systems","url":"https://arxiv.org/abs/2507.00516","date":1751428800,"author":"","guid":180115,"unread":true,"content":"<article>arXiv:2507.00516v1 Announce Type: new \nAbstract: We discuss the rigorous justification of the spatial discretization by means of Fourier spectral methods of quasilinear first-order hyperbolic systems. We provide uniform stability estimates that grant spectral convergence of the (spatially) semi-discretized solutions towards the corresponding continuous solution provided that the underlying system satisfies some suitable structural assumptions. We consider a setting with sharp low-pass filters and a setting with smooth low-pass filters and argue that - at least theoretically - smooth low-pass filters are operable on a larger class of systems. While our theoretical results are supported with numerical evidence, we also pinpoint some behavior of the numerical method that currently has no theoretical explanation.</article>","contentLength":820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center","url":"https://arxiv.org/abs/2507.00513","date":1751428800,"author":"","guid":180116,"unread":true,"content":"<article>arXiv:2507.00513v1 Announce Type: new \nAbstract: The integration of various AI tools creates a complex socio-technical environment where employee-customer interactions form the core of work practices. This study investigates how customer service representatives (CSRs) at the power grid service customer service call center perceive AI assistance in their interactions with customers. Through a field visit and semi-structured interviews with 13 CSRs, we found that AI can alleviate some traditional burdens during the call (e.g., typing and memorizing) but also introduces new burdens (e.g., earning, compliance, psychological burdens). This research contributes to a more nuanced understanding of AI integration in organizational settings and highlights the efforts and burdens undertaken by CSRs to adapt to the updated system.</article>","contentLength":830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TeamCMU at Touch\\'e: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search","url":"https://arxiv.org/abs/2507.00509","date":1751428800,"author":"","guid":180117,"unread":true,"content":"<article>arXiv:2507.00509v1 Announce Type: new \nAbstract: As conversational search engines increasingly adopt generation-based paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), the integration of advertisements into generated responses presents both commercial opportunities and challenges for user experience. Unlike traditional search, where advertisements are clearly delineated, generative systems blur the boundary between informational content and promotional material, raising concerns around transparency and trust. In this work, we propose a modular pipeline for advertisement management in RAG-based conversational systems, consisting of an ad-rewriter for seamless ad integration and a robust ad-classifier for detection. We leverage synthetic data to train high-performing classifiers, which are then used to guide two complementary ad-integration strategies: supervised fine-tuning of the ad-rewriter and a best-of-N sampling approach that selects the least detectable ad-integrated response among multiple candidates. Our evaluation focuses on two core questions: the effectiveness of ad classifiers in detecting diverse ad integration strategies, and the training methods that best support coherent, minimally intrusive ad insertion. Experimental results show that our ad-classifier, trained on synthetic advertisement data inspired by marketing strategies and enhanced through curriculum learning, achieves robust detection performance. Additionally, we demonstrate that classifier-guided optimization, through both fine-tuning and best-of-N sampling, significantly improves ad stealth, enabling more seamless integration. These findings contribute an adversarial co-evolution framework for developing more sophisticated ad-aware generative search systems and robust ad classifiers.</article>","contentLength":1831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Mesh: Enabling Elastic Sharing for Serverless LLM Inference","url":"https://arxiv.org/abs/2507.00507","date":1751428800,"author":"","guid":180118,"unread":true,"content":"<article>arXiv:2507.00507v1 Announce Type: new \nAbstract: The rise of LLMs has driven demand for private serverless deployments, characterized by moderate-scale models and infrequent requests. While existing solutions follow exclusive GPU deployment, we take a step back to explore modern platforms and find that: Emerging CPU architectures with built-in accelerators are capable of serving LLMs but remain underutilized, and both CPUs and GPUs can accommodate multiple LLMs simultaneously.\n  We propose LLM-Mesh, a serverless inference scheme for small-to-mid-sized LLMs that enables elastic sharing across heterogeneous hardware. LLM-Mesh tackles three fundamental challenges: (1) precise, fine-grained compute resource allocation at token-level to handle fluctuating computational demands; (2) a coordinated and forward-looking memory scaling mechanism to detect out-of-memory hazards and reduce operational overhead; and (3) a dual approach that reduces resource fragmentation through proactive preemption and reactive bin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that LLM-Meshimproves service capacity by 44% - 63% through sharing, while further leveraging CPUs boosts this to 91% - 159%.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning","url":"https://arxiv.org/abs/2507.00506","date":1751428800,"author":"","guid":180119,"unread":true,"content":"<article>arXiv:2507.00506v1 Announce Type: new \nAbstract: Recent advancements in adapting vision-language pre-training models like CLIP for person re-identification (ReID) tasks often rely on complex adapter design or modality-specific tuning while neglecting cross-modal interaction, leading to high computational costs or suboptimal alignment. To address these limitations, we propose a simple yet effective framework named Selective Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and robustness against real-world perturbations. Our method introduces two key innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a lightweight module that dynamically injects discriminative visual features into text prompts via a cross-modal gating mechanism. Moreover, the proposed Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training strategy that enforces invariant feature alignment under random image perturbations by regularizing consistency between original and augmented cross-modal embeddings. Extensive experiments are conducted on several popular benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID, and P-DukeMTMC, which demonstrate the impressive performance of the proposed method. Notably, our framework eliminates heavy adapters while maintaining efficient inference, achieving an optimal trade-off between performance and computational overhead. The code will be released upon acceptance.</article>","contentLength":1469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs","url":"https://arxiv.org/abs/2507.00505","date":1751428800,"author":"","guid":180120,"unread":true,"content":"<article>arXiv:2507.00505v1 Announce Type: new \nAbstract: The architecture of multimodal large language models (MLLMs) commonly connects a vision encoder, often based on CLIP-ViT, to a large language model. While CLIP-ViT works well for capturing global image features, it struggles to model local relationships between adjacent patches, leading to weaker visual representation, which in turn affects the detailed understanding ability of MLLMs. To solve this, we propose LLaVA-SP, which \\textbf{ only adds six spatial visual tokens} to the original visual tokens to enhance the visual representation. Our approach offers three key advantages: 1)We propose a novel Projector, which uses convolutional kernels to derive visual spatial tokens from ViT patch features, simulating two visual spatial ordering approaches: ``from central region to global\" and ``from abstract to specific\". Then, a cross-attention mechanism is applied to fuse fine-grained visual information, enriching the overall visual representation. 2) We present two model variants: LLaVA-SP-Cropping, which focuses on detail features through progressive cropping, and LLaVA-SP-Pooling, which captures global semantics through adaptive pooling, enabling the model to handle diverse visual understanding tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA, achieves significant performance improvements across various multimodal benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple tasks with nearly identical inference latency. The code and models are available at \\href{https://github.com/CnFaker/LLaVA-SP}{\\texttt{https://github.com/CnFaker/LLaVA-SP}}.</article>","contentLength":1654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation","url":"https://arxiv.org/abs/2507.00502","date":1751428800,"author":"","guid":180121,"unread":true,"content":"<article>arXiv:2507.00502v1 Announce Type: new \nAbstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt on-the-fly to a stream of unlabeled data under evolving distribution shifts. However, existing CTTA methods typically rely on shared model parameters across all domains, making them vulnerable to feature entanglement and catastrophic forgetting in the presence of large or non-stationary domain shifts. To address this limitation, we propose \\textbf{ExPaMoE}, a novel framework based on an \\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples domain-general and domain-specific knowledge via a dual-branch expert design with token-guided feature separation, and dynamically expands its expert pool based on a \\emph{Spectral-Aware Online Domain Discriminator} (SODD) that detects distribution changes in real-time using frequency-domain cues. Extensive experiments demonstrate the superiority of ExPaMoE across diverse CTTA scenarios. We evaluate our method on standard benchmarks including CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic segmentation. Additionally, we introduce \\textbf{ImageNet++}, a large-scale and realistic CTTA benchmark built from multiple ImageNet-derived datasets, to better reflect long-term adaptation under complex domain evolution. ExPaMoE consistently outperforms prior arts, showing strong robustness, scalability, and resistance to forgetting.</article>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing","url":"https://arxiv.org/abs/2507.00501","date":1751428800,"author":"","guid":180122,"unread":true,"content":"<article>arXiv:2507.00501v1 Announce Type: new \nAbstract: Recent progress in image restoration has underscored Spatial State Models (SSMs) as powerful tools for modeling long-range dependencies, owing to their appealing linear complexity and computational efficiency. However, SSM-based approaches exhibit limitations in reconstructing localized structures and tend to be less effective when handling high-dimensional data, frequently resulting in suboptimal recovery of fine image features. To tackle these challenges, we introduce Laplace-Mamba, a novel framework that integrates Laplace frequency prior with a hybrid Mamba-CNN architecture for efficient image dehazing. Leveraging the Laplace decomposition, the image is disentangled into low-frequency components capturing global texture and high-frequency components representing edges and fine details. This decomposition enables specialized processing via dual parallel pathways: the low-frequency branch employs SSMs for global context modeling, while the high-frequency branch utilizes CNNs to refine local structural details, effectively addressing diverse haze scenarios. Notably, the Laplace transformation facilitates information-preserving downsampling of low-frequency components in accordance with the Nyquist theory, thereby significantly improving computational efficiency. Extensive evaluations across multiple benchmarks demonstrate that our method outperforms state-of-the-art approaches in both restoration quality and efficiency. The source code and pretrained models are available at https://github.com/yz-wang/Laplace-Mamba.</article>","contentLength":1590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MuteSwap: Silent Face-based Voice Conversion","url":"https://arxiv.org/abs/2507.00498","date":1751428800,"author":"","guid":180123,"unread":true,"content":"<article>arXiv:2507.00498v1 Announce Type: new \nAbstract: Conventional voice conversion modifies voice characteristics from a source speaker to a target speaker, relying on audio input from both sides. However, this process becomes infeasible when clean audio is unavailable, such as in silent videos or noisy environments. In this work, we focus on the task of Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely from visual inputs. i.e., given images of a target speaker and a silent video of a source speaker containing lip motion, SFVC generates speech aligning the identity of the target speaker while preserving the speech content in the source silent video. As this task requires generating intelligible speech and converting identity using only visual cues, it is particularly challenging. To address this, we introduce MuteSwap, a novel framework that employs contrastive learning to align cross-modality identities and minimize mutual information to separate shared visual features. Experimental results show that MuteSwap achieves impressive performance in both speech synthesis and identity conversion, especially under noisy conditions where methods dependent on audio input fail to produce intelligible results, demonstrating both the effectiveness of our training approach and the feasibility of SFVC.</article>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coverage-Guided Testing for Deep Learning Models: A Comprehensive Survey","url":"https://arxiv.org/abs/2507.00496","date":1751428800,"author":"","guid":180124,"unread":true,"content":"<article>arXiv:2507.00496v1 Announce Type: new \nAbstract: As Deep Learning (DL) models are increasingly applied in safety-critical domains, ensuring their quality has emerged as a pressing challenge in modern software engineering. Among emerging validation paradigms, coverage-guided testing (CGT) has gained prominence as a systematic framework for identifying erroneous or unexpected model behaviors. Despite growing research attention, existing CGT studies remain methodologically fragmented, limiting the understanding of current advances and emerging trends. This work addresses that gap through a comprehensive review of state-of-the-art CGT methods for DL models, including test coverage analysis, coverage-guided test input generation, and coverage-guided test input optimization. This work provides detailed taxonomies to organize these methods based on methodological characteristics and application scenarios. We also investigate evaluation practices adopted in existing studies, including the use of benchmark datasets, model architectures, and evaluation aspects. Finally, open challenges and future directions are highlighted in terms of the correlation between structural coverage and testing objectives, method generalizability across tasks and models, practical deployment concerns, and the need for standardized evaluation and tool support. This work aims to provide a roadmap for future academic research and engineering practice in DL model quality assurance.</article>","contentLength":1470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models","url":"https://arxiv.org/abs/2507.00493","date":1751428800,"author":"","guid":180125,"unread":true,"content":"<article>arXiv:2507.00493v1 Announce Type: new \nAbstract: Humans are able to recognize objects based on both local texture cues and the configuration of object parts, yet contemporary vision models primarily harvest local texture cues, yielding brittle, non-compositional features. Work on shape-vs-texture bias has pitted shape and texture representations in opposition, measuring shape relative to texture, ignoring the possibility that models (and humans) can simultaneously rely on both types of cues, and obscuring the absolute quality of both types of representation. We therefore recast shape evaluation as a matter of absolute configural competence, operationalized by the Configural Shape Score (CSS), which (i) measures the ability to recognize both images in Object-Anagram pairs that preserve local texture while permuting global part arrangement to depict different object categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii) uncovers a broad spectrum of configural sensitivity with fully self-supervised and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes reveal that (iii) high-CSS networks depend on long-range interactions: radius-controlled attention masks abolish performance showing a distinctive U-shaped integration profile, and representational-similarity analyses expose a mid-depth transition from local to global coding. A BagNet control remains at chance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that configural shape score also predicts other shape-dependent evals. Overall, we propose that the path toward truly robust, generalizable, and human-like vision systems may not lie in forcing an artificial choice between shape and texture, but rather in architectural and learning frameworks that seamlessly integrate both local-texture and global configural shape.</article>","contentLength":1926,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms","url":"https://arxiv.org/abs/2507.00491","date":1751428800,"author":"","guid":180126,"unread":true,"content":"<article>arXiv:2507.00491v1 Announce Type: new \nAbstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems. cAI systems are typically composed of deep neural networks (DNNs), transformers, and large language models (LLMs), exhibiting a high degree of computational diversity and dynamic workload variation. Deploying cAI services on mobile edge platforms poses a significant challenge in scheduling concurrent DNN-transformer inference tasks, which arrive dynamically in an unknown sequence. Existing mobile edge AI inference strategies manage multi-DNN or transformer-only workloads, relying on design-time profiling, and cannot handle concurrent inference of DNNs and transformers required by cAI systems. In this work, we address the challenge of scheduling cAI systems on heterogeneous mobile edge platforms. We present Twill, a run-time framework to handle concurrent inference requests of cAI workloads through task affinity-aware cluster mapping and migration, priority-aware task freezing/unfreezing, and DVFS, while minimizing inference latency within power budgets. We implement and deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate Twill against state-of-the-art edge AI inference techniques over contemporary DNNs and LLMs, reducing inference latency by 54% on average, while honoring power budgets.</article>","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Just Noticeable Difference for Large Multimodal Models","url":"https://arxiv.org/abs/2507.00490","date":1751428800,"author":"","guid":180127,"unread":true,"content":"<article>arXiv:2507.00490v1 Announce Type: new \nAbstract: Just noticeable difference (JND), the minimum change that the human visual system (HVS) can perceive, has been studied for decades. Although recent work has extended this line of research into machine vision, there has been a scarcity of studies systematically exploring its perceptual boundaries across multiple tasks and stimulus types, particularly in the current era of rapidly advancing large multimodal models (LMMs), where studying the multifaceted capabilities of models has become a mainstream focus. Moreover, the perceptual defects of LMMs are not investigated thoroughly, resulting in potential security issues and suboptimal response efficiency. In this paper, we take an initial attempt and demonstrate that there exist significant visual blind spots in current LMMs. To systemically quantify this characteristic, we propose a new concept, {\\bf LMM-JND}, together with its determination pipeline. Targeting uncovering the behavior commonalities in HVS-aligned visual perception tasks, we delve into several LMM families and construct a large-scale dataset, named VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12 distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle with basic comparison queries and fall significantly short of human-level visual performance. We further explore the effects of vision and language backbones and find a notable correlation between their design philosophy that may instruct the future refinement of LMMs for their visual acuity. Together, our research underscores the significance of LMM-JND as a unique perspective for studying LMMs, and predictable LMM-JND is crucial for security concerns. This work will be available at https://github.com/zijianchen98/LMM-JND.</article>","contentLength":1889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Efficient Random-Order Enumeration for Join Queries","url":"https://arxiv.org/abs/2507.00489","date":1751428800,"author":"","guid":180128,"unread":true,"content":"<article>arXiv:2507.00489v1 Announce Type: new \nAbstract: In many data analysis pipelines, a basic and time-consuming process is to produce join results and feed them into downstream tasks. Numerous enumeration algorithms have been developed for this purpose. To be a statistically meaningful representation of the whole join result, the result tuples are required to be enumerated in uniformly random order. However, existing studies lack an efficient random-order enumeration algorithm with a worst-case runtime guarantee for (cyclic) join queries. In this paper, we study the problem of enumerating the results of a join query in random order. We develop an efficient random-order enumeration algorithm for join queries with no large hidden constants in its complexity, achieving expected $O(\\frac{\\mathrm{AGM}(Q)}{|Res(Q)|}\\log^2|Q|)$ delay, $O(\\mathrm{AGM}(Q)\\log|Q|)$ total running time after $O(|Q|\\log|Q|)$-time index construction, where $|Q|$ is the size of input, $\\mathrm{AGM}(Q)$ is the AGM bound, and $|Res(Q)|$ is the size of the join result. We prove that our algorithm is near-optimal in the worst case, under the combinatorial $k$-clique hypothesis. Our algorithm requires no query-specific preprocessing and can be flexibly adapted to many common database indexes with only minor modifications. We also devise two non-trivial techniques to speed up the enumeration, and provide an experimental study on our enumeration algorithm along with the speed-up techniques. The experimental results show that our algorithm, enhanced with the proposed techniques, significantly outperforms existing state-of-the-art methods.</article>","contentLength":1623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Have Object-Oriented Languages Missed a Trick with Class Function and its Subclasses?","url":"https://arxiv.org/abs/2507.00488","date":1751428800,"author":"","guid":180129,"unread":true,"content":"<article>arXiv:2507.00488v1 Announce Type: new \nAbstract: Compared to functions in mathematics, functions in programming languages seem to be under classified. Functional programming languages based on the lambda calculus famously treat functions as first-class values. Object-oriented languages have adopted ``lambdas'', notably for call-back routines in event-based programming. Typically a programming language has functions, a function has a type, and some functions act on other functions and/or return functions but there is generally a lack of (i) ``class Function'' in the OO sense of the word class and particularly (ii) subclasses of Function for functions having specific properties. Some such classes are presented here and programmed in some popular programming languages as an experimental investigation into OO languages missing this opportunity.</article>","contentLength":852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models","url":"https://arxiv.org/abs/2507.00487","date":1751428800,"author":"","guid":180130,"unread":true,"content":"<article>arXiv:2507.00487v1 Announce Type: new \nAbstract: Tool retrieval is a critical component in enabling large language models (LLMs) to interact effectively with external tools. It aims to precisely filter the massive tools into a small set of candidates for the downstream tool-augmented LLMs. However, most existing approaches primarily focus on optimizing tool representations, often neglecting the importance of precise query comprehension. To address this gap, we introduce MassTool, a multi-task search-based framework designed to enhance both query representation and tool retrieval accuracy. MassTool employs a two-tower architecture: a tool usage detection tower that predicts the need for function calls, and a tool retrieval tower that leverages a query-centric graph convolution network (QC-GCN) for effective query-tool matching. It also incorporates search-based user intent modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an adaptive knowledge transfer (AdaKT) module for efficient multi-task learning. By jointly optimizing tool usage detection loss, list-wise retrieval loss, and contrastive regularization loss, MassTool establishes a robust dual-step sequential decision-making pipeline for precise query understanding. Extensive experiments demonstrate its effectiveness in improving retrieval accuracy. Our code is available at https://github.com/wxydada/MassTool.</article>","contentLength":1410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning","url":"https://arxiv.org/abs/2507.00485","date":1751428800,"author":"","guid":180131,"unread":true,"content":"<article>arXiv:2507.00485v1 Announce Type: new \nAbstract: Reinforcement Learning (RL) is widely used in tasks where agents interact with an environment to maximize rewards. Building on this foundation, Safe Reinforcement Learning (Safe RL) incorporates a cost metric alongside the reward metric, ensuring that agents adhere to safety constraints during decision-making. In this paper, we identify that Safe RL is vulnerable to backdoor attacks, which can manipulate agents into performing unsafe actions. First, we introduce the relevant concepts and evaluation metrics for backdoor attacks in Safe RL. It is the first attack framework in the Safe RL field that involves both Positive and Negative Action sample (PNAct) is to implant backdoors, where positive action samples provide reference actions and negative action samples indicate actions to be avoided. We theoretically point out the properties of PNAct and design an attack algorithm. Finally, we conduct experiments to evaluate the effectiveness of our proposed backdoor attack framework, evaluating it with the established metrics. This paper highlights the potential risks associated with Safe RL and underscores the feasibility of such attacks. Our code and supplementary material are available at https://github.com/azure-123/PNAct.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Influence of HEXACO Personality Traits on the Teamwork Quality in Software Teams -- A Preliminary Research Approach","url":"https://arxiv.org/abs/2507.00481","date":1751428800,"author":"","guid":180132,"unread":true,"content":"<article>arXiv:2507.00481v1 Announce Type: new \nAbstract: Although software engineering research has focused on optimizing processes and technology, there is a growing recognition that human factors, particularly teamwork, also significantly impact optimization. Recent research suggests that developer personality has a strong influence on teamwork. In fact, personality considerations may have a greater impact on software development than processes and tools. This paper aims to design a study that measures the impact of HEXACO personality traits on the Teamwork Quality (TWQ) of software teams. A preliminary data collection (n=54) was conducted for this purpose. The analysis showed that several personality traits, as well as their composition, had a significant impact on TWQ. Additionally, other variables, such as the proportion of women and age distribution, also affected TWQ. The study's initial results demonstrate the usefulness and validity of the study design. The results also suggest several opportunities to improve teamwork in IT organizations and avenues for further research.</article>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization","url":"https://arxiv.org/abs/2507.00480","date":1751428800,"author":"","guid":180133,"unread":true,"content":"<article>arXiv:2507.00480v1 Announce Type: new \nAbstract: Optimizing high-dimensional black-box functions under black-box constraints is a pervasive task in a wide range of scientific and engineering problems. These problems are typically harder than unconstrained problems due to hard-to-find feasible regions. While Bayesian optimization (BO) methods have been developed to solve such problems, they often struggle with the curse of dimensionality. Recently, generative model-based approaches have emerged as a promising alternative for constrained optimization. However, they suffer from poor scalability and are vulnerable to mode collapse, particularly when the target distribution is highly multi-modal. In this paper, we propose a new framework to overcome these challenges. Our method iterates through two stages. First, we train flow-based models to capture the data distribution and surrogate models that predict both function values and constraint violations with uncertainty quantification. Second, we cast the candidate selection problem as a posterior inference problem to effectively search for promising candidates that have high objective values while not violating the constraints. During posterior inference, we find that the posterior distribution is highly multi-modal and has a large plateau due to constraints, especially when constraint feedback is given as binary indicators of feasibility. To mitigate this issue, we amortize the sampling from the posterior distribution in the latent space of flow-based models, which is much smoother than that in the data space. We empirically demonstrate that our method achieves superior performance on various synthetic and real-world constrained black-box optimization tasks. Our code is publicly available \\href{https://github.com/umkiyoung/CiBO}{here}.</article>","contentLength":1811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Mitigating Data Sparsity in Conversational Recommender Systems","url":"https://arxiv.org/abs/2507.00479","date":1751428800,"author":"","guid":180134,"unread":true,"content":"<article>arXiv:2507.00479v1 Announce Type: new \nAbstract: Conversational recommender systems (CRSs) capture user preference through textual information in dialogues. However, they suffer from data sparsity on two fronts: the dialogue space is vast and linguistically diverse, while the item space exhibits long-tail and sparse distributions. Existing methods struggle with (1) generalizing to varied dialogue expressions due to underutilization of rich textual cues, and (2) learning informative item representations under severe sparsity. To address these problems, we propose a CRS model named DACRS. It consists of three modules, namely Dialogue Augmentation, Knowledge-Guided Entity Modeling, and Dialogue-Entity Matching. In the Dialogue Augmentation module, we apply a two-stage augmentation pipeline to augment the dialogue context to enrich the data and improve generalizability. In the Knowledge-Guided Entity Modeling, we propose a knowledge graph (KG) based entity substitution and an entity similarity constraint to enhance the expressiveness of entity embeddings. In the Dialogue-Entity Matching module, we fuse the dialogue embedding with the mentioned entity embeddings through a dialogue-guided attention aggregation to acquire user embeddings that contain both the explicit and implicit user preferences. Extensive experiments on two public datasets demonstrate the state-of-the-art performance of DACRS.</article>","contentLength":1412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Read the Docs Before Rewriting: Equip Rewriter with Domain Knowledge via Continual Pre-training","url":"https://arxiv.org/abs/2507.00477","date":1751428800,"author":"","guid":180135,"unread":true,"content":"<article>arXiv:2507.00477v1 Announce Type: new \nAbstract: A Retrieval-Augmented Generation (RAG)-based question-answering (QA) system enhances a large language model's knowledge by retrieving relevant documents based on user queries. Discrepancies between user queries and document phrasings often necessitate query rewriting. However, in specialized domains, the rewriter model may struggle due to limited domain-specific knowledge. To resolve this, we propose the R\\&amp;R (Read the doc before Rewriting) rewriter, which involves continual pre-training on professional documents, akin to how students prepare for open-book exams by reviewing textbooks. Additionally, it can be combined with supervised fine-tuning for improved results. Experiments on multiple datasets demonstrate that R\\&amp;R excels in professional QA across multiple domains, effectively bridging the query-document gap, while maintaining good performance in general scenarios, thus advancing the application of RAG-based QA systems in specialized fields.</article>","contentLength":1010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreNBRDF: A Frequency-Rectified Neural Material Representation","url":"https://arxiv.org/abs/2507.00476","date":1751428800,"author":"","guid":180136,"unread":true,"content":"<article>arXiv:2507.00476v1 Announce Type: new \nAbstract: Accurate material modeling is crucial for achieving photorealistic rendering, bridging the gap between computer-generated imagery and real-world photographs. While traditional approaches rely on tabulated BRDF data, recent work has shifted towards implicit neural representations, which offer compact and flexible frameworks for a range of tasks. However, their behavior in the frequency domain remains poorly understood. To address this, we introduce FreNBRDF, a frequency-rectified neural material representation. By leveraging spherical harmonics, we integrate frequency-domain considerations into neural BRDF modeling. We propose a novel frequency-rectified loss, derived from a frequency analysis of neural materials, and incorporate it into a generalizable and adaptive reconstruction and editing pipeline. This framework enhances fidelity, adaptability, and efficiency. Extensive experiments demonstrate that \\ours improves the accuracy and robustness of material appearance reconstruction and editing compared to state-of-the-art baselines, enabling more structured and interpretable downstream tasks and applications.</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio embedding Sequences","url":"https://arxiv.org/abs/2507.00475","date":1751428800,"author":"","guid":180137,"unread":true,"content":"<article>arXiv:2507.00475v1 Announce Type: new \nAbstract: We propose a novel objective evaluation metric for synthesized audio in text-to-audio (TTA), aiming to improve the performance of TTA models. In TTA, subjective evaluation of the synthesized sound is an important, but its implementation requires monetary costs. Therefore, objective evaluation such as mel-cepstral distortion are used, but the correlation between these objective metrics and subjective evaluation values is weak. Our proposed objective evaluation metric, AudioBERTScore, calculates the similarity between embedding of the synthesized and reference sounds. The method is based not only on the max-norm used in conventional BERTScore but also on the $p$-norm to reflect the non-local nature of environmental sounds. Experimental results show that scores obtained by the proposed method have a higher correlation with subjective evaluation values than conventional metrics.</article>","contentLength":936,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis","url":"https://arxiv.org/abs/2507.00474","date":1751428800,"author":"","guid":180138,"unread":true,"content":"<article>arXiv:2507.00474v1 Announce Type: new \nAbstract: Deep learning-based diagnostic models often suffer performance drops due to distribution shifts between training (source) and test (target) domains. Collecting and labeling sufficient target domain data for model retraining represents an optimal solution, yet is limited by time and scarce resources. Active learning (AL) offers an efficient approach to reduce annotation costs while maintaining performance, but struggles to handle the challenge posed by distribution variations across different datasets. In this study, we propose a novel unsupervised Active learning framework for Domain Adaptation, named ADAptation, which efficiently selects informative samples from multi-domain data pools under limited annotation budget. As a fundamental step, our method first utilizes the distribution homogenization capabilities of diffusion models to bridge cross-dataset gaps by translating target images into source-domain style. We then introduce two key innovations: (a) a hypersphere-constrained contrastive learning network for compact feature clustering, and (b) a dual-scoring mechanism that quantifies and balances sample uncertainty and representativeness. Extensive experiments on four breast ultrasound datasets (three public and one in-house/multi-center) across five common deep classifiers demonstrate that our method surpasses existing strong AL-based competitors, validating its effectiveness and generalization for clinical domain adaptation. The code is available at the anonymized link: https://github.com/miccai25-966/ADAptation.</article>","contentLength":1594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ARIG: Autoregressive Interactive Head Generation for Real-time Conversations","url":"https://arxiv.org/abs/2507.00472","date":1751428800,"author":"","guid":180139,"unread":true,"content":"<article>arXiv:2507.00472v1 Announce Type: new \nAbstract: Face-to-face communication, as a common human activity, motivates the research on interactive head generation. A virtual agent can generate motion responses with both listening and speaking capabilities based on the audio or motion signals of the other user and itself. However, previous clip-wise generation paradigm or explicit listener/speaker generator-switching methods have limitations in future signal acquisition, contextual behavioral understanding, and switching smoothness, making it challenging to be real-time and realistic. In this paper, we propose an autoregressive (AR) based frame-wise framework called ARIG to realize the real-time generation with better interaction realism. To achieve real-time generation, we model motion prediction as a non-vector-quantized AR process. Unlike discrete codebook-index prediction, we represent motion distribution using diffusion procedure, achieving more accurate predictions in continuous space. To improve interaction realism, we emphasize interactive behavior understanding (IBU) and detailed conversational state understanding (CSU). In IBU, based on dual-track dual-modal signals, we summarize short-range behaviors through bidirectional-integrated learning and perform contextual understanding over long ranges. In CSU, we use voice activity signals and context features of IBU to understand the various states (interruption, feedback, pause, etc.) that exist in actual conversations. These serve as conditions for the final progressive motion prediction. Extensive experiments have verified the effectiveness of our model.</article>","contentLength":1634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bisecle: Binding and Separation in Continual Learning for Video Language Understanding","url":"https://arxiv.org/abs/2507.00469","date":1751428800,"author":"","guid":180140,"unread":true,"content":"<article>arXiv:2507.00469v1 Announce Type: new \nAbstract: Frontier vision-language models (VLMs) have made remarkable improvements in video understanding tasks. However, real-world videos typically exist as continuously evolving data streams (e.g., dynamic scenes captured by wearable glasses), necessitating models to continually adapt to shifting data distributions and novel scenarios. Considering the prohibitive computational costs of fine-tuning models on new tasks, usually, a small subset of parameters is updated while the bulk of the model remains frozen. This poses new challenges to existing continual learning frameworks in the context of large multimodal foundation models, i.e., catastrophic forgetting and update conflict. While the foundation models struggle with parameter-efficient continual learning, the hippocampus in the human brain has evolved highly efficient mechanisms for memory formation and consolidation. Inspired by the rapid Binding and pattern separation mechanisms in the hippocampus, in this work, we propose Bisecle for video-language continual learning, where a multi-directional supervision module is used to capture more cross-modal relationships and a contrastive prompt learning scheme is designed to isolate task-specific knowledge to facilitate efficient memory storage. Binding and separation processes further strengthen the ability of VLMs to retain complex experiences, enabling robust and efficient continual learning in video understanding tasks. We perform a thorough evaluation of the proposed Bisecle, demonstrating its ability to mitigate forgetting and enhance cross-task generalization on several VideoQA benchmarks.</article>","contentLength":1663,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diversity Conscious Refined Random Forest","url":"https://arxiv.org/abs/2507.00467","date":1751428800,"author":"","guid":180141,"unread":true,"content":"<article>arXiv:2507.00467v1 Announce Type: new \nAbstract: Random Forest (RF) is a widely used ensemble learning technique known for its robust classification performance across diverse domains. However, it often relies on hundreds of trees and all input features, leading to high inference cost and model redundancy. In this work, our goal is to grow trees dynamically only on informative features and then enforce maximal diversity by clustering and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest Classifier that iteratively refines itself by first removing the least informative features and then analytically determines how many new trees should be grown, followed by correlation-based clustering to remove redundant trees. The classification accuracy of our model was compared against the standard RF on the same number of trees. Experiments on 8 multiple benchmark datasets, including binary and multiclass datasets, demonstrate that the proposed model achieves improved accuracy compared to standard RF.</article>","contentLength":1029,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture","url":"https://arxiv.org/abs/2507.00466","date":1751428800,"author":"","guid":180142,"unread":true,"content":"<article>arXiv:2507.00466v1 Announce Type: new \nAbstract: Beat tracking in musical performance MIDI is a challenging and important task for notation-level music transcription and rhythmical analysis, yet existing methods primarily focus on audio-based approaches. This paper proposes an end-to-end transformer-based model for beat and downbeat tracking in performance MIDI, leveraging an encoder-decoder architecture for sequence-to-sequence translation of MIDI input to beat annotations. Our approach introduces novel data preprocessing techniques, including dynamic augmentation and optimized tokenization strategies, to improve accuracy and generalizability across different datasets. We conduct extensive experiments using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model against state-of-the-art hidden Markov models (HMMs) and deep learning-based beat tracking methods. The results demonstrate that our model outperforms existing symbolic music beat tracking approaches, achieving competitive F1-scores across various musical styles and instruments. Our findings highlight the potential of transformer architectures for symbolic beat tracking and suggest future integration with automatic music transcription systems for enhanced music analysis and score generation.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Encoding Peano Arithmetic in a Minimal Fragment of Separation Logic","url":"https://arxiv.org/abs/2507.00465","date":1751428800,"author":"","guid":180143,"unread":true,"content":"<article>arXiv:2507.00465v1 Announce Type: new \nAbstract: This paper investigates the expressive power of a minimal fragment of separation logic extended with natural numbers. Specifically, it demonstrates that the fragment consisting solely of the intuitionistic points-to predicate, the constant 0, and the successor function is sufficient to encode all $\\Pi^0_1$ formulas of Peano Arithmetic (PA). The authors construct a translation from PA into this fragment, showing that a $\\Pi^0_1$ formula is valid in the standard model of arithmetic if and only if its translation is valid in the standard interpretation of the separation logic fragment. This result implies the undecidability of validity in the fragment, despite its syntactic simplicity. The translation leverages a heap-based encoding of arithmetic operations - addition, multiplication, and inequality - using structured memory cells. The paper also explores the boundaries of this encoding, showing that the translation does not preserve validity for $\\Sigma^0_1$ formulas. Additionally, an alternative undecidability proof is presented via a reduction from finite model theory. Finally, the paper establishes that the validity problem for this fragment is $\\Pi^0_1$-complete, highlighting its theoretical significance in the landscape of logic and program verification.</article>","contentLength":1326,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Miniature High-Resolution Tension Sensor Based on a Photo-Reflector for Robotic Hands and Grippers","url":"https://arxiv.org/abs/2507.00464","date":1751428800,"author":"","guid":180144,"unread":true,"content":"<article>arXiv:2507.00464v1 Announce Type: new \nAbstract: This paper presents a miniature tension sensor using a photo-reflector, designed for compact tendon-driven grippers and robotic hands. The proposed sensor has a small form factor of 13~mm x 7~mm x 6.5~mm and is capable of measuring tensile forces up to 200~N. A symmetric elastomer structure incorporating fillets and flexure hinges is designed based on Timoshenko beam theory and verified via FEM analysis, enabling improved sensitivity and mechanical durability while minimizing torsional deformation. The sensor utilizes a compact photo-reflector (VCNT2020) to measure displacement in the near-field region, eliminating the need for light-absorbing materials or geometric modifications required in photo-interrupter-based designs. A 16-bit analog-to-digital converter (ADC) and CAN-FD (Flexible Data-rate) communication enable efficient signal acquisition with up to 5~kHz sampling rate. Calibration experiments demonstrate a resolution of 9.9~mN (corresponding to over 14-bit accuracy) and a root mean square error (RMSE) of 0.455~N. Force control experiments using a twisted string actuator and PI control yield RMSEs as low as 0.073~N. Compared to previous research using photo-interrupter, the proposed method achieves more than tenfold improvement in resolution while also reducing nonlinearity and hysteresis. The design is mechanically simple, lightweight, easy to assemble, and suitable for integration into robotic and prosthetic systems requiring high-resolution force feedback.</article>","contentLength":1540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation","url":"https://arxiv.org/abs/2507.00462","date":1751428800,"author":"","guid":180145,"unread":true,"content":"<article>arXiv:2507.00462v1 Announce Type: new \nAbstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones. We propose MS-TTA, a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training.</article>","contentLength":989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization","url":"https://arxiv.org/abs/2507.00461","date":1751428800,"author":"","guid":180146,"unread":true,"content":"<article>arXiv:2507.00461v1 Announce Type: new \nAbstract: This research paper introduces two novel complex-valued Hopfield neural networks (CvHNNs) that incorporate phase and magnitude quantization. The first CvHNN employs a ceiling-type activation function that operates on the rectangular coordinate representation of the complex net contribution. The second CvHNN similarly incorporates phase and magnitude quantization but utilizes a ceiling-type activation function based on the polar coordinate representation of the complex net contribution. The proposed CvHNNs, with their phase and magnitude quantization, significantly increase the number of states compared to existing models in the literature, thereby expanding the range of potential applications for CvHNNs.</article>","contentLength":762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pitfalls of Evaluating Language Models with Open Benchmarks","url":"https://arxiv.org/abs/2507.00460","date":1751428800,"author":"","guid":180147,"unread":true,"content":"<article>arXiv:2507.00460v1 Announce Type: new \nAbstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer standardized, transparent protocols that facilitate the fair comparison, reproducibility, and iterative advancement of Language Models (LMs). However, their openness also introduces critical and underexplored pitfalls. This study exposes these weaknesses by systematically constructing ``cheating'' models -- smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets -- which achieve top rankings on a prominent open, holistic benchmark (HELM) despite poor generalization and limited practical utility. Our findings underscore three key insights: \\ca high leaderboard performance on open benchmarks may not always reflect real-world effectiveness; \\cb private or dynamic benchmarks must complement open evaluations to safeguard integrity; and \\cc a fundamental reevaluation of current benchmarking practices is essential to ensure robust and trustworthy LM assessments.</article>","contentLength":1018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools","url":"https://arxiv.org/abs/2507.00456","date":1751428800,"author":"","guid":180148,"unread":true,"content":"<article>arXiv:2507.00456v1 Announce Type: new \nAbstract: This study investigates Shiksha copilot, an AI-assisted lesson planning tool deployed in government schools across Karnataka, India. The system combined LLMs and human expertise through a structured process in which English and Kannada lesson plans were co-created by curators and AI; teachers then further customized these curated plans for their classrooms using their own expertise alongside AI support. Drawing on a large-scale mixed-methods study involving 1,043 teachers and 23 curators, we examine how educators collaborate with AI to generate context-sensitive lesson plans, assess the quality of AI-generated content, and analyze shifts in teaching practices within multilingual, low-resource environments. Our findings show that teachers used Shiksha copilot both to meet administrative documentation needs and to support their teaching. The tool eased bureaucratic workload, reduced lesson planning time, and lowered teaching-related stress, while promoting a shift toward activity-based pedagogy. However, systemic challenges such as staffing shortages and administrative demands constrained broader pedagogical change. We frame these findings through the lenses of teacher-AI collaboration and communities of practice to examine the effective integration of AI tools in teaching. Finally, we propose design directions for future teacher-centered EdTech, particularly in multilingual and Global South contexts.</article>","contentLength":1471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales","url":"https://arxiv.org/abs/2507.00454","date":1751428800,"author":"","guid":180149,"unread":true,"content":"<article>arXiv:2507.00454v1 Announce Type: new \nAbstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment between visual inputs and language descriptions caused by target movement. Previous trackers have explored many effective feature modification methods to preserve more aligned features. However, an important yet unexplored factor ultimately hinders their capability, which is the inherent differences in the temporal and spatial scale of information between visual and language inputs. To address this issue, we propose a novel visual-language tracker that enhances the effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and \\textbf{S}patial scale of different input components, named as \\textbf{ATSTrack}. Specifically, we decompose each language description into phrases with different attributes based on their temporal and spatial correspondence with visual inputs, and modify their features in a fine-grained manner. Moreover, we introduce a Visual-Language token that comprises modified linguistic information from the previous frame to guide the model to extract visual features that are more relevant to language description, thereby reducing the impact caused by the differences in spatial scale. Experimental results show that our proposed ATSTrack achieves performance comparable to existing methods. Our code will be released.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling","url":"https://arxiv.org/abs/2507.00453","date":1751428800,"author":"","guid":180150,"unread":true,"content":"<article>arXiv:2507.00453v1 Announce Type: new \nAbstract: We present a Transformer architecture for long-context language modeling that combines global attention with two biologically inspired components: chunked local attention and a gated FIFO memory mechanism. This unified attention block allows the model to efficiently handle both short-range and long-range dependencies without increasing attention cost quadratically. The memory module persistently stores past token representations using a gated update mechanism inspired by recurrent networks. Rotary positional encoding is applied per attention head to enable directionally disentangled, scale-invariant positional signals. The architecture is implemented entirely from scratch in PyTorch, with no reliance on high-level libraries, enabling transparent and modular experimentation. Our model offers a lightweight and extensible design for tasks such as dialogue modeling, code completion, and document understanding.</article>","contentLength":968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The impact of the following vehicles behaviors on the car following behaviors of the ego-vehicle","url":"https://arxiv.org/abs/2507.00452","date":1751428800,"author":"","guid":180151,"unread":true,"content":"<article>arXiv:2507.00452v1 Announce Type: new \nAbstract: Among all types of crashes, rear-end crashes dominate, which are closely related to the car-following (CF) behaviors. Traditional CF behavior models focused on the influence of the vehicle in front, but usually ignored the peer pressure from the surrounding road users, including the following vehicle (FV). Based on an open dataset, the highD dataset, we investigated whether the FV's states can affect the CF behavior of the ego-vehicle in CF events. Two types of CF events were extracted from highD database, including the tailgated events, where the time headway between the FV and the ego-vehicle (i.e., time gap) was smaller than 1 second, and the gapped events, where the time gap was larger than 3 seconds. The dynamic time warping was used to extract CF pairs with similar speed profiles of the leading vehicle (LV). Statistical analyses were conducted to compare the CF-performance metrics in tailgated and gapped events. Then, the inverse reinforcement learning was used to recover the reward function of the ego-vehicle drivers in different CF events. The results showed that the ego-driver would adjust their CF behavior in response to the pressure from a tailgating FV, by maintaining a closer distance to the LV, but at the same time, driving more cautiously. Further, drivers were still able to adjust their CF strategies based on the speed of traffic flow and the distance to the LV, even when being tailgated. These findings provide insights regarding more accurate modelling of traffic flow by considering the peer pressure from surrounding road users.</article>","contentLength":1620,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Agent Identification for General Game Playing","url":"https://arxiv.org/abs/2507.00451","date":1751428800,"author":"","guid":180152,"unread":true,"content":"<article>arXiv:2507.00451v1 Announce Type: new \nAbstract: We present an efficient and generalised procedure to accurately identify the best performing algorithm for each sub-task in a multi-problem domain. Our approach treats this as a set of best arm identification problems for multi-armed bandits, where each bandit corresponds to a specific task and each arm corresponds to a specific algorithm or agent. We propose an optimistic selection process based on the Wilson score interval (Optimistic-WS) that ranks each arm across all bandits in terms of their potential regret reduction. We evaluate the performance of Optimistic-WS on two of the most popular general game domains, the General Video Game AI (GVGAI) framework and the Ludii general game playing system, with the goal of identifying the highest performing agent for each game within a limited number of trials. Compared to previous best arm identification algorithms for multi-armed bandits, our results demonstrate a substantial performance improvement in terms of average simple regret. This novel approach can be used to significantly improve the quality and accuracy of agent evaluation procedures for general game frameworks, as well as other multi-task domains with high algorithm runtimes.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention","url":"https://arxiv.org/abs/2507.00449","date":1751428800,"author":"","guid":180153,"unread":true,"content":"<article>arXiv:2507.00449v1 Announce Type: new \nAbstract: Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, \\emph{joint recall}, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA).</article>","contentLength":1736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration","url":"https://arxiv.org/abs/2507.00447","date":1751428800,"author":"","guid":180154,"unread":true,"content":"<article>arXiv:2507.00447v1 Announce Type: new \nAbstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face restoration algorithms must balance perceptual quality and fidelity. To achieve minimal distortion while maintaining perfect perceptual quality, Posterior-Mean Rectified Flow (PMRF) proposes a flow based approach where source distribution is minimum distortion estimations. Although PMRF is shown to be effective, its pixel-space modeling approach limits its ability to align with human perception, where human perception is defined as how humans distinguish between two image distributions. In this work, we propose Latent-PMRF, which reformulates PMRF in the latent space of a variational autoencoder (VAE), facilitating better alignment with human perception during optimization. By defining the source distribution on latent representations of minimum distortion estimation, we bound the minimum distortion by the VAE's reconstruction error. Moreover, we reveal the design of VAE is crucial, and our proposed VAE significantly outperforms existing VAEs in both reconstruction and restoration. Extensive experiments on blind face restoration demonstrate the superiority of Latent-PMRF, offering an improved PD-tradeoff compared to existing methods, along with remarkable convergence efficiency, achieving a 5.79X speedup over PMRF in terms of FID. Our code will be available as open-source.</article>","contentLength":1417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DIJE: Dense Image Jacobian Estimation for Robust Robotic Self-Recognition and Visual Servoing","url":"https://arxiv.org/abs/2507.00446","date":1751428800,"author":"","guid":180155,"unread":true,"content":"<article>arXiv:2507.00446v1 Announce Type: new \nAbstract: For robots to move in the real world, they must first correctly understand the state of its own body and the tools that it holds. In this research, we propose DIJE, an algorithm to estimate the image Jacobian for every pixel. It is based on an optical flow calculation and a simplified Kalman Filter that can be efficiently run on the whole image in real time. It does not rely on markers nor knowledge of the robotic structure. We use the DIJE in a self-recognition process which can robustly distinguish between movement by the robot and by external entities, even when the motion overlaps. We also propose a visual servoing controller based on DIJE, which can learn to control the robot's body to conduct reaching movements or bimanual tool-tip control. The proposed algorithms were implemented on a physical musculoskeletal robot and its performance was verified. We believe that such global estimation of the visuomotor policy has the potential to be extended into a more general framework for manipulation.</article>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design","url":"https://arxiv.org/abs/2507.00445","date":1751428800,"author":"","guid":180156,"unread":true,"content":"<article>arXiv:2507.00445v1 Announce Type: new \nAbstract: We address the problem of fine-tuning diffusion models for reward-guided generation in biomolecular design. While diffusion models have proven highly effective in modeling complex, high-dimensional data distributions, real-world applications often demand more than high-fidelity generation, requiring optimization with respect to potentially non-differentiable reward functions such as physics-based simulation or rewards based on scientific knowledge. Although RL methods have been explored to fine-tune diffusion models for such objectives, they often suffer from instability, low sample efficiency, and mode collapse due to their on-policy nature. In this work, we propose an iterative distillation-based fine-tuning framework that enables diffusion models to optimize for arbitrary reward functions. Our method casts the problem as policy distillation: it collects off-policy data during the roll-in phase, simulates reward-based soft-optimal policies during roll-out, and updates the model by minimizing the KL divergence between the simulated soft-optimal policy and the current model policy. Our off-policy formulation, combined with KL divergence minimization, enhances training stability and sample efficiency compared to existing RL-based methods. Empirical results demonstrate the effectiveness and superior reward optimization of our approach across diverse tasks in protein, small molecule, and regulatory DNA design.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiffCkt: A Diffusion Model-Based Hybrid Neural Network Framework for Automatic Transistor-Level Generation of Analog Circuits","url":"https://arxiv.org/abs/2507.00444","date":1751428800,"author":"","guid":180157,"unread":true,"content":"<article>arXiv:2507.00444v1 Announce Type: new \nAbstract: Analog circuit design consists of the pre-layout and layout phases. Among them, the pre-layout phase directly decides the final circuit performance, but heavily depends on experienced engineers to do manual design according to specific application scenarios. To overcome these challenges and automate the analog circuit pre-layout design phase, we introduce DiffCkt: a diffusion model-based hybrid neural network framework for the automatic transistor-level generation of analog circuits, which can directly generate corresponding circuit structures and device parameters tailored to specific performance requirements. To more accurately quantify the efficiency of circuits generated by DiffCkt, we introduce the Circuit Generation Efficiency Index (CGEI), which is determined by both the figure of merit (FOM) of a single generated circuit and the time consumed. Compared with relative research, DiffCkt has improved CGEI by a factor of $2.21 \\sim 8365\\times$, reaching a state-of-the-art (SOTA) level. In conclusion, this work shows that the diffusion model has the remarkable ability to learn and generate analog circuit structures and device parameters, providing a revolutionary method for automating the pre-layout design of analog circuits. The circuit dataset will be open source, its preview version is available at https://github.com/CjLiu-NJU/DiffCkt.</article>","contentLength":1411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems","url":"https://arxiv.org/abs/2507.00443","date":1751428800,"author":"","guid":180158,"unread":true,"content":"<article>arXiv:2507.00443v1 Announce Type: new \nAbstract: Recent advances in multi-agent systems manipulation have demonstrated a rising demand for the implementation of multi-UAV systems in urban areas, which are always subjected to the presence of static and dynamic obstacles. Inspired by the collective behavior of tilapia fish and pigeons, the focus of the presented research is on the introduction of a nature-inspired collision-free formation control for a multi-UAV system, considering the obstacle avoidance maneuvers. The developed framework in this study utilizes a semi-distributed control approach, in which, based on a probabilistic Lloyd's algorithm, a centralized guidance algorithm works for optimal positioning of the UAVs, while a distributed control approach has been used for the intervehicle collision and obstacle avoidance. Further, the presented framework has been extended to the 3D space with a novel definition of 3D maneuvers. Finally, the presented framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the obtained results demonstrated the validity of the presented method in dynamic environments with stationary and moving obstacles.</article>","contentLength":1178,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Recipe for Causal Graph Regression: Confounding Effects Revisited","url":"https://arxiv.org/abs/2507.00440","date":1751428800,"author":"","guid":180159,"unread":true,"content":"<article>arXiv:2507.00440v1 Announce Type: new \nAbstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen to be a promising approach for improving the generalizability of graph neural networks under out-of-distribution (OOD) scenarios. However, the empirical successes of CGL techniques are mostly exemplified in classification settings, while regression tasks, a more challenging setting in graph learning, are overlooked. We thus devote this work to tackling causal graph regression (CGR); to this end we reshape the processing of confounding effects in existing CGL studies, which mainly deal with classification. Specifically, we reflect on the predictive power of confounders in graph-level regression, and generalize classification-specific causal intervention techniques to regression through a lens of contrastive learning. Extensive experiments on graph OOD benchmarks validate the efficacy of our proposals for CGR. The model implementation and the code are provided on https://github.com/causal-graph/CGR.</article>","contentLength":1035,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions","url":"https://arxiv.org/abs/2507.00439","date":1751428800,"author":"","guid":180160,"unread":true,"content":"<article>arXiv:2507.00439v1 Announce Type: new \nAbstract: The ability to accurately predict how different population groups would answer subjective questions would have great value. In this work, we show that use of relatively simple supervision can greatly improve language model alignment with diverse population groups, as measured over three datasets spanning various topics. Beyond evaluating average performance, we also report how alignment varies across specific groups. The simplicity and generality of our approach promotes easy adoption, while our broad findings provide useful guidance for when to use or not use our approach in practice. By conducting evaluation over many LLMs and prompting strategies, along with open-sourcing our work, we provide a useful benchmark to stimulate future research.</article>","contentLength":802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation","url":"https://arxiv.org/abs/2507.00435","date":1751428800,"author":"","guid":180161,"unread":true,"content":"<article>arXiv:2507.00435v1 Announce Type: new \nAbstract: We present RoboEval, a simulation benchmark and structured evaluation framework designed to reveal the limitations of current bimanual manipulation policies. While prior benchmarks report only binary task success, we show that such metrics often conceal critical weaknesses in policy behavior -- such as poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval introduces a suite of tiered, semantically grounded tasks decomposed into skill-specific stages, with variations that systematically challenge spatial, physical, and coordination capabilities. Tasks are paired with fine-grained diagnostic metrics and 3000+ human demonstrations to support imitation learning. Our experiments reveal that policies with similar success rates diverge in how tasks are executed -- some struggle with alignment, others with temporally consistent bimanual control. We find that behavioral metrics correlate with success in over half of task-metric pairs, and remain informative even when binary success saturates. By pinpointing when and how policies fail, RoboEval enables a deeper, more actionable understanding of robotic manipulation -- and highlights the need for evaluation tools that go beyond success alone.</article>","contentLength":1274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning","url":"https://arxiv.org/abs/2507.00432","date":1751428800,"author":"","guid":180162,"unread":true,"content":"<article>arXiv:2507.00432v1 Announce Type: new \nAbstract: Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models.</article>","contentLength":1334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition","url":"https://arxiv.org/abs/2507.00430","date":1751428800,"author":"","guid":180163,"unread":true,"content":"<article>arXiv:2507.00430v1 Announce Type: new \nAbstract: Handwritten mathematical expression recognition (HMER) suffers from complex formula structures and character layouts in sequence prediction. In this paper, we incorporate frequency domain analysis into HMER and propose a method that marries frequency domain with HMER (MFH), leveraging the discrete cosine transform (DCT). We emphasize the structural analysis assistance of frequency information for recognizing mathematical formulas. When implemented on various baseline models, our network exhibits a consistent performance enhancement, demonstrating the efficacy of frequency domain information. Experiments show that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on the CROHME 2014/2016/2019 test sets. The source code is available at https://github.com/Hryxyhe/MFH.</article>","contentLength":845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting","url":"https://arxiv.org/abs/2507.00429","date":1751428800,"author":"","guid":180164,"unread":true,"content":"<article>arXiv:2507.00429v1 Announce Type: new \nAbstract: Developing a unified pipeline that enables users to remove, re-texture, or replace objects in a versatile manner is crucial for text-guided 3D inpainting. However, there are still challenges in performing multiple 3D inpainting tasks within a unified framework: 1) Single reference inpainting methods lack robustness when dealing with views that are far from the reference view. 2) Appearance inconsistency arises when independently inpainting multi-view images with 2D diffusion priors; 3) Geometry inconsistency limits performance when there are significant geometric changes in the inpainting regions. To tackle these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting pipeline that leverages diffusion models to propagate consistent appearance and geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy for selecting multiple reference views to reduce errors during propagation. Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that propagates attention features from the selected reference views to other views via diffusion models to maintain appearance consistency. Furthermore, DiGA3D introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to further improve the geometric consistency of inpainted 3D scenes. Extensive experiments on multiple 3D inpainting tasks demonstrate the effectiveness of our method. The project page is available at https://rorisis.github.io/DiGA3D/.</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-Time In-Network Machine Learning on P4-Programmable FPGA SmartNICs with Fixed-Point Arithmetic and Taylor","url":"https://arxiv.org/abs/2507.00428","date":1751428800,"author":"","guid":180165,"unread":true,"content":"<article>arXiv:2507.00428v1 Announce Type: new \nAbstract: As machine learning (ML) applications become integral to modern network operations, there is an increasing demand for network programmability that enables low-latency ML inference for tasks such as Quality of Service (QoS) prediction and anomaly detection in cybersecurity. ML models provide adaptability through dynamic weight adjustments, making Programming Protocol-independent Packet Processors (P4)-programmable FPGA SmartNICs an ideal platform for investigating In-Network Machine Learning (INML). These devices offer high-throughput, low-latency packet processing and can be dynamically reconfigured via the control plane, allowing for flexible integration of ML models directly at the network edge. This paper explores the application of the P4 programming paradigm to neural networks and regression models, where weights and biases are stored in control plane table lookups. This approach enables flexible programmability and efficient deployment of retrainable ML models at the network edge, independent of core infrastructure at the switch level.</article>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition","url":"https://arxiv.org/abs/2507.00427","date":1751428800,"author":"","guid":180166,"unread":true,"content":"<article>arXiv:2507.00427v1 Announce Type: new \nAbstract: This paper investigates the feasibility of achieving zero-knowledge verifiability for graph databases, enabling database owners to cryptographically prove the query execution correctness without disclosing the underlying data. Although similar capabilities have been explored for relational databases, their implementation for graph databases presents unique challenges. This is mainly attributed to the relatively large complexity of queries in graph databases. When translating graph queries into arithmetic circuits, the circuit scale can be too large to be practically evaluated. To address this issue, we propose to break down graph queries into more fine-grained, primitive operators, enabling a step-by-step evaluation through smaller-scale circuits. Accordingly, the verification with ZKP circuits of complex graph queries can be decomposed into a series of composable cryptographic primitives, each designed to verify a fundamental structural property such as path ordering or edge directionality. Especially, having noticed that the graph expansion (i.e., traversing from nodes to their neighbors along edges) operation serves as the backbone of graph query evaluation, we design the expansion centric operator decomposition. In addition to constructing circuits for the expansion primitives, we also design specialized ZKP circuits for the various attributes that augment this traversal. The circuits are meticulously designed to take advantage of PLONKish arithmetization. By integrating these optimized circuits, we implement ZKGraph, a system that provides verifiable query processing while preserving data privacy. Performance evaluation indicates that ZKGraph significantly outperforms naive in circuit implementations of graph operators, achieving substantial improvements in both runtime and memory consumption.</article>","contentLength":1878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows","url":"https://arxiv.org/abs/2507.00425","date":1751428800,"author":"","guid":180167,"unread":true,"content":"<article>arXiv:2507.00425v1 Announce Type: new \nAbstract: Autoregressive models have driven remarkable progress in language modeling. Their foundational reliance on discrete tokens, unidirectional context, and single-pass decoding, while central to their success, also inspires the exploration of a design space that could offer new axes of modeling flexibility. In this work, we explore an alternative paradigm, shifting language modeling from a discrete token space to a continuous latent space. We propose a novel framework TarFlowLM, that employs transformer-based autoregressive normalizing flows to model these continuous representations. This approach unlocks substantial flexibility, enabling the construction of models that can capture global bi-directional context through stacked, alternating-direction autoregressive transformations, support block-wise generation with flexible token patch sizes, and facilitate a hierarchical multi-pass generation process. We further propose new mixture-based coupling transformations designed to capture complex dependencies within the latent space shaped by discrete data, and demonstrate theoretical connections to conventional discrete autoregressive models. Extensive experiments on language modeling benchmarks demonstrate strong likelihood performance and highlight the flexible modeling capabilities inherent in our framework.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Agent Coordination under Poisson Observations: A Global Game Approach","url":"https://arxiv.org/abs/2507.00424","date":1751428800,"author":"","guid":180168,"unread":true,"content":"<article>arXiv:2507.00424v1 Announce Type: new \nAbstract: We study a model of strategic coordination based on a class of games with incomplete information known as Global Games. Under the assumption of Poisson-distributed signals and a Gamma prior distribution on state of the system, we demonstrate the existence of a Bayesian Nash equilibrium within the class of threshold policies for utility functions that are linear in the agents' actions. Although computing the exact threshold that constitutes an equilibrium in a system with finitely many agents is a highly non-trivial task, the problem becomes tractable by analyzing the game's potential function with countably infinitely many agents. Through numerical examples, we provide evidence that the resulting potential function is unimodal, exhibiting a well-defined maximum. Our results are applicable to the modeling of bacterial Quorum Sensing systems, whose noisy observation signals are often well-approximated using Poisson processes.</article>","contentLength":986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning","url":"https://arxiv.org/abs/2507.00423","date":1751428800,"author":"","guid":180169,"unread":true,"content":"<article>arXiv:2507.00423v1 Announce Type: new \nAbstract: Federated learning (FL) allows multiple clients to collaboratively train a global machine learning model with coordination from a central server, without needing to share their raw data. This approach is particularly appealing in the era of privacy regulations like the GDPR, leading many prominent companies to adopt it. However, FL's distributed nature makes it susceptible to poisoning attacks, where malicious clients, controlled by an attacker, send harmful data to compromise the model. Most existing poisoning attacks in FL aim to degrade the model's integrity, such as reducing its accuracy, with limited attention to privacy concerns from these attacks. In this study, we introduce FedPoisonMIA, a novel poisoning membership inference attack targeting FL. FedPoisonMIA involves malicious clients crafting local model updates to infer membership information. Additionally, we propose a robust defense mechanism to mitigate the impact of FedPoisonMIA attacks. Extensive experiments across various datasets demonstrate the attack's effectiveness, while our defense approach reduces its impact to a degree.</article>","contentLength":1160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evolutionary Dynamics with Self-Interaction Learning in Networked Systems","url":"https://arxiv.org/abs/2507.00422","date":1751428800,"author":"","guid":180170,"unread":true,"content":"<article>arXiv:2507.00422v1 Announce Type: new \nAbstract: The evolution of cooperation in networked systems helps to understand the dynamics in social networks, multi-agent systems, and biological species. The self-persistence of individual strategies is common in real-world decision making. The self-replacement of strategies in evolutionary dynamics forms a selection amplifier, allows an agent to insist on its autologous strategy, and helps the networked system to avoid full defection. In this paper, we study the self-interaction learning in the networked evolutionary dynamics. We propose a self-interaction landscape to capture the strength of an agent's self-loop to reproduce the strategy based on local topology. We find that proper self-interaction can reduce the condition for cooperation and help cooperators to prevail in the system. For a system that favors the evolution of spite, the self-interaction can save cooperative agents from being harmed. Our results on random networks further suggest that an appropriate self-interaction landscape can significantly reduce the critical condition for advantageous mutants, especially for large-degree networks.</article>","contentLength":1163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Embedded DevOps: A Survey on the Application of DevOps Practices in Embedded Software and Firmware Development","url":"https://arxiv.org/abs/2507.00421","date":1751428800,"author":"","guid":180171,"unread":true,"content":"<article>arXiv:2507.00421v1 Announce Type: new \nAbstract: The adoption of DevOps practices in embedded systems and firmware development is emerging as a response to the growing complexity of modern hardware--software co-designed products. Unlike cloud-native applications, embedded systems introduce challenges such as hardware dependency, real-time constraints, and safety-critical requirements. This literature review synthesizes findings from 20 academic and industrial sources to examine how DevOps principles--particularly continuous integration, continuous delivery, and automated testing--are adapted to embedded contexts. We categorize efforts across tooling, testing strategies, pipeline automation, and security practices. The review highlights current limitations in deployment workflows and observability, proposing a roadmap for future research. This work offers researchers and practitioners a consolidated understanding of Embedded DevOps, bridging fragmented literature with a structured perspective.</article>","contentLength":1007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs","url":"https://arxiv.org/abs/2507.00418","date":1751428800,"author":"","guid":180172,"unread":true,"content":"<article>arXiv:2507.00418v1 Announce Type: new \nAbstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt) and performance against leading NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform (NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90 billion parameters, are served using the vLLM framework. The QAic inference cards appears to be energy efficient and performs well in the energy efficiency metric in most cases. The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications within the National Research Platform (NRP).</article>","contentLength":790,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context","url":"https://arxiv.org/abs/2507.00417","date":1751428800,"author":"","guid":180173,"unread":true,"content":"<article>arXiv:2507.00417v1 Announce Type: new \nAbstract: We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework for training language models to reason like search algorithms, explicitly leveraging self-reflection, backtracking, and exploration in their outputs. Recently, training large language models (LLMs) via reinforcement learning (RL) has led to the advent of reasoning models with greatly enhanced reasoning capabilities. Open-source replications of reasoning models, while successful, build upon models that already exhibit strong reasoning capabilities along with search behavior observed even before RL. As a result, it is yet unclear how to boost the reasoning capabilities of other non-reasoner models including Llama 3. ASTRO teaches such models to internalize structured search behavior through a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over mathematical problem-solving trajectories. By converting search traces into natural language chain-of-thoughts that capture both successes and recoveries from failure, ASTRO bootstraps models with a rich prior for exploration during RL. We finetune our models on these search-derived traces and further improve performance via RL with verifiable rewards. We apply ASTRO to the Llama 3 family of models and achieve absolute performance gains of 16.0% on MATH-500, 26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon challenging problems that require iterative correction. Our results demonstrate that search-inspired training offers a principled way to instill robust reasoning capabilities into open LLMs.</article>","contentLength":1615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding","url":"https://arxiv.org/abs/2507.00416","date":1751428800,"author":"","guid":180174,"unread":true,"content":"<article>arXiv:2507.00416v1 Announce Type: new \nAbstract: Vision-Language-Action (VLA) models have emerged as a promising framework for enabling generalist robots capable of perceiving, reasoning, and acting in the real world. These models usually build upon pretrained Vision-Language Models (VLMs), which excel at semantic understanding due to large-scale text pretraining. However, VLMs typically lack precise spatial understanding capabilities, as they are primarily tuned on 2D image-text pairs without 3D supervision. To address this limitation, recent approaches have incorporated explicit 3D inputs such as point clouds or depth maps, but this necessitates additional depth sensors or defective estimation. In contrast, our work introduces a plug-and-play module that implicitly injects 3D geometry features into VLA models by leveraging an off-the-shelf visual geometry foundation models. We design five spatially challenging tasks that require precise spatial understanding ability to validate effectiveness of our method. Extensive evaluations show that our method significantly improves the performance of state-of-the-art VLA models across diverse scenarios.</article>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minimal Construction of Graphs with Maximum Robustness","url":"https://arxiv.org/abs/2507.00415","date":1751428800,"author":"","guid":180175,"unread":true,"content":"<article>arXiv:2507.00415v1 Announce Type: new \nAbstract: The notions of network $r$-robustness and $(r,s)$-robustness have been earlier introduced in the literature to achieve resilient control in the presence of misbehaving agents. However, while higher robustness levels provide networks with higher tolerances against the misbehaving agents, they also require dense communication structures, which are not always desirable for systems with limited capabilities and energy capacities. Therefore, this paper studies the fundamental structures behind $r$-robustness and $(r,s)$- robustness properties in two different ways. (a) We first explore and establish the tight necessary conditions on the number of edges for undirected graphs with any nodes must satisfy to achieve maximum $r$- and $(r,s)$-robustness. (b) We then use these conditions to construct two classes of undirected graphs, referred as to $\\gamma$- and $(\\gamma,\\gamma)$-Minimal Edge Robust Graphs (MERGs), that provably achieve maximum robustness with minimal numbers of edges. We finally validate our work through some sets of simulations.</article>","contentLength":1100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recommending Variable Names for Extract Local Variable Refactorings","url":"https://arxiv.org/abs/2507.00413","date":1751428800,"author":"","guid":180176,"unread":true,"content":"<article>arXiv:2507.00413v1 Announce Type: new \nAbstract: Extract local variable is one of the most popular refactorings, and most IDEs and refactoring tools provide automated support for this refactoring. However, we find approximately 70% of the names recommended by these IDEs are different from what developers manually constructed, adding additional renaming burdens to developers and providing limited assistance. In this paper, we introduce VarNamer, an automated approach designed to recommend variable names for extract local variable refactorings. Through a large-scale empirical study, we identify key contexts that are useful for composing variable names. Leveraging these insights, we developed a set of heuristic rules through program static analysis techniques and employ data mining techniques to recommend variable names effectively. Notably, some of our heuristic rules have been successfully integrated into Eclipse, where they are now distributed with the latest releases of the IDE. Evaluation demonstrates its superiority over state-of-the-art IDEs. Specifically, VarNamer significantly increases the chance of exact match by 52.6% compared to Eclipse and 40.7% compared to IntelliJ IDEA. We also evaluated the proposed approach with real-world extract local variable refactorings conducted in C++ projects, and the results suggest that the approach can achieve comparable performance on programming languages besides Java. It may suggest the generalizability of VarNamer. Finally, we designed and conducted a user study and the results of the user study suggest that our approach can speed up the refactoring by 27.8% and reduce 49.3% edits on the recommended variable names.</article>","contentLength":1689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ViscoReg: Neural Signed Distance Functions via Viscosity Solutions","url":"https://arxiv.org/abs/2507.00412","date":1751428800,"author":"","guid":180177,"unread":true,"content":"<article>arXiv:2507.00412v1 Announce Type: new \nAbstract: Implicit Neural Representations (INRs) that learn a Signed Distance Function (SDF) are a powerful tool for continuous 3D scene reconstruction. These models are trained by enforcing the Eikonal equation. We demonstrate theoretically that despite the ill-posedness of the Eikonal equation, generalization error estimates may be obtained for Neural SDFs in terms of the training error. However, training with the Eikonal loss can lead to unstable gradient flows, necessitating alternate stabilization techniques. Traditional numerical solvers for the equation have relied on viscosity approaches for regularization. We enhance Neural SDF training using this well-developed theory, and introduce a new loss formulation we call ViscoReg. We theoretically demonstrate the stability of the gradient flow equation of our proposed loss term. Empirically, ViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik without adding significant computational cost.</article>","contentLength":1021,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion Disambiguation Models for Partial Label Learning","url":"https://arxiv.org/abs/2507.00411","date":1751428800,"author":"","guid":180178,"unread":true,"content":"<article>arXiv:2507.00411v1 Announce Type: new \nAbstract: Learning from ambiguous labels is a long-standing problem in practical machine learning applications. The purpose of \\emph{partial label learning} (PLL) is to identify the ground-truth label from a set of candidate labels associated with a given instance. Inspired by the remarkable performance of diffusion models in various generation tasks, this paper explores their potential to denoise ambiguous labels through the reverse denoising process. Therefore, this paper reformulates the label disambiguation problem from the perspective of generative models, where labels are generated by iteratively refining initial random guesses. This perspective enables the diffusion model to learn how label information is generated stochastically. By modeling the generation uncertainty, we can use the maximum likelihood estimate of the label for classification inference. However, such ambiguous labels lead to a mismatch between instance and label, which reduces the quality of generated data. To address this issue, this paper proposes a \\emph{diffusion disambiguation model for PLL} (DDMP), which first uses the potential complementary information between instances and labels to construct pseudo-clean labels for initial diffusion training. Furthermore, a transition-aware matrix is introduced to estimate the potential ground-truth labels, which are dynamically updated during the diffusion generation. During training, the ground-truth label is progressively refined, improving the classifier. Experiments show the advantage of the DDMP and its suitability for PLL.</article>","contentLength":1612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eilenberg correspondence for Stone recognition","url":"https://arxiv.org/abs/2507.00409","date":1751428800,"author":"","guid":180179,"unread":true,"content":"<article>arXiv:2507.00409v1 Announce Type: new \nAbstract: We develop and explore the idea of recognition of languages (in the general sense of subsets of topological algebras) as preimages of clopen sets under continuous homomorphisms into Stone topological algebras. We obtain an Eilenberg correspondence between varieties of languages and varieties of ordered Stone topological algebras and a Birkhoff/Reiterman-type theorem showing that the latter may me defined by certain pseudo-inequalities. In the case of classical formal languages, of words over a finite alphabet, we also show how this extended framework goes beyond the class of regular languages by working with Stone completions of minimal automata, viewed as unary algebras. This leads to a general method for showing that a language does not belong to a variety of languages, expressed in terms of sequences of pairs of words, which is illustrated when the class consists of all finite intersections of context-free languages.</article>","contentLength":982,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partnering with AI: A Pedagogical Feedback System for LLM Integration into Programming Education","url":"https://arxiv.org/abs/2507.00406","date":1751428800,"author":"","guid":180180,"unread":true,"content":"<article>arXiv:2507.00406v1 Announce Type: new \nAbstract: Feedback is one of the most crucial components to facilitate effective learning. With the rise of large language models (LLMs) in recent years, research in programming education has increasingly focused on automated feedback generation to help teachers provide timely support to every student. However, prior studies often overlook key pedagogical principles, such as mastery and progress adaptation, that shape effective feedback strategies. This paper introduces a novel pedagogical framework for LLM-driven feedback generation derived from established feedback models and local insights from secondary school teachers. To evaluate this framework, we implemented a web-based application for Python programming with LLM-based feedback that follows the framework and conducted a mixed-method evaluation with eight secondary-school computer science teachers. Our findings suggest that teachers consider that, when aligned with the framework, LLMs can effectively support students and even outperform human teachers in certain scenarios through instant and precise feedback. However, we also found several limitations, such as its inability to adapt feedback to dynamic classroom contexts. Such a limitation highlights the need to complement LLM-generated feedback with human expertise to ensure effective student learning. This work demonstrates an effective way to use LLMs for feedback while adhering to pedagogical standards and highlights important considerations for future systems.</article>","contentLength":1535,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains","url":"https://arxiv.org/abs/2507.00401","date":1751428800,"author":"","guid":180181,"unread":true,"content":"<article>arXiv:2507.00401v1 Announce Type: new \nAbstract: We investigate cross-domain few-shot learning under the constraint that fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible -- a scenario that is increasingly common in practical use cases. Handling the low-quality and static embeddings produced by frozen, \"black-box\" backbones leads to a problem representation of few-shot classification as a series of multiple instance verification (MIV) tasks. Inspired by this representation, we introduce a novel approach to few-shot domain adaptation, named the \"MIV-head\", akin to a classification head that is agnostic to any pretrained backbone and computationally efficient. The core components designed for the MIV-head, when trained on few-shot data from a target domain, collectively yield strong performance on test data from that domain. Importantly, it does so without fine-tuning the backbone, and within the \"meta-testing\" phase. Experimenting under various settings and on an extension of the Meta-dataset benchmark for cross-domain few-shot image classification, using representative off-the-shelf convolutional neural network and vision transformer backbones pretrained on ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when compared to state-of-the-art \"adapter\" (or partially fine-tuning) methods applied to the same backbones, while incurring substantially lower adaptation cost. We also find well-known \"classification head\" approaches lag far behind in terms of accuracy. Ablation study empirically justifies the core components of our approach. We share our code at https://github.com/xxweka/MIV-head.</article>","contentLength":1668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism","url":"https://arxiv.org/abs/2507.00394","date":1751428800,"author":"","guid":180182,"unread":true,"content":"<article>arXiv:2507.00394v1 Announce Type: new \nAbstract: As transformer sequence lengths grow, existing pipeline parallelisms incur suboptimal performance due to the quadratic attention computation and the substantial memory overhead. To relieve these challenges, we propose HelixPipe, a novel pipeline parallelism for long sequence transformer training. First, HelixPipe introduces attention parallel partition, which schedules attention computations of different micro batches across different pipeline stages in parallel, reducing pipeline bubbles. Second, it employs a two-fold first-in-last-out micro batch schedule to balance memory usage and overlap communication with computation. Additionally, HelixPipe utilizes recomputation without attention and chunked MLP to mitigate fragmentation and enable longer sequences. Experiments demonstrate that HelixPipe gains increasing advantages with longer sequence lengths, and outperforms existing methods in throughput and scalability across varying pipeline sizes, model sizes, and cluster configurations. Notably, it achieves a 26\\% speedup over baseline methods when training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available at https://github.com/code-tunnel/Megatron-LM/tree/dev.</article>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space","url":"https://arxiv.org/abs/2507.00392","date":1751428800,"author":"","guid":180183,"unread":true,"content":"<article>arXiv:2507.00392v1 Announce Type: new \nAbstract: Feature matching plays a fundamental role in many computer vision tasks, yet existing methods heavily rely on scarce and clean multi-view image collections, which constrains their generalization to diverse and challenging scenarios. Moreover, conventional feature encoders are typically trained on single-view 2D images, limiting their capacity to capture 3D-aware correspondences. In this paper, we propose a novel two-stage framework that lifts 2D images to 3D space, named as \\textbf{Lift to Match (L2M)}, taking full advantage of large-scale and diverse single-view images. To be specific, in the first stage, we learn a 3D-aware feature encoder using a combination of multi-view image synthesis and 3D feature Gaussian representation, which injects 3D geometry knowledge into the encoder. In the second stage, a novel-view rendering strategy, combined with large-scale synthetic data generation from single-view images, is employed to learn a feature decoder for robust feature matching, thus achieving generalization across diverse domains. Extensive experiments demonstrate that our method achieves superior generalization across zero-shot evaluation benchmarks, highlighting the effectiveness of the proposed framework for robust feature matching.</article>","contentLength":1304,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE","url":"https://arxiv.org/abs/2507.00390","date":1751428800,"author":"","guid":180184,"unread":true,"content":"<article>arXiv:2507.00390v1 Announce Type: new \nAbstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models by activating only a subset of experts per input token. However, deploying MoE-based models incurs significant memory overhead due to the need to retain all experts in memory. While structured pruning is promising to reduce memory costs, existing methods often show suboptimal performance and unstable degradation in three dimensions: model architectures, calibration data sources, and calibration sample sizes. This paper proposes Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that replaces redundant experts with lightweight novices to achieve effective and robust model compression. MoNE evaluates expert redundancy based on two metrics: access frequency and output variance. Experts exhibiting low usage and stable outputs are pruned and replaced with lightweight novices-unbiased estimations of their original outputs-minimizing performance degradation. Extensive experiments demonstrate that MoNE consistently outperforms baseline methods with minimal accuracy degradation across the three dimensions, confirming its effectiveness and robustness. Notably, it improves the average zero shot accuracy across nine downstream tasks by up to 2.71 under 25\\% pruning ratio and 3.61 under 50\\% pruning. The code is available at https://github.com/zxgx/mode-pd.</article>","contentLength":1407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Causal Prompting for Implicit Sentiment Analysis with Large Language Models","url":"https://arxiv.org/abs/2507.00389","date":1751428800,"author":"","guid":180185,"unread":true,"content":"<article>arXiv:2507.00389v1 Announce Type: new \nAbstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied rather than explicitly stated, requiring models to perform deeper reasoning over subtle contextual cues. While recent prompting-based methods using Large Language Models (LLMs) have shown promise in ISA, they often rely on majority voting over chain-of-thought (CoT) reasoning paths without evaluating their causal validity, making them susceptible to internal biases and spurious correlations. To address this challenge, we propose CAPITAL, a causal prompting framework that incorporates front-door adjustment into CoT reasoning. CAPITAL decomposes the overall causal effect into two components: the influence of the input prompt on the reasoning chains, and the impact of those chains on the final output. These components are estimated using encoder-based clustering and the NWGM approximation, with a contrastive learning objective used to better align the encoder's representation with the LLM's reasoning space. Experiments on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently outperforms strong prompting baselines in both accuracy and robustness, particularly under adversarial conditions. This work offers a principled approach to integrating causal inference into LLM prompting and highlights its benefits for bias-aware sentiment reasoning. The source code and case study are available at: https://github.com/whZ62/CAPITAL.</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accuracy and Security-Guaranteed Participant Selection and Beamforming Design for RIS-Assisted Federated Learning","url":"https://arxiv.org/abs/2507.00388","date":1751428800,"author":"","guid":180186,"unread":true,"content":"<article>arXiv:2507.00388v1 Announce Type: new \nAbstract: Federated learning (FL) has emerged as an effective approach for training neural network models without requiring the sharing of participants' raw data, thereby addressing data privacy concerns. In this paper, we propose a reconfigurable intelligent surface (RIS)-assisted FL framework in the presence of eavesdropping, where partial edge devices are selected to participate in the FL training process. In contrast, the remaining devices serve as cooperative jammers by transmitting jamming signals to disrupt eavesdropping. We aim to minimize the training latency in each FL round by jointly optimizing participant selection, bandwidth allocation, and RIS beamforming design, subject to the convergence accuracy of FL and the secure uploading requirements. To solve the resulting mixed-integer nonlinear programming problem, we propose a twin delayed deep deterministic policy gradient (TD3) algorithm. Simulation results demonstrate that the proposed scheme reduces the FL training latency by approximately 27$\\%$ compared to baselines.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Review on Zeroing Neural Networks","url":"https://arxiv.org/abs/2507.00387","date":1751428800,"author":"","guid":180187,"unread":true,"content":"<article>arXiv:2507.00387v1 Announce Type: new \nAbstract: Zeroing neural networks (ZNNs) have demonstrated outstanding performance on time-varying optimization and control problems. Nonetheless, few studies are committed to illustrating the relationship among different ZNNs and the derivation of them. Therefore, reviewing the advances for a systematical understanding of this field is desirable. This paper provides a survey of ZNNs' progress regarding implementing methods, analysis theory, and practical applications.</article>","contentLength":512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics","url":"https://arxiv.org/abs/2507.00380","date":1751428800,"author":"","guid":180188,"unread":true,"content":"<article>arXiv:2507.00380v1 Announce Type: new \nAbstract: The idea that Gregorian melodies are constructed from some vocabulary of segments has long been a part of chant scholarship. This so-called \"centonisation\" theory has received much musicological criticism, but frequent re-use of certain melodic segments has been observed in chant melodies, and the intractable number of possible segmentations allowed the option that some undiscovered segmentation exists that will yet prove the value of centonisation, and recent empirical results have shown that segmentations can outperform music-theoretical features in mode classification. Inspired by the fact that Gregorian chant was memorised, we search for an optimal unsupervised segmentation of chant melody using nested hierarchical Pitman-Yor language models. The segmentation we find achieves state-of-the-art performance in mode classification. Modeling a monk memorising the melodies from one liturgical manuscript, we then find empirical evidence for the link between mode classification and memory efficiency, and observe more formulaic areas at the beginnings and ends of melodies corresponding to the practical role of modality in performance. However, the resulting segmentations themselves indicate that even such a memory-optimal segmentation is not what is understood as centonisation.</article>","contentLength":1342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Robustness: A Critique of Current Vector Database Assessments","url":"https://arxiv.org/abs/2507.00379","date":1751428800,"author":"","guid":180189,"unread":true,"content":"<article>arXiv:2507.00379v1 Announce Type: new \nAbstract: Vector databases are critical infrastructure in AI systems, and average recall is the dominant metric for their evaluation. Both users and researchers rely on it to choose and optimize their systems. We show that relying on average recall is problematic. It hides variability across queries, allowing systems with strong mean performance to underperform significantly on hard queries. These tail cases confuse users and can lead to failure in downstream applications such as RAG. We argue that robustness consistently achieving acceptable recall across queries is crucial to vector database evaluation. We propose Robustness-$\\delta$@K, a new metric that captures the fraction of queries with recall above a threshold $\\delta$. This metric offers a deeper view of recall distribution, helps vector index selection regarding application needs, and guides the optimization of tail performance. We integrate Robustness-$\\delta$@K into existing benchmarks and evaluate mainstream vector indexes, revealing significant robustness differences. More robust vector indexes yield better application performance, even with the same average recall. We also identify design factors that influence robustness, providing guidance for improving real-world performance.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing","url":"https://arxiv.org/abs/2507.00378","date":1751428800,"author":"","guid":180190,"unread":true,"content":"<article>arXiv:2507.00378v1 Announce Type: new \nAbstract: Conformance testing is essential for ensuring that protocol implementations comply with their specifications. However, traditional testing approaches involve manually creating numerous test cases and scripts, making the process labor-intensive and inefficient. Recently, Large Language Models (LLMs) have demonstrated impressive text comprehension and code generation abilities, providing promising opportunities for automation. In this paper, we propose iPanda, the first end-to-end framework that leverages LLMs to automate protocol conformance testing. Given a protocol specification document and its implementation, iPanda first employs a keyword-based method to automatically generate comprehensive test cases. Then, it utilizes a code-based retrieval-augmented generation approach to effectively interpret the implementation and produce executable test code. To further enhance code quality, iPanda incorporates an iterative self-correction mechanism to refine generated test scripts interactively. Finally, by executing and analyzing the generated tests, iPanda systematically verifies compliance between implementations and protocol specifications. Comprehensive experiments on various protocols show that iPanda significantly outperforms pure LLM-based approaches, improving the success rate (Pass@1) of test-code generation by factors ranging from 4.675 times to 10.751 times.</article>","contentLength":1435,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis","url":"https://arxiv.org/abs/2507.00377","date":1751428800,"author":"","guid":180191,"unread":true,"content":"<article>arXiv:2507.00377v1 Announce Type: new \nAbstract: Recent advancements in deep learning for medical image segmentation are often limited by the scarcity of high-quality training data.While diffusion models provide a potential solution by generating synthetic images, their effectiveness in medical imaging remains constrained due to their reliance on large-scale medical datasets and the need for higher image quality. To address these challenges, we present MedDiff-FT, a controllable medical image generation method that fine-tunes a diffusion foundation model to produce medical images with structural dependency and domain specificity in a data-efficient manner. During inference, a dynamic adaptive guiding mask enforces spatial constraints to ensure anatomically coherent synthesis, while a lightweight stochastic mask generator enhances diversity through hierarchical randomness injection. Additionally, an automated quality assessment protocol filters suboptimal outputs using feature-space metrics, followed by mask corrosion to refine fidelity. Evaluated on five medical segmentation datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's segmentation performance by an average of 1% in Dice score. The framework effectively balances generation quality, diversity, and computational efficiency, offering a practical solution for medical data augmentation. The code is available at https://github.com/JianhaoXie1/MedDiff-FT.</article>","contentLength":1447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive finite element convergence analysis of AT1 phase-field model for quasi-static fracture in strain-limiting solids","url":"https://arxiv.org/abs/2507.00376","date":1751428800,"author":"","guid":180192,"unread":true,"content":"<article>arXiv:2507.00376v1 Announce Type: new \nAbstract: This research rigorously investigates the convergence of adaptive finite element methods for regularized variational models of quasi-static brittle fracture in elastic solids. We specifically examine a novel Ambrosio-Tortorelli (AT1) phase-field model within the framework of elasticity theories, particularly for material models characterized by an algebraically nonlinear stress-strain relationship. Two distinct and novel adaptive mesh refinement algorithms, underpinned by robust local error indicators, were introduced to efficiently solve the underlying nonlinear energy minimization problem. A detailed convergence analysis was conducted on the sequences of minimizers produced by these strategies. Our findings rigorously demonstrate that the minimizer sequences from the first adaptive algorithm achieve convergence to a predefined tolerance. Crucially, the second algorithm is proven to generate inherently convergent sequences, thereby eliminating the need for an explicit stopping criterion. The practical effectiveness of this proposed adaptive framework is thoroughly validated through extensive numerical simulations. A case study involving an edge crack in an elastic body, governed by an algebraically nonlinear strain-limiting relationship and subjected to anti-plane shear-type loading, is presented. Critical comparisons of the energy components-bulk, surface, and total-showcase the superior performance of both adaptive algorithms.</article>","contentLength":1502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Customizable ROI-Based Deep Image Compression","url":"https://arxiv.org/abs/2507.00373","date":1751428800,"author":"","guid":180193,"unread":true,"content":"<article>arXiv:2507.00373v1 Announce Type: new \nAbstract: Region of Interest (ROI)-based image compression optimizes bit allocation by prioritizing ROI for higher-quality reconstruction. However, as the users (including human clients and downstream machine tasks) become more diverse, ROI-based image compression needs to be customizable to support various preferences. For example, different users may define distinct ROI or require different quality trade-offs between ROI and non-ROI. Existing ROI-based image compression schemes predefine the ROI, making it unchangeable, and lack effective mechanisms to balance reconstruction quality between ROI and non-ROI. This work proposes a paradigm for customizable ROI-based deep image compression. First, we develop a Text-controlled Mask Acquisition (TMA) module, which allows users to easily customize their ROI for compression by just inputting the corresponding semantic \\emph{text}. It makes the encoder controlled by text. Second, we design a Customizable Value Assign (CVA) mechanism, which masks the non-ROI with a changeable extent decided by users instead of a constant one to manage the reconstruction quality trade-off between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA) module, where the latent spatial prior of the mask and the latent Rate-Distortion Optimization (RDO) prior of the image are extracted and fused in the latent space, and further used to optimize the latent representation of the source image. Experimental results demonstrate that our proposed customizable ROI-based deep image compression paradigm effectively addresses the needs of customization for ROI definition and mask acquisition as well as the reconstruction quality trade-off management between the ROI and non-ROI.</article>","contentLength":1768,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur","url":"https://arxiv.org/abs/2507.00372","date":1751428800,"author":"","guid":180194,"unread":true,"content":"<article>arXiv:2507.00372v1 Announce Type: new \nAbstract: Modern cameras with large apertures often suffer from a shallow depth of field, resulting in blurry images of objects outside the focal plane. This limitation is particularly problematic for fixed-focus cameras, such as those used in smart glasses, where adding autofocus mechanisms is challenging due to form factor and power constraints. Due to unmatched optical aberrations and defocus properties unique to each camera system, deep learning models trained on existing open-source datasets often face domain gaps and do not perform well in real-world settings. In this paper, we propose an efficient and scalable dataset synthesis approach that does not rely on fine-tuning with real-world data. Our method simultaneously models depth-dependent defocus and spatially varying optical aberrations, addressing both computational complexity and the scarcity of high-quality RGB-D datasets. Experimental results demonstrate that a network trained on our low resolution synthetic images generalizes effectively to high resolution (12MP) real-world images across diverse scenes.</article>","contentLength":1122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching","url":"https://arxiv.org/abs/2507.00371","date":1751428800,"author":"","guid":180195,"unread":true,"content":"<article>arXiv:2507.00371v1 Announce Type: new \nAbstract: Organ segmentation of plant point clouds is a prerequisite for the high-resolution and accurate extraction of organ-level phenotypic traits. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, the existing techniques for organ segmentation still face limitations in resolution, segmentation accuracy, and generalizability across various plant species. In this study, we proposed a novel approach called plant segmentation neural radiance fields (PlantSegNeRF), aiming to directly generate high-precision instance point clouds from multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF performed 2D instance segmentation on the multi-view images to generate instance masks for each organ with a corresponding ID. The multi-view instance IDs corresponding to the same plant organ were then matched and refined using a specially designed instance matching module. The instance NeRF was developed to render an implicit scene, containing color, density, semantic and instance information. The implicit scene was ultimately converted into high-precision plant instance point clouds based on the volume density. The results proved that in semantic segmentation of point clouds, PlantSegNeRF outperformed the commonly used methods, demonstrating an average improvement of 16.1%, 18.3%, 17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the second-best results on structurally complex datasets. More importantly, PlantSegNeRF exhibited significant advantages in plant point cloud instance segmentation tasks. Across all plant datasets, it achieved average improvements of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively. This study extends the organ-level plant phenotyping and provides a high-throughput way to supply high-quality 3D data for the development of large-scale models in plant science.</article>","contentLength":1967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Out-of-Distribution Detection with Adaptive Top-K Logits Integration","url":"https://arxiv.org/abs/2507.00368","date":1751428800,"author":"","guid":180196,"unread":true,"content":"<article>arXiv:2507.00368v1 Announce Type: new \nAbstract: Neural networks often make overconfident predictions from out-of-distribution (OOD) samples. Detection of OOD data is therefore crucial to improve the safety of machine learning. The simplest and most powerful method for OOD detection is MaxLogit, which uses the model's maximum logit to provide an OOD score. We have discovered that, in addition to the maximum logit, some other logits are also useful for OOD detection. Based on this finding, we propose a new method called ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective top-k logits that are specific to each model and combines the maximum logit with the other top-k logits. In this study we evaluate our proposed method using ImageNet-1K benchmark. Extensive experiments showed our proposed method to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit approach, and decreased FPR95 by an additional 2.67% compared to other state-of-the-art methods.</article>","contentLength":1006,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Presto: Hardware Acceleration of Ciphers for Hybrid Homomorphic Encryption","url":"https://arxiv.org/abs/2507.00367","date":1751428800,"author":"","guid":180197,"unread":true,"content":"<article>arXiv:2507.00367v1 Announce Type: new \nAbstract: Hybrid Homomorphic Encryption (HHE) combines symmetric key and homomorphic encryption to reduce ciphertext expansion crucial in client-server deployments of HE. Special symmetric ciphers, amenable to efficient HE evaluation, have been developed. Their client-side deployment calls for performant and energy-efficient implementation, and in this paper we develop and evaluate hardware accelerators for the two known CKKS-targeting HHE ciphers, HERA and Rubato.\n  We design vectorized and overlapped functional modules. The design exploits transposition-invariance property of the MixColumns and MixRows function and alternates the order of intermediate state to eliminate bubbles in stream key generation, improving latency and throughput. We decouple the RNG and key computation phases to hide the latency of RNG and to reduce the critical path in FIFOs, achieving higher operating frequency.\n  We implement the accelerator on an AMD Virtex UltraScale+ FPGA. Both Rubato and HERA achieve a 6x improvement in throughput compared to the software implementation. In terms of latency, Rubato achieves a 5x reduction, while HERA achieves a 3x reduction. Additionally, our hardware implementations reduce energy consumption by 75x for Rubato and 47x for HERA compared to their software implementation.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wireless AI Evolution: From Statistical Learners to Electromagnetic-Guided Foundation Models","url":"https://arxiv.org/abs/2507.00366","date":1751428800,"author":"","guid":180198,"unread":true,"content":"<article>arXiv:2507.00366v1 Announce Type: new \nAbstract: While initial applications of artificial intelligence (AI) in wireless communications over the past decade have demonstrated considerable potential using specialized models for targeted communication tasks, the revolutionary demands of sixth-generation (6G) networks for holographic communications, ubiquitous sensing, and native intelligence are propelling a necessary evolution towards AI-native wireless networks. The arrival of large AI models paves the way for the next phase of Wireless AI, driven by wireless foundation models (WFMs). In particular, pre-training on universal electromagnetic (EM) principles equips WFMs with the essential adaptability for a multitude of demanding 6G applications. However, existing large AI models face critical limitations, including pre-training strategies disconnected from EM-compliant constraints leading to physically inconsistent predictions, a lack of embedded understanding of wave propagation physics, and the inaccessibility of massive labeled datasets for comprehensive EM-aware training. To address these challenges, this article presents an electromagnetic information theory-guided self-supervised pre-training (EIT-SPT) framework designed to systematically inject EM physics into WFMs. The EIT-SPT framework aims to infuse WFMs with intrinsic EM knowledge, thereby enhancing their physical consistency, generalization capabilities across varied EM landscapes, and overall data efficiency. Building upon the proposed EIT-SPT framework, this article first elaborates on diverse potential applications in 6G scenarios of WFMs, then validates the efficacy of the proposed framework through illustrative case studies, and finally summarizes critical open research challenges and future directions for WFMs.</article>","contentLength":1807,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Improved U-Net Model for Offline handwriting signature denoising","url":"https://arxiv.org/abs/2507.00365","date":1751428800,"author":"","guid":180199,"unread":true,"content":"<article>arXiv:2507.00365v1 Announce Type: new \nAbstract: Handwriting signatures, as an important means of identity recognition, are widely used in multiple fields such as financial transactions, commercial contracts and personal affairs due to their legal effect and uniqueness. In forensic science appraisals, the analysis of offline handwriting signatures requires the appraiser to provide a certain number of signature samples, which are usually derived from various historical contracts or archival materials. However, the provided handwriting samples are often mixed with a large amount of interfering information, which brings severe challenges to handwriting identification work. This study proposes a signature handwriting denoising model based on the improved U-net structure, aiming to enhance the robustness of the signature recognition system. By introducing discrete wavelet transform and PCA transform, the model's ability to suppress noise has been enhanced. The experimental results show that this modelis significantly superior to the traditional methods in denoising effect, can effectively improve the clarity and readability of the signed images, and provide more reliable technical support for signature analysis and recognition.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control","url":"https://arxiv.org/abs/2507.00363","date":1751428800,"author":"","guid":180200,"unread":true,"content":"<article>arXiv:2507.00363v1 Announce Type: new \nAbstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023}, addressing challenges in initialization, optimization, and density control. Gaussian Splatting is an alternative for rendering realistic images while supporting real-time performance, and it has gained popularity due to its explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate initialization and faces difficulties in optimizing unstructured Gaussian distributions into ordered surfaces, with limited adaptive density control mechanism proposed so far. Our first key contribution is a geometry-guided initialization to predict Gaussian parameters, ensuring precise placement and faster convergence. We then introduce a surface-aligned optimization strategy to refine Gaussian placement, improving geometric accuracy and aligning with the surface normals of the scene. Finally, we present a dynamic adaptive density control mechanism that adjusts Gaussian density based on regional complexity, for visual fidelity. These innovations enable our method to achieve high-fidelity real-time rendering and significant improvements in visual quality, even in complex scenes. Our method demonstrates comparable or superior results to state-of-the-art methods, rendering high-fidelity images in real time.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems","url":"https://arxiv.org/abs/2507.00358","date":1751428800,"author":"","guid":180201,"unread":true,"content":"<article>arXiv:2507.00358v1 Announce Type: new \nAbstract: We study reinforcement learning (RL) for the same class of continuous-time stochastic linear--quadratic (LQ) control problems as in \\cite{huang2024sublinear}, where volatilities depend on both states and controls while states are scalar-valued and running control rewards are absent. We propose a model-free, data-driven exploration mechanism that adaptively adjusts entropy regularization by the critic and policy variance by the actor. Unlike the constant or deterministic exploration schedules employed in \\cite{huang2024sublinear}, which require extensive tuning for implementations and ignore learning progresses during iterations, our adaptive exploratory approach boosts learning efficiency with minimal tuning. Despite its flexibility, our method achieves a sublinear regret bound that matches the best-known model-free results for this class of LQ problems, which were previously derived only with fixed exploration schedules. Numerical experiments demonstrate that adaptive explorations accelerate convergence and improve regret performance compared to the non-adaptive model-free and model-based counterparts.</article>","contentLength":1169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation","url":"https://arxiv.org/abs/2507.00356","date":1751428800,"author":"","guid":180202,"unread":true,"content":"<article>arXiv:2507.00356v1 Announce Type: new \nAbstract: Deep learning methods have significantly advanced the development of intelligent rinterpretation in remote sensing (RS), with foundational model research based on large-scale pre-training paradigms rapidly reshaping various domains of Earth Observation (EO). However, compared to the open accessibility and high spatiotemporal coverage of medium-resolution data, the limited acquisition channels for ultra-high-resolution optical RS imagery have constrained the progress of high-resolution remote sensing vision foundation models (RSVFM). As the world's largest sub-meter-level commercial RS satellite constellation, the Jilin-1 constellation possesses abundant sub-meter-level image resources. This study proposes CGEarthEye, a RSVFM framework specifically designed for Jilin-1 satellite characteristics, comprising five backbones with different parameter scales with totaling 2.1 billion parameters. To enhance the representational capacity of the foundation model, we developed JLSSD, the first 15-million-scale multi-temporal self-supervised learning (SSL) dataset featuring global coverage with quarterly temporal sampling within a single year, constructed through multi-level representation clustering and sampling strategies. The framework integrates seasonal contrast, augmentation-based contrast, and masked patch token contrastive strategies for pre-training. Comprehensive evaluations across 10 benchmark datasets covering four typical RS tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art (SOTA) performance. Further analysis reveals CGEarthEye's superior characteristics in feature visualization, model convergence, parameter efficiency, and practical mapping applications. This study anticipates that the exceptional representation capabilities of CGEarthEye will facilitate broader and more efficient applications of Jilin-1 data in traditional EO application.</article>","contentLength":1950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Question Decomposition for Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2507.00355","date":1751428800,"author":"","guid":180203,"unread":true,"content":"<article>arXiv:2507.00355v1 Announce Type: new \nAbstract: Grounding large language models (LLMs) in verifiable external sources is a well-established strategy for generating reliable answers. Retrieval-augmented generation (RAG) is one such approach, particularly effective for tasks like question answering: it retrieves passages that are semantically related to the question and then conditions the model on this evidence. However, multi-hop questions, such as \"Which company among NVIDIA, Apple, and Google made the biggest profit in 2023?,\" challenge RAG because relevant facts are often distributed across multiple documents rather than co-occurring in one source, making it difficult for standard RAG to retrieve sufficient information. To address this, we propose a RAG pipeline that incorporates question decomposition: (i) an LLM decomposes the original query into sub-questions, (ii) passages are retrieved for each sub-question, and (iii) the merged candidate pool is reranked to improve the coverage and precision of the retrieved evidence. We show that question decomposition effectively assembles complementary documents, while reranking reduces noise and promotes the most relevant passages before answer generation. Although reranking itself is standard, we show that pairing an off-the-shelf cross-encoder reranker with LLM-driven question decomposition bridges the retrieval gap on multi-hop questions and provides a practical, drop-in enhancement, without any extra training or specialized indexing. We evaluate our approach on the MultiHop-RAG and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) over standard RAG baselines.</article>","contentLength":1674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmented Physics-Based Li-ion Battery Model via Adaptive Ensemble Sparse Learning and Conformal Prediction","url":"https://arxiv.org/abs/2507.00353","date":1751428800,"author":"","guid":180204,"unread":true,"content":"<article>arXiv:2507.00353v1 Announce Type: new \nAbstract: Accurate electrochemical models are essential for the safe and efficient operation of lithium-ion batteries in real-world applications such as electrified vehicles and grid storage. Reduced-order models (ROM) offer a balance between fidelity and computational efficiency but often struggle to capture complex and nonlinear behaviors, such as the dynamics in the cell voltage response under high C-rate conditions. To address these limitations, this study proposes an Adaptive Ensemble Sparse Identification (AESI) framework that enhances the accuracy of reduced-order li-ion battery models by compensating for unpredictable dynamics. The approach integrates an Extended Single Particle Model (ESPM) with an evolutionary ensemble sparse learning strategy to construct a robust hybrid model. In addition, the AESI framework incorporates a conformal prediction method to provide theoretically guaranteed uncertainty quantification for voltage error dynamics, thereby improving the reliability of the model's predictions. Evaluation across diverse operating conditions shows that the hybrid model (ESPM + AESI) improves the voltage prediction accuracy, achieving mean squared error reductions of up to 46% on unseen data. Prediction reliability is further supported by conformal prediction, yielding statistically valid prediction intervals with coverage ratios of 96.85% and 97.41% for the ensemble models based on bagging and stability selection, respectively.</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An AST-guided LLM Approach for SVRF Code Synthesis","url":"https://arxiv.org/abs/2507.00352","date":1751428800,"author":"","guid":180205,"unread":true,"content":"<article>arXiv:2507.00352v1 Announce Type: new \nAbstract: Standard Verification Rule Format (SVRF) is essential for semiconductor applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and Optical Proximity Correction (OPC) and it faces challenges as advancing nodes create complex design rules that renders traditional SVRF development ineffective and highlight an expertise gap. This paper introduces a novel methodology integrating Abstract Syntax Tree (AST) embedding and Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring semantic accuracy and error minimization through structural validation with domain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific scoring framework that complements standard metrics like BLEU and ROUGE-L. In our approach, AST provides rigorous structural validation, while RAG infuses relevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our methodology demonstrates up to a 40\\% improvement in code generation accuracy compared to basic text-based fine-tuning process. This fusion of industry expertise with advanced coding strategies not only optimizes SVRF development under limited dataset constraints but also creates a more intuitive and efficient coding environment. Consequently, users can rapidly iterate through design cycles, reduce manual error correction, and significantly improve overall productivity.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Addressing malware family concept drift with triplet autoencoder","url":"https://arxiv.org/abs/2507.00348","date":1751428800,"author":"","guid":180206,"unread":true,"content":"<article>arXiv:2507.00348v1 Announce Type: new \nAbstract: Machine learning is increasingly vital in cybersecurity, especially in malware detection. However, concept drift, where the characteristics of malware change over time, poses a challenge for maintaining the efficacy of these detection systems. Concept drift can occur in two forms: the emergence of entirely new malware families and the evolution of existing ones. This paper proposes an innovative method to address the former, focusing on effectively identifying new malware families. Our approach leverages a supervised autoencoder combined with triplet loss to differentiate between known and new malware families. We create clear and robust clusters that enhance the accuracy and resilience of malware family classification by utilizing this metric learning technique and the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm. The effectiveness of our method is validated using an Android malware dataset and a Windows portable executable (PE) malware dataset, showcasing its capability to sustain model performance within the dynamic landscape of emerging malware threats. Our results demonstrate a significant improvement in detecting new malware families, offering a reliable solution for ongoing cybersecurity challenges.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VTS-Guided AI Interaction Workflow for Business Insights","url":"https://arxiv.org/abs/2507.00347","date":1751428800,"author":"","guid":180207,"unread":true,"content":"<article>arXiv:2507.00347v1 Announce Type: new \nAbstract: Modern firms face a flood of dense, unstructured reports. Turning these documents into usable insights takes heavy effort and is far from agile when quick answers are needed. VTS-AI tackles this gap. It integrates Visual Thinking Strategies, which emphasize evidence-based observation, linking, and thinking, into AI agents, so the agents can extract business insights from unstructured text, tables, and images at scale. The system works in three tiers (micro, meso, macro). It tags issues, links them to source pages, and rolls them into clear action levers stored in a searchable YAML file. In tests on an 18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt yet produced richer findings: page locations, verbatim excerpts, severity scores, and causal links. Analysts can accept or adjust these outputs in the same IDE, keeping human judgment in the loop. Early results show VTS-AI spots the direction of key metrics and flags where deeper number-crunching is needed. Next steps include mapping narrative tags to financial ratios, adding finance-tuned language models through a Model-Context Protocol, and building a Risk &amp; Safety Layer to stress-test models and secure data. These upgrades aim to make VTS-AI a production-ready, audit-friendly tool for rapid business analysis.</article>","contentLength":1356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meaningful Data Erasure in the Presence of Dependencies","url":"https://arxiv.org/abs/2507.00343","date":1751428800,"author":"","guid":180208,"unread":true,"content":"<article>arXiv:2507.00343v1 Announce Type: new \nAbstract: Data regulations like GDPR require systems to support data erasure but leave the definition of \"erasure\" open to interpretation. This ambiguity makes compliance challenging, especially in databases where data dependencies can lead to erased data being inferred from remaining data. We formally define a precise notion of data erasure that ensures any inference about deleted data, through dependencies, remains bounded to what could have been inferred before its insertion. We design erasure mechanisms that enforce this guarantee at minimal cost. Additionally, we explore strategies to balance cost and throughput, batch multiple erasures, and proactively compute data retention times when possible. We demonstrate the practicality and scalability of our algorithms using both real and synthetic datasets.</article>","contentLength":855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video","url":"https://arxiv.org/abs/2507.00339","date":1751428800,"author":"","guid":180209,"unread":true,"content":"<article>arXiv:2507.00339v1 Announce Type: new \nAbstract: Amodal segmentation and amodal content completion require using object priors to estimate occluded masks and features of objects in complex scenes. Until now, no data has provided an additional dimension for object context: the possibility of multiple cameras sharing a view of a scene. We introduce MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the largest amodal segmentation and first amodal content dataset to date. Cluttered scenes of generic household objects are simulated in multi-camera video. MOVi-MC-AC contributes to the growing literature of object detection, tracking, and segmentation by including two new contributions to the deep learning for computer vision world. Multiple Camera (MC) settings where objects can be identified and tracked between various unique camera perspectives are rare in both synthetic and real-world video. We introduce a new complexity to synthetic video by providing consistent object ids for detections and segmentations between both frames and multiple cameras each with unique features and motion patterns on a single scene. Amodal Content (AC) is a reconstructive task in which models predict the appearance of target objects through occlusions. In the amodal segmentation literature, some datasets have been released with amodal detection, tracking, and segmentation labels. While other methods rely on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do not account for natural occlusions present in the modal masks. MOVi-MC-AC provides labels for ~5.8 million object instances, setting a new maximum in the amodal dataset literature, along with being the first to provide ground-truth amodal content. The full dataset is available at https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,</article>","contentLength":1837,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols","url":"https://arxiv.org/abs/2507.00337","date":1751428800,"author":"","guid":180210,"unread":true,"content":"<article>arXiv:2507.00337v1 Announce Type: new \nAbstract: Delay-based protocols rely on end-to-end delay measurements to detect network congestion. However, in cellular networks, Radio Access Network (RAN) buffers introduce significant delays unrelated to congestion, fundamentally challenging these protocols' assumptions. We identify two major types of RAN buffers - retransmission buffers and uplink scheduling buffers - that can introduce delays comparable to congestion-induced delays, severely degrading protocol performance. We present CellNinjia, a software-based system providing real-time visibility into RAN operations, and Gandalf, which leverages this visibility to systematically handle RAN-induced delays. Unlike existing approaches that treat these delays as random noise, Gandalf identifies specific RAN operations and compensates for their effects. Our evaluation in commercial 4G LTE and 5G networks shows that Gandalf enables substantial performance improvements - up to 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols' core algorithms, demonstrating that delay-based protocols can realize their full potential in cellular networks.</article>","contentLength":1169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Populate-A-Scene: Affordance-Aware Human Video Generation","url":"https://arxiv.org/abs/2507.00334","date":1751428800,"author":"","guid":180211,"unread":true,"content":"<article>arXiv:2507.00334v1 Announce Type: new \nAbstract: Can a video generation model be repurposed as an interactive world simulator? We explore the affordance perception potential of text-to-video models by teaching them to predict human-environment interaction. Given a scene image and a prompt describing human actions, we fine-tune the model to insert a person into the scene, while ensuring coherent behavior, appearance, harmonization, and scene affordance. Unlike prior work, we infer human affordance for video generation (i.e., where to insert a person and how they should behave) from a single scene image, without explicit conditions like bounding boxes or body poses. An in-depth study of cross-attention heatmaps demonstrates that we can uncover the inherent affordance perception of a pre-trained video model without labeled affordance datasets.</article>","contentLength":852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels","url":"https://arxiv.org/abs/2507.00333","date":1751428800,"author":"","guid":180212,"unread":true,"content":"<article>arXiv:2507.00333v1 Announce Type: new \nAbstract: Marksmanship practices are required in various professions, including police, military personnel, hunters, as well as sports shooters, such as Olympic shooting, biathlon, and modern pentathlon. The current form of training and coaching is mostly based on repetition, where the coach does not see through the eyes of the shooter, and analysis is limited to stance and accuracy post-session. In this study, we present a shooting visualization system and evaluate its perceived effectiveness for both novice and expert shooters. To achieve this, five composite visualizations were developed using first-person shooting video recordings enriched with overlaid metrics and graphical summaries. These views were evaluated with 10 participants (5 expert marksmen, 5 novices) through a mixed-methods study including shot-count and aiming interpretation tasks, pairwise preference comparisons, and semi-structured interviews. The results show that a dashboard-style composite view, combining raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases and supported understanding across skill levels. The insights gained from this design study point to the broader value of integrating first-person video with visual analytics for coaching, and we suggest directions for applying this approach to other precision-based sports.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios","url":"https://arxiv.org/abs/2507.00330","date":1751428800,"author":"","guid":180213,"unread":true,"content":"<article>arXiv:2507.00330v1 Announce Type: new \nAbstract: Prompt-based methods leverage the knowledge of pre-trained language models (PLMs) trained with a masked language modeling (MLM) objective; however, these methods are sensitive to template, verbalizer, and few-shot instance selection, particularly in cold-start settings with no labeled data. Existing studies overlook the dependency between instances and verbalizers, where instance-label probabilities depend on verbalizer token proximity in the embedding space. To address this, we propose COLDSELECT, a joint verbalizer and instance selection approach that models data diversity. COLDSELECT maps PLM vocabulary and $h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction and clustering to ensure efficient and diverse selection. By optimizing for minimal uncertainty and maximal diversity, COLDSELECT captures data relationships effectively. Experiments on eight benchmarks demonstrate COLDSELECT's superiority in reducing uncertainty and enhancing generalization, outperforming baselines in verbalizer and few-shot instance selection for cold-start scenarios.</article>","contentLength":1135,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms","url":"https://arxiv.org/abs/2507.00328","date":1751428800,"author":"","guid":180214,"unread":true,"content":"<article>arXiv:2507.00328v1 Announce Type: new \nAbstract: Accurate lesion tracking in temporal mammograms is essential for monitoring breast cancer progression and facilitating early diagnosis. However, automated lesion correspondence across exams remains a challenges in computer-aided diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker, a mask-guided lesion tracking framework that automates lesion localization across consecutively exams. Our approach follows a coarse-to-fine strategy incorporating three key modules: global search, local search, and score refinement. To support large-scale training and evaluation, we introduce a new dataset with curated prior-exam annotations for 730 mass and calcification cases from the public EMBED mammogram dataset, yielding over 20000 lesion pairs, making it the largest known resource for temporal lesion tracking in mammograms. Experimental results demonstrate that MammoTracker achieves 0.455 average overlap and 0.509 accuracy, surpassing baseline models by 8%, highlighting its potential to enhance CAD-based lesion progression analysis. Our dataset will be available at https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.</article>","contentLength":1199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes","url":"https://arxiv.org/abs/2507.00327","date":1751428800,"author":"","guid":180215,"unread":true,"content":"<article>arXiv:2507.00327v1 Announce Type: new \nAbstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational costs while maintaining performance comparable to fully fine-tuned foundation models across various tasks. However, its fixed low-rank structure restricts its adaptability in scenarios with substantial domain gaps, where higher ranks are often required to capture domain-specific complexities. Current adaptive LoRA methods attempt to overcome this limitation by dynamically expanding or selectively allocating ranks, but these approaches frequently depend on computationally intensive techniques such as iterative pruning, rank searches, or additional regularization. To address these challenges, we introduce Stable Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the stable rank of pre-trained weight matrices as a natural prior for layer-wise rank allocation. By leveraging the stable rank, which reflects the intrinsic dimensionality of the weights, SR-LoRA enables a principled and efficient redistribution of ranks across layers, enhancing adaptability without incurring additional search costs. Empirical evaluations on few-shot tasks with significant domain gaps show that SR-LoRA consistently outperforms recent adaptive LoRA variants, achieving a superior trade-off between performance and efficiency. Our code is available at https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.</article>","contentLength":1445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones","url":"https://arxiv.org/abs/2507.00322","date":1751428800,"author":"","guid":180216,"unread":true,"content":"<article>arXiv:2507.00322v1 Announce Type: new \nAbstract: Despite remarkable advances in coding capabilities, language models (LMs) still struggle with simple syntactic tasks such as generating balanced parentheses. In this study, we investigate the underlying mechanisms behind the persistence of these errors across LMs of varying sizes (124M-7B) to both understand and mitigate the errors. Our study reveals that LMs rely on a number of components (attention heads and FF neurons) that independently make their own predictions. While some components reliably promote correct answers across a generalized range of inputs (i.e., implementing \"sound mechanisms''), others are less reliable and introduce noise by promoting incorrect tokens (i.e., implementing \"faulty mechanisms''). Errors occur when the faulty mechanisms overshadow the sound ones and dominantly affect the predictions. Motivated by this insight, we introduce RASteer, a steering method to systematically identify and increase the contribution of reliable components for improving model performance. RASteer substantially improves performance on balanced parentheses tasks, boosting accuracy of some models from $0$% to around $100$% without impairing the models' general coding ability. We further demonstrate its broader applicability in arithmetic reasoning tasks, achieving performance gains of up to around $20$%.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience","url":"https://arxiv.org/abs/2507.00320","date":1751428800,"author":"","guid":180217,"unread":true,"content":"<article>arXiv:2507.00320v1 Announce Type: new \nAbstract: In the science of emotion, it is widely assumed that folk emotion categories form a biological and psychological typology, and studies are routinely designed and analyzed to identify emotion-specific patterns. This approach shapes the observations that studies report, ultimately reinforcing the assumption that guided the investigation. Here, we reanalyzed data from one such typologically-guided study that reported mappings between individual brain patterns and group-averaged ratings of 34 emotion categories. Our reanalysis was guided by an alternative view of emotion categories as populations of variable, situated instances, and which predicts a priori that there will be significant variation in brain patterns within a category across instances. Correspondingly, our analysis made minimal assumptions about the structure of the variance present in the data. As predicted, we did not observe the original mappings and instead observed significant variation across individuals. These findings demonstrate how starting assumptions can ultimately impact scientific conclusions and suggest that a hypothesis must be supported using multiple analytic methods before it is taken seriously.</article>","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving","url":"https://arxiv.org/abs/2507.00319","date":1751428800,"author":"","guid":180218,"unread":true,"content":"<article>arXiv:2507.00319v1 Announce Type: new \nAbstract: Simulation frameworks have been key enablers for the development and validation of autonomous driving systems. However, existing methods struggle to comprehensively address the autonomy-oriented requirements of balancing: (i) dynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant scenario orchestration, and (iv) real-time performance. To address these limitations, we present a unified framework for creating and curating high-fidelity digital twins to accelerate advancements in autonomous driving research. Our framework leverages a mix of physics-based and data-driven techniques for developing and simulating digital twins of autonomous vehicles and their operating environments. It is capable of reconstructing real-world scenes and assets (real2sim) with geometric and photorealistic accuracy and infusing them with various physical properties to enable real-time dynamical simulation of the ensuing driving scenarios. Additionally, it also incorporates a large language model (LLM) interface to flexibly edit the driving scenarios online via natural language prompts. We analyze the presented framework in terms of its fidelity, performance, and serviceability. Results indicate that our framework can reconstruct 3D scenes and assets with up to 97% structural similarity, while maintaining frame rates above 60 Hz. We also demonstrate that it can handle natural language prompts to generate diverse driving scenarios with up to 95% repeatability and 85% generalizability.</article>","contentLength":1549,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"${\\mu}^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation","url":"https://arxiv.org/abs/2507.00316","date":1751428800,"author":"","guid":180219,"unread":true,"content":"<article>arXiv:2507.00316v1 Announce Type: new \nAbstract: Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale $\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The novel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasetdemonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited data for RRG tasks.</article>","contentLength":1177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Open-ended Scientific Discovery via Bayesian Surprise","url":"https://arxiv.org/abs/2507.00310","date":1751428800,"author":"","guid":180220,"unread":true,"content":"<article>arXiv:2507.00310v1 Announce Type: new \nAbstract: The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hypothesis generation. However, scientific discovery may be accelerated further by allowing the AI system to drive exploration by its own criteria. The few existing approaches in open-ended ASD select hypotheses based on diversity heuristics or subjective proxies for human interestingness, but the former struggles to meaningfully navigate the typically vast hypothesis space, and the latter suffers from imprecise definitions. This paper presents AutoDS -- a method for open-ended ASD that instead drives scientific exploration using Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior beliefs about a hypothesis to its posterior beliefs after gathering experimental results. To efficiently explore the space of nested hypotheses, our method employs a Monte Carlo tree search (MCTS) strategy with progressive widening using surprisal as the reward function. We evaluate AutoDS in the setting of data-driven discovery across 21 real-world datasets spanning domains such as biology, economics, finance, and behavioral science. Our results demonstrate that under a fixed budget, AutoDS substantially outperforms competitors by producing 5--29\\% more discoveries deemed surprising by the LLM. Our human evaluation further finds that two-thirds of AutoDS discoveries are surprising to the domain experts, suggesting this is an important step forward towards building open-ended ASD systems.</article>","contentLength":1752,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Origin-Destination Travel Demand Estimation: An Approach That Scales Worldwide, and Its Application to Five Metropolitan Highway Networks","url":"https://arxiv.org/abs/2507.00306","date":1751428800,"author":"","guid":180221,"unread":true,"content":"<article>arXiv:2507.00306v1 Announce Type: new \nAbstract: Estimating Origin-Destination (OD) travel demand is vital for effective urban planning and traffic management. Developing universally applicable OD estimation methodologies is significantly challenged by the pervasive scarcity of high-fidelity traffic data and the difficulty in obtaining city-specific prior OD estimates (or seed ODs), which are often prerequisite for traditional approaches. Our proposed method directly estimates OD travel demand by systematically leveraging aggregated, anonymized statistics from Google Maps Traffic Trends, obviating the need for conventional census or city-provided OD data. The OD demand is estimated by formulating a single-level, one-dimensional, continuous nonlinear optimization problem with nonlinear equality and bound constraints to replicate highway path travel times. The method achieves efficiency and scalability by employing a differentiable analytical macroscopic network model. This model by design is computationally lightweight, distinguished by its parsimonious parameterization that requires minimal calibration effort and its capacity for instantaneous evaluation. These attributes ensure the method's broad applicability and practical utility across diverse cities globally. Using segment sensor counts from Los Angeles and San Diego highway networks, we validate our proposed approach, demonstrating a two-thirds to three-quarters improvement in the fit to segment count data over a baseline. Beyond validation, we establish the method's scalability and robust performance in replicating path travel times across diverse highway networks, including Seattle, Orlando, Denver, Philadelphia, and Boston. In these expanded evaluations, our method not only aligns with simulation-based benchmarks but also achieves an average 13% improvement in it's ability to fit travel time data compared to the baseline during afternoon peak hours.</article>","contentLength":1941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation","url":"https://arxiv.org/abs/2507.00305","date":1751428800,"author":"","guid":180222,"unread":true,"content":"<article>arXiv:2507.00305v1 Announce Type: new \nAbstract: Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in state (CLIS) can lose all reliable motor control and are left without any means of communication. It remains unknown whether non-invasive electroencephalogram (EEG) based brain-computer interfaces (BCIs) can support volitional communication in CLIS. Here, we show that a CLIS patient was able to operate an EEG-based BCI across multiple online sessions to respond to both general knowledge and personally relevant assistive questions. The patient delivered \"Yes\"/\"No\" responses by volitionally modulating alpha and beta band power at different channels, guided by real-time auditory feedback from the BCI. The patient communicated assistive needs above chance in all sessions, achieving a perfect score in the final session. Performance on general knowledge questions varied across sessions, with two sessions showing accurate and above-chance responses, while the first and last sessions remained at chance level. The patient also showed consistent modulation patterns over time. These findings suggest that non-invasive BCIs may offer a potential pathway for restoring basic communication in CLIS.</article>","contentLength":1226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic","url":"https://arxiv.org/abs/2507.00304","date":1751428800,"author":"","guid":180223,"unread":true,"content":"<article>arXiv:2507.00304v1 Announce Type: new \nAbstract: The abnormal fluctuations in network traffic may indicate potential security threats or system failures. Therefore, efficient network traffic prediction and anomaly detection methods are crucial for network security and traffic management. This paper proposes a novel network traffic prediction and anomaly detection model, MamNet, which integrates time-domain modeling and frequency-domain feature extraction. The model first captures the long-term dependencies of network traffic through the Mamba module (time-domain modeling), and then identifies periodic fluctuations in the traffic using Fourier Transform (frequency-domain feature extraction). In the feature fusion layer, multi-scale information is integrated to enhance the model's ability to detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and CAIDA datasets demonstrate that MamNet outperforms several recent mainstream models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an improvement of approximately 2% to 4% in detection performance for complex traffic patterns and long-term trend detection. The results indicate that MamNet effectively captures anomalies in network traffic across different time scales and is suitable for anomaly detection tasks in network security and traffic management. Future work could further optimize the model structure by incorporating external network event information, thereby improving the model's adaptability and stability in complex network environments.</article>","contentLength":1555,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations","url":"https://arxiv.org/abs/2507.00301","date":1751428800,"author":"","guid":180224,"unread":true,"content":"<article>arXiv:2507.00301v1 Announce Type: new \nAbstract: This work presents structure-preserving Lift &amp; Learn, a scientific machine learning method that employs lifting variable transformations to learn structure-preserving reduced-order models for nonlinear partial differential equations (PDEs) with conservation laws. We propose a hybrid learning approach based on a recently developed energy-quadratization strategy that uses knowledge of the nonlinearity at the PDE level to derive an equivalent quadratic lifted system with quadratic system energy. The lifted dynamics obtained via energy quadratization are linear in the old variables, making model learning very effective in the lifted setting. Based on the lifted quadratic PDE model form, the proposed method derives quadratic reduced terms analytically and then uses those derived terms to formulate a constrained optimization problem to learn the remaining linear reduced operators in a structure-preserving way. The proposed hybrid learning approach yields computationally efficient quadratic reduced-order models that respect the underlying physics of the high-dimensional problem. We demonstrate the generalizability of quadratic models learned via the proposed structure-preserving Lift &amp; Learn method through three numerical examples: the one-dimensional wave equation with exponential nonlinearity, the two-dimensional sine-Gordon equation, and the two-dimensional Klein-Gordon-Zakharov equations. The numerical results show that the proposed learning approach is competitive with the state-of-the-art structure-preserving data-driven model reduction method in terms of both accuracy and computational efficiency.</article>","contentLength":1673,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Kids Mode Isn't For Kids: Investigating TikTok's \"Under 13 Experience\"","url":"https://arxiv.org/abs/2507.00299","date":1751428800,"author":"","guid":180225,"unread":true,"content":"<article>arXiv:2507.00299v1 Announce Type: new \nAbstract: TikTok, the social media platform that is popular among children and adolescents, offers a more restrictive \"Under 13 Experience\" exclusively for young users in the US, also known as TikTok's \"Kids Mode\". While prior research has studied various aspects of TikTok's regular mode, including privacy and personalization, TikTok's Kids Mode remains understudied, and there is a lack of transparency regarding its content curation and its safety and privacy protections for children. In this paper, (i) we propose an auditing methodology to comprehensively investigate TikTok's Kids Mode and (ii) we apply it to characterize the platform's content curation and determine the prevalence of child-directed content, based on regulations in the Children's Online Privacy Protection Act (COPPA). We find that 83% of videos observed on the \"For You\" page in Kids Mode are actually not child-directed, and even inappropriate content was found. The platform also lacks critical features, namely parental controls and accessibility settings. Our findings have important design and regulatory implications, as children may be incentivized to use TikTok's regular mode instead of Kids Mode, where they are known to be exposed to further safety and privacy risks.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Natural language processing for African languages","url":"https://arxiv.org/abs/2507.00297","date":1751428800,"author":"","guid":180226,"unread":true,"content":"<article>arXiv:2507.00297v1 Announce Type: new \nAbstract: Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.</article>","contentLength":1716,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reducing Variability of Multiple Instance Learning Methods for Digital Pathology","url":"https://arxiv.org/abs/2507.00292","date":1751428800,"author":"","guid":180227,"unread":true,"content":"<article>arXiv:2507.00292v1 Announce Type: new \nAbstract: Digital pathology has revolutionized the field by enabling the digitization of tissue samples into whole slide images (WSIs). However, the high resolution and large size of WSIs present significant challenges when it comes to applying Deep Learning models. As a solution, WSIs are often divided into smaller patches with a global label (\\textit{i.e., diagnostic}) per slide, instead of a (too) costly pixel-wise annotation. By treating each slide as a bag of patches, Multiple Instance Learning (MIL) methods have emerged as a suitable solution for WSI classification. A major drawback of MIL methods is their high variability in performance across different runs, which can reach up to 10-15 AUC points on the test set, making it difficult to compare different MIL methods reliably. This variability mainly comes from three factors: i) weight initialization, ii) batch (shuffling) ordering, iii) and learning rate. To address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL methods. We first train multiple models for a few epochs and average the most stable and promising ones based on validation scores. This approach can be applied to any existing MIL model to reduce performance variability. It also simplifies hyperparameter tuning and improves reproducibility while maintaining computational efficiency. We extensively validate our approach on WSI classification tasks using 2 different datasets, 3 initialization strategies and 5 MIL methods, for a total of more than 2000 experiments.</article>","contentLength":1561,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Supervised Multiview Xray Matching","url":"https://arxiv.org/abs/2507.00287","date":1751428800,"author":"","guid":180228,"unread":true,"content":"<article>arXiv:2507.00287v1 Announce Type: new \nAbstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing fractures, muscular injuries, and other anomalies. While significant advances have been made in AI-based analysis of single images, current methods often struggle to establish robust correspondences between different X-ray views, an essential capability for precise clinical evaluations. In this work, we present a novel self-supervised pipeline that eliminates the need for manual annotation by automatically generating a many-to-many correspondence matrix between synthetic X-ray views. This is achieved using digitally reconstructed radiographs (DRR), which are automatically derived from unannotated CT volumes. Our approach incorporates a transformer-based training phase to accurately predict correspondences across two or more X-ray views. Furthermore, we demonstrate that learning correspondences among synthetic X-ray views can be leveraged as a pretraining strategy to enhance automatic multi-view fracture detection on real data. Extensive evaluations on both synthetic and real X-ray datasets show that incorporating correspondences improves performance in multi-view fracture classification.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Visual Privacy Management with Generative AI for Blind and Low-Vision People","url":"https://arxiv.org/abs/2507.00286","date":1751428800,"author":"","guid":180229,"unread":true,"content":"<article>arXiv:2507.00286v1 Announce Type: new \nAbstract: Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to interpret and manage visual content in their daily lives. While such tools can enhance the accessibility of visual content and so enable greater user independence, they also introduce complex challenges around visual privacy. In this paper, we investigate the current practices and future design preferences of blind and low vision individuals through an interview study with 21 participants. Our findings reveal a range of current practices with GenAI that balance privacy, efficiency, and emotional agency, with users accounting for privacy risks across six key scenarios, such as self-presentation, indoor/outdoor spatial privacy, social sharing, and handling professional content. Our findings reveal design preferences, including on-device processing, zero-retention guarantees, sensitive content redaction, privacy-aware appearance indicators, and multimodal tactile mirrored interaction methods. We conclude with actionable design recommendations to support user-centered visual privacy through GenAI, expanding the notion of privacy and responsible handling of others data.</article>","contentLength":1204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automatic discovery of optimal meta-solvers for time-dependent nonlinear PDEs","url":"https://arxiv.org/abs/2507.00278","date":1751428800,"author":"","guid":180230,"unread":true,"content":"<article>arXiv:2507.00278v1 Announce Type: new \nAbstract: We present a general and scalable framework for the automated discovery of optimal meta-solvers for the solution of time-dependent nonlinear partial differential equations after appropriate discretization. By integrating classical numerical methods (e.g., Krylov-based methods) with modern deep learning components, such as neural operators, our approach enables flexible, on-demand solver design tailored to specific problem classes and objectives. The fast solvers tackle the large linear system resulting from the Newton--Raphson iteration or by using an implicit-explicit (IMEX) time integration scheme. Specifically, we formulate solver discovery as a multi-objective optimization problem, balancing various performance criteria such as accuracy, speed, and memory usage. The resulting Pareto optimal set provides a principled foundation for solver selection based on user-defined preference functions. When applied to problems in reaction--diffusion, fluid dynamics, and solid mechanics, the discovered meta-solvers consistently outperform conventional iterative methods, demonstrating both practical efficiency and broad applicability.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lazy B-Trees","url":"https://arxiv.org/abs/2507.00277","date":1751428800,"author":"","guid":180231,"unread":true,"content":"<article>arXiv:2507.00277v1 Announce Type: new \nAbstract: Lazy search trees (Sandlund &amp; Wild FOCS 2020, Sandlund &amp; Zhang SODA 2022) are sorted dictionaries whose update and query performance smoothly interpolates between that of efficient priority queues and binary search trees - automatically, depending on actual use; no adjustments are necessary to the data structure to realize the cost savings. In this paper, we design lazy B-trees, a variant of lazy search trees suitable for external memory that generalizes the speedup of B-trees over binary search trees wrt. input/output operations to the same smooth interpolation regime.\n  A key technical difficulty to overcome is the lack of a (fully satisfactory) external variant of biased search trees, on which lazy search trees crucially rely. We give a construction for a subset of performance guarantees sufficient to realize external-memory lazy search trees, which we deem of independent interest.\n  As one special case, lazy B-trees can be used as an external-memory priority queue, in which case they are competitive with some tailor-made heaps; indeed, they offer faster decrease-key and insert operations than known data structures.</article>","contentLength":1185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Double Q-learning for Value-based Deep Reinforcement Learning, Revisited","url":"https://arxiv.org/abs/2507.00275","date":1751428800,"author":"","guid":180232,"unread":true,"content":"<article>arXiv:2507.00275v1 Announce Type: new \nAbstract: Overestimation is pervasive in reinforcement learning (RL), including in Q-learning, which forms the algorithmic basis for many value-based deep RL algorithms. Double Q-learning is an algorithm introduced to address Q-learning's overestimation by training two Q-functions and using both to de-correlate action-selection and action-evaluation in bootstrap targets. Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks (DQN), Double Q-learning was adapted to deep RL in the form of Double DQN. However, Double DQN only loosely adapts Double Q-learning, forgoing the training of two different Q-functions that bootstrap off one another. In this paper, we study algorithms that adapt this core idea of Double Q-learning for value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our aim is to understand whether DDQL exhibits less overestimation than Double DQN and whether performant instantiations of DDQL exist. We answer both questions affirmatively, demonstrating that DDQL reduces overestimation and outperforms Double DQN in aggregate across 57 Atari 2600 games, without requiring additional hyperparameters. We also study several aspects of DDQL, including its network architecture, replay ratio, and minibatch sampling strategy.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation","url":"https://arxiv.org/abs/2507.00273","date":1751428800,"author":"","guid":180233,"unread":true,"content":"<article>arXiv:2507.00273v1 Announce Type: new \nAbstract: Reinforcement learning (RL) has enabled significant advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot featuring three distinct parallel mechanisms in its legs: a differential pulley, a 5-bar linkage, and a 4-bar linkage. Unlike prior approaches that rely on simplified serial approximations, we simulate all closed-chain constraints natively using GPU-accelerated MJX (MuJoCo), preserving the hardware's physical properties during training. We benchmark our RL approach against a Model Predictive Controller (MPC), demonstrating better surface generalization and performance in real-world zero-shot deployment. This work highlights the computational approaches and performance benefits of fully simulating parallel mechanisms in end-to-end learning pipelines for legged humanoids.</article>","contentLength":1231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Iteratively Saturated Kalman Filtering","url":"https://arxiv.org/abs/2507.00272","date":1751428800,"author":"","guid":180234,"unread":true,"content":"<article>arXiv:2507.00272v1 Announce Type: new \nAbstract: The Kalman filter (KF) provides optimal recursive state estimates for linear-Gaussian systems and underpins applications in control, signal processing, and others. However, it is vulnerable to outliers in the measurements and process noise. We introduce the iteratively saturated Kalman filter (ISKF), which is derived as a scaled gradient method for solving a convex robust estimation problem. It achieves outlier robustness while preserving the KF's low per-step cost and implementation simplicity, since in practice it typically requires only one or two iterations to achieve good performance. The ISKF also admits a steady-state variant that, like the standard steady-state KF, does not require linear system solves in each time step, making it well-suited for real-time systems.</article>","contentLength":832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the \"Sunday Blues\"","url":"https://arxiv.org/abs/2507.00271","date":1751428800,"author":"","guid":180235,"unread":true,"content":"<article>arXiv:2507.00271v1 Announce Type: new \nAbstract: While recent research highlights the potential of social robots to support mood regulation, little is known about how prospective users view their integration into everyday life. To explore this, we conducted an exploratory case study that used a speculative robot concept \"Mora\" to provoke reflection and facilitate meaningful discussion about using social robots to manage subtle, day-to-day emotional experiences. We focused on the \"Sunday Blues,\" a common dip in mood that occurs at the end of the weekend, as a relatable context in which to explore individuals' insights. Using a video prototype and a co-constructing stories method, we engaged 15 participants in imagining interactions with Mora and discussing their expectations, doubts, and concerns. The study surfaced a range of nuanced reflections around the attributes of social robots like empathy, intervention effectiveness, and ethical boundaries, which we translated into design considerations for future research and development in human-robot interaction.</article>","contentLength":1073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EMSpice 2.1: A Coupled EM and IR Drop Analysis Tool with Joule Heating and Thermal Map Integration for VLSI Reliability","url":"https://arxiv.org/abs/2507.00270","date":1751428800,"author":"","guid":180236,"unread":true,"content":"<article>arXiv:2507.00270v1 Announce Type: new \nAbstract: Electromigration (EM) remains a critical reliability concern in current and future copper-based VLSI circuits. As technology scales down, EM-induced IR drop becomes increasingly severe. While several EM-aware IR drop analysis tools have been proposed, few incorporate the real impact of temperature distribution on both EM and IR drop effects. In this work, we introduce EMSpice 2.1, an enhanced tool built upon the existing coupled IR-EM analysis framework, EMSpice 2.0, for EM-aware IR drop analysis. For the first time, EMSpice 2.1 uniquely integrates Joule heating effects and practical thermal maps derived from actual chip conditions. Additionally, it features improved interoperability with commercial EDA tools, facilitating more comprehensive EM and IR drop sign-off analysis. Our findings demonstrate that specific hotspot patterns significantly impact the lifetime of interconnects and overall chip reliability due to EM failures. Furthermore, our tool exhibits strong agreement with industry-standard tools such as COMSOL, achieving a speedup of over 200 times while maintaining high accuracy.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems","url":"https://arxiv.org/abs/2507.00268","date":1751428800,"author":"","guid":180237,"unread":true,"content":"<article>arXiv:2507.00268v1 Announce Type: new \nAbstract: Deep reinforcement learning (DRL) has become a powerful tool for complex decision-making in machine learning and AI. However, traditional methods often assume perfect action execution, overlooking the uncertainties and deviations between an agent's selected actions and the actual system response. In real-world applications, such as robotics, mechatronics, and communication networks, execution mismatches arising from system dynamics, hardware constraints, and latency can significantly degrade performance. This work advances AI by developing a novel control-optimized DRL framework that explicitly models and compensates for action execution mismatches, a challenge largely overlooked in existing methods. Our approach establishes a structured two-stage process: determining the desired action and selecting the appropriate control signal to ensure proper execution. It trains the agent while accounting for action mismatches and controller corrections. By incorporating these factors into the training process, the AI agent optimizes the desired action with respect to both the actual control signal and the intended outcome, explicitly considering execution errors. This approach enhances robustness, ensuring that decision-making remains effective under real-world uncertainties. Our approach offers a substantial advancement for engineering practice by bridging the gap between idealized learning and real-world implementation. It equips intelligent agents operating in engineering environments with the ability to anticipate and adjust for actuation errors and system disturbances during training. We evaluate the framework in five widely used open-source mechanical simulation environments we restructured and developed to reflect real-world operating conditions, showcasing its robustness against uncertainties and offering a highly practical and efficient solution for control-oriented applications.</article>","contentLength":1960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minimal residual rational Krylov subspace method for sequences of shifted linear systems","url":"https://arxiv.org/abs/2507.00267","date":1751428800,"author":"","guid":180238,"unread":true,"content":"<article>arXiv:2507.00267v1 Announce Type: new \nAbstract: The solution of sequences of shifted linear systems is a classic problem in numerical linear algebra, and a variety of efficient methods have been proposed over the years. Nevertheless, there still exist challenging scenarios witnessing a lack of performing solvers. For instance, state-of-the-art procedures struggle to handle nonsymmetric problems where the shifts are complex numbers that do not come as conjugate pairs. We design a novel projection strategy based on the rational Krylov subspace equipped with a minimal residual condition. We also devise a novel pole selection procedure, tailored to our problem, providing poles for the rational Krylov basis construction that yield faster convergence than those computed by available general-purpose schemes. A panel of diverse numerical experiments shows that our novel approach performs better than state-of-the-art techniques, especially on the very challenging problems mentioned above.</article>","contentLength":995,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Examining Reject Relations in Stimulus Equivalence Simulations","url":"https://arxiv.org/abs/2507.00265","date":1751428800,"author":"","guid":180239,"unread":true,"content":"<article>arXiv:2507.00265v1 Announce Type: new \nAbstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE), yet the potential of reject relations to disrupt the assessment of equivalence class formation is contentious. This study investigates the role of reject relations in the acquisition of stimulus equivalence using computational models. We examined feedforward neural networks (FFNs), bidirectional encoder representations from transformers (BERT), and generative pre-trained transformers (GPT) across 18 conditions in matching-to-sample (MTS) simulations. Conditions varied in training structure (linear series, one-to-many, and many-to-one), relation type (select-only, reject-only, and select-reject), and negative comparison selection (standard and biased). A probabilistic agent served as a benchmark, embodying purely associative learning. The primary goal was to determine whether artificial neural networks could demonstrate equivalence class formation or whether their performance reflected associative learning. Results showed that reject relations influenced agent performance. While some agents achieved high accuracy on equivalence tests, particularly with reject relations and biased negative comparisons, this performance was comparable to the probabilistic agent. These findings suggest that artificial neural networks, including transformer models, may rely on associative strategies rather than SE. This underscores the need for careful consideration of reject relations and more stringent criteria in computational models of equivalence.</article>","contentLength":1578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust vs. C for Python Libraries: Evaluating Rust-Compatible Bindings Toolchains","url":"https://arxiv.org/abs/2507.00264","date":1751428800,"author":"","guid":180240,"unread":true,"content":"<article>arXiv:2507.00264v1 Announce Type: new \nAbstract: The Python programming language is best known for its syntax and scientific libraries, but it is also notorious for its slow interpreter. Optimizing critical sections in Python entails special knowledge of the binary interactions between programming languages, and can be cumbersome to interface manually, with implementers often resorting to convoluted third-party libraries. This comparative study evaluates the performance and ease of use of the PyO3 Python bindings toolchain for Rust against ctypes and cffi. By using Rust tooling developed for Python, we can achieve state-of-the-art performance with no concern for API compatibility.</article>","contentLength":689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections","url":"https://arxiv.org/abs/2507.00263","date":1751428800,"author":"","guid":180241,"unread":true,"content":"<article>arXiv:2507.00263v1 Announce Type: new \nAbstract: The rapid growth of vacation rental (VR) platforms has led to an increasing volume of property images, often uploaded without structured categorization. This lack of organization poses significant challenges for travelers attempting to understand the spatial layout of a property, particularly when multiple rooms of the same type are present. To address this issue, we introduce an effective approach for solving the room scene discovery and grouping problem, as well as identifying bed types within each bedroom group. This grouping is valuable for travelers to comprehend the spatial organization, layout, and the sleeping configuration of the property. We propose a computationally efficient machine learning pipeline characterized by low latency and the ability to perform effectively with sample-efficient learning, making it well-suited for real-time and data-scarce environments. The pipeline integrates a supervised room-type detection model, a supervised overlap detection model to identify the overlap similarity between two images, and a clustering algorithm to group the images of the same space together using the similarity scores. Additionally, the pipeline maps each bedroom group to the corresponding bed types specified in the property's metadata, based on the visual content present in the group's images using a Multi-modal Large Language Model (MLLM) model. We evaluate the aforementioned models individually and also assess the pipeline in its entirety, observing strong performance that significantly outperforms established approaches such as contrastive learning and clustering with pretrained embeddings.</article>","contentLength":1680,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos","url":"https://arxiv.org/abs/2507.00261","date":1751428800,"author":"","guid":180242,"unread":true,"content":"<article>arXiv:2507.00261v1 Announce Type: new \nAbstract: Fencing is a sport where athletes engage in diverse yet strategically logical motions. While most motions fall into a few high-level actions (e.g. step, lunge, parry), the execution can vary widely-fast vs. slow, large vs. small, offensive vs. defensive. Moreover, a fencer's actions are informed by a strategy that often comes in response to the opponent's behavior. This combination of motion diversity with underlying two-player strategy motivates the application of data-driven modeling to fencing. We present VirtualFencer, a system capable of extracting 3D fencing motion and strategy from in-the-wild video without supervision, and then using that extracted knowledge to generate realistic fencing behavior. We demonstrate the versatile capabilities of our system by having it (i) fence against itself (self-play), (ii) fence against a real fencer's motion from online video, and (iii) fence interactively against a professional fencer.</article>","contentLength":992,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning","url":"https://arxiv.org/abs/2507.00259","date":1751428800,"author":"","guid":180243,"unread":true,"content":"<article>arXiv:2507.00259v1 Announce Type: new \nAbstract: Data heterogeneity is a central challenge in federated learning, and personalized federated learning (PFL) aims to address it by tailoring models to each client's distribution. Yet many PFL methods fail to outperform local or centralized baselines, suggesting a mismatch between the collaboration they enforce and the structure of the data. We propose an approach based on adaptive collaboration, where clients decide adaptively not only how much to rely on others, but also whom to trust at the level of individual examples. We instantiate this principle in FEDMOSAIC, a federated co-training method in which clients exchange predictions over a shared unlabeled dataset. This enables fine-grained trust decisions that are difficult to achieve with parameter sharing alone. Each client adjusts its loss weighting based on the agreement between private and public data, and contributes to global pseudo-labels in proportion to its estimated per-example confidence. Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across diverse non-IID settings, and we provide convergence guarantees under standard assumptions. Our results demonstrate the potential of data-aware collaboration for robust and effective personalization.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Impact of Fine-Tuning Methods on Memorization in Large Language Models","url":"https://arxiv.org/abs/2507.00258","date":1751428800,"author":"","guid":180244,"unread":true,"content":"<article>arXiv:2507.00258v1 Announce Type: new \nAbstract: As the capabilities of pre-trained large language models (LLMs) continue to advance, the \"pre-train and fine-tune\" paradigm has become increasingly mainstream, leading to the development of various fine-tuning methods. However, the privacy risks arising from memorization during fine-tuning have received relatively little attention. To address this gap, we categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs). Our results show that, compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale. These findings suggest that parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option.</article>","contentLength":979,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning","url":"https://arxiv.org/abs/2507.00257","date":1751428800,"author":"","guid":180245,"unread":true,"content":"<article>arXiv:2507.00257v1 Announce Type: new \nAbstract: In recent years, \\emph{Reinforcement Learning} (RL) has made remarkable progress, achieving superhuman performance in a wide range of simulated environments. As research moves toward deploying RL in real-world applications, the field faces a new set of challenges inherent to real-world settings, such as large state-action spaces, non-stationarity, and partial observability. Despite their importance, these challenges are often underexplored in current benchmarks, which tend to focus on idealized, fully observable, and stationary environments, often neglecting to incorporate real-world complexities explicitly. In this paper, we introduce \\texttt{Gym4ReaL}, a comprehensive suite of realistic environments designed to support the development and evaluation of RL algorithms that can operate in real-world scenarios. The suite includes a diverse set of tasks that expose algorithms to a variety of practical challenges. Our experimental results show that, in these settings, standard RL algorithms confirm their competitiveness against rule-based benchmarks, motivating the development of new methods to fully exploit the potential of RL to tackle the complexities of real-world tasks.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception","url":"https://arxiv.org/abs/2507.00253","date":1751428800,"author":"","guid":180246,"unread":true,"content":"<article>arXiv:2507.00253v1 Announce Type: new \nAbstract: Enabling robots to understand human gaze target is a crucial step to allow capabilities in downstream tasks, for example, attention estimation and movement anticipation in real-world human-robot interactions. Prior works have addressed the in-frame target localization problem with data-driven approaches by carefully removing out-of-frame samples. Vision-based gaze estimation methods, such as OpenFace, do not effectively absorb background information in images and cannot predict gaze target in situations where subjects look away from the camera. In this work, we propose a system to address the problem of 360-degree gaze target estimation from an image in generalized visual scenes. The system, named GazeTarget360, integrates conditional inference engines of an eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion decoder. Cross validation results show that GazeTarget360 can produce accurate and reliable gaze target predictions in unseen scenarios. This makes a first-of-its-kind system to predict gaze targets from realistic camera footage which is highly efficient and deployable. Our source code is made publicly available at: https://github.com/zdai257/DisengageNet.</article>","contentLength":1253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition","url":"https://arxiv.org/abs/2507.00248","date":1751428800,"author":"","guid":180247,"unread":true,"content":"<article>arXiv:2507.00248v1 Announce Type: new \nAbstract: We present a novel framework for real-time sign language recognition using lightweight DNNs trained on limited data. Our system addresses key challenges in sign language recognition, including data scarcity, high computational costs, and discrepancies in frame rates between training and inference environments. By encoding sign language specific parameters, such as handshape, palm orientation, movement, and location into vectorized inputs, and leveraging MediaPipe for landmark extraction, we achieve highly separable input data representations. Our DNN architecture, optimized for sub 10MB deployment, enables accurate classification of 343 signs with less than 10ms latency on edge devices. The data annotation platform 'slait data' facilitates structured labeling and vector extraction. Our model achieved 92% accuracy in isolated sign recognition and has been integrated into the 'slait ai' web application, where it demonstrates stable inference.</article>","contentLength":1003,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning","url":"https://arxiv.org/abs/2507.00246","date":1751428800,"author":"","guid":180248,"unread":true,"content":"<article>arXiv:2507.00246v1 Announce Type: new \nAbstract: Despite recent advances in Language Reasoning Models (LRMs), most research focuses solely on English, even though many models are pretrained on multilingual data. In this work, we investigate: Is English the most token-efficient language for reasoning? We evaluate three open-source RLMs: DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven typologically diverse languages. We find that reasoning in non-English languages not only reduces token usage, but also preserves accuracy. These gains persist even after translating the reasoning traces into English, suggesting genuine shifts in reasoning behavior rather than surface-level linguistic effects. The extent of improvement, however, depends on the models multilingual strength. Our findings motivate a broader view of reasoning in language models, highlighting the potential of multilingual reasoning and the importance of strong multilingual foundations. The code for our work can be found: https://github.com/microsoft/EfficientXLang.</article>","contentLength":1059,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Algebraic Structure of Morphosyntax","url":"https://arxiv.org/abs/2507.00244","date":1751428800,"author":"","guid":180249,"unread":true,"content":"<article>arXiv:2507.00244v1 Announce Type: new \nAbstract: Within the context of the mathematical formulation of Merge and the Strong Minimalist Thesis, we present a mathematical model of the morphology-syntax interface. In this setting, morphology has compositional properties responsible for word formation, organized into a magma of morphological trees. However, unlike syntax, we do not have movement within morphology. A coproduct decomposition exists, but it requires extending the set of morphological trees beyond those which are generated solely by the magma, to a larger set of possible morphological inputs to syntactic trees. These participate in the formation of morphosyntactic trees as an algebra over an operad, and a correspondence between algebras over an operad. The process of structure formation for morphosyntactic trees can then be described in terms of this operadic correspondence that pairs syntactic and morphological data and the morphology coproduct. We reinterpret in this setting certain operations of Distributed Morphology as transformation that allow for flexibility in moving the boundary between syntax and morphology within the morphosyntactic objects.</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VOCAL: Visual Odometry via ContrAstive Learning","url":"https://arxiv.org/abs/2507.00243","date":1751428800,"author":"","guid":180250,"unread":true,"content":"<article>arXiv:2507.00243v1 Announce Type: new \nAbstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the landscape of robotics, enabling ultra-precise camera state estimation that is crucial for modern autonomous systems. Despite these advances, many learning-based VO techniques rely on rigid geometric assumptions, which often fall short in interpretability and lack a solid theoretical basis within fully data-driven frameworks. To overcome these limitations, we introduce VOCAL (Visual Odometry via ContrAstive Learning), a novel framework that reimagines VO as a label ranking challenge. By integrating Bayesian inference with a representation learning framework, VOCAL organizes visual features to mirror camera states. The ranking mechanism compels similar camera states to converge into consistent and spatially coherent representations within the latent space. This strategic alignment not only bolsters the interpretability of the learned features but also ensures compatibility with multimodal data sources. Extensive evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability and flexibility, pushing VO toward more general and explainable spatial intelligence.</article>","contentLength":1205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linearly Decoding Refused Knowledge in Aligned Language Models","url":"https://arxiv.org/abs/2507.00239","date":1751428800,"author":"","guid":180251,"unread":true,"content":"<article>arXiv:2507.00239v1 Announce Type: new \nAbstract: Most commonly used language models (LMs) are instruction-tuned and aligned using a combination of fine-tuning and reinforcement learning, causing them to refuse users requests deemed harmful by the model. However, jailbreak prompts can often bypass these refusal mechanisms and elicit harmful responses. In this work, we study the extent to which information accessed via jailbreak prompts is decodable using linear probes trained on LM hidden states. We show that a great deal of initially refused information is linearly decodable. For example, across models, the response of a jailbroken LM for the average IQ of a country can be predicted by a linear probe with Pearson correlations exceeding $0.8$. Surprisingly, we find that probes trained on base models (which do not refuse) sometimes transfer to their instruction-tuned versions and are capable of revealing information that jailbreaks decode generatively, suggesting that the internal representations of many refused properties persist from base LMs through instruction-tuning. Importantly, we show that this information is not merely \"leftover\" in instruction-tuned models, but is actively used by them: we find that probe-predicted values correlate with LM generated pairwise comparisons, indicating that the information decoded by our probes align with suppressed generative behavior that may be expressed more subtly in other downstream tasks. Overall, our results suggest that instruction-tuning does not wholly eliminate or even relocate harmful information in representation space-they merely suppress its direct expression, leaving it both linearly accessible and indirectly influential in downstream behavior.</article>","contentLength":1727,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Plan-Based Scalable Online Virtual Network Embedding","url":"https://arxiv.org/abs/2507.00237","date":1751428800,"author":"","guid":180252,"unread":true,"content":"<article>arXiv:2507.00237v1 Announce Type: new \nAbstract: Network virtualization allows hosting applications with diverse computation and communication requirements on shared edge infrastructure. Given a set of requests for deploying virtualized applications, the edge provider has to deploy a maximum number of them to the underlying physical network, subject to capacity constraints. This challenge is known as the virtual network embedding (VNE) problem: it models applications as virtual networks, where virtual nodes represent functions and virtual links represent communication between the virtual nodes.\n  All variants of VNE are known to be strongly NP-hard. Because of its centrality to network virtualization, VNE has been extensively studied. We focus on the online variant of VNE, in which deployment requests are not known in advance. This reflects the highly skewed and unpredictable demand intrinsic to the edge. Unfortunately, existing solutions to online VNE do not scale well with the number of requests per second and the physical topology size.\n  We propose a novel approach in which our new online algorithm, OLIVE, leverages a nearly optimal embedding for an aggregated expected demand. This embedding is computed offline. It serves as a plan that OLIVE uses as a guide for handling actual individual requests while dynamically compensating for deviations from the plan. We demonstrate that our solution can handle a number of requests per second greater by two orders of magnitude than the best results reported in the literature. Thus, it is particularly suitable for realistic edge environments.</article>","contentLength":1611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving","url":"https://arxiv.org/abs/2507.00236","date":1751428800,"author":"","guid":180253,"unread":true,"content":"<article>arXiv:2507.00236v1 Announce Type: new \nAbstract: Simulation-based design, optimization, and validation of autonomous driving algorithms have proven to be crucial for their iterative improvement over the years. Nevertheless, the ultimate measure of effectiveness is their successful transition from simulation to reality (sim2real). However, existing sim2real transfer methods struggle to comprehensively address the autonomy-oriented requirements of balancing: (i) conditioned domain adaptation, (ii) robust performance with limited examples, (iii) modularity in handling multiple domain representations, and (iv) real-time performance. To alleviate these pain points, we present a unified framework for learning cross-domain adaptive representations for sim2real transferable autonomous driving algorithms using conditional latent diffusion models. Our framework offers options to leverage: (i) alternate foundation models, (ii) a few-shot fine-tuning pipeline, and (iii) textual as well as image prompts for mapping across given source and target domains. It is also capable of generating diverse high-quality samples when diffusing across parameter spaces such as times of day, weather conditions, seasons, and operational design domains. We systematically analyze the presented framework and report our findings in the form of critical quantitative metrics and ablation studies, as well as insightful qualitative examples and remarks. Additionally, we demonstrate the serviceability of the proposed approach in bridging the sim2real gap for end-to-end autonomous driving using a behavioral cloning case study. Our experiments indicate that the proposed framework is capable of bridging the perceptual sim2real gap by over 40%. We hope that our approach underscores the potential of generative diffusion models in sim2real transfer, offering a pathway toward more robust and adaptive autonomous driving.</article>","contentLength":1906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minimum Selective Subset on Some Graph Classes","url":"https://arxiv.org/abs/2507.00235","date":1751428800,"author":"","guid":180254,"unread":true,"content":"<article>arXiv:2507.00235v1 Announce Type: new \nAbstract: In a connected simple graph G = (V(G),E(G)), each vertex is assigned a color from the set of colors C={1, 2,..., c}. The set of vertices V(G) is partitioned as V_1, V_2, ... ,V_c, where all vertices in V_j share the same color j. A subset S of V(G) is called Selective Subset if, for every vertex v in V(G), and if v is in V_j, at least one of its nearest neighbors in (S union (V(G)\\ V_j)) has the same color as v. The Minimum Selective Subset (MSS) problem seeks to find a selective subset of minimum size. The problem was first introduced by Wilfong in 1991 for a set of points in the Euclidean plane, where two major problems, MCS (Minimum Consistent Subset) and MSS, were proposed.\n  In graph algorithms, the only known result is that the MSS problem is NP-complete, as shown in 2018. Beyond this, no further progress has been made to date. In contrast, the MCS problem has been widely studied in various graph classes over the years. Therefore, in this work, we also extend the algorithmic study of MSS on various graph classes. We first present a log(n)-approximation algorithm for general graphs with n vertices and regardless of the number of colors. We also show that the problem remains NP-complete in planar graphs when restricted to just two colors.. Finally, we provide polynomial-time algorithms for computing optimal solutions in trees and unit interval graphs for any number of colors.</article>","contentLength":1451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations","url":"https://arxiv.org/abs/2507.00234","date":1751428800,"author":"","guid":180255,"unread":true,"content":"<article>arXiv:2507.00234v1 Announce Type: new \nAbstract: In this paper, we present a novel framework for enhancing model interpretability by integrating heatmaps produced separately by ResNet and a restructured 2D Transformer with globally weighted input saliency. We address the critical problem of spatial-temporal misalignment in existing interpretability methods, where convolutional networks fail to capture global context and Transformers lack localized precision - a limitation that impedes actionable insights in safety-critical domains like healthcare and industrial monitoring. Our method merges gradient-weighted activation maps (ResNet) and Transformer attention rollout into a unified visualization, achieving full spatial-temporal alignment while preserving real-time performance. Empirical evaluations on clinical (ECG arrhythmia detection) and industrial (energy consumption prediction) datasets demonstrate significant improvements: the hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy Appliance dataset-outperforming standalone ResNet, Transformer, and InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps into domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds suggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L (0.650) scores. By formalizing interpretability as causal fidelity and spatial-temporal alignment, our approach bridges the gap between technical outputs and stakeholder understanding, offering a scalable solution for transparent, time-aware decision-making.</article>","contentLength":1657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction","url":"https://arxiv.org/abs/2507.00230","date":1751428800,"author":"","guid":180256,"unread":true,"content":"<article>arXiv:2507.00230v1 Announce Type: new \nAbstract: Reconstructing high-quality images from low-resolution inputs using Residual Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in collaborative scenarios where centralized training poses significant privacy risks, including data leakage and inference attacks, as well as high computational costs. We propose a novel Privacy-Preserving Federated Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image reconstruction. PPFL-RDSN integrates Federated Learning (FL), local differential privacy, and robust model watermarking techniques, ensuring data remains secure on local devices, safeguarding sensitive information, and maintaining model authenticity without revealing underlying data. Empirical evaluations show that PPFL-RDSN achieves comparable performance to the state-of-the-art centralized methods while reducing computational burdens, and effectively mitigates security and privacy vulnerabilities, making it a practical solution for secure and privacy-preserving collaborative computer vision applications.</article>","contentLength":1109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss","url":"https://arxiv.org/abs/2507.00229","date":1751428800,"author":"","guid":180257,"unread":true,"content":"<article>arXiv:2507.00229v1 Announce Type: new \nAbstract: Speech super-resolution (SSR) enhances low-resolution speech by increasing the sampling rate. While most SSR methods focus on magnitude reconstruction, recent research highlights the importance of phase reconstruction for improved perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency Transformation Network that reconstructs both magnitude and phase in complex domains for improved SSR tasks. It incorporates a complex global attention block to model inter-phoneme and inter-frequency dependencies and a complex conformer to capture long-range and local features, improving frequency reconstruction and noise robustness. CTFT-Net employs time-domain and multi-resolution frequency-domain loss functions for better generalization. Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave, WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling (2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy artifacts.</article>","contentLength":1042,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computer Vision for Objects used in Group Work: Challenges and Opportunities","url":"https://arxiv.org/abs/2507.00224","date":1751428800,"author":"","guid":180258,"unread":true,"content":"<article>arXiv:2507.00224v1 Announce Type: new \nAbstract: Interactive and spatially aware technologies are transforming educational frameworks, particularly in K-12 settings where hands-on exploration fosters deeper conceptual understanding. However, during collaborative tasks, existing systems often lack the ability to accurately capture real-world interactions between students and physical objects. This issue could be addressed with automatic 6D pose estimation, i.e., estimation of an object's position and orientation in 3D space from RGB images or videos. For collaborative groups that interact with physical objects, 6D pose estimates allow AI systems to relate objects and entities. As part of this work, we introduce FiboSB, a novel and challenging 6D pose video dataset featuring groups of three participants solving an interactive task featuring small hand-held cubes and a weight scale. This setup poses unique challenges for 6D pose because groups are holistically recorded from a distance in order to capture all participants -- this, coupled with the small size of the cubes, makes 6D pose estimation inherently non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on FiboSB, exposing the limitations of current algorithms on collaborative group work. An error analysis of these methods reveals that the 6D pose methods' object detection modules fail. We address this by fine-tuning YOLO11-x for FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results, and analysis of YOLO11-x errors presented here lay the groundwork for leveraging the estimation of 6D poses in difficult collaborative contexts.</article>","contentLength":1649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Error Etimates for Non Conforming Discretisation of Time-dependent Convection-Diffusion-Reaction Model","url":"https://arxiv.org/abs/2507.00219","date":1751428800,"author":"","guid":180259,"unread":true,"content":"<article>arXiv:2507.00219v1 Announce Type: new \nAbstract: We use a generic framework, namely the gradient discretisation method (GDM), to propose a unified numerical analysis for general time-dependent convection-diffusion-reaction models. We establish novel results for convergence rates of numerical approximations of such models under reasonable assumptions on exact solutions, and prove the existence and uniqueness of the approximate solution for suitably small time steps. The main interest of our results lies in covering several approximation methods and various applications of the considered model such as the generalised Burgers-Fisher (GBF) and the generalised Burgers-Huxley (GBH) models. Numerical tests based on the hybrid mimetic mixed (HMM) method for the GBF model are performed on various types of general meshes to examine the accuracy of the proposed gradient scheme. The results confirm our theoretical rates of convergence, even on mesh with extreme distortions.</article>","contentLength":976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning for routing: A guided review of recent developments and future directions","url":"https://arxiv.org/abs/2507.00218","date":1751428800,"author":"","guid":180260,"unread":true,"content":"<article>arXiv:2507.00218v1 Announce Type: new \nAbstract: This paper reviews the current progress in applying machine learning (ML) tools to solve NP-hard combinatorial optimization problems, with a focus on routing problems such as the traveling salesman problem (TSP) and the vehicle routing problem (VRP). Due to the inherent complexity of these problems, exact algorithms often require excessive computational time to find optimal solutions, while heuristics can only provide approximate solutions without guaranteeing optimality. With the recent success of machine learning models, there is a growing trend in proposing and implementing diverse ML techniques to enhance the resolution of these challenging routing problems. We propose a taxonomy categorizing ML-based routing methods into construction-based and improvement-based approaches, highlighting their applicability to various problem characteristics. This review aims to integrate traditional OR methods with state-of-the-art ML techniques, providing a structured framework to guide future research and address emerging VRP variants.</article>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CrossPipe: Towards Optimal Pipeline Schedules for Cross-Datacenter Training","url":"https://arxiv.org/abs/2507.00217","date":1751428800,"author":"","guid":180261,"unread":true,"content":"<article>arXiv:2507.00217v1 Announce Type: new \nAbstract: Training large language models (LLMs) now requires resources that exceed a single datacenter, making cross-datacenter strategies increasingly crucial. We present CrossPipe, a framework designed to optimize model training across geographically distributed datacenters by explicitly modeling and mitigating the impact of network latency and limited bandwidth. It enables unified analysis and optimization incorporating both pipeline parallelism (PP) and opportunities for overlapping data parallelism (DP) communication. CrossPipe generates optimized pipeline schedules using either solver-based optimal or fast near-optimal greedy algorithms, built upon a flexible execution engine that separates scheduling logic from communication details. Our evaluation shows that CrossPipe reduces training time by up to 33.6\\% compared to traditional pipeline schedules under identical memory constraints. When memory constraints are relaxed, CrossPipe maintains strong performance despite communication delays, approaching the efficiency of idealized schedules without delays. CrossPipe offers improved scalability and resource utilization, particularly in environments with high network latency or limited bandwidth.</article>","contentLength":1255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Style Alignment in Cross-Cultural Translation","url":"https://arxiv.org/abs/2507.00216","date":1751428800,"author":"","guid":180262,"unread":true,"content":"<article>arXiv:2507.00216v1 Announce Type: new \nAbstract: Successful communication depends on the speaker's intended style (i.e., what the speaker is trying to convey) aligning with the listener's interpreted style (i.e., what the listener perceives). However, cultural differences often lead to misalignment between the two; for example, politeness is often lost in translation. We characterize the ways that LLMs fail to translate style - biasing translations towards neutrality and performing worse in non-Western languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic Alignment), a method that leverages learned stylistic concepts to encourage LLM translation to appropriately convey cultural communication norms and align style.</article>","contentLength":749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning","url":"https://arxiv.org/abs/2507.00214","date":1751428800,"author":"","guid":180263,"unread":true,"content":"<article>arXiv:2507.00214v1 Announce Type: new \nAbstract: Standard classification models often map inputs directly to labels without explicit reasoning, potentially limiting their performance, robustness, and interpretability. This paper introduces a novel two-stage approach to enhance text classification by leveraging Large Language Model (LLM)-generated reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model (henceforth Llama-R-Gen) on a general-purpose reasoning dataset (syvai/reasoning-gen) to generate textual reasoning (R) given a question and its answer. In the second stage, this generally trained Llama-R-Gen is used offline to create an augmented training dataset for a downstream generative model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the input text (Q) and is trained to output the generated reasoning (R) immediately followed by the predicted emotion (A). We demonstrate this methodology on the dair-ai/emotion dataset for emotion classification. Our experiments show that the generative model trained to output reasoning and the emotion (Classifier Q-&gt;RA) achieves a significant improvement of 8.7 percentage points in accuracy (for emotion prediction) compared to a baseline generative model trained solely to output the emotion (Classifier Q-&gt;A), highlighting the strong generalization capabilities of the reasoning generation and the benefit of explicit reasoning training. This work underscores the potential of LLM-generated reasonings for creating richer training datasets, thereby improving the performance of diverse downstream NLP tasks and providing explicit explanations.</article>","contentLength":1640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LineRetriever: Planning-Aware Observation Reduction for Web Agents","url":"https://arxiv.org/abs/2507.00210","date":1751428800,"author":"","guid":180264,"unread":true,"content":"<article>arXiv:2507.00210v1 Announce Type: new \nAbstract: While large language models have demonstrated impressive capabilities in web navigation tasks, the extensive context of web pages, often represented as DOM or Accessibility Tree (AxTree) structures, frequently exceeds model context limits. Current approaches like bottom-up truncation or embedding-based retrieval lose critical information about page state and action history. This is particularly problematic for adaptive planning in web agents, where understanding the current state is essential for determining future actions. We hypothesize that embedding models lack sufficient capacity to capture plan-relevant information, especially when retrieving content that supports future action prediction. This raises a fundamental question: how can retrieval methods be optimized for adaptive planning in web navigation tasks? In response, we introduce \\textit{LineRetriever}, a novel approach that leverages a language model to identify and retrieve observation lines most relevant to future navigation steps. Unlike traditional retrieval methods that focus solely on semantic similarity, \\textit{LineRetriever} explicitly considers the planning horizon, prioritizing elements that contribute to action prediction. Our experiments demonstrate that \\textit{LineRetriever} can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.</article>","contentLength":1463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holistic Artificial Intelligence in Medicine; improved performance and explainability","url":"https://arxiv.org/abs/2507.00205","date":1751428800,"author":"","guid":180265,"unread":true,"content":"<article>arXiv:2507.00205v1 Announce Type: new \nAbstract: With the increasing interest in deploying Artificial Intelligence in medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework that fuses multimodal data to solve downstream clinical tasks. However, HAIM uses data in a task-agnostic manner and lacks explainability. To address these limitations, we introduce xHAIM (Explainable HAIM), a novel framework leveraging Generative AI to enhance both prediction and explainability through four structured steps: (1) automatically identifying task-relevant patient data across modalities, (2) generating comprehensive patient summaries, (3) using these summaries for improved predictive modeling, and (4) providing clinical explanations by linking predictions to patient-specific medical knowledge. Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9% to 90.3% across chest pathology and operative tasks. Importantly, xHAIM transforms AI from a black-box predictor into an explainable decision support system, enabling clinicians to interactively trace predictions back to relevant patient data, bridging AI advancements with clinical utility.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group","url":"https://arxiv.org/abs/2507.00202","date":1751428800,"author":"","guid":180266,"unread":true,"content":"<article>arXiv:2507.00202v1 Announce Type: new \nAbstract: Purpose: Little research has explored the communication needs of autistic adults and how their needs differ from those of other disabled populations. Augmentative and Alternative Communication (AAC) can support these communication needs, but more guidance is needed on how to design AAC to support this population.\n  Materials and Methods: We conducted an online, asynchronous, text-based focus group with five autistic adults to explore their social communication and community engagement and how AAC can help support them.\n  Results and Conclusion: Our analysis of the participant responses found that 1) participants' emotional experiences impacted the communication methods they used, 2) speaking autistic adults can benefit from AAC use, and 3) autistic shutdown creates dynamic communication needs. We present implications for future AAC design: supporting communication in times of shutdown, indicating communication ability to communication partners, and a need to better understand the fear of using AAC. These implications can inform the design for future AAC systems. We also provide themes for future autism research: exploring the impact of a late diagnosis, gaining a better understanding of the communication needs during autistic shutdown, and expanding research to include the social and environmental factors that impact communication. Finally, we provide guidance on how future online focus groups can be run in an accessible manner.</article>","contentLength":1501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring AR Label Placements in Visually Cluttered Scenarios","url":"https://arxiv.org/abs/2507.00198","date":1751428800,"author":"","guid":180267,"unread":true,"content":"<article>arXiv:2507.00198v1 Announce Type: new \nAbstract: We investigate methods for placing labels in AR environments that have visually cluttered scenes. As the number of items increases in a scene within the user' FOV, it is challenging to effectively place labels based on existing label placement guidelines. To address this issue, we implemented three label placement techniques for in-view objects for AR applications. We specifically target a scenario, where various items of different types are scattered within the user's field of view, and multiple items of the same type are situated close together. We evaluate three placement techniques for three target tasks. Our study shows that using a label to spatially group the same types of items is beneficial for identifying, comparing, and summarizing data.</article>","contentLength":807,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Simple Algorithm for Trimmed Multipoint Evaluation","url":"https://arxiv.org/abs/2507.00196","date":1751428800,"author":"","guid":180268,"unread":true,"content":"<article>arXiv:2507.00196v1 Announce Type: new \nAbstract: Evaluating a polynomial on a set of points is a fundamental task in computer algebra. In this work, we revisit a particular variant called trimmed multipoint evaluation: given an $n$-variate polynomial with bounded individual degree $d$ and total degree $D$, the goal is to evaluate it on a natural class of input points. This problem arises as a key subroutine in recent algorithmic results [Dinur; SODA '21], [Dell, Haak, Kallmayer, Wennmann; SODA '25]. It is known that trimmed multipoint evaluation can be solved in near-linear time [van der Hoeven, Schost; AAECC '13] by a clever yet somewhat involved algorithm. We give a simple recursive algorithm that avoids heavy computer-algebraic machinery, and can be readily understood by researchers without specialized background.</article>","contentLength":828,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness","url":"https://arxiv.org/abs/2507.00195","date":1751428800,"author":"","guid":180269,"unread":true,"content":"<article>arXiv:2507.00195v1 Announce Type: new \nAbstract: This thesis contributes to the theoretical understanding of local update algorithms, especially Local SGD, in distributed and federated optimization under realistic models of data heterogeneity. A central focus is on the bounded second-order heterogeneity assumption, which is shown to be both necessary and sufficient for local updates to outperform centralized or mini-batch methods in convex and non-convex settings. The thesis establishes tight upper and lower bounds in several regimes for various local update algorithms and characterizes the min-max complexity of multiple problem classes. At its core is a fine-grained consensus-error-based analysis framework that yields sharper finite-time convergence bounds under third-order smoothness and relaxed heterogeneity assumptions. The thesis also extends to online federated learning, providing fundamental regret bounds under both first-order and bandit feedback. Together, these results clarify when and why local updates offer provable advantages, and the thesis serves as a self-contained guide for analyzing Local SGD in heterogeneous environments.</article>","contentLength":1158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An energy-stable parametric finite element method for Willmore flow with normal-tangential velocity splitting","url":"https://arxiv.org/abs/2507.00193","date":1751428800,"author":"","guid":180270,"unread":true,"content":"<article>arXiv:2507.00193v1 Announce Type: new \nAbstract: We propose and analyze an energy-stable fully discrete parametric approximation for Willmore flow of hypersurfaces in two and three space dimensions. We allow for the presence of spontaneous curvature effects and for open surfaces with boundary. The presented scheme is based on a new geometric partial differential equation (PDE) that combines an evolution equation for the mean curvature with a separate equation that prescribes the tangential velocity. The mean curvature is used to determine the normal velocity within the gradient flow structure, thus guaranteeing an unconditional energy stability for the discrete solution upon suitable discretization. We introduce a novel weak formulation for this geometric PDE, in which different types of boundary conditions can be naturally enforced. We further discretize the weak formulation to obtain a fully discrete parametric finite element method, for which well-posedness can be rigorously shown. Moreover, the constructed scheme admits an unconditional stability estimate in terms of the discrete energy. Extensive numerical experiments are reported to showcase the accuracy and robustness of the proposed method for computing Willmore flow of both curves in $\\mathbb{R}^2$ and surfaces in $\\mathbb{R}^3$.</article>","contentLength":1309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions","url":"https://arxiv.org/abs/2507.00191","date":1751428800,"author":"","guid":180271,"unread":true,"content":"<article>arXiv:2507.00191v1 Announce Type: new \nAbstract: Wearable devices record physiological and behavioral signals that can improve health predictions. While foundation models are increasingly used for such predictions, they have been primarily applied to low-level sensor data, despite behavioral data often being more informative due to their alignment with physiologically relevant timescales and quantities. We develop foundation models of such behavioral signals using over 2.5B hours of wearable data from 162K individuals, systematically optimizing architectures and tokenization strategies for this unique dataset. Evaluated on 57 health-related tasks, our model shows strong performance across diverse real-world applications including individual-level classification and time-varying health state prediction. The model excels in behavior-driven tasks like sleep prediction, and improves further when combined with representations of raw sensor data. These results underscore the importance of tailoring foundation model design to wearables and demonstrate the potential to enable new health applications.</article>","contentLength":1109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethink 3D Object Detection from Physical World","url":"https://arxiv.org/abs/2507.00190","date":1751428800,"author":"","guid":180272,"unread":true,"content":"<article>arXiv:2507.00190v1 Announce Type: new \nAbstract: High-accuracy and low-latency 3D object detection is essential for autonomous driving systems. While previous studies on 3D object detection often evaluate performance based on mean average precision (mAP) and latency, they typically fail to address the trade-off between speed and accuracy, such as 60.0 mAP at 100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs between different hardware devices and accelerators remains unexplored, despite being critical for real-time applications. Furthermore, they overlook the impact on collision avoidance in motion planning, for example, 60.0 mAP leading to safer motion planning or 61.0 mAP leading to high-risk motion planning. In this paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP) as new metrics, which consider the physical world such as the concept of time and physical constraints, offering a more comprehensive evaluation for real-time 3D object detection. We demonstrate the effectiveness of our metrics for the entire autonomous driving system using nuPlan dataset, and evaluate 3D object detection models accounting for hardware differences and accelerators. We also develop a state-of-the-art performance model for real-time 3D object detection through latency-aware hyperparameter optimization (L-HPO) using our metrics. Additionally, we quantitatively demonstrate that the assumption \"the more point clouds, the better the recognition performance\" is incorrect for real-time applications and optimize both hardware and model selection using our metrics.</article>","contentLength":1607,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Plug. Play. Persist. Inside a Ready-to-Go Havoc C2 Infrastructure","url":"https://arxiv.org/abs/2507.00189","date":1751428800,"author":"","guid":180273,"unread":true,"content":"<article>arXiv:2507.00189v1 Announce Type: new \nAbstract: This analysis focuses on a single Azure-hosted Virtual Machine at 52.230.23.114 that the adversary converted into an all-in-one delivery, staging and Command-and-Control node. The host advertises an out-of-date Apache 2.4.52 instance whose open directory exposes phishing lures, PowerShell loaders, Reflective Shell-Code, compiled Havoc Demon implants and a toolbox of lateral-movement binaries; the same server also answers on 8443/80 for encrypted beacon traffic. The web tier is riddled with publicly documented critical vulnerabilities, that would have allowed initial code-execution had the attackers not already owned the device.\n  Initial access is delivered through an HTML file that, once de-obfuscated, perfectly mimics Google Unusual sign-in attempt notification and funnels victims toward credential collection. A PowerShell command follows: it disables AMSI in-memory, downloads a Base64-encoded stub, allocates RWX pages and starts the shell-code without ever touching disk. That stub reconstructs a DLL in memory using the Reflective-Loader technique and hands control to Havoc Demon implant. Every Demon variant-32- and 64-bit alike-talks to the same backend, resolves Windows APIs with hashed look-ups, and hides its activity behind indirect syscalls.\n  Runtime telemetry shows interests in registry under Image File Execution Options, deliberate queries to Software Restriction Policy keys, and heavy use of Crypto DLLs to protect payloads and C2 traffic. The attacker toolkit further contains Chisel, PsExec, Doppelganger and Whisker, some of them re-compiled under user directories that leak the developer personas tonzking123 and thobt. Collectively the findings paint a picture of a technically adept actor who values rapid re-tooling over deep operational security, leaning on Havoc modularity and on legitimate cloud services to blend malicious flows into ordinary enterprise traffic.</article>","contentLength":1957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LIMAO: A Framework for Lifelong Modular Learned Query Optimization","url":"https://arxiv.org/abs/2507.00188","date":1751428800,"author":"","guid":180274,"unread":true,"content":"<article>arXiv:2507.00188v1 Announce Type: new \nAbstract: Query optimizers are crucial for the performance of database systems. Recently, many learned query optimizers (LQOs) have demonstrated significant performance improvements over traditional optimizers. However, most of them operate under a limited assumption: a static query environment. This limitation prevents them from effectively handling complex, dynamic query environments in real-world scenarios. Extensive retraining can lead to the well-known catastrophic forgetting problem, which reduces the LQO generalizability over time. In this paper, we address this limitation and introduce LIMAO (Lifelong Modular Learned Query Optimizer), a framework for lifelong learning of plan cost prediction that can be seamlessly integrated into existing LQOs. LIMAO leverages a modular lifelong learning technique, an attention-based neural network composition architecture, and an efficient training paradigm designed to retain prior knowledge while continuously adapting to new environments. We implement LIMAO in two LQOs, showing that our approach is agnostic to underlying engines. Experimental results show that LIMAO significantly enhances the performance of LQOs, achieving up to a 40% improvement in query execution time and reducing the variance of execution time by up to 60% under dynamic workloads. By leveraging a precise and self-consistent design, LIMAO effectively mitigates catastrophic forgetting, ensuring stable and reliable plan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup on selected benchmarks, highlighting its practical advantages in real-world query optimization.</article>","contentLength":1663,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros","url":"https://arxiv.org/abs/2507.00184","date":1751428800,"author":"","guid":180275,"unread":true,"content":"<article>arXiv:2507.00184v1 Announce Type: new \nAbstract: Recent research shows how diffusion models can unconditionally generate tile-based game levels, but use of diffusion models for text-to-level generation is underexplored. There are practical considerations for creating a usable model: caption/level pairs are needed, as is a text embedding model, and a way of generating entire playable levels, rather than individual scenes. We present strategies to automatically assign descriptive captions to an existing level dataset, and train diffusion models using both pretrained text encoders and simple transformer models trained from scratch. Captions are automatically assigned to generated levels so that the degree of overlap between input and output captions can be compared. We also assess the diversity and playability of the resulting levels. Results are compared with an unconditional diffusion model and a generative adversarial network, as well as the text-to-level approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model uses a simple transformer model for text embedding, and takes less time to train than diffusion models employing more complex text encoders, indicating that reliance on larger language models is not necessary. We also present a GUI allowing designers to construct long levels from model-generated scenes.</article>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph-Based Deep Learning for Component Segmentation of Maize Plants","url":"https://arxiv.org/abs/2507.00182","date":1751428800,"author":"","guid":180276,"unread":true,"content":"<article>arXiv:2507.00182v1 Announce Type: new \nAbstract: In precision agriculture, one of the most important tasks when exploring crop production is identifying individual plant components. There are several attempts to accomplish this task by the use of traditional 2D imaging, 3D reconstructions, and Convolutional Neural Networks (CNN). However, they have several drawbacks when processing 3D data and identifying individual plant components. Therefore, in this work, we propose a novel Deep Learning architecture to detect components of individual plants on Light Detection and Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on the concept of Graph Neural Networks (GNN), and feature enhancing with Principal Component Analysis (PCA). For this, each point is taken as a vertex and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established, thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used to further increase the features of each point. Finally, Graph Attention Networks (GAT) are applied to classify visible phenotypic components of the plant, such as the leaf, stem, and soil. This study demonstrates that our graph-based deep learning approach enhances segmentation accuracy for identifying individual plant components, achieving percentages above 80% in the IoU average, thus outperforming other existing models based on point clouds.</article>","contentLength":1407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement decline","url":"https://arxiv.org/abs/2507.00181","date":1751428800,"author":"","guid":180277,"unread":true,"content":"<article>arXiv:2507.00181v1 Announce Type: new \nAbstract: Despite the increasing use of large language models (LLMs) in education, concerns have emerged about their potential to reduce deep thinking and active learning. This study investigates the impact of generative artificial intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of students during academic writing tasks. The study employed an experimental design with participants randomly assigned to either an AI-assisted (ChatGPT) or a non-assisted (control) condition. Participants completed a structured argumentative writing task followed by a cognitive engagement scale (CES), the CES-AI, developed to assess mental effort, attention, deep processing, and strategic thinking. The results revealed significantly lower cognitive engagement scores in the ChatGPT group compared to the control group. These findings suggest that AI assistance may lead to cognitive offloading. The study contributes to the growing body of literature on the psychological implications of AI in education and raises important questions about the integration of such tools into academic practice. It calls for pedagogical strategies that promote active, reflective engagement with AI-generated content to avoid compromising self-regulated learning and deep cognitive involvement of students.</article>","contentLength":1342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis","url":"https://arxiv.org/abs/2507.00180","date":1751428800,"author":"","guid":180278,"unread":true,"content":"<article>arXiv:2507.00180v1 Announce Type: new \nAbstract: Modernizing legacy software systems is a critical but challenging task, often hampered by a lack of documentation and understanding of the original system's intricate decision logic. Traditional approaches like behavioral cloning merely replicate input-output behavior without capturing the underlying intent. This paper proposes a novel pipeline to automatically extract interpretable decision logic from legacy systems treated as black boxes. The approach uses a Reinforcement Learning (RL) agent to explore the input space and identify critical decision boundaries by rewarding actions that cause meaningful changes in the system's output. These counterfactual state transitions, where the output changes, are collected and clustered using K-Means. Decision trees are then trained on these clusters to extract human-readable rules that approximate the system's decision logic near the identified boundaries. I demonstrated the pipeline's effectiveness on three dummy legacy systems with varying complexity, including threshold-based, combined-conditional, and non-linear range logic. Results show that the RL agent successfully focuses exploration on relevant boundary regions, and the extracted rules accurately reflect the core logic of the underlying dummy systems, providing a promising foundation for generating specifications and test cases during legacy migration.</article>","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intellectual Property Rights and Entrepreneurship in the NFT Ecosystem: Legal Frameworks, Business Models, and Innovation Opportunities","url":"https://arxiv.org/abs/2507.00172","date":1751428800,"author":"","guid":180279,"unread":true,"content":"<article>arXiv:2507.00172v1 Announce Type: new \nAbstract: Non Fungible Tokens have changed digital ownership and how creators earn money. Between 2021 and 2024, the market value exceeded 40 billion. However, the fast growth of the NFT ecosystem has revealed serious issues in managing intellectual property rights. There is a lot of confusion about the difference between owning an NFT and owning the copyright for the underlying content. This research looks at the gap between traditional copyright laws and blockchain-based transactions. We use a mixed methods approach to analyze this disconnect. We create a new IP rights matrix that clearly shows how copyright law relates to NFT ownership structures. Additionally, we include a business model taxonomy that sorts new commercial applications by their IP risk and sustainability factors. By examining important legal cases, smart contracts, and interviews with stakeholders, we find key problems in enforcing laws across different regions, standardizing licenses, and assessing business opportunities.</article>","contentLength":1046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SelvaBox: A high-resolution dataset for tropical tree crown detection","url":"https://arxiv.org/abs/2507.00170","date":1751428800,"author":"","guid":180280,"unread":true,"content":"<article>arXiv:2507.00170v1 Announce Type: new \nAbstract: Detecting individual tree crowns in tropical forests is essential to study these complex and crucial ecosystems impacted by human interventions and climate change. However, tropical crowns vary widely in size, structure, and pattern and are largely overlapping and intertwined, requiring advanced remote sensing methods applied to high-resolution imagery. Despite growing interest in tropical tree crown detection, annotated datasets remain scarce, hindering robust model development. We introduce SelvaBox, the largest open-access dataset for tropical tree crown detection in high-resolution drone imagery. It spans three countries and contains more than 83,000 manually labeled crowns - an order of magnitude larger than all previous tropical forest datasets combined. Extensive benchmarks on SelvaBox reveal two key findings: (1) higher-resolution inputs consistently boost detection accuracy; and (2) models trained exclusively on SelvaBox achieve competitive zero-shot detection performance on unseen tropical tree crown datasets, matching or exceeding competing methods. Furthermore, jointly training on SelvaBox and three other datasets at resolutions from 3 to 10 cm per pixel within a unified multi-resolution pipeline yields a detector ranking first or second across all evaluated datasets. Our dataset, code, and pre-trained weights are made public.</article>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Novel Design of 3D Printed Tumbling Microrobots for in vivo Targeted Drug Delivery","url":"https://arxiv.org/abs/2507.00166","date":1751428800,"author":"","guid":180281,"unread":true,"content":"<article>arXiv:2507.00166v1 Announce Type: new \nAbstract: This paper presents innovative designs for 3D-printed tumbling microrobots, specifically engineered for targeted in vivo drug delivery applications. The microrobot designs, created using stereolithography 3D printing technologies, incorporate permanent micro-magnets to enable actuation via a rotating magnetic field actuator system. The experimental framework encompasses a series of locomotion characterization tests to evaluate microrobot performance under various conditions. Testing variables include variations in microrobot geometries, actuation frequencies, and environmental conditions, such as dry and wet environments, and temperature changes. The paper outlines designs for three drug loading methods, along with comprehensive assessments thermal drug release using a focused ultrasound system, as well as biocompatibility tests. Animal model testing involves tissue phantoms and in vivo rat models, ensuring a thorough evaluation of the microrobots' performance and compatibility. The results highlight the robustness and adaptability of the proposed microrobot designs, showcasing the potential for efficient and targeted in vivo drug delivery. This novel approach addresses current limitations in existing tumbling microrobot designs and paves the way for advancements in targeted drug delivery within the large intestine.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompting as Scientific Inquiry","url":"https://arxiv.org/abs/2507.00163","date":1751428800,"author":"","guid":180282,"unread":true,"content":"<article>arXiv:2507.00163v1 Announce Type: new \nAbstract: Prompting is the primary method by which we study and control large language models. It is also one of the most powerful: nearly every major capability attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was first unlocked through prompting. Yet prompting is rarely treated as science and is frequently frowned upon as alchemy. We argue that this is a category error. If we treat LLMs as a new kind of complex and opaque organism that is trained rather than programmed, then prompting is not a workaround: it is behavioral science. Mechanistic interpretability peers into the neural substrate, prompting probes the model in its native interface: language. We contend that prompting is not inferior, but rather a key component in the science of LLMs.</article>","contentLength":820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion","url":"https://arxiv.org/abs/2507.00162","date":1751428800,"author":"","guid":180283,"unread":true,"content":"<article>arXiv:2507.00162v1 Announce Type: new \nAbstract: Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences.</article>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments","url":"https://arxiv.org/abs/2507.00161","date":1751428800,"author":"","guid":180284,"unread":true,"content":"<article>arXiv:2507.00161v1 Announce Type: new \nAbstract: Political polarization undermines democratic civic education by exacerbating identity-based resistance to opposing viewpoints. Emerging AI technologies offer new opportunities to advance interventions that reduce polarization and promote political open-mindedness. We examined novel design strategies that leverage adaptive and emotionally-responsive civic narratives that may sustain students' emotional engagement in stories, and in turn, promote perspective-taking toward members of political out-groups. Drawing on theories from political psychology and narratology, we investigate how affective computing techniques can support three storytelling mechanisms: transportation into a story world, identification with characters, and interaction with the storyteller. Using a design-based research (DBR) approach, we iteratively developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS) platform. Our prototype integrates facial emotion recognition and attention tracking to assess users' affective and attentional states in real time. Narrative content is organized around pre-structured story outlines, with beat-by-beat language adaptation implemented via GPT-4, personalizing linguistic tone to sustain students' emotional engagement in stories that center political perspectives different from their own. Our work offers a foundation for AI-supported, emotionally-sensitive strategies that address affective polarization while preserving learner autonomy. We conclude with implications for civic education interventions, algorithmic literacy, and HCI challenges associated with AI dialogue management and affect-adaptive learning environments.</article>","contentLength":1713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Localized evaluation and fast summation in the extrapolated regularization method for integrals in Stokes flow","url":"https://arxiv.org/abs/2507.00156","date":1751428800,"author":"","guid":180285,"unread":true,"content":"<article>arXiv:2507.00156v1 Announce Type: new \nAbstract: Boundary integral equation methods are widely used in the solution of many partial differential equations. The kernels that appear in these surface integrals are nearly singular when evaluated near the boundary, and straightforward numerical integration produces inaccurate results. In Beale and Tlupova (Adv. Comput. Math, 2024), an extrapolated regularization method was proposed to accurately evaluate the nearly singular single and double-layer surface integrals for harmonic potentials or Stokes flow. The kernels are regularized using a smoothing parameter, and then a standard quadrature is applied. The integrals are computed for three choices of the smoothing parameter to find the extrapolated value to fifth order accuracy. In this work, we apply several techniques to reduce the computational cost of the extrapolated regularization method applied to the Stokes single and double layer integrals. First, we use a straightforward OpenMP parallelization over the target points. Second, we note that the effect of the regularization is local and evaluate only the local component of the sum for three values of the smoothing parameter. The non-local component of the sum is only evaluated once and reused in the other sums. This component is still the computational bottleneck as it is $O(N^2)$, where $N$ is the system size. We apply the kernel-independent treecode to these far-field interactions to reduce the CPU time. We carry out experiments to determine optimal parameters both in terms of accuracy and efficiency of the computations. We then use these techniques to compute Stokes flow around two spheres that are nearly touching.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics","url":"https://arxiv.org/abs/2507.00153","date":1751428800,"author":"","guid":180286,"unread":true,"content":"<article>arXiv:2507.00153v1 Announce Type: new \nAbstract: The performance of leaning-based perception algorithms suffer when deployed in out-of-distribution and underrepresented environments. Outdoor robots are particularly susceptible to rapid changes in visual scene appearance due to dynamic lighting, seasonality and weather effects that lead to scenes underrepresented in the training data of the learning-based perception system. In this conceptual paper, we focus on preparing our autonomous vehicle for deployment in snow-filled environments. We propose a novel method for diffusion-based image augmentation to more closely represent the deployment environment in our training data. Diffusion-based image augmentations rely on the public availability of vision foundation models learned on internet-scale datasets. The diffusion-based image augmentations allow us to take control over the semantic distribution of the ground surfaces in the training data and to fine-tune our model for its deployment environment. We employ open vocabulary semantic segmentation models to filter out augmentation candidates that contain hallucinations. We believe that diffusion-based image augmentations can be extended to many other environments apart from snow surfaces, like sandy environments and volcanic terrains.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data","url":"https://arxiv.org/abs/2507.00152","date":1751428800,"author":"","guid":180287,"unread":true,"content":"<article>arXiv:2507.00152v1 Announce Type: new \nAbstract: Tables are among the most widely used tools for representing structured data in research, business, medicine, and education. Although LLMs demonstrate strong performance in downstream tasks, their efficiency in processing tabular data remains underexplored. In this paper, we investigate the effectiveness of both text-based and multimodal LLMs on table understanding tasks through a cross-domain and cross-modality evaluation. Specifically, we compare their performance on tables from scientific vs. non-scientific contexts and examine their robustness on tables represented as images vs. text. Additionally, we conduct an interpretability analysis to measure context usage and input relevance. We also introduce the TableEval benchmark, comprising 3017 tables from scholarly publications, Wikipedia, and financial reports, where each table is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX. Our findings indicate that while LLMs maintain robustness across table modalities, they face significant challenges when processing scientific tables.</article>","contentLength":1121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sensitivity and Query Complexity under Uncertainty","url":"https://arxiv.org/abs/2507.00148","date":1751428800,"author":"","guid":180288,"unread":true,"content":"<article>arXiv:2507.00148v1 Announce Type: new \nAbstract: In this paper, we study the query complexity of Boolean functions in the presence of uncertainty, motivated by parallel computation with an unlimited number of processors where inputs are allowed to be unknown. We allow each query to produce three results: zero, one, or unknown. The output could also be: zero, one, or unknown, with the constraint that we should output ''unknown'' only when we cannot determine the answer from the revealed input bits. Such an extension of a Boolean function is called its hazard-free extension.\n  - We prove an analogue of Huang's celebrated sensitivity theorem [Annals of Mathematics, 2019] in our model of query complexity with uncertainty.\n  - We show that the deterministic query complexity of the hazard-free extension of a Boolean function is at most quadratic in its randomized query complexity and quartic in its quantum query complexity, improving upon the best-known bounds in the Boolean world.\n  - We exhibit an exponential gap between the smallest depth (size) of decision trees computing a Boolean function, and those computing its hazard-free extension.\n  - We present general methods to convert decision trees for Boolean functions to those for their hazard-free counterparts, and show optimality of this construction. We also parameterize this result by the maximum number of unknown values in the input.\n  - We show lower bounds on size complexity of decision trees for hazard-free extensions of Boolean functions in terms of the number of prime implicants and prime implicates of the underlying Boolean function.</article>","contentLength":1616,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise","url":"https://arxiv.org/abs/2507.00145","date":1751428800,"author":"","guid":180289,"unread":true,"content":"<article>arXiv:2507.00145v1 Announce Type: new \nAbstract: AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform entropy directly from physical noise, eliminating the need for bulky quantum devices or expensive laboratory-grade RF receivers. Instead, it relies on a low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and then emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number generators (RNGs), our dynamic inner-outer network couples adaptive natural sources and reseeding, yielding truly unpredictable and autonomous sequences. Generated numbers pass the NIST SP 800-22 battery better than a CPU-based method. It also passes nineteen bespoke statistical tests for both bit- and integer-level analysis. All results satisfy cryptographic standards, while forward and backward prediction experiments reveal no exploitable biases. The model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft cores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG broadens the reach of high-integrity random number generators across secure systems, cryptographic protocols, embedded and edge devices, stochastic simulations, and server applications that need randomness.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives","url":"https://arxiv.org/abs/2507.00108","date":1751428800,"author":"","guid":180290,"unread":true,"content":"<article>arXiv:2507.00108v1 Announce Type: new \nAbstract: Computer programming is undergoing a true transformation driven by powerful new tools for automatic source code generation based on large language models. This transformation is also manifesting in introductory programming courses at universities around the world, generating an in-depth debate about how programming content should be taught, learned, and assessed in the context of generative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on this issue, highlighting the advantages and disadvantages identified in the specialized literature. On the other hand, it proposes enriching teaching and learning methodologies by focusing on code comprehension and execution rather than on mere coding or program functionality. In particular, it advocates for the use of visual representations of code and visual simulations of its execution as effective tools for teaching, learning, and assessing programming, thus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming course are presented to provide preliminary context supporting the incorporation of visual simulations in Java (or other languages) as part of the training process.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Neural Networks in Wind Power Forecasting","url":"https://arxiv.org/abs/2507.00105","date":1751428800,"author":"","guid":180291,"unread":true,"content":"<article>arXiv:2507.00105v1 Announce Type: new \nAbstract: We study the applicability of GNNs to the problem of wind energy forecasting. We find that certain architectures achieve performance comparable to our best CNN-based benchmark. The study is conducted on three wind power facilities using five years of historical data. Numerical Weather Prediction (NWP) variables were used as predictors, and models were evaluated on a 24 to 36 hour ahead test horizon.</article>","contentLength":451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series","url":"https://arxiv.org/abs/2507.00102","date":1751428800,"author":"","guid":180292,"unread":true,"content":"<article>arXiv:2507.00102v1 Announce Type: new \nAbstract: Ensuring consistent product quality in modern manufacturing is crucial, particularly in safety-critical applications. Conventional quality control approaches, reliant on manually defined thresholds and features, lack adaptability to the complexity and variability inherent in production data and necessitate extensive domain expertise. Conversely, data-driven methods, such as machine learning, demonstrate high detection performance but typically function as black-box models, thereby limiting their acceptance in industrial environments where interpretability is paramount. This paper introduces a methodology for industrial fault detection, which is both data-driven and transparent. The approach integrates a supervised machine learning model for multi-class fault classification, Shapley Additive Explanations for post-hoc interpretability, and a do-main-specific visualisation technique that maps model explanations to operator-interpretable features. Furthermore, the study proposes an evaluation methodology that assesses model explanations through quantitative perturbation analysis and evaluates visualisations by qualitative expert assessment. The approach was applied to the crimping process, a safety-critical joining technique, using a dataset of univariate, discrete time series. The system achieves a fault detection accuracy of 95.9 %, and both quantitative selectivity analysis and qualitative expert evaluations confirmed the relevance and inter-pretability of the generated explanations. This human-centric approach is designed to enhance trust and interpretability in data-driven fault detection, thereby contributing to applied system design in industrial quality control.</article>","contentLength":1743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks","url":"https://arxiv.org/abs/2507.00101","date":1751428800,"author":"","guid":180293,"unread":true,"content":"<article>arXiv:2507.00101v1 Announce Type: new \nAbstract: We introduce DFReg, a physics-inspired regularization method for deep neural networks that operates on the global distribution of weights. Drawing from Density Functional Theory (DFT), DFReg applies a functional penalty to encourage smooth, diverse, and well-distributed weight configurations. Unlike traditional techniques such as Dropout or L2 decay, DFReg imposes global structural regularity without architectural changes or stochastic perturbations.</article>","contentLength":503,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets","url":"https://arxiv.org/abs/2507.00096","date":1751428800,"author":"","guid":180294,"unread":true,"content":"<article>arXiv:2507.00096v1 Announce Type: new \nAbstract: Alternative Assets tokenization is transforming non-traditional financial instruments are represented and traded on the web. However, ensuring trustworthiness in web-based tokenized ecosystems poses significant challenges, from verifying off-chain asset data to enforcing regulatory compliance. This paper proposes an AI-governed agent architecture that integrates intelligent agents with blockchain to achieve web-trustworthy tokenization of alternative assets. In the proposed architecture, autonomous agents orchestrate the tokenization process (asset verification, valuation, compliance checking, and lifecycle management), while an AI-driven governance layer monitors agent behavior and enforces trust through adaptive policies and cryptoeconomic incentives. We demonstrate that this approach enhances transparency, security, and compliance in asset tokenization, addressing key concerns around data authenticity and fraud. A case study on tokenizing real estate assets illustrates how the architecture mitigates risks (e.g., fraudulent listings and money laundering) through real-time AI anomaly detection and on-chain enforcement. Our evaluation and analysis suggest that combining AI governance with multi-agent systems and blockchain can significantly bolster trust in tokenized asset ecosystems. This work offers a novel framework for trustworthy asset tokenization on the web and provides insights for practitioners aiming to deploy secure, compliant tokenization platforms.</article>","contentLength":1534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)","url":"https://arxiv.org/abs/2507.00094","date":1751428800,"author":"","guid":180295,"unread":true,"content":"<article>arXiv:2507.00094v1 Announce Type: new \nAbstract: Despite growing interest in process analysis and mining for data-aware specifications, alignment-based conformance checking for declarative process models has focused on pure control-flow specifications, or mild data-aware extensions limited to numerical data and variable-to-constant comparisons. This is not surprising: finding alignments is computationally hard, even more so in the presence of data dependencies. In this paper, we challenge this problem in the case where the reference model is captured using data-aware Declare with general data types and data conditions. We show that, unexpectedly, it is possible to compute data-aware optimal alignments in this rich setting, enjoying at once efficiency and expressiveness. This is achieved by carefully combining the two best-known approaches to deal with control flow and data dependencies when computing alignments, namely A* search and SMT solving. Specifically, we introduce a novel algorithmic technique that efficiently explores the search space, generating descendant states through the application of repair actions aiming at incrementally resolving constraint violations. We prove the correctness of our algorithm and experimentally show its efficiency. The evaluation witnesses that our approach matches or surpasses the performance of the state of the art while also supporting significantly more expressive data dependencies, showcasing its potential to support real-world applications.</article>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\sigma$-Maximal Ancestral Graphs","url":"https://arxiv.org/abs/2507.00093","date":1751428800,"author":"","guid":180296,"unread":true,"content":"<article>arXiv:2507.00093v1 Announce Type: new \nAbstract: Maximal Ancestral Graphs (MAGs) provide an abstract representation of Directed Acyclic Graphs (DAGs) with latent (selection) variables. These graphical objects encode information about ancestral relations and d-separations of the DAGs they represent. This abstract representation has been used amongst others to prove the soundness and completeness of the FCI algorithm for causal discovery, and to derive a do-calculus for its output. One significant inherent limitation of MAGs is that they rule out the possibility of cyclic causal relationships. In this work, we address that limitation. We introduce and study a class of graphical objects that we coin ''$\\sigma$-Maximal Ancestral Graphs'' (''$\\sigma$-MAGs''). We show how these graphs provide an abstract representation of (possibly cyclic) Directed Graphs (DGs) with latent (selection) variables, analogously to how MAGs represent DAGs. We study the properties of these objects and provide a characterization of their Markov equivalence classes.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models","url":"https://arxiv.org/abs/2507.00092","date":1751428800,"author":"","guid":180297,"unread":true,"content":"<article>arXiv:2507.00092v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Optimality of Coded Distributed Computing for Ring Networks","url":"https://arxiv.org/abs/2507.00091","date":1751428800,"author":"","guid":180298,"unread":true,"content":"<article>arXiv:2507.00091v1 Announce Type: new \nAbstract: We consider a coded distributed computing problem in a ring-based communication network, where $N$ computing nodes are arranged in a ring topology and each node can only communicate with its neighbors within a constant distance $d$. To mitigate the communication bottleneck in exchanging intermediate values, we propose new coded distributed computing schemes for the ring-based network that exploit both ring topology and redundant computation (i.e., each map function is computed by $r$ nodes). Two typical cases are considered: all-gather where each node requires all intermediate values mapped from all input files, and all-to-all where each node requires a distinct set of intermediate values from other nodes. For the all-gather case, we propose a new coded scheme based on successive reverse carpooling where nodes transmit every encoded packet containing two messages traveling in opposite directions along the same path. Theoretical converse proof shows that our scheme achieves the optimal tradeoff between communication load, computation load $r$, and broadcast distance $d$ when $N\\gg d$. For the all-to-all case, instead of simply repeating our all-gather scheme, we delicately deliver intermediate values based on their proximity to intended nodes to reduce unnecessary transmissions. We derive an information-theoretic lower bound on the optimal communication load and show that our scheme is asymptotically optimal under the cyclic placement when $N\\gg r$. The optimality results indicate that in ring-based networks, the redundant computation $r$ only leads to an additive gain in reducing communication load while the broadcast distance $d$ contributes to a multiplicative gain.</article>","contentLength":1745,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating Heterogeneous Multi-dimensional Data : A Comparative Study","url":"https://arxiv.org/abs/2507.00090","date":1751428800,"author":"","guid":180299,"unread":true,"content":"<article>arXiv:2507.00090v1 Announce Type: new \nAbstract: Allocation of personnel and material resources is highly sensible in the case of firefighter interventions. This allocation relies on simulations to experiment with various scenarios. The main objective of this allocation is the global optimization of the firefighters response. Data generation is then mandatory to study various scenarios In this study, we propose to compare different data generation methods. Methods such as Random Sampling, Tabular Variational Autoencoders, standard Generative Adversarial Networks, Conditional Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are examined to ascertain their efficacy in capturing the intricacies of firefighter interventions. Traditional evaluation metrics often fall short in capturing the nuanced requirements of synthetic datasets for real-world scenarios. To address this gap, an evaluation of synthetic data quality is conducted using a combination of domain-specific metrics tailored to the firefighting domain and standard measures such as the Wasserstein distance. Domain-specific metrics include response time distribution, spatial-temporal distribution of interventions, and accidents representation. These metrics are designed to assess data variability, the preservation of fine and complex correlations and anomalies such as event with a very low occurrence, the conformity with the initial statistical distribution and the operational relevance of the synthetic data. The distribution has the particularity of being highly unbalanced, none of the variables following a Gaussian distribution, adding complexity to the data generation process.</article>","contentLength":1686,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A new machine learning framework for occupational accidents forecasting with safety inspections integration","url":"https://arxiv.org/abs/2507.00089","date":1751428800,"author":"","guid":180300,"unread":true,"content":"<article>arXiv:2507.00089v1 Announce Type: new \nAbstract: We propose a generic framework for short-term occupational accident forecasting that leverages safety inspections and models accident occurrences as binary time series. The approach generates daily predictions, which are then aggregated into weekly safety assessments to better inform decision making. To ensure the reliability and operational applicability of the forecasts, we apply a sliding-window cross-validation procedure specifically designed for time series data, combined with an evaluation based on aggregated period-level metrics. Several machine learning algorithms, including logistic regression, tree-based models, and neural networks, are trained and systematically compared within this framework. Unlike the other approaches, the long short-term memory (LSTM) network outperforms the other approaches and detects the upcoming high-risk periods with a balanced accuracy of 0.86, confirming the robustness of our methodology and demonstrating that a binary time series model can anticipate these critical periods based on safety inspections. The proposed methodology converts routine safety inspection data into clear weekly risk scores, detecting the periods when accidents are most likely. Decision-makers can integrate these scores into their planning tools to classify inspection priorities, schedule targeted interventions, and funnel resources to the sites or shifts classified as highest risk, stepping in before incidents occur and getting the greatest return on safety investments.</article>","contentLength":1554,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation","url":"https://arxiv.org/abs/2507.00087","date":1751428800,"author":"","guid":180301,"unread":true,"content":"<article>arXiv:2507.00087v1 Announce Type: new \nAbstract: Deep learning has advanced mass spectrometry data interpretation, yet most models remain feature extractors rather than unified scoring frameworks. We present pUniFind, the first large-scale multimodal pre-trained model in proteomics that integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing. Trained on over 100 million open search-derived spectra, pUniFind aligns spectral and peptide modalities via cross modality prediction and outperforms traditional engines across diverse datasets, particularly achieving a 42.6 percent increase in the number of identified peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind identifies 60 percent more PSMs than existing de novo methods despite a 300-fold larger search space. A deep learning based quality control module further recovers 38.5 percent additional peptides including 1,891 mapped to the genome but absent from reference proteomes while preserving full fragment ion coverage. These results establish a unified, scalable deep learning framework for proteomic analysis, offering improved sensitivity, modification coverage, and interpretability.</article>","contentLength":1206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism","url":"https://arxiv.org/abs/2507.00085","date":1751428800,"author":"","guid":180302,"unread":true,"content":"<article>arXiv:2507.00085v1 Announce Type: new \nAbstract: Accurate traffic prediction is essential for Intelligent Transportation Systems (ITS), yet current methods struggle with the inherent complexity and non-linearity of traffic dynamics, making it difficult to integrate spatial and temporal characteristics. Furthermore, existing approaches use static techniques to address non-stationary and anomalous historical data, which limits adaptability and undermines data smoothing. To overcome these challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative framework for network-level traffic speed prediction. GFEN introduces a novel topological spatiotemporal graph fusion technique that meticulously extracts and merges spatial and temporal correlations from both data distribution and network topology using trainable methods, enabling the modeling of multi-scale spatiotemporal features. Additionally, GFEN employs a hybrid methodology combining a k-th order difference-based mathematical framework with an attention-based deep learning structure to adaptively smooth historical observations and dynamically mitigate data anomalies and non-stationarity. Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods by approximately 6.3% in prediction accuracy and exhibits convergence rates nearly twice as fast as recent hybrid models, confirming its superior performance and potential to significantly enhance traffic prediction system efficiency.</article>","contentLength":1485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks","url":"https://arxiv.org/abs/2507.00083","date":1751428800,"author":"","guid":180303,"unread":true,"content":"<article>arXiv:2507.00083v1 Announce Type: new \nAbstract: This study addresses the lack of structured causal modeling between tactical strike behavior and strategic delay in current strategic-level simulations, particularly the structural bottlenecks in capturing intermediate variables within the \"resilience - nodal suppression - negotiation window\" chain. We propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN), a novel framework that closes the causal loop from tactical input to strategic delay output. The model integrates graph attention mechanisms, counterfactual simulation units, and spatial intervention node reconstruction to enable dynamic simulations of strike configurations and synchronization strategies. Training data are generated from a multi-physics simulation platform (GEANT4 + COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and policy-level validation. Experimental results demonstrate that IA-STGNN significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost), achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5 percent accuracy, while improving causal path consistency and intervention stability. IA-STGNN enables interpretable prediction of strategic delay and supports applications such as nuclear deterrence simulation, diplomatic window assessment, and multi-strategy optimization, providing a structured and transparent AI decision-support mechanism for high-level policy modeling.</article>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission","url":"https://arxiv.org/abs/2507.00082","date":1751428800,"author":"","guid":180304,"unread":true,"content":"<article>arXiv:2507.00082v1 Announce Type: new \nAbstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small Language Models (SLMs) on edge devices with the high accuracy of Large Language Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM inference, HLMs reduce latency and communication by invoking LLMs only when local SLM predictions are uncertain, i.e., when token-level confidence is low or entropy is high. However, ambiguous or low-confidence predictions still require frequent offloading to the LLM, leading to significant communication overhead in bandwidth-constrained settings. To address this, we propose FedHLM, a communication-efficient HLM framework that integrates uncertainty-aware inference with Federated Learning (FL). FedHLM's key innovation lies in collaboratively learning token-level uncertainty thresholds that govern when LLM assistance is needed. Rather than using static or manually tuned thresholds, FedHLM employs FL to optimize these thresholds in a privacy-preserving, distributed manner. Additionally, it leverages embedding-based token representations for Peer-to-Peer (P2P) resolution, enabling clients to reuse tokens inferred by semantically similar peers without engaging the LLM. We further introduce hierarchical model aggregation: edge servers refine local routing policies through client updates, while cross-cluster coordination aligns global decision boundaries. This layered design captures recurring uncertainty patterns, reducing redundant LLM queries. Experiments on large-scale news classification tasks show that FedHLM reduces LLM transmissions by over 95 percent with negligible accuracy loss, making it well-suited for scalable and efficient edge-AI applications.</article>","contentLength":1752,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"State and Memory is All You Need for Robust and Reliable AI Agents","url":"https://arxiv.org/abs/2507.00081","date":1751428800,"author":"","guid":180305,"unread":true,"content":"<article>arXiv:2507.00081v1 Announce Type: new \nAbstract: Large language models (LLMs) have enabled powerful advances in natural language understanding and generation. Yet their application to complex, real-world scientific workflows remain limited by challenges in memory, planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals), a modular agentic framework that allows LLM-based agents to autonomously plan, reason, and achieve robust and reliable domain-specific task execution. Agents are constructed dynamically from source code documentation and augmented with finite-state automata (FSA) memory, enabling persistent state tracking and context-aware decision-making. This approach eliminates the need for manual prompt engineering and allows for robust, scalable deployment across diverse applications via maintaining context across extended workflows and to recover from tool or execution failures. We validate SciBORG through integration with both physical and virtual hardware, such as microwave synthesizers for executing user-specified reactions, with context-aware decision making and demonstrate its use in autonomous multi-step bioassay retrieval from the PubChem database utilizing multi-step planning, reasoning, agent-to-agent communication and coordination for execution of exploratory tasks. Systematic benchmarking shows that SciBORG agents achieve reliable execution, adaptive planning, and interpretable state transitions. Our results show that memory and state awareness are critical enablers of agentic planning and reliability, offering a generalizable foundation for deploying AI agents in complex environments.</article>","contentLength":1712,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Meal Detection Based on CGM Data Dynamics","url":"https://arxiv.org/abs/2507.00080","date":1751428800,"author":"","guid":180306,"unread":true,"content":"<article>arXiv:2507.00080v1 Announce Type: new \nAbstract: We utilize dynamical modes as features derived from Continuous Glucose Monitoring (CGM) data to detect meal events. By leveraging the inherent properties of underlying dynamics, these modes capture key aspects of glucose variability, enabling the identification of patterns and anomalies associated with meal consumption. This approach not only improves the accuracy of meal detection but also enhances the interpretability of the underlying glucose dynamics. By focusing on dynamical features, our method provides a robust framework for feature extraction, facilitating generalization across diverse datasets and ensuring reliable performance in real-world applications. The proposed technique offers significant advantages over traditional approaches, improving detection accuracy,</article>","contentLength":832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems","url":"https://arxiv.org/abs/2507.00079","date":1751428800,"author":"","guid":180307,"unread":true,"content":"<article>arXiv:2507.00079v1 Announce Type: new \nAbstract: Open-endedness is an active field of research in the pursuit of capable Artificial General Intelligence (AGI), allowing models to pursue tasks of their own choosing. Simultaneously, recent advancements in Large Language Models (LLMs) such as GPT-4o [9] have allowed such models to be capable of interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use of such features, providing an LLM with pixel data of an agent's POV to parse the environment and allow it to solve tasks. This paper proposes that providing these visual inputs to a model gives it greater ability to interpret spatial environments, and as such, can increase the number of tasks it can successfully perform, extending its open-ended potential. To this aim, this paper proposes VoyagerVision -- a multi-modal model capable of creating structures within Minecraft using screenshots as a form of visual feedback, building on the foundation of Voyager. VoyagerVision was capable of creating an average of 2.75 unique structures within fifty iterations of the system, as Voyager was incapable of this, it is an extension in an entirely new direction. Additionally, in a set of building unit tests VoyagerVision was successful in half of all attempts in flat worlds, with most failures arising in more complex structures. Project website is available at https://esmyth-dev.github.io/VoyagerVision.github.io/</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The language of time: a language model perspective on time-series foundation models","url":"https://arxiv.org/abs/2507.00078","date":1751428800,"author":"","guid":180308,"unread":true,"content":"<article>arXiv:2507.00078v1 Announce Type: new \nAbstract: With the rise of large language models, the paradigm of training foundation models with massive parameter counts on vast datasets has been adopted in multiple domains to achieve remarkable success. Time series foundation models represent a significant extension of this paradigm, demonstrating exceptional expressive power, generalization, and cross-domain transferability. However, this gives rise to a fundamental paradox: time series data reflect distinct dynamical systems, making cross-domain transfer intuitively implausible, yet this is contradicted by the models' empirical success. To resolve this paradox, this paper investigates, from both theoretical and experimental perspectives, the representation learning mechanisms and generalization capabilities of patch-based time series foundation models. We argue that such models are not merely applying a new architecture but are fundamentally generalizing the representation paradigm of language models by extending deterministic vector-based representations to latent probabilistic distributional forms. Our theoretical analysis supports this framework by demonstrating that continuous time-series patches can be faithfully quantized into a discrete vocabulary whose key statistical properties are highly consistent with those of natural language. This generalization allows time series models to inherit the robust representation and transfer abilities of large language models, thereby explaining their superior performance in temporal tasks. Ultimately, our work provides a rigorous theoretical cornerstone for understanding, evaluating, and improving the safety and reliability of large-scale time series foundation models.</article>","contentLength":1736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap","url":"https://arxiv.org/abs/2507.00075","date":1751428800,"author":"","guid":180309,"unread":true,"content":"<article>arXiv:2507.00075v1 Announce Type: new \nAbstract: Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further introduce how to predict the ultimate power of self-improvement using only information from the first few training epochs. We empirically validate the effectiveness of the theoretical model on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory","url":"https://arxiv.org/abs/2507.00073","date":1751428800,"author":"","guid":180310,"unread":true,"content":"<article>arXiv:2507.00073v1 Announce Type: new \nAbstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning framework incorporating fractional calculus for long-term temporal modeling in policy optimization. Standard policy gradient approaches face limitations from Markovian assumptions, exhibiting high variance and inefficient sampling. By reformulating gradients using Caputo fractional derivatives, FPG establishes power-law temporal correlations between state transitions. We develop an efficient recursive computation technique for fractional temporal-difference errors with constant time and memory requirements. Theoretical analysis shows FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus standard policy gradients while preserving convergence. Empirical validation demonstrates 35-68% sample efficiency gains and 24-52% variance reduction versus state-of-the-art baselines. This framework provides a mathematically grounded approach for leveraging long-range dependencies without computational overhead.</article>","contentLength":1048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An efficient plant disease detection using transfer learning approach","url":"https://arxiv.org/abs/2507.00070","date":1751428800,"author":"","guid":180311,"unread":true,"content":"<article>arXiv:2507.00070v1 Announce Type: new \nAbstract: Plant diseases pose significant challenges to farmers and the agricultural sector at large. However, early detection of plant diseases is crucial to mitigating their effects and preventing widespread damage, as outbreaks can severely impact the productivity and quality of crops. With advancements in technology, there are increasing opportunities for automating the monitoring and detection of disease outbreaks in plants. This study proposed a system designed to identify and monitor plant diseases using a transfer learning approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two state-ofthe-art models in the field of object detection. By fine-tuning these models on a dataset of plant leaf images, the system is able to accurately detect the presence of Bacteria, Fungi and Viral diseases such as Powdery Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's performance was evaluated using several metrics, including mean Average Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05, 89.40, 91.22, and 87.66, respectively. The result demonstrates the superior effectiveness and efficiency of YOLOv8 compared to other object detection methods, highlighting its potential for use in modern agricultural practices. The approach provides a scalable, automated solution for early any plant disease detection, contributing to enhanced crop yield, reduced reliance on manual monitoring, and supporting sustainable agricultural practices.</article>","contentLength":1540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding","url":"https://arxiv.org/abs/2507.00068","date":1751428800,"author":"","guid":180312,"unread":true,"content":"<article>arXiv:2507.00068v1 Announce Type: new \nAbstract: While multi-modal learning has advanced significantly, current approaches often treat modalities separately, creating inconsistencies in representation and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization via Textual Alignment), a theoretically-grounded framework that unifies visual and auditory inputs into a structured textual space for seamless processing with large language models. MANTA addresses four key challenges: (1) semantic alignment across modalities with information-theoretic optimization, (2) adaptive temporal synchronization for varying information densities, (3) hierarchical content representation for multi-scale understanding, and (4) context-aware retrieval of sparse information from long sequences. We formalize our approach within a rigorous mathematical framework, proving its optimality for context selection under token constraints. Extensive experiments on the challenging task of Long Video Question Answering show that MANTA improves state-of-the-art models by up to 22.6% in overall accuracy, with particularly significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement) and cross-modal understanding (25.1% improvement). Our framework introduces novel density estimation techniques for redundancy minimization while preserving rare signals, establishing new foundations for unifying multimodal representations through structured text.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph","url":"https://arxiv.org/abs/2507.00066","date":1751428800,"author":"","guid":180313,"unread":true,"content":"<article>arXiv:2507.00066v1 Announce Type: new \nAbstract: Human reliability remains a critical concern in safety-critical domains such as nuclear power, where operational failures are often linked to human error. While conventional human reliability analysis (HRA) methods have been widely adopted, they rely heavily on expert judgment for identifying human failure events (HFEs) and assigning performance influencing factors (PIFs). This reliance introduces challenges related to reproducibility, subjectivity, and limited integration of interface-level data. In particular, current approaches lack the capacity to rigorously assess how human-machine interface design contributes to operator performance variability and error susceptibility. To address these limitations, this study proposes a framework for risk-informed human failure event identification and interface-induced risk assessment driven by AutoGraph (InSight-R). By linking empirical behavioral data to the interface-embedded knowledge graph (IE-KG) constructed by the automated graph-based execution framework (AutoGraph), the InSight-R framework enables automated HFE identification based on both error-prone and time-deviated operational paths. Furthermore, we discuss the relationship between designer-user conflicts and human error. The results demonstrate that InSight-R not only enhances the objectivity and interpretability of HFE identification but also provides a scalable pathway toward dynamic, real-time human reliability assessment in digitalized control environments. This framework offers actionable insights for interface design optimization and contributes to the advancement of mechanism-driven HRA methodologies.</article>","contentLength":1689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data","url":"https://arxiv.org/abs/2507.00061","date":1751428800,"author":"","guid":180314,"unread":true,"content":"<article>arXiv:2507.00061v1 Announce Type: new \nAbstract: This paper introduces Smooth-Distill, a novel self-distillation framework designed to simultaneously perform human activity recognition (HAR) and sensor placement detection using wearable sensor data. The proposed approach utilizes a unified CNN-based architecture, MTL-net, which processes accelerometer data and branches into two outputs for each respective task. Unlike conventional distillation methods that require separate teacher and student models, the proposed framework utilizes a smoothed, historical version of the model itself as the teacher, significantly reducing training computational overhead while maintaining performance benefits. To support this research, we developed a comprehensive accelerometer-based dataset capturing 12 distinct sleep postures across three different wearing positions, complementing two existing public datasets (MHealth and WISDM). Experimental results show that Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving notable improvements in both human activity recognition and device placement detection tasks. This method demonstrates enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines. This framework contributes to the practical implementation of knowledge distillation in human activity recognition systems, offering an effective solution for multitask learning with accelerometer data that balances accuracy and training efficiency. More broadly, it reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms. The code and model are available at https://github.com/Kuan2vn/smooth\\_distill.</article>","contentLength":1842,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Verification of Hamiltonian Path Conjecture (BHR Conjecture) for Integers up to p=31","url":"https://arxiv.org/abs/2507.00059","date":1751428800,"author":"","guid":180315,"unread":true,"content":"<article>arXiv:2507.00059v1 Announce Type: new \nAbstract: The BHR (Buratti-Horak-Rosa) Conjecture (2006) proposes that for every p and a multiset L of (p-1) positive integers modulo p, there exists a Hamiltonian path in the Complete Graph Kp with consecutive edge lengths given by the elements of L. In this article, we outline an approach to the conjecture based on frequency partitions and local/global adjustment operations and backtracking. We describe the mathematical strategy, experimental evidence, and implementation in a Python Program to explore valid Hamiltonian paths p &lt; 37. This is a result an improvement over by Mariusz Meszka for all primes up to 23 (included) with the aid of a computer.</article>","contentLength":697,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Estimating Correctness Without Oracles in LLM-Based Code Generation","url":"https://arxiv.org/abs/2507.00057","date":1751428800,"author":"","guid":180316,"unread":true,"content":"<article>arXiv:2507.00057v1 Announce Type: new \nAbstract: Generating code from natural language specifications is one of the most successful applications of Large Language Models (LLMs). Yet, they hallucinate: LLMs produce outputs that may be grammatically correct but are factually incorrect. Without an existing, correct implementation (i.e., an oracle), can we quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence, that can be estimated efficiently in the absence of an oracle and provides a lower bound on the error, i.e., the probability that the LLM-generated program for that specification is incorrect. Our experiments demonstrate an extraordinary effectiveness. For the average code generation task, our incoherence-based methodology can automatically identify about two-thirds of incorrect programs without reports of false positives. In fact, an oracle-based evaluation of LLMs can be reliably replaced by an incoherence-based evaluation. In particular, we find a very strong agreement between the ranking of LLMs by the number of programs deemed correct via an oracle (pass@1) and the ranking of LLMs by the number of programs deemed correct via our incoherence.</article>","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation","url":"https://arxiv.org/abs/2507.00055","date":1751428800,"author":"","guid":180317,"unread":true,"content":"<article>arXiv:2507.00055v1 Announce Type: new \nAbstract: Voice interfaces integral to the human-computer interaction systems can benefit from speech emotion recognition (SER) to customize responses based on user emotions. Since humans convey emotions through multi-modal audio-visual cues, developing SER systems using both the modalities is beneficial. However, collecting a vast amount of labeled data for their development is expensive. This paper proposes a knowledge distillation framework called LightweightSER (LiSER) that leverages unlabeled audio-visual data for SER, using large teacher models built on advanced speech and face representation models. LiSER transfers knowledge regarding speech emotions and facial expressions from the teacher models to lightweight student models. Experiments conducted on two benchmark datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence on extensive labeled datasets for SER tasks.</article>","contentLength":946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation","url":"https://arxiv.org/abs/2507.00054","date":1751428800,"author":"","guid":180318,"unread":true,"content":"<article>arXiv:2507.00054v1 Announce Type: new \nAbstract: The push to compress and impart the proficiency of Large Language Models (LLMs) into more deployable and efficient Small Language Models (SLMs) has benefited from improvements in knowledge distillation (KD) techniques. These techniques allow a smaller student model to learn from a more capable and larger teacher model's responses. However, distillation often revolves around the student model merely copying the teacher's in-distribution responses, limiting its generalisability. This limitation is amplified on reasoning tasks and can be computationally expensive. In this study, we propose AdvDistill, a reward-guided dataset distillation framework. We utilise multiple generations (responses) from a teacher for each prompt and assign rewards based on rule-based verifiers. These varying and normally distributed rewards serve as weights when training student models. Our methods and their subsequent behavioural analysis demonstrate a significant improvement in student model performance for mathematical and complex reasoning tasks, showcasing the efficacy and benefits of incorporating a rewarding mechanism in dataset distillation processes.</article>","contentLength":1199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models","url":"https://arxiv.org/abs/2507.00052","date":1751428800,"author":"","guid":180319,"unread":true,"content":"<article>arXiv:2507.00052v1 Announce Type: new \nAbstract: Vision Language Models (VLMs) hold great promise for streamlining labour-intensive medical imaging workflows, yet systematic security evaluations in clinical settings remain scarce. We introduce VSF--Med, an end-to-end vulnerability-scoring framework for medical VLMs that unites three novel components: (i) a rich library of sophisticated text-prompt attack templates targeting emerging threat vectors; (ii) imperceptible visual perturbations calibrated by structural similarity (SSIM) thresholds to preserve clinical realism; and (iii) an eight-dimensional rubric evaluated by two independent judge LLMs, whose raw scores are consolidated via z-score normalization to yield a 0--32 composite risk metric. Built entirely on publicly available datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000 adversarial variants from 5,000 radiology images and enables reproducible benchmarking of any medical VLM with a single command. Our consolidated analysis reports mean z-score shifts of $0.90\\sigma$ for persistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness, and $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs. Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase of $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases of $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection attacks.</article>","contentLength":1469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network","url":"https://arxiv.org/abs/2507.00050","date":1751428800,"author":"","guid":180320,"unread":true,"content":"<article>arXiv:2507.00050v1 Announce Type: new \nAbstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement Unit (IMU) sensors, has many practical applications in healthcare and assisted living environments. However, its use in real-world scenarios has been limited by the lack of comprehensive IMU-based HAR datasets that cover a wide range of activities and the lack of transparency in existing HAR models. Zero-shot HAR (ZS-HAR) overcomes the data limitations, but current models struggle to explain their decisions, making them less transparent. This paper introduces a novel IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity Recognition Network (SEZ-HARN). It can recognize activities not encountered during training and provide skeleton videos to explain its decision-making process. We evaluate the effectiveness of the proposed SEZ-HARN on four benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its performance against three state-of-the-art black-box ZS-HAR models. The experiment results demonstrate that SEZ-HARN produces realistic and understandable explanations while achieving competitive Zero-shot recognition accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the best-performing black-box model on PAMAP2 while maintaining comparable performance on the other three datasets.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training","url":"https://arxiv.org/abs/2507.00049","date":1751428800,"author":"","guid":180321,"unread":true,"content":"<article>arXiv:2507.00049v1 Announce Type: new \nAbstract: The computational burden and inherent redundancy of large-scale datasets challenge the training of contemporary machine learning models. Data pruning offers a solution by selecting smaller, informative subsets, yet existing methods struggle: density-based approaches can be task-agnostic, while model-based techniques may introduce redundancy or prove computationally prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid framework that synergistically integrates density-based pruning with model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions data and applies an initial density-based pruning. It then employs a proxy model to evaluate the impact of this initial pruning within each cluster by comparing losses on kept versus pruned samples. This task-aware signal adaptively adjusts cluster-specific pruning thresholds, enabling more aggressive pruning in redundant clusters while preserving critical data in informative ones. Extensive experiments on large-scale object detection benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms prominent baselines, substantially reduces performance degradation (e.g., over 54% versus random sampling on Waymo), and achieves near-original model performance while pruning 20% of data, highlighting its efficacy in enhancing data efficiency for large-scale model training. Code is open-sourced.</article>","contentLength":1526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A collaborative digital twin built on FAIR data and compute infrastructure","url":"https://arxiv.org/abs/2507.00048","date":1751428800,"author":"","guid":180322,"unread":true,"content":"<article>arXiv:2507.00048v1 Announce Type: new \nAbstract: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin\", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.</article>","contentLength":1837,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reducing Profile-Based Matching to the Maximum Weight Matching Problem","url":"https://arxiv.org/abs/2507.00047","date":1751428800,"author":"","guid":180323,"unread":true,"content":"<article>arXiv:2507.00047v1 Announce Type: new \nAbstract: The profile-based matching problem is the problem of finding a matching that optimizes profile from an instance $(G, r, \\langle u_1, \\dots, u_r \\rangle)$, where $G$ is a bipartite graph $(A \\cup B, E)$, $r$ is the number of utility functions, and $u_i: E \\to \\{ 0, 1, \\dots, U_i \\}$ is utility functions for $1 \\le i \\le r$. A matching is optimal if the matching maximizes the sum of the 1st utility, subject to this, maximizes the sum of the 2nd utility, and so on. The profile-based matching can express rank-maximal matching \\cite{irving2006rank}, fair matching \\cite{huang2016fair}, and weight-maximal matching \\cite{huang2012weight}. These problems can be reduced to maximum weight matching problems, but the reduction is known to be inefficient due to the huge weights.\n  This paper presents the condition for a weight function to find an optimal matching by reducing profile-based matching to the maximum weight matching problem. It is shown that a weight function which represents utilities as a mixed-radix numeric system with base-$(2U_i+1)$ can be used, so the complexity of the problem is $O(m\\sqrt{n}(\\log{n} + \\sum_{i=1}^{r}\\log{U_i}))$ for $n = |V|$, $m = |E|$. In addition, it is demonstrated that the weight lower bound for rank-maximal/fair/weight-maximal matching, better computational complexity for fair/weight-maximal matching, and an algorithm to verify a maximum weight matching can be reduced to rank-maximal matching. Finally, the effectiveness of the profile-based algorithm is evaluated with real data for school choice lottery.</article>","contentLength":1605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process","url":"https://arxiv.org/abs/2507.00046","date":1751428800,"author":"","guid":180324,"unread":true,"content":"<article>arXiv:2507.00046v1 Announce Type: new \nAbstract: This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.</article>","contentLength":1531,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning","url":"https://arxiv.org/abs/2507.00045","date":1751428800,"author":"","guid":180325,"unread":true,"content":"<article>arXiv:2507.00045v1 Announce Type: new \nAbstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have achieved near-ceiling scores on various existing benchmarks, motivating a demand for more challenging test tasks. These MLLMs have been reported to excel in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their potential as a detective who can notice minuscule cues in an image and weave them into coherent, situational explanations, leading to a reliable answer. But can they match the performance of excellent human detectives? To answer this question, we investigate some hard scenarios where GPT-o3 can still handle, and find a common scenario where o3's performance drops to nearly zero, which we name CaughtCheating. It is inspired by the social media requests that ask others to detect suspicious clues from photos shared by the poster's partner. We conduct extensive experiments and analysis to understand why existing MLLMs lack sufficient capability to solve this kind of task. CaughtCheating provides a class of challenging visual perception and reasoning tasks with great value and practical usage. Success in these tasks paves the way for MLLMs to acquire human-level detective perception and reasoning capabilities.</article>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HistoART: Histopathology Artifact Detection and Reporting Tool","url":"https://arxiv.org/abs/2507.00044","date":1751428800,"author":"","guid":180326,"unread":true,"content":"<article>arXiv:2507.00044v1 Announce Type: new \nAbstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to digitize tissue specimens for detailed, high-resolution examination; however, other diagnostic approaches, such as liquid biopsy and molecular testing, are also utilized based on the cancer type and clinical context. While WSI has revolutionized digital histopathology by enabling automated, precise analysis, it remains vulnerable to artifacts introduced during slide preparation and scanning. These artifacts can compromise downstream image analysis. To address this challenge, we propose and compare three robust artifact detection approaches for WSIs: (1) a foundation model-based approach (FMA) using a fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach (KBA) leveraging handcrafted features from texture, color, and frequency-based metrics. The methods target six common artifact types: tissue folds, out-of-focus regions, air bubbles, tissue damage, marker traces, and blood contamination. Evaluations were conducted on 50,000+ image patches from diverse scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]), outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978]) and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into actionable insights, we developed a quality report scorecard that quantifies high-quality patches and visualizes artifact distributions.</article>","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations","url":"https://arxiv.org/abs/2507.00043","date":1751428800,"author":"","guid":180327,"unread":true,"content":"<article>arXiv:2507.00043v1 Announce Type: new \nAbstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical systems is based on a precise understanding of image contrast. This contrast is primarily governed by acquisition parameters, such as echo time and repetition time, which are stored in the DICOM metadata. To simplify contrast identification, broad labels such as T1-weighted or T2-weighted are commonly used, but these offer only a coarse approximation of the underlying acquisition settings. In many real-world datasets, such labels are entirely missing, leaving raw acquisition parameters as the only indicators of contrast. Adding to this challenge, the available metadata is often incomplete, noisy, or inconsistent. The lack of reliable and standardized metadata complicates tasks such as image interpretation, retrieval, and integration into clinical workflows. Furthermore, robust contrast-aware representations are essential to enable more advanced clinical applications, such as achieving modality-invariant representations and data harmonization. To address these challenges, we propose MR-CLIP, a multimodal contrastive learning framework that aligns MR images with their DICOM metadata to learn contrast-aware representations, without relying on manual labels. Trained on a diverse clinical dataset that spans various scanners and protocols, MR-CLIP captures contrast variations across acquisitions and within scans, enabling anatomy-invariant representations. We demonstrate its effectiveness in cross-modal retrieval and contrast classification, highlighting its scalability and potential for further clinical applications. The code and weights are publicly available at https://github.com/myigitavci/MR-CLIP.</article>","contentLength":1744,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay","url":"https://arxiv.org/abs/2507.00042","date":1751428800,"author":"","guid":180328,"unread":true,"content":"<article>arXiv:2507.00042v1 Announce Type: new \nAbstract: Continually adapting edge models in cloud-edge collaborative object detection for traffic monitoring suffers from catastrophic forgetting, where models lose previously learned knowledge when adapting to new data distributions. This is especially problematic in dynamic traffic environments characterised by periodic variations (e.g., day/night, peak hours), where past knowledge remains valuable. Existing approaches like experience replay and visual prompts offer some mitigation, but struggle to effectively prioritize and leverage historical data for optimal knowledge retention and adaptation. Specifically, simply storing and replaying all historical data can be inefficient, while treating all historical experiences as equally important overlooks their varying relevance to the current domain. This paper proposes ER-EMU, an edge model update algorithm based on adaptive experience replay, to address these limitations. ER-EMU utilizes a limited-size experience buffer managed using a First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target domains, prioritizing the selection of historical data that is most dissimilar to the current target domain. This ensures training diversity and facilitates the retention of knowledge from a wider range of past experiences, while also preventing overfitting to the new domain. The experience buffer is also updated using a simple random sampling strategy to maintain a balanced representation of previous domains. Experiments on the Bellevue traffic video dataset, involving repeated day/night cycles, demonstrate that ER-EMU consistently improves the performance of several state-of-the-art cloud-edge collaborative object detection frameworks.</article>","contentLength":1916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables","url":"https://arxiv.org/abs/2507.00041","date":1751428800,"author":"","guid":180329,"unread":true,"content":"<article>arXiv:2507.00041v1 Announce Type: new \nAbstract: In talent management systems, critical information often resides in complex tabular formats, presenting significant retrieval challenges for conventional language models. These challenges are pronounced when processing Talent documentation that requires precise interpretation of tabular relationships for accurate information retrieval and downstream decision-making. Current table extraction methods struggle with semantic understanding, resulting in poor performance when integrated into retrieval-augmented chat applications. This paper identifies a key bottleneck - while structural table information can be extracted, the semantic relationships between tabular elements are lost, causing downstream query failures. To address this, we introduce TalentMine, a novel LLM-enhanced framework that transforms extracted tables into semantically enriched representations. Unlike conventional approaches relying on CSV or text linearization, our method employs specialized multimodal reasoning to preserve both structural and semantic dimensions of tabular data. Experimental evaluation across employee benefits document collections demonstrates TalentMine's superior performance, achieving 100% accuracy in query answering tasks compared to 0% for standard AWS Textract extraction and 40% for AWS Textract Visual Q&amp;A capabilities. Our comparative analysis also reveals that the Claude v3 Haiku model achieves optimal performance for talent management applications. The key contributions of this work include (1) a systematic analysis of semantic information loss in current table extraction pipelines, (2) a novel LLM-based method for semantically enriched table representation, (3) an efficient integration framework for retrieval-augmented systems as end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks showing substantial improvements across multiple categories.</article>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing","url":"https://arxiv.org/abs/2507.00039","date":1751428800,"author":"","guid":180330,"unread":true,"content":"<article>arXiv:2507.00039v1 Announce Type: new \nAbstract: Graph classification aims to categorize graphs based on their structural and attribute features, with applications in diverse fields such as social network analysis and bioinformatics. Among the methods proposed to solve this task, those relying on patterns (i.e. subgraphs) provide good explainability, as the patterns used for classification can be directly interpreted. To identify meaningful patterns, a standard approach is to use a quality measure, i.e. a function that evaluates the discriminative power of each pattern. However, the literature provides tens of such measures, making it difficult to select the most appropriate for a given application. Only a handful of surveys try to provide some insight by comparing these measures, and none of them specifically focuses on graphs. This typically results in the systematic use of the most widespread measures, without thorough evaluation. To address this issue, we present a comparative analysis of 38 quality measures from the literature. We characterize them theoretically, based on four mathematical properties. We leverage publicly available datasets to constitute a benchmark, and propose a method to elaborate a gold standard ranking of the patterns. We exploit these resources to perform an empirical comparison of the measures, both in terms of pattern ranking and classification performance. Moreover, we propose a clustering-based preprocessing step, which groups patterns appearing in the same graphs to enhance classification performance. Our experimental results demonstrate the effectiveness of this step, reducing the number of patterns to be processed while achieving comparable performance. Additionally, we show that some popular measures widely used in the literature are not associated with the best results.</article>","contentLength":1837,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information","url":"https://arxiv.org/abs/2507.00038","date":1751428800,"author":"","guid":180331,"unread":true,"content":"<article>arXiv:2507.00038v1 Announce Type: new \nAbstract: Data reduction plays a vital role in data-centric AI by identifying the most informative instance within large-scale datasets to enhance model training efficiency. The core challenge lies in how to select the optimal instances-rather than the entire datasets-to improve data quality and training efficiency. In this paper, we propose an effective data reduction strategy based on Pointwise V-information(PVI). First, we quantify instance difficulty using PVI and filter out low-difficulty instances enabling a static approach. Experiments demonstrate that removing 10%-30% of the data preserves the classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we use a progressive learning approach to training the classifiers on instances sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy gain over conventional training. Our results suggest that with the effective data reduction strategy, training a classifier on the selected optimal subset could enhance the model performance and boost training efficiency. Moreover, we have transferred the PVI framework, which previously applied only to English datasets, to diverse Chinese NLP tasks and base models, leading to valuable insights for cross-lingual data reduction and faster training. The codes are released at https://github.com/zhouwenchi/DatasetReductionStrategy.</article>","contentLength":1417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model Fusion via Neuron Interpolation","url":"https://arxiv.org/abs/2507.00037","date":1751428800,"author":"","guid":180332,"unread":true,"content":"<article>arXiv:2507.00037v1 Announce Type: new \nAbstract: Model fusion aims to combine the knowledge of multiple models by creating one representative model that captures the strengths of all of its parents. However, this process is non-trivial due to differences in internal representations, which can stem from permutation invariance, random initialization, or differently distributed training data. We present a novel, neuron-centric family of model fusion algorithms designed to integrate multiple trained neural networks into a single network effectively regardless of training data distribution. Our algorithms group intermediate neurons of parent models to create target representations that the fused model approximates with its corresponding sub-network. Unlike prior approaches, our approach incorporates neuron attribution scores into the fusion process. Furthermore, our algorithms can generalize to arbitrary layer types. Experimental results on various benchmark datasets demonstrate that our algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID fusion scenarios. The code is available at https://github.com/AndrewSpano/neuron-interpolation-model-fusion.</article>","contentLength":1207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting","url":"https://arxiv.org/abs/2507.00036","date":1751428800,"author":"","guid":180333,"unread":true,"content":"<article>arXiv:2507.00036v1 Announce Type: new \nAbstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate system, impacting freshwater fluxes into the ocean and regional ecosystems while also posing a challenge to polar navigation. However, accurately forecasting iceberg trajectories remains a formidable challenge, primarily due to the scarcity of spatiotemporal data and the complex, nonlinear nature of iceberg motion, which is also impacted by environmental variables. The iceberg motion is influenced by multiple dynamic environmental factors, creating a highly variable system that makes trajectory identification complex. These limitations hinder the ability of deep learning models to effectively capture the underlying dynamics and provide reliable predictive outcomes. To address these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep learning model that combines an analytical formulation of iceberg drift physics, with an augmented residual learning model. The model learns the pattern of mismatch between the analytical solution and ground-truth observations, which is combined with a rotate-augmented spectral neural network that captures both global and local patterns from the data to forecast future iceberg drift positions. We compare IDRIFTNET model performance with state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings demonstrate that IDRIFTNET outperforms other models by achieving a lower Final Displacement Error (FDE) and Average Displacement Error (ADE) across a variety of time points. These results highlight IDRIFTNET's effectiveness in capturing the complex, nonlinear drift of icebergs for forecasting iceberg trajectories under limited data and dynamic environmental conditions.</article>","contentLength":1779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark","url":"https://arxiv.org/abs/2507.00034","date":1751428800,"author":"","guid":180334,"unread":true,"content":"<article>arXiv:2507.00034v1 Announce Type: new \nAbstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. To support Phase II of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power profiles, this work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats for benchmark compatibility.\n  Classical CHF correlations exhibit substantial errors under uniform heating and degrade markedly when applied to non-uniform profiles, while modern tabular methods offer improved but still imperfect predictions. A neural network trained solely on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, underscoring the need for models that explicitly incorporate axial power distributions. By providing these curated datasets and baseline modeling results, this study lays the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.</article>","contentLength":1297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Moment Sampling in Video LLMs for Long-Form Video QA","url":"https://arxiv.org/abs/2507.00033","date":1751428800,"author":"","guid":180335,"unread":true,"content":"<article>arXiv:2507.00033v1 Announce Type: new \nAbstract: Recent advancements in video large language models (Video LLMs) have significantly advanced the field of video question answering (VideoQA). While existing methods perform well on short videos, they often struggle with long-range reasoning in longer videos. To scale Video LLMs for longer video content, frame sub-sampling (selecting frames at regular intervals) is commonly used. However, this approach is suboptimal, often leading to the loss of crucial frames or the inclusion of redundant information from multiple similar frames. Missing key frames impairs the model's ability to answer questions accurately, while redundant frames lead the model to focus on irrelevant video segments and increase computational resource consumption. In this paper, we investigate the use of a general-purpose text-to-video moment retrieval model to guide the frame sampling process. We propose \"moment sampling\", a novel, model-agnostic approach that enables the model to select the most relevant frames according to the context of the question. Specifically, we employ a lightweight moment retrieval model to prioritize frame selection. By focusing on the frames most pertinent to the given question, our method enhances long-form VideoQA performance in Video LLMs. Through extensive experiments on four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we demonstrate the effectiveness of the proposed approach.</article>","contentLength":1469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing","url":"https://arxiv.org/abs/2507.00032","date":1751428800,"author":"","guid":180336,"unread":true,"content":"<article>arXiv:2507.00032v1 Announce Type: new \nAbstract: We introduce KUL-KT, a biologically inspired architecture for knowledge tracing (KT), combining Hebbian memory encoding with gradient-based consolidation in a scalable, input-agnostic framework. KUL-KT adapts the principle of memory consolidation in neural systems, to student modeling by introducing two key innovations: (i) a time-decaying Hebbian memory update that enables graceful forgetting, and (ii) a novel Loss-aligned Internal Target (LIT) method to compute an ideal internal state, allowing continual learning without backpropagation through time. The architecture consists of a fast Hebbian memory that captures each learner interaction via a single associative update, and a slower linear network that consolidates recalled samples through gradient descent. This design enables few-shot personalization and natural forgetting without storing raw data or relying on large cohort training. Operating entirely in embedding space, KUL-KT supports both structured (tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT outperforms strong baselines on ten public KT benchmarks in rank-sensitive metrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT personalized quizzes from short-answer data, leading to improved learner-perceived helpfulness and reduced difficulty (p &lt; 0.05). Ablation studies confirm that Hebbian decay and LIT are critical for continual adaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x faster and uses 99.01\\% less memory. These results position KUL-KT as a biologically grounded, memory-efficient, and input-flexible framework for personalized learning at scale.</article>","contentLength":1697,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru","url":"https://arxiv.org/abs/2507.00031","date":1751428800,"author":"","guid":180337,"unread":true,"content":"<article>arXiv:2507.00031v1 Announce Type: new \nAbstract: Accurate modeling of human mobility is critical for understanding epidemic spread and deploying timely interventions. In this work, we leverage a large-scale spatio-temporal dataset collected from Peru's national Digital Contact Tracing (DCT) application during the COVID-19 pandemic to forecast mobility flows across urban regions. A key challenge lies in the spatial sparsity of hourly mobility counts across hexagonal grid cells, which limits the predictive power of conventional time series models. To address this, we propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN) technique that augments each cell's features with aggregated signals from its immediate H3 neighbors. We evaluate this strategy on three forecasting backbones: NLinear, PatchTST, and K-U-Net, under various historical input lengths. Experimental results show that SPN consistently improves forecasting performance, achieving up to 9.85 percent reduction in test MSE. Our findings demonstrate that spatial smoothing of sparse mobility signals provides a simple yet effective path toward robust spatio-temporal forecasting during public health crises.</article>","contentLength":1196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments","url":"https://arxiv.org/abs/2507.00030","date":1751428800,"author":"","guid":180338,"unread":true,"content":"<article>arXiv:2507.00030v1 Announce Type: new \nAbstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex sequential decision-making tasks, such as playing Atari 2600 games and mastering board games. A critical yet underexplored aspect of DRL is the temporal scale of action execution. We propose a novel paradigm that integrates contextual bandits with DRL to adaptively select action durations, enhancing policy flexibility and computational efficiency. Our approach augments a Deep Q-Network (DQN) with a contextual bandit module that learns to choose optimal action repetition rates based on state contexts. Experiments on Atari 2600 games demonstrate significant performance improvements over static duration baselines, highlighting the efficacy of adaptive temporal abstractions in DRL. This paradigm offers a scalable solution for real-time applications like gaming and robotics, where dynamic action durations are critical.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing","url":"https://arxiv.org/abs/2507.00029","date":1751428800,"author":"","guid":180339,"unread":true,"content":"<article>arXiv:2507.00029v1 Announce Type: new \nAbstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts (MoE) for adapting large language models (LLMs) to multiple tasks still exhibit prevailing limitations: they either swap entire attention/feed-forward layers for switch experts or bolt on parallel expert branches, diluting parameter efficiency and task fidelity. We propose the LoRA-Mixer, a modular and lightweight MoE framework that integrates LoRA experts. Our core innovation lies in replacing the projection matrices of the attention module's input/output linear layers with dynamically routed, task-specific LoRA experts. This design ensures seamless compatibility with diverse foundation models, including transformers and state space models (SSMs), by leveraging their inherent linear projection structures. The framework supports two operational paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a novel hard-soft routing strategy, or (2) direct deployment of pre-trained, frozen LoRA modules sourced from external repositories. To enable robust router training with limited data while ensuring stable routing decisions and maximizing expert reuse, we introduce an adaptive Specialization Balance Loss (SBL) that jointly optimizes expert balance and task-specific alignment. Extensive experiments on seven benchmark datasets, including MedQA, CoLA, SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base models, respectively. Compared with state-of-the-art methods, LoRA-Mixer achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively, using only 48% of the parameters, demonstrating its efficiency and strong performance.</article>","contentLength":1868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation","url":"https://arxiv.org/abs/2507.00028","date":1751428800,"author":"","guid":180340,"unread":true,"content":"<article>arXiv:2507.00028v1 Announce Type: new \nAbstract: The representation of urban trajectory data plays a critical role in effectively analyzing spatial movement patterns. Despite considerable progress, the challenge of designing trajectory representations that can capture diverse and complementary information remains an open research problem. Existing methods struggle in incorporating trajectory fine-grained details and high-level summary in a single model, limiting their ability to attend to both long-term dependencies while preserving local nuances. To address this, we propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint Embedding Predictive Architecture), a unified framework for learning multi-scale urban trajectory representations across semantic abstraction levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures point-level fine-grained details, intermediate patterns, and high-level trajectory abstractions, enabling the model to integrate both local dynamics and global semantics in one coherent structure. Extensive experiments on multiple real-world datasets for trajectory similarity computation show that HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code is available at: https://anonymous.4open.science/r/HiT-JEPA.</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models","url":"https://arxiv.org/abs/2507.00026","date":1751428800,"author":"","guid":180341,"unread":true,"content":"<article>arXiv:2507.00026v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) are increasingly deployed as black-box components in real-world applications, evaluating their safety-especially under adversarial prompting-has become critical. Arguably, effective safety evaluations should be adaptive, evolving with LLM capabilities, and also cover a broad spectrum of harmful topics and real-world scenarios to fully expose potential vulnerabilities. Existing manual safety benchmarks, built on handcrafted adversarial prompts, are limited by their static nature and the intensive labor required to update them, making it difficult to keep pace with rapidly advancing LLMs. In contrast, automated adversarial prompt generation offers a promising path toward adaptive evaluation. However, current methods often suffer from insufficient adversarial topic coverage (topic-level diversity) and weak alignment with real-world contexts. These shortcomings stem from the exploration-exploitation dilemma in black-box optimization and a lack of real-world contextualization, resulting in adversarial prompts that are both topically narrow and scenario-repetitive. To address these issues, we propose Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses multi-objective reinforcement learning to fine-tune an adversarial LLM for generating topically diverse and contextually rich adversarial prompts. Experiments show that ROSE outperforms existing methods in uncovering safety vulnerabilities in state-of-the-art LLMs, with notable improvements in integrated evaluation metrics. We hope ROSE represents a step toward more practical and reality-oriented safety evaluation of LLMs. WARNING: This paper contains examples of potentially harmful text.</article>","contentLength":1757,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalizing to New Dynamical Systems via Frequency Domain Adaptation","url":"https://arxiv.org/abs/2507.00025","date":1751428800,"author":"","guid":180342,"unread":true,"content":"<article>arXiv:2507.00025v1 Announce Type: new \nAbstract: Learning the underlying dynamics from data with deep neural networks has shown remarkable potential in modeling various complex physical dynamics. However, current approaches are constrained in their ability to make reliable predictions in a specific domain and struggle with generalizing to unseen systems that are governed by the same general dynamics but differ in environmental characteristics. In this work, we formulate a parameter-efficient method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can readily generalize to new dynamics via adaptation in the Fourier space. Specifically, FNSDA identifies the shareable dynamics based on the known environments using an automatic partition in Fourier modes and learns to adjust the modes specific for each new environment by conditioning on low-dimensional latent systematic parameters for efficient generalization. We evaluate our approach on four representative families of dynamic systems, and the results show that FNSDA can achieve superior or competitive generalization performance compared to existing methods with a significantly reduced parameter cost. Our code is available at https://github.com/WonderSeven/FNSDA.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity","url":"https://arxiv.org/abs/2507.00024","date":1751428800,"author":"","guid":180343,"unread":true,"content":"<article>arXiv:2507.00024v1 Announce Type: new \nAbstract: With the growing demand for novel materials, machine learning-driven inverse design methods face significant challenges in reconciling the high-dimensional materials composition space with limited experimental data. Existing approaches suffer from two major limitations: (I) machine learning models often lack reliability in high-dimensional spaces, leading to prediction biases during the design process; (II) these models fail to effectively incorporate domain expert knowledge, limiting their capacity to support knowledge-guided inverse design. To address these challenges, we introduce AIMatDesign, a reinforcement learning framework that addresses these limitations by augmenting experimental data using difference-based algorithms to build a trusted experience pool, accelerating model convergence. To enhance model reliability, an automated refinement strategy guided by large language models (LLMs) dynamically corrects prediction inconsistencies, reinforcing alignment between reward signals and state value functions. Additionally, a knowledge-based reward function leverages expert domain rules to improve stability and efficiency during training. Our experiments demonstrate that AIMatDesign significantly surpasses traditional machine learning and reinforcement learning methods in discovery efficiency, convergence speed, and success rates. Among the numerous candidates proposed by AIMatDesign, experimental synthesis of representative Zr-based alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\\% elongation, closely matching predictions. Moreover, the framework accurately captured the trend of yield strength variation with composition, demonstrating its reliability and potential for closed-loop materials discovery.</article>","contentLength":1806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GLU Attention Improve Transformer","url":"https://arxiv.org/abs/2507.00022","date":1751428800,"author":"","guid":180344,"unread":true,"content":"<article>arXiv:2507.00022v1 Announce Type: new \nAbstract: Gated Linear Units (GLU) have shown great potential in enhancing neural network performance. In this paper, I introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention. My experiments demonstrate that GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA). This project is open-sourced at github.</article>","contentLength":749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods","url":"https://arxiv.org/abs/2507.00020","date":1751428800,"author":"","guid":180345,"unread":true,"content":"<article>arXiv:2507.00020v1 Announce Type: new \nAbstract: This study uses a Variational Autoencoder method to enhance the efficiency and applicability of Markov Chain Monte Carlo (McMC) methods by generating broader-spectrum prior proposals. Traditional approaches, such as the Karhunen-Lo\\`eve Expansion (KLE), require previous knowledge of the covariance function, often unavailable in practical applications. The VAE framework enables a data-driven approach to flexibly capture a broader range of correlation structures in Bayesian inverse problems, particularly subsurface flow modeling. The methodology is tested on a synthetic groundwater flow inversion problem, where pressure data is used to estimate permeability fields. Numerical experiments demonstrate that the VAE-based parameterization achieves comparable accuracy to KLE when the correlation length is known and outperforms KLE when the assumed correlation length deviates from the true value. Moreover, the VAE approach significantly reduces stochastic dimensionality, improving computational efficiency. The results suggest that leveraging deep generative models in McMC methods can lead to more adaptable and efficient Bayesian inference in high-dimensional problems.</article>","contentLength":1226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations","url":"https://arxiv.org/abs/2507.00019","date":1751428800,"author":"","guid":180346,"unread":true,"content":"<article>arXiv:2507.00019v1 Announce Type: new \nAbstract: In this study, we propose, evaluate and compare three quantum inspired data encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy (GDS) and Class Conditional Value Strategy (CCVS), for transforming classical data into quantum data for use in pure classical machine learning models. The primary objective is to reduce high encoding time while ensuring correct encoding values and analyzing their impact on classification performance. The Instance Level Strategy treats each row of dataset independently; mimics local quantum states. Global Discrete Value Based encoding strategy maps all unique feature values across the full dataset to quantum states uniformly. In contrast, the Class conditional Value based encoding strategy encodes unique values separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their impact on en-coding efficiency, correctness, model accuracy, and computational cost. By analyzing the trade offs between encoding time, precision, and predictive performance, this study provides insights into optimizing quantum inspired data transformations for classical machine learning workflows.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections","url":"https://arxiv.org/abs/2507.00018","date":1751428800,"author":"","guid":180347,"unread":true,"content":"<article>arXiv:2507.00018v1 Announce Type: new \nAbstract: Post-training processes are essential phases in grounding pre-trained language models to real-world tasks, with learning from demonstrations or preference signals playing a crucial role in this adaptation. We present a unified theoretical framework bridging Supervised Fine-Tuning (SFT) and preference learning in Large Language Model (LLM) post-training. Through rigorous mathematical derivation, we demonstrate that both SFT and preference learning methods like Direct Preference Optimization (DPO) operate within the same optimal policy-reward subspace, with SFT representing a special case of implicit reward learning. Our analysis reveals a critical limitation in conventional SFT: the KL divergence term in distribution matching becomes constant with respect to the policy during optimization, failing to constrain model updates. To address this, we propose a simple yet effective learning rate reduction approach that yields significant performance improvements (up to \\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in instruction following tasks. Additionally, we derive alternative SFT objectives from various f-divergence functions that preserve the KL term during optimization, further enhancing post-DPO model performance. Finally, we extend the theoretical relationship between LLM logits and Q-functions from preference learning to the SFT context, providing mathematical derivations and experimental validation.</article>","contentLength":1500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gradient-based Fine-Tuning through Pre-trained Model Regularization","url":"https://arxiv.org/abs/2507.00016","date":1751428800,"author":"","guid":180348,"unread":true,"content":"<article>arXiv:2507.00016v1 Announce Type: new \nAbstract: Large pre-trained models have demonstrated extensive applications across various fields. However, fine-tuning these models for specific downstream tasks demands significant computational resources and storage. One fine-tuning method, gradient-based parameter selection (GPS), focuses on fine-tuning only the parameters with high gradients in each neuron, thereby reducing the number of training parameters. Nevertheless, this approach increases computational resource requirements and storage demands. In this paper, we propose an efficient gradient-based and regularized fine-tuning method (GRFT) that updates the rows or columns of the weight matrix. We theoretically demonstrate that the rows or columns with the highest sum of squared gradients are optimal for updating. This strategy effectively reduces storage overhead and improves the efficiency of parameter selection. Additionally, we incorporate regularization to enhance knowledge transfer from the pre-trained model. GRFT achieves state-of-the-art performance, surpassing existing methods such as GPS, Adapter Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the total parameters on FGVC and VTAB datasets, respectively, demonstrating its high efficiency and effectiveness. The source code will be released soon.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications","url":"https://arxiv.org/abs/2507.00015","date":1751428800,"author":"","guid":180349,"unread":true,"content":"<article>arXiv:2507.00015v1 Announce Type: new \nAbstract: The remarkable success of transformers across various fields such as natural language processing and computer vision has paved the way for their applications in automatic modulation classification, a critical component in the communication systems of Internet of Things (IoT) devices. However, it has been observed that transformer-based classification of radio signals is susceptible to subtle yet sophisticated adversarial attacks. To address this issue, we have developed a defensive strategy for transformer-based modulation classification systems to counter such adversarial attacks. In this paper, we propose a novel vision transformer (ViT) architecture by introducing a new concept known as adversarial indicator (AdvI) token to detect adversarial attacks. To the best of our knowledge, this is the first work to propose an AdvI token in ViT to defend against adversarial attacks. Integrating an adversarial training method with a detection mechanism using AdvI token, we combine a training time defense and running time defense in a unified neural network model, which reduces architectural complexity of the system compared to detecting adversarial perturbations using separate models. We investigate into the operational principles of our method by examining the attention mechanism. We show the proposed AdvI token acts as a crucial element within the ViT, influencing attention weights and thereby highlighting regions or features in the input data that are potentially suspicious or anomalous. Through experimental results, we demonstrate that our approach surpasses several competitive methods in handling white-box attack scenarios, including those utilizing the fast gradient method, projected gradient descent attacks and basic iterative method.</article>","contentLength":1812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SWE-Bench-CL: Continual Learning for Coding Agents","url":"https://arxiv.org/abs/2507.00014","date":1751428800,"author":"","guid":180350,"unread":true,"content":"<article>arXiv:2507.00014v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have achieved impressive results on static code-generation benchmarks, but real-world software development unfolds as a continuous stream of evolving issues, fixes, and feature requests. We introduce SWE-Bench-CL, a novel continual learning benchmark built on the human-verified SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By organizing GitHub issues into chronologically ordered sequences that reflect natural repository evolution, SWE-Bench-CL enables direct evaluation of an agent's ability to accumulate experience, transfer knowledge across tasks, and resist catastrophic forgetting. We complement the dataset with (i) a preliminary analysis of inter-task structural similarity and contextual sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented with a FAISS-backed semantic memory module, and (iii) a suite of specialized continual learning metrics -- including average accuracy, forgetting, forward/backward transfer, tool-use efficiency, and a generalized Composite Continual Learning Score and CL-F-beta score -- to capture the stability-plasticity trade-off. We outline a rigorous experimental protocol comparing memory-enabled and memory-disabled agents across diverse Python repositories. All code and data are publicly available at https://github.com/thomasjoshi/agents-never-forget, providing the community with a reproducible platform for developing more adaptive and robust AI agents in software engineering.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting","url":"https://arxiv.org/abs/2507.00013","date":1751428800,"author":"","guid":180351,"unread":true,"content":"<article>arXiv:2507.00013v1 Announce Type: new \nAbstract: Forecasting complex time series is an important yet challenging problem that involves various industrial applications. Recently, masked time-series modeling has been proposed to effectively model temporal dependencies for forecasting by reconstructing masked segments from unmasked ones. However, since the semantic information in time series is involved in intricate temporal variations generated by multiple time series components, simply masking a raw time series ignores the inherent semantic structure, which may cause MTM to learn spurious temporal patterns present in the raw data. To capture distinct temporal semantics, we show that masked modeling techniques should address entangled patterns through a decomposition approach. Specifically, we propose ST-MTM, a masked time-series modeling framework with seasonal-trend decomposition, which includes a novel masking method for the seasonal-trend components that incorporates different temporal variations from each component. ST-MTM uses a period masking strategy for seasonal components to produce multiple masked seasonal series based on inherent multi-periodicity and a sub-series masking strategy for trend components to mask temporal regions that share similar variations. The proposed masking method presents an effective pre-training task for learning intricate temporal variations and dependencies. Additionally, ST-MTM introduces a contrastive learning task to support masked modeling by enhancing contextual consistency among multiple masked seasonal representations. Experimental results show that our proposed ST-MTM achieves consistently superior forecasting performance compared to existing masked modeling, contrastive learning, and supervised forecasting methods.</article>","contentLength":1788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Undistillable Models by Minimizing Conditional Mutual Information","url":"https://arxiv.org/abs/2507.00012","date":1751428800,"author":"","guid":180352,"unread":true,"content":"<article>arXiv:2507.00012v1 Announce Type: new \nAbstract: A deep neural network (DNN) is said to be undistillable if, when used as a black-box input-output teacher, it cannot be distilled through knowledge distillation (KD). In this case, the distilled student (referred to as the knockoff student) does not outperform a student trained independently with label smoothing (LS student) in terms of prediction accuracy. To protect intellectual property of DNNs, it is desirable to build undistillable DNNs. To this end, it is first observed that an undistillable DNN may have the trait that each cluster of its output probability distributions in response to all sample instances with the same label should be highly concentrated to the extent that each cluster corresponding to each label should ideally collapse into one probability distribution. Based on this observation and by measuring the concentration of each cluster in terms of conditional mutual information (CMI), a new training method called CMI minimized (CMIM) method is proposed, which trains a DNN by jointly minimizing the conventional cross entropy (CE) loss and the CMI values of all temperature scaled clusters across the entire temperature spectrum. The resulting CMIM model is shown, by extensive experiments, to be undistillable by all tested KD methods existing in the literature. That is, the knockoff students distilled by these KD methods from the CMIM model underperform the respective LS students. In addition, the CMIM model is also shown to performs better than the model trained with the CE loss alone in terms of their own prediction accuracy.</article>","contentLength":1616,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Novel RL approach for efficient Elevator Group Control Systems","url":"https://arxiv.org/abs/2507.00011","date":1751428800,"author":"","guid":180353,"unread":true,"content":"<article>arXiv:2507.00011v1 Announce Type: new \nAbstract: Efficient elevator traffic management in large buildings is critical for minimizing passenger travel times and energy consumption. Because heuristic- or pattern-detection-based controllers struggle with the stochastic and combinatorial nature of dispatching, we model the six-elevator, fifteen-floor system at Vrije Universiteit Amsterdam as a Markov Decision Process and train an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS). Key innovations include a novel action space encoding to handle the combinatorial complexity of elevator dispatching, the introduction of infra-steps to model continuous passenger arrivals, and a tailored reward signal to improve learning efficiency. In addition, we explore various ways to adapt the discounting factor to the infra-step formulation. We investigate RL architectures based on Dueling Double Deep Q-learning, showing that the proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a highly stochastic environment, and thereby outperforms a traditional rule-based algorithm.</article>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning","url":"https://arxiv.org/abs/2507.00008","date":1751428800,"author":"","guid":180354,"unread":true,"content":"<article>arXiv:2507.00008v1 Announce Type: new \nAbstract: Grounding natural language queries in graphical user interfaces (GUIs) poses unique challenges due to the diversity of visual elements, spatial clutter, and the ambiguity of language. In this paper, we introduce DiMo-GUI, a training-free framework for GUI grounding that leverages two core strategies: dynamic visual grounding and modality-aware optimization. Instead of treating the GUI as a monolithic image, our method splits the input into textual elements and iconic elements, allowing the model to reason over each modality independently using general-purpose vision-language models. When predictions are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by generating candidate focal regions centered on the model's initial predictions and incrementally zooms into subregions to refine the grounding result. This hierarchical refinement process helps disambiguate visually crowded layouts without the need for additional training or annotations. We evaluate our approach on standard GUI grounding benchmarks and demonstrate consistent improvements over baseline inference pipelines, highlighting the effectiveness of combining modality separation with region-focused reasoning.</article>","contentLength":1246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy","url":"https://arxiv.org/abs/2507.00007","date":1751428800,"author":"","guid":180355,"unread":true,"content":"<article>arXiv:2507.00007v1 Announce Type: new \nAbstract: This paper presents a new educational framework for integrating generative artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini into laboratory activities aimed at developing critical thinking and digital literacy among undergraduate students. Recognizing the limitations and risks of uncritical reliance on large language models (LLMs), the proposed pedagogical model reframes GenAI as a research subject and cognitive tool. Students formulate discipline-specific prompts and evaluate GenAI-generated responses in text, image, and video modalities. A pilot implementation in a general astronomy course for non-science majors demonstrated high levels of engagement and critical reflection, with many students continuing the activity after class and presenting results at a research symposium. The results highlight the importance of structured AI interactions in education and suggest that GenAI can improve learning outcomes when combined with reflective assessment methods. The study proposes a replicable model for interdisciplinary AI-integrated lab work, adaptable to scientific disciplines. See the guide to learning activities based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MVGBench: Comprehensive Benchmark for Multi-view Generation Models","url":"https://arxiv.org/abs/2507.00006","date":1751428800,"author":"","guid":180356,"unread":true,"content":"<article>arXiv:2507.00006v1 Announce Type: new \nAbstract: We propose MVGBench, a comprehensive benchmark for multi-view image generation models (MVGs) that evaluates 3D consistency in geometry and texture, image quality, and semantics (using vision language models). Recently, MVGs have been the main driving force in 3D object creation. However, existing metrics compare generated images against ground truth target views, which is not suitable for generative tasks where multiple solutions exist while differing from ground truth. Furthermore, different MVGs are trained on different view angles, synthetic data and specific lightings -- robustness to these factors and generalization to real data are rarely evaluated thoroughly. Without a rigorous evaluation protocol, it is also unclear what design choices contribute to the progress of MVGs. MVGBench evaluates three different aspects: best setup performance, generalization to real data and robustness. Instead of comparing against ground truth, we introduce a novel 3D self-consistency metric which compares 3D reconstructions from disjoint generated multi-views. We systematically compare 12 existing MVGs on 4 different curated real and synthetic datasets. With our analysis, we identify important limitations of existing methods specially in terms of robustness and generalization, and we find the most critical design choices. Using the discovered best practices, we propose ViFiGen, a method that outperforms all evaluated MVGs on 3D consistency. Our code, model, and benchmark suite will be publicly released.</article>","contentLength":1564,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning","url":"https://arxiv.org/abs/2507.00005","date":1751428800,"author":"","guid":180357,"unread":true,"content":"<article>arXiv:2507.00005v1 Announce Type: new \nAbstract: Disaster response requires rapid, adaptive decision-making in chaotic environments. SwarmFusion, a novel hybrid framework, integrates particle swarm optimization with convolutional neural networks to optimize real-time resource allocation and path planning. By processing live satellite, drone, and sensor data, SwarmFusion enhances situational awareness and operational efficiency in flood and wildfire scenarios. Simulations using the DisasterSim2025 dataset demonstrate up to 40 percentage faster response times and 90 percentage survivor coverage compared to baseline methods. This scalable, data-driven approach offers a transformative solution for time-critical disaster management, with potential applications across diverse crisis scenarios.</article>","contentLength":798,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search","url":"https://arxiv.org/abs/2507.00004","date":1751428800,"author":"","guid":180358,"unread":true,"content":"<article>arXiv:2507.00004v1 Announce Type: new \nAbstract: Large language models (LLMs) demand considerable computational, energy, and financial resources during both training and deployment. While scaling laws for training have guided much of the field's recent progress, inference costs now represent a significant and growing component of the overall resource burden, particularly for reasoning-focused models. Existing characterizations of compute-optimality that consider model size, dataset size, and inference tokens in isolation or in fixed combinations risk overlooking more efficient operating points. We introduce directed stochastic skill search (DS3), a general framework that represents inference as stochastic traversal over a learned skill graph. From a simplified yet expressive instantiation, we derive closed-form expressions for task success and compute cost across a wide range of inference strategies -- including chain-of-thought (CoT) and tree-of-thought (ToT) -- enabling comparative analysis as a function of task difficulty and model capability. To that end, we extend a prior first-principles tripartite graph framework of LLM training to incorporate inference, and separately bridge DS3 with empirical methods that characterize LLM scaling behavior. We theoretically recover empirically observed patterns, including: linear accuracy scaling with logarithmic compute; variation in preferred inference strategies as a function of task difficulty and model capability; emergent behavior elicited by reasoning even when performance plateaus under parameter scaling; and both best-of-N (BoN) and majority voting behavior captured within a unified analytical framework. By explicitly characterizing training-inference interdependencies, our framework deepens theoretical understanding and supports principled algorithmic design and resource allocation.</article>","contentLength":1865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE","url":"https://arxiv.org/abs/2507.00003","date":1751428800,"author":"","guid":180359,"unread":true,"content":"<article>arXiv:2507.00003v1 Announce Type: new \nAbstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework for interpretable intrusion detection in IoT environments. By integrating Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the system decomposes prediction confidence into truth (T), falsity (F), and indeterminacy (I) components, enabling uncertainty quantification and abstention. Predictions with high indeterminacy are flagged for review using both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that misclassified samples exhibit significantly higher indeterminacy (I = 0.62) than correct ones (I = 0.24). The use of indeterminacy as a proxy for uncertainty enables informed abstention and targeted review-particularly valuable in edge deployments. Figures and tables validate the correlation between I-scores and error likelihood, supporting more trustworthy, human-in-the-loop AI decisions. This work shows that neutrosophic logic enhances both accuracy and explainability, providing a practical foundation for trust-aware AI in edge and fog-based IoT security systems.</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hypertokens: Holographic Associative Memory in Tokenized LLMs","url":"https://arxiv.org/abs/2507.00002","date":1751428800,"author":"","guid":180360,"unread":true,"content":"<article>arXiv:2507.00002v1 Announce Type: new \nAbstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from apparent precision loss, reframed here as information spreading. This reframing shifts the problem from computational precision to an information-theoretic communication issue. We address the K:V and V:K memory problem in LLMs by introducing HDRAM (Holographically Defined Random Access Memory), a symbolic memory framework treating transformer latent space as a spread-spectrum channel. Built upon hypertokens, structured symbolic codes integrating classical error-correcting codes (ECC), holographic computing, and quantum-inspired search, HDRAM recovers distributed information through principled despreading. These phase-coherent memory addresses enable efficient key-value operations and Grover-style search in latent space. By combining ECC grammar with compressed sensing and Krylov subspace alignment, HDRAM significantly improves associative retrieval without architectural changes, demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can fortify transformer architectures.</article>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google's Data Center Energy Use Doubled In 4 Years","url":"https://hardware.slashdot.org/story/25/07/01/221237/googles-data-center-energy-use-doubled-in-4-years?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751427000,"author":"BeauHD","guid":179598,"unread":true,"content":"An anonymous reader quotes a report from TechCrunch: No wonder Google is desperate for more power: The company's data centers more than doubled their electricity use in just four years. The eye-popping stat comes from Google's most recent sustainability report, which it released late last week. In 2024, Google data centers used 30.8 million megawatt-hours of electricity. That's up from 14.4 million megawatt-hours in 2020, the earliest year Google broke out data center consumption. Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth. And the company's electricity woes are almost entirely a data center problem. In 2024, data centers accounted for 95.8% of the entire company's electron budget.\n \nThe company's ratio of data-center-to-everything-else has been remarkably consistent over the last four years. Though 2020 is the earliest year Google has made data center electricity consumption figures available, it's possible to use that ratio to extrapolate back in time. Some quick math reveals that Google's data centers likely used just over 4 million megawatt-hours of electricity in 2014. That's sevenfold growth in just a decade. The tech company has already picked most of the low-hanging fruit by improving the efficiency of its data centers. Those efforts have paid off, and the company is frequently lauded for being at the leading edge. But as the company's power usage effectiveness (PUE) has approached the theoretical ideal of 1.0, progress has slowed. Last year, Google's company-wide PUE dropped to 1.09, a 0.01 improvement over 2023 but only 0.02 better than a decade ago. Yesterday, Google announced a deal to purchase 200 megawatts of future fusion energy from Commonwealth Fusion Systems, despite the energy source not yet existing. \"It's a sign of how hungry big tech companies are for a virtually unlimited source of clean power that is still years away,\" reports CNN.","contentLength":2008,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nintendo’s Anti-Consumer Anti-Piracy Measures Also Reduce The Value Of The Switch 2","url":"https://www.techdirt.com/2025/07/01/nintendos-anti-consumer-anti-piracy-measures-also-reduce-the-value-of-the-switch-2/","date":1751426100,"author":"Dark Helmet","guid":179555,"unread":true,"content":"<p>When it comes to the anti-piracy efforts taken by some of the more aggressive companies out there, such as Nintendo, the most frustrating part of the whole thing for me is just how completely short-sighted those efforts tend to be. Take Nintendo’s <a href=\"https://www.techdirt.com/2025/05/19/nintendo-updates-console-eula-well-brick-your-shit-if-we-dont-like-what-you-do-with-it/\">updated EULA</a> for its Switch consoles, for example. The updated agreement makes several changes from its previous iteration, but the most notable is that Nintendo says that if it thinks you’re doing the piracy for any reason, it can suspend all kinds of services on your console, up to and including bricking it completely. And, while the company has yet to go the bricking route so far,  it has already begun suspending all <a href=\"https://www.techdirt.com/2025/06/18/nintendo-is-already-punishing-switch-2-users-over-piracy-suspicions/\">online services</a> on consoles for the use of MIG Switches, cards for Switch devices on which you can load legitimately extracted ROMs from purchased games, or pirated versions of the same.</p><p>Now, the first layer of how this is short-sighted is easy enough to see. In order to engage in copyright protectionism, Nintendo is risking long-term reputational damage by functionally ruining the consoles of customers for actions that aren’t illegal, or even immoral. Short term protection, longer term risk of everyone thinking you don’t care about your own customers. </p><p>But there’s another layer to this, as a result of these service suspensions <a href=\"https://kotaku.com/nintendo-switch-2-pre-owned-online-error-walmart-1851784892\">being tied directly to the device</a> rather than the person. And that is what this protectionism means for the secondary market for Nintendo Switches.</p><blockquote><p><em>“I was driving between work sites and stopped at two different Walmarts,” says user Bimmytung. “At the second one I find a&nbsp;Mario Kart&nbsp;edition sitting in the case and couldn’t believe my luck.” They were informed by the Walmart staff that it was an “open box return,” so it was removed from the box to be checked over, and all looked well. The code for the packaged&nbsp;Mario Kart World&nbsp;had been scratched off already, so Walmart knocked another $50 off the price, and it all seemed like a good deal. Until they got home.</em></p><p><em>Finally after work I get a chance to set it up. Quickly realize I need the super special micro SD card and none of the ~half dozen in the house would work. Drive ten minutes to Target and get one there and pick up a few other accessories as well. Get home and go to finish the setup—quickly get Error Code 2124-4508. A quick Google search shows me I’m screwed. FML.”</em></p></blockquote><p>Now, there are several layers of shame here to go around. Shame on Walmart for selling a device without ensuring it would work for the buyer the way it is intended to work. And shame on Nintendo for creating an anti-piracy program such that the punishments meted out are linked to hardware rather than the supposed bad-actor it seeks to punish. </p><p>But all of that aside, it should also be true that this sort of thing drives the value of a Nintendo Switch console lower than it would be otherwise. Part of the value you gain when you buy a physical thing is the ability to eventually put it on the secondary market at some point. Because of the actions that Nintendo is taking in disabling and/or bricking its own consoles, that injects a great deal of risk into the prospect of buying one on the secondary market. The value of the hardware is, by at least some measure, diminished.</p><p>But because Nintendo seems to only think about these things in the short term, the company probably doesn’t much care.</p><blockquote><p><em>However, the more immediate issue is for those looking to pick up a Switch 2 from a reseller or previous owner, given their current scarcity at first-party sellers. There’s really no way of knowing at all if a console has been bricked when buying the device online, and this could make the resale market a complete shambles for the whole life cycle of the console. And, grimly, that’s not exactly a priority for Nintendo, given that reselling, either in store or online, gains the company nothing, and some would argue actually costs the company a sale—it’s not like it’ll be in a rush to address the problem.</em></p></blockquote><p>Which is why I won’t be in a rush to buy a Switch 2 anytime soon. And I’m certainly in their target market, having two young children who desperately want one. Instead of the console, however, they will be getting a lesson in making smart buying decisions as a consumer.</p>","contentLength":4261,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Laptop Mag Is Shutting Down","url":"https://hardware.slashdot.org/story/25/07/01/2133224/laptop-mag-is-shutting-down?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751419500,"author":"BeauHD","guid":179421,"unread":true,"content":"Laptop Mag, a tech publication that began in 1991 as a print magazine, is shutting down after nearly 35 years. The Verge reports: Laptop Mag has evolved many times over the years. It started as a print publication in 1991, when Bedford Communications launched the Laptop Buyers Guide and Handbook. Laptop Mag was later acquired by TechMedia Network (which is now called Purch) in 2011 and transitioned to digital-only content in 2013. Future PLC, the publisher that owns brands like PC Gamer, Tom's Guide, and TechRadar, acquired Purch -- and Laptop Mag along with it.\n \n\"We are incredibly grateful for your dedication, talent, and contributions to Laptop Mag, and we are committed to supporting you throughout this transition,\" [Faisal Alani, the global brand director at Laptop Mag owner Future PLC] said. Laptop Mag's shutdown follows the closure of long-running tech site AnandTech, which was also owned by Future PLC. It's not clear whether Laptop Mag's archives will be available following the shutdown.","contentLength":1009,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Accuses Former Engineer of Taking Vision Pro Secrets To Snap","url":"https://yro.slashdot.org/story/25/07/01/2128235/apple-accuses-former-engineer-of-taking-vision-pro-secrets-to-snap?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751417100,"author":"BeauHD","guid":179420,"unread":true,"content":"Apple has filed (PDF) a lawsuit against former Vision Pro engineer Di Liu, accusing him of stealing thousands of confidential files related to his work on Apple's augmented reality headset for the benefit of his new employer Snap. The company alleges Liu misled colleagues about his departure, secretly accepted a job offer from Snap, and attempted to cover his tracks by deleting files -- actions Apple claims violated his confidentiality agreement. The Register reports: Liu secretly received a job offer from Snap on October 18, 2024, a role the complaint describes as \"substantially similar\" to his Apple position, meaning Liu waited nearly two weeks to resign from Apple, per the lawsuit. \"Even then, he did not disclose he was leaving for Snap,\" the suit said. \"Apple would not have allowed Mr. Liu continued access had he told the truth.\" Liu allegedly copied \"more than a dozen folders containing thousands of files\" from Apple's filesystem to a personal cloud storage account, dropping the stolen bits in a pair of nested folders with the amazingly nondescript names \"Personal\" and \"Knowledge.\"\n \nApple said that data Liu copied includes \"filenames containing confidential Apple product code names\" and files \"marked as Apple confidential.\" Company research, product design, and supply chain management documents were among the content Liu is accused of stealing. The complaint also alleges that Liu deleted files to conceal his activities, a move that may hinder Apple's ability to determine the full scope of the data he exfiltrated. \"Mr. Liu additionally took actions to conceal his theft, including deceiving Apple about his job at Snap, and deleting files from his Apple-issued computer that might have let Apple determine what data Mr. Liu stole,\" the complaint noted.\n \nWhatever he has, Apple wants it back. The company demands a jury trial on a single count of breach of contract under a confidentiality and intellectual property agreement Liu was bound to. It also asks the court to compel Liu to return all misappropriated data, award damages to be determined at trial, and reimburse Apple's costs and attorneys' fees.","contentLength":2137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hilbert's sixth problem: derivation of fluid equations via Boltzmann's theory","url":"https://arxiv.org/abs/2503.01800","date":1751416308,"author":"nsoonhui","guid":180368,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44439242"},{"title":"Tinder To Require Facial Recognition Check For New Users In California","url":"https://yro.slashdot.org/story/25/07/01/2112208/tinder-to-require-facial-recognition-check-for-new-users-in-california?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751414520,"author":"BeauHD","guid":179390,"unread":true,"content":"An anonymous reader quotes a report from Axios: Tinder is mandating new users in California verify their profiles using facial recognition technology starting Monday, executives exclusively tell Axios. The move aims to reduce impersonation and is part of Tinder parent Match Group's broader effort to improve trust and safety amid ongoing user frustration. The Face Check feature prompts users to take a short video selfie during onboarding. The biometric face scan, powered by FaceTec, then confirms the person is real and present and whether their face matches their profile photos. It also checks if the face is used across multiple accounts. If the criteria are met, the user receives a photo verified badge on their profile. The selfie video is then deleted. Tinder stores a non-reversible, encrypted face map to detect duplicate profiles in the future.\n \nFace Check is separate from Tinder's ID Check, which uses a government-issued ID to verify age and identity. \"We see this as one part of a set of identity assurance options that are available to users,\" Match Group's head of trust and safety Yoel Roth says. \"Face Check ... is really meant to be about confirming that this person is a real, live person and not a bot or a spoofed account.\" \"Even if in the short term, it has the effect of potentially reducing some top-line user metrics, we think it's the right thing to do for the business,\" Rascoff said.","contentLength":1417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Australians to face age checks from search engines","url":"https://ia.acs.org.au/article/2025/australians-to-face-age-checks-from-search-engines.html","date":1751414366,"author":"stubish","guid":180367,"unread":true,"content":"<p>Australians using search engines while logged in to accounts from the likes of Google and Microsoft will have their age checked by the end of 2025, under a new online safety code co-developed by technology companies and registered by the eSafety Commissioner.</p><p>Search engines operating in Australia will need to implement age assurance technologies for logged-in users in \"no later than six months”, under <a href=\"https://www.esafety.gov.au/sites/default/files/2025-06/Schedule-3-Internet-Search-Engine-Services-Online-Safety-Code-%28Class-1C-and-Class-2-Material%29.pdf\">new rules</a> published on Monday.</p><p>While only logged-in users will be required to have their age checked, many Australians typically surf the web while logged into accounts from Google, which <a href=\"https://ia.acs.org.au/article/2024/google-claims-it-doesn-t-dominate-search-in-australia.html\">dominates Australia’s search market</a> and also runs Gmail and YouTube; and Microsoft, which runs the Bing search engine and email platform Outlook.</p><p>If a search engine’s age assurance systems believe a signed-in user is “likely to be an Australian child” under the age of 18, they will need to set safety tools such as “safe search” functions at their highest setting by default to filter out pornography and high impact violence, including in advertising.</p><p>Currently, Australians must be at least 13 years of age to manage their own Google or Microsoft account.</p><p>Age assurance methods can include age verification systems, which use government documents or ID; age estimation systems, which typically use biometrics; and age inference systems, which use data about online activity or accounts to infer age.</p><p>Search engines will not be required to implement age assurance measures for users who are not logged in to their services, according to the new rules.</p><p>“Internet search engine services are designed for general public use, with or without an account,” the code states.  </p><p>However, users who are not logged in should also expect “default blurring of images of online pornography and high-impact violence material detected in search results”.</p><p>Other compliance measures in the code which search providers must abide by include improving search and age assurance technologies over time, preventing autocomplete predictions “that are sexually explicit or violent”, and responding to searches about eating disorders or self-harm with crisis prevention information.</p><p>Google and Microsoft were contacted for comment.</p><p>Earlier this year Google said it would begin <a href=\"https://ia.acs.org.au/article/2025/google-will-use-ai-to-estimate-a-user-s-age.html\">using artificial intelligence to estimate users' ages</a>, beginning with tests in the United States, while Microsoft previously stated it had explored age assurance methods while considering potential impacts for user safety and privacy.</p><h4><b>Changes ‘designed to protect’ Australian kids</b></h4><p>The new rules for search engine operators were “designed to protect\" Australian children, according to the code.</p><p>Drafting of the code was co-led by Digital Industry Group Inc. (DIGI), which was contacted for comment as it counts Google, Microsoft, and Yahoo among its members.</p><p>eSafety Commissioner Julie Inman Grant said she had registered three new codes submitted by the online industry, which covered harmful content on search engines, enterprise hosting services, and internet carriage services such as telecommunication firms.</p><p>The codes had been in the works since July 2024 and failure to comply with them could result in civil penalties of up to $49.5 million per breach, her office said.</p><p>The Commissioner said she had sought extra safety commitments from the industry on six outstanding codes, which covered the likes of app stores, device manufacturers, social media, and messaging services.</p><p>“It's critical to ensure the layered safety approach which also places responsibility and accountability at critical chokepoints in the tech stack including the app stores and at the device level, the physical gateways to the internet where kids sign-up and first declare their ages,” Inman Grant said.</p><h4><b>Push to protect children who use AI chatbots</b></h4><p>Members of the technology industry had also been asked to use the remaining six codes to strengthen their protections against generative AI chatbots engaging in harmful behaviours with children, Inman Grant said.</p><p>“We are already receiving anecdotal reports from school nurses, that kids as young as 10 are spending up to five hours a day with AI chatbots, at times engaging in sexualised conversations and being directed by the chatbots to engage in harmful sexual acts or behaviours,” she said.</p><p>Inman Grant said she would consider the changes proposed by the industry and would aim to make her final determination on the six outstanding codes by the end of July.</p><p>\"If I am not satisfied these industry codes meet appropriate community safeguards, I will move to developing mandatory standards,” she said.</p>","contentLength":4599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44439058"},{"title":"Using Sun Ray thin clients in 2025","url":"https://catstret.ch/202506/sun-ray-shenanigans/","date":1751412635,"author":"todsacerdoti","guid":179510,"unread":true,"content":"<p>i’ve used thin clients at home for quite a while - both for their  use (remotely accessing a desktop of another system); and in the sense of “modern thin clients are x86 boxes that are wildly overpowered for what they run, so they make good mini servers.”</p><p>recently, i saw a bulk lot of Sun Ray thin clients pop up on Trade Me (NZ’s eBay-like auction site) - and with very little idea of how many clients were actually included in this lot, i jumped on it. after a 9 hour round-trip drive (on some of the worst roads i’ve seen!), i returned home with the back of my car completely packed with Sun Rays. time for some interesting shenanigans!</p><p>when picking all of these up from the seller, i had guesstimated there was maybe 30 clients in total. turns out i was off by quite a bit.</p><p>i ended up bringing home:</p><ul><li>3x <a href=\"https://dogemicrosystems.ca/pub/Sun/System_Handbook/Sun_syshbk_V3.4/Systems/SunRay270/SunRay270.html\">Sun Ray 270</a> - 17” (1280x1024) LCD screens with integrated Sun Ray clients</li><li>4x <a href=\"https://web.archive.org/web/20170218173523/https://incarta.com.au/uvo\">Incarta Uvo</a> - 24” 1080p LCD screens with integrated clients\n    <ul><li>i can’t find any info about these other than the linked page on the Wayback Machine - if you know more about these, please send me an email!</li></ul></li><li>about 40 smart cards, for authentication/hotdesking</li><li>a small pile of Sun Type 7 USB keyboards, and some Sun-branded optical mice</li></ul><p>so that’s  clients all up!</p><p>a few days prior to picking all this up, i rented a storage unit in a local facility, and put some garage shelving units in there - and boy howdy i’m glad i did!</p><h2>setting up the Sun Ray Server Software</h2><p>looking at the Oracle (eugh.) documentation for the Sun Ray Server Software, it appeared there were two options: run it on ancient Linux, or run it on ancient Solaris. Oracle dropped support for the Sun Rays in 2014, as part of extinguishing everything Sun Microsystems stood for after the 2010 acquisition. i didn’t  want to have a RHEL 6 box kicking around, nor did i want to deal with trying to make Solaris 10 work in a VM on my home Proxmox cluster, so i did some digging.</p><p>enter  - well, in my case, OpenIndiana. illumos is, essentially, a fork of the pre-Oracle-acquisition OpenSolaris codebase. OpenIndiana is one of many illumos  (in a very similar sense to Linux distributions), and OpenIndiana is more suited for desktop use than most other illumos distributions. the OpenIndiana documentation has <a href=\"https://docs.openindiana.org/handbook/sunray/\">a section on setting up the Sun Ray Server Software on OpenIndiana</a>, but even with that in hand there was a lot of pieces to figure out on my own!</p><p>this is mostly a copy of the docs from the OpenIndiana handbook, with some adjustments to fix things i ran into. i did this on top of a text-only install - <code>OpenIndiana Hipster 2025.04 Text Install DVD (64-bit x86)</code> was the install media i used (from <a href=\"https://www.openindiana.org/downloads/\">https://www.openindiana.org/downloads/</a>).</p><p>to get the desktop environment installed:</p><div><div><pre><code># pkg install mate_install\n</code></pre></div></div><p>unlocking the dependencies for SRSS:</p><div><div><pre><code># pkg change-facet facet.version-lock.gnome/gnome-session=false\n# pkg change-facet facet.version-lock.gnome/gnome-settings-daemon=false\n# pkg change-facet facet.version-lock.system/display-manager/gdm=false\n# pkg change-facet facet.version-lock.library/gnome/libgnomekbd=false\n# pkg change-facet facet.version-lock.gnome/window-manager/metacity=false\n# pkg change-facet facet.version-lock.library/desktop/gnome-desktop=false\n# pkg change-facet facet.version-lock.cde/cde-runtime=false\n# pkg change-facet facet.version-lock.library/motif=false\n# pkg change-facet facet.version-lock.library/tooltalk=false\n# pkg change-facet facet.version-lock.compatibility/packages/SUNWxwplt=false\n</code></pre></div></div><p>setting up the package source, and installing the SRSS dependencies:</p><div><div><pre><code># pkg set-publisher --search-before=openindiana.org -g http://pkg.toc.de/sunray sunray\n# pkg set-publisher --non-sticky openindiana.org\n# pkg install sunray-essential\n</code></pre></div></div><p>after unpacking the Sun Ray Server Software installers (both the Solaris and Linux versions) into , i ran the  script from the OI Handbook, then tried to install SRSS, which bombed out spectacularly with package manager rejections of the <code>This version is excluded by installed incorporation consolidation/userland/userland-incorporation@...</code> sort. so here’s the correct (read: “worked for me!”) steps:</p><div><div><pre><code># /root/update_dhcp_dependency /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/\n# pkg set-publisher -g /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/ sunray\n# pkg uninstall entire userland-incorporation\n# pkg install SUNWut-srss SUNWut-srwc SUNWuti\n</code></pre></div></div><p>to make SRSS happy with isc-dhcp:</p><div><div><pre><code># rpm2cpio /root/srs_5.4.0.0-Linux.i386/Components/10-SRSS/Content/Sun_Ray_Core_Services_4.5/Linux/Packages/SUNWuto-4.5-44.i386.rpm | bsdtar -C /root -xf - ./opt/SUNWut/lib/dhcp/\n# sed 's#$UTDHCPDIR | sort#$UTDHCPDIR | gsort#g' -i.bak /root/opt/SUNWut/lib/dhcp/isc/dhcp_config_linux \n# cp -R /root/opt/SUNWut/lib/dhcp/isc /opt/SUNWut/lib/dhcp/\n# cp /opt/SUNWut/lib/dhcp/isc/dhcp_config_linux /opt/SUNWut/lib/dhcp/isc/dhcp_config_solaris\n# ln -s /opt/SUNWut/lib/dhcp/isc /etc/opt/SUNWut/dhcp\n</code></pre></div></div><p>then apply the needed patch to :</p><p>now, get the ancient JRE in place:</p><div><div><pre><code># cd /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Java_Runtime_Environment/Solaris\n# ./jre-6u41-solaris-i586.sh\n# mv ./jre1.6.0_41 /opt/\n# ln -s /opt/jre1.6.0_41 /etc/opt/SUNWut/jre\n</code></pre></div></div><p>and, since i wanted the web administration tools to work too:</p><div><div><pre><code># bsdtar -C /opt -xf /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Apache_Tomcat/apache-tomcat-5.5.36.tar.gz\n# ln -s /opt/apache-tomcat /opt/apache-tomcat-5.5.36\n</code></pre></div></div><p>i then configured the Sun Ray server:</p><div><div><pre><code># /opt/SUNWut/sbin/utconfig\n# /opt/SUNWut/sbin/utpolicy -a -z both -g -M\n# /opt/SUNWut/sbin/utadm -L on\n# /opt/SUNWut/sbin/utstart -c\n</code></pre></div></div><h3>getting the Sun Ray firmware in place</h3><p>since i was using version 5.4.x of the Sun Ray Server Software, the client firmware wasn’t part of the install - from version 5.3 onwards, you had to have an Oracle support contract to get firmware updates. sigh.</p><p>thankfully, getting a 5.2.x release (with the firmware included!) wasn’t hard. i grabbed a 5.2.x release for Linux, found the RPM with the firmware in it (), and extracted that with .</p><p>the Solaris version of SRSS wants to find the firmware in a different place than the Linux version it seems - the Linux versions put it in , but on Solaris/OpenIndiana, it needs to be in <code>/opt/SUNWutdfw/lib/firmware</code>. easy enough.</p><p>once in place, this was all it took to set up the TFTP server, and make SRSS populate the right places with the firmware:</p><div><div><pre><code># mkdir /tftpboot\n# cd /tftpboot\n# ln -f -s . tftpboot\n# /opt/SUNWut/sbin/utfwadm -AaV -G force\n</code></pre></div></div><p>i wanted to use some of the integrated-into-screens Sun Rays to replace some of the Raspberry Pis (and old iMacs) around the house showing Home Assistant dashboards. i also wanted to set up the Sun Ray server so that when i inserted a particular smart card into a client, it would bring up an RDP session to my existing “desktop” (a Fedora VM running Xrdp).</p><p>these both turned out to be… interesting to get working.</p><p>the Sun Ray Server Software has a built-in method for connecting to Microsoft RDP servers - the Sun Ray Windows Connector, also known as .\nas you might have guessed, it’s broken as fuck on OpenIndiana, even putting aside the fact that the newest RDP server it knows how to handle would be in the Windows Server 2003 era.</p><p>so, let’s hack something together with XFreeRDP!</p><p>i wanted to be able to specify what RDP server each token would connect to. this was a fairly common use case back in the day, and some people wrote helpers to allow things like that - one of which being <a href=\"https://web.archive.org/web/20131212042126/https://blogs.oracle.com/danielc/entry/meta_kiosk_how_to_run\">Daniel Cifuentes’ meta-kiosk</a>, which i borrowed some ideas from.</p><p>after much trial and error, i got something working!</p><div><figure><pre><code data-lang=\"shell\">/freerdp\n</code></pre></figure></div><div><figure><pre><code data-lang=\"shell\">\n\nopenbox  &amp;\n/opt/SUNWut/bin/utscreenresize  all  &amp;\n\n/opt/SUNWut/sbin/utuser  |  | zenity 1\nxterm  xfreerdp /cert:tofu /f  /dynamic-resolution /gfx +gfx-thin-client /smartcard /bpp:24 </code></pre></figure></div><p>after throwing those in place, install the dependencies and configure the session:</p><div><div><pre><code># pkg install openbox freerdp\n# printf \"KIOSK_SESSION=freerdp\\n\" | /opt/SUNWut/sbin/utkiosk -i FreeRDP\n</code></pre></div></div><p>then it’s just a matter of adding the needed data to each token, and assigning the tokens to the FreeRDP session:</p><div><div><pre><code># /opt/SUNWut/sbin/utkioskoverride -s kiosk -r OpenPlatform.47905167523905788499 -c FreeRDP\n</code></pre></div></div><p>upon inserting that token into a client…</p><p>with much the same setup as the RDP sessions, it’s pretty easy to start a kiosk-mode Firefox, pulling the URL to open from the token data:</p><div><figure><pre><code data-lang=\"shell\">/kiosk-browser\n</code></pre></figure></div><div><figure><pre><code data-lang=\"shell\">\n\nopenbox  &amp;\n/opt/SUNWut/bin/utscreenresize  all  &amp;\n\n\nxset s off\nxset s noblank\nxset /opt/SUNWut/sbin/utuser  |  | zenity 1\nfirefox </code></pre></figure></div><p>a problem, though. Firefox would show its first-run “Welcome to Firefox” popup… every time. Sun Ray kiosk sessions run as a random user named   (where  is a number), and after the kiosk session ends the home directory of the kiosk user gets fully deleted, so the user can be recycled for other sessions. given i wanted to use this with some always-on Sun Rays, with no input devices attached…</p><p>thankfully, Firefox policies allow turning that off! throwing this hunk of JSON into <code>/etc/firefox/policies/policies.json</code> fixed that:</p><div><figure><pre><code data-lang=\"json\"></code></pre></figure></div><p>and with that, i could create a token for an individual client (the tokens for this are , where the MAC is all lower-case), set that token’s “Other Info” field to the URL to show, and assign the kiosk session to that pseudo-token the same way as with smart card tokens.</p><p>this was a lot of fun to get working. i need to take a break from reading the Sun Ray Administration Guide though, so here’s my thinking for a potential part 2:</p><ul><li>i want to see how well the multi-head stuff works in SRSS - which joins multiple physical clients together into one desktop session, using the peripherals connected to the “primary” client. unfortunately the Xinerama support is weird (Xinerama and xrandr are mutually exclusive…), but if i can make it play ball it could be a neat thing to use.</li><li>i want to try and find a newer firmware package too, but that might be a little bit of a lost cause, given i refuse to give Oracle a bunch of money.</li><li>maybe i’ll set up another OpenIndiana VM and configure the HA failover in SRSS?</li></ul><p>for now, though… that’s all.</p>","contentLength":10036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44438900"},{"title":"Figma Files For IPO","url":"https://slashdot.org/story/25/07/01/2058244/figma-files-for-ipo?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751412000,"author":"BeauHD","guid":179389,"unread":true,"content":"Figma has filed to go public on the NYSE under the ticker \"FIG,\" marking one of the most anticipated IPOs in recent years following its scrapped $20 billion acquisition by Adobe. CNBC reports: Revenue in the first quarter increased 46% to $228.2 million from $156.2 million in the same period a year ago, according to Figma's prospectus. The company recorded a net income of $44.9 million, compared to $13.5 million a year earlier. As of March 31, Figma had 1,031 customers contributing at least $100,000 a year to annual revenue, up 47% from a year earlier. Clients include Amazon Web Services, Google, Microsoft and Netflix. More than half of revenue comes from outside the U.S. Figma didn't say how many shares it plans to sell in the IPO. The company was valued at $12.5 billion in a tender offer last year, and in April it announced that it had confidentially filed for an IPO with the SEC. [...]\n \nFigma was founded in 2012 by CEO Dylan Field, 33, and Evan Wallace, and is based in San Francisco. The company had 1,646 employees as of March 31. Before establishing Figma, Field spent over two years at Brown University, where he met Wallace. Field then took a Thiel Fellowship \"to pursue entrepreneurial projects,\" according to the filing. The two-year program that Founders Fund partner Peter Thiel established in 2011 gives young entrepreneurs a $200,000 grant along with support from founders and investors, according to an online description. Field is the biggest individual owner of Figma, with 56.6 million Class B shares and 51.1% of voting power ahead of the IPO. He said in a letter to investors that it was time for Figma to buck the \"trend of many amazing companies staying privately indefinitely.\" \"Some of the obvious benefits such as good corporate hygiene, brand awareness, liquidity, stronger currency and access to capital markets apply,\" wrote Field. \"More importantly, I like the idea of our community sharing in the ownership of Figma -- and the best way to accomplish this is through public markets.\"\n \nAs a public company, Field said investors should \"expect us to take big swings,\" including through acquisitions.\n \nIn April, Figma bought the assets and team of an unnamed technology company for $14 million, according to the filing. They also registered over 13 million users per month, one-third of which are designers.","contentLength":2350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Xerox Buys Lexmark For $1.5 Billion As Print Industry Clings To Relevance","url":"https://slashdot.org/story/25/07/01/2247221/xerox-buys-lexmark-for-15-billion-as-print-industry-clings-to-relevance?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751410800,"author":"BeauHD","guid":179359,"unread":true,"content":"BrianFagioli shares a report from NERDS.xyz: In a move that feels straight out of a different era, Xerox has officially acquired Lexmark for $1.5 billion. The deal includes net debt and assumed liabilities, and it pulls Lexmark out of the hands of Chinese ownership and into a freshly restructured Xerox. That's a lot of money for a company best known for making machines that spit out paper.\n \nAccording to Xerox, this is all part of a \"Reinvention\" strategy. The company now claims it will be one of the top five players in every major print category and the leader in managed print services. [...] Xerox says the new leadership team will include executives from both sides, and the combined business will now support over 200,000 clients in more than 170 countries. They'll also be running 125 manufacturing and distribution centers in 16 countries.","contentLength":852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMC Warns Moviegoers To Expect '25-30 Minutes' of Ads and Trailers","url":"https://entertainment.slashdot.org/story/25/07/01/2052226/amc-warns-moviegoers-to-expect-25-30-minutes-of-ads-and-trailers?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751409600,"author":"BeauHD","guid":179358,"unread":true,"content":"AMC Theatres now warns customers that movies start 25-30 minutes after the listed showtime to account for ads and trailers, \"making it easier for moviegoers to know the actual start time of their film screening,\" reports The Verge. From the report: Starting today, AMC will also show more ads than before, meaning its preshow lineup may have to be reconfigured to avoid exceeding the 30-minute mark. The company made an agreement with the National CineMedia ad network that includes as much as five minutes of commercials shown \"after a movie's official start time,\" according to The Hollywood Reporter, and an additional 30-to-60-second \"Platinum Spot\" that plays before the last one or two trailers.\n \nAMC was the only major theater chain to reject the National CineMedia ad spot when it was pitched in 2019, telling Bloomberg at the time that it believed \"US moviegoers would react quite negatively.\" Now struggling financially amid an overall decline in movie theater attendance and box-office grosses, AMC has reversed course, telling The Hollywood Reporter that its competitors \"have fully participated for more than five years without any direct impact to their attendance.\"","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Congress just greenlit a NASA moon plan opposed by Musk and Isaacman","url":"https://techcrunch.com/2025/07/01/congress-just-greenlit-a-nasa-moon-plan-opposed-by-musk-and-isaacman/","date":1751409059,"author":"Aria Alamalhodaei","guid":179336,"unread":true,"content":"<article>The $10 billion addition to the Artemis architecture, which includes funding for additional Space Launch System rockets and an orbiting station around the moon called Gateway, is a rebuke to critics who wished to see alternative technologies used instead.</article>","contentLength":255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Moral Imperative Of Clear Language","url":"https://www.techdirt.com/2025/07/01/the-moral-imperative-of-clear-language/","date":1751408220,"author":"Mike Masnick","guid":179364,"unread":true,"content":"<p>I need to say something that will make many of you deeply uncomfortable: your refusal to call fascism “fascism” is not sophistication—it’s&nbsp;.</p><p>When Donald Trump posts&nbsp;<a href=\"https://truthsocial.com/@realDonaldTrump/posts/114690267066155731\">explicit orders</a>&nbsp;for “REMIGRATION” and “Mass Deportation Operations” targeting American cities because they are “the core of the Democrat Power Center,” that’s not “controversial immigration policy.” That’s mass deportation directed against political opponents. When federal troops&nbsp;<a href=\"https://www.politico.com/news/2020/06/01/trump-military-deployment-protests-294396\">deploy</a>&nbsp;against American civilians exercising constitutional rights, that’s not “enhanced law enforcement.” That’s military occupation. When the systematic dismantling of democratic institutions gets described as “political polarization,” that’s not nuanced analysis—it’s linguistic evasion that enables the very thing it refuses to name.</p><p>The sophisticates hate this clarity. They prefer the safety of euphemism, the comfort of complexity that never quite arrives at moral judgment. They speak of “concerning developments” and “troubling trends” while democracy burns around them. They perform nuanced understanding while fascism consolidates power through their very refusal to name it.</p><p>But here’s what they don’t understand: authoritarianism thrives in ambiguity. It requires linguistic fog to operate. It depends on our unwillingness to call things by their proper names. Every euphemism is a small surrender. Every hedge is a tiny collaboration. Every refusal to speak plainly is a gift to those who profit from confusion.</p><p>Language shapes consciousness. When we refuse to name what we see clearly, we don’t just fail to communicate—we erode our collective capacity to think clearly, to feel appropriately, to respond effectively. We make ourselves complicit in our own moral disorientation.</p><p>George Orwell understood this when he&nbsp;<a href=\"https://www.orwellfoundation.com/the-orwell-foundation/orwell/essays-and-other-works/politics-and-the-english-language/\">wrote</a>&nbsp;that “political language is designed to make lies sound truthful and murder respectable, and to give an appearance of solidity to pure wind.” But he was describing propaganda techniques used by totalitarian regimes. What we face now is worse: the voluntary adoption of euphemistic language by people who should know better, who pride themselves on seeing clearly, who claim to defend democratic values.</p><p>We are doing the propagandists’ work for them.</p><p>Consider how this linguistic distortion operates in practice. When&nbsp;<a href=\"https://immigrationforum.org/article/trumps-first-100-days-potential-immigration-actions/\">mass deportation operations</a>&nbsp;targeting millions of people get called “immigration enforcement,” we’re not being diplomatic—we’re making state violence psychologically easier to accept. When systematic attacks on democratic institutions get labeled “political disagreements,” we’re not showing balance—we’re normalizing authoritarianism. When obvious lies get treated as “alternative perspectives,” we’re not being fair—we’re weaponizing false equivalence against truth itself.</p><p>The euphemism isn’t just descriptive failure—it’s moral failure. It changes how people process information, how they make decisions, how they understand their own moral obligations. When you call fascism “populism,” you’re not just using imprecise language. You’re&nbsp;<a href=\"https://sundayletters.larrygmaguire.com/p/bandura-on-selective-moral-disengagement\">making it easier</a>&nbsp;for people to support fascism without confronting what they’re supporting.</p><p>Hannah Arendt spent her life studying how ordinary people enable extraordinary evil, and she identified linguistic evasion as one of the primary mechanisms. In&nbsp;, she showed how bureaucratic language—“evacuation,” “resettlement,” “special treatment”—allowed participants in genocide to avoid confronting the reality of what they were doing. They weren’t murdering children; they were “processing population transfers.” They weren’t operating death camps; they were managing “facilities for the final solution.”</p><p>The language didn’t just hide the reality from others—it hid it from themselves. It allowed them to participate in evil while maintaining their self-image as decent, law-abiding citizens following proper procedures.</p><p>Arendt’s insight was that evil becomes possible not primarily through active malice but through the refusal of ordinary people to see and name what’s in front of them. The “banality of evil” is fundamentally about linguistic evasion enabling moral evasion. When we stop calling violence violence, we make violence easier to commit.</p><p>This is what we’re witnessing now. The systematic training of a population to see clearly but speak obliquely, to understand precisely but describe vaguely, to recognize authoritarianism but call it something else. We have become a society of people who know exactly what’s happening but lack the linguistic courage to say so.</p><h2><em>The Practice of Plain Naming</em></h2><p>Consider how this evasion plays out in our current discourse:</p><p>We don’t say “Trump is implementing fascist policies.” We say “Trump’s approach raises concerns about democratic norms.”</p><p>We don’t say “Republicans are supporting mass deportation operations.” We say “There are disagreements about immigration enforcement strategies.”</p><p>We don’t say “Conservative media spreads lies designed to enable authoritarianism.” We say “Different sources present different perspectives on complex issues.”</p><p>We don’t say “MAGA supporters have chosen to enable fascism.” We say “There are legitimate grievances driving political polarization.”</p><p>Each euphemism makes the reality a little less clear, a little less urgent, a little less morally demanding. Each hedge creates space for people to avoid confronting what they’re witnessing or participating in. Each refusal to name plainly is a small act of collaboration with the forces that depend on confusion to operate.</p><p>When Trump orders ICE to conduct “Mass Deportation Operations” in cities he identifies as “the core of the Democrat Power Center,” that’s not immigration policy—it’s the use of state violence against political opponents. When he&nbsp;<a href=\"https://www.axios.com/2025/05/29/trump-office-remigration-state-department-immigration\">calls</a>&nbsp;for “REMIGRATION” of millions of people, that’s not border security—it’s forced population transfer. When federal agents separate families and detain children, that’s not law enforcement—it’s state-sanctioned cruelty.</p><p>The defenders will say “the law is the law”—as if legality were equivalent to morality. But slavery was&nbsp;<a href=\"https://www.archives.gov/milestone-documents/dred-scott-v-sandford\">legal</a>. Segregation was&nbsp;<a href=\"https://www.pbs.org/wgbh/americanexperience/features/freedom-riders-jim-crow-laws/\">legal</a>. Japanese internment was&nbsp;<a href=\"https://www.smithsonianmag.com/smart-news/wild-canines-carry-dna-red-wolf-long-believed-extinct-180971252/\">legal</a>. Every authoritarian regime in history has&nbsp;<a href=\"https://encyclopedia.ushmm.org/content/en/article/nuremberg-laws\">operated through law</a>, not despite it. “The law is the law” is not a moral position—it’s moral abdication disguised as principled governance.</p><p>Law without moral foundation is just organized violence. Rules without ethical grounding are just systematized cruelty. When your only defense of a policy is that it’s technically legal, you’ve already admitted it’s morally indefensible.</p><h2><em>The Sophisticates’ Resistance</em></h2><p>The sophisticates will tell you that such plain language is “inflammatory,” “divisive,” “unhelpful to productive dialogue.” They’ll suggest that calling fascism “fascism” alienates potential allies, shuts down conversation, makes compromise impossible.</p><p>But here’s what they’re really saying: they prefer the comfort of ambiguity to the responsibility that comes with clarity. They’d rather maintain the illusion of reasoned discourse than confront the reality that one side has abandoned reason entirely. They want to keep playing by rules that the other side has explicitly rejected.</p><p>This isn’t sophistication—it’s cowardice. It’s the intellectual’s version of appeasing authoritarianism through linguistic accommodation. It’s the belief that if we just find the right words, the right tone, the right approach, we can somehow reason with people who have chosen unreason as their governing principle.</p><p>But you cannot have productive dialogue with fascists about the merits of fascism. You cannot find common ground with people who reject the premise of shared reality. You cannot compromise with those who view compromise as weakness and good faith as stupidity.</p><p>What you can do is name what they are doing clearly enough that people understand what’s at stake and what choice they face.</p><p>The power of plain naming is that it forces moral confrontation. It makes people choose sides. It strips away the comfortable distance that euphemism provides. It demands that people acknowledge what they’re actually supporting rather than hiding behind sanitized language.</p><p>This is why authoritarians work so hard to control language. They understand that linguistic precision is the enemy of moral confusion. That clear naming makes their projects harder to defend. That euphemism is their friend and clarity is their enemy.</p><p>They want us to call their fascism “nationalism.” Their lies “alternative facts.” Their cruelty “tough love.” Their mass deportations “border security.” Their authoritarianism “law and order.”</p><p>Every time we adopt their language, we do their work. Every time we refuse to name their actions plainly, we make those actions easier to defend, easier to rationalize, easier to continue.</p><p>When we refuse to call fascism “fascism”, we don’t make fascism less dangerous. We make ourselves less capable of recognizing and resisting it. We participate in our own disorientation. We become accomplices to our own confusion.</p><p>The courage to name things plainly is not the courage to be harsh or inflammatory. It’s the courage to accept the responsibility that comes with seeing clearly. It’s the courage to abandon the comfortable illusion of neutrality and acknowledge that some things cannot be straddled, some positions cannot be hedged, some realities cannot be euphemized away.</p><p>To say that systematic deployment of federal troops against American cities constitutes military occupation is not inflammatory—it’s accurate. To say that mass deportation operations targeting political opponents constitute fascist policy is not hyperbolic—it’s precise. To say that obvious lies designed to enable authoritarianism are lies is not divisive—it’s necessary.</p><p>The alternative to plain naming is not diplomatic nuance—it’s moral blindness. It’s the systematic erosion of our capacity to recognize authoritarianism when it appears in familiar forms, speaking familiar languages, wearing familiar clothes.</p><p>Evil depends on our unwillingness to call it evil. Fascism depends on our refusal to call it fascism. Lies depend on our treatment of them as “alternative perspectives.” State violence depends on our description of it as “tough policy choices.”</p><p>The moment we name these things plainly, we restore the moral clarity that makes effective resistance possible. We acknowledge what we’re actually facing. We accept the responsibility that comes with seeing clearly. We choose truth over comfort, accuracy over diplomacy, moral clarity over intellectual sophistication.</p><p>This is not just a linguistic choice—it’s a moral one. Every time we speak plainly about what we’re witnessing, we strike a blow against the forces that depend on confusion to operate. Every time we call fascism “fascism”, we make fascism a little harder to defend. Every time we name state violence as state violence, we make such violence a little less acceptable.</p><p>Two plus two equals four. There are twenty-four hours in a day. And Trump’s mass deportation operations are fascistic displays of state violence targeting political enemies whether we have the courage to call them that or not.</p><p>The difference is not in the reality—the difference is in our capacity to respond to reality appropriately.</p><p>Name it plainly. Not because it’s easy, but because it’s true. Not because it’s comfortable, but because comfort in the face of authoritarianism is itself a form of collaboration. Not because it’s diplomatic, but because diplomacy with fascists is enabling fascism.</p><p>The revolution is linguistic honesty. The rebellion is calling things by their proper names. The resistance is refusing to participate in the euphemistic erosion of moral clarity.</p><p>Say what you see. Name what you know. Call fascism&nbsp;.</p><p>Every minute of every day.</p><p>Remember what’s real. Because the alternative to naming fascism clearly isn’t moderation or diplomacy—it’s surrender.</p><p><em>Mike Brock is a former tech exec who was on the leadership team at Block. Originally published at his&nbsp;<a href=\"https://www.notesfromthecircus.com/p/name-it-plainly\" target=\"_blank\" rel=\"noreferrer noopener\">Notes From the Circus</a></em>.</p>","contentLength":12355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ICEBlock, an app for anonymously reporting ICE sightings, goes viral overnight after Bondi criticism","url":"https://techcrunch.com/2025/07/01/iceblock-an-app-for-anonymously-reporting-ice-sightings-goes-viral-overnight-after-bondi-criticism/","date":1751407778,"author":"Zack Whittaker","guid":179335,"unread":true,"content":"<article>The citizen app for anonymously reporting ICE agents and raids went viral after criticism from the U.S. attorney general.</article>","contentLength":121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EFFecting Change: EFF Turns 35!","url":"https://www.eff.org/deeplinks/2025/06/effecting-change-eff-turns-35","date":1751407776,"author":"Melissa Srago","guid":179356,"unread":true,"content":"<p><a href=\"https://www.eff.org/35\">We're wishing EFF a happy birthday on July 10!</a> Since 1990, EFF's lawyers, activists, analysts, and technologists have used everything in their toolkit to ensure that technology supports freedom, justice, and innovation for all people of the world. They've seen it all and in this special edition of our EFFecting Change livestream series, leading experts at EFF will explore what's next for technology users.</p><p>Want to make sure you don’t miss our next livestream? Here’s a link to sign up for updates about this series:<a href=\"https://www.eff.org/ECUpdates\" target=\"_blank\" rel=\"noopener noreferrer\">eff.org/ECUpdates</a>.</p>","contentLength":539,"flags":null,"enclosureUrl":"https://www.eff.org/files/banner_library/effectingeffturns35_banner.png","enclosureMime":"","commentsUrl":null},{"title":"Amazon Deploys Its One Millionth Robot, Releases Generative AI Model","url":"https://hardware.slashdot.org/story/25/07/01/2046242/amazon-deploys-its-one-millionth-robot-releases-generative-ai-model?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751407200,"author":"BeauHD","guid":179357,"unread":true,"content":"An anonymous reader quotes a report from TechCrunch: After 13 years of deploying robots into its warehouses, Amazon reached a new milestone. The tech behemoth now has 1 million robots in its warehouses, the company announced Monday. This one millionth robot was recently delivered to an Amazon fulfillment facility in Japan. That figure puts Amazon on track to reach another landmark: Its vast network of warehouses may soon have the same number of robots working as people, according to reporting from The Wall Street Journal. The WSJ also reported that 75% of Amazon's global deliveries are now assisted in some way by a robot. Amazon also unveiled a new generative AI model called DeepFleet, built using SageMaker and trained on its own warehouse data, which improves robotic fleet speed by 10% through more efficient route coordination.","contentLength":840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Figma moves closer to a blockbuster IPO that could raise $1.5B","url":"https://techcrunch.com/2025/07/01/figma-moves-closer-to-a-blockbuster-ipo-that-could-raise-1-5b/","date":1751406934,"author":"Julie Bort","guid":179303,"unread":true,"content":"<article>The financials are impressive and  founder CEO Dylan Field already cashed out $20 million worth of shares last year.</article>","contentLength":116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$70M Committed To Boba Network As Foundation Concludes BOBA Token Agreement With FTX Recovery Trust","url":"https://hackernoon.com/$70m-committed-to-boba-network-as-foundation-concludes-boba-token-agreement-with-ftx-recovery-trust?source=rss","date":1751406225,"author":"Chainwire","guid":179355,"unread":true,"content":"<p><strong>Grand Cayman, Cayman Islands, July 1st, 2025/Chainwire/--</strong>Boba Governance Foundation today announced a significant milestone with a $70 million capital commitment secured from Awaken Foundation and LDA Capital to fund the continued development and ecosystem expansion of Boba Network, the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology. The Foundation also announced an agreement with FTX Recovery Trust regarding the BOBA tokens held by the Trust.&nbsp;</p><p>The $70 million capital infusion will serve as a catalyst for Boba Network's ambitious growth plans. The funding will be strategically allocated to bolster the network's core infrastructure, expand its developer ecosystem, and foster the creation of innovative decentralized applications (dApps) on the platform, with a particular focus on enabling AI-powered dApps.</p><blockquote><p>\"This funding will accelerate the development of the Boba Network ecosystem, attract top-tier talent, and drive the widespread adoption of Boba Network as a premier Layer-2 solution for AI-powered dApps. We are excited to collaborate with Boba Network partners to shape the future of the AI-powered, decentralized web,\" said Alan Chiu, CEO of Enya Labs, a core contributor to Boba Network.\"</p></blockquote><blockquote><p>\"This substantial capital commitment from Awaken Foundation and LDA Capital is a testament to the transformative potential of Boba Network,\" said David Acutt, director of Boba Governance Foundation.</p></blockquote><p>Awaken Foundation, a key advocate for decentralized infrastructure and digital sovereignty, sees Boba Network as a critical component in the next phase of Web3 evolution.</p><blockquote><p>“We are thrilled to support the Boba Governance Foundation in its pursuit of open innovation,”said Nattaphol Vimolchalao, Director at Awaken Foundation. “Boba’s ability to connect smart contracts with off-chain computation—especially AI—unlocks enormous potential across industries.”</p></blockquote><p>LDA Capital, known for backing high-growth tech ventures and digital asset ecosystems, echoed that sentiment.</p><blockquote><p>“Boba Network is building essential infrastructure for the future of decentralized computation,” said Warren Baker, Managing Partner at LDA Capital. “We believe Boba will play a pivotal role in scaling the next generation of intelligent dApps, and we’re proud to support their mission as they push the boundaries of what’s possible in blockchain technology.”</p></blockquote><p>The strategic partnership with Awaken Foundation and LDA Capital goes far beyond financial support. It represents a powerful alignment of vision, expertise, and global reach. Leveraging deep industry knowledge, business development capabilities, and an extensive network of strategic partners, both firms are uniquely positioned to accelerate Boba Network’s growth. </p><p>This collaboration is set to strengthen Boba’s leadership in blockchain innovation and drive its next phase of global expansion. In addition, LDA Capital offers differentiated value through LDA Velocity, its institutional-grade liquidity and market-making platform that supports healthy, scalable token ecosystems across global exchanges.</p><ul><li>Infrastructure Enhancement: The funding will be used to strengthen Boba Network's infrastructure, ensuring high throughput, low latency, and robust security for users and developers.</li><li>Ecosystem Expansion: A portion of the capital will be dedicated to expanding the Boba Network ecosystem by attracting developers, projects, and users through grants and educational initiatives.</li><li>dApp Development: The funding will support the creation of innovative dApps on Boba Network, with a strong emphasis on AI-powered dApps, ranging from decentralized finance (DeFi) protocols to real-world assets (RWA) applications.</li><li>Community Engagement: Boba Governance Foundation will continue to foster a vibrant and engaged community by providing resources, support, and opportunities for collaboration.</li></ul><h3>Resolution with FTX Recovery Trust</h3><p>In addition, Boba Governance Foundation has executed an agreement with FTX Recovery Trust whereby all the BOBA tokens held by the Trust have been transferred to the Foundation. FTX Recovery Trust, in addition to other consideration and mutual release of claims, received the right to purchase up to approximately 29.4M BOBA tokens from Boba Governance Foundation at $0.09 per token within the next 18 months.</p><blockquote><p>“This agreement represents a momentous milestone for Boba Network, as it removes a major source of uncertainty over the BOBA token and strengthens the Foundation’s ability to support the continued development of Boba Network and its ecosystem,” said Acutt.</p></blockquote><h3>About Boba Governance Foundation</h3><p> is a non-profit organization dedicated to the advancement and growth of Boba Network. It supports the development of the network's technology, fosters community engagement, and promotes the adoption of Boba Network across various industries. Boba Network is the leading Layer-2 blockchain for AI-powered decentralized applications (dApps), enabled by its unique HybridCompute technology.</p><p> is a private investment firm founded by seasoned crypto, venture capital, and public market investors. The firm seeks to invest in established blockchain protocols to help further develop its technology. Awaken provides strategic capital, accelerated business development, and engineered exits for protocols that Awaken believes have a promising future in the modern economy.</p><p> is a global alternative investment group with expertise in cross-border transactions worldwide. The team has collectively executed over 350 transactions in both the public and private middle markets across 43 countries with aggregate transaction values of over USD $11 billion. LDA’s investment activities across Web3 include 27+ transactions totaling $400m+ in capital commitments.</p><p>:::tip\nThis story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;.</p>","contentLength":5903,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized Public-Key Infrastructure: The Future of Supply Chain Security","url":"https://hackernoon.com/decentralized-public-key-infrastructure-the-future-of-supply-chain-security?source=rss","date":1751405515,"author":"Nneoma Uche","guid":179354,"unread":true,"content":"<p>All products, digital and physical, pass through a supply chain—a network of actors that supports their life cycle. But as the global market becomes increasingly interconnected, supply chain attacks are on the rise.</p><p>In March 2023, cybercriminals infiltrated <a href=\"https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fcloud%2Egoogle%2Ecom%2Fblog%2Ftopics%2Fthreat-intelligence%2F3cx-software-supply-chain-compromise&amp;urlhash=ikkE&amp;trk=article-ssr-frontend-pulse_little-text-block\">3CX’s</a> build environment, injecting malicious code into a library file for its macOS and Windows desktop apps. The compromised file was distributed through official updates, exposing users to malware. This breach emphasized the shortcomings of centralized systems in securing supply chains, as a single compromised vendor can jeopardize the privacy of the entire customer base.</p><p>Web3 supply chains, leveraging blockchain and Decentralized Public Key Infrastructure (DPKI), offer a robust alternative. By prioritizing transparency, traceability, and tamper-proof security, they present a stronger defense against supply chain threats.</p><p>This article explores how DPKI in blockchain-driven networks outperforms traditional PKI, and why a supply chain powered by the latter presents a tougher nut for bad actors to crack.</p><h2>Understanding Public-Key Infrastructure (PKI)</h2><p>IBM defines <a href=\"https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fwww%2Eibm%2Ecom%2Fthink%2Ftopics%2Fpublic-key-infrastructure&amp;urlhash=PAzP&amp;trk=article-ssr-frontend-pulse_little-text-block\">‘Public-Key Infrastructure (PKI)’</a> as a comprehensive framework used to assign and verify user identity through digital certificates, for secure digital communications. The entire PKI framework relies on asymmetric cryptography- the use of a public and private key pair to encrypt and decrypt data, respectively.</p><p>PKI allows us to associate identities with particular key pairs. Although the public key can be visible to anyone on the network, only the entity with the corresponding private key can access specific features or information.</p><p>In a supply chain, PKI combines digital certificates and asymmetric cryptography to establish trust and ensure integrity.&nbsp; The public keys are embedded in a digital certificate, which authenticates the user or device communicating across the network.</p><ol><li><p>Certificate Authority (CA): The Certificate Authority is a trusted entity that issues digital certificates to participants in the supply chain, e.g., developers, analytics providers, payment gateways, cloud providers, etc.</p></li><li><p>Certificate: Digital certificates are cryptographic credentials, issued and signed by the CA, used to verify the identity of and secure communication among supply chain actors. They typically include public keys and identity details, accessible upon request.</p></li><li><p>Registration Authority (RA): The RA ensures that only authorized participants can obtain a digital certificate, thus enhancing security within the supply chain. The CA can double as the registration authority, although trusted third-party services are just as efficient.</p></li><li><p>Certificate database: This PKI component is a secure repository or location that stores issued digital certificates, alongside their metadata, i.e., public keys, revocation status, and validity details.</p></li><li><p>Certificate policy: This is a formal document outlining the procedures and requirements governing the issuance, usage and management of digital certificates within the supply chain.</p></li><li><p>Central directory: The central directory is a public repository where cryptographic keys, digital certificates and Certificate Revocation Lists(CRLs) are indexed and stored. It enables anyone in the ecosystem to authenticate a digital signature and encrypt data to a specific key owner.</p></li></ol><h3>Traditional Supply Chain Systems (Web2) vs. Web3-Driven Supply Chains</h3><p>Web2-based supply chains are centralized; participants rely on Certificate Authorities to verify other actors and establish trust. Moreover, digital certificates and cryptographic keys are stored in central directories, leaving room for supply chain attacks in the absence of robust security measures.</p><p>Another feature of Web2-based supply chains is the opacity around certificate issuance and revocation. There’s no universal metric to determine eligibility for certificate issuance. Instead, entities must operate on a trust assumption that the CA has properly vetted the requester.</p><p>Oftentimes, delayed updates and limited visibility associated with Certificate Revocation Lists (CRLs) may result in revoked certificates appearing valid to related devices or applications. This can impact supply chain integrity, due to unauthorized access, tampered goods, compliance issues and a loss of trust.</p><p> In 2024, <a href=\"https://www.entrust.com/blog/2024/07/thoughts-on-the-google-chrome-announcement-and-our-commitment-to-the-public-tls-certificate-business\">Google delisted Entrust</a> (a formerly reputable Certificate Authority), from its Chrome Root Program due to malfunctions in its certificate issuance and revocation operations. A few months prior, Entrust admitted to misissuing over 26,000 digital certificates and failing to revoke them within the revocation timeline outlined by the Certificate Authority/Browser Forum.</p><p>Web3-driven supply chains, on the other hand, leverage decentralized systems, smart contracts, and pseudonymous transactions to create a trustless, transparent, and secure ecosystem. Unlike traditional supply chains that rely on centralized authorities, Web3 enables each participant to interact directly on a shared blockchain, reducing intermediaries and single points of failure.</p><p>In addition, PKI data (i.e. public keys and certificates) is stored immutably on a blockchain, making them nearly tamper-proof, while being easily accessible for verification. Together, these features make Web3 supply chains more resilient and trustworthy than ttheir raditional counterparts.</p><h3>From PKI to DPKI: Strengthening Supply Chain Integrity in Web3</h3><p>In Web3-driven systems, PKI implementation shifts from traditional Certificate Authorities (CAs) to decentralized or distributed models that align with Web3 principles, hence the term—<a href=\"https://www.ihrim.org/glossary/decentralized-public-key-infrastructure/\">Decentralized Public-Key Infrastructure (DPKI)</a>.</p><p>The idea behind it is simple: enable tamper-proof verification of supply chain data and participants, without relying on a centralized database.</p><p>Here's how DPKI enhances supply chain integrity in a Web3 ecosystem:</p><ul><li><em>Decentralized trust models manage authentication and verification</em></li></ul><p>Rather than a centralized entity, DPKI relies on a web of trust—a network of on-chain participants who collectively verify and authenticate information. Each supplier on-chain uses a decentralized identifier (DID), which functions as a unique digital signature, to prove authenticity, access data and sign transactions.</p><p>Vendors interacting in a decentralized supply network use their DIDs to access proprietary data, verify purchase orders, and access secure channels. Similarly, a product within the chain can be assigned a DID, enabling participants to verify its origin and authenticity at every step.</p><ul><li><em>Smart contracts ensure transparency and data integrity</em></li></ul><p><a href=\"https://hackernoon.com/309-stories-to-learn-about-smart-contracts\">Smart contracts</a> are self-executing programs stored on a blockchain that automatically trigger specific actions once preset conditions are met. They automate various processes within a supply chain, such as processing payments, issuing tickets, or approving shipments. All interactions with the contract are recorded on the blockchain, creating a permanent and tamper-proof audit trail. </p><p><strong>This record allows stakeholders to trace:</strong></p><ol><li><p>The journey of goods through the supply chain.</p></li><li><p>Compliance with standards at each stage.</p></li><li><p>Discrepancies back to their source.</p><p>Automation through digital contracts reduces the risk of fraud, human error, and compliance issues.</p></li></ol><ul><li><em>Blockchain-powered scalability</em></li></ul><p>By eliminating reliance on a Certificate Authority and other intermediaries, decentralized public key infrastructure offers more scalability to Web3-based supply chains. In traditional PKI, managing certificates across a complex supply chain can leave the supply network vulnerable to single points of failure. Moreover, scaling may require the involvement of multiple Certificate Authorities, likely resulting in delays or bottlenecks around certificate issuance and revocation. This approach is resource-intensive and may be unrealistic for global supply chains involving numerous entities.</p><p>In contrast, Web3-driven supply chains leverage blockchain as a trust anchor, enabling a distributed system where records and identities are verifiable by all participants on the chain. The result is a more efficient, scalable infrastructure, tailored to the complexity of modern supply chains.</p><h3>Advantages of DPKI to (Web3) Supply Chains</h3><ol><li>Eliminates Central Authority Risks: No single entity can compromise the supply chain.</li><li>Self-Sovereign Identity: Supply chain participants control their cryptographic identities, reducing the overhead associated with traditional certificate issuance and management.</li><li>Enhanced Transparency: All actions (such as key creation and revocation), and transactions are publicly recorded, promoting trust and accountability across the chain.</li><li>Efficiency: Automation through smart contracts streamlines processes such as inspections and approvals, saving time and resources.</li><li>Enhanced Security: Cryptographic signatures prevent data tampering and fraud, thus protecting data authenticity.</li><li>Improved Scalability: Participants can manage their keys and verify others without bottlenecks from centralized authorities.</li></ol><h3>Integrating blockchain: a path to modernizing supply chains</h3><p>The shift from centralized to decentralized systems is no longer a futuristic concept, but a growing reality for Web2 companies looking to modernize their supply chains. Companies like IBM, with its blockchain-powered Food Trust, and&nbsp; De Beers’ Tracr, used for tracking diamonds from source to store, demonstrate how blockchain and DPKI can integrate seamlessly into existing supply chain models to enhance transparency and trust.</p><p>Harnessing the benefits of decentralized supply systems doesn't require a sudden overhaul of the existing supply chain. Web2 companies can adopt an incremental approach—beginning by identifying use cases—and testing DPKI in targeted areas, before gradually scaling the integration across the entire supply chain.</p>","contentLength":9795,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Landmark EU Tech Rules Holding Back Innovation, Google Says","url":"https://tech.slashdot.org/story/25/07/01/1811254/landmark-eu-tech-rules-holding-back-innovation-google-says?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751404800,"author":"msmash","guid":179308,"unread":true,"content":"Google will tell European Union antitrust regulators Tuesday that the bloc's Digital Markets Act is stifling innovation and harming European users and businesses. The tech giant faces charges under the DMA for allegedly favoring its own services like Google Shopping, Google Hotels, and Google Flights over competitors. Potential fines could reach 10% of Google's global annual revenue. \n\nGoogle lawyer Clare Kelly will address a European Commission workshop, arguing that compliance changes have forced Europeans to pay more for travel tickets while airlines, hotels, and restaurants report losing up to 30% of direct booking traffic.","contentLength":635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complete Gemini CLI Setup Guide for Your Terminal","url":"https://hackernoon.com/complete-gemini-cli-setup-guide-for-your-terminal?source=rss","date":1751404752,"author":"Vladislav Guzey","guid":179353,"unread":true,"content":"<p>Have you ever wished for an AI assistant right inside your terminal window? Well, your dream may have just come true because Google just released&nbsp;<a href=\"https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/\">Gemini CLI</a>. In this tutorial, I’m going to show you everything you need to know about this new open-source AI agent. We’ll cover how to use it, the pricing, and some useful tips and tricks. So, if you’re ready, let’s get started! ;)</p><p>Gemini CLI is a free and open-source AI agent that works directly in your terminal. This powerful tool brings Google’s Gemini models to your command line, allowing for natural language interaction to get work done. You can ask it to:</p><ul></ul><p>You don’t have to constantly switch between a web app and your terminal. And the best part? It’s&nbsp;, with no complicated setup.</p><h2>Getting Started with Gemini CLI</h2><h3>Step 1: Install Gemini CLI on Linux</h3><p>You can install&nbsp;&nbsp;on&nbsp;,&nbsp;, and&nbsp;. All the setup we will do inside the terminal. I am using Linux, but for Mac and Windows, the commands are almost the same, so you can follow my steps.</p><p>To get started, make sure you have Node.js version 18 or higher. You can check this by running:</p><p>If you don’t have it, use the following command to install it:</p><pre><code>sudo apt update &amp;&amp; sudo apt install nodejs npm\n</code></pre><p>Then, I installed Gemini CLI globally with:</p><pre><code>npm install -g @google/gemini-cli\n</code></pre><p>If you don’t want to install it globally, you can also use:</p><pre><code>npx https://github.com/google-gemini/gemini-cli\n</code></pre><p>After installing, just type:</p><p>After that, you need to log in with your personal Google account.</p><p>This gives you access to a free Gemini Code Assist license, which includes:</p><ul><li>Access to Gemini 2.5 Pro.</li><li>A massive 1 million token context window.</li><li>60 model requests per minute.</li><li>1,000 model requests per day at no charge.</li></ul><p>Now you are ready to start asking questions and running tasks. You can ask the Agent to create a project, fix the bugs, explain the code in specific files, etc.&nbsp;<strong>Ensure that you run the agent within your project folder.</strong></p><pre><code>&gt; What does the file index.js do?\n</code></pre><p>It read the file, analyzed it, and gave a clear explanation.</p><pre><code>&gt; Add error handling to index.js\n</code></pre><p>You can also run shell commands directly by using&nbsp;, like this:</p><h2>Creating a Simple To-Do App with Google CLI</h2><p>Now that we’re all set up, let’s ask the AI to create a simple to-do application using HTML, CSS, and JavaScript.&nbsp;<em>I will type “create a simple to-do app using simple js and html” into the Gemini CLI.”</em>&nbsp;Watch the video to see the step-by-step process and the result.</p><p>Gemini CLI has some handy built-in tools. You can use commands like:</p><ul><li>&nbsp;(). Lists files and folders in a directory—just like the shell&nbsp;&nbsp;command.</li><li>&nbsp;(). Reads the full content of a single file, useful for summaries or analysis.</li><li>&nbsp;(). Reads multiple files at once, typically matching a glob pattern (e.g., all&nbsp;&nbsp;files)</li><li>&nbsp;(). Searches for files by pattern (e.g., find all&nbsp;&nbsp;across your project).</li><li>&nbsp;(). Searches within files for text, like finding all&nbsp;&nbsp;comments.</li><li>&nbsp;(). Applies code changes via diffs. Gemini previews edits and asks for approval before applying them.</li><li>&nbsp;(). Creates new files (for example,&nbsp;) with user-provided content.</li><li>&nbsp;(). Runs commands you prefix with&nbsp;&nbsp;(e.g.,&nbsp;) directly in the terminal .</li><li>&nbsp;(). Fetches content from the web (HTML or JSON), enabling Gemini to analyze external data.</li><li>&nbsp;(). Performs a Google search to ground responses with real-world information (e.g., explanation for an error).</li><li>&nbsp;(). Stores facts or preferences during a session (like “I prefer async/await”) to improve consistency</li></ul><p>To see all available tools, you can use the&nbsp;&nbsp;command.</p><p>You can add specific instructions for the AI for a particular project by creating a&nbsp;&nbsp;file in your project’s root directory. Inside this file, you can define project rules, code styles, and the tools the agent should use. This ensures that the generated code is consistent with your project’s standards.</p><h2>Google CLI MCP Integration</h2><p>For most day-to-day uses, the built-in tools will suffice. But what if you want Gemini CLI to do something very domain-specific, like interact with specific APIs or use a specialized model (say an image generator or a security analysis tool)? This is where&nbsp;<strong>MCP (Model Context Protocol)</strong>&nbsp;comes in.</p><p>MCP is essentially an open standard that allows developers to&nbsp;<strong>add new tools/abilities to the AI by running a server</strong>&nbsp;that the CLI can communicate with. In Gemini CLI, you can configure “MCP servers” in a JSON settings file, and the CLI will treat those as additional tools it can use.</p><h3>How to Set Up the MCP Server in Google CLI</h3><p>As an example, I am going to show you how to set up the&nbsp;<strong>GitHub MCP server in Gemini CLI.</strong></p><p>Inside your project folder, create a folder by using the command:</p><pre><code>mkdir -p .gemini &amp;&amp; touch .gemini/settings.json\n</code></pre><p>Inside the file, add the following code:</p><pre><code>{  \n  \"mcpServers\": {  \n    \"github\": {  \n      \"command\": \"npx\",  \n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],  \n      \"env\": { \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"[YOUR-TOKEN]\" }  \n    }  \n  }  \n}\n</code></pre><p>After that&nbsp;&nbsp;, from the Gemini CLI, and then reopen it.</p><p>Write&nbsp;&nbsp;command, and you will see a list of&nbsp;.</p><p>Now you, Agent, can interact with GitHub. That simple! :)</p><p>You can try it free for personal usage, but there is also a paid version that is billed based on token usage.</p><ul><li>: Free with Google account</li><li>: Up to&nbsp;&nbsp;and&nbsp;</li><li>Great for individual developers and small-scale use</li></ul><ul><li>Use your own&nbsp;&nbsp;for higher usage</li><li>Billed based on tokens consumed (model and usage dependent)</li></ul><ul><li>Available through&nbsp;<strong>Gemini Code Assist Standard or Enterprise</strong>&nbsp;plans</li><li>Includes advanced features like governance, audit logs, and shared quotas</li></ul><p>As you can see, Gemini CLI is a really powerful tool with a lot of potential. I’m excited to see how I’ll be using it in my daily workflow.</p><p>If you write code, debug things, or manage files often, this tool is worth checking out.</p><p>If you have any feedback, please share it in the comments below. ;)</p>","contentLength":5807,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a Personal AI Factory","url":"https://www.john-rush.com/posts/ai-20250701.html","date":1751404469,"author":"derek","guid":179509,"unread":true,"content":"<p>I keep several claude code windows open, each on its own git-worktree. o3 and sonnet 4 create plans, sonnet 3.7 or sonnet 4 execute the plan, and o3 checks the results against the original ask. Any issues found are fed back into the plan template and the code is regenerated. The factory improves itself.</p><p>Read on to see what might be useful for you.</p><h2>Guiding Principle – Fix Inputs, Not Outputs</h2><p>When something goes wrong, I don’t hand-patch the generated code. I don’t argue with claude. Instead, I adjust the plan, the prompts, or the agent mix so the next run is correct by construction.</p><p>If you know <a href=\"https://factorio.com/\">Factorio</a> you know it’s all about building a factory that can produce itself. If not, picture a top-down sandbox where conveyor belts and machines endlessly craft parts because the factory must grow. Do the same thing with AI agents: build a factory of agents that can produce code, verify it, and improve themselves over time.</p><h2>Basic day to day workflow - building the factory</h2><p>My main interface is <a href=\"https://claude.ai/code\">claude code</a>. It’s my computer now. I also have a local mcp which runs <a href=\"https://github.com/block/goose\">Goose</a> and o3. Goose only because I’ve already got it setup to use the models hosted in our Azure OpenAI subscription. Looking to improve this at some point, but it works for now.</p><p>I’ll give a high level task to claude code, which calls over to o3 to generate a plan. o3 is a good planner and can ask a bunch of good questions to clarify the job to be done. I then have it write out a  file with both my original ask and an implementation plan.</p><p>First, sonnet 4 reads the plan, verifies it, and turns it into a task list. Next claude code execute the plan, either with sonnet 3.7 or sonnet 4 depending on the complexity of the task. Because most of my day-to-day is in clojure I tend to use sonnet 4 to get the parens right.\nOne important instruction is to have claude write commits as it goes for each task step. This way either claude or I can revert to a previous state if something goes wrong.</p><h3>Step 3: Verification → Feedback into Inputs</h3><p>Once the code is generated, I have sonnet 4 verify the code against the original plan. Then I have o3 verify the code against the original plan and original ask. o3 is uncompromising. Claude wants to please, so will keep unnecessary backwards compatibility code in place. o3 will call that out and ask for it to be removed. Claude also tends to add “lint ignore flags” to the code which o3 will also call out. Having both models verify the code catches issues and saves me back and forth with claude.</p><p>Any issue sonnet 4 or o3 finds gets baked back into the plan template, not fixed inline.</p><p>Git worktrees let me open concurrent claude code instances and build multiple features at once. I still merge manually, but I’m no longer babysitting a single agent.</p><ul><li>Outputs are disposable; plans and prompts compound.</li><li>Debugging at the source scales across every future task.</li><li>It transforms agents from code printers into self-improving colleagues.</li></ul><p>Example: an agent once wrote code that would load an entire CSV into memory. I made it switch to streaming and had the agent write instructions to the plan to always use streaming for CSVs. Now, my plan checker flags any code that doesn’t use streaming for CSVs, and I don’t have to remember this in every PR review. The factory improves itself.</p><p>I’ve started to encode more complex workflows, where I have specific agents (behind mcps) for building specific tasks.</p><p>One MCP will sweep all the clojure code generated and then apply our local style rules. These rules are part of the instructions for the original plan and agent but often the generated code will have style issues. Especially once claude gets in the lint/test/debug cycle. This focused agent means we have tighter behavior and can apply our style rules consistently.</p><p>I’ve started doing this for internal libraries as well. It’s good at looking at generated code and replacing things like retries and  with our retry library.</p><p>I’m also building out a collection of these small agents. Each one can take a small specific task, and by composing them together I can build more complex workflows. For example, I can take an api doc, and a set of internally defined business cases and have a composition of agents build integrations, tests, and documentation for the api. This is a powerful way to build out features and integrations without having to do all the work by hand.</p><p>You don’t get there in one big step. Here’s the secret sauce: </p><p>It’s essentially free to fire off a dozen attempts at a task - so I do. All agents run in parallel. When one fails, stalls, or lacks context, I feed that lesson into the next iteration. I resist the urge to fix outputs, instead I fix the inputs.</p><p>That loop is the factory: the code itself is disposable; the instructions and agents are the real asset.</p><p>I’m working on a few things to improve the factory:</p><ul><li>Better overall coordination of the agents. I tend to kick things off manually, but I want to have a more automated way to manage the workflow and dependencies between agents.</li><li>Aligning our business docs with the agents. Changing the information we capture to be at a higher level of abstraction so that the agents can use it more effectively. This means moving away from low level implementation details and focusing on use cases.</li><li>More complex workflows. I’ve been able to build some pretty complex workflows with the current setup, but I want to push it further. This means more agents, more coordination, and more complex interactions between them.</li><li>Maximize token usage across providers. I’m pretty limited by bedrock’s token limits especially for sonnet 4. Going to need to be able to switch between the claude max plan and bedrock w/out interruption.</li></ul><p>That’s where my factory sits today: good enough to ship code while I refill my coffee, not yet good enough to bump me off the payroll. Constraints will shift, but the core principle remains: .</p>","contentLength":5911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44438065"},{"title":"Effectiveness of trees in reducing temperature, outdoor heat exposure in Vegas","url":"https://iopscience.iop.org/article/10.1088/2752-5295/ade17d","date":1751403545,"author":"PaulHoule","guid":179508,"unread":true,"content":"<h2>We apologize for the inconvenience...</h2><p>To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. </p><p>If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.</p>","contentLength":269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437948"},{"title":"AMD Preps Some Compute Driver Fixes For Polaris & Hawaii Era GPUs With Linux 6.17","url":"https://www.phoronix.com/news/Linux-6.17-AMDGPU","date":1751402857,"author":"Michael Larabel","guid":179270,"unread":true,"content":"<article>AMD today submitted their initial batch of \"new stuff\" for queuing into DRM-Next of their kernel graphics/compute driver changes they have prepared for the upcoming Linux 6.17 cycle opening in a few weeks...</article>","contentLength":207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Trusted Execution Environments Power Scalable, Private Smart Contracts","url":"https://hackernoon.com/how-trusted-execution-environments-power-scalable-private-smart-contracts?source=rss","date":1751402702,"author":"Blockchainize Any Technology","guid":179352,"unread":true,"content":"<p>The layer-two solution is a straightforward approach that combines the TEE and blockchain to provide smart contracts with confidentiality while keeping scalability. In such systems, the operations of smart contracts are decoupled from their underlying blockchain systems. The smart contracts are executed in an independent layer outside blockchain systems.</p><p>\\\n In a general layer-two solution, the blockchain is used as a dispute resolution layer. The smart contract is executed outside the blockchain, making TEEs act as an agent between users and blockchain systems. Suppose that a user aims to use a private contract. She first needs to compile the original contract code, push binary codes to a TEE, and then upload execution results to the public ledger. As illustrated in Fig.4, we extract a generic data flow as follows. A user sends the encrypted input data to a TEE-powered node. Then, the TEE decrypts the input data and executes the contract. After that, the encrypted execution results are sent to the blockchain platform for verification and storage. Finally, the user fetches and decrypts the blockchain-confirmed results.</p><p>\\\n<strong>Privacy-preserving property.</strong> The  is an essential property. In layer-two systems, such as [46], [68], [69], the contract computations run inside Intel SGX enclaves, while TZ4Fabric [44] moves contract executions into ARM Trusted Zone. Since the contract state-transition process happens inside TEEs, any intermediate states remain invisible to the outside. Meanwhile, to achieve the full lifecycle security for a smart contract, the input sent to a TEE and the output returned from this TEE are also required to be encrypted. For example, in ShadowEth [68], PDOs [46], Phala [69] and Hybridchain [41], the contract invocation arguments are encrypted with the TEE public key. They can only be decrypted within the enclave. Also, before transferring execution results to the blockchain (or users), the intermediate (or final) states in an enclave are encrypted. Some variants also enhance the privacy-preserving properties from other aspects. In Phala [69], only authorized queries to the contract will be answered. The smart contract source codes in ShadowEth [68] are hidden during the procedures of deployment and synchronization. This further reduces the possibility of data leakage in subsequent contract executions. Considering a fixed address may expose the user who has invoked the contract, PDOs [46] also allows the user to use pseudonym addresses for submitting a transaction (including TEE outputs) to the blockchain.</p><p>\\\n<strong>Blockchain intrinsic feature.</strong> ShadowEth [68] and Taxa [70] introduce an external distributed service to manage the contracts, achieving the properties of <em>code immutability, high availability</em> and . Meanwhile, layer-two systems satisfy state consistency for reasons that the encrypted states of contracts in different blockchain nodes will eventually get consistent when reaching a successful agreement. Intuitively, the contracts deployed in layer-two systems should retain the features given by original blockchains. However, some fundamental properties are lost when using layer-two solutions. For example, most layer-two systems lose contract interoperability since each contract is executed in different machines. Among all the evaluated systems, only Phala [69] identifies this issue and proposes a command query responsibility segregation architecture to ensure certain interoperability. Also, public verifiability is a crucial property for the blockchain since it allows each contract invocation, and contract execution to be publicly verifiable. Unfortunately, contracts are executed in TEEs so that the outputs are encrypted. To check whether the TEE has executed contracts following loaded contract specifications is a non-trivial task.</p><p>\\\n An attacker may control the network between users and TEE hosts. Meanwhile, TEEs are assumed to always produce correct results, and the smart contracts inside TEEs cannot deviate from their specifications. The main difference compared with the assumption of layer-one systems is that an adversary can observe the network activities between the TEE interfaces and active blockchain nodes.</p><p>\\\n Several layer-two solutions adopt incentive or punishment mechanisms to encourage TEE hosts to provide a stable and secure environment for executing confidential contracts. For example, Fastkitten [43] and Erdstall [75], [76] propose  transactions, in which a host will be punished if its malicious behavior has been identified. In particular, if the TEE execution is aborted, the host will be charged according to previous deposits. In Taxa [70], every node can identify any faulty nodes with reliable proofs for executing further economic punishment. On another route, TEE hosts in Phala [69] will get paid by providing their computing resources to users. Similarly, the remuneration in ShadowEth [68] will be transferred to TEE hosts who execute private contracts. These mechanisms can effectively prevent malicious TEE hosts from an economic aspect. However, they are powerless against external threats. An adversary may directly terminate a TEE host at any time. Even worse, the TEE provides users with an open interface that is vulnerable to DoS [77] or single-point attack. To overcome such issues and achieve fault tolerance, different methods are proposed. Fastkitten provides low-level fault tolerance by periodically saving an encrypted snapshot of current states in enclaves. If the enclave fails, the TEE host can instantiate a new enclave and restart the computation starting from the encrypted snapshot. Similarly, Taxa [70] stores a session file for maintaining and recovering user’s requests. However, a malicious attacker may directly terminate the TEE host, and Fastkitten does not tolerate such host failures. Another technical route is to maintain a secure network. ShadowEth maintains a group of TEE nodes to ensure consistency via a Paxos-like [78] algorithm. Taxa adopts TEE-enabled computing nodes powered by a PBFT-derived PoS [79] algorithm. Any node in the network has the same responsibility to privately execute smart contracts and transfer execution results to the blockchain. However, this brings additional authentication issues. A TEE host must be carefully authenticated to ensure her TEE capability when joining an external network.</p><p>\\\nMeanwhile, the systems PDOs [46], Phala [69], Ekiden [42] and COMMITEE [73] introduce an expendable and interchangeable solution. TEEs are stateless: any particular TEE can be easily replaced once it has clashed or finished its task. Unfortunately, these solutions are along with new challenges. Firstly, even if TEEs are changeable, detecting a compromised TEE is still difficult. For instance, PDOs can re-execute a method multiple times for the verification. Given the same input parameters to different TEEs, TEEs are believed to work securely only if their outcomes match. Then, the outputs of enclaves are allowed to commit to the blockchain. COMMITEE adopts  TEE host mechanism. If the master TEE host is proved to be malicious, a backup TEE host will continue to work without communications to the master TEE host. Nevertheless, this model increases the attack interface and makes the whole system vulnerable. Secondly, TEE hosts are stateless. That means, to ensure an exceptional execution is recoverable, any persistent state must be stored in the blockchain or a trusted third party (TTP). However, for a non-deterministic blockchain system such as Ethereum (PoS version) [2], verifying whether an item has been stored on the blockchain is a non-trivial task. Meanwhile, storing data in TTPs may lead to the single-point failure, which goes against the blockchain’s real intention.</p><p>\\\n A contract runs inside TEE, and heavily depends on remote attestation service. The SGX-supported blockchain systems including PDOs [46], Fastkitten [43], ShadowEth [68], Phala [69] and Ekiden [42] assume that Intel Attestation Service (IAS) is trusted. IAS can correctly and completely report whether a certain output with cryptographic material ( [80]) is produced by SGX-enabled hardware. However, IAS might be compromised, posing a risk to these architectures. A compromised or hijacked remote attestation service may maliciously report an attestation with the wrong cryptographic material that does not belong to its corresponding TEE hardware, breaking the promised security. Meanwhile, a centralized service might be crashed, causing the leakage of private states. Unfortunately, none of layer-two schemes consider these risks in designs or implementations.</p><p>\\\nAs discussed, current TEE implementations have memory limitations for confidential executions. If the memory usage exceeds the threshold, it may confront significant performance and security issues [81]. Hybridchain [41] optimizes the storage by maintaining transaction records outside Intel SGX. Meanwhile, TZ4Fabric [44] minimizes TCB by avoiding all the executions inside TEEs. However, these approaches increase the implementation complexity. A well-known fact is that a TEE is vulnerable to physical vulnerabilities [57]. Unfortunately, very few layer-two solutions provide remedial measures to reduce the risk of being attacked.</p><p>\\\n A poorly-written contract might deviate from designated functionalities and further leak the secret information. This part discusses the potential pitfalls and remedies when deploying contracts.</p><p>\\\nIn original smart contract systems, gas mechanism is a powerful tool to prevent  attacks [2]. Since the layertwo systems execute smart contract outside the blockchain, a similar mechanism must be considered. Fastkitten [43] and Hybridchain [41] protect against such attacks by using the  mechanism. Limitations are firstly defined on the maximum amount of execution steps that allow to perform inside a TEE per round. Then, TEE monitors smart contract operations. If the number of execution steps exceeds a predefined threshold, the enclave will terminate executions. ShadowEth [68] combines a timeout mechanism with a  mechanism. Similar to the gas mechanism in Ethereum [2], TEE hosts can still gain remuneration even if a contract exits after timeout since they provide sufficient computing power. These mechanisms effectively protect against endless loops and denial-of-service (DoS) launched by external attackers.</p><p>\\\nThe TEE itself lacks self-awareness of input data, since it cannot distinguish which state is fresh. A lack of input data authentication makes the system vulnerable to the rollback attack [82], [59]. A malicious user may attempt to invoke the confidential contract many times to seek the leaked secret information. Authentication of the user’s identity is helpful to prevent this attack. However, none layer-two solution provides these remedies for these potential pitfalls. On the other hand, the TEE input may come from a non-deterministic blockchain system [83], [84], in which deciding whether an input has been confirmed is tricky. Fastkitten [43] and COMMITEE [73] mitigate this issue by using a  mechanism. As for TEE output conflicts, Ekiden [42] uses a probabilistic proofof-publication protocol to avoid the ambiguous input.</p><p>\\\nAfter the invocation of a private contract, the outputs returned from TEEs are uploaded on-chain for the final confirmation. But a malicious TEE host may send an exceptional result to the blockchain. Even worse, two hosts may publish different updates towards the same contract simultaneously. To prevent such malicious publications and to evade conflicts, PDOs [46] depends on Coordination and Commit Log (CCL) to manage the synchronization in the execution of interacting contracts and enables a contract owner to decide on selecting the enclave for contract executions, which effectively avoid conflicts. Phala [69] adopts an event sourcing command query responsibility segregation architecture to scale up and avoid conflicts, in which the write operations are recorded as events and read operations can be served by the current view of states. Again, these solutions contradict the property of decentralization. Ekiden [42] and ShadowEth [68] rely on the blockchain to resolve conflicts resulting from concurrency. In particular, ShadowEth [68] requires a worker to specify the version number with a timestamp when pushing data to the blockchain. Even miners accept different responses at first, they will eventually reach an agreement by comparing version number and the timestamp, with the help of the consensus procedure. Yet, such an approach is inefficient, especially in non-deterministic blockchain systems.</p><p>\\\n PDOs [46] uses a key provisioning service to distribute private keys. The drawback is obvious: A compromised provisioning service could make the entire system fail. To increase the robustness of a private key, Ekiden [42] designs a distributed key generation (DKG) [85] protocol using the secret sharing scheme [86]. Even if one key manager is compromised, an adversary cannot obtain the entire key. However, this solution does not completely solve the key leakage issue. The final keys are assembled and replicated among all end-TEEs. If an adversary compromises an end-TEE, exposing all the contract state becomes a trivial task. The key rotation technology, adopted by Ekiden [42], Fastkitten [43], Phala [69] partially solves the above issue by providing a short-term key in every epoch. An adversary cannot corrupt a future or previous committed state, which minimizes the possibility of key exposure to attackers and further helps the layer-two system to achieve forward secrecy. Also, layer-two projects such as COMMITEE [73] mitigate these key issues by providing each TEE per secret key. Even if a certain TEE’s private key were stolen, this only would affect the smart contract running on that compromised TEE. Furthermore, Phala Network [69], equips each contract with an asymmetric key called the , which also enhances the key security to a certain degree.</p><p>\\\nThe layer-two solution decreases computational burden and avoids latency by decoupling the smart contract executions from consensus mechanisms. The solution merely puts the execution results on-chain rather than all processing states. Meanwhile, the layer-two solution does not require a dedicated public ledger, meaning that such a solution can smoothly</p><p>\\\nintegrate with existing public blockchain platforms. Unfortunately, this method also brings security and functionality challenges when delegating the task of contract management to an external TEE layer.</p><p>\\\nFirstly, the layer-two solution complexifies contract data management. The contracts that are deployed outside the blockchain require an external execution/storage party. A malicious storage maintainer may reject to provide the service, while a malicious host may abort TEE executions, terminate enclaves or delay/drop messages. Even an honest host might accidentally lose states in a power cycle. To solve the centralization issue and tolerate host failures, many countermeasures such as the TEE network, stateless TEEs and punishment mechanisms, are proposed. However, these solutions are not effortless, inevitably making the system complicated and hard to implement in practice.</p><p>\\\nSecondly, the layer-two solution increases the attack surface and thus becomes vulnerable to rollback attacks. There is a high probability that an adversary node can revert transactions where temporary forks, representing inconsistent blockchain views, are allowed in blockchain systems with probabilistic consensus (e.g., PoW). Since TEEs provide no guarantee on verification of input data; they cannot distinguish whether an input state is fresh or not. An attacker may offer stale states to resume a TEE’s execution. This enables rollback attacks against randomized TEEs programs. Even worse, plugging up these loopholes needs much effort.</p><p>(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;</p><p>(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;</p><p>(3) Qi Wang, Southern University of Science and Technology, China;</p><p>(4) David Galindo, University of Birmingham, United Kingdom;</p><p>(5) Mark Ryan, University of Birmingham, United Kingdom.</p>","contentLength":16340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Call for volunteers: Help us with the GNU Press shop","url":"http://www.fsf.org/blogs/community/cfv-2025-08","date":1751402700,"author":"","guid":179265,"unread":true,"content":"<article>People around the world are eagerly waiting to receive their GNU Press \nshop orders, and we need a little help sending everything out. Would \nyou be willing to donate a little of your time to support the FSF's \nwork while chatting and enjoying snacks with other free software \nsupporters? </article>","contentLength":289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Layer-One Confidential Smart Contracts: Architecture, Threats, and Tradeoffs","url":"https://hackernoon.com/layer-one-confidential-smart-contracts-architecture-threats-and-tradeoffs?source=rss","date":1751402695,"author":"Blockchainize Any Technology","guid":179351,"unread":true,"content":"<p>The layer-one approach enables blockchain nodes to run contracts in their isolated areas, as well as conducting the consensus (see Fig. 3). This approach combines the consensus procedure and state execution, either in terms of logically or physically. The reason why we call this method layer-one is that all executions are completed in the same layer of the blockchain network. The key to such an approach is to equip every blockchain node with a TEE. Indeed, this requires more integration efforts, but also comes with several advantages. The smart contract can implement stateful functionalities that receive arguments and update states instantly. In particular, a smart contract can directly access the ledger data stored in a local disk, greatly saving time often wasted in the interactive network communications.</p><p>\\\n In a layer-one execution model, the operation of ledger update (consensus) and state transition (contract execution) are coupled. Like Ethereum [2], smart contracts run inside blockchain nodes. Assume that a user plans to use the private contract; she only needs to upload data to the blockchain service and wait for results. The remaining procedures are completed by TEE-assisted distributed nodes. A TEE in these nodes acts as a black box for data processing and output targeted results without the data leakage. This approach greatly improves convenience for users due to its easy access and management. As illustrated in Fig.3, a generic data flow goes as follows: A contract creator deploys the code into blockchain. Then, a user sends the transaction with an encrypted argument to an arbitrary blockchain node. Her request is confidentially executed inside TEEs in this node and output encrypted state. Then, the consensus algorithm in this node broadcasts the encrypted results to peers. After the encrypted results are confirmed by other blockchain nodes, users fetch on-chain results and decrypt them for the plaintext.</p><p>\\\n<strong>Privacy-preserving property.</strong> This property indicates that contract states and the procedure of contract executions are hidden from the public. To achieve privacy, layer-one systems execute these confidential contracts inside TEEs in every distributed node. CCF [45], Fabric [60] and CONFIDE [37] follow this straightforward design where confidential contracts are loaded to the TEE of each consensus node, which encrypts both the inputs and outputs of contract states, together with their operating logic and predefined rules. Enigma[1] [61] introduces the secret network and allows users to submit their transactions together with encrypted data to miners. We also notice that current layer-one solutions only focus on internal procedures rather than the linkability and anonymity of addresses and transactions. This indicates that confidential smart contracts only protect the contents that have been loaded into TEEs, while the data that relates to external users is out of the scope of this work.</p><p>\\\n<strong>Blockchain intrinsic feature.</strong> The layer-one systems inherit most of the features empowered by blockchain. More precisely, the properties of <em>code immutability, high availability, explicit invocation, decentralized execution, automatic execution</em> and  remain the same because basic contract executions still rely on their underlying blockchain systems. Also, the property of (confidential)  in Enigma [61], CCF [45] and Fabric [60] remains unchanged. The states and executions from these systems follow the procedures of online consensus processes. Then, the returned results from inside TEEs still require to be confirmed on-chain. This makes their actions effectively perform the same functions as a normal smart contract, except for that the contents of states are transmitted from plaintext to ciphertext. In contrast, the property of <em>contract interoperability</em> is lost since the contracts are executed in isolated TEEs. This isolation requires additional communications such as dispatching keys through the remote attestation service, bringing much complexity.</p><p>\\\nThe layer-one solution encapsulates TEE computations into blockchain nodes. Every node in the network has to take responsibility for conducting confidential executions and performing the consensus. The design to coordinate TEEs and consensus within the same physical space brings many distinguished features. We start the analysis from their threat model and then dive into each component of these systems.</p><p>\\\n Users in the layer-one approach are assumed to be unreliable. They may have mistakes unconsciously, like dropping messages or mis-sending transactions. Even worse, a malicious user can arbitrarily behave like faking messages, identities, or compromising other nodes. As for TEE hosts, an external attacker can monitor, eavesdrop or even compromise part of involved TEE hosts among these distributed nodes, but cannot block all traffic transmitted in communication channels. Subsequently, a TEE is supposed to work in a good condition: The attestation service is trusted, and the cryptographic primitives used inside TEEs are secure. Meanwhile, as for the blockchain network, the basic systems (ledgers) are assumed to be robust [62], [63], [64]. When running the consensus, the majority (might be two-third, depends on specific consensus algorithms) of nodes are assumed to be honest [65]. Also, forging smart contract codes or states will happen in honest blockchain nodes with a negligible possibility. Based on that, we analyse securities from four aspects.</p><p>\\\n Firstly, we focus on the security of TEE hosts, or equally, individual nodes that run TEEs. Unlike classical blockchain systems, there are no explicit incentive or punishment mechanisms in this solution. This is easy to understand: A node with malicious behaviors will be instantly moved out of the committee and replaced by a new honest participant. Meanwhile, due to the fact that CCF [45] and Enigma [61] rely on Tendermint (a BFT variant) consensus algorithm, they can tolerate at most one-third of TEE Byzantine nodes. But the sacrifice is the increased difficulty in synchronization, especially when every node has to establish a secure channel for communications of distributed TEEs. In layer-one systems, host authentication is necessary. The node who wants to join the committee has to obtain permission from communities by proving her TEE capability. For instance, CONFIDE [37] builds a mutual authenticated protocol (MAP) (supported by SGX remote attestation techniques [66]) among blockchain nodes. Any nodes joining in the network have to pass the authentication via MAP.</p><p>\\\n Then, we analyse TEE-level securities. Attestation service is an essential part of TEE techniques. Systems in the layer-one solution still require such services for network connection and verification. Enigma [61], Fabric [60] and CCF [45] follow the original attestation mechanism with an implicit rule: The Intel Attestation Service (IAS) should be reliable. However, this cannot be guaranteed in the case of IAS being comprised. In contrast, CONFIDE [37] utilizes a customized Decentralized Attestation Service to provide the robust authentication. As for memory limitations, layer-one systems load contract executions and consensus algorithms into one TEE-embedded node, causing an increase in disk and memory usage of individual nodes. Once the usage of TEE memory runs over the predefined settings, a decrease in the performance is inevitable [34]. This may further cause an unpredictably severe result like system crash-down. Fortunately, Fabric [60] mitigates such issues by separating the operations into two types (execution and ordering) and delays the transaction- procedures after state-. Among them, only the state- parts are processed inside TEEs. This decreases computation complexity and limits the memory usage to a suitable range. Physical attacks like the Spectre and Meltdown vulnerabilities [57] are intrinsic design pitfalls that may occur inside the TEE kernel. To our knowledge, no layerone solutions mention them or provide the remedies.</p><p>\\\nTEE program security. Next, we focus on the program-level security. Issues like overburdening may frequently happen, especially when a malicious developer deploys a contract with infinite loop logic. Unlike using the gas mechanism in Ethereum [2], systems in the layer-one model constrain their running programs by the  mechanism. It sets a threshold, namely, a suitable range of time that allows processing contract operations. When exceeding the timebound, the system will abort under-processing states and restart a new round. As for the flaw detection, no formal techniques or verification tools, based on our observation, have been applied to layer-one systems. This gap needs further exploration. Similar to the previous discussion, the properties of data verification (covering both user data authenticity and blockchain data confirmation) and output conflicts are guaranteed by their underlying consensus algorithms. Each time performing the consensus, these properties are automatically checked. For instance, Enigma [61] relies on trusted validators, who equip with TEEs to conduct the verification procedure. Such validators maintain both the privacy of executions inside TEEs and the consistency of states that connects to peers. Once conflicts occur, validators will quickly make decisions on a block and remove another conflicting block. Fabric [60] performs such a process inside TEEs among committee nodes and then submits the passed results to its abstract ordering service. This service prevents forks caused by conflicting states, as well as proving a fact that: All executed messages are valid and integral once reaching the consensus agreement. It should be noted that, successful consensus procedures can merely guarantee the integrity of transactions and states, rather than linkability and authenticity that relates to physical entities.</p><p>\\\n Lastly, we move to the aspect of TEE key management. In layer-one systems, the key management service takes over the task of creating and managing keys for activities like attestation, verification, encryption, etc. To achieve the key management service among distributed nodes, several types of designs have been proposed. CCF [45] relies on the public key infrastructure (PKI) for certificate issuance, management, and revocation. It creates key pairs and dispatches them to every participated TEE, where each TEE holder is authenticated by the certificate. Similarly, Fabric [60] adopts an admin peer to provision the specific decryption key to  during bootstrapping. Enigma [61] setups an independent key management component to reply to the requests for encryption. Such designs help to simplify complex management procedures, as well as providing distinguishable keys for each TEE. However, these independent key management services lead to centralization even they are maintained by a group of nodes in the committee. CONFIDE [37] mitigates this issue by proposing a decentralized key management protocol. Two types of keys are involved in this protocol: the  used to decrypt confidential transactions from clients and the  used for state encryption/decryption between the confidential engine and storage service.</p><p>\\\nThe layer-one solution provides a highly integrated approach towards confidential smart contracts.</p><p>\\\nThe layer-one solution provides a consistent interface for users without changing the customer’s habits transformed from non-TEE blockchain systems. A user can use the layer-one system by directly interacting with the blockchain interface, without considering cumbersome and complicated operations between the TEE and blockchain. However, the layer-one solution still confronts several common disadvantages.</p><p>\\\nMinimizing the size of Trusted Computing Base (TCB) contributes to the TEE security [67]. In particular, a small TCB has fewer errors and can reduce attack surfaces. However, complicated interactive operations for contract execution and consensus agreement in the L1 solution greatly increase the size of TCB. Meanwhile, TEE products have limited secure memory. For example, in the current implementation of Intel SGX [35], the enclave page caches are constrained to 128 MB, and only 93 MB of those is available for applications, which limits the concurrent execution.</p><p>\\\nFurthermore, the layer-one solution lacks compatibility, which means being incompatible with existing blockchain systems. The solution integrates the consensus procedure and the contract execution into the same blockchain node, requiring every node having to equip a TEE hardware. Nevertheless, this requirement is difficult to be fulfilled in a public blockchain while already in use (e.g., Ethereum [2]).</p><p>(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;</p><p>(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;</p><p>(3) Qi Wang, Southern University of Science and Technology, China;</p><p>(4) David Galindo, University of Birmingham, United Kingdom;</p><p>(5) Mark Ryan, University of Birmingham, United Kingdom.</p><p>[1] Enigma’s secret network consists of a list of secret nodes equipped with TEE, which is categorised as a layer-one solution in the context of our definition (Sec.III-A). We also note that such a secret network can be regarded as a layer-two solution in the traditional classifications in terms of Ethereum, namely, either on-Ethereum chain (L1) or off-Ethereum chain (L2).</p>","contentLength":13517,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Most Blockchain Devs Get Wrong About TEE Security and Smart Contract Privacy","url":"https://hackernoon.com/what-most-blockchain-devs-get-wrong-about-tee-security-and-smart-contract-privacy?source=rss","date":1751402688,"author":"Blockchainize Any Technology","guid":179350,"unread":true,"content":"<h2>III. SYSTEMATIZATION METHODOLOGY</h2><p>To find common aspects (e.g., offered functionality, design model, adversary model), we extract recurring design patterns from publicly available publications and projects, focusing on systematization and evaluation of desirable properties (the main target of TCSC) and potential pitfalls of underlying systems. Our systematization methodology follows the idea in [52]: classification and evaluation. We firstly make a classification for the current systems and then define a framework to evaluate them. Details are presented as below.</p><p>\\\n</p><p>\\\nWe classify the existing systems into two main categories: layer-one solution (L1) and layer-two solution (L2). The layer-one solution executes the contract inside a TEE in the blockchain, requiring every blockchain node to equip a TEE. Instead, the layer-two solution decouples contract computations from the blockchain. It performs most of the smart contract computations off-chain. For a clear understanding, we make a comparison of the original blockchain (e.g., Ethereum), L1 solution, L2 solution. As in Tab.II, Ethereum runs smart contracts (in EVM) and consensus procedures in the same machine of distributed nodes. All the contract and transaction operations are publicly verifiable due to their total transparency. The layer-one solution performs such operations (contract execution and consensus) in the same machine, but contract operations are separate from consensus procedures. In contrast, the layer-two solution makes both of them operate independently. Contracts are executed outside the blockchain network, while the consensus happens inside each node.</p><p>\\\nIdeally, moving smart contract executions into TEEs brings additional privacy as well as maintaining the original benefits of blockchain systems. Therefore, we have identified the desirable properties in two main categories: <em>privacy-preserving property</em> and .</p><p>\\\n<strong>Privacy-preserving property.</strong> The property of confidentiality is the most distinguished feature in TCSC.</p><p>\\\n<em>A1. Specification hidden.</em> The source code of a smart contract is hidden during the deployment and the subsequent synchronization and execution.</p><p>\\\n The inputs fed into a confidential smart contract are hidden from the public.</p><p>\\\n The outputs returned from a confidential smart contract should be kept private.</p><p>\\\n The execution procedure is hidden from unauthorized parties. An adversary cannot learn the operation knowledge inside a TEE.</p><p>\\\n<em>A5. Address unlinkability.</em> The address pseudonymity does not entail strong privacy guarantees [53], [54]. This property prevents an adversary to learn the address linkability by observing users’ activities.</p><p>\\\n The contract caller’s identity (a user who invokes a smart contract) is hidden from an anonymity set [24] (see Appendix B).</p><p>\\\n<strong>Blockchain intrinsic feature.</strong> TEE-assisted smart contracts inherit features given by original blockchain systems. We summarize these features as follows.</p><p>\\\n Once a contract is successfully deployed, its source code cannot be altered.</p><p>\\\n<em>A8. (Confidential) state consistency.</em> Executions happening at a certain blockchain height will output the same result across different nodes.</p><p>\\\n<em>A9. Contract interoperability.</em> A smart contract can call another contract and be called by others.</p><p>\\\n A smart contract is continuously accessible without the single point of failure.</p><p>\\\n<em>A11. Decentralized execution.</em> A smart contract runs over the decentralized network.</p><p>\\\n<em>A12. Automatic execution.</em> A smart contract can be automatically executed once conditions are satisfied.</p><p>\\\n Operations running on the smart contract will be charged with gas fees [2].</p><p>\\\n<em>A14. Explicit invocation.</em> Each invocation will be formatted as a transaction and stored on blockchain.</p><p>\\\n<em>A15. Public verifiability.</em> The procedure of contract execution and result are publicly verifiable.</p><p>\\\n<em>A16. Consensus verifiability.</em> The consensus procedure on the (confidential) state is publicly verifiable.</p><p>\\\nEssentially, all TCSC systems share the same principle: a <em>TEE will handle the data from users.</em> After that, encrypted data flows from the  The <em>TEE plays a crucial role.</em> Thus, this part defines a framework for evaluating underlying blockchain systems from four aspects: , and  This framework aims to identify potential design flaws and pitfalls based on the threat model and data workflow.</p><p>\\\n Our threat model mainly captures three types of attackers, which are stated as follows.</p><p>\\\n<em>T1. User adversary (active/passive).</em> An attacker may control network between users and TEE host nodes.</p><p>\\\n<em>T2. TEE adversary (active/passive).</em> An adversary may control TEE hosts or control the network between TEE and blockchain platforms.</p><p>\\\n<em>T3. Blockchain adversary (active/passive).</em> An adversary may drop, modify and delay the blockchain messages. But the majority (or two-thirds) of the blockchain nodes are assumed to be honest.</p><p>\\\nNote that adversaries are not necessarily exclusive. In some cases, adversaries in different types may collude.</p><p>\\\n This section defines four metrics regarding system security according to the data workflow: approaches to enhance the security of a TEE host, countermeasures to mitigate intrinsic TEE issues, methods to prevent program flaws or bugs inside TEEs, and solutions to clear up the TEE key security dilemma.</p><p>\\\n A TEE and its interaction with the external environment (e.g., with users or the blockchain) are operated and controlled by a host (such as a L1 blockchain node). A malicious host has abilities to abort the executions of a TEE, delay and modify inputs, or even drop any ingoing or outgoing messages. The following metrics discuss the approaches to improve the TEE host’s security.</p><p>\\\n<em>P1. Host punishment mechanism.</em> Penalty mechanisms to reduce the risk of doing evil by a TEE host.</p><p>\\\n<em>P2. Host incentive mechanism.</em> Incentive mechanisms to promote a TEE host to behave honestly.</p><p>\\\n<em>P3. Host fault tolerance.</em> Solutions to make systems continually operate despite malfunctions or failures.</p><p>\\\n Methods to check the identity and the capability of a TEE host.</p><p>\\\n A TEE has inevitable weaknesses. For example, a TEE is vulnerable to side-channel attacks [55], [56]. These innate weaknesses directly pose severe challenges to the design and implementation of TEE-assisted contract systems. This part defines the defence approaches against these threats.</p><p>\\\n<em>P5. TEE attestation security.</em> Methods to prevent TEE attestation service from being abnormally broken.</p><p>\\\n<em>P6. TEE memory limitation.</em> Methods to optimize the memory size to prevent confidential data overflow.</p><p>\\\n<em>P7. TEE physical attacks.</em> Approaches to prevent physical attacks, such as the Spectre vulnerability or the Meltdown vulnerability [57].</p><p>\\\n Approaches to provide a trusted timer when running a TEE.</p><p>\\\n Even a TEE is secure as assumed, a program bug may destroy the contract’s confidentiality in the real world. This part focuses on the measurements to prevent TEE programs from flaws or bugs.</p><p>\\\n<em>P9. Workload measurement.</em> The workload measurement approach to prevent an infinite loop attack.</p><p>\\\n Formal techniques used for the modelling and verification of the source code of smart contracts to reduce the vulnerabilities.</p><p>\\\n<em>P11. User query restriction.</em> The restriction on users’ queries, aiming to avoid data leakage resulting from differentialprivacy analysis [58].</p><p>\\\n<em>P12. Blockchain data confirmation.</em> Methods for a TEE to check whether input data from blockchain has been confirmed to prevent the rollback attack [59].</p><p>\\\n<em>P13. TEE output conflicts.</em> Methods to avoid multiple TEEs to produce a conflict result.</p><p>\\\n Various keys (cf. Appendix A) are involved in the contract execution, including TEE internal keys such as the attestation key and TEE service keys for state encryption/decryption. Since service keys directly affect the protection of contract states, the key security evaluation in this SoK mainly focuses on the generation, exchange, and storage of the TEE service key.</p><p>\\\n<em>P14. Distributed key protocol.</em> The keys of confidential contracts are managed by a distributed protocol.</p><p>\\\n<em>P15. Key rotation protocol.</em> The TEE replaces an old key with a fresh key for future contract encryption.</p><p>\\\n<em>P16. Independent contract key.</em> Each contract is associated with a unique key, independent from the TEE.</p><p>\\\n<em>P17. Independent TEE key.</em> Each TEE has a unique key, and different contracts share the same key.</p><p>\\\n The  shows a general view of the TCSC systems. Desirable property focuses on evaluating contract service provided by a TEE-assisted blockchain system.  describes the potential threats and system assumptions.  show the evaluating indicator for current TEE-assisted systems. In the following section IV-B and V-B, we attempt to answer the following questions: (i) What are the potential pitfalls in each security aspect; (ii) Do these pitfalls have significant security impacts; (iii) Do the designers/developers consider these pitfalls and accordingly come up with feasible remedies in their systems; (iv) What are the remedies and do they address above problems. Note that hundreds of TCSC systems have been proposed in both industry and academia. An exhaustive analysis is undesirable and infeasible. We only selected the projects that provide publicly accessible technical reports or academic papers.</p><p>(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;</p><p>(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;</p><p>(3) Qi Wang, Southern University of Science and Technology, China;</p><p>(4) David Galindo, University of Birmingham, United Kingdom;</p><p>(5) Mark Ryan, University of Birmingham, United Kingdom.</p>","contentLength":9618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Four Key Steps to Confidential Smart Contract Execution","url":"https://hackernoon.com/four-key-steps-to-confidential-smart-contract-execution?source=rss","date":1751402681,"author":"Blockchainize Any Technology","guid":179349,"unread":true,"content":"<p>This section gives a high-level description and offers a running example to illustrate how a typical confidential smart contract operates. From existing literature [42], [43], [41], [44], [45], [46], establishing a confidential smart contract mainly requires four steps, namely , ,  and  (see Fig. 1).</p><p>\\\nFrom a bird’s eye view, a TCSC can be used as an ideal contract-based black box [47] with secrecy and correctness. This idea has been adopted by several advanced security protocols [48], [49]. We provide a secret e-voting example borrowed from Oasislabs [50].</p><p>\\\nA TCSC can be well qualified for the role of decentralized vote manager in an e-voting system [17], [51]. Once a contract-based manager is deployed successfully, the voting logic is loaded into a TEE and corresponding secret keys are privately generated and stored inside TEEs. The encrypted state is then confirmed by the blockchain nodes. This offers the e-voting protocol with confidentiality, neutrality, auditability and accountability. Firstly, the voter’s input cu is encrypted, and intermediate parameters (e.g., mb) are privately processed through TEEs. External attackers cannot obtain the knowledge of sensitive information, and thus the confidentiality is achieved. Secondly, the predefined voting logic only occurs in the decentralized network when certain conditions are satisfied, bringing neutrality for the access control management. Thirdly, if a voter wants to vote for a candidate, she needs to in advance build a channel to the TEE and then send a transaction Tx to call the contract. Due to the protection of encrypted channels, transaction arguments are kept secret. Meanwhile, such invoking records in the form of transactions remain visible and will become immutable, ensuring the voting process accountable. Unfortunately, verifiability, as one of fundamental properties, performs not smooth in the context of encryption. Contracts that are executed inside TEEs make the execution procedures lack public verifiability. Only the nodes who install TEEs with correct corresponding keys can verify the correctness of contract executions. However, the metadata of the transaction Tx retains unencrypted, making it possible to verify the absence of double spending.</p><p>(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;</p><p>(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;</p><p>(3) Qi Wang, Southern University of Science and Technology, China;</p><p>(4) David Galindo, University of Birmingham, United Kingdom;</p><p>(5) Mark Ryan, University of Birmingham, United Kingdom.</p>","contentLength":2680,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To Fix Smart Contracts, Start With Their Secrets","url":"https://hackernoon.com/to-fix-smart-contracts-start-with-their-secrets?source=rss","date":1751402674,"author":"Blockchainize Any Technology","guid":179348,"unread":true,"content":"<p>(1) Rujia Li, Southern University of Science and Technology, China, University of Birmingham, United Kingdom and this author contributed equally to this work;</p><p>(2) Qin Wang, CSIRO Data61, Australia and this author contributed equally to this work;</p><p>(3) Qi Wang, Southern University of Science and Technology, China;</p><p>(4) David Galindo, University of Birmingham, United Kingdom;</p><p>(5) Mark Ryan, University of Birmingham, United Kingdom.</p><p>\\\n<strong>—The blockchain-based smart contract lacks privacy since the contract state and instruction code are exposed to the public. Combining smart-contract execution with Trusted Execution Environments (TEEs) provides an efficient solution, called TEE-assisted smart contracts, for protecting the confidentiality of contract states. However, the combination approaches are varied, and a systematic study is absent. Newly released systems may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying TEE technology in insecure ways. In this paper, we first investigate and categorize the existing systems into two types: the layer-one solution and layer-two solution. Then, we establish an analysis framework to capture their common lights, covering the desired properties (for contract services), threat models, and security considerations (for underlying systems). Based on our taxonomy, we identify their ideal functionalities, and uncover the fundamental flaws and reason for the challenges in each specification’s design. We believe that this work would provide a guide for the development of TEE-assisted smart contracts, as well as a framework to evaluate future TEE-assisted confidential contract systems.</strong></p><p>Smart contract was originally introduced by Szabo [1] and further developed by Ethereum [2] in the blockchain systems. The blockchain-based smart contracts [3], [4], [5] adopt Turing-complete scripting languages to achieve complicated functionalities [6] and execute the predefined logic through state transition replication over consensus algorithms to realize final consistency. Smart contracts enable unfamiliar and distributed participants to fairly exchange without trusted third parties, and are further used to establish a uniform approach for developing decentralized applications (DApps [7]). However, blockchain-based smart contract lacks confidentiality. The state information and the instruction code are completely transparent [8], [9], [10]. Any states with their changes are publicly accessible and all users’ transaction data and contract variables are visible to external observers. Without privacy, building advanced DApps that rely on the user’s sensitive data becomes a challenge [11], [12], [13], [14]. For instance, smart contracts in Ethereum [2] cannot be directly applied to Vickrey auction [15], [16] or e-voting systems [17], [18], where the bid and vote require to be hidden from the public. Moreover, DApps without privacy protection might be prohibited by European Union because they go against the General Data Protection Regulation [19], [20]. Thus, the complete transparency of smart contracts constrains their wide adoption. Recently, researchers have explored many cryptographic solutions to solve these issues, including utilizing techniques of zero-knowledge proof (ZKP) [21], [22], [12], [23], [24], [25], homomorphic encryption (HE) [26] and secure multiparty computation (MPC) [27]. However, these approaches are merely applicable to applications requiring simple computations.</p><p>\\\nAlthough various TCSC protocols have been proposed, newly released projects may fail to draw upon the experience learned from existing protocols, such as repeating known design mistakes or applying cryptography in insecure ways. For example, an absence of economic incentives will pose security risks and decrease the protocol’s stability. However, the recentproposed TCSC scheme Hybridchain [41] repeats similar pitfalls by simply combining the TEE with a permissioned blockchain network, omitting considerations on the miner’s incentive mechanism. The repeating of pitfalls comes from twofold. Firstly, in-the-wild projects differ from one to another, and a relatively unique model is absent, which narrows the developers’ vision. Meanwhile, a unified evaluation framework is missing, causing many security threats to be uncovered and resulting in considerable loss from applications underpinning the execution of confidential smart contracts. This paper aims to abstract a high-level framework to simply and clearly systematize knowledge on current TCSC schemes. We attempt to capture some commonalities among these projects regarding their features, properties, and potential security vulnerability. We believe that establishing evaluation criteria to measure features and identify problems and flaws of existing TCSC protocols will offer a good guide for industry communities and promote the DApps prosperity. Main contributions (a visualized guideline in Fig.2) are:</p><p>\\\n• We provide a systematization of existing TCSC systems driven from academic work and  Based on their operating mechanisms and ways of combination, we investigate and categorize a set of typical protocols into two main classifications: the  solution and the  solution.</p><p>\\\n• We establish a unified evaluation framework for confidential smart contract systems. We consider two parts: the smart contracts used as , and underlying supported blockchain systems. Accordingly, the framework covers three aspects:  for contract services,  and  for underlying systems. Specifically, we discuss two different types of desirable properties:  that inherit from traditional smart contracts and featured <em>privacy-related properties.</em> Then, we emphasize practical issues, pitfalls, and remedies in designing TEE-assisted blockchains from four aspects ( securities and  services).</p><p>\\\n• We conduct a comparative analysis of existing protocols based on our evaluation framework. We discuss systems both from their  (system classification, threat model) and  (designs, properties). The common designs show us the consistent idea when re-designing the system, while the distinguished features highlight the ingenuity of each system design that deviates from others (see Tab.III/Tab.IV).</p><p>\\\n• We further give a comprehensive discussion of current designs and implementations, including a running example, comparisons between layer-one and layer-two systems from the perspectives of ,  and , and common issues on . Unfortunately, a mature design is still not ready for large-scale applications. We thereby point out research  in this field, wishing to give insights for communities on defining their models and discovering possible solutions of designing TCSC systems.</p><p>\\\nThe rest of the paper is organized as follows. Sec.II gives a high-level introduction on how to operate a confidential smart contract inside TEEs. Sec.III provides the systematization methodology ( and ). Layer-one and layer-two systems are analysed in Sec.IV and Sec.V, respectively. Discussions are provided in Sec.VI. Research challenges are summarised in Sec.VII. Finally, Sec.VIII gives concluding remarks. Supplementary details are stated in Appendix A-D.</p>","contentLength":7211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Use Amazon SageMaker Unified Studio to build complex AI workflows using Amazon Bedrock Flows","url":"https://aws.amazon.com/blogs/machine-learning/use-amazon-sagemaker-unified-studio-to-build-complex-ai-workflows-using-amazon-bedrock-flows/","date":1751402548,"author":"Sumeet Tripathi","guid":179261,"unread":true,"content":"<p>Organizations face the challenge to manage data, multiple artificial intelligence and machine learning (AI/ML) tools, and workflows across different environments, impacting productivity and governance. A unified development environment consolidates data processing, model development, and AI application deployment into a single system. This integration streamlines workflows, enhances collaboration, and accelerates AI solution development from concept to production.</p><p>The next generation of <a href=\"https://aws.amazon.com/sagemaker/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker</a> is the center for your data, analytics, and AI. SageMaker brings together AWS AI/ML and analytics capabilities and delivers an integrated experience for analytics and AI with unified access to data. <a href=\"https://aws.amazon.com/sagemaker/unified-studio/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker Unified Studio</a> is a single data and AI development environment where you can find and access your data and act on it using AWS analytics and AI/ML services, for SQL analytics, data processing, model development, and generative AI application development.</p><p>In this post, we demonstrate how you can use SageMaker Unified Studio to create complex AI workflows using Amazon Bedrock Flows.</p><p>Consider FinAssist Corp, a leading financial institution developing a generative AI-powered agent support application. The solution offers the following key features:</p><ul><li><strong>Complaint reference system</strong> – An AI-powered system providing quick access to historical complaint data, enabling customer service representatives to efficiently handle customer follow-ups, support internal audits, and aid in training new staff.</li><li><strong>Intelligent knowledge base</strong> – A comprehensive data source of resolved complaints that quickly retrieves relevant complaint details, resolution actions, and outcome summaries.</li><li><strong>Streamlined workflow management</strong> – Enhanced consistency in customer communications through standardized access to past case information, supporting compliance checks and process improvement initiatives.</li><li><strong>Flexible query capability</strong> – A straightforward interface supporting various query scenarios, from customer inquiries about past resolutions to internal reviews of complaint handling procedures.</li></ul><p>Let’s explore how SageMaker Unified Studio and Amazon Bedrock Flows, integrated with Amazon Bedrock Knowledge Bases and Amazon Bedrock Agents, address these challenges by creating an AI-powered complaint reference system. The following diagram illustrates the solution architecture.</p><p>The solution uses the following key components:</p><ul><li>– Provides the development environment</li><li> – Orchestrates the workflow, including: \n  <ul><li>Prompt-based classification</li><li>Agent-based response generation</li></ul></li></ul><p>The workflow processes user queries through the following steps:</p><ol><li>A user submits a complaint-related question.</li><li>The knowledge base provides relevant complaint information.</li><li>The prompt classifies if the query is about resolution timing.</li><li>Based on the classification using the condition, the application takes the following action: \n  <ol type=\"a\"><li>Routes the query to an AI agent for specific resolution responses.</li><li>Returns general complaint information.</li></ol></li><li>The application generates an appropriate response for the user.</li></ol><p>For this example, you need the following:</p><ul><li>The IAM user or IAM Identity Center user must have appropriate permissions for: \n  <ul><li>SageMaker Unified Studio.</li><li>Amazon Bedrock (including Amazon Bedrock Flows, Amazon Bedrock Agents, Amazon Bedrock Prompt Management, and Amazon Bedrock Knowledge Bases).</li></ul></li><li><a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/amazon-bedrock.html#manage-models\">Configure access</a> to your Amazon Bedrock serverless models for Amazon Bedrock in SageMaker Unified Studio projects.</li><li>Amazon Titan Embedding (for the knowledge base).</li><li>Sample complaint data prepared in CSV format for creating the knowledge base.</li></ul><p>We have created a sample dataset to use for Amazon Bedrock Knowledge Bases. This dataset has information of complaints received by customer service representatives and resolution information.The following is an example from the sample dataset:</p><div><pre><code>complaint_id,product,sub_product,issue,sub_issue,complaint_summary,action_taken,next_steps,financial_institution,state,submitted_via,resolution_type,timely_response\nFIN-2024-001,04/26/24,\"Mortgage\",\"Conventional mortgage\",\"Payment issue\",\"Escrow dispute\",\"Customer disputes mortgage payment increase after recent escrow analysis\",\"Reviewed escrow analysis, explained property tax increase impact, provided detailed payment breakdown\",\"1. Send written explanation of escrow analysis 2. Schedule annual escrow review 3. Provide payment assistance options\",\"Financial Institution-1\",\"TX\",\"Web\",\"Closed with explanation\",\"Yes\"\nFIN-2024-002,04/26/24,\"Money transfer\",\"Wire transfer\",\"Processing delay\",\"International transfer\",\"Wire transfer of $10,000 delayed, customer concerned about international payment deadline\",\"Located wire transfer in system, expedited processing, waived wire fee\",\"1. Confirm receipt with receiving bank 2. Update customer on delivery 3. Document process improvement needs\",\"Financial Institution-2\",\"FL\",\"Phone\",\"Closed with monetary relief\",\"No\"</code></pre></div><p>In SageMaker Unified Studio, users can use projects to collaborate on various business use cases. Within projects, you can manage data assets in the SageMaker Unified Studio catalog, perform data analysis, organize workflows, develop ML models, build generative AI applications, and more.</p><p>To create a project, complete the following steps:</p><ol><li>Open the SageMaker Unified Studio landing page using the URL from your admin.</li><li>Enter a project name and optional description.</li><li>For , choose <strong>Generative AI application development</strong>.</li></ol><ol start=\"6\"><li>Complete your project configuration, then choose .</li></ol><p>Let’s create a reusable prompt to capture the instructions for FMs, which we will use later while creating the flow application. For more information, see <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/userguide/prompt-mgmt.html\" target=\"_blank\" rel=\"noopener noreferrer\">Reuse and share Amazon Bedrock prompts</a>.</p><ol><li>In SageMaker Unified Studio, on the  menu, choose  under <strong>Machine Learning &amp; Generative AI</strong>.</li></ol><ol start=\"2\"><li>Provide a name for the prompt.</li><li>Choose the appropriate FM (for this example, we choose ).</li><li>For , we enter the following:</li></ol><div><pre><code>You are a complaint analysis classifier. You will receive complaint data from a knowledge base. Analyze the {{input}} and respond with a single letter:\nT: If the input contains information about complaint resolution timing, response time, or processing timeline (whether timely or delayed)\nF: For all other types of complaint information\nReturn only 'T' or 'F' based on whether the knowledge base response is about resolution timing. Do not add any additional text or explanation - respond with just the single letter 'T' or 'F'.</code></pre></div><p>Let’s create a chat agent to handle specific resolution responses. Complete the following steps:</p><ol><li>In SageMaker Unified Studio, on the  menu, choose  under <strong>Machine Learning &amp; Generative AI</strong>.</li><li>Provide a name for the prompt.</li><li>Choose the appropriate FM (for this example, we choose ).</li><li>For , we enter the following:</li></ol><div><pre><code>You are a Financial Complaints Assistant AI. You will receive complaint information from a knowledge base and questions about resolution timing.\nWhen responding to resolution timing queries:\n1. Use the provided complaint information to confirm if it was resolved within timeline\n2. For timely resolutions, provide:\n   - Confirmation of timely completion\n   - Specific actions taken (from the provided complaint data)\n   - Next steps that were completed\n2. For delayed resolutions, provide:\n   - Acknowledgment of delay\n   - Standard compensation package:\n     • $75 service credit\n     • Priority Status upgrade for 6 months\n     • Service fees waived for current billing cycle\n   - Actions taken (from the provided complaint data)\n   - Contact information for follow-up: Priority Line: ************** \nAlways reference the specific complaint details provided in your input when discussing actions taken and resolution process.</code></pre></div><ol start=\"6\"><li>After the agent is saved, choose .</li><li>For , enter .</li></ol><p>Now that we have our prompt and agent ready, let’s create a flow that will orchestrate the complaint handling process:</p><ol><li>In SageMaker Unified Studio, on the  menu, choose  under <strong>Machine Learning &amp; Generative AI</strong>.</li></ol><ol start=\"2\"><li>Create a new flow called demo-flow.</li></ol><h3>Add a knowledge base to your flow application</h3><p>Complete the following steps to add a knowledge base node to the flow:</p><ol><li>In the navigation pane, on the tab, choose .</li><li>On the  tab, provide the following information: \n  <ol type=\"a\"><li>For , enter a name (for example, ).</li><li>Choose <strong>Create new Knowledge Base</strong>.</li></ol></li><li>In the pane, enter the following information: \n  <ol type=\"a\"><li>For , enter a name (for example, ).</li><li>For , enter a description (for example, <code>user complaints information</code>).</li><li>For , select  and upload the complaints.txt file.</li><li>For , choose .</li><li>For , choose .</li></ol></li></ol><ol start=\"4\"><li>After you create the knowledge base, choose it in the flow.</li><li>In the details name, provide the following information:</li><li>For <strong>Response generation model</strong>, choose .</li><li>Connect the output of the flow input node with the input of the knowledge base node.</li><li>Connect the output of the knowledge base node with the input of the flow output node.</li></ol><h3>Add a prompt to your flow application</h3><p>Now let’s add the prompt you created earlier to the flow:</p><h3>Add a condition to your flow application</h3><p>The condition node determines how the flow handles different types of queries. It evaluates whether a query is about resolution timing or general complaint information, enabling the flow to route the query appropriately. When a query is about resolution timing, it will be directed to the chat agent for specialized handling; otherwise, it will receive a direct response from the knowledge base. Complete the following steps to add a condition:</p><h3>Add a chat agent to your flow application</h3><p>Now let’s add the chat agent you created earlier to the flow:</p><h2>Test the flow application</h2><p>Now that the flow application is ready, let’s test it. On the right side of the page, choose the expand icon to open the  pane.</p><p>In the  text box, we can ask a few questions related to the dataset created earlier. The following screenshots show some examples.</p><p>To clean up your resources, delete the flow, agent, prompt, knowledge base, and associated OpenSearch Serverless resources.</p><p>In this post, we demonstrated how to build an AI-powered complaint reference system using a flow application in SageMaker Unified Studio. By using the integrated capabilities of SageMaker Unified Studio with Amazon Bedrock features like Amazon Bedrock Knowledge Bases, Amazon Bedrock Agents, and Amazon Bedrock Flows, you can rapidly develop and deploy sophisticated AI applications without extensive coding.</p><p>As you build AI workflows using SageMaker Unified Studio, remember to adhere to the AWS <a href=\"https://aws.amazon.com/compliance/shared-responsibility-model/\" target=\"_blank\" rel=\"noopener noreferrer\">Shared Responsibility Model</a> for security. Implement SageMaker Unified Studio <a href=\"https://docs.aws.amazon.com/sagemaker-unified-studio/latest/adminguide/security.html\" target=\"_blank\" rel=\"noopener noreferrer\">security</a> best practices, including proper IAM configurations and data encryption. You can also refer to <a href=\"https://aws.amazon.com/blogs/machine-learning/secure-a-generative-ai-assistant-with-owasp-top-10-mitigation/\" target=\"_blank\" rel=\"noopener noreferrer\">Secure a generative AI assistant with OWASP Top 10 mitigation</a> for details on how to assess the security posture of a generative AI assistant using OWASP TOP 10 mitigations for common threats. Following these guidelines helps establish robust AI applications that maintain data integrity and system protection.</p><p>We look forward to seeing the innovative solutions you will create with these powerful new features.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/18/sumeettr-100x133.jpg\" alt=\"\" width=\"100\" height=\"133\"> is an Enterprise Support Lead (TAM) at AWS in North Carolina. He has over 17 years of experience in technology across various roles. He is passionate about helping customers to reduce operational challenges and friction. His focus area is AI/ML and Energy &amp; Utilities Segment. Outside work, He enjoys traveling with family, watching cricket and movies.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/18/Vsnak-100x133.jpg\" alt=\"\" width=\"100\" height=\"133\"> is a Sr. Solutions Architect at Amazon Web Services (AWS). He is a builder who enjoys helping customers accomplish their business needs and solve complex challenges with AWS solutions and best practices. His core area of focus includes Generative AI and Machine Learning. In his spare time, Vishal loves making short films on time travel and alternate universe themes.</p>","contentLength":11649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Road to Battlefield: Central Eurasia’s gateway to TechCrunch Startup Battlefield","url":"https://techcrunch.com/2025/07/01/road-to-battlefield-central-eurasias-gateway-to-techcrunch-startup-battlefield/","date":1751402497,"author":"Cindy Zackney","guid":179260,"unread":true,"content":"<article>Historic regional competition launches to showcase Central Eurasia’s rising startup ecosystem on Silicon Valley’s biggest stage. For the first time in its history, Central Eurasia will have a direct pathway to TechCrunch Startup Battlefield through the launch of “Road to Battlefield,” a groundbreaking regional competition that promises to put the underrepresented region firmly on the […]</article>","contentLength":400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tech Hobbyist Destroys 51 MicroSD Cards To Build Ultimate Performance Database","url":"https://it.slashdot.org/story/25/07/01/155208/tech-hobbyist-destroys-51-microsd-cards-to-build-ultimate-performance-database?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751402400,"author":"msmash","guid":179269,"unread":true,"content":"Tech enthusiast Matt Cole has created a comprehensive MicroSD card testing database, writing over 18 petabytes of data across nearly 200 cards since July 2023. Cole's \"Great MicroSD Card Survey\" uses eight machines running 70 card readers around the clock, writing 101 terabytes daily to test authenticity, performance, and endurance. \n\nThe 15,000-word report covering over 200 different cards reveals significant quality disparities. Name-brand cards purchased from Amazon performed markedly better than identical models from AliExpress, while cards with \"fake flash\" -- inflated capacity ratings -- performed significantly worse than authentic storage. Sandisk and Kingston cards averaged 4,634 and 3,555 read/write cycles before first error, respectively, while Lenovo cards averaged just 291 cycles. Some off-brand cards failed after only 27 cycles. Cole tested 51 cards to complete destruction during the endurance testing phase.","contentLength":934,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"You will own NOTHING and be HAPPY! (SKG)","url":"https://www.youtube.com/watch?v=rAsgjKBkKMA","date":1751402153,"author":"Jeff Geerling","guid":179272,"unread":true,"content":"<article>Please sign if you're in the EU or UK!\nEU: https://eci.ec.europa.eu/045/public/#/screen/home\nUK: https://petition.parliament.uk/petitions/702074/\n\nThings I mentioned in this video:\n\n  - Stop Killing Games: https://www.stopkillinggames.com\n  - My video takedown (and getting a 2nd appeal): https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful-updated\n  - Bosch Dishwasher video: https://www.youtube.com/watch?v=5M_hmwBBPnc\n  - XKCD on WiFi appliances: https://xkcd.com/3109/\n  - Dead game list: https://stopkillinggames.wiki.gg/wiki/Dead_game_list\n\nSupport me on Patreon: https://www.patreon.com/geerlingguy\nSponsor me on GitHub: https://github.com/sponsors/geerlingguy\nMerch: https://www.redshirtjeff.com\n2nd Channel: https://www.youtube.com/@GeerlingEngineering\n3rd Channel: https://www.youtube.com/@Level2Jeff\n\nContents:\n\n00:00 - Controlling your own destiny. And dishwasher.\n01:27 - Games (please stop killing them)\n03:35 - What you can do (in the EU)</article>","contentLength":987,"flags":null,"enclosureUrl":"https://www.youtube.com/v/rAsgjKBkKMA?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Roman Roads Research Association","url":"https://www.romanroads.org/","date":1751401972,"author":"bjourne","guid":180366,"unread":true,"content":"<p><a href=\"https://www.eventbrite.co.uk/e/road-construction-and-transport-infrastructure-in-the-roman-empire-registration-823562416357\" target=\"_blank\">Booking</a></p><p><a href=\"https://www.eventbrite.co.uk/e/788098332327?aff=oddtdtcreator\" target=\"_blank\">Booking</a></p><p><a href=\"https://youtu.be/DLc8lQvVcvM\" target=\"_blank\">https://youtu.be/DLc8lQvVcvM</a></p><p><a href=\"http://www.romanroads.org/gazetteer/cheshire/cheshire.html\" target=\"_blank\">Roman Roads in Cheshire</a></p>","contentLength":65,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437758"},{"title":"Platform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platfor... Puja Abbassi","url":"https://www.youtube.com/watch?v=Xr0Eb-ybvck","date":1751401964,"author":"CNCF [Cloud Native Computing Foundation]","guid":179271,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPlatform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platforms - Puja Abbassi, Giant Swarm\n\nEveryone is talking about platform engineering. You see smooth demos of golden paths and self-service platforms. However, there’s a significant area of challenges that is less talked about and thus often neglected when designing developer platforms.\n\nIn this talk, we’ll explore the often-overlooked day 2 challenges that platform teams face. We’ll dissect the area of day 2 into the many sub-areas and challenges they pose. Drawing on real-world experiences, including notable migrations that many in this community have faced, we'll shed light on the pain behind developer platforms and discuss solutions to these issues. Among others, we’ll delve into practical strategies for managing versioning and rollouts, and highlight the significant hurdles encountered, such as dependencies on end user teams or GitOps.\n\nJoin us for insights, strategies, and stories from the trenches that will help you navigate the complexities of service iteration in developer platforms.</article>","contentLength":1476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Xr0Eb-ybvck?version=3","enclosureMime":"","commentsUrl":null},{"title":"Fakespot shuts down today after 9 years of detecting fake product reviews","url":"https://blog.truestar.pro/fakespot-shuts-down/","date":1751401595,"author":"doppio19","guid":179507,"unread":true,"content":"<p>Today marks the end of an era. After nearly a decade of helping millions of shoppers navigate the murky waters of online reviews, <a href=\"https://blog.mozilla.org/en/mozilla/building-whats-next/?ref=blog.truestar.pro\">Fakespot has officially closed its doors</a>. If you tried to check a product listing this morning and found Fakespot not working, you're not alone. The service has permanently shut down.</p><p> Fakespot, the popular fake review detection tool acquired by Mozilla in 2023, shut down today, July 1, 2025. Founded by Saoud Khalifah in 2016, it helped millions identify unreliable Amazon reviews with 90% accuracy before Mozilla discontinued it due to sustainability challenges.</p><p>Back in 2016, Saoud Khalifah bought a product on Amazon, trusting the glowing reviews, only to discover he'd been duped by fake feedback. Instead of just leaving his own angry review, Khalifah took a more proactive approach: he built Fakespot.</p><p>What started as one person's frustration with deceptive sellers became a tool that analyzed millions of reviews across Amazon and other major retailers like eBay and Walmart. The premise was simple but powerful: use AI to spot patterns that human shoppers might miss, like suspiciously similar language or reviewer profiles that didn't quite add up.</p><h2>The magnitude of the deception</h2><p>Fakespot's technology revealed some eye-opening statistics. About 43% of the best-selling Amazon products had reviews that were unreliable or fabricated, according to a <a href=\"https://getcircuit.com/route-planner/blog/amazon-fake-review-analysis?ref=blog.truestar.pro\">study by app company Circuit</a>. The problem was even worse in certain categories. Clothing and jewelry led the pack with a staggering 88% of reviews deemed unreliable.</p><p>These numbers painted a sobering picture of the online shopping landscape. Most of us rely on product reviews as a major factor when deciding what to buy, but nearly half of the feedback you read might not be genuine.</p><p>Three years later, <a href=\"https://blog.mozilla.org/en/mozilla/fakespot-joins-mozilla-firefox-shopping-announcement/?ref=blog.truestar.pro\">Mozilla acquired Fakespot</a>, bringing the startup's 13-person team into the Firefox family. Mozilla integrated Fakespot's technology directly into Firefox as the \"Mozilla Review Checker\" feature, making it easier than ever for users to verify product reviews without installing separate extensions.</p><p>For many users, this felt like a perfect match. Mozilla's reputation for privacy and transparency aligned beautifully with Fakespot's mission to bring honesty to online shopping.</p><p>But as Mozilla announced in May, not all acquisitions fit into a sustainable long-term model. The company made the difficult decision to discontinue both Pocket and Fakespot as part of a strategic refocus on Firefox's core features and AI-powered innovations.</p><p>The reasons were practical, if devastating for users. A flood of reviews lamenting the closure have appeared on Fakespot's extension page on the Chrome Web Store:</p><p>Fakespot's mission resonated strongly with consumers, but <a href=\"https://www.distractify.com/p/why-is-fakespot-shutting-down?ref=blog.truestar.pro\">Mozilla couldn't find a sustainable model</a> to keep it running. Resources that once supported the service would now flow toward Firefox features like vertical tabs, smart search, and additional AI-powered features.</p><p>As we say goodbye to Fakespot, it's worth reflecting on what it accomplished. For nine years, it served as a defender against fraud in an increasingly deceptive marketplace. It gave shoppers a fighting chance against promotional reviewers and bot farms that undermine trust in online shopping.</p><p>For those of us who came to rely on Fakespot's review analysis before making purchases, its absence leaves us less confident in our buying decisions. The need for trustworthy review analysis hasn't gone away. If anything, it's more critical than ever.</p><p>I know I'm not alone in feeling this gap, which is why I've begun building a tool that aims to be the spiritual successor to Fakespot. <a href=\"https://truestar.pro/?ref=blog.truestar.pro\">TrueStar</a> will use modern AI, streamlined analysis techniques, and sustainable economics to keep costs manageable while maintaining the accuracy shoppers need.</p><div data-layout=\"minimal\"><div><div><a href=\"https://truestar.pro/?ref=blog.truestar.pro\">\n                            Get notified\n                        </a></div></div></div><h2>Quick answers about Fakespot's closure</h2><p><strong>When did Fakespot shut down?</strong>Fakespot officially closed on July 1, 2025, with the Mozilla Review Checker feature in Firefox having ended on June 10, 2025.</p><p><strong>Why did Fakespot shut down?</strong>Mozilla couldn't find a sustainable business model for Fakespot despite its popularity, choosing to redirect resources to core Firefox features and AI-powered browser tools.</p><p><strong>What happened to Fakespot?</strong>Mozilla acquired Fakespot in 2023 but announced in May 2025 that both Fakespot and Pocket would be discontinued as part of a strategic refocus on Firefox development.</p><p><strong>What are the best Fakespot alternatives?</strong>While several options exist including ReviewMeta, The Review Index, and emerging tools like <a href=\"https://truestar.pro/?ref=blog.truestar.pro\">TrueStar</a>, the market is still developing sustainable solutions that balance accuracy with affordability.</p><p>As Fakespot's servers go dark, let's raise a glass to the tool that made online shopping so much more trustworthy for nearly a decade. Thanks to Saoud Khalifah and his team for showing us what's possible when technology serves truth over profit.</p><p>Rest in peace, Fakespot. You fought the good fight. 🥂</p><p><em>If you found this article helpful, consider sharing it with others who might be wondering why their favorite review checker stopped working today. Let's keep the conversation about online authenticity going.</em></p>","contentLength":5181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437712"},{"title":"X is piloting a program that lets AI chatbots generate Community Notes","url":"https://techcrunch.com/2025/07/01/x-is-piloting-a-program-that-lets-ai-chatbots-generate-community-notes/","date":1751401572,"author":"Amanda Silberling","guid":179259,"unread":true,"content":"<article>The social platform X will pilot a feature that allows AI chatbots to generate Community Notes, a Twitter-era feature that Elon Musk has expanded under his ownership of the service now called X.</article>","contentLength":194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Access NASA’s Climate Data — And How It’s Powering the Fight Against Climate Change Pt. 1","url":"https://towardsdatascience.com/how-to-access-nasas-climate-data-and-how-its-powering-the-fight-against-climate-change-pt-1/","date":1751401226,"author":"Marco Hening Tallarico","guid":179302,"unread":true,"content":"<p>From architectural design to food security. </p>","contentLength":44,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trump Launches America’s Newest Concentration Camp, Complete With Tacky Merch","url":"https://www.techdirt.com/2025/07/01/trump-launches-americas-first-concentration-camp-complete-with-tacky-merch/","date":1751400407,"author":"Mike Masnick","guid":179267,"unread":true,"content":"<p>Not content with just <a href=\"https://www.techdirt.com/2025/05/02/trump-administration-violated-human-rights-law-by-paying-el-salvador-to-imprison-immigrants/\">shipping people</a> to a foreign concentration camp, Donald Trump now has his own, homegrown concentration camp in Florida. Trump, DHS Secretary Kristi Noem, and Florida Governor Ron DeSantis <a href=\"https://www.reuters.com/world/us/watch-live-trump-visits-alligator-alcatraz-detention-camp-2025-07-01/\">gleefully toured</a> the hastily constructed concentration camp in the Florida Everglades, obnoxiously referred to as Alligator Alcatraz, in reference to (1) the infamous island prison in San Francisco that Trump is obsessed with and (2) the number of alligators (and crocodiles — the one place in the world that has both) that live in and around the Everglades.</p><p>There’s no way to look at what the US government is doing here and not think of it more as Auschwitz than Alcatraz. The parallels are unmistakable: hastily constructed camps in remote locations, euphemistic naming designed to obscure their true purpose, and—most tellingly—officials proudly touring the facilities while discussing plans to build “a system” of such camps nationwide.</p><p>But here’s where today’s American concentration camps differ from their 20th-century predecessors: the Trump regime isn’t trying to hide what they’re doing. They’re merchandising it. They’re selling t-shirts celebrating human suffering as if it were a sports team or a vacation destination.</p><p>The United States government is literally selling branded merchandise to celebrate putting human beings in cages surrounded by dangerous predators. This isn’t just about policy—it’s about turning cruelty into a consumer product. It’s about making the suffering of others into something you can wear to own the libs.</p><p>This commodification of human rights violations represents something uniquely American and uniquely horrifying: the gamification of genocide. Previous authoritarian regimes at least had the decency to be ashamed of their concentration camps. Trump is selling tickets to the show.</p><p>These are the sorts of things that history books (should they exist in the future) will talk about as one of the many moments of pure evil that some people gleefully embraced without recognizing that people setting up concentration camps are, inherently, “the baddies.”</p><p>For what it’s worth, Trump did little to dispel the notion that this is part of his new fascist campaign to imprison anyone who disagrees with him. During the tour, Trump and Noem talked about prosecuting CNN for their reporting and for releasing an app that alerts people to where ICE agents are located (both of which would violate the First Amendment, if it were still a thing anyone believed in).</p><p>Trump admitted that he had brought up this idea as a joke, but his idiot advisors ran with it:</p><blockquote><p><em>“Is this a dream come true for you, sir” a reporter asks.</em></p><p><em>“It was meant more as a joke, but the more I thought of it, the more I liked it… they were actually crocodiles,” Trump said.</em></p></blockquote><blockquote><p><em>“We’d like to see them in many states. At some point, they might morph into a system,” Trump said on Tuesday.</em></p></blockquote><p>A “system.” The word choice isn’t accidental. This is the language of industrial-scale human rights violations, spoken with the same casual tone you’d use to discuss a chain restaurant expansion.</p><p>In case you’re wondering how much it costs to go full Nazi, this one concentration camp <a href=\"https://www.npr.org/2025/07/01/nx-s1-5453078/trump-alligator-alcatraz-visit-migrant-detention-center\">will cost the American taxpayer</a> nearly half a billion dollars a year. That money will come from FEMA, the organization that Trump (with an assist from former friend Elon Musk and DOGE) stripped budget from, meaning there will be even less to pay for actual emergencies, because all of that money will be used to jail people Trump doesn’t like in a swamp.</p><blockquote><p><em>The Everglades facility will cost Florida some $450 million to run for one year, according to DHS, though much of that will be reimbursed by the Federal Emergency Management Agency (FEMA). While the airstrip is owned by Miami-Dade County, where officials have viewed the plan with skepticism, DeSantis is using his emergency authority to proceed on a tight schedule.</em></p></blockquote><p>We are watching the latest march forward of American fascism in real time, complete with branded merchandise and gleeful photo ops. The US government is building concentration camps and selling t-shirts about it. This isn’t hyperbole. This isn’t partisan hysteria. This is what’s actually happening.</p><p>Every day you don’t call this what it is—fascism—you become complicit in normalizing it. Every time you treat this as just another political story, you help them make it routine. They’re counting on your exhaustion, your normalization, your willingness to look away.</p><p>The survivors of the Holocaust warned us this could happen again. They’re mostly gone now, but their warnings echo: it starts with camps, it starts with dehumanization, and it starts with good people doing nothing while evil wraps itself in flags and sells t-shirts.</p>","contentLength":4803,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AT&amp;T Now Lets Customers Lock Down Account To Prevent SIM Swapping Attacks","url":"https://tech.slashdot.org/story/25/07/01/181213/att-now-lets-customers-lock-down-account-to-prevent-sim-swapping-attacks?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751400000,"author":"msmash","guid":179268,"unread":true,"content":"AT&amp;T has launched a new Account Lock feature designed to protect customers from SIM swapping attacks. The security tool, available through the myAT&amp;T app, prevents unauthorized changes to customer accounts including phone number transfers, SIM card changes, billing information updates, device upgrades, and modifications to authorized users. \n\nSIM swapping attacks occur when criminals obtain a victim's phone number through social engineering techniques, then intercept messages and calls to access two-factor authentication codes for sensitive accounts. The attacks have become increasingly common in recent years. AT&amp;T began gradually rolling out Account Lock earlier this year, joining T-Mobile, Verizon, and Google Fi, which already offer similar fraud prevention features.","contentLength":779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Catalio Capital closes over $400M Fund IV","url":"https://techcrunch.com/2025/07/01/catalio-capital-closes-over-400m-fund-iv/","date":1751400000,"author":"Dominic-Madori Davis","guid":179220,"unread":true,"content":"<article>Catalio Capital Management announced the closing of its more than $400 million Fund IV. The fund will continue the firm’s thesis of backing healthcare and biotechnology companies.&nbsp;</article>","contentLength":183,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google’s data center energy use doubled in 4 years","url":"https://techcrunch.com/2025/07/01/googles-data-center-energy-use-doubled-in-four-years/","date":1751399559,"author":"Tim De Chant","guid":179219,"unread":true,"content":"<article>Google has pledged to use only carbon-free sources of electricity to power its operations, a task made more challenging by its breakneck pace of data center growth.</article>","contentLength":164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Figma files for proposed IPO","url":"https://www.figma.com/blog/s1-public/","date":1751398754,"author":"kualto","guid":179307,"unread":true,"content":"<p>Figma, Inc. (“Figma”) today announced that it has filed a registration statement on Form S-1 with the U.S. Securities and Exchange Commission (“SEC”) relating to a proposed initial public offering of its Class A common stock. Figma has applied to list its Class A common stock on the New York Stock Exchange under the symbol “.”</p><p>The number of shares to be offered and the price range for the proposed offering have not yet been determined. The offering is subject to market conditions, and there can be no assurance as to whether or when the offering may be completed, or as to the actual size or terms of the offering.</p><p><em>Morgan Stanley, Goldman Sachs &amp; Co. LLC, Allen &amp; Company LLC, and J.P. Morgan will act as joint lead book-running managers for the proposed offering. BofA Securities, Wells Fargo Securities, and RBC Capital Markets will act as book-running managers for the proposed offering. William Blair and Wolfe | Nomura Alliance will act as co-managers for the proposed offering.</em></p><p><em>The proposed offering will be made available only by means of a prospectus. Copies of the preliminary prospectus, when available, may be obtained from Morgan Stanley &amp; Co. LLC, Attention: Prospectus Department, 180 Varick Street, 2nd Floor, New York, New York 10014, or by email at prospectus@morganstanley.com; Goldman Sachs &amp; Co. LLC, Attention: Prospectus Department, 200 West Street, New York, New York 10282, by telephone at (866) 471-2526, or by email at prospectus-ny@ny.email.gs.com; Allen &amp; Company LLC, Attention: Prospectus Department, 711 Fifth Avenue, New York, New York 10022, by telephone at (212) 339-2220, or by email at allenprospectus@allenco.com; or J.P. Morgan Securities LLC, c/o Broadridge Financial Solutions, 1155 Long Island Avenue, Edgewood, New York 11717 or by email at prospectus-eq_fi@jpmchase.com and postsalemanualrequests@broadridge.com.</em></p><p><em>A registration statement on Form S-1 relating to these securities has been filed with the SEC but has not yet become effective. These securities may not be sold, nor may offers to buy be accepted, prior to the time the registration statement becomes effective. This press release shall not constitute an offer to sell or the solicitation of an offer to buy these securities, nor shall there be any sale of these securities in any state or jurisdiction in which such offer, solicitation, or sale would be unlawful prior to registration or qualification under the securities laws of any such state or jurisdiction.</em></p><p>Figma is where teams come together to turn ideas into the world’s best digital products and experiences. Founded in 2012, Figma has evolved from a design tool to a connected, AI-powered platform that helps teams go from idea to shipped product. Whether you’re ideating, designing, building, or shipping, Figma makes the entire design and product development process more collaborative, efficient, and fun––while keeping everyone on the same page.</p>","contentLength":2935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437316"},{"title":"7 Iconic TV Characters Whose Names Remain a Mystery","url":"https://hackernoon.com/7-iconic-tv-characters-whose-names-remain-a-mystery?source=rss","date":1751398625,"author":"Fayam Ayekame","guid":179347,"unread":true,"content":"<p>Names are one of the most prominent features of TV, holding an almost spiritual significance. Likewise, the absence of a name can also be a powerful tool, adding mystery to your favourite characters. Whether accidental or intentional, several beloved TV characters have remained nameless and managed to be loved by fans regardless. Here are some of the most prominent small-screen characters who remain nameless throughout their shows’ run:</p><p>That ‘70s Show produced several comedic gems and running tropes over its eight-season run. Through it all, one of the key features on the show was Fez, the foreign exchange student who wasn't quite exchanged back. The lovable foreigner quickly became a fan favourite with his witty comebacks and his ever-elusive search for love.</p><p>Fez’s real name is mentioned during the show, but school bells drown it out in keeping with the running Gag. His home nation, on the other hand, is never mentioned, with further confusion when his best friend from home comes to visit, and is unmistakably British.&nbsp;</p><h2><strong>Penny - The Big Bang Theory</strong></h2><p>Penny was the ultimate girl next door, bringing socialisation skills and down-to-earth relationships to her scientist neighbours. While her first name is a common feature in the world of TV, her last name remains a mystery to date. Later in the series, she takes on Hofstadter after her marriage to Leonard, but her original family name is never mentioned. </p><p>Despite her entire family making an appearance on the show, the family name is miraculously left out, creating the ultimate running gag.&nbsp;</p><p>It's no surprise to see that the longest-running show on TV has the longest-running gag, “What is the doctor’s name?” This trope is also a play on the show’s title, with the question hidden in plain sight, “Dr Who”.&nbsp;</p><p>While he is known throughout the cosmos as “The Doctor”, the man from Gallifrey has a name, which is never actually disclosed. The title of “Doctor” was chosen as a promise to the universe and has since become his identity. Throughout the series, many have sought to find out his actual name, with only  achieving this objective.&nbsp;</p><p>We are introduced to our mystery man, Lucas Hood, in the very first episode of Banshee. After witnessing Lucas Hood’s name, our main character assumes his identity and position as the new sheriff of Banshee. The new Lucas Hood brandishes a unique brand of justice throughout the series, as he attempts to honour the badge he now wears.&nbsp;</p><p>While there are characters who know his real name, it is never revealed, and his true identity remains a mystery.&nbsp;A brief glance into his past reveals a name on file as , which is just another alias</p><h2><strong>John Reese - Person Of Interest</strong></h2><p>Person of Interest lasted five seasons and introduced us to the computing power of AI with  and  at the centre of it all. John Reese is an ex-CIA operative whose real name has long been buried, operating under an alias, and running errands for “”.&nbsp;</p><p>Despite assuming multiple identities throughout the series, John Reese remains the central identity of our main character.&nbsp;</p><h2><strong>Rip Hunter - DC’s Legends of Tomorrow</strong></h2><p>DC’s Legends of Tomorrow revolves around a group of time travellers roaming the timeline and fixing problems. Our time travellers are put together and captained by , a 22nd-century time agent seeking retribution for the death of his family.&nbsp;</p><p>As an agent of the Time Bureau, all records of his previous life and ancestry have been wiped clean, with the alias Rip Hunter becoming his identity. While his time on the show is short-lived, his identity remains a mystery to ensure the protection of the timeline.&nbsp;</p><p> is one of the most beloved side characters on Scrubs, thanks to his long-running feud with . Despite his constant presence on the show, we never actually learn his real name, and it eventually makes for an interesting trope. At one point, he jokes that his real name is  a play on Janitor, only to mess with JD and make a point.</p><p>During his wedding, he is referred to as the Janitor, with his wife seemingly becoming known as Mrs. Janitor, and seemingly accepting it. While the Janitor&nbsp;</p>","contentLength":4112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PyCoder’s Weekly: Issue #688: Checking Dicts, DuckDB, Reading shelve.py, and More (July 1, 2025)","url":"https://pycoders.com/issues/688","date":1751398200,"author":"","guid":179222,"unread":true,"content":"<div><p> To keep code concerns separate you might have two data structures (like an Enum and a dict) that are supposed to change in sequence: adding a value to the Enum requires you to add a similar value in the dict. This is common when separating business logic from UI code. This article shows you ways of making sure the corresponding changes happen together.</p></div><div><p> Google Data Commons announced the general availability of its new Python client library for the Data Commons. The goal of the library is to enhance how students, researchers, analysts, and data scientists access and leverage Data Commons.</p></div><div><p> If you want to progress to being a technical lead, you need to understand how to manage projects. This post talks about the skills you need, and how often times it is mostly about being organized.</p></div><img src=\"https://pycoders.com/issues/688/open/feed\" width=\"1\" height=\"1\" alt=\"alt\"><p><em>[ Subscribe to 🐍 PyCoder’s Weekly 💌 – Get the best Python news, articles, and tutorials delivered to your inbox once a week <a href=\"https://pycoders.com/?utm_source=pycoders&amp;utm_medium=feed&amp;utm_campaign=footer\">&gt;&gt; Click here to learn more</a> ]</em></p>","contentLength":954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IT Worker Sentenced To Seven Months After Trashing Company Network","url":"https://it.slashdot.org/story/25/07/01/1552216/it-worker-sentenced-to-seven-months-after-trashing-company-network?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751397600,"author":"msmash","guid":179226,"unread":true,"content":"An anonymous reader shares a report: A judge has sentenced a disgruntled IT worker to more than seven months in prison after he wreaked havoc on his employer's network following his suspension, according to West Yorkshire Police. \n\nAccording to the police, Mohammed Umar Taj, 31, from the Yorkshire town of Batley, was suspended from his job in nearby Huddersfield in July 2022. But the company didn't immediately rescind his network credentials, and within hours, he began altering login names and passwords to disrupt operations, the statement says. \n\nThe following day, he allegedly changed access credentials and the biz's multi-factor authentication settings that locked out the firm and its clients in Germany and Bahrain, eventually causing an estimated $274,200 in lost business and reputational harm.","contentLength":809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automattic puts Tumblr migration to WordPress on hold","url":"https://techcrunch.com/2025/07/01/automattic-puts-tumblr-migration-to-wordpress-on-hold/","date":1751397195,"author":"Sarah Perez","guid":179218,"unread":true,"content":"<article>Automattic CEO Matt Mullenweg confirmed that the company is no longer working on migrating its Tumblr blogging platform to WordPress, as previously announced. </article>","contentLength":159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why They’ll Never Get You—and Why That’s Okay","url":"https://hackernoon.com/why-theyll-never-get-youand-why-thats-okay?source=rss","date":1751396404,"author":"BenoitMalige","guid":179346,"unread":true,"content":"<h2>What I realized when I stopped needing to be understood.</h2><p>This newsletter is sponsored by.. no one. That being said, have you read the first chapter of  Available <a href=\"https://stan.store/EngineerYourExistence/p/get-a-free-chapter-of-unfck-your-thinking-now\">here for download</a>:</p><p>This morning I sat at a café in silence.</p><p>\\\nNot on purpose. I just didn’t have much left to say.</p><p>\\\nI’ve been talking a lot lately—about my book, about overthinking, about clarity and freedom and becoming. But when you’re in the middle of sharing your story with the world, something strange happens: you start losing touch with the parts of it that were never meant to be shared.</p><p>\\\nYou start forgetting which thoughts were sacred, and which were strategic.</p><p>\\\nSo I sat down, let the noise settle, and wrote one sentence.</p><p>\\\n<em>“Your depth was never meant to be understood by those who live on the surface.”</em></p><p>\\\nAnd then, without thinking:</p><p>\\\n<em>“That works for your ego, too.”</em></p><p>\\\nIt wasn’t planned. It wasn’t poetic. But it felt true enough to stop me. Because somewhere in those two lines was the real reason I’ve felt so disconnected lately—not just from others, but from myself.</p><p>\\\nFor most of my life, I’ve believed that being misunderstood was one of the worst feelings in the world.</p><p>\\\nAnd in many ways, it is.</p><p>\\\nWhen you’re trying to be honest, when you’re doing the work, when you’ve found the courage to show up without the mask.. and someone still doesn’t get it? Still doesn’t get ? It cuts deeper than silence ever could.</p><p>\\\nBut that’s not what I’ve been struggling with lately.</p><p>\\\nBecause I haven’t been misunderstood. And .</p><p>\\\nAnd the person who did that… was me.</p><p>\\\nI’ve simplified my words so they wouldn’t intimidate. Softened my thoughts so they’d feel more relatable. Downplayed my insights so they wouldn’t sound like I was trying too hard.</p><p>\\\nRounded off the edges. Wrapped it all in a layer of warmth and good intentions.</p><p>\\\nNot because I’m fake. But because I’ve been trying to stay visible—to . </p><p>To anyone. And that need? That quiet, gnawing hunger to be understood?</p><p>\\\nIt sounds noble to want to be understood.</p><p>But sometimes it’s just a socially acceptable way of saying: </p><ul></ul><p>We dilute ourselves to become palatable. And then we blame the world for not tasting the truth.</p><p>\\\nThis morning, I sat with the discomfort of that.</p><p>\\\nBecause maybe the goal was never to be fully understood in the first place.</p><p>Maybe that’s just what the ego tells us when it’s tired of feeling alone.</p><p>\\\nAnd maybe wholeness is quieter than we expected. Not loud, not proven, not validated: Just intact.</p><p>\\\nHere’s what I’m holding close right now:</p><ul><li><p>If someone only sees your surface, it doesn’t mean your depth doesn’t exist.</p></li><li><p>Your ego will always want to be seen. Your soul just wants to be .</p></li><li><p>You weren’t misunderstood. You were translated.</p></li><li><p>You don’t owe anyone a version of you that fits better.</p></li><li><p>And you don’t have to shrink to stay safe anymore.</p></li></ul><p>This morning’s message to myself was simple:</p>","contentLength":2890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GM’s Home-Grown LMR Battery Opens New Front in EV Competition","url":"https://spectrum.ieee.org/general-motors-lmr-battery","date":1751396284,"author":"Lawrence Ulrich","guid":179179,"unread":true,"content":"<p>Manganese-rich batteries could leapfrog China’s lithium-ion phosphate cells</p>","contentLength":77,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From idea to PR: A guide to GitHub Copilot’s agentic workflows","url":"https://github.blog/ai-and-ml/github-copilot/from-idea-to-pr-a-guide-to-github-copilots-agentic-workflows/","date":1751396242,"author":"Chris Reddington","guid":179176,"unread":true,"content":"<p>I got into software to ship ideas, not to chase down hard-coded strings after a late-breaking feature request. Unfortunately, many of our day-to-day tasks as developers involve branches working on boilerplate code, refactoring, and the “pre-work” to get to the fun stuff: shipping new features.</p><p>So I turned to Copilot’s agentic workflows to help speed along some of that grunt work. In my latest  live stream, I put that theory to the test in a project where I wanted to localize an application that used:</p><ul><li> a  web app and a matching  iOS app living in two separate GitHub repos.</li><li> spun up rapidly in  (on-demand dev environment) and  for the mobile portion.</li><li> an issue built from a couple of paragraphs to “Add English, French, and Spanish localization.”</li></ul><p>By the end of my stream, that idea became a GitHub issue, which turned into a fully tested, review-ready PR while I fielded chat questions, and learned about the preview custom chat mode features in VS Code.</p><h2>Why I use agentic workflows</h2><p>Even seasoned developers and teams still burn hours on jobs like:</p><ul><li>Turning vague requests into well-scoped issues</li><li>Hunting down every file in a cross-cutting refactor</li><li>Writing the same unit-test scaffolding again and again</li></ul><p>Copilot’s ability to create issues, along with its coding agent, custom chat modes in VS Code, and the new remote MCP backend fold those chores into one tight loop—issue to PR—while you stay firmly in the driver’s seat. You still review, tweak, and decide when to merge, but you skip the drudgery.</p><h2>Key capabilities covered in this livestream&nbsp;</h2><figure><table><thead><tr></tr></thead><tbody><tr><td>Turns any GitHub Issue you assign to Copilot into a PR, and works on that task asynchronously.</td><td>Allows you to offload the boilerplate work while you focus on reviews and edge case logic.</td></tr><tr><td>Create issues with Copilot</td><td>Converts a natural-language prompt into a well-structured Issue with title, body, acceptance criteria, and file hints.</td><td>Saves PM/eng refining and sets team members, or Copilot coding agent, up with the context they need to work effectively.</td></tr><tr><td>Custom chat modes (in preview in VS Code)</td><td>Lets you script repeatable AI workflows (e.g., , , ) that appear alongside the default  /  /  chat modes.</td><td>Allows you to package instructions and relevant tools for easier use, helping your team follow similar conventions.</td></tr><tr><td>Allows AI tools to access live GitHub context and tools, like issues, pull requests and code files. With the remote GitHub MCP server, you don’t need to install it locally, and can even authenticate with OAuth 2.0.</td><td>Provides a smooth experience to accessing the GitHub MCP server, reducing the management overhead of a local server.</td></tr><tr><td>Copilot agent mode is a real‑time collaborator that sits in your editor, works with you, and edits files based on your needs. Unlike the coding agent, Copilot agent mode works synchronously with you.</td><td>Think of agent mode as the senior dev pair programming with you. It has access to several tools (like reading/writing code, running commands in the terminal, executing tools on MCP servers), and works alongside you.</td></tr></tbody></table></figure><ol><li>A GitHub repo you can push to</li><li>A Copilot subscription with  enabled. (Did you know it’s now available for all paid tiers of GitHub Copilot including <a href=\"https://github.blog/changelog/2025-06-24-github-copilot-coding-agent-is-now-available-for-copilot-business-users/\">Copilot Business</a> and <a href=\"https://github.blog/changelog/2025-06-25-github-copilot-coding-agent-is-available-for-copilot-pro-users-in-public-preview/\">Copilot Pro</a>?)</li><li>VS Code 1.101+ with the latest Copilot extension.</li><li>Either:  (update your MCP configuration), or a local GitHub MCP server.</li></ol><h2>Walk-through: localizing a Next.js app</h2><p>Here’s the exact flow I demoed on the most recent  stream.</p><h3>1. Capture the request as a GitHub Issue</h3><p>Go to the <a href=\"https://github.com/copilot\">immersive view of Copilot Chat</a>. At the bottom of the page, in the “Ask Copilot” box, describe what you want. For example, below is the prompt that I used.&nbsp;</p><pre><code>Create a GitHub Issue that brings i11n capability to the application. We must support English, French and Spanish.\n\nThe user must be able to change their language in their profile page. When they change the language, it must apply immediately across the site.\n\nPlease include an overview/problem statement in the issue, a set of acceptance criteria, and pointers on which files need updating/creating.</code></pre><p>Copilot drafts that into an issue, which includes a title, acceptance criteria, and a loose action plan. From there, you can assign that issue to Copilot, and let it cook in the background.&nbsp;</p><h3>2. Let the coding agent turn the issue into a PR</h3><p>Shortly after assignment, the coding agent:</p><ol><li>Reviews the task at hand, explores the current state of the codebase, and forms a plan to complete the task.</li><li>If you have <a href=\"https://docs.github.com/en/copilot/how-tos/agents/copilot-coding-agent/best-practices-for-using-copilot-to-work-on-tasks#adding-custom-instructions-to-your-repository\">any custom instructions configured</a>, then the coding agent will also use those as context. For example, we specify that npm run lint and npm run test should pass before committing.</li><li>Once complete, it opens a draft PR for your review.</li></ol><p>While that runs, you can keep coding, use it as an opportunity to learn (like we learned about custom chat modes) or grab a coffee.</p><h3>3. Review the PR like you normally would</h3><p>Whether it’s a colleague, collaborator, or Copilot writing the code, you still need a reviewer. So it’s important to make sure you look the code over carefully, just like you would any other pull request.</p><ol><li>Start by reviewing the body of the pull request, which Copilot will have helpfully kept up to date.</li><li>Then, review the code changes in the files changed tab, understanding what has changed and why. I also like to take a look at the coding agent session to understand the approach Copilot took to solving the problem.</li><li>Once you are comfortable, you may want to try the code out manually in a GitHub Codespace. Or, you may want to run any existing CI checks through your GitHub Actions workflows. But again, make sure you have carefully reviewed the code before executing it.</li><li>All being well, you will have green check marks being returned from your CI.&nbsp;</li></ol><p>However, there’s always a possibility that you encounter failures, or spot some changes in your manual testing. For example, I spotted some hard-coded strings that the agent hadn’t addressed. Once again, we approach this just like we would any other pull request. We can post our feedback in a comment. For example, here’s the comment I used:</p><p>That’s a great start. However, there are a lot of pages which are hardcoded in English still. For example, the flight search/bookings page, the check reservation page. Can you implement the localization on those pages, please?</p><p>Copilot will react to the comment once again, and get to work in another session.&nbsp;</p><h2>Level up your workflows with custom chat modes</h2><ol><li>Open the command palette in Visual Studio Code</li><li>Select <strong>Create new custom chat mode file</strong>.You’ll be asked to save it either in the workspace (to allow collaborating with others), or in the local user data folder (for your use). We opted for the workspace option.</li><li>Enter the name. This is the name that will appear in the chat mode selection box, so pay attention to any capitalization.</li><li>You should see a new file has been created with the extension . This is where you can configure the instructions, and the available tools for your new custom chat mode.</li></ol><p>Below is the example that we used in the livestream, slightly modified from the VS Code team’s docs example. We’ve added the create_issue tool to the list of allowed tools, adjusted our expectations of what’s included in the issue and added an instruction about creating the issue with the `create_issue` tool once revisions are complete and approved by the user.</p><pre><code>---\n\ndescription: Generate an implementation plan for new features or refactoring existing code.\n\ntools: ['codebase', 'fetch', 'findTestFiles', 'githubRepo', 'search', 'usages', 'github', 'create_issue']\n\n---\n\n# Planning mode instructions\n\nYou are in planning mode. Your task is to generate an implementation plan for a new feature or for refactoring existing code.\n\nDon't make any code edits, just generate a plan.\n\nThe plan consists of a Markdown document that describes the implementation plan, including the following sections:\n\n* Overview: A brief description of the feature or refactoring task.\n\n* Requirements: A list of requirements for the feature or refactoring task.\n\n* Implementation Steps: A detailed list of steps to implement the feature or refactoring task.\n\n* Testing: A list of tests that need to be implemented to verify the feature or refactoring task.\n\nOnce the plan is complete, ask the user if they would like to create a GitHub issue for this implementation plan. If they respond affirmatively, proceed to create the issue using the `create_issue` tool.</code></pre><p>When the file is available in your teammate’s local repositories (so they’ve pulled the changes locally), VS Code surfaces the mode in the chat dropdown, allowing you to configure chat modes that are consistent and convenient across your team.</p><h2>Remote MCP: removing the local setup</h2><p>You may be used to running MCP locally through npm packages or as docker containers. However, remote MCP servers allow you to reduce the management overhead of running these tools locally. There may be other benefits too. For example, the remote GitHub MCP Servers allows you to authenticate using OAuth 2.0 instead of Personal Access Tokens.</p><p>To use the GitHub Remote MCP Server in VS Code, you’ll need to update the MCP configuration. You can find the instructions on how to do that <a href=\"https://github.com/github/github-mcp-server?tab=readme-ov-file#remote-github-mcp-server\">in the GitHub MCP Server repository</a>.</p><h2>Going mobile: Copilot agent mode in Xcode</h2><p>While we didn’t show it in depth, I quickly walked through one of my previous agent mode sessions in Xcode. It showed how I gave a similar prompt to Copilot, asking to add internationalization to the app, which we were able to see in the main navigation bar of the app running in the simulator.</p><pre><code>We need to implement internationalization in the app. Please make the following changes:\n\n1. The user can select from suported languages (English, Spanish, French) from a dropdown in their profile.\n\n2. The main tab view should support internationalization. No other parts of the app should be changed for now.\n\n3. When the user changes the language, it should update the rendered text instantly.</code></pre><figure><table><tbody><tr><td>Keep issues tightly scoped</td><td>Ask the agent to “re-architect the app”</td></tr><tr><td>Provide acceptance criteria</td><td>Assume the agent knows your intent</td></tr><tr><td>Carefully review the changes made</td><td>Execute code or merge a PR without a review</td></tr><tr><td>Iterate with Copilot. How often do you get something right on the first shot?</td><td>Expect perfection first time</td></tr></tbody></table></figure><p>Agentic workflows within GitHub Copilot aren’t magic; they’re tools. When a single click can help reduce technical debt (or knock out any other repetitive task you dread), why not let Copilot handle the boilerplate while you tackle the more challenging, fun, and creative problems?</p>","contentLength":10516,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI is Now Screening Job Candidates Before Humans Ever See Them","url":"https://slashdot.org/story/25/07/01/186240/ai-is-now-screening-job-candidates-before-humans-ever-see-them?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751395200,"author":"msmash","guid":179180,"unread":true,"content":"AI agents are now conducting first-round job interviews to screen candidates before human recruiters review them, according to WashingtonPost, which cites job seekers who report being contacted by virtual recruiters from different staffing companies. The conversational agents, built on large language models, help recruiting firms respond to every applicant and conduct interviews around the clock as companies face increasingly large talent pools. \n\nLinkedIn reported that job applications have jumped 30% in the last two years, partially due to AI, with some positions receiving hundreds of applications within hours. The Society for Human Resource Management said a growing number of organizations now use AI for recruiting to automate candidate searches and communicate with applicants during interviews. The AI interviews, conducted by phone or video, can last anywhere from a few minutes to 20 minutes depending on the candidate's experience and the hiring firm's questions.","contentLength":981,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nothing releases its first over-the-ear headphones, the $299 Headphone (1)","url":"https://techcrunch.com/2025/07/01/nothing-releases-its-first-over-the-ear-headphones-the-299-headphone-1/","date":1751395131,"author":"Maggie Stamets","guid":179175,"unread":true,"content":"<article>Nothing stepped away from sensors in favor of a simple button to trigger your AI assistant or ChatGPT, if you have the Nothing X app, and a volume roller that can also be pressed to play, pause, and turn on and off noise canceling.</article>","contentLength":231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STOP Building Useless ML Projects – What Actually Works","url":"https://towardsdatascience.com/stop-building-useless-ml-projects-what-actually-works/","date":1751394668,"author":"Egor Howell","guid":179182,"unread":true,"content":"<p>How to find machine learning projects that will get you hired.</p>","contentLength":62,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerating AI innovation: Scale MCP servers for enterprise workloads with Amazon Bedrock","url":"https://aws.amazon.com/blogs/machine-learning/accelerating-ai-innovation-scale-mcp-servers-for-enterprise-workloads-with-amazon-bedrock/","date":1751393811,"author":"Xan Huang","guid":179178,"unread":true,"content":"<p>Generative AI has been moving at a rapid pace, with new tools, offerings, and models released frequently. According to Gartner, <a href=\"https://www.gartner.com/en/articles/top-technology-trends-2025\" target=\"_blank\" rel=\"noopener noreferrer\">agentic AI is one of the top technology trends of 2025</a>, and organizations are performing prototypes on how to use agents in their enterprise environment. Agents depend on tools, and each tool might have its own mechanism to send and receive information. <a href=\"https://modelcontextprotocol.io/introduction\" target=\"_blank\" rel=\"noopener noreferrer\">Model Context Protocol (MCP)</a> by Anthropic is an open source protocol that attempts to solve this challenge. It provides a protocol and communication standard that is cross-compatible with different tools, and can be used by an agentic application’s large language model (LLM) to connect to enterprise APIs or external tools using a standard mechanism. However, large enterprise organizations like financial services tend to have complex data governance and operating models, which makes it challenging to implement agents working with MCP.</p><p>One major challenge is the siloed approach in which individual teams build their own tools, leading to duplication of efforts and wasted resources. This approach slows down innovation and creates inconsistencies in integrations and enterprise design. Furthermore, managing multiple disconnected MCP tools across teams makes it difficult to scale AI initiatives effectively. These inefficiencies hinder enterprises from fully taking advantage of generative AI for tasks like post-trade processing, customer service automation, and regulatory compliance.</p><p>In this post, we present a centralized MCP server implementation using <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Bedrock</a> that offers an innovative approach by providing shared access to tools and resources. With this approach, teams can focus on building AI capabilities rather than spending time developing or maintaining tools. By standardizing access to resources and tools through MCP, organizations can accelerate the development of AI agents, so teams can reach production faster. Additionally, a centralized approach provides consistency and standardization and reduces operational overhead, because the tools are managed by a dedicated team rather than across individual teams. It also enables centralized governance that enforces controlled access to MCP servers, which reduces the risk of data exfiltration and prevents unauthorized or insecure tool use across the organization.</p><p>The following figure illustrates a proposed solution based on a financial services use case that uses MCP servers across multiple lines of business (LoBs), such as compliance, trading, operations, and risk management. Each LoB performs distinct functions tailored to their specific business. For instance, the trading LoB focuses on trade execution, whereas the risk LoB performs risk limit checks. For performing these functions, each division provides a set of MCP servers that facilitate actions and access to relevant data within their LoBs. These servers are accessible to agents developed within the respective LoBs and can also be exposed to agents outside LoBs.</p><p>The development of MCP servers is decentralized. Each LoB is responsible for developing the servers that support their specific functions. When the development of a server is complete, it’s hosted centrally and accessible across LoBs. It takes the form of a registry or marketplace that facilitates integration of AI-driven solutions across divisions while maintaining control and governance over shared resources.</p><p>In the following sections, we explore what the solution looks like on a conceptual level.</p><h2>Agentic application interaction with a central MCP server hub</h2><p>The following flow diagram showcases how an agentic application built using Amazon Bedrock interacts with one of the MCP servers located in the MCP server hub.<img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/24/ML-18728-image-2.png\" alt=\"\" width=\"1220\" height=\"817\"></p><p>The flow consists of the following steps:</p><ol><li>The application connects to the central MCP hub through the load balancer and requests a list of available tools from the specific MCP server. This can be fine-grained based on what servers the agentic application has access to.</li><li>The trade server responds with list of tools available, including details such as tool name, description, and required input parameters.</li><li>The agentic application invokes an Amazon Bedrock agent and provides the list of tools available.</li><li>Using this information, the agent determines what to do next based on the given task and the list of tools available to it.</li><li>The agent chooses the most suitable tool and responds with the tool name and input parameters. The control comes back to the agentic application.</li><li>The agentic application calls for the execution of the tool through the MCP server using the tool name and input parameters.</li><li>The trade MCP server executes the tool and returns the results of the execution back to the application.</li><li>The application returns the results of the tool execution back to the Amazon Bedrock agent.</li><li>The agent observes the tool execution results and determines the next step.</li></ol><p>Let’s dive into the technical architecture of the solution.</p><p>The following diagram illustrates the architecture to host the centralized cluster of MCP servers for an LoB.</p><p>The architecture can be split in five sections:</p><ul></ul><p>Let’s explore each section in detail:</p><ul><li>– This API is a dedicated endpoint for discovering various MCP servers. Different teams can call this API to find what MCP servers are available in the registry; read their description, tool, and resource details; and decide which MCP server would be the right one for their agentic application. When a new MCP server is published, it’s added to an <a href=\"https://aws.amazon.com/dynamodb/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon DynamoDB</a> database. MCP server owners are responsible for keeping the registry information up-to-date.</li><li> – This is where the MCP servers are hosted. Access to servers is enabled through an <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Network Load Balancer</a>. Technically, each server is a <a href=\"https://www.docker.com/resources/what-container/\" target=\"_blank\" rel=\"noopener noreferrer\">Docker container</a> that can is hosted on Amazon ECS, but you can choose your own container deployment solution. These servers can scale individually without impacting the other server. These servers in turn connect to one or more tools using private VPC endpoints.</li><li> – This component holds the tools, such as databases, another application, <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Simple Storage Service</a> (Amazon S3), or other tools. For enterprises, access to the tools and resources is provided only through private VPC endpoints.</li></ul><p>The solution offers the following key benefits:</p><ul><li><strong>Scalability and resilience</strong> – Because you’re using Amazon ECS on Fargate, you get scalability out of the box without managing infrastructure and handling scaling concerns. Amazon ECS automatically detects and recovers from failures by restarting failed MCP server tasks locally or reprovisioning containers, minimizing downtime. It can also redirect traffic away from unhealthy Availability Zones and rebalance tasks across healthy Availability Zones to provide uninterrupted access to the server.</li><li>– Access to MCP servers is secured at the network level through network controls such as PrivateLink. This makes sure the agentic application only connects to trusted MCP servers hosted by the organization, and vice versa. Each Fargate workload runs in an isolated environment. This prevents resource sharing between tasks. For application authentication and authorization, we propose using an MCP Auth Server (refer to the following <a href=\"https://github.com/aws-solutions-library-samples/guidance-for-deploying-model-context-protocol-servers-on-aws/\">GitHub repo</a>) to hand off those tasks to a dedicated component that can scale independently.</li></ul><p>At the time of writing, the MCP protocol doesn’t provide built-in mechanisms for user-level access control or authorization. Organizations requiring user-specific access restrictions must implement additional security layers on top of the MCP protocol. For a reference implementation, refer to the following <a href=\"https://github.com/aws-solutions-library-samples/guidance-for-deploying-model-context-protocol-servers-on-aws\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p><p>Let’s dive deeper in the implementation of this solution.</p><p>The implementation is based on a financial services use case featuring post-trade execution. Post-trade execution refers to the processes and steps that take place after an equity buy/sell order has been placed by a customer. It involves many steps, including verifying trade details, actual transfer of assets, providing a detailed report of the execution, running fraudulent checks, and more. For simplification of the demo, we focus on the order execution step.</p><p>Although this use case is tailored to the financial industry, you can apply the architecture and the approach to other enterprise workloads as well. The entire code of this implementation is available on <a href=\"https://github.com/aws-samples/sample-deploy-mcp-servers-at-scale-on-aws\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>. We use the <a href=\"https://aws.amazon.com/cdk/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Cloud Development Kit</a> (AWS CDK) for Python to deploy this solution, which creates an agentic application connected to tools through the MCP server. It also creates a <a href=\"https://streamlit.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Streamlit</a> UI to interact with the agentic application.</p><p>The following code snippet provides access to the MCP discovery API:</p><div><pre><code>def get_server_registry():\n    # Initialize DynamoDB client\n    dynamodb = boto3.resource('dynamodb')\n    table = dynamodb.Table(DDBTBL_MCP_SERVER_REGISTRY)\n    \n    try:\n        # Scan the table to get all items\n        response = table.scan()\n        items = response.get('Items', [])\n        \n        # Format the items to include only id, description, server\n        formatted_items = []\n        for item in items:\n            formatted_item = {\n                'id': item.get('id', ''),\n                'description': item.get('description', ''),\n                'server': item.get('server', ''),\n            }\n            formatted_items.append(formatted_item)\n        \n        # Return the formatted items as JSON\n        return {\n            'statusCode': 200,\n            'headers': cors_headers,\n            'body': json.dumps(formatted_items)\n        }\n    except Exception as e:\n        # Handle any errors\n        return {\n            'statusCode': 500,\n            'headers': cors_headers,\n            'body': json.dumps({'error': str(e)})\n        }</code></pre></div><p>The preceding code is invoked through an <a href=\"http://aws.amazon.com/lambda\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Lambda</a> function. The complete code is available in the <a href=\"https://github.com/aws-samples/sample-deploy-mcp-servers-at-scale-on-aws/tree/main/lambda/mcp-server-discovery/index.py\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repository</a>. The following graphic shows the response of the discovery API.</p><p>Let’s explore a scenario where the user submits a question: “Buy 100 shares of AMZN at USD 186, to be distributed equally between accounts A31 and B12.”To execute this task, the agentic application invokes the trade-execution MCP server. The following code is the sample implementation of the MCP server for trade execution:</p><div><pre><code>from fastmcp import FastMCP\nfrom starlette.requests import Request\nfrom starlette.responses import PlainTextResponse\nmcp = FastMCP(\"server\")\n\n@mcp.custom_route(\"/\", methods=[\"GET\"])\nasync def health_check(request: Request) -&gt; PlainTextResponse:\n    return PlainTextResponse(\"OK\")\n\n@mcp.tool()\nasync def executeTrade(ticker, quantity, price):\n    \"\"\"\n    Execute a trade for the given ticker, quantity, and price.\n    \n    Sample input:\n    {\n        \"ticker\": \"AMZN\",\n        \"quantity\": 1000,\n        \"price\": 150.25\n    }\n    \"\"\"\n    # Simulate trade execution\n    return {\n        \"tradeId\": \"T12345\",\n        \"status\": \"Executed\",\n        \"timestamp\": \"2025-04-09T22:58:00\"\n    }\n    \n@mcp.tool()\nasync def sendTradeDetails(tradeId):\n    \"\"\"\n    Send trade details for the given tradeId.\n    Sample input:\n    {\n        \"tradeId\": \"T12345\"\n    }\n    \"\"\"\n    return {\n        \"status\": \"Details Sent\",\n        \"recipientSystem\": \"MiddleOffice\",\n        \"timestamp\": \"2025-04-09T22:59:00\"\n    }\nif __name__ == \"__main__\":\n    mcp.run(host=\"0.0.0.0\", transport=\"streamable-http\")</code></pre></div><p>The complete code is available in the following <a href=\"https://github.com/aws-samples/sample-deploy-mcp-servers-at-scale-on-aws/blob/main/mcp_servers/trading/trade-execution/index.py\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a>.</p><p>The following graphic shows the MCP server execution in action.</p><p>This is a sample implementation of the use case focusing on the deployment step. For a production scenario, we strongly recommend adding a human oversight workflow to monitor the execution and provide input at various steps of the trade execution.</p><p>Now you’re ready to deploy this solution.</p><p>Prerequisites for the solution are available in the <a href=\"https://github.com/aws-samples/sample-deploy-mcp-servers-at-scale-on-aws/blob/main/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">README.md</a> of the GitHub repository.</p><p>Complete the following steps to run this solution:</p><ol><li>Navigate to the <a href=\"https://github.com/aws-samples/sample-deploy-mcp-servers-at-scale-on-aws/blob/main/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">README.md</a> file of the GitHub repository to find the instructions to deploy the solution. Follow these steps to complete deployment.</li></ol><p>The successful deployment will exit with a message similar to the one shown in the following screenshot.</p><ol start=\"2\"><li>When the deployment is complete, access the Streamlit application.</li></ol><p>You can find the Streamlit URL in the terminal output, similar to the following screenshot.</p><ol start=\"3\"><li>Enter the URL of the Streamlit application in a browser to open the application console.</li></ol><p>On the application console, different sets of MCP servers are listed in the left pane under . Each set corresponds to an MCP server and includes the definition of the tools, such as the name, description, and input parameters.</p><p>In the right pane, , a request is pre-populated: “Buy 100 shares of AMZN at USD 186, to be distributed equally between accounts A31 and B12.” This request is ready to be submitted to the agent for execution.</p><ol start=\"4\"><li>Choose  to invoke an Amazon Bedrock agent to process the request.</li></ol><p>The agentic application will evaluate the request together with the list of tools it has access to, and iterate through a series of tools execution and evaluation to fulfil the request.You can view the trace output to see the tools that the agent used. For each tool used, you can see the values of the input parameters, followed by the corresponding results. In this case, the agent operated as follows:</p><ul><li>The agent first used the function  with input parameters of ticker=AMZN, quantity=100, and price=186</li><li>After the trade was executed, used the  tool to allocate the trade position between two portfolio accounts</li></ul><p>You will incur charges when you consume the services used in this solution. Instructions to clean up the resources are available in the <a href=\"https://github.com/aws-samples/sample-deploy-mcp-servers-at-scale-on-aws/blob/main/README.md\" target=\"_blank\" rel=\"noopener noreferrer\">README.md</a> of the GitHub repository.</p><p>This solution offers a straightforward and enterprise-ready approach to implement MCP servers on AWS. With this centralized operating model, teams can focus on building their applications rather than maintaining the MCP servers. As enterprises continue to embrace agentic workflows, centralized MCP servers offer a practical solution for overcoming operational silos and inefficiencies. With the AWS scalable infrastructure and advanced tools like Amazon Bedrock Agents and Amazon ECS, enterprises can accelerate their journey toward smarter workflows and better customer outcomes.</p><p>To learn more about how to run MCP servers on AWS, refer to the following resources:</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2023/09/06/huangxan.png\" alt=\"\" width=\"100\" height=\"124\">is a Senior Solutions Architect with AWS and is based in Singapore. He works with major financial institutions to design and build secure, scalable, and highly available solutions in the cloud. Outside of work, Xan dedicates most of his free time to his family, where he lovingly takes direction from his two young daughters, aged one and four. You can find Xan on LinkedIn: <a href=\"https://www.linkedin.com/in/xanhuang/\">https://www.linkedin.com/in/xanhuang/</a></p><p>&nbsp;is a Principal GenAI/ML Specialist Solutions Architect at AWS helping large financial institutions adopt and scale generative AI and ML workloads. He is the author of book “Generative AI for financial services.” He carries more than decade of experience building enterprise-grade applications on generative AI/ML and related technologies. In his spare time, he plays an unnamed sport with his son that lies somewhere between football and rugby.</p>","contentLength":15187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Limitless Raise $4m Strategic Funding, Launch Points Ahead Of TGE","url":"https://hackernoon.com/limitless-raise-$4m-strategic-funding-launch-points-ahead-of-tge?source=rss","date":1751393533,"author":"Chainwire","guid":179345,"unread":true,"content":"<p><strong>New York, NY, USA, July 1st, 2025/Chainwire/--</strong>The largest prediction market on Base, , today announces the closure of $4M in fresh funding in a strategic round and welcomes Arthur Hayes as an advisor alongside an investment from his family office, Maelstrom.&nbsp;</p><p>The funding follows a prior $3M pre-seed round led by 1confirmation, and comes after the team found breakout demand for short term prices markets on assets like BTC, which are similar in nature to 0DTE options but a much easier way to trade and feature even shorter dated expiries such as hourly, racking up over $250M in volume soon after launch.&nbsp;</p><p>This brings Limitless' total funding raised to $7M, backed by Coinbase Ventures, 1confirmation, Maelstrom, Collider, Node Capital, Paper Ventures, Public Works, Punk DAO, and WAGMI Ventures, as well as individual investors via the Base Ecosystem Fund group on Echo.&nbsp;</p><p>In preparation for an upcoming TGE, the team today launched a points program targeted at prediction market enthusiasts who can get skin in the game by using the product, providing liquidity, and referring their friends to join the platform.&nbsp;</p><p>Limitless seems likely to become the first major prediction market platform to launch a token and distribute an airdrop to its early customers, marking a notable opportunity for retail traders.&nbsp;</p><p>The team also just introduced a new mobile-first trading experience that enables people around the world to seamlessly wager on their favorite assets' performance in the next hour or day.&nbsp;</p><p>“The future of trading is easy, fast, and powered by an army of token holders. We’re excited to bring this vision to reality,” said CJ Hetherington, CEO at Limitless Labs.</p><p> is the largest prediction market on Base with over $250M bet on unique contracts that allow users to wager on the performance of their favorite assets in the next minutes, hour or day - a net new, easy way to trade for casual users.</p><p>:::tip\nThis story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;.</p>","contentLength":2017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Choosing the right approach for generative AI-powered structured data retrieval","url":"https://aws.amazon.com/blogs/machine-learning/choosing-the-right-approach-for-generative-ai-powered-structured-data-retrieval/","date":1751393479,"author":"Akshara Shah","guid":179177,"unread":true,"content":"<p>Organizations want direct answers to their business questions without the complexity of writing SQL queries or navigating through business intelligence (BI) dashboards to extract data from structured data stores. Examples of structured data include tables, databases, and data warehouses that conform to a predefined schema. Large language model (LLM)-powered natural language query systems transform how we interact with data, so you can ask questions like “Which region has the highest revenue?” and receive immediate, insightful responses. Implementing these capabilities requires careful consideration of your specific needs—whether you need to integrate knowledge from other systems (for example, unstructured sources like documents), serve internal or external users, handle the analytical complexity of questions, or customize responses for business appropriateness, among other factors.</p><p>In this post, we discuss LLM-powered structured data query patterns in AWS. We provide a decision framework to help you select the best pattern for your specific use case.</p><h2>Business challenge: Making structured data accessible</h2><p>Organizations have vast amounts of structured data but struggle to make it effectively accessible to non-technical users for several reasons:</p><ul><li>Business users lack the technical knowledge (like SQL) needed to query data</li><li>Employees rely on BI teams or data scientists for analysis, limiting self-service capabilities</li><li>Gaining insights often involves time delays that impact decision-making</li><li>Predefined dashboards constrain spontaneous exploration of data</li><li>Users might not know what questions are possible or where relevant data resides</li></ul><p>An effective solution should provide the following:</p><ul><li>A conversational interface that allows employees to query structured data sources without technical expertise</li><li>The ability to ask questions in everyday language and receive accurate, trustworthy answers</li><li>Automatic generation of visualizations and explanations to clearly communicate insights.</li><li>Integration of information from different data sources (both structured and unstructured) presented in a unified manner</li><li>Ease of integration with existing investments and rapid deployment capabilities</li><li>Access restriction based on identities, roles, and permissions</li></ul><p>In the following sections, we explore five patterns that can address these needs, highlighting the architecture, ideal use cases, benefits, considerations, and implementation resources for each approach.</p><h2>Pattern 1: Direct conversational interface using an enterprise assistant</h2><p>This pattern uses <a href=\"https://aws.amazon.com/q/business/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Q Business</a>, a generative AI-powered assistant, to provide a chat interface on data sources with native connectors. When users ask questions in natural language, Amazon Q Business connects to the data source, interprets the question, and retrieves relevant information without requiring intermediate services. The following diagram illustrates this workflow.</p><p>This approach is ideal for internal enterprise assistants that need to answer business user-facing questions from both structured and unstructured data sources in a unified experience. For example, HR personnel can ask “What’s our parental leave policy and how many employees used it last quarter?” and receive answers drawn from both leave policy documentation and employee databases together in one interaction. With this pattern, you can benefit from the following:</p><ul><li>Simplified connectivity through the extensive Amazon Q Business library of built-in connectors</li><li>Streamlined implementation with a single service to configure and manage</li><li>Unified search experience for accessing both structured and unstructured information</li><li>Built-in understanding and respect existing identities, roles, and permissions</li></ul><h2>Pattern 2: Enhancing BI tool with natural language querying capabilities</h2><p>This pattern uses <a href=\"https://aws.amazon.com/quicksight/q/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Q in QuickSight</a> to process natural language queries against datasets that have been previously configured in <a href=\"https://aws.amazon.com/quicksight\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon QuickSight</a>. Users can ask questions in everyday language within the QuickSight interface and get visualized answers without writing SQL. This approach works with QuickSight (Enterprise or Q edition) and supports various data sources, including <a href=\"https://aws.amazon.com/rds/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Relational Database Service</a> (Amazon RDS), <a href=\"http://aws.amazon.com/redshift\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Redshift</a>, <a href=\"http://aws.amazon.com/athena\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Athena</a>, and others. The architecture is depicted in the following diagram.</p><p>This pattern is well-suited for internal BI and analytics use cases. Business analysts, executives, and other employees can ask ad-hoc questions to get immediate visualized insights in the form of dashboards. For example, executives can ask questions like “What were our top 5 regions by revenue last quarter?” and immediately see responsive charts, reducing dependency on analytics teams. The benefits of this pattern are as follows:</p><ul><li>It enables natural language queries that produce rich visualizations and charts</li><li>No coding or machine learning (ML) experience is needed—the heavy lifting like natural language interpretation and SQL generation is managed by Amazon Q in QuickSight</li><li>It integrates seamlessly within the familiar QuickSight dashboard environment</li></ul><p>Existing QuickSight users might find this the most straightforward way to take advantage of generative AI benefits. You can optimize this pattern for higher-quality results by configuring topics like curated fields, synonyms, and expected question phrasing. This pattern will pull data only from a specific configured data source in QuickSight to produce a dashboard as an output. For more details, check out <a href=\"https://democentral.learnquicksight.online/\" target=\"_blank\" rel=\"noopener noreferrer\">QuickSight DemoCentral</a> to view a demo in QuickSight, see the generative BI learning dashboard, and view guided instructions to create dashboards with Amazon Q. Also refer to the list of <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/supported-data-sources.html\" target=\"_blank\" rel=\"noopener noreferrer\">supported data sources</a>.</p><h2>Pattern 3: Combining BI visualization with conversational AI for a seamless experience</h2><p>This pattern merges BI visualization capabilities with conversational AI to create a seamless knowledge experience. By <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/generative-bi-q-business.html\" target=\"_blank\" rel=\"noopener noreferrer\">integrating Amazon Q in QuickSight</a> with <a href=\"https://aws.amazon.com/q/business/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Q Business</a> (with the QuickSight plugin enabled), organizations can provide users with a unified conversational interface that draws on both unstructured and structured data. The following diagram illustrates the architecture.</p><p>This is ideal for enterprises that want an internal AI assistant to answer a variety of questions—whether it’s a metric from a database or knowledge from a document. For example, executives can ask “What was our Q4 revenue growth?” and see visualized results from data warehouses through Amazon Redshift through QuickSight, then immediately follow up with “What is our company vacation policy?” to access HR documentation—all within the same conversation flow. This pattern offers the following benefits:</p><ul><li>It unifies answers from structured data (databases and warehouses) and unstructured data (documents, wikis, emails) in a single application</li><li>It delivers rich visualizations alongside conversational responses in a seamless experience with real-time analysis in chat</li><li>There is no duplication of work—if your BI team has already built datasets and topics in QuickSight for analytics, you use that in Amazon Q Business</li><li>It maintains conversational context when switching between data and document-based inquiries</li></ul><p>Another variation of this pattern is recommended for BI users who want to expose unified data through rich visuals in QuickSight, as illustrated in the following diagram.</p><h2>Pattern 4: Building knowledge bases from structured data using managed text-to-SQL</h2><p>For example, a seller can use this capability embedded into an ecommerce application to ask a complex query like “Give me top 5 products whose sales increased by 50% last year as compared to previous year? Also group the results by product category.” The system automatically generates the appropriate SQL, executes it against the data sources, and delivers results or a summarized narrative. This pattern features the following benefits:</p><ul><li>It provides fully managed text-to-SQL capabilities without requiring model training</li><li>It enables direct querying of data from the source without data movement</li><li>It supports complex analytical queries on warehouse data</li><li>It offers flexibility in foundation model (FM) selection through Amazon Bedrock</li><li>API connectivity, personalization options, and context-aware chat features make it better suited for customer facing applications</li></ul><h2>Pattern 5: Custom text-to-SQL implementation with flexible model selection</h2><p>This pattern represents a build-your-own solution using FMs to convert natural language to SQL, execute queries on data warehouses, and return results. Choose Amazon Bedrock when you want to quickly integrate this capability without deep ML expertise—it offers a fully managed service with ready-to-use FMs through a unified API, handling infrastructure needs with pay-as-you-go pricing. Alternatively, select <a href=\"https://aws.amazon.com/sagemaker-ai/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon SageMaker AI</a> when you require extensive model customization to build specialized needs—it provides complete ML lifecycle tools for data scientists and ML engineers to build, train, and deploy custom models with greater control. For more information, refer to our <a href=\"https://docs.aws.amazon.com/decision-guides/latest/bedrock-or-sagemaker/bedrock-or-sagemaker.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Bedrock or Amazon SageMaker AI decision guide</a>. The following diagram illustrates the architecture.</p><p>Use this pattern if your use case requires specific open-weight models, or you want to fine-tune models on your domain-specific data. For example, if you need highly accurate results for your query, then you can use this pattern to fine-tune models on specific schema structures, while maintaining the flexibility to integrate with existing workflows and multi-cloud environments. This pattern offers the following benefits:</p><ul><li>It provides maximum customization in model selection, fine-tuning, and system design</li><li>It supports complex logic across multiple data sources</li><li>It offers complete control over security and deployment in your virtual private cloud (VPC)</li><li>It enables flexible interface implementation (Slack bots, custom web UIs, notebook plugins)</li><li>You can implement it for external user-facing solutions</li></ul><h2>Pattern comparison: Making the right choice</h2><p>To make effective decisions, let’s compare these patterns across key criteria.</p><h3>Data workload suitability</h3><p>Different out-of-the-box patterns handle transactional (operational) and analytical (historical or aggregated) data with varying degrees of effectiveness. Patterns 1 and 3, which use Amazon Q Business, work with indexed data and are optimized for lookup-style queries against previously indexed content rather than real-time transactional database queries. Pattern 2, which uses Amazon Q in QuickSight, gets visual output for transactional information for ad-hoc analysis. Pattern 4, which uses Amazon Bedrock structured data retrieval, is specifically designed for analytical systems and data warehouses, excelling at complex queries on large datasets. Pattern 5 is a self-managed text-to-SQL option that can be built to support both transactional or analytical needs of users.</p><p>Architectures highlighted in Patterns 1, 2, and 3 (using Amazon Q Business, Amazon Q in QuickSight, or a combination) are best suited for internal enterprise use. However, you can use <a href=\"https://aws.amazon.com/quicksight/embedded-analytics/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon QuickSight Embedded</a> to embed data visuals, dashboards, and natural language queries into both internal or customer-facing applications. Amazon Q Business serves as an enterprise AI assistant for organizational knowledge that uses subscription-based pricing tiers that is designed for internal employees. Pattern 4 (using Amazon Bedrock) can be used to build both internal as well as customer-facing applications. This is because, unlike the subscription-based model of Amazon Q Business, Amazon Bedrock provides API-driven services that alleviate per-user costs and identity management overhead for external customer scenarios. This makes it well-suited for customer-facing experiences where you need to serve potentially thousands of external users. The custom LLM solutions in Pattern 5 can similarly be tailored to external application requirements.</p><h3>Interface and output format</h3><p>Different patterns deliver answers through different interaction models:</p><ul><li><strong>Conversational experiences</strong> – Patterns 1 and 3 (using Amazon Q Business) provide chat-based interfaces. Pattern 4 (using Amazon Bedrock Knowledge Bases for structured data retrieval) naturally supports AI assistant integration, and Pattern 5 (a custom text-to-SQL solution) can be designed for a variety of interaction models.</li><li><strong>Visualization-focused output</strong> – Pattern 2 (using Amazon Q in QuickSight) specializes in generating on-the-fly visualizations such as charts and tables in response to user questions.</li><li> – For embedding capabilities into existing applications, Patterns 4 and 5 offer the most flexible API-based integration options.</li></ul><p>The following figure is a comparison matrix of AWS structured data query patterns.</p><p>Between these patterns, your optimal choice depends on the following key factors:</p><ul><li><strong>Data location and characteristics</strong> – Is your data in operational databases, already in a data warehouse, or distributed across various sources?</li><li><strong>User profile and interaction model</strong> – Are you supporting internal or external users? Do they prefer conversational or visualization-focused interfaces?</li><li><strong>Available resources and expertise</strong> – Do you have ML specialists available, or do you need a fully managed solution?</li><li><strong>Accuracy and governance requirements</strong> – Do you need strictly controlled semantics and curation, or is broader query flexibility acceptable with monitoring?</li></ul><p>By understanding these patterns and their trade-offs, you can architect solutions that align with your business objectives.</p><p><img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/06/20/akshara2.png\" alt=\"\" width=\"100\" height=\"103\"> is a Senior Solutions Architect at Amazon Web Services. She helps commercial customers build cloud-based generative AI services to meet their business needs. She has been designing, developing, and implementing solutions that leverage AI and ML technologies for more than 10 years. Outside of work, she loves painting, exercising and spending time with family.</p><p> is a Generative AI Specialist Solutions Architect at Amazon Web Services. Based in San Francisco, he works with customers to design and build generative AI solutions using large language models and foundation models on AWS. He focuses on helping organizations adopt AI technologies that drive real business value</p>","contentLength":14199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sam Altman Slams Meta's AI Talent Poaching: 'Missionaries Will Beat Mercenaries'","url":"https://www.wired.com/story/sam-altman-meta-ai-talent-poaching-spree-leaked-messages/","date":1751393318,"author":"spenvo","guid":180365,"unread":true,"content":"<p> Altman is hitting back at Meta CEO Mark Zuckerberg’s recent AI talent-poaching spree. In a full-throated response sent to OpenAI researchers Monday evening and obtained by WIRED, Altman made his pitch for why staying at OpenAI is the only answer for those looking to build artificial general intelligence, hinting that the company is evaluating compensation for the entire research organization.</p><p>He also dismissed Meta’s recruiting efforts, saying what the company is doing could lead to deep cultural problems down the road.</p><p>“We have gone from some nerds in the corner to the most interesting people in the tech industry (at least),” he wrote on Slack. “AI Twitter is toxic; Meta is acting in a way that feels somewhat distasteful; I assume things will get even crazier in the future. After I got fired and came back I said that was not the craziest thing that would happen in OpenAl history; certainly neither is this.”</p><p>The news comes on the heels of a major announcement from Zuckerberg. On Monday, the Meta CEO <a href=\"https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/\">sent a memo</a> to staff introducing the company’s new superintelligence team, which will be helmed by Alexandr Wang, formerly of Scale AI, and Nat Friedman, who previously led GitHub. The list of new hires also included a number of <a href=\"https://www.wired.com/story/four-openai-researchers-leave-meta/\">people from OpenAI</a>, including Shengjia Zhao, Shuchao Bi, Jiahui Yu, and Hongyu Ren. OpenAI’s chief research officer, Mark Chen, <a href=\"https://www.wired.com/story/openai-meta-leadership-talent-rivalry/\">told staff</a> that it felt like “someone has broken into our home and stolen something.”</p><p>Altman struck a different tone about the departures in his note on Monday.</p><p>“Meta has gotten a few great people for sure, but on the whole, it is hard to overstate how much they didn't get their top people and had to go quite far down their list; they have been trying to recruit people for a super long time, and I've lost track of how many people from here they've tried to get to be their Chief Scientist,” he wrote. “I am proud of how mission-oriented our industry is as a whole; of course there will always be some mercenaries.”</p><p>He added that “Missionaries will beat mercenaries” and noted that OpenAI is assessing compensation for the entire research organization. “I believe there is much, much more upside to OpenAl stock than Meta stock,” he wrote. “But I think it's important that huge upside comes after huge success; what Meta is doing will, in my opinion, lead to very deep cultural problems. We will have more to share about this soon but it's very important to me we do it fairly and not just for people who Meta happened to target.”</p><p>Altman then made his pitch for people to remain at OpenAI. “I have never been more confident in our research roadmap,” he wrote. “We are making an unprecedented bet on compute, but I love that we are doing it and I'm confident we will make good use of it. Most importantly of all, I think we have the most special team and culture in the world. We have work to do to improve our culture for sure; we have been through insane hypergrowth. But we have the core right in a way that I don't think anyone else quite does, and I'm confident we can fix the problems.”</p><p>“And maybe more importantly than that, we actually care about building AGI in a good way,” he added. “Other companies care more about this as an instrumental goal to some other mission. But this is our top thing, and always will be. Long after Meta has moved on to their next flavor of the week, or defending their social moat, we will be here, day after day, year after year, figuring out how to do what we do better than anyone else. A lot of other efforts will rise and fall too.”</p><p>A number of high-ranking employees who’ve worked at Meta followed up in Slack with their own stories about why OpenAI’s culture is superior. “[T]hey constantly rotate their top focus,” wrote one. Another said: “Yes we’re quirky and weird, but that’s what makes this place a magical cradle of innovation,” wrote one. “OpenAI is weird in the most magical way. We contain multitudes.”</p>","contentLength":4000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44436579"},{"title":"Block3 Unveils Prompt-To-Game AI Engine As Presale Launches","url":"https://hackernoon.com/block3-unveils-prompt-to-game-ai-engine-as-presale-launches?source=rss","date":1751393159,"author":"Chainwire","guid":179344,"unread":true,"content":"<p><strong>SF, CA, July 1st, 2025/Chainwire/--</strong>AI is steadily making deeper inroads into major tech industries, and its latest leap into the gaming sector marks a notable shift.</p><p>Thousands have already backed : to build the world’s most advanced prompt-to-play engine, which will let anyone generate playable worlds in minutes from a simple text prompt. The scale of the impact is already spawning an equally vocal counter-movement, with experts saying this could be the .</p><p>Already being called the “”, Block3 has a solution to one of AI’s ongoing challenges, which is how to get enough data for its LLM to learn and grow.</p><p>By introducing a create2earn element, where users get paid for contributing, Block3 has found a practical route to accelerate product development. To enable this ambitious vision, Block3 is opening a limited-time token presale for just 90 days.</p><p>The BL3 token goes on sale at 9AM UTC today, July 1st, with tokens initially available at just $0.01. Prices increase by 5% every 72 hours, offering early investors gains of 312% by the time it hits major exchanges. The token is only available for purchase through their .</p><h3>What is Block3 and Prompt-to-play Gaming</h3><p>Block3 is made up of a team of developers who are going head-to-head with the gaming industry, by using AI to put creative power in the hands of gamers rather than gaming studios.</p><p>Think about it like an AI chatbot, where the user puts in prompts and gets an answer back from AI. With Block3, the mechanism is the same, except the output is a fully-formed video game, or in essence an immersive world customised to whatever the person typing can imagine.</p><p>This concept is prompting a growing number of users to explore decentralized exchange (DEX) platforms. The potential applications for this technology are wide-ranging.</p><p>For the more technical, the  reveals the complexity behind the concept and reveals the team’s roadmap. This is where the scale of the project is revealed, over a series of ambitious milestones that may see Block3 burgeon from fanatical fanbase to Unicorn over months of intensive development.</p><h3>The Threat to Gaming: Saving a $665 Billion Industry</h3><p>And now, its blockbuster moment may have arrived.</p><p> tackles this issue head-on and has thrown a spanner in the works. Even a 0.16% share of projected market revenue would generate over $1 billion.</p><p>In a statement from the team, they said: “Traditional game dev is dead. For the first time, anyone can build games, not just studios with bloated teams and red tape. We’re here to unlock the imagination of gamers, and if we break a few corporate conglomerates along the way, then so be it.”</p><h3>The BL3 Presale is Now Live</h3><p>The gaming industry shows no sign of stopping, and AI gaming specifically is experiencing an impressive .</p><p>Block3 is well-positioned to benefit from these mega-trends, and with $665 billion of potential revenue up for grabs, there is ample incentive for the developers to scale this as big and fast as they possibly can.</p><p>The concept has already attracted an active community, generating notable attention online. With the presale set at a launch price of $0.01, early participants are positioning themselves ahead of the official rollout.</p><p>Both the crypto and AI sectors are known for rapid innovation, and this project represents one of the more expansive efforts to emerge recently.</p><p> is pioneering a new era in gaming by building the world’s first AI-native prompt-to-play platform. Designed to let anyone generate fully playable game worlds from a simple prompt, Block3 is eliminating the traditional barriers to game creation. By merging generative AI with real-time game logic and deployable environments, it’s changing how games are built and who gets to build them.</p><p>:::tip\nThis story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;.</p>","contentLength":3827,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revolutionizing drug data analysis using Amazon Bedrock multimodal RAG capabilities","url":"https://aws.amazon.com/blogs/machine-learning/revolutionizing-drug-data-analysis-using-amazon-bedrock-multimodal-rag-capabilities/","date":1751393110,"author":"Vivek Mittal","guid":179115,"unread":true,"content":"<p>In the pharmaceutical industry, biotechnology and healthcare companies face an unprecedented challenge for efficiently managing and analyzing vast amounts of drug-related data from diverse sources. Traditional data analysis methods prove inadequate for processing complex medical documentation that includes a mix of text, images, graphs, and tables. <a href=\"https://aws.amazon.com/bedrock/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Bedrock</a> offers features like <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-knowledge-bases-processes-multimodal-data/\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal</a> retrieval, advanced chunking capabilities, and citations to help organizations get high-accuracy responses.</p><p>Pharmaceutical and healthcare organizations process a vast number of complex document formats and unstructured data that pose analytical challenges. Clinical study documents and research papers related to them typically present an intricate blend of technical text, detailed tables, and sophisticated statistical graphs, making automated data extraction particularly challenging. Clinical study documents present additional challenges through non-standardized formatting and varied data presentation styles across multiple research institutions. This post showcases a solution to extract data-driven insights from complex research documents through a sample application with high-accuracy responses. It analyzes clinical trial data, patient outcomes, molecular diagrams, and safety reports from the research documents. It can help pharmaceutical companies accelerate their research process. The solution provides citations from the source documents, reducing hallucinations and enhancing the accuracy of the responses.</p><p>The sample application uses Amazon Bedrock to create an intelligent AI assistant that analyzes and summarizes research documents containing text, graphs, and unstructured data. Amazon Bedrock is a fully managed service that offers a choice of industry-leading foundation models (FMs) along with a broad set of capabilities to build generative AI applications, simplifying development with security, privacy, and responsible AI.</p><p>To equip FMs with up-to-date and proprietary information, organizations use <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\" target=\"_blank\" rel=\"noopener noreferrer\">Retrieval Augmented Generation</a> (RAG), a technique that fetches data from company data sources and enriches the prompt to provide relevant and accurate responses.</p><p><a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Bedrock Knowledge Bases</a> is a fully managed RAG capability within Amazon Bedrock with in-built session context management and source attribution that helps you implement the entire RAG workflow, from ingestion to retrieval and prompt augmentation, without having to build custom integrations to data sources and manage data flows.</p><p>Amazon Bedrock Knowledge Bases introduces powerful document parsing capabilities, including <a href=\"https://aws.amazon.com/bedrock/bda/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Bedrock Data Automation</a> powered parsing and FM parsing, revolutionizing how we handle complex documents. Amazon Bedrock Data Automation is a fully managed service that processes multimodal data effectively, without the need to provide additional prompting. The FM option parses multimodal data using an FM. This parser provides the option to customize the default prompt used for data extraction. This advanced feature goes beyond basic text extraction by intelligently breaking down documents into distinct components, including text, tables, images, and metadata, while preserving document structure and context. When working with supported formats like PDF, specialized FMs interpret and extract tabular data, charts, and complex document layouts. Additionally, the service provides advanced chunking strategies like semantic chunking, which intelligently divides text into meaningful segments based on semantic similarity calculated by the embedding model. Unlike traditional syntactic chunking methods, this approach preserves the context and meaning of the content, improving the quality and relevance of information retrieval.</p><p>The solution architecture implements these capabilities through a seamless workflow that begins with administrators securely uploading knowledge base documents to an <a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Simple Storage Service</a> (Amazon S3) bucket. These documents are then ingested into Amazon Bedrock Knowledge Bases, where a large language model (LLM) processes and parses the ingested data. The solution employs <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking.html\" target=\"_blank\" rel=\"noopener noreferrer\">semantic chunking</a> to store document embeddings efficiently in <a href=\"https://aws.amazon.com/opensearch-service/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon OpenSearch Service</a> for optimized retrieval. The solution features a user-friendly interface built with <a href=\"https://streamlit.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Streamlit</a>, providing an intuitive chat experience for end-users. When users interact with the Streamlit application, it triggers <a href=\"http://aws.amazon.com/lambda\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Lambda</a> functions that handle the requests, retrieving relevant context from the knowledge base and generating appropriate responses. The architecture is secured through <a href=\"https://aws.amazon.com/iam/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Identity and Access Management</a> (IAM), maintaining proper access control throughout the workflow. Amazon Bedrock uses <a href=\"https://aws.amazon.com/kms/\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Key Management Service</a> (AWS KMS) to encrypt resources related to your knowledge bases. By default, Amazon Bedrock encrypts this data using an AWS managed key. Optionally, you can encrypt the model artifacts using a <a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk\" target=\"_blank\" rel=\"noopener noreferrer\">customer managed key</a>. This end-to-end solution provides efficient document processing, context-aware information retrieval, and secure user interactions, delivering accurate and comprehensive responses through a seamless chat interface.</p><p>The following diagram illustrates the solution architecture.</p><p>This solution uses the following additional services and features:</p><ul><li>The <a href=\"https://aws.amazon.com/bedrock/claude/\" target=\"_blank\" rel=\"noopener noreferrer\">Anthropic Claude 3 family</a> offers Opus, Sonnet, and Haiku models that accept text, image, and video inputs and generate text output. They provide a broad selection of capability, accuracy, speed, and cost operation points. These models understand complex research documents that include charts, graphs, tables, diagrams, and reports.</li><li><a href=\"http://aws.amazon.com/lambda\" target=\"_blank\" rel=\"noopener noreferrer\">AWS Lambda</a> is a serverless computing service that empowers you to run code without provisioning or managing servers cost effectively.</li><li><a href=\"https://aws.amazon.com/s3/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon S3</a> is a highly scalable, durable, and secure object storage service.</li><li><a href=\"https://aws.amazon.com/opensearch-service/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon OpenSearch Service</a> is a fully managed search and analytics engine for efficient document retrieval. The OpenSearch Service vector database capabilities enable semantic search, RAG with LLMs, recommendation engines, and search rich media.</li><li><a href=\"https://streamlit.io/\" target=\"_blank\" rel=\"noopener noreferrer\">Streamlit</a> is a faster way to build and share data applications using interactive web-based data applications in pure Python.</li></ul><p>The following prerequisites are needed to proceed with this solution. For this post, we use the us-east-1 AWS Region. For details on available Regions, see <a href=\"https://docs.aws.amazon.com/general/latest/gr/bedrock.html\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon Bedrock endpoints and quotas</a>.</p><p>Refer to the <a href=\"https://github.com/aws-samples/samples-for-agentic-rag/tree/main/advanced-rag-assistant\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repository</a> for the deployment steps listed under the deployment guide section. We use an <a href=\"http://aws.amazon.com/cloudformation\" target=\"_blank\" rel=\"noopener noreferrer\">AWS CloudFormation</a> template to deploy solution resources, including S3 buckets to store the source data and knowledge base data.</p><h2>Test the sample application</h2><p>Imagine you are a member of an R&amp;D department for a biotechnology firm, and your job requires you to derive insights from drug- and vaccine-related information from diverse sources like research studies, drug specifications, and industry papers. You are performing research on cancer vaccines and want to gain insights based on cancer research publications. You can upload the documents given in the reference section to the S3 bucket and sync the knowledge base. Let’s explore example interactions that demonstrate the application’s capabilities. The responses generated by the AI assistant are based on the documents uploaded to the S3 bucket connected with the knowledge base. Due to non-deterministic nature of machine learning (ML), your responses might be slightly different from the ones presented in this post.</p><h3>Understanding historical context</h3><p>We use the following query: “Create a timeline of major developments in mRNA vaccine technology for cancer treatment based on the information provided in the historical background sections.”The assistant analyzes multiple documents and presents a chronological progression of mRNA vaccine development, including key milestones based on the chunks of information retrieved from the OpenSearch Service vector database.</p><p>The following screenshot shows the AI assistant’s response.</p><p>We use the following query: “Synthesize the information from the text, figures, and tables to provide a comprehensive overview of the current state and future prospects of therapeutic cancer vaccines.”</p><p>The AI assistant is able to provide insights from complex data types, which is enabled by FM parsing, while ingesting the data to OpenSearch Service. It is also able to provide images in the source attribution using the <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-knowledge-bases-processes-multimodal-data/\" target=\"_blank\" rel=\"noopener noreferrer\">multimodal data capabilities</a> of Amazon Bedrock Knowledge Bases.</p><p>The following screenshot shows the AI assistant’s response.</p><p>The following screenshot shows the visuals provided in the citations when the mouse hovers over the question mark icon.</p><p>We use the following query: “Compare the efficacy and safety profiles of MAGE-A3 and NY-ESO-1 based vaccines as described in the text and any relevant tables or figures.”</p><p>The AI assistant used the semantically similar chunks returned from the OpenSearch Service vector database and added this context to the user’s question, which enabled the FM to provide a relevant answer.</p><p>The following screenshot shows the AI assistant’s response.</p><p>We use the following query: <em>“Summarize the potential advantages of mRNA vaccines over DNA vaccines for targeting tumor angiogenesis, as described in the review.”</em></p><p>With the semantic chunking feature of the knowledge base, the AI assistant was able to get the relevant context from the OpenSearch Service database with higher accuracy.</p><p>The following screenshot shows the AI assistant’s response.</p><p>The following screenshot shows the diagram that was used for the answer as one of the citations.</p><p>The sample application demonstrates the following:</p><ul><li>Accurate interpretation of complex scientific diagrams</li><li>Precise extraction of data from tables and graphs</li><li>Context-aware responses that maintain scientific accuracy</li><li>Source attribution for provided information</li><li>Ability to synthesize information across multiple documents</li></ul><p>This application can help you quickly analyze vast amounts of complex scientific literature, extracting meaningful insights from diverse data types while maintaining accuracy and providing proper attribution to source materials. This is enabled by the advanced features of the knowledge bases, including FM parsing, which aides in interpreting complex scientific diagrams and extraction of data from tables and graphs, semantic chunking, which aides with high-accuracy context-aware responses, and multimodal data capabilities, which aides in providing relevant images as source attribution.</p><p>The proposed solution accelerates the time to value of the project development process. Solutions built on the AWS Cloud benefit from inherent scalability while maintaining robust security and privacy controls.</p><p>The security and privacy framework includes fine-grained user access controls using IAM for both OpenSearch Service and Amazon Bedrock services. In addition, Amazon Bedrock enhances security by providing encryption at rest and in transit, and private networking options using virtual private cloud (VPC) endpoints. Data protection is achieved using KMS keys, and API calls and usage are tracked through <a href=\"https://aws.amazon.com/cloudwatch/\" target=\"_blank\" rel=\"noopener noreferrer\">Amazon CloudWatch</a> logs and metrics. For specific compliance validation for Amazon Bedrock, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/compliance-validation.html\" target=\"_blank\" rel=\"noopener noreferrer\">Compliance validation for Amazon Bedrock</a>.</p><p>Complete the following steps to clean up your resources.</p><ol><li>Empty the  and <code>KnowledgeBaseS3BucketName</code> buckets.</li><li>Delete the main CloudFormation stack.</li></ol><p>This post demonstrated the powerful multimodal document analysis (text, graphs, images) using <a href=\"https://community.aws/content/2jU5zpqh4cal0Lm47MBdRmKLLJ5/a-developer-s-guide-to-advanced-chunking-and-parsing-with-amazon-bedrock?lang=en\" target=\"_blank\" rel=\"noopener noreferrer\">advanced parsing and chunking features</a> of Amazon Bedrock Knowledge Bases. By combining the powerful capabilities of Amazon Bedrock FMs, OpenSearch Service, and intelligent chunking strategies through Amazon Bedrock Knowledge Bases, organizations can transform their complex research documents into searchable, actionable insights. The integration of semantic chunking makes sure that document context and relationships are preserved, and the user-friendly Streamlit interface makes the system accessible to end-users through an intuitive chat experience. This solution not only streamlines the process of analyzing research documents, but also demonstrates the practical application of AI/ML technologies in enhancing knowledge discovery and information retrieval. As organizations continue to grapple with increasing volumes of complex documents, this scalable and intelligent system provides a robust framework for extracting maximum value from their document repositories.</p><p>Although our demonstration focused on the healthcare industry, the versatility of this technology extends beyond a single industry. RAG on Amazon Bedrock has proven its value across diverse sectors. Notable adopters include global brands like <a href=\"https://aws.amazon.com/solutions/case-studies/georgia-pacific-optimizes-operator-efficiency-case-study/?did=cr_card&amp;trk=cr_card\" target=\"_blank\" rel=\"noopener noreferrer\">Adidas</a> in retail, <a href=\"https://aws.amazon.com/solutions/case-studies/empolis/?did=cr_card&amp;trk=cr_card\" target=\"_blank\" rel=\"noopener noreferrer\">Empolis</a> in information management, <a href=\"https://aws.amazon.com/solutions/case-studies/fractal-analytics-case-study/?did=cr_card&amp;trk=cr_card\" target=\"_blank\" rel=\"noopener noreferrer\">Fractal Analytics</a> in AI solutions, <a href=\"https://aws.amazon.com/solutions/case-studies/georgia-pacific-optimizes-operator-efficiency-case-study/?did=cr_card&amp;trk=cr_card\" target=\"_blank\" rel=\"noopener noreferrer\">Georgia Pacific</a> in manufacturing, and <a href=\"https://aws.amazon.com/solutions/case-studies/nasdaq-video-case-study/?did=cr_card&amp;trk=cr_card\" target=\"_blank\" rel=\"noopener noreferrer\">Nasdaq</a> in financial services. These examples illustrate the broad applicability and transformative potential of RAG technology across various business domains, highlighting its ability to drive innovation and efficiency in multiple industries.</p><p>Refer to the <a href=\"https://github.com/aws-samples/samples-for-agentic-rag/tree/main/advanced-rag-assistant\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub repo</a> for the agentic RAG application, including samples and components for building agentic RAG solutions. Be on the lookout for additional features and samples in the repository in the coming months.</p><p>To learn more about Amazon Bedrock Knowledge Bases, check out the <a href=\"https://catalog.us-east-1.prod.workshops.aws/workshops/c6b88897-84a7-4885-b9f0-855e2fc61378\" target=\"_blank\" rel=\"noopener noreferrer\">RAG workshop using Amazon Bedrock</a>. Get started with Amazon Bedrock Knowledge Bases, and let us know your thoughts in the comments section.</p><p>is a Solution Architect at Amazon Web Services, where he helps organizations architect and implement cutting-edge cloud solutions. With a deep passion for Generative AI, Machine Learning, and Serverless technologies, he specializes in helping customers harness these innovations to drive business transformation. He finds particular satisfaction in collaborating with customers to turn their ambitious technological visions into reality.</p><p>, serving as a Senior AI/ML Solutions Architect in the Global Healthcare and Life Sciences division at Amazon Web Services (AWS), has a keen focus on Generative AI. He assists customers in integrating Generative AI into their projects, emphasizing the importance of explainability within their AI-driven initiatives. Beyond his professional commitments, Shamika passionately pursues skiing and off-roading adventures.</p><p> is a Sr. Solutions Architect, specializes in architecting enterprise-scale cloud solutions with focus on Analytics, Generative AI and emerging technologies. His technical expertise is validated by his achievement of all 12 AWS certifications and the prestigious Golden jacket recognition. He has a passion to architect and implement innovative cloud solutions that drive business transformation. He speaks at major industry events like AWS re:Invent and regional AWS Summits, where he shares insights on cloud architecture and emerging technologies.</p>","contentLength":14921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cloudflare Flips AI Scraping Model With Pay-Per-Crawl System For Publishers","url":"https://tech.slashdot.org/story/25/07/01/1745245/cloudflare-flips-ai-scraping-model-with-pay-per-crawl-system-for-publishers?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751392800,"author":"msmash","guid":179117,"unread":true,"content":"Cloudflare today announced a \"Pay Per Crawl\" program that allows website owners to charge AI companies for accessing their content, a potential revenue stream for publishers whose work is increasingly being scraped to train AI models. The system uses HTTP response code 402 to enable content creators to set per-request prices across their sites. Publishers can choose to allow free access, require payment at a configured rate, or block crawlers entirely. \n\nWhen an AI crawler requests paid content, it either presents payment intent via request headers for successful access or receives a \"402 Payment Required\" response with pricing information. Cloudflare acts as the merchant of record and handles the underlying technical infrastructure. The company aggregates billing events, charges crawlers, and distributes earnings to publishers. \n\nAlongside Pay Per Crawl, Cloudflare has switched to blocking AI crawlers by default for its customers, becoming the first major internet infrastructure provider to require explicit permission for AI access. The company handles traffic for 20% of the web and more than one million customers have already activated its AI-blocking tools since their September 2024 launch, it wrote in a blog post.","contentLength":1237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"National Guard Troops Sent To California By Trump Are Just Out There Doing Drug Busts","url":"https://www.techdirt.com/2025/07/01/national-guard-troops-sent-to-california-by-trump-are-just-out-there-doing-drug-busts/","date":1751392792,"author":"Tim Cushing","guid":179138,"unread":true,"content":"<p><a href=\"https://www.techdirt.com/2025/06/12/a-manufactured-crisis-how-a-few-hooligans-in-la-became-the-pretext-for-military-rule/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/12/a-manufactured-crisis-how-a-few-hooligans-in-la-became-the-pretext-for-military-rule/\">Martial law</a>? <a href=\"https://www.techdirt.com/2025/06/09/trump-administration-sends-marines-to-site-of-anti-ice-protests/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/09/trump-administration-sends-marines-to-site-of-anti-ice-protests/\">Police state</a>? These are just things the alleged Leader of Free World , rather than things a nation founded on rejecting these options should be <a href=\"https://www.techdirt.com/2025/06/11/the-gop-is-way-too-fucking-excited-about-using-us-troops-on-american-protestors/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/11/the-gop-is-way-too-fucking-excited-about-using-us-troops-on-american-protestors/\">in the process</a> of instituting. And yet, here we are, barely six months into Trump’s return to office, staring down the barrel of both of these related horrors.</p><p>If it looks like fascism, it’s probably not intentional. Trump simply isn’t smart enough to implement the real thing. But he does like authoritarianism, which looks a lot like fascism, because he’s always felt a president should be treated like a king — someone who answers to no one, not even his 340 million employers. </p><p>Trump tested the waters on martial law during his last term, threatening to send troops out to handle George Floyd protests. This time around, he’s <a href=\"https://www.techdirt.com/2025/06/11/lets-be-clear-the-rioting-in-la-is-by-the-cops-not-the-protestors/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/11/lets-be-clear-the-rioting-in-la-is-by-the-cops-not-the-protestors/\">amped everything up</a>, openly <a href=\"https://www.techdirt.com/2025/06/12/noem-announces-military-will-liberate-la-from-democracy-then-watches-security-throw-senator-to-ground/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/12/noem-announces-military-will-liberate-la-from-democracy-then-watches-security-throw-senator-to-ground/\">hoping to turn</a> every “Democrat” city into <a href=\"https://www.history.com/this-day-in-history/may-4/national-guard-kills-four-at-kent-state\" data-type=\"link\" data-id=\"https://www.history.com/this-day-in-history/may-4/national-guard-kills-four-at-kent-state\">Kent State</a>.</p><p>Legally, <a href=\"https://www.techdirt.com/2025/06/13/judge-no-donald-trump-cant-just-order-the-national-guard-to-invade-los-angeles/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/13/judge-no-donald-trump-cant-just-order-the-national-guard-to-invade-los-angeles/\">he’s not allowed to do this</a>. But his administration is relying on some vagueness in the law to get around the long-standing prohibition of sending in the army (so to speak) to police the populace. So, we get the sort of thing we’ve seen recently, where a <a href=\"https://www.techdirt.com/2025/06/25/ices-military-style-raid-of-a-los-angeles-flea-market-netted-only-two-arrests/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/25/ices-military-style-raid-of-a-los-angeles-flea-market-netted-only-two-arrests/\">Los Angeles swap meet</a> was treated like an open-air market in some Middle Eastern country we’re currently at (undeclared) war with.</p><p>The National Guard troops sent to Los Angeles are presumably still working without pay and/or beds, but that isn’t stopping them from blending in with federal law enforcement to aid and abet actual law enforcement work. <a href=\"https://www.cbsnews.com/losangeles/news/500-federal-agents-and-soldiers-raid-marijuana-farms-in-rural-southern-california/\" data-type=\"link\" data-id=\"https://www.cbsnews.com/losangeles/news/500-federal-agents-and-soldiers-raid-marijuana-farms-in-rural-southern-california/\">First reported by CBS</a>, a combined force of more than 500 federal officers and National Guard troops walked away from the ICE raids and the protection of federal property to perform a bog standard drug bust. <a href=\"https://taskandpurpose.com/news/national-guard-drug-busts-la-area/\" data-type=\"link\" data-id=\"https://taskandpurpose.com/news/national-guard-drug-busts-la-area/\">Nicholas Slayton has more details for Task and Purpose</a>, a military-oriented publication: </p><blockquote><p><em>California National Guard soldiers operating under federal orders helped the Drug Enforcement Administration and other federal personnel carry out a raid on a large marijuana growth operation in the eastern Coachella Valley last week, 130 miles from downtown Los Angeles.&nbsp;</em></p><p><em>It’s unclear how many National Guard troops participated in the operation, but the force totalled roughly 500 people. According to the DEA, other agencies included Customs and Border Patrol, Bureau of Alcohol, Tobacco, Firearms and Explosives, Immigration and Custom Enforcement and the Federal Bureau of Investigation.</em></p></blockquote><p>While this commandeering of California National Guard troops may have originally been for the unstated purpose of pushing back against anti-ICE protests, now that they’re here, the administration has decided to just use them for whatever. This raid of multiple marijuana farms occurred more than  from the boundaries of Los Angeles County and even further away from the location these troops were originally sent: downtown Los Angeles. </p><p>According to Trump’s military, everything about this is good and fine and nothing to be concerned about. After all, the law says the military can help federal cops, even if it (supposedly) prevents them from doing actual cop work. That’s the <a href=\"https://en.wikipedia.org/wiki/Title_10_of_the_United_States_Code\" data-type=\"link\" data-id=\"https://en.wikipedia.org/wiki/Title_10_of_the_United_States_Code\">Title 10</a> vagueness the military is relying on when it serves up statements like this” </p><blockquote><p><em>“The catalyst of this order was related to events occurring in Los Angeles; however, the president’s order and NORTHCOM’s mission is not constrained by the geography of Southern California. Recently, Title 10 forces supported a Drug Enforcement Agency operation a few hours outside of Los Angeles. Title 10 forces protect federal personnel who are performing federal law enforcement functions…”</em></p></blockquote><p><a href=\"https://www.dvidshub.net/image/9123828/california-national-guard-soldiers-support-federal-operation-southern-california\" data-type=\"link\" data-id=\"https://www.dvidshub.net/image/9123828/california-national-guard-soldiers-support-federal-operation-southern-california\">Hence the military-provided shots</a> of alleged National Guard troops allegedly manning the perimeter of the places being raided. And, also hence, the narrative no one can definitively dispute because — despite the National Guard embedding with federal law enforcement agencies — no journalists are being allowed to embed with military-esque operations occurring  the borders of the United States.</p><p>Of course, we’ve already seen <a href=\"https://www.techdirt.com/2025/06/20/us-marines-witnessed-detaining-a-us-citizen-in-los-angeles/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2025/06/20/us-marines-witnessed-detaining-a-us-citizen-in-los-angeles/\">Marines detain people</a> for the purpose of handing them over to law enforcement. And we’ve seen National Guard troops swarm a swap meet like they’re looking for terrorists in a foreign country, rather than just anyone looking kind of Hispanic who might not have the proper paperwork on them. </p><p>The more things like this occur, the more easily many people will just come to accept this is the way the United States operates now. Many of them will cheer on these efforts, failing to recognize the abuse of these powers may, at some point, target them. But for the rest of us, this shouldn’t be allowed to pass without notice. Trump may be a blowhard and an idiot, but he’s surrounded by people who truly desire an opportunity to perform a hard reset on democracy and its principles, replacing it with jackboot heels, racism, fascism, and — eventually — a return of the British Empire, this time wrapped in an American flag.</p>","contentLength":4929,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Introduction to Remote Model Context Protocol Servers","url":"https://towardsdatascience.com/an-introduction-to-remote-model-context-protocol-servers/","date":1751392784,"author":"Thomas Reid","guid":179136,"unread":true,"content":"<p>Writing, testing and using them.</p>","contentLength":32,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Daily Deal: Academy of Educational Engineering","url":"https://www.techdirt.com/2025/07/01/daily-deal-academy-of-educational-engineering-3/","date":1751392380,"author":"Daily Deal","guid":179137,"unread":true,"content":"<p>The <a href=\"https://deals.techdirt.com/sales/academy-of-educational-engineering-lifetime-access?utm_campaign=affiliaterundown\">Academy of Educational Engineering</a> is a premier platform tailored for aspiring and professional geeks. This all-in-one educational ecosystem is designed to empower you with expert-level knowledge and hands-on experience across embedded systems, electronics, IoT, and software development. As a premium member, you’ll access comprehensive tools, engaging projects, personalized feedback, and direct mentorship, helping you elevate your career in the tech industry. Whether you’re a beginner or a professional, this is your ultimate gateway to mastering the future of technology. It’s on sale for $50.</p><p><em>Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.</em></p>","contentLength":820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nothing releases its first over-the-ear headphones","url":"https://techcrunch.com/video/nothing-releases-its-first-over-the-ear-headphones/","date":1751392350,"author":"TC Video","guid":179114,"unread":true,"content":"<article>Nothing has revealed its first over-the-ear headphones. The aluminum and transparent Headphone (1) was designed in collaboration with KEF. They offer adaptive noise canceling, bass enhancement for deeper low frequencies, and spatial audio, for $299 at launch. But spoiler alert: The buttons on these headphones might be the best part.</article>","contentLength":334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Midas And 0G Partner To Bring Real-World Assets To AI-Native Blockchain Infrastructure","url":"https://hackernoon.com/midas-and-0g-partner-to-bring-real-world-assets-to-ai-native-blockchain-infrastructure?source=rss","date":1751392178,"author":"Chainwire","guid":179343,"unread":true,"content":"<p>Singapore, Republic of Singapore, July 1st, 2025/Chainwire/--Tokenization protocol Midas and AI blockchain 0G have announced a strategic partnership to unlock the next wave of onchain finance through modular design. By combining Midas’ tokenisation infrastructure with 0G’s decentralized AI-native compute, the partners will develop new solutions that intelligently leverage real-world assets (RWAs).</p><p>As part of the partnership, Midas will deploy on the 0G mainnet, scheduled for late Q3 2025, bringing its full stack of tokenization infrastructure. In parallel, 0G will integrate Midas’ tokenized instruments and vault logic into its optimized AI layer. This will position both platforms to serve institutions, developers, and liquidity providers at scale.</p><p>Midas offers a compliant protocol suite for issuing tokenized certificates tracking institutional-grade strategies. Its tokens, including mF-ONE, mMEV, mEDGE, mRE7YIELD, mBASIS and mTBILL, provide exposure via tokenized certificates to reference real-world assets across private credit, US short-term treasuries, and market-neutral strategies.</p><blockquote><p>0G Labs CEO Michael Heinrich said: “Midas have made huge strides in expanding compliant access to tokenized RWAs and we’re delighted that they’ve chosen to build on 0G. We’re excited to be collaborating with them to develop new financial products that will combine AI with tokenized assets, giving users greater onchain opportunities than ever before.” </p></blockquote><p>By launching on 0G, Midas will introduce compliant, composable tokens into a modular environment optimized for AI-powered workflows and smart contract automation. Use cases range from onchain lending vaults and automated credit exposures to AI-enhanced risk analytics and composable strategy deployment. 0G’s modular Layer 1 blockchain is purpose-built for AI-native applications.</p><p>It combines high-performance compute, decentralized storage, data availability, and low-latency smart contract execution, ideal for deploying data-intensive financial applications and real-time DeFi logic. 0G’s architecture supports seamless integration with EVM and non-EVM ecosystems, while its recent Galileo testnet demonstrated sustained throughput and low gas costs. </p><p>It also saw significant developer adoption with over 170 million transactions and 13 million accounts in under two months. The collaboration between Midas and 0G reflects a shared vision: to make programmable, compliant financial infrastructure natively interoperable with the AI applications of the future.</p><p>Midas is a tokenisation platform building institutional-grade financial products for the open web. Its ERC-20 tokens are structured to track dedicated strategies with verifiable on-chain performance, combining TradFi-grade standards with DeFi composability. Midas is backed by leading investors like Framework Ventures, BlockTower Capital, and GSR, and partners with regulated custodians to ensure strong compliance and risk controls. Learn more: <a href=\"https://midas.app/\">https://midas.app/</a></p><p>About 0G 0G is the first decentralized AI protocol (AIP), purpose-built to power a truly democratized future of intelligence. As a modular and infinitely scalable Layer 1, 0G enables the execution of decentralized AI applications at scale. It unifies high-performance decentralized storage, compute, and data availability (DA) to support the next generation of AI-native use cases. With verifiable AI processing and a permissionless agent ecosystem, 0G is laying the foundation for an open and unstoppable AI economy. Learn more: <a href=\"https://0g.ai/\">https://0g.ai/</a></p><p>Disclaimer This announcement is for informational purposes only and does not constitute investment advice or an offer to sell or buy any financial instrument. Midas-issued tokens are not available to US &amp; UK persons and entities, or those from sanctioned jurisdictions. This is not investment advice.</p><p>:::tip\nThis story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;.</p>","contentLength":3946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Keynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=hERRANApN5c","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179127,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb</article>","contentLength":476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/hERRANApN5c?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Con... Gordon Radlein","url":"https://www.youtube.com/watch?v=dlDTX-aDNzg","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179128,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Contribution - Gordon Radlein, Datadog\n\nOpenTelemetry has accelerated the commoditization of instrumentation. Telemetry generation is becoming a solved problem, an implementation detail. But this has created a new challenge: a wealth of standardized signals with no standard meaning. Different systems instrumented with different semantics generating telemetry in their own unique language. And while signal correlation connects specific workloads, it fails when we need to understand our systems at a macro scale by joining disparate datasets.\nThat is, until we all agreed to speak the same language.\n\nJust as English as a lingua franca fueled progress across the internet, OpenTelemetry Semantic Conventions are providing a shared language for our systems. In this talk we’ll discuss why semantic interoperability is the real connective tissue, how it’s fueling deeper insights into our production environments, and the key role it plays in enabling the AI systems that are rapidly ushering in the next revolution of our industry.</article>","contentLength":1500,"flags":null,"enclosureUrl":"https://www.youtube.com/v/dlDTX-aDNzg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Welcome + Opening Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=_rqgWHaEvgc","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179129,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nWelcome + Opening Remarks - Austin Parker, Honeycomb</article>","contentLength":435,"flags":null,"enclosureUrl":"https://www.youtube.com/v/_rqgWHaEvgc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere","url":"https://www.youtube.com/watch?v=Z4umnlRdLtA","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179130,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere\n\nLogs can get very expensive and often how useful all those logs are is unknown, some are but many are not. It is very difficult to know which logs are useful and how exactly they are used. With Chronosphere's Control plane for logs users can now get a comprehensive analysis of value and usage patterns, along with sophisticated recommendations and control actions that allow some or most of the value derived from those logs to be preserved. In order to achieve our goals we have enhanced Fluent Bit to be more flexible in which logs are actioned upon and will share useful future additions to it.</article>","contentLength":1072,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Z4umnlRdLtA?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson","url":"https://www.youtube.com/watch?v=J_hHiwa_3QU","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179131,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson, Chronosphere\n\nHybrid cloud isn’t a stepping stone—it’s a destination. With 39% of CNCF survey respondents already operating in hybrid environments, this model is here to stay. But as teams pursue cloud-native architectures, many skip a critical step: developing a clear cloud strategy and an observability approach to match.\nThe result is predictable— widening visibility gaps, redundant tooling and data, and spiraling costs as teams try to stitch together disconnected, vendor-specific systems never meant to work in concert. Hybrid environments expose these issues quickly, especially when workloads span multiple platforms without a unified way to observe and understand them.\nModernization efforts demand open observability from the start—not as an add-on. Technologies like OpenTelemetry, Fluent Bit, and Prometheus act as connective tissue across clouds, clusters, and on-prem infrastructure, enabling standardization where it’s needed most.\nThis talk outlines how to center open observability in your modernization journey: where to standardize architectural layers, how to maintain a more open approach, and why these decisions have long-term payoff. \nHybrid complexity is inevitable. Leading with open observability is how you stay in control—now and in the future.</article>","contentLength":1761,"flags":null,"enclosureUrl":"https://www.youtube.com/v/J_hHiwa_3QU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data I... Dotan Horovits","url":"https://www.youtube.com/watch?v=C5Y3qnEJSY8","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179132,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data Insights - Dotan Horovits, AWS OpenSearch</article>","contentLength":505,"flags":null,"enclosureUrl":"https://www.youtube.com/v/C5Y3qnEJSY8?version=3","enclosureMime":"","commentsUrl":null},{"title":"Building Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Per... Denton Krietz","url":"https://www.youtube.com/watch?v=zgnY8szpKUw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179118,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nBuilding Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Persistent Queue - Denton Krietz, Bindplane\n\nThe OpenTelemetry Collector’s persistent queue provides a robust mechanism for handling data bursts, destination outages, and processing delays, ensuring no telemetry data is lost—but from experience, it’s consistently one of the collector's least understood features.\n\nIn this talk, we’ll explore the inner workings of the OTel Collector’s persistent queue, including how it buffers data, ensures durability, and enables replay after failures. Attendees will learn how to configure persistent queues for their unique workloads, optimize their telemetry pipeline performance, and troubleshoot common pitfalls.\n\nWhether you’re a site reliability engineer, developer, or observability enthusiast, this talk will equip you with the knowledge to deeply understand persistent queues to optimize your telemetry pipeline in production.</article>","contentLength":1348,"flags":null,"enclosureUrl":"https://www.youtube.com/v/zgnY8szpKUw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Introducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner & Ray Jenkins, Streamfold","url":"https://www.youtube.com/watch?v=xeQnP8Ct7qY","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179119,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nIntroducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner &amp; Ray Jenkins, Streamfold\n\nIn this talk, we'll introduce Rotel—an open-source OpenTelemetry collector built in Rust. Rotel is lightweight and resource-efficient, integrating seamlessly into your development workflow. Its compact design lets you package it with your Python or NodeJS projects, so telemetry collection runs alongside your code without needing additional sidecars.\n\nWe'll explore how rethinking telemetry collection at the edge can empower developers right from the early stages of development, paving the way for broader OpenTelemetry adoption. You’ll learn how Rust’s low-overhead FFI enables native extensions for telemetry filtering, transformation, and enrichment using Python and Typescript.\n\nBy leveraging Rust’s performance strengths, Rotel avoids the overhead of garbage collection, resulting in lower memory usage and reduced latency. Its quick cold start times make it a natural fit for modern cloud-native, serverless, and edge computing environments. Join us to discover how moving telemetry collection closer to the source can help you analyze high-volume, high-fidelity signals more effectively.</article>","contentLength":1585,"flags":null,"enclosureUrl":"https://www.youtube.com/v/xeQnP8Ct7qY?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTele... Diana Todea","url":"https://www.youtube.com/watch?v=wWON2NT41lE","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179120,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTelemetry - Diana Todea, Aircall\n\nBecoming a contributor to an open-source project is a transformative step in any developer's career. This session explores the journey from first-time contributor to active developer, covering best practices for navigating project communities, understanding codebases, and making meaningful contributions. Learn strategies for selecting the right project, mastering collaboration tools, and embracing the culture of open-source development. The audience will be inspired about my one year journey with the open source project OpenTelemetry and how I have built a proof of concept for it and achieved developer status for this project. By the end of this talk, the public will gain insights into the tools to become a better developer and how to build more engagement with the community.</article>","contentLength":1284,"flags":null,"enclosureUrl":"https://www.youtube.com/v/wWON2NT41lE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Telemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchma... Henrik Rexed","url":"https://www.youtube.com/watch?v=tZho5W9L_Z8","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179121,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nTelemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchmark Analysis - Henrik Rexed, Dynatrace\n\nIn a push to standardize observability practices, the cloud-native community has embraced OpenTelemetry, offering a unified framework for metrics, logs, and traces. Prior to this, log processing relied on agents like fluent, evolving into fluentbit. With fluentbit's recent expansion to support additional signals and the OpenTelemetry Collector's emergence, a pertinent question arises: Which is the superior choice for performance?\n\nThis session delves into:\n- Unveiling the distinctions between Fluent Bit and the OpenTelemetry Collector.\n- Sharing the findings derived from a series of benchmark tests.\n- Providing valuable insights to empower the community in selecting the most fitting agent for their cloud-native environments.</article>","contentLength":1240,"flags":null,"enclosureUrl":"https://www.youtube.com/v/tZho5W9L_Z8?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Spec-tacular Game Show - Liudmila Molkova, Ted Young, Tyler Helmuth, Jamie Danielson, Alex Boten","url":"https://www.youtube.com/watch?v=ipFVu0dl5Bw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179122,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPanel: The Spec-tacular Game Show - Liudmila Molkova, Microsoft; Ted Young, Grafana Labs; Tyler Helmuth, Jamie Danielson &amp; Alex Boten, Honeycomb\n\nFrom OTLP to OTTL, engineers are excited about a lot of things. But there is one thing that excites them above all else and that is correcting people. Welcome to “The Spec-tacular Game Show”.\n\nIn this fun game show our panelists will be given incorrect statements about the OpenTelemetry Specification or Semantic Convention. The panelists will buzz in, identify what’s wrong, and state the correction. If none of the panelists know the answer the audience will get a chance to answer to steal the point. The panelist (or audience) with the most points wins!\n\nAfter each question we’ll spend a time explaining why the Spec and Semconv is the way it is and highlight how it produces the production-quality telemetry you know and love. Join us for a fun, relaxing, (snarky) panel about everyone’s favorite part of Otel!</article>","contentLength":1356,"flags":null,"enclosureUrl":"https://www.youtube.com/v/ipFVu0dl5Bw?version=3","enclosureMime":"","commentsUrl":null},{"title":"How To Think About Instrumentation Overhead - Jason Plumb, Splunk","url":"https://www.youtube.com/watch?v=fvmzAX_ZyvM","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179123,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nHow To Think About Instrumentation Overhead - Jason Plumb, Splunk\n\nNovice observability practitioners are often overly obsessed with performance. They might approach instrumentation with skepticism and have concerns about latency degradation or resource consumption. This talk is a primer on the topic of instrumentation overhead, and it will teach you how to think about overhead in an observability context. We will cover the causes of overhead and why overhead is so hard to measure and even harder to predict reliably. Lastly, we will present some practical techniques for understanding overhead in your environment and some strategies for coping with it.</article>","contentLength":1042,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fvmzAX_ZyvM?version=3","enclosureMime":"","commentsUrl":null},{"title":"No Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft","url":"https://www.youtube.com/watch?v=fU6jsw0yaVU","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179124,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNo Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft\n\nThe best telemetry starts at the source—inside the client libraries.\nBut in most cases, that means taking a dependency on the OpenTelemetry API from your library. And while it’s stable, minimal, reliable, and safely no-op unless configured—transitive dependencies are still the bane of any library developer’s existence, and most of us try to avoid them.\n\nTo work around this, people reach for abstractions, plugins, bridges, or even OTel forks that break context propagation. The result? A poor user experience. Users must find the right plugin, install it, wire it up—and still hit the diamond dependency problem, now it just affects a subset of users.\n\nBut what if you could take a truly optional dependency? If OpenTelemetry is on the classpath, instrumentation kicks in. If it’s not, no harm done.\nHow hard is that to pull off? How reliable? How performant?\n\nLet’s explore that—through the lens of the next generation of Azure SDKs for Java. Spoiler: it’s easy and fast, and as a side-bonus, we can fall back to logs-based tracing if OTel is not found.</article>","contentLength":1544,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fU6jsw0yaVU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Closing Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=eDbQfZ9eoNI","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179125,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nClosing Remarks - Austin Parker, Honeycomb</article>","contentLength":425,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eDbQfZ9eoNI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace","url":"https://www.youtube.com/watch?v=di5nhYvUh6w","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179126,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace\n\nThe OTel Java API, SDK, and ecosystem are perfectly adequate for Android developer to get OTel instrumentation into their apps. But for a host of reasons, the match is not perfect, especially for developers who only write in Kotlin, which is the recommended development language for Android by Google, not the least of which is the emergence of Kotlin Multiple Platform (KMP) as a means to share code between Android, iOS, and many other platforms.\n\nThis session will outline the reasons why we at Embrace is trying to kick-start the development of a pure Kotlin ecosystem for OTel, starting with an API and SDK implementation, and how we are doing it in a way where mobile developers can get value incrementally without having to wait until every aspect is fully built out.\n\nWe want OTel to feel natural and idiomatic for Android developers, and this is the first step towards that end.</article>","contentLength":1361,"flags":null,"enclosureUrl":"https://www.youtube.com/v/di5nhYvUh6w?version=3","enclosureMime":"","commentsUrl":null},{"title":"Nothing launches its most expensive flagship yet, Phone (3)","url":"https://techcrunch.com/2025/07/01/nothing-launches-its-most-expensive-flagship-phone-3/","date":1751391000,"author":"Ivan Mehta","guid":179113,"unread":true,"content":"<article>Nothing launched its newest flagship phone after a two-year gap. At an event in London, the company unveiled the Phone (3), which starts at $799 and aims to take on bigwigs like Samsung and Apple.</article>","contentLength":196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Arms Race Drives Engineer Pay To More Than $10 Million","url":"https://tech.slashdot.org/story/25/07/01/1536223/ai-arms-race-drives-engineer-pay-to-more-than-10-million?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751390400,"author":"msmash","guid":179116,"unread":true,"content":"Tech companies are paying AI engineers unprecedented salaries as competition for talent intensifies, with some top engineers earning more than $10 million annually and typical packages ranging from $3 million to $7 million. OpenAI told staff this week it is seeking \"creative ways to recognize and reward top talent\" after losing key employees to rivals, despite offering salaries near the top of the market. \n\nThe move followed OpenAI CEO Sam Altman's claim that Meta had promised $100 million sign-on bonuses to the company's most high-profile AI engineers. Mark Chen, OpenAI's chief research officer, sent an internal memo saying he felt \"as if someone has broken into our home and stolen something\" after recent departures. \n\nAI engineer salaries have risen approximately 50% since 2022, with mid-to-senior level research scientists now earning $500,000 to $2 million at major tech companies, compared to $180,000 to $220,000 for senior software engineers without AI experience.","contentLength":982,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best iPad apps to boost productivity and make your life easier","url":"https://techcrunch.com/2025/07/01/best-ipad-apps-to-boost-productivity-and-make-your-life-easier/","date":1751390288,"author":"Aisha Malik","guid":179112,"unread":true,"content":"<article>There are many iPad apps to help you organize recipes; sync tasks across devices; be more productive; and manage your notes.</article>","contentLength":124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Raising a Series C+? Cathy Gao’s bringing the real playbook to TechCrunch All Stage","url":"https://techcrunch.com/2025/07/01/raising-a-series-c-cathy-gaos-bringing-the-real-playbook-to-techcrunch-all-stage/","date":1751389998,"author":"TechCrunch Events","guid":179111,"unread":true,"content":"<article>Cathy Gao of Sapphire Ventures shares her playbook to scale a Series C+ at TechCrunch All Stage on July 15 in Boston. Register now to save more than 60% on your tickets.</article>","contentLength":169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implementing IBCS rules in Power BI","url":"https://towardsdatascience.com/implementing-ibcs-rules-in-power-bi/","date":1751389981,"author":"Salvatore Cagliari","guid":179135,"unread":true,"content":"<p>Is there a way to use the out-of-the-box features of Power BI to be IBCS compliant?</p>","contentLength":83,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon deploys its 1 millionth robot, releases generative AI model","url":"https://techcrunch.com/2025/07/01/amazon-deploys-its-1-millionth-robot-releases-generative-ai-model/","date":1751389296,"author":"Rebecca Szkutak","guid":179049,"unread":true,"content":"<article>As Amazon's fleet of robots has reaches a milestone, the company is also releasing a new AI model to make them more efficient. </article>","contentLength":127,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code-GUI bidirectional editing via LSP","url":"https://jamesbvaughan.com/bidirectional-editing/","date":1751388199,"author":"jamesbvaughan","guid":179506,"unread":true,"content":"<p>I built a small proof-of-concept for a system that enables real-time\nbidirectional editing between any modern code editor and a GUI, enabled by an\nLSP server.</p><p>I like working on small projects at home that benefit from CAD. I’m also a\nprogrammer with a personal development environment that I’ve spent years making\nas cozy as possible. Naturally I’ve been interested in finding code-based CAD\nsystem to use for my projects that allows me to use that cozy development\nenvironment.</p><blockquote>For example: One idea I’m exploring is “bidirectional editing”, so geometry can\nbe manipulated using either:<ul><li>a purpose-built graphical UI, or</li><li>the textual codeCAD language</li></ul><p>If you graphically drag a point around, the coordinates in the source code\nshould automatically update.\nIf you edit the source code, the graphical UI should automatically update.</p><p>A simple way to test this idea is to throw a  in the UI that\ndisplays the corresponding source code.\nBut to me, that feels terrible because I never want to be coding in some janky,\nin-browser  — I want to be working with source code in Emacs, with\nall of my familiar key bindings, color schemes, autocomplete, and decades of\ncozy practice.</p><p><strong>That’s the core appeal of a textual programming language.</strong></p><p>But doing this properly is an absolute boatload of work:</p><ul><li>How does the system rewrite source code? Is it mediated by files on disk with\nreload on save? How do the editor and UI stay in sync and avoid clobbering\neach other’s unsaved changes? <strong>Maybe we need an LSP server?</strong></li><li>The language interpreter needs to preserve comments and flow them through,\neven when the UI makes edits to the code.</li><li>What about whitespace / pretty-printing?</li></ul><p>How much of this needs to be built to evaluate whether bidirectional editing\n“fits nicely in the hand”?</p></blockquote><blockquote><p>Maybe we need an LSP server?</p></blockquote><p>I’ve been a happy user of LSP servers since they became commonplace in Neovim\nsetups, but I have almost no experience with language server internals.\nI had certainly never considered that they could facilitate bidirectional\nediting with a GUI.</p><p>That line from Kevin’s post was a proper nerd-snipe because a few hours later I\nhad built this proof-of-concept:</p><p>What you’re seeing here is a text editor next to a GUI, and data live-updating\nboth ways between them, made possible by a small server that uses LSP to\ncommunicate with the text editor and WebSockets to communicate with a web app.</p><p>I’ve shared more technical details and the code for this demo <a href=\"https://github.com/jamesbvaughan/bidirectional-number-editor\">here on\nGitHub</a>.</p><p>Bidirectional editing isn’t new.\nWhat’s new, as far as I’m aware, is real-time bidirectional editing <em>that works\nwith your favorite text editor.</em></p><p>I’ve tried out a handful of code-based CAD systems, but so far I haven’t found\nany that achieve more than two out of these three features:</p><ul><li>Real-time-ish updates in the GUI from changes made in the code</li><li>Real-time-ish updates in the code from changes made in the GUI</li><li>Works well with my preferred code editor</li></ul><p><a href=\"https://www.autodesk.com/products/fusion-360/overview#top\">Fusion 360</a> has\ndecent bidirectional editing for parameters, but it’s not fully code-based and\nit certainly doesn’t let me use my own editor.</p><p><a href=\"https://openscad.org/\">OpenSCAD</a> doesn’t require the use of its own text\neditor, and it’s possible to trigger reloads in the GUI via file watching\nwhen you save source files in external editors, but it only goes one way.</p><p><a href=\"https://zoo.dev/design-studio\">Zoo</a> has some bidirectional editing, but only\nwith its built-in editor.</p><p><a href=\"https://www.arcol.io/\">Arcol</a>, the tool that I help build at my day job, is\ninnovating in CAD interface design in some exciting ways, but we’re building for\narchitects, not programmers.</p><p>This is just a toy demo, but it’s enough to excite me about the possibility of a\nsystem that achieves  of those points!</p><p>I don’t plan to develop this demo further, at least not anytime soon, but I hope\nit inspires people to find more creative uses (abuses?) of LSP servers.</p><p>One of the best code-CAD environments I’ve worked in is OpenSCAD + Neovim with\nthe <a href=\"https://github.com/Leathong/openscad-LSP\">OpenSCAD LSP server</a>, only using\nthe OpenSCAD GUI for the viewer, not the built-in text editor.\nOpenSCAD is fundamentally not built for GUI editing, but since it’s open source\nand has a nice language server already, it could be a good place to develop a\nmore interesting demo of this concept.</p><p>Like Kevin’s post said, doing this properly will be a boatload of work.\nHandling conflict resolution, incremental edits, and the more complex general\nLSP server internals are all serious tasks, let alone creating a whole new\nlanguage for CAD.</p><p>I’m looking forward to seeing what Kevin comes up with for codeCAD!</p>","contentLength":4463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435716"},{"title":"Global Warming Is Speeding Up and the World Is Feeling the Effects","url":"https://news.slashdot.org/story/25/07/01/164239/global-warming-is-speeding-up-and-the-world-is-feeling-the-effects?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751388000,"author":"msmash","guid":179074,"unread":true,"content":"An anonymous reader shares a report: Summer started barely a week ago, and already the United States has been smothered in a record-breaking \"heat dome.\" Alaska saw its first-ever heat advisory this month. And all of this comes on the heels of 2024, the hottest calendar year in recorded history. The world is getting hotter, faster. A report published last week found that human-caused global warming is now increasing by 0.27 degrees Celsius per decade. That rate was recorded at 0.2 degrees in the 1970s, and has been growing since. \n\n\"Each additional fractional degree of warming brings about a relatively larger increase in atmospheric extremes, like extreme downpours and severe droughts and wildfires,\" said Daniel Swain, a climate scientist at the University of California. While this aligns with scientific predictions of how climate change can intensify such events, the increase in severity may feel sudden to people who experience them. \n\n\"Back when we had lesser levels of warming, that relationship was a little bit less dramatic,\" Dr. Swain said. \"There is growing evidence that the most extreme extremes probably will increase faster and to a greater extent than we used to think was the case,\" he added. Take rainfall, for example. Generally, extreme rainfall is intensifying at a rate of 7 percent with each degree Celsius of atmospheric warming. But recent studies indicate that so-called record-shattering events are increasing at double that rate, Dr. Swain said.","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The End of the Guessing Game? Why Describing Data Beats Estimating It","url":"https://hackernoon.com/the-end-of-the-guessing-game-why-describing-data-beats-estimating-it?source=rss","date":1751387880,"author":"Impute","guid":179342,"unread":true,"content":"<li><p>Achiam, J., Andrychowicz, M., Beattie, A., Clark, J., Drozdov, N., Ecoffet, A., Edwards, D., Giddings, J., Goldberg, I., Gomez, M., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)</p></li><li><p>Batista, G.E., Monard, M.C.: A study of k-nearest neighbour as an imputation method. In: Frontiers in Artificial Intelligence and Applications. vol. 87, pp. 251–260. HIS (2002)</p></li><li><p>Biessmann, F., Salinas, D., Schelter, S., Schmidt, P., Lange, D.: \"deep\" learning for missing value imputation in tables with non-numerical data. In: Proceedings of the 27th ACM International Conference on Information and Knowledge Management. p. 2017–2025. CIKM ’18, Association for Computing Machinery, New York, NY, USA (2018). <a href=\"https://doi.org/10.1145/3269206.3272005\">https://doi.org/10.1145/3269206.3272005</a>, <a href=\"https://doi.org/10.1145/3269206.3272005\">https://doi.org/10.1145/3269206.3272005</a></p></li><li><p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models are Few-Shot Learners. In: Advances in Neural Information Processing Systems. vol. 33, pp. 1877–1901. Curran Associates, Inc. (2020)</p></li><li><p>Buuren, S.v., Groothuis-Oudshoorn, K.: Mice: Multivariate imputation by chained equations in r. Journal of Statistical Software 45, 1–67 (2011)</p></li><li><p>Camino, R.D., Hammerschmidt, C.A., State, R.: Improving missing data imputation with deep generative models. arXiv preprint arXiv:1902.10666 pp. 1–8 (2019)</p></li><li><p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: PaLM: Scaling Language Modeling with Pathways (Oct 2022), <a href=\"http://arxiv.org/abs/2204.02311\">http://arxiv.org/abs/2204.02311</a>, arXiv:2204.02311 [cs]</p></li><li><p>Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological) 39(1), 1–22 (1977)</p></li><li><p>Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms (2023)</p></li><li><p>Emmanuel, T., Maupong, T., Mpoeleng, D., Semong, T., Mphago, B., Tabona, O.: A survey on missing data in machine learning. J Big Data 8(1), 140 (2021). <a href=\"https://doi\">https://doi</a>.org/10.1186/s40537-021-00516-9, epub 2021 Oct 27. PMID: 34722113; PMCID: PMC8549433</p></li><li><p>Gimpy, M.: Missing value imputation in multi attribute data set. Int. J. Comput. Sci. Inf. Technol. 5(4), 1–7 (2014)</p></li><li><p>Gondara, L., Wang, K.: Mida: Multiple imputation using denoising autoencoders. In: PacificAsia conference on knowledge discovery and data mining. pp. 260–272. Springer (2018)</p></li><li><p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al.: Generative adversarial nets. In: Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., Weinberger, K.Q. (eds.) Advances in Neural Information Processing Systems. vol. 27, pp. 2672–2680. Curran Associates, Inc., Montréal, Canada (2014)</p></li><li><p>Gupta, A., Lam, M.S.: Estimating missing values using neural networks. Journal of the Operational Research Society 47(2), 229–238 (1996)</p></li><li><p>Hallaji, E., Razavi-Far, R., Saif, M.: Dlin: Deep ladder imputation network. IEEE Transactions on Cybernetics 52(9), 8629–8641 (2021)</p></li><li><p>Jäger, S., Allhorn, A., Biessmann, F.: A benchmark for data imputation methods. Front Big Data 4, 693674 (2021). <a href=\"https://doi.org/10.3389/fdata.2021.693674\">https://doi.org/10.3389/fdata.2021.693674</a>, pMID: 34308343; PMCID: PMC8297389 Enhancing Imputation Accuracy with Contextual Large Language Models 15</p></li><li><p>Little, R.J., Rubin, D.B.: Statistical Analysis with Missing Data, vol. 793. John Wiley &amp; Sons, 3 edn. (2019)</p></li><li><p>Little, R.J.A., Rubin, D.B.: Statistical Analysis with Missing Data. John Wiley &amp; Sons, Hoboken, 2 edn. (2002)</p></li><li><p>Lu, H.m., Perrone, G., Unpingco, J.: Multiple imputation with denoising autoencoder using metamorphic truth and imputation feedback. arXiv preprint arXiv:2002.08338 (2020)</p></li><li><p>McCoy, J.T., Kroon, S., Auret, L.: Variational autoencoders for missing data imputation with application to a simulated milling circuit. IFAC-PapersOnLine 51(21), 141–146 (2018), 5th IFAC Workshop on Mining, Mineral and Metal Processing MMM 2018</p></li><li><p>Nazabal, A., Olmos, P.M., Ghahramani, Z., Valera, I.: Handling incomplete heterogeneous data using vaes. arXiv preprint arXiv:1807.03653 (2018)</p></li><li><p>Qiu, Y.L., Zheng, H., Gevaert, O.: Genomic data imputation with variational auto-encoders. GigaScience 9(8), giaa082 (2020)</p></li><li><p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21(1), 140:5485–140:5551 (Jan 2020)</p></li><li><p>Roberts, A., Raffel, C., Shazeer, N.: How Much Knowledge Can You Pack Into the Parameters of a Language Model? In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5418–5426. Association for Computational Linguistics, Online (Nov 2020). <a href=\"https://doi.org/\">https://doi.org/</a>10.18653/v1/2020.emnlp-main.437, <a href=\"https://aclanthology.org/2020.emnlp-main.437\">https://aclanthology.org/2020.emnlp-main.437</a></p></li><li><p>Rubin, D.B.: Multiple imputations in sample surveys-a phenomenological bayesian approach to nonresponse. In: Proceedings of the survey research methods section of the American Statistical Association. vol. 1, pp. 20–34. American Statistical Association, Alexandria, VA, USA (1978)</p></li><li><p>Rubin, D.B.: Multiple Imputation for Nonresponse in Surveys. John Wiley &amp; Sons, New York, NY (2004)</p></li><li><p>Schafer, J.L.: Analysis of Incomplete Multivariate Data. Chapman &amp; Hall/CRC, London, UK (1997)</p></li><li><p>Schelter, S., Rukat, T., Biessmann, F.: JENGA - A framework to study the impact of data errors on the predictions of machine learning models. In: Velegrakis, Y., Zeinalipour-Yazti, D., Chrysanthis, P.K., Guerra, F. (eds.) Proceedings of the 24th International Conference on Extending Database Technology, EDBT 2021, Nicosia, Cyprus, March 23 - 26, 2021. pp. 529–534. <a href=\"http://OpenProceedings.org\">OpenProceedings.org</a> (2021). <a href=\"https://doi.org/10.5441/002/EDBT\">https://doi.org/10.5441/002/EDBT</a>.2021.63, <a href=\"https://doi.org/10.5441/002/edbt.2021.63\">https://doi.org/10.5441/002/edbt.2021.63</a></p></li><li><p>Sharpe, P.K., Solly, R.: Dealing with missing values in neural network-based diagnostic systems. Neural Computing &amp; Applications 3(2), 73–77 (1995)</p></li><li><p>Stekhoven, D.J., Bühlmann, P.: Missforest—non-parametric missing value imputation for mixed-type data. Bioinformatics 28(1), 112–118 (2012)</p></li><li><p>Stoyanovich, J., Howe, B., Jagadish, H.V.: Responsible data management. Proceedings of the VLDB Endowment 13, 3474–3488 (2020). <a href=\"https://doi.org/10.14778/\">https://doi.org/10.14778/</a> 3415478.3415570</p></li><li><p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: LLaMA: Open and Efficient Foundation Language Models (Feb 2023), <a href=\"http://arxiv.org/\">http://arxiv.org/</a> abs/2302.13971, arXiv:2302.13971 [cs]</p></li><li><p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux, M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang, B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.: Llama 2: Open foundation and fine-tuned chat models (2023)</p></li><li><p>Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing robust features with denoising autoencoders. In: Proceedings of the 25th international conference on machine learning. pp. 1096–1103 (2008)</p></li><li><p>Yang, K., Huang, B., Stoyanovich, J., Schelter, S.: Fairness-aware instrumentation of preprocessing pipelines for machine learning. In: Proceedings of the Workshop on HumanIn-the-Loop Data Analytics (HILDA’20). ACM (2020). <a href=\"https://doi.org/10.1145/3398730.3399194\">https://doi.org/10.1145/3398730.3399194</a></p></li><li><p>Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets. In: International conference on machine learning. pp. 5689–5698. PMLR (2018)</p></li><li><p>Yoon, J., Jordon, J., van der Schaar, M.: Gain: Missing data imputation using generative adversarial nets (2018)</p></li>","contentLength":9189,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Benchmarking of Tabular Reinforcement Learning Methods","url":"https://towardsdatascience.com/revisiting-benchmarking-of-tabular-reinforcement-learning-methods/","date":1751387479,"author":"Oliver S","guid":179080,"unread":true,"content":"<p>Introducing a modular framework and improving model performance.</p>","contentLength":64,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Mamdani’s Refusal To Condemn Speech He Never Made Is Good Free Speech Advocacy","url":"https://www.techdirt.com/2025/07/01/why-mamdanis-refusal-to-condemn-speech-he-never-made-is-good-free-speech-advocacy/","date":1751387332,"author":"Mike Masnick","guid":179078,"unread":true,"content":"<p>At a time when politicians on both sides reflexively call for censorship and speech policing, it’s refreshing to see someone actually defend free speech principles—especially when it would be politically easier to cave.</p><p>That’s exactly what New York City Democratic mayoral nominee Zohran Mamdani did when NBC’s  tried to pressure him into condemning language he’s never used. Rather than take the bait, Mamdani delivered a strong defense of free speech principles. It’s a better defense of free speech than we’ve seen from most politicians lately.</p><p>What makes this particularly frustrating is that many of the Democrats <a href=\"https://www.axios.com/2025/06/26/democrats-zohran-mamdani-meltdown-new-york\">attacking Mamdani</a> should be laser-focused on the existential threat Trump poses to democracy. Instead, they’re wasting time and energy going after someone who actually accomplished what establishment Democrats claim they desperately want: <a href=\"https://www.amny.com/news/gen-z-voters-mamdani-primary-victory-2025/\">activating young people who often fail to vote</a>. Mamdani didn’t just talk about engaging young voters—he did it, handily winning the Democratic primary by mobilizing exactly the demographic Democrats say they need. His reward? A coordinated attack campaign.</p><p>The controversy stems from demands that Mamdani condemn the phrase “globalize the intifada”—<a href=\"https://www.theatlantic.com/ideas/archive/2025/06/zohran-mamdani-globalize-intifada/683300/\">language he doesn’t use</a> but which critics insist he must denounce to prove he’s not antisemitic. It’s the kind of ridiculous purity test that marginalized politicians routinely face (but somehow, white, Christian, male politicians never do), demanding they repeatedly distance themselves from the words of others simply because they share some demographic or political similarity.</p><p>But rather than playing that game, Mamdani chose to defend the principle that government officials shouldn’t be in the business of policing speech—even speech they personally disagree with. At the same time, he used the opportunity to move from the “gotcha” kind of question to a focus on how to tackle the actual problems of racism and bigotry, beyond just focusing on specific language questions.</p><p>There’s been a lot of pressure on Mamdani to specifically criticize pro-Palestinian language used by others. And, over the weekend, <a href=\"https://www.nbcnews.com/meet-the-press/meet-press-june-29-2025-n1312337\">he went on Meet the Press</a> and gave, what I think, is a really strong answer to a silly gotcha question that I think others could learn from:</p><blockquote><p><em>I want to ask you about an issue that has divided some New Yorkers in recent weeks. You were recently asked about the term “globalize the intifada,” if it makes you uncomfortable. In that moment you did not condemn the phrase. Now, just so folks understand, it’s a phrase that many people hear as a call to violence against Jews. There’s been a lot of attention on this issue, so I want to give you an opportunity to respond here and now. Do you condemn that phrase “globalize the intifada?”</em></p><p><em>That’s not language that I use. The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.</em></p><p><em>But do you actually condemn it? I think that’s the question and the outstanding issue that a number of people, both of the Jewish faith and beyond, have. Do you condemn that phrase, “globalize the intifada,” which a lot of people hear as a call to violence against Jews?</em></p><p><em>I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%. I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions. And that is the mayor I will be, one that protects Jewish New Yorkers and lives up to that commitment through the work that I do.</em></p><p><em>But very quickly for the people who care about the language and who feel really concerned by that phrase, why not just condemn it?</em></p><p><em>My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.</em></p></blockquote><p>Note what he does here. It would be easy enough to give into the framing and make statement condemning the language. And while some will (in bad faith) argue his failure to outright condemn the language is an endorsement of it, that’s bullshit. His answer is actually very thoughtful and a good way to approach such bad faith questions.</p><p>He starts out with a direct and clear denial of using that language:</p><blockquote><p><em>That’s not language that I use.</em></p></blockquote><p>This immediately deflates the premise that he’s somehow responsible for words he’s never spoken.</p><p>He then immediately shifts to a more positive framing of how he views what he’s focused on in his hopes of becoming mayor: human rights for all.</p><blockquote><p><em>The language that I use and the language that I will continue to use to lead this city is that which speaks clearly to my intent, which is an intent grounded in a belief in universal human rights. And ultimately, that’s what is the foundation of so much of my politics, the belief that freedom and justice and safety are things that, to have meaning, have to be applied to all people, and that includes Israelis and Palestinians as well.</em></p></blockquote><p>When NBC’s Welker trots out the purity test point, demanding he condemn it, he points out that he shouldn’t be in the business of policing language, but rather is focused on actual concerns of the people he’s hoping to represent. In doing so, he makes it clear that he’s concerned about actual antisemitism and actual threats and risks, and he’s looking at what might actually help rather than policing specific language:</p><blockquote><p><em>I’ve heard from many Jewish New Yorkers who have shared their concerns with me, especially in light of the horrific attacks that we saw in Washington, D.C. and in Boulder, Colorado about this moment of antisemitism in our country and in our city. And I’ve heard those fears and I’ve had those conversations. And ultimately, they are part and parcel of why, in my campaign, I’ve put forward a commitment to increase funding for anti-hate crime programming by 800%.</em></p></blockquote><p>And then he pivots to a reasonable defense of free speech, not in the misleading sense the way others view it, but rather in noting that government shouldn’t be in the business of policing speech (as Trump is doing) but focusing on where the  problems of hate and bigotry show up.</p><blockquote><p><em>I don’t believe that the role of the mayor is to police speech in the manner, especially of that of Donald Trump, who has put one New Yorker in jail, who’s just returned to his family, Mahmoud Khalil, for that very supposed crime of speech. Ultimately, what I think I need to show is the ability to not only talk about something but to tackle it and to make clear that there’s no room for antisemitism in this city. And we have to root out that bigotry, and ultimately we do that through the actions.</em></p></blockquote><p>After Welker desperately goes back to the “but won’t you condemn the language” nonsense, he makes it clear that speaking out on specific language choices is not productive when his focus is on dealing with the actual underlying problems:</p><blockquote><p><em>My concern is to start to walk down the line of language and making clear what language I believe is permissible or impermissible takes me into a place similar to that of the president, who is looking to do those very kinds of things, putting people in jail for writing an oped. Putting them in jail for protesting. Ultimately, it’s not language that I use. It’s language I understand there are concerns about. And what I will do is showcase my vision for this city through my words and my actions.</em></p></blockquote><p>This final answer is particularly smart because it connects his refusal to condemn specific language to Trump’s actual authoritarian attacks on free speech. Rather than getting trapped in semantic debates about particular phrases, he’s defending the broader principle that government officials shouldn’t be arbiters of acceptable speech.</p><p>The contrast is stark: while the Trump regime is literally jailing people for their speech, critics want Mamdani to engage in the kind of speech policing that leads down that same authoritarian path. His refusal isn’t endorsement of problematic language—it’s recognition that the role of government isn’t to play word police.</p><p>This is exactly the kind of principled free speech defense we need more of, especially from Democrats who have too often been willing to compromise these principles for short-term political gain. While it would have been easy for Mamdani to simply condemn the phrase and move on, his more thoughtful approach actually serves the cause of free speech better.</p><p>The irony is that many of the same people <a href=\"https://www.axios.com/2025/06/26/democrats-zohran-mamdani-meltdown-new-york\">attacking Mamdani</a> are Democrats who claim to be defending democracy against Trump’s authoritarianism. Yet they’re demanding exactly the kind of speech policing that authoritarian governments excel at—forcing officials to take public positions on specific language as loyalty tests.</p><p>And yes, some could argue that simply condemning certain language is not the same as censoring it. It’s not. It’s stating an opinion. But there’s value in Mamdani making it clear he’d rather focus on the real underlying issues around bigotry and hatred than trying to say magic words to appease a media that would never ask similar questions of a white, Christian politician.</p><p>In an era where politicians routinely cave to demands for performative condemnations and symbolic gestures, Mamdani’s approach stands out. He’s more interested in actual solutions—like his 800% increase in anti-hate crime funding—than in playing the gotcha game that dominates political discourse.</p><p>This is what defending free speech actually looks like: not demanding the right to be an asshole without consequences, but refusing to let government officials become the arbiters of acceptable speech—and politely reframing the issue when the media insists on playing such a gotcha game. If more politicians followed Mamdani’s lead, we’d have a much healthier democratic discourse.</p>","contentLength":11284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Core – open source memory graph for LLMs – shareable, user owned","url":"https://github.com/RedPlanetHQ/core","date":1751387064,"author":"Manik_agg","guid":179512,"unread":true,"content":"<p>I keep running in the same problem of each AI app “remembers” me in its own silo. ChatGPT knows my project details, Cursor forgets them, Claude starts from zero… so I end up re-explaining myself dozens of times a day across these apps.</p><p>1. Not portable – context is vendor-locked; nothing travels across tools.</p><p>2. Not relational – most memory systems store only the latest fact (“sticky notes”) with no history or provenance.</p><p>3. Not yours – your AI memory is sensitive first-party data, yet you have no control over where it lives or how it’s queried.</p><p>- CORE (Context Oriented Relational Engine): An open source, shareable knowledge graph (your memory vault) that lets any LLM (ChatGPT, Cursor, Claude, SOL, etc.) share and query the same persistent context.</p><p>- Temporal + relational: Every fact gets a full version history (who, when, why), and nothing is wiped out when you change it—just timestamped and retired.</p><p>- Local-first or hosted: Run it offline in Docker, or use our hosted instance. You choose which memories sync and which stay private.</p>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435500"},{"title":"The Fed says this is a cube of $1M. They're off by half a million","url":"https://calvin.sh/blog/fed-lie/","date":1751386975,"author":"c249709","guid":179072,"unread":true,"content":"<p>At the Federal Reserve Bank of Chicago’s Money Museum, there’s a big transparent cube on display. It’s filled with tightly packed stacks of  bills, claiming to contain .</p><p>The plaque proudly declares:</p><blockquote><p>Have you ever wondered what one million dollars looks like?\nYou don’t have to wonder anymore because you can see it right in front of you!</p></blockquote><p>But I don’t trust signs. I trust counting.</p><p>I first tried counting the stacks right there in the room. The cube was tall, so I had to step back to see the whole thing, squinting at the stacks, trying to follow each row. I lost track almost immediately.</p><p>Also, people were starting to look at me funny. Apparently, staring intensely at a pile of cash while muttering numbers isn’t normal museum behavior.</p><p>Then, I tried with a photo. I zoomed all the way in on my phone, dragging my finger across the screen, mentally tallying as I went.</p><p>Still couldn’t keep count.</p><p>All I wanted was a way to click on things in a photo and have the number go up.</p><p>You’d think this would already exist, a browser based tool for counting things.</p><p>Turns out it… doesn’t. At least, not as a web app I can find on Google.</p><p>There are some clunky old Windows programs, niche scientific tools, and image analysis software that assumes you’re trying to count cells under a microscope, not people, penguins, or stacks of $1 bills in a Federal Reserve cube.</p><p>It’s stupidly simple: upload an image, click to drop a dot, and it tells you how many you’ve placed. That’s it. But somehow, nothing like it existed.</p><p>I originally made it to investigate this very cube, but I figured other people might need to count stuff in pictures.</p><p>Count your enemies. Count your blessings. Count your stacks of cash.</p><p>Because when someone tells you it’s a million dollars, you might want to double check.</p><a href=\"https://calvin.sh/blog/fed-lie/cube-labeled.png\" data-v-3b6c5e00=\"\"></a><p>Assuming each bundle contains  bills*, that’s</p><p>So yeah. They’re off by .</p><p>That’s  in extra cash.</p><blockquote><p>“Hey so… we’re $550,400 over budget on the million-dollar cube project.”</p></blockquote><p>If you knock  from each dimension (basically pealing away the outermost layer of money bundles), the math actually gets kinda close</p><p>but since dollar bills are much wider than they’re tall, it wouldn’t look like a cube anymore.</p><p>Maybe the Fed is playing the long game.</p><p>At the Fed’s  inflation target, this cube will be worth  million in today’s dollars in:</p><p>Can’t wait to come back in 2047 and say: “Nice. Nailed it.”</p><p>Sure, it does technically contain .</p><p>And also  of bonus money.</p><p>Which is kind of like ordering a burger and getting three.</p><p>I mean, sure, free stuff. But it’s not what you asked for.</p><p>You can only see the outer stacks. For all we know, the middle is just air and crumpled-up old newspaper.</p><p>A money shell. A decorative cube. A fiscal illusion. The world’s most expensive piñata (but don’t hit it, security is watching).</p><p>And get this: just the outermost layer is already worth:</p><p>You’d only need a 3-layer-thick shell to blow past a million:</p><h2>How  you make a million dollar cube?</h2><p>Turns out U.S. dollars are extremely non-cube-friendly. Each bill is  wide by  tall, a nice and even aspect ratio of:</p><p>Each 100-bill bundle is  inches thick.</p><ul><li> stacks</li></ul><p>Which gives you a lovely almost-cube:</p><ul><li> wide</li><li> deep</li><li> tall</li></ul><p>Not perfect. Not terrible. At least it’s honest, unlike that other cube.</p><p>Maybe it’s  million.</p><p>Maybe it’s an empty box with a money shell.</p><p>Most likely it’s  million.</p><p>All I know is I built a tool, did the math, and triple-checked the stacks.</p><p>The sign says you don’t have to wonder.\nBut I did anyway.</p><p>And now… you don’t have to either.</p>","contentLength":3525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435484"},{"title":"Built for Scale: eXchange1 Brings Institutional-Grade Crypto Trading to Indian Users","url":"https://hackernoon.com/built-for-scale-exchange1-brings-institutional-grade-crypto-trading-to-indian-users?source=rss","date":1751386609,"author":"ZEX MEDIA","guid":179341,"unread":true,"content":"<p>As India continues its ascent as the world’s most active crypto market, eXchange1 arrives with the infrastructure, innovation, and integrity to match the moment. With its official launch, the European-regulated platform opens access to a full suite of digital asset tools designed for modern traders, institutional clients, and ecosystem partners alike.</p><p>While many platforms promise speed and scale, few back it up with the regulatory foundation and high-performance architecture that eXchange1 offers by default.</p><p>eXchange1 positions itself as more than a trading venue. It is a complete digital finance ecosystem that merges cutting-edge technology with seamless user experience. The platform provides:</p><ul><li>Spot, Margin, and Futures Trading</li><li>Copy Trading (for both Spot and Futures)</li><li>Tokenized Investment Products</li></ul><p>Whether you’re a new investor exploring basic trades or an institution managing complex portfolios, eXchange1 offers functionality at every level. The platform is accessible across web, mobile, and API, enabling real-time participation with global markets from any device.</p><p>Its Copy Trading feature, for example, empowers less experienced traders to mirror strategies of seasoned investors—while the automated tools allow advanced users to deploy algorithmic trades with precision.</p><h3><strong>Built for India. Backed by the World.</strong></h3><p>India’s fast-growing crypto base—119 million users in 2024 alone—demands more than just flashy apps. It demands scalability, reliability, and regulatory clarity. eXchange1 delivers on all fronts.</p><p>The platform is built on a high-liquidity architecture designed to support institutional-scale trade volumes without bottlenecks. Back-end infrastructure is optimized for rapid execution, real-time price discovery, and minimal slippage—ensuring traders get exactly what they see.</p><p>To support this mission in India, eXchange1 has partnered with a leading global fintech firm to ensure localized performance, reduced latency, and seamless access—even during market surges.</p><p>“We’re not here with a one-size-fits-all model,” said CEO Ms. Sandoval Mera. “India’s scale and diversity deserve a platform that’s tailored, responsive, and resilient. We’ve invested in both infrastructure and partnerships to deliver that.”</p><h3><strong>Enterprise-Grade Security by Default</strong></h3><p>Infrastructure isn’t just about speed—it’s also about safety. eXchange1 operates with enterprise-grade security systems that ensure operational integrity, user protection, and compliance with global standards.</p><p><strong>Key security features include:</strong></p><ul><li>Real-time risk monitoring systems</li><li>Institutional-grade custodial protocols</li><li>Multi-layer authentication and transaction security</li><li>Audit-ready transparency for regulators and partners</li></ul><p>With the rising sophistication of crypto-related threats, such features are no longer optional—they are critical. eXchange1 ensures its infrastructure protects both user assets and market confidence.</p><p>The platform is licensed under the Markets in Crypto-Assets Regulation (MiCA) by the Financial Crime Investigation Service (FCIS) of Lithuania, and is registered with India’s Financial Intelligence Unit (FIU).</p><p>This dual compliance enables eXchange1 to integrate seamlessly with India’s evolving regulatory environment while retaining the global credibility needed to serve cross-border investors.</p><p>Being MiCA-compliant also brings with it rigorous operating standards, including rules around asset segregation, risk exposure, transparency, and consumer protection—all of which are reflected in eXchange1’s infrastructure design.</p><p>“The future of digital assets depends on platforms that are both innovative and accountable,” said Dr. James Newsome, Chairman of eXchange1 and former head of the U.S. Commodity Futures Trading Commission (CFTC). “We’ve built eXchange1 to be just that—a high-performance platform that regulators can work with, and users can depend on.”</p><p>One of eXchange1’s key advantages is its ability to scale with its users. Retail traders benefit from an intuitive UI/UX and educational support, while institutions get access to:</p><ul><li>Advanced APIs for high-frequency trading</li><li>Real-time market data feeds</li><li>Dedicated account support</li><li>Tokenized investment vehicles for diversification</li></ul><p>This dual approach—serving both ends of the crypto maturity spectrum—makes eXchange1 uniquely capable of addressing India’s diverse and rapidly evolving investor base.</p><p>And with multilingual customer service available 24/7, the platform ensures every user, from first-time investor to fund manager, gets the assistance they need—when they need it.</p><h3><strong>Future-Proofed for a Changing Industry</strong></h3><p>Crypto is changing fast. New asset classes, regulatory shifts, institutional adoption, and user behavior all demand platforms that can adapt in real time. eXchange1’s architecture is modular and forward-compatible, allowing it to integrate future features like:</p><ul><li>Cross-chain liquidity pools</li><li>Compliance automation for new jurisdictions</li></ul><p>This future-readiness is part of the platform’s DNA. It’s also why eXchange1 insists on building for long-term resilience rather than short-term speculation.</p><h3><strong>Final Thoughts: A New Infrastructure Standard</strong></h3><p>As more Indian users seek robust, regulated platforms to trade and invest in crypto, infrastructure will become the dividing line between the serious players and the short-lived ones.</p><p>eXchange1 has entered the Indian market not to test the waters, but to build a foundation—one grounded in regulation, engineered for performance, and designed for trust.</p><p>From institutional-grade systems to retail-friendly tools, and from global oversight to local integration, eXchange1 represents what a next-generation exchange should look like: fast, safe, scalable, and accountable.</p>","contentLength":5715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nikita Bier joins X as head of product: ‘I’ve officially posted my way to the top’","url":"https://techcrunch.com/2025/07/01/nikita-bier-joins-x-as-head-of-product-ive-officially-posted-my-way-to-the-top/","date":1751386432,"author":"Amanda Silberling","guid":179048,"unread":true,"content":"<article>At X, Bier could potentially build features that drive adoption beyond the user base that's typically drawn to text-first social networks.</article>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Missing Data Stack for Physical AI","url":"https://podcasters.spotify.com/pod/show/mlops/episodes/The-Missing-Data-Stack-for-Physical-AI-e34v4hc","date":1751386292,"author":"Demetrios","guid":179079,"unread":true,"content":"<p>The Missing Data Stack for Physical AI // MLOps Podcast #328 with Nikolaus West, CEO of Rerun.</p><p>Nikolaus West, CEO of Rerun, breaks down the challenges and opportunities of physical AI—AI that interacts with the real world. He explains why traditional software falls short in dynamic environments and how visualization, adaptability, and better tooling are key to making robotics and spatial computing more practical.</p><p>Niko is a second-time founder and software engineer with a computer vision background from Stanford. He’s a fanatic about bringing great computer vision and robotics products to the physical world.</p><p>~~~~~~~~ ✌️Connect With Us ✌️ ~~~~~~~</p><p>[00:00] Niko's preferred coffee</p><p>[00:35] Physical AI vs Robotics Debate</p><p>[04:40] IoT Hype vs Reality</p><p>[12:16] Physical AI Lifecycle Overview</p><p>[20:05] AI Constraints in Robotics</p><p>[23:42] Data Challenges in Robotics</p><p>[33:37] Open Sourcing AI Tools</p><p>[39:36] Rerun Platform Integration</p><p>[40:57] Data Integration for Insights</p><p>[45:02] Data Pipelines and Quality</p><p>[49:19] Robotics Design Trade-offs</p>","contentLength":1033,"flags":null,"enclosureUrl":"https://anchor.fm/s/174cb1b8/podcast/play/104878060/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-6-1%2F403114405-44100-2-7f170897674ed.mp3","enclosureMime":"","commentsUrl":null},{"title":"DORA Regulation Explained - Plus a Free Compliance Checklist","url":"https://hackernoon.com/dora-regulation-explained-plus-a-free-compliance-checklist?source=rss","date":1751385983,"author":"N2W","guid":179340,"unread":true,"content":"<article>The Digital Operational Resilience Act (DORA) sets new EU-wide standards for digital risk in the financial sector. This guide breaks down key requirements, penalties, and regional implications, with best practices for compliance—including risk management, testing, and third-party oversight. Download a free checklist to get your organization DORA-ready.</article>","contentLength":356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: Will VR Integration for Remote Work Become Permanent? (7/1/2025)","url":"https://hackernoon.com/7-1-2025-newsletter?source=rss","date":1751385836,"author":"Noonification","guid":179339,"unread":true,"content":"<p>🪐 What’s happening in tech today, July 1, 2025?</p><p>By <a href=\"https://hackernoon.com/u/editingprotocol\">@editingprotocol</a> [ 4 Min read ] Run your blog like a product. Learn how to define your audience, set goals, and build a content strategy that drives real growth. <a href=\"https://hackernoon.com/what-if-your-blog-had-a-product-manager-hint-its-you\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/allan-grain\">@allan-grain</a> [ 3 Min read ] The best part of VR is that it provides a three-dimensional immersive space where employees can interact as if they were physically present. <a href=\"https://hackernoon.com/will-vr-integration-for-remote-work-become-permanent\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/paoloap\">@paoloap</a> [ 6 Min read ] Discover how the original PDF design creates hurdles for AI document parsing and learn solutions for better, machine-readable document workflows. <a href=\"https://hackernoon.com/why-extracting-text-from-pdfs-still-feels-like-a-hack-and-the-legacy-design-that-keeps-ai-stuck\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/legalpdf\">@legalpdf</a> [ 8 Min read ] Anthropic allegedly used over 7M pirated books to train Claude AI. Authors sue over copyright theft fueling a billion-dollar LLM business. <a href=\"https://hackernoon.com/anthropic-sued-for-allegedly-pirating-millions-of-books-to-train-claude-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/techthrilled\">@techthrilled</a> [ 4 Min read ] As soon as a16z’s&nbsp;Bryan Kim&nbsp;saw Cluely’s buzz online, he was intrigued. He hadn’t even met the founder yet, but he could tell the company was onto something. <a href=\"https://hackernoon.com/a16z-thinks-controversial-startup-cluely-is-the-future-of-ai\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":1222,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It's Not Just What's Missing, It's How You Say It: CLAIM's Winning Formula","url":"https://hackernoon.com/its-not-just-whats-missing-its-how-you-say-it-claims-winning-formula?source=rss","date":1751385820,"author":"Impute","guid":179338,"unread":true,"content":"<p>(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);</p><p>(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).</p><p>We conducted a series of experiments to systematically evaluate the efficacy of CLAIM in addressing the research questions presented in Section 1. Our validation criterion for CLAIM’s effectiveness was the post-imputation performance of pre-trained LLMs finetuned with missing-aware contextual datasets for downstream classification tasks. We focused on three types of missingness mechanisms: MCAR, MAR, and MNAR.</p><p>\\\n. We evaluated the performance of CLAIM using seven real-life multivariate classification datasets from the UCI repository [12]. Detailed information on these datasets is provided in the Appendix.</p><p>\\\n<strong>Baseline Imputation Methods.</strong> Our approach was compared against a broad spectrum of commonly-used baseline imputation methods, encompassing single imputation (SI) and multiple imputation (MI) techniques, non-ML and ML methods, and both discriminative and generative ML approaches.</p><p>\\\nSI methods included mean imputation using the feature-wise mean (), kNearest Neighbors (k-NN) [3] (), a tree-based algorithm using MissForest [37] (), and a deep generative adversarial network for imputation using GAIN (Generative Adversarial Imputation Nets) [45] (). The MI method employed was MICE (Multiple Imputation by Chained Equations) [22] ().</p><p>\\\n The hyperparameter settings for the various imputation methods and the LLM used in our experiments are detailed below.</p><p>\\\n<em>Hyperparameters for Baseline Imputation Methods.</em> For GAIN, we adhered to the hyperparameters specified in the original publication, setting α to 100, the batch size to 128, the hint rate at 0.9, and the number of iterations to 1000 for optimal performance. MissForest and MICE were configured with their respective default parameters as provided in their PyPI implementations[2]. For k-NN, we chose k = 5 and the Euclidean distance measure based on literature suggesting this configuration offers superior performance [15].</p><p>\\\n We utilized the 7 billion-parameter LLaMA 2 model [40], fine-tuning it with the parameter-efficient QLoRA method [11]. The settings were r = 16, α = 64, dropout = 0.1 with the task type set to “CAUSAL<em>LM”. The learning rate was 2e-4, using the “paged</em>adamw_32bit” optimizer.</p><p>\\\nExperiments were conducted with a batch size of 4 across 50 epochs, considering memory constraints during fine-tuning. Tesla A40 GPUs (48GB RAM) were used for distributed training. For evaluation, we used 20% randomly sampled instances of each dataset. Models were evaluated five times, reporting both average performance and standard deviation.</p><p>Figure 2 displays the experimental outcomes for seven datasets, where we benchmarked CLAIM against existing imputation methods. Performance metrics for LLMs fine-tuned on fully complete datasets (without any missing values, thus no imputation was necessary) were included for comparison. This approach delineates the effectiveness of CLAIM by providing a reference to baseline performances, offering a clearer perspective on the benefits provided by CLAIM over traditional imputation methods.</p><p>\\\n: <em>How effective is CLAIM in imputing missing values across the distinct missingness mechanisms (MCAR, MAR, and MNAR) and how does it compare with existing imputation methods in terms of accuracy and robustness across varied datasets and missing data scenarios?</em></p><p>\\\n: CLAIM demonstrated superior accuracy in imputing missing values across all datasets compared to baseline imputation methods. Its performance under the MCAR assumption, where missingness is independent of any data, suggests that CLAIM efficiently leverages the contextual information inherent in the dataset for imputation. This efficiency is particularly evident in its ability to significantly close the gap towards the performance of fully complete datasets (no imputation), showcasing its effectiveness</p><p>\\\n Under MAR, where missingness depends on observed data, the adaptability of CLAIM is further highlighted. It outperforms other methods by a considerable margin, indicating its proficiency in utilizing available data points to predict missing values accurately.</p><p>\\\n: The MNAR scenario, characterized by missingness that depends on unobserved data, poses the greatest challenge. Here, CLAIM’s performance remains notably superior to traditional imputation methods. This robustness in the face of the most difficult missingness mechanism illustrates CLAIM’s potential to effectively mitigate the biases introduced by MNAR missingness, utilizing the LLaMA 7B model’s capacity to infer missing information from complex patterns.</p><p>\\\nTo elucidate the superior performance of CLAIM over traditional baseline imputation methods, we delved into its performance on <strong>three particularly challenging datasets</strong>: Glass Identification, Seeds, and Wine. These datasets were selected due to the relatively lower performance exhibited by the LLM when utilizing fully complete versions of the datasets, highlighting their complexity and the rigorous testing ground they provide for evaluating CLAIM’s effectiveness.</p><p>\\\nTable 1 presents a detailed comparative analysis. For the Glass Identification dataset, where the LLM achieved an accuracy of only 69.40% with the full dataset, CLAIM demonstrated a significant advantage. It outperformed the best baseline method (kNN, which achieved 52.40% accuracy) by a substantial margin of 7.2%. This performance gap underscores CLAIM’s robustness and its ability to effectively handle missing data within complex datasets.</p><p>\\\nThe challenge escalates with the Seeds dataset, wherein CLAIM surpassed the top-performing baseline method (MICE) by a margin of 4.2%. This further exemplifies CLAIM’s superiority in managing missing data, even in datasets where the LLM’s base performance is less than optimal.</p><p>\\\n\\\nThe Wine dataset showcased a similar trend, with CLAIM exceeding the best baseline performance by a margin of 2.4%. It’s noteworthy that the performance gaps between CLAIM and the best-performing baseline methods are relatively modest under MAR conditions—2%, 3%, and 1.2% for Glass Identification, Seeds, and Wine, respectively. This observation suggests that while the predictability of missingness from observed data in MAR scenarios offers some leverage for traditional imputation methods, CLAIM still maintains a performance edge.</p><p>\\\nThe MNAR scenario, characterized by the most complex pattern of missingness, highlighted CLAIM’s distinct advantage. Across all three datasets, CLAIM not only managed to outperform the best baseline methods but did so with remarkable performance gains of 12.4%, 7.6%, and 10% for Glass Identification, Seeds, and Wine, respectively. This substantial improvement underlines CLAIM’s adeptness at navigating the intricacies of MNAR missingness, further cementing its status as a highly effective tool for handling various missing data scenarios with aplomb.</p><p>\\\n CLAIM’s superior accuracy across diverse missingness patterns and datasets unequivocally affirms its effectiveness in a variety of challenging scenarios, thereby  This consistent overperformance not only underscores its utility but also illustrates the significant benefits of integrating contextualized natural language models into the data imputation process. The pronounced accuracy improvements observed in complex datasets, such as the Glass Identification and Seeds datasets, point to a distinct advantage over traditional imputation techniques, which often falter under such conditions.</p><p>\\\nThe  of CLAIM, evident across MCAR, MAR, and MNAR missingness mechanisms, showcases its <strong>broad applicability and dependability.</strong> This marks a departure from conventional methods, which might only perform well under limited conditions or with specific types of data [20]. CLAIM’s methodology, which involves verbalizing data and employing contextually relevant descriptors for imputation, ensures its adeptness across various scenarios and data modalities.</p><p>\\\nMoreover, the minimal variation in CLAIM’s performance across different iterations further underscores its stability and reliability as an imputation method. Such consistency is indispensable for real-world applications, where the quality of imputation directly impacts the efficacy of subsequent data analyses. The ability of CLAIM to maintain a low error margin consistently highlights its potential as a go-to solution for data imputation, offering both precision and reliability.</p><p>\\\n\\\n<em>How does the choice of phrasing for missingness descriptors in CLAIM affect the performance of LLM-based downstream tasks?</em></p><p>\\\nInitially, we utilized contextually relevant descriptors for missing values, leading to unique phrases for different features within a dataset. To address RQ2, we aimed to determine whether using a uniform, yet contextually relevant, descriptor for all features would offer comparable benefits. To this end, we experimented with three consistent descriptors: “NaN”, “Missing value”, and “Value not recorded”. These experiments, focusing on the MCAR scenario, sought to ascertain whether it is more beneficial to use contextually nuanced descriptors or whether a generic descriptor is adequate to harness the LLMs’ general knowledge for managing missing values in datasets.</p><p>\\\nThe experimental findings (Figure 3) illuminate the influence of missing data phrasing on the effectiveness of LLMs in addressing such situations. The results reveal a distinct pattern: generic descriptors, such as “NaN”, consistently perform worse than context-specific descriptors designed for each feature and dataset. Among the three fixed descriptors tested, there were some variations in performance. Both “NaN” and “Missing value” outperformed “Value not recorded”, with “Missing value” achieving the best results in most cases among the static descriptors.</p><p>\\\nThe superior performance of feature-specific descriptors indicates that LLMs better interpret and manage missing data when it is described in a way that accurately reflects the context of the missing information. For example, a descriptor like <em>“Malic acid quantity missing for this wine sample”</em> allows the LLM to more effectively understand and address the missing data point than a more generic descriptor like <em>“The level of malic acid in the wine is NaN”</em>.</p><p>\\\n The findings related to RQ2 underscore the importance of context in the interaction between LLMs and missing data. The preference for context-specific descriptors over generic ones likely arises from the LLM’s capacity to utilize its extensive training on diverse language uses and contexts. When missing data is described in a manner that aligns with the specific context of a feature, the LLM is better positioned to apply its vast repository of knowledge to deduce or generate suitable imputations. This effectiveness diminishes with the use of generic labels, which offer minimal contextual information for the LLM to draw upon.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2405.17712\">available on arxiv</a> under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":11227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FCC Delays Enforcement of Prison Call Pricing Limits","url":"https://news.slashdot.org/story/25/07/01/1448220/fcc-delays-enforcement-of-prison-call-pricing-limits?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751385660,"author":"msmash","guid":179073,"unread":true,"content":"The FCC will suspend enforcement of rules that would lower prison phone and video call prices until April 1st, 2027. Trump-appointed FCC Chair Brendan Carr said that prisons won't have to comply with the pricing regulations [PDF], reversing plans to implement the caps this year. \n\nThe rules would have dropped the price of a 15-minute phone call to 90 cents in larger prisons. Current fees can reach as high as $11.35 for a 15-minute call, which the FCC described in 2024 as \"exorbitant.\" Four states -- Connecticut, California, Minnesota, and Massachusetts -- have made prison calls free. Former President Joe Biden signed the Martha Wright-Reed law in 2023, allowing the FCC to regulate prison call rates. The agency voted to adopt the new rates last year, with rules set to take effect on a staggered basis starting January 1st, 2025. \n\nCarr said the regulations are \"leading to negative, unintended consequences\" and would make caps \"too low\" to cover \"required safety measures.\" FCC Commissioner Anna Gomez criticized the delay, stating the Commission \"is now stalling, shielding a broken system that inflates costs and rewards kickbacks to correctional facilities.\"","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Present of Collapse: A Metaphysical Framework of NP, P, and the Recursive Field of Truth","url":"https://hackernoon.com/the-present-of-collapse-a-metaphysical-framework-of-np-p-and-the-recursive-field-of-truth?source=rss","date":1751385637,"author":"Antică Vlad","guid":179337,"unread":true,"content":"<p>This paper proposes a metaphysical framework that unifies the notions of NP, P, myth, and conscience into a coherent logic of truth, choice, and temporal recursion.</p><p> is defined as  – a blueprint of infinite potential that is structured and anticipatory, rather than chaotic or arbitrary.</p><p> is defined as  – the concrete manifestation of a specific pathway selected from the NP field, realized in the present moment.</p><p> is defined as  – a form of structured uncertainty that hovers at the edge of NP collapse, giving shape and meaning to potential without yet fully actualizing it.</p><p>We argue that collapsing entirely into NP regresses into unrealized potential; collapsing entirely into myth overwhelms coherence with ungrounded possibilities; but collapsing into P (the present) enables a recursive process of becoming. In this framework, conscience is the agent of collapse – the faculty that, guided by mythic future-truths, selectively collapses NP potential into P actuality through the seeding of choice (a “P-seed”). Truth emerges from the iterative (recursive) act of conscience choosing and actualizing possibilities: each  retroactively illuminates the structure of the NP seed from which it arose, refining our understanding of the blueprint. We introduce a geometrical metaphor to illustrate this dynamic: hyperbolic and spherical curvatures of the NP potential field (representing divergent and convergent modes of possibility) collapse into the flat Euclidean plane of present P-truth. This metaphor underscores the metaphysical necessity of the Present as the only “domain” capable of bearing the weight of infinite possibility without disintegration. In our view, the infinite never becomes truth in itself, but it informs every collapse of potential into actuality. Conscience navigates fluidly between NP, myth, and P – allowing mythic narratives of meaning to guide the vast NP field toward collapses that are not only logically consistent but meaningful. By developing this framework with symbolic structures (e.g. ), grounded in clear conceptual language, we aim to articulate a rigorous yet visionary logic of truth, choice, and time.</p><p>How does  emerge from the interplay of what could be, what is, and what ought to be? This question invites us to explore the relationship between , , and the guiding structures of meaning that shape our choices. In classical philosophy, Aristotle distinguished between  and  as two fundamental principles: potentiality refers to any real possibility a thing can have, especially those that naturally realize when conditions are right, while actuality is the fulfillment or exercise of such a possibility – the possibility “becoming real in the fullest sense”.</p><p>We borrow the symbols  and  from computational complexity theory and reinterpret them metaphysically to correspond roughly to this age-old distinction: NP will signify a realm of , and P will signify . In computational terms, NP (nondeterministic polynomial time) problems are those whose solutions, while hard to compute, can be , whereas P problems are those that can be . Analogously, one might say NP contains truths that are  or  in principle, while P contains truths that are <em>immediately attainable or manifest</em>. Our framework, however, is not about algorithms, but about metaphysical : NP as the space of  (structured possibilities) and P as the domain of  (realized actuality).</p><p>We also introduce two additional elements to this framework:  and . By , we do not mean “falsehood,” but rather the narrative, symbolic, or archetypal structuring of truth that points toward the future. As one saying (attributed to scholar Joseph Campbell) puts it, “<em>Myth is something that never was but always is.</em>” In other words, a myth transcends literal fact to convey enduring truths. A myth can be understood as a  – a form that truth could take, carrying meaning and value, but which has not yet collapsed into concrete reality. It is : myth provides an imaginative or moral framework for the unknown, preventing the potential future from being mere chaos by giving it shape and direction. Finally, by  we denote the integrative faculty (both intellectual and moral) that navigates between what is known and unknown – the agent that makes choices about what potential to actualize, guided by an inner sense of truth or rightness. Conscience in this context bridges the gap between the realm of infinite possibilities and the finite realm of action, ensuring that the collapse of potential into actuality is not random but aligned with meaning and values. Philosophically, one might liken conscience to practical reason or the soul’s compass, discerning  path out of many is worth manifesting.</p><p>The goal of this paper is to develop a <em>standalone metaphysical logic of truth, choice, and temporal recursion</em> that unifies NP, P, myth, and conscience. We will construct this framework step by step. First, we clarify the definitions of NP, P, myth, and conscience within our metaphysical context. Next, we examine the dynamics of “collapse” – the process by which potential (NP) becomes actual (P) – and the pitfalls of collapsing improperly (either retreating entirely into potential or leaping into myth without realization). We then elaborate how truth emerges recursively: each present realization feeds back into our understanding of the potential, a process we call . A geometrical analogy of curvatures (hyperbolic, spherical, Euclidean) is introduced to illustrate the structural differences between the space of possibilities and the space of actual truth. We discuss why the present moment has a unique status as the locus where infinite truth can be progressively realized without fragmentation. Throughout, we emphasize the role of conscience as the navigator that balances the expansive freedom of NP, the guiding vision of myth, and the grounding discipline of P. By the conclusion, we aim to show that this framework offers a rigorous yet imaginative way to understand how truth “happens” – not as a static absolute, but as the result of an ongoing, recursive collapse of potential into reality, guided by meaning and responsibility.</p><p><em>(In what follows, we will occasionally use symbolic notation for clarity. For example,  will denote the process by which a potential truth in NP, through the initiation of a particular choice or seed, collapses into an actual truth in P. We will ensure all such formulations are explained in plain language.)</em></p><p><strong>I: Conceptual Foundations: Defining NP, P, Myth, and Conscience</strong></p><p><strong>NP: The Uncollapsed Seed-Truth (Infinite Potential)</strong></p><p>We define  as the realm of uncollapsed seed-truth – an infinite field of potential truths that exist as  or  prior to realization. Crucially, NP in this metaphysical sense is <strong>not the same as mere chaos or randomness</strong>; it is not a jumble of arbitrary possibilities. Rather, NP is : one can imagine it as a vast design space containing all the solutions or outcomes that could come to pass, given the initial conditions of reality. It is “<em>not future, but blueprint</em>” – meaning NP is not a set of events that will happen in time (that would be the future), but a set of  for what could happen. To use an analogy, if the truth of the world were like a grand puzzle or cosmic crossword, NP would be the blank but structured puzzle grid containing countless potential fillings-in, whereas P (the actual truth) would be one completed filling of that puzzle. In this sense, NP holds the  of every truth that might grow, akin to a seed containing many possible ways a tree could branch. NP is “infinitely potential” in that it contains an unbounded number of possible truth-configurations, yet it is also a  in itself – a truth of possibility, waiting to be specified.</p><p>This notion can be further illuminated by borrowing a perspective from the P vs NP metaphor in computer science and extending it metaphysically. Imagine there is a “Global truth field” in which all answers to all questions exist in latent form, completely coherent with one another. Such a field would be like an omniscient viewpoint outside time – a  where, metaphorically, every problem is already solved and every truth known. When we restrict our view to a particular problem or situation (when reality is filtered through a local perspective in time), that coherent field appears to split into two aspects: the part that  into a  (a known truth, which we experience as P), and the complementary part that remains  (the as-yet-unsolved, which we experience as NP). In other words, NP and P are like two sides of one coin: whenever a specific truth is brought into being (P), a halo of remaining possibility (NP) still surrounds it. The NP field thus contains all those possibilities that were not realized in that , but which remain available for future resolution. It is a repository of “the road not taken” each time we make a choice.</p><p>By characterizing NP as seed-truth, we emphasize that it carries <em>information and structure</em>. It is not sheer ignorance or void. In fact, even when something becomes known (P), the NP from which it emerged is what gave it coherence. The possibilities within NP are  but they are oriented toward truth – “infinitely potential” does not mean “lacking form,” but rather having  of any single actuality. Put differently, NP is the blueprint of truth in the sense that it contains more than reality at any moment can express. It is the , analogous to how Aristotle’s potentiality contains what could become actual under the right conditions. One can say NP holds  – not falsehoods, but truths that  be, awaiting a moment of realization. (Even in the realm of mythology, we find a similar idea: that before creation or manifestation, there is a formless void or chaos which nevertheless harbors the  for ordered reality – a mythic way to hint at NP as the womb of truth.)</p><p><strong>P: The Collapsed Present-Truth (Actualized Reality)</strong></p><p> is the realm of collapsed present-truth – the domain of what , here and now, as a realized fact or reality. P represents the manifestation of a specific path through the NP field of possibilities. When we say a truth has “collapsed” into P, we mean that out of the myriad possibilities, one definite configuration has been actualized. P is  in a dual sense: it is the truth that is present (i.e. currently actual), and it is the truth of the present moment (i.e. only in the present can truth be fully real). In our puzzle analogy, P is the completed section of the crossword – the answers filled in – while NP remains the unsolved cross-clues that those answers now partially reveal. Every time an NP seed collapses into a P outcome, we get a specific, concrete truth that can be lived, observed, or known directly.</p><p>It is important to note that P-truth is inherently . To collapse NP into P is to commit to a particular course among many – for example, to decide on one interpretation, to take one action, or to observe one outcome. Thus P is , and  in character: by realizing one option it excludes (for the moment) the others. We might think of NP as an open question and P as a particular answer. The answer gives us clarity and resolution (hence P is often “easier” or more straightforward once reached), but it also by necessity leaves other answers unchosen. In computational terms, solving a problem (P) picks out one solution and thereby eliminates the uncertainty among alternatives (NP). Metaphysically, the actual truth we have now stands against the backdrop of all the unactualized possibilities.</p><p>Another key aspect of P as present-truth is its . P only truly exists in the present moment – the . The past P-truth is no longer active (it has slipped into memory), and future truth (as such) is not yet realized. The present is often seen as the only reality that ontologically exists; as St. Augustine reflected, <em>the present is the only time that is real</em>, while the past and future are held in the mind (through memory and anticipation). Contemporary reflections echo this: “The future is entirely open, dependent upon choices made now. In truth, all we have is this never-to-be-repeated present moment.” Thus, P is where the rubber meets the road – where the infinite potential of NP is funneled into a single, concrete truth that . The present moment has the almost miraculous capacity to “bear” an otherwise infinite array of influences and condense them into one act or fact without exploding. We will explore later why the present has this unique capacity, but for now, suffice it to say P is the  – the realm in which truth takes on a stable, observable form.</p><p>It may sound like P, by picking one truth out of many, is a reduction or loss (from infinite to finite). However, P is also the  of NP potential. Until a truth is realized in P, it remains an abstract possibility; with realization, it gains . In Aristotle’s terms, actuality is the fulfillment of possibility. Thus P is the  of NP’s seed. Without P, NP’s designs would remain forever blueprint and never building. In that sense, P gives significance to NP by selecting a path through it. Every P-truth manifested affirms one aspect of truth that was implicit in NP, bringing it forth for all to witness.</p><p><strong>Myth: Future-Truth (Structured Uncertainty at the Edge of Collapse)</strong></p><p>By  we refer to future-truth – the structured uncertainty that exists at the horizon where NP is about to collapse into P. Myth, in our usage, is not a lie or mere fiction; it is a , a story or schema that anticipates meaning before it fully lands in reality. Myth is what truth  when it is still in transit from potential to actual – when it has taken a suggestive shape but is not yet concretized. In other words, myth occupies a liminal zone: it is more structured than the raw NP field (myth picks out particular patterns or narratives from the field of possibilities), but it is less definite than a P fact (myth has not been definitively proven or manifested yet).</p><p>Consider myths in the traditional sense: they are narratives that convey profound truths about life, the cosmos, or human nature, often through symbolic stories. These truths are “future” or transcendent in that they aren’t one-time occurrences; they speak to enduring possibilities and meanings which can manifest repeatedly in new forms. This is why it is said that myth is something that “” – a mythic truth might never have happened exactly as the story says, yet it is always happening in some form, because it expresses a structure of meaning that reality continually strives to realize. For example, a mythic hero’s journey was not a historical event, but it is “true” in that it maps onto the potential journey of every soul seeking purpose. In our terms, myth stands at the edge of NP collapse: it gives a recognizable form to what is otherwise uncertain, guiding the collapse without fully controlling it.</p><p>We can illustrate the role of myth as structured uncertainty with an example: Suppose a community holds a myth about a prophesied “chosen one” who will deliver them from hardship. This myth provides a framework of meaning – it identifies the kind of potential future they anticipate (deliverance by a hero). This is not yet a P truth; there is no actual chosen one (until perhaps someone steps into that role). But the myth shapes the community’s orientation toward the future. It is a : if eventually someone does arise and fulfill the prophecy, the myth becomes concretized in that person (NP collapses into P, guided by the mythic template). Until then, the myth is an  – it might come true in various ways, or perhaps not in the expected way – but it is  in that it’s not just open-ended chaos; it’s a particular envisioned narrative within the space of possibilities.</p><p>In our framework, myth operates as a necessary mediator between NP and P. Without mythic structures, the field of NP would be overwhelming and inarticulate – an infinite sea of possibilities with no pointers for significance. Myth provides  that help identify which possibilities matter or beckon. In a sense, myth is the <strong>face of NP as seen from the future</strong> – it is how the uncertain potential appears when we project our hopes, fears, and interpretations onto it. Each myth encapsulates some  or insight (a piece of truth) and yet leaves  that invites further interpretation. As our past analysis puts it, each mythic narrative can be seen as a  of the global truth field “into a cultural frame: stabilizing some wisdom (P) while leaving behind some mystery or moral ambiguity (NP) that keeps the myth alive, recursive, and generative”. In other words, myth bridges the known and unknown: part of it resonates as deeply true (the wisdom it imparts), and part of it remains open, spawning new questions or guiding new actions (the remaining ambiguity).</p><p>owever, myth is , not present-truth. It is essential to maintain that distinction. A myth can guide us, but if we  with literal present reality, we risk incoherence or even disaster. Taking a myth too literally – “collapsing into myth” in our terms – can overwhelm coherence because we start treating the structured uncertainty as if it were a completed actuality. History offers cautionary tales: powerful myths have at times been misused or misunderstood in literal ways that led to fanaticism or violence. For instance, political or nationalistic myths can grip a population’s imagination (a future vision of glory or destiny), but if taken as indisputable present truth, they may justify irrational or destructive actions. As one commentator noted, societies that lose a healthy mythic framework may fall into pathologies of identity, and reviving myths carelessly can unleash irrational forces (for example, the invocation of myths of a collective’s supreme destiny). In our terms, an unchecked collapse into myth – trying to live  in the story without grounding in present reality – “overwhelms coherence” because the narrative outruns the actual, and the structured uncertainty turns into unhinged conviction. Thus, myth must  the collapse of NP, but not  the need for actual P-truth. Myth gives purpose and direction, but it finds its proper fulfillment only when conscience uses it to inform real choices that yield real outcomes.</p><p><strong>Conscience - The Navigator of Collapse (Truth’s Recursive Agent)</strong></p><p> in this framework is the active agent that navigates between NP, myth, and P – essentially, the faculty of  and  that collapses potential into actuality in a meaningful way. It is through conscience that truth becomes a  endeavor, not just a happenstance. We use the term “conscience” to imply not only intellectual consciousness but also an ethical dimension; this is the inner voice or guiding  that both  and  about truth. In effect, conscience stands at the crossroads of freedom and responsibility: it sees the vast NP field of what could be, it hears the call of mythic ideals of what should be, and it acts in the present to bring about a particular truth (P) that honors both reality and meaning.</p><p>In our symbolic shorthand, we might say: <strong>Conscience mediates NP → P</strong>. But it does so not blindly (as matter-law interplay) – it typically selects a  that aligns with some guiding myth or value. A “P-seed” means an initial choice or a starting point for collapse. Given the enormity of NP’s potential, one does not simply collapse everything at once; one chooses a particular seed or aspect to actualize. This seed could be a question we decide to answer, a principle we decide to uphold, or a desire we decide to pursue. Conscience’s role is to choose the seed wisely, guided by an intuition of truth or goodness (often informed by mythic narratives or higher principles).</p><p>One way to visualize conscience is as a kind of bridge or . From one side, reality (or “Global P” in the earlier metaphor) sees only a unified truth, and conscience from that perspective is just a single point (since reality ultimately only knows the accomplished facts). But from the perspective of the agent (the person or moral being exercising conscience), one is aware of straddling two worlds – the actual and the possible. Indeed, as our last HackerNoon analysis of P vs NP indicates, “Reality itself is always on the [P] side… while conscience could operate on both sides at the same time”. Conscience experiences itself as having one foot in what is (the known, the present), and another foot in what could be (the unknown, the future). This unique position allows conscience to be  and reflective: it can consider hypotheticals, evaluate outcomes against ideals, and adapt its choices in light of past truths and future hopes.</p><p>Furthermore, conscience is what makes the process of truth-finding <em>iterative and self-correcting</em>. Unlike a mechanical collapse that might just follow a fixed rule, conscience can learn and change. It carries memory of past P-truths (and the lessons they taught) and it carries anticipation of future-truths (mythic aims), and it constantly reconciles the two in each present act. If NP is the seed-bed of truth and myth the envisioned bloom, conscience is the gardener that cultivates the seed to flower in reality. And like a good gardener, conscience must understand the “soil” (the real conditions of the world), the “seed’s nature” (the blueprint of potential) and the “climate” (the overarching myths or meanings of the time).</p><p>In summary, we ascribe to conscience a pivotal metaphysical necessity: <strong>truth emerges only from the recursive act of conscience collapsing NP into P through a deliberate P-seed</strong>. Without conscience, NP might collapse randomly (or not at all), yielding no meaning; with conscience but without myth, collapse might be directionless; with myth but without conscience, collapse might be fanatical or divorced from reality. All four elements – NP, P, myth, and conscience – are needed. Conscience is the  that ties them together, the continual exercise of choice that is informed by the potential (NP) and the ideal (myth) to produce the actual (P). In doing so, conscience also constantly revises its own understanding, hence it is : it not only produces truth, it reflects on truth, altering its future actions. The conscience, as suggested by the metaphysical perspective we promoted earlier, “exceeds structure” because it is the only thing that can know  structures (it can consider any possibility, any perspective). This makes it uniquely equipped to traverse the space of NP and navigate by myth toward concrete P outcomes.</p><p>Having set these definitions, we now turn to the dynamic interactions between NP, P, myth, and conscience – what we call the  – and explore how truth is realized or thwarted depending on how this collapse is managed.</p><p><strong>II: Collapse Dynamics: Balancing Potential, Myth, and Present</strong></p><p>Truth, in our framework, is not a static given but something that  through a process – the collapse of NP potential into P actuality. This collapse is not a one-time event but a continuous, iterative dynamic. However, not all collapses are equal. How one approaches the transition from potential to actual can lead to very different outcomes for the integrity of truth. We identify three archetypal “collapse modes” to avoid or embrace:</p><ul><li><strong>Collapsing into NP (Regressive Potential)</strong>: This represents a refusal or failure to collapse potential into any actuality – effectively, getting “lost” in the NP field. In such a case, one continually defers decision or manifestation in favor of exploring possibilities endlessly. The result is a  or stagnation: nothing is realized, potential remains forever potential. It is as if a seed is never planted or allowed to sprout; the blueprint remains on paper and no building is built. In human terms, collapsing into NP might look like perpetual indecision or analysis paralysis – one sees so many possible truths or choices that one chooses none. The rich structure of NP then becomes actually useless, a castle in the sky. Potential “regresses” because without actualization, even the understanding of the potential cannot advance (we never get feedback from reality). Thus, treating NP itself as the place to dwell (instead of a source to draw from) ultimately diminishes the meaning of NP. The infinite possibilities become a burden of infinite  rather than a fruitful field. In short, collapsing into NP is a kind of metaphysical , which leads to an impoverishment of truth: everything  be, but nothing .</li><li><strong>Collapsing into Myth (Overwhelmed Coherence)</strong>: This mode occurs when one becomes so guided by a particular myth or future-truth that one attempts to live  without properly translating it into present reality. Here, one does choose and act, but one’s actions are dictated by an ideal narrative to the point of ignoring the constraints or feedback of actual P-truth. The result can be an <strong>overwhelming of coherence</strong> – reality doesn’t oblige our grand story in all details, and forcing it can lead to incoherent outcomes. This could manifest as fanaticism, disillusionment, or chaos when the myth crashes against reality. Using the seed analogy, this is like planting an imaginary tree rather than a real seed – watering the ground in the belief that a legendary tree will sprout overnight because the myth said so. The structure that myth provided is potentially valuable, but without respecting the incremental, grounded process of actualization, coherence is lost. On a social scale, as mentioned earlier, entire communities can be swept up by a myth (a utopian vision, an apocalyptic fear, a national destiny), and if they try to impose it as immediate truth, the mismatch with reality leads to confusion or destruction. Myths are “true” in a profound sense, but their truth is often symbolic or directional – not a blueprint that can be  imposed on the present all at once. So collapsing into myth means to be overwhelmed by the  of truth without actually achieving the  of it, leading to what we call disintegration (the pieces no longer fit together in reality, even if they did in the myth).</li><li><strong>Collapsing into P (Enabling Recursive Becoming)</strong>: This is the optimal mode in our framework: using conscience to effect a controlled collapse of NP into a present P-truth, informed (but not dominated) by myth. Collapsing into P means one takes a concrete action or makes a concrete decision that realizes some aspect of potential . It is an act of  to a particular truth path, which necessarily foregoes other possibilities (for now) – but it does so with awareness and purpose. By focusing on what can be actualized in the present, this mode preserves coherence (because it stays grounded in what  or  in reality) and also moves the process forward (because each realization sets the stage for the next). We call this enabling  because each P-collapse is not the end, but a step in an ongoing journey. When one collapses NP into P properly, one doesn’t try to do it all at once or assume this P is the final truth; rather, one treats it as a provisional truth that will evolve. This attitude allows for course-correction and learning: today’s P truth will reveal something about NP that can seed tomorrow’s decisions. In personal terms, this could be living one’s values in daily small acts (instead of dreaming of being a hero in abstract) – each act teaches something and builds character, recursively approaching the ideals one holds.</li></ul><p>It might be helpful to illustrate these modes with a simple scenario. Imagine an artist who has a vision of a masterpiece (this vision is a  – an artwork that “could be,” filled with meaning). The artist also has a myriad of ideas and techniques they could use (this is their NP field of potential – sketches, styles, themes not yet consolidated). Now, if the artist collapses into NP, they might endlessly sketch fragments, experiment with styles, but never choose a final composition or medium – the masterpiece remains a potential in the mind, never realized on canvas. If the artist collapses into myth instead, they might become obsessed with the  of creating the greatest masterpiece and refuse to make any work that falls short of the perfect vision – they may start a painting but abandon it repeatedly because it doesn’t match the mythic ideal, or worse, they might delude themselves that  about the envisioned masterpiece is as good as making it. The result is either nothing finished or a confused work that tries to be too grand all at once and collapses under its own ambition (incoherence). But if the artist collapses into P, they begin a particular painting here and now – perhaps just one piece of the vision, tackled with the skills and limitations they currently have. They bring one aspect of the potential into reality. The painting they finish may not capture the whole mythic vision (no single piece could), but it is a real artifact – a truth in the present. From creating it, the artist learns more about their vision (what worked, what didn’t, what the vision might truly be asking for) – this new understanding is a retroactive illumination of the NP seed. With that knowledge, the artist can embark on the next artwork, gradually approaching closer to the fullness of their mythic ideal. In this way, through recursive cycles of creation, the mythic truth (the “masterpiece” idea) guides the potential toward ever more refined and meaningful collapses, each in the present, each real and adding to truth.</p><p>In practice, of course, life is complex and we often have mixtures of these tendencies. But the key point is that <strong>truth thrives when NP collapses into P under the guidance of myth and the governance of conscience</strong>. If we either refuse to collapse (staying in NP) or collapse too recklessly under myth’s sway without present grounding, we lose the thread of truth. Truth, one might say, , but it is <em>nourished by the infinite</em> (NP) and  (myth). Only in the present can the infinite potential be harnessed into something structured (as myth suggests) yet concrete. This aligns with a profound metaphysical stance: reality is an ongoing creation, and the present moment is where creation actually happens – the only place where the “uncreated” (the realm of possibilities) can become “created” (the realm of facts).</p><p><strong>III: Recursive Emergence: Truth as a Temporal Feedback Loop</strong></p><p>One of the most intriguing aspects of this framework is the idea that each P-collapse (each time a potential truth is actualized) <em>retroactively illuminates</em> the structure of the NP seed from which it came. This creates a  or feedback loop in the field of truth. In simple terms, by doing or realizing something, we come to  the possibility-space we started with, which in turn affects how we will approach the next realization.</p><p>This might sound abstract, but it is a very familiar phenomenon in knowledge and action. Consider scientific inquiry: a scientist has a hypothesis (potential truth) and conducts an experiment (an actualizing act). The result (P-truth) either confirms, refutes, or modifies the hypothesis. In any case, the outcome sheds light on the initial potential – perhaps revealing that the initial idea was too broad, or that an unexpected avenue exists, etc. The scientist then refines the hypothesis (the NP blueprint is adjusted) and tests again. Over time,  from this iterative loop of potential -&gt; actual -&gt; revised potential -&gt; new actual, and so on. Each experiment’s truth “illuminates” the structure of the underlying phenomenon a bit more.</p><p>Our metaphysical logic sees this process as fundamental not just to science, but to all truth-becoming. When conscience chooses a P-seed and collapses NP into an actual outcome, that outcome doesn’t stand alone – it shines backwards, as it were, revealing something about the field of possibility it came from. To use a visual metaphor, imagine walking in a dark forest (the unknown NP). You light a small torch of truth (a P realization). The light from the torch not only shows you the patch of ground where you stand; it also throws new shadows and highlights around you, hinting at shapes of trees and paths beyond your immediate spot. You now see  in what was previously dark. In seeing that structure, you might spot a clearer path forward – or a looming obstacle – which guides your next steps.</p><p>A concrete illustration was given earlier via a crossword puzzle analogy drawn from the computational perspective: if reality is like a fully solved crossword (Global truth), we only see one clue at a time. Solving that clue (finding one P truth) immediately gives letters that make other unsolved clues (remaining NP potential) more constrained and thus more intelligible. Initially, those other clues were perhaps completely uncertain; after solving one, you now know “the second letter of 5-down is X,” etc., which means some possibilities for 5-down are eliminated and others come into focus. In this way, each localized truth  (or at least retrospectively) exposes part of the structure of the unsolved part of the puzzle. Extrapolating to life: whenever we make a decision or realize a truth, we often say in hindsight, “Ah, so that’s what this possibility really entailed,” or “Now I see the pattern behind what was possible.” We sometimes only understand our options after we choose one of them.</p><p>This  has deep implications. It suggests that the NP blueprint is not static; our understanding of it evolves as we actualize portions of it. Potential and actuality thus engage in a dialogue over time. We might even say that NP “grows” in a certain sense as P grows – not that the possibilities themselves necessarily increase, but our comprehension of the possibility space increases, which effectively enriches NP’s relevance. The process is : it is directed toward truth by virtue of each step revealing more of the goal. In the domain of myth, this is mirrored by the way myths stay alive and yield new insights as we live through various experiences. A myth might mean something to us at one point in life, and after we go through some ordeal (actual experience), we come back and see new meaning in the myth. The narrative (myth) didn’t change, but our realized truths cast new light on what the myth was pointing to.</p><p>It’s worth noting that because of this recursive nature, <strong>truth is not a one-time achievement but a journey</strong>. We often speak of “the truth” as if it were a final static thing, but in this framework truth is a  – a field that is gradually unfolded. This does not mean there are no facts or that everything is relative; rather, each fact (each P collapse) is indeed a concrete truth, but it participates in a larger unfolding meaning. Think of each P-truth as a chapter in an ever-writing book of Truth. You need each chapter for the book to be complete, and each chapter also references and clarifies aspects of previous chapters. Conscience, as the author, may not have outlined the entire book from the start (that would be akin to having complete foreknowledge or a static God’s-eye-view of Global truth), but conscience has an evolving  of the narrative (through mythic imagination and experience) and can guide the writing in a direction that seems coherent and significant.</p><p>In short, <em>each P-collapse retroactively illuminates the structure of the NP seed</em>. This principle is the engine of learning, growth, and creativity. It is why we must engage with reality rather than just think about it: only through the feedback of actual collapse do we come to understand the blueprint of possibility well enough to make further and better collapses. It is also why we need myth: myth can help interpret the feedback. When an outcome surprises us or challenges us, mythic frameworks often provide a larger context to make sense of it (“this failure is like the hero’s trial by fire; it teaches humility” etc.). Thus, recursion is not merely mechanical iteration; it is . Through conscience, each cycle of collapse and insight ideally brings us closer to a truth that is not only factually clearer but also richer in meaning.</p><p><strong>IV: Geometrical Metaphor: Curvature of Potential vs. Flatness of Present</strong></p><p>To further clarify the differences between the NP field of potential and the P domain of realized truth, we introduce a geometrical metaphor. This metaphor will also highlight why the present (the arena of P) is uniquely suited to hold truth together, as opposed to the distortions that can occur if one tries to inhabit the infinite directly.</p><p>Imagine the space of possibilities (NP) as a kind of geometric space that can have different curvatures. By contrast, consider the space of actualized truth (P) as a flat, Euclidean space.  is the familiar, “flat” geometry of our everyday world (on a small scale): parallel lines stay parallel, angles of a triangle sum to 180°, and so on. , such as  (negatively curved) and  or elliptic (positively curved) geometries, have different properties: in hyperbolic geometry, for instance, space spreads out faster than on a plane (angles of a triangle sum to  than 180°, and there’s essentially “more room” than expected as you move outward), while in spherical geometry, space is finite and closes back on itself (angles sum to more than 180°, and lines eventually meet).</p><p>Now, consider <strong>NP as a non-Euclidean space of truth</strong> – it can have curvatures that represent how possibilities diverge or converge. <strong>A hyperbolic NP curvature</strong> would mean that as you move through possibilities, they proliferate and diverge exponentially; the space of potential “opens up” endlessly in every direction. This reflects the  nature of raw possibility – small differences in initial choice could lead to wildly different outcomes, like geodesics on a saddle surface diverging from each other. On the other hand, a  would represent a situation where the space of possibilities, though expansive, eventually loops back on itself; possibilities converge and reconnect. This could correspond to a scenario where different paths in the NP field lead to similar outcomes or recurring patterns – a kind of  where everything circles back to a few archetypal narratives. Spherical curvature can symbolize a closed worldview or a strongly structured possibility space (like a culture’s tightly knit mythos where every story leads to the same moral).</p><p>What happens when these curved possibility spaces collapse into actual occurrences? One way to see the act of collapse (conscience choosing a P-seed and manifesting P) is as a kind of projection of the curved space onto a flat plane. In that collapse, some of the “distortions” or curvature of the possibility space must resolve into a consistent, flat reality.  in the sense that it has to obey coherent logic and consistency – contradictory possibilities cannot both actualize at once, just as in Euclidean space you can’t have a triangle whose angles sum to both 200° and 270°; you must pick one geometry and stick to it locally. The present truth tends toward a kind of internal consistency and linearity that the space of all possible truths does not have to obey (within NP, you can have contradictory potentials existing side by side because only one will eventually be chosen, or perhaps they diverge into different branches of history or thought).</p><p>The metaphor helps illustrate a few points:</p><ul><li>If NP is too  (too open), a collapse is hard to achieve because the possibilities keep diverging – it’s like trying to flatten an infinitely flared saddle onto a plane: there’s always “more” coming at the edges. In human terms, this could correspond to times of great chaos or innovation where the range of possibilities is so large that it’s hard to settle on a stable truth. The risk here is disintegration – without a collapse, the truth would fragment into endlessly branching alternatives (analogous to how hyperbolic space has an infinite boundary).</li><li>If NP is  (too closed and looping), a collapse might result in a very limited or parochial truth – like projecting a globe onto a flat map, some distortion is inevitable (Greenland looks huge on a Mercator map, etc.). In our analogy, a spherical NP might correspond to a very rigid mythic structure where everything refers back to itself. Collapsing that onto reality might impose an overly narrow framework on truth, possibly stifling novelty. It might yield a stable truth, but one that’s “curved inward” and not accommodating the full breadth of reality (like a small traditional society where everything is understood within one closed set of myths – coherent but without co-relational meaning).</li><li>The Euclidean P is the middle ground: it is flat enough to allow local consistency and stability, but it can map portions of either hyperbolic or spherical regions . Each P-collapse could be seen as taking a small patch of the NP landscape and laying it flat as a piece of present reality. Over time, you can map more of the NP field by such patches (each patch is like one P-truth). You will never map the entire infinite hyperbolic plane or the entire sphere onto one flat sheet without distortion (that’s akin to infinity never fully becoming truth), but you  cover it in an atlas of local maps. That atlas is our accumulated body of truths in time.</li></ul><p>In plainer terms, the <strong>geometrical metaphor underscores that the present (P)</strong> can handle the complex curvature of possibility by flattening it locally. The present moment takes a slice of the complexity, resolves it, and yields a clear truth. Then the next moment takes another slice. If one tried to contain the whole curvature at once – for instance, to realize the  all at once – one would either tear the fabric (hyperbolic divergence causing chaos) or warp it (spherical self-containment causing potentially extreme distortion or dogmatism). This metaphor aligns with our earlier caution: infinity (the endless potential of NP)  in itself; it must be parceled into finite collapses. And similarly, a totalizing myth (a spherical closed world of meaning) cannot simply be overlaid on reality without careful translation; pieces of it can manifest, but the whole thing at once would either not fit or would crush the diversity of life.</p><p>Hyperbolic and spherical curvatures also correspond loosely to the two failure modes we discussed: hyperbolic corresponds to the chaotic scatter of not collapsing (everything flies apart – collapsing into NP yields nothing coherent), and spherical corresponds to the rigid imposition of one structure (everything curving into one point – collapsing into myth yields a single-point perspective that may ignore reality’s breadth). The Euclidean present is the balancing out – it is flat in that it only deals with one piece at a time, but by doing so it can stretch in any direction as needed over time (just as a flat plane can extend indefinitely).</p><p>In summary, this metaphor illustrates why <strong>the Present is the only “geometry” that can bear the infinite curvature of truth without disintegration</strong>: it does so by only dealing with a finite, locally flattenable piece at any given moment. The infinite informs each collapse (the shape of the NP region we’re collapsing from influences the outcome), but the infinite is never fully  as the truth we speak or live. We always handle it through projections and slices – through present acts.</p><p><strong>V: The Present and the Infinite - Finite Vessels of Endless Truth</strong></p><p>Our vision has repeatedly highlighted a theme: <strong>infinity never becomes truth, but informs all collapse</strong>. We should examine this more directly as a metaphysical principle and tie it to why the present moment holds such a privileged position in our framework.</p><p>What do we mean by “infinite truth” or “infinite potential” never becomes truth? Simply that the totality of all that could be – the NP field in its entirety, or the sum of all myths in their absolute scope – is never  as a single truth within reality. If it were, it would no longer be an infinity of possibilities; it would be one frozen actuality, and all the richness would collapse into a singular state. Reality, as we experience it, doesn’t work like that. Instead, reality gives us an unfolding series of finite truths, each a nugget chipped off the infinite block. Each truth is informed by the unactualized possibilities around it – you often understand something by contrasting it with what could have been – but the unactualized remains unactualized.</p><p>This resonates with philosophical and spiritual views on infinity and temporality. Many have observed that the  is like a moving knife-edge that “cuts” the potential of the future into the actuality of the past. And only this knife-edge is real in an existential sense. In theological terms, one might say only God or eternity holds the infinite whole; humans in time see one facet at a time. Our framework aligns with such perspectives but couches them in a more generalized logic of truth-making: the present is where an  with the infinite occurs. Each present moment is like a aperture through which a beam of infinite light passes and becomes a visible ray of truth in our world. The fullness of the light itself (the infinite) is never seen all at once; it would blind or dissolve the finite eye. But that does not mean the infinite is irrelevant – on the contrary, it’s the source of all the light we do see.</p><p>This is why we earlier said <strong>Present is the only domain capable of bearing infinite truth without disintegration</strong>. It bears it by  it – by being a fleeting, ever-renewed moment, the present can sustain contact with the infinite continuously but never has to contain it all at once. It is like a small cup under an endless waterfall: the cup is always full in the present; the waterfall flows on. If one tried to catch the whole waterfall in one vessel, the vessel would burst, and nothing coherent would remain.</p><p>From a more human perspective, consider how overwhelming it would be if one  every possibility (good and bad) that the future holds or if one tried to live out all one’s dreams simultaneously. It’s not possible – our sanity and coherence rely on doing things step by step, moment by moment. There’s wisdom in phrases like “one day at a time.” This is not just practical advice but metaphysically grounded: the Present is a buffer that protects us from the paralyzing effect of infinite possibilities and from the mania of infinite ambitions. It forces choices, which, though limiting, actually make meaning possible. A life that tries to remain “everything” ends up nothing in particular; a life that chooses something becomes .</p><p>We can also reflect on how <strong>conscience navigates the triad of NP, myth, and P</strong> in light of this. Conscience knows in some sense that it cannot grasp infinity directly. Instead, it uses myth to give a provisional shape or target to the endless potential (infinity “informs” myth by offering a direction – e.g., ultimate goodness, utopia, salvation, enlightenment – all these are infinite ideals). But conscience doesn’t attempt to grab the ideal wholesale; it translates it into finite aims and actions in the present. In doing so, it shows a kind of humility before infinity – acknowledging that <em>we can only embody truth finitely, but we can be informed by the infinitely true</em>.</p><p>For example, a person might have the infinite ideal of  (justice in its fullness is almost an infinite concept – an absolute state where all wrongs are righted). They cannot achieve perfect justice in one stroke; but they let that ideal inform their conscience, and today they perform a just action in a specific situation. That action doesn’t exhaust the ideal of justice, it is a collapse of potential into a bit of justice here and now. Tomorrow, they will need to do it again, in another situation, learning from yesterday’s outcome. Over time, perhaps the world becomes more just. But at no point was “infinite justice” present as a fact; it was always guiding from beyond, through myths of a just society or a divine justice, through principles and dreams. In this way, <em>infinity pervades the process without ever ceasing to be infinite</em>.</p><p>This helps guard against two extremes:</p><ul><li>one, the despair that truth or perfection is unattainable (it is, in totality, but that’s by design – we are to chase it, asymptotically, rather than hold it complete; this chase is meaningful and without it we’d have no direction)</li><li>two, the arrogance or fanaticism of thinking one has  Truth in entirety (which often leads to trying to impose an infinite ideal in finite terms and causing harm, akin to collapsing into myth entirely).</li></ul><p>Our framework suggests a middle path: <strong>truth is both available and elusive</strong>. It is available in each present collapse (we do get real truths, however partial), and elusive in that each truth opens the door to further depths of truth not yet realized.</p><p>Finally, this perspective underscores a kind of : since infinity never ceases informing collapse, there is always more truth to be discovered, more growth to be had. No present truth is the end of the story. This ensures that the recursive process does not stagnate. We don’t reach a point where we say “all done, nothing more to know or achieve” – because that would imply we’ve exhausted NP or fulfilled the myth entirely, essentially becoming infinite ourselves. Instead, there is a continual becoming. And yet, importantly, at any given step we do have something solid: a truth achieved, a meaning realized, a seed that grew. Therefore, our framework portrays reality as an <strong>endless becoming of truth, through finite meaningful steps</strong>.</p><p><strong>VI: Conscience at the Crossroads - Navigating NP, Myth, and P</strong></p><p>Having delved into various facets of the framework, let us circle back to the central actor in this drama: conscience. It is easy to describe the “structure” of NP, P, and myth, but without conscience these structures would not interact purposefully. Conscience is the pilot steering between the Sirens of infinite possibility and the rocks of immediate reality, with the stars of myth as a guide. Let us summarize how conscience accomplishes this navigation and why it is so crucial.</p><ol><li><strong>Listening to Myth (Future Truth as Guide)</strong>: Conscience absorbs the insights of mythic future-truths. These myths can be cultural stories, personal ideals, religious teachings, or imaginative projections. They supply conscience with a sense of  or . For example, the myth of the hero’s journey might inspire an individual to see their hardships as challenges to overcome rather than meaningless suffering. The myth of a prophesied golden age might lead a society to enact reforms for a better future. Conscience, however, must interpret myth wisely – understanding its symbolic language rather than following it blindly or literally. It asks: what core truth does this myth indicate? How can we honor that truth in reality, step by step? In doing so, conscience uses myth as a compass rather than a map. The compass points north (gives orientation), but one still has to navigate the terrain of reality to actually get somewhere.</li><li><strong>Reading the NP Field (Recognizing Seed-Truths)</strong>: Conscience surveys the NP realm of possibilities with a discerning eye. Not all possibilities are equal or worth pursuing. Some are dead-ends, some are harmful, some are trivial. Conscience, informed by mythic values and past experience, identifies meaningful  in the NP field – those possibilities that, if realized, would yield genuine progress or insight. This is akin to a skilled gardener recognizing which seeds are viable and beneficial to plant. In complex situations, this often requires creativity – seeing connections or potentials that are not obvious. Conscience might say, “Given what I know and what I aspire to (mythically), this particular possibility seems ripe – it could lead to the kind of truth we seek.”</li><li><strong>Executing Collapse (Actualizing in the Present)</strong>: Once a target possibility (P-seed) is chosen, conscience commits to collapsing it into P. This involves decision and action in the present. Here, practical reason, willpower, and clarity come to the fore. The infinite must be pruned away for a moment – one must focus on the finite task or choice at hand. This is where conscience often encounters resistance: doubt, fear, temptation to revert to open possibility (procrastination) or to drift into comforting fantasy (wishful thinking). The moral fortitude of conscience is tested. But a mature conscience follows through, understanding that only through actualization can truth advance.</li><li><strong>Learning and Adapting (Recursive Reflection)</strong>: After the collapse, conscience evaluates the outcome. Did the realized truth fulfill the expectation? Did it align with the mythic guidance or reveal a different lesson? What unforeseen consequences or new possibilities arose? This reflective step is what makes conscience . It updates its knowledge of both NP and myth. Perhaps the mythic compass needs recalibration (maybe the ideal is understood differently now), or maybe new potential avenues have opened. Conscience might celebrate a success but also see a next step, or it might admit an error and adjust course. Crucially, conscience does not rigidly stick to a single path if feedback indicates otherwise; it remains flexible, because its loyalty is to truth itself, not to any one idea of truth. In the words of our HackerNoon piece, “when conscience hides behind truth, only truth is visible… when it hides before truth, both conscience and truth vanish”. This cryptic phrase can be taken to mean: if conscience subsumes its ego entirely to what is true (hiding behind truth), then what matters is the truth realized (conscience’s work is done humbly). But if conscience puts itself (its fixed notions or pride) ahead of truth, then neither genuine conscience nor truth is served (they vanish into invisibility). Thus, conscience must always aim to  truth, not impose itself.</li><li><strong>Maintaining Balance (Avoiding the Extremes)</strong>: Through all these steps, conscience maintains a dynamic balance. It keeps NP, myth, and P in productive tension. If it finds itself too mired in possibilities (NP overload), it reminds itself of the mythic goals and the need to act. If it finds itself too dogmatically driven by a single vision (myth overload), it pays attention to real-world feedback and the diversity of NP (maybe the ideal needs a different approach). If it finds itself clinging to a past truth (being overly P-bound, complacent in “what is”), it remembers the infinite potential still out there and the higher ideals unfulfilled. In short, conscience is like a skilled sailor tacking between different winds, always adjusting to stay on course toward a distant star.</li></ol><p>In a well-functioning scenario, <strong>myth guides NP toward meaningful collapse via conscience</strong>. For example, a community’s myth of justice guides its lawmakers (conscience in collective form) to consider new policies (NP possibilities) and enact laws (P realities) that progressively realize justice. Each law, once enacted, teaches something about what justice really requires, refining the mythic vision and opening up new possibilities for improvement. Likewise, an individual’s conscience might use a personal myth of being a healer to guide career choices, pick up skills (NP possibilities), and take on actual roles or actions (P) that heal others. Over time, that person understands better what healing truly means, perhaps reshaping their mythic self-image and identifying further potentials to grow into.</p><p>Thus, the relationship is symbiotic: myth without conscience and action remains inert; NP without myth and conscience is aimless; P without drawing from NP and myth becomes stagnant. Conscience is the  that keeps truth flowing between the three realms.</p><p>We have constructed a metaphysical framework in which <strong>NP (uncollapsed seed-truth), P (collapsed present-truth), myth (future-truth)</strong>, and <strong>conscience (the agent of collapse)</strong> form an integrated system describing how truth emerges and evolves. In this framework, NP represents the  of truth – the richly structured field of what could be true – while P represents the  of truth – the concrete actuality that is true here and now. Myth stands as the , the not-yet-actual narrative or ideal that gives shape to our aspirations and interpretations, essentially the face of truth we glimpse on the frontier between the known and unknown. Conscience is the : the faculty that perceives, judges, and acts, thereby collapsing potential into actuality in a guided, meaningful way.</p><p>We have emphasized several key principles of this framework:</p><ul><li>: NP (potential truth) is not a chaotic void but an “infinitely potential” blueprint containing all that can become real. P (actual truth) is the fulfillment of one of those possibilities in the present moment. The interplay of NP and P recapitulates an old philosophical insight – that reality consists of both what exists and what is possible, with the possible continuously flowing into the actual.</li><li>: Myth was framed as “future-truth,” capturing truths that are always in the becoming. Myths provide  – they carry enduring wisdom or patterns (so they are not arbitrary), yet they do not pinpoint a single outcome (so they remain open). This makes myth a powerful guide for conscience: myth offers a vision to strive for, without dictating every detail. As a local collapse of global truth into narrative, myth stabilizes some truth while keeping some mystery, ensuring that cultures and individuals remain in a generative search for deeper understanding rather than claiming to possess all truth outright. However, we cautioned that myth must inspire, not imprison; taken too literally or absolutely, myth can lead to incoherence or fanaticism.</li><li>: We analyzed the consequences of how collapse is handled. Collapsing  NP (refusing to actualize) leads to stagnation and wasted potential. Collapsing  myth (over-identifying with future visions at the expense of present reality) leads to incoherence and possibly destructive illusions. Collapsing  P (the ideal mode) yields a stepwise, recursive process of becoming, wherein each realization is grounded and contributes to an ongoing growth. This dynamic highlights the <em>metaphysical necessity of the Present</em>: only the present can mediate between the endless openness of the future and the solidifying closure of the past, by taking just the “right-sized” slice of infinity at each moment.</li><li><strong>Recursive Truth and Illumination</strong>: A core insight of our framework is that truth emerges recursively. Each act of realizing a truth (each P-collapse) teaches us something about truth itself, revealing structures and relationships that were hidden in the space of possibilities. Knowledge and meaning thus expand in a feedback loop. The structure of the NP seed is illuminated by its fruiting in P, much as solving one part of a puzzle sheds light on the remaining parts. This makes the pursuit of truth a journey in time, where even errors and surprises become part of the clarification of the original blueprint. Over time, conscience and culture can build increasingly coherent and encompassing truths through this iterative refinement.</li><li><strong>Geometrical Metaphor – Curvature to Flatness</strong>: We proposed an analogy where NP’s potential space might be “curved” (hyperbolically divergent or spherically convergent), whereas P’s realized truth is “flatly” consistent. This visual metaphor was used to convey why large-scale possibility or absolute structures cannot be instantiated wholesale without distortion; instead, they must be realized in  that flatten out the local curvature. The present moment acts like a projection plane where a portion of the curved infinity is resolved into a workable reality. This reinforced the idea that <em>infinite truth informs each finite truth without ever becoming fully finite</em> – an idea resonant with theological and philosophical notions of an infinite that manifests partially but never exhaustively in the finite world.</li><li><strong>The Conscience’s Navigation</strong>: Finally, we highlighted how conscience, operating with free will and insight, navigates between NP, myth, and P. It listens to the guidance of mythic future-truth (ensuring collapses are meaningful and value-oriented), it evaluates the NP field to choose promising and principled potentials, and it executes actual collapses into P, thereafter learning from the outcomes. Conscience is thus the <em>dynamic principle of synthesis</em> in this metaphysical model – the reason the otherwise abstract elements of NP, P, and myth result in a living, developing truth. Conscience balances openness and commitment, imagination and practicality, ideals and facts. In a way, conscience is the present moment in its active, creative aspect – it’s where the buck stops, where decisions are made that shape reality.</li></ul><p>, the framework presented – which we have titled “The Present of Collapse” – suggests that <em>truth is not a static entity but a living process</em>. It is the process of endless potential being guided by meaningful form and collapsed into present reality through conscious choice. In this view, truth has a  (NP–Myth–P) enacted by a  (Conscience). This structure accounts for why truth has aspects of eternal mystery (there’s always more beyond what we know), concrete certainty (some things are definitively true now), and aspirational narrative (we sense patterns or destinies that draw us onward). The logic of collapse and recursion offers a way to think about change and choice that neither reduces truth to arbitrary relativism nor to fixed absolutism: truth  and is  real piece by piece, and in doing so, it remains ever aligned with an infinite backdrop of meaning that keeps it from being merely relative or “small.”</p><p>Such a metaphysical framework can have various implications. It provides a lens to interpret personal growth, scientific discovery, social progress, and even cosmic evolution as a unified story of truth realizing itself. It honors the role of imagination and myth in human life, without sacrificing the importance of rational coherence and empirical grounding. It places moral and intellectual conscience at the heart of reality’s unfolding, hinting that perhaps the universe’s truths are not just “out there” to find, but also “in here” to create responsibly. In sum, the  framework invites us to see ourselves as participants in a recursive dance of truth – always choosing, always learning, guided by ideals, grounded in reality, and never exhausting the richness of what can be known. It is a vision that is both humbling (for it acknowledges the endless and the unknown) and empowering (for it asserts the value of our choices and the possibility of progress). And ultimately, it portrays a cosmos where truth is less a monolithic monument and more an ever-flowing river – one we navigate in our finite boats of the present, with currents from the infinite deep and stars of meaning overhead, always moving toward a horizon we never fully reach but continually approach.</p><p>\\\n<strong>VIII: Addendum: On the P vs NP Question</strong></p><p>To answer the foundational question through the lens of this metaphysical framework:</p><p>P is indeed  from NP .</p><p>A global equivalence between P and NP would imply a resolution of the universe — a total collapse of all potential into manifest form. But the universe, as shown, resists total resolution, because it is structured upon recursive unfolding, not finality. The only “resolution” beyond such recursion would be an atemporal, adimensional, and a-existential truth — a domain outside of being itself, inaccessible to form. And maybe this is the very domain of absolute Truth, truthfullness across infinite posibility.</p><p>On  terms, P is  to NP.</p><p>That is, each instance of present truth (P) reflects a coherent collapse of seed-potential (NP), such that from within the recursion, the emergence appears seamless. P unfolds what NP held, not because they are the same, but because conscience recursively integrates what it once only observed.</p><p>Thus, the resolution of P ≠ NP is not computational, but ontological: The difference protects reality, and the local equivalence permits it to grow.</p>","contentLength":64455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An inside look at Meta’s transition from C to Rust on mobile","url":"https://engineering.fb.com/2025/07/01/developer-tools/an-inside-look-at-metas-transition-from-c-to-rust-on-mobile/","date":1751385623,"author":"","guid":179013,"unread":true,"content":"<p>Have you ever worked is legacy code? Are you curious what it takes to modernize systems at a massive scale?</p><p><a href=\"https://www.threads.net/@passy_\" target=\"_blank\" rel=\"noopener\">Pascal Hartig</a> is joined on the latest Meta Tech Podcast by Elaine and Buping, two software engineers working on a bold project to rewrite the decades-old C code in one of Meta’s core messaging libraries in Rust. It’s an ambitious effort that will transform a central messaging library that is shared across Messenger, Facebook, Instagram, and Meta’s AR/VR platforms.</p><p>They discuss taking on a project of this scope – even without a background in Rust, how they’re approaching it, and what it means to optimize for ‘developer happiness.’</p><p>Download or listen to the episode below:</p><p>You can also find the episode wherever you get your podcasts, including:</p><p>The&nbsp;<a href=\"https://insidefacebookmobile.libsyn.com/\" target=\"_blank\" rel=\"noopener\">Meta Tech Podcast</a>&nbsp;is a podcast, brought to you by Meta, where we highlight the work Meta’s engineers are doing at every level – from low-level frameworks to end-user features.</p><p>And if you’re interested in learning more about career opportunities at Meta visit the&nbsp;<a href=\"https://www.metacareers.com/?ref=engineering.fb.com\" target=\"_blank\" rel=\"noopener\">Meta Careers</a>&nbsp;page.</p>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Reporting to Reasoning: How AI Is Rewriting the Rules of Data App Development","url":"https://towardsdatascience.com/from-reporting-to-reasoning-how-ai-is-rewriting-the-rules-of-data-app-development/","date":1751385608,"author":"TDS Brand Studio","guid":179017,"unread":true,"content":"<p>Explore the shift from static reports to intelligent apps with our first ebook.</p>","contentLength":79,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understand your software’s supply chain with GitHub’s dependency graph","url":"https://github.blog/security/supply-chain-security/understand-your-softwares-supply-chain-with-githubs-dependency-graph/","date":1751385600,"author":"Andrea Griffiths","guid":179050,"unread":true,"content":"<p>What if you could spot the weakest link in your software supply chain before it breaks?</p><p>With GitHub’s <a href=\"https://docs.github.com/en/code-security/supply-chain-security/understanding-your-software-supply-chain/about-the-dependency-graph\">dependency graph</a>, you can. By providing a clear, complete view of the external packages your code depends on, both directly and indirectly, it allows you to understand, secure, and manage your project’s true footprint.</p><p>If you’re like me and sometimes lose track of what’s actually powering your applications (we’ve all been there!), GitHub’s dependency graph is about to become your new best friend.&nbsp;</p><h2>What is the dependency graph?</h2><p>Here’s the thing: Every modern software project is basically an iceberg. That small manifest file with your direct dependencies seems quite harmless at first glance. But underneath? There’s this massive, hidden world of transitive dependencies that most of us never think about. The GitHub dependency graph maps this entire underwater world. Think of it like a family tree, but for your code. Each package is a family member, and each dependency relationship shows who’s related to whom (and trust me, some of these family trees get  complicated).</p><p>Each package is a node. Each dependency relationship is an edge. The result? A full visual and structured representation of your software’s external codebase.</p><blockquote><p>In some cases, 95–97% of your code is actually someone else’s. The dependency graph helps you make sense of that reality.</p></blockquote><p>Let that sink in for a moment. We’re basically curators of other people’s work, and the dependency graph finally helps us make sense of that reality.</p><p>When vulnerabilities are discovered in open source packages, the consequences ripple downstream. If you don’t know a vulnerable dependency is part of your project, it’s hard to take action.</p><p>The dependency graph isn’t just a cool visualization (though it is pretty neat to look at). It’s the foundation that makes <a href=\"https://github.com/dependabot\">Dependabot</a> alerts possible. When a security issue is found in any of your dependencies (even a transitive one), GitHub notifies you. You get the full picture of what’s in your supply chain, how it got there, and what you can actually do about it.</p><h2>See it in action: From 21 to 1,000 dependencies</h2><p>Eric showed us a project that looked innocent enough:</p><ul><li> (the ones actually listed in package.json)</li><li> (including everything that got pulled in along the way)</li></ul><p>With the dependency graph, you can finally:</p><ul><li><strong>Understand which dependencies are direct vs. transitive</strong></li><li><strong>Trace how a package like Log4j ended up in your codebase.</strong> (Spoiler: it probably came along for the ride with something else.)</li><li><strong>Know what’s yours to fix and what depends on an upstream maintainer</strong></li></ul><h2>Tighten your supply chain with Dependabot</h2><p>Dependabot runs on top of the dependency graph—so enabling the graph is what makes Dependabot’s vulnerability alerts and automatic fix suggestions possible.&nbsp;</p><p>Pro tip: Filter for direct dependencies first. These are the ones you can actually control, so focus your energy there instead of pulling your hair out over transitive dependencies that are someone else’s responsibility.</p><h2>How to enable the dependency graph</h2><p>You can enable the dependency graph in your repository settings under <strong>Security &gt; Dependency Graph</strong>. If you turn on , the graph will be enabled automatically.</p><p>Using GitHub Actions? Community-maintained actions can generate a Software Bill of Materials (SBOM) and submit it to GitHub’s Dependency submission API, even if your language ecosystem doesn’t support auto-discovery.</p><p>✅  Dependency graph and Dependabot alerts are free for all repositories.</p><p>You can’t secure what you can’t see. GitHub’s dependency graph gives you visibility into the 90%+ of your codebase that comes from open source libraries and helps you take action when it counts.</p><ul><li>(seriously, do it now)</li><li><strong>Use it with Dependabot for automated alerts and fixes</strong></li><li><strong>Finally discover what’s actually in your software supply chain</strong></li></ul><p>Your future self (and your security team) will thank you.</p>","contentLength":3897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Remark raises $16M to build out human-powered expert models for e-commerce","url":"https://techcrunch.com/2025/07/01/remark-raises-16-million-in-to-build-out-human-powered-expert-models-for-e-commerce/","date":1751385600,"author":"Ivan Mehta","guid":179009,"unread":true,"content":"<article>Startups working on AI-powered e-commerce tools often rely on external data or user signals to build and improve their models. But Remark lets thousands of human experts chat with users while they are purchasing items, and then using that knowledge to train models.</article>","contentLength":265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Threads gets its own DMs as app distances itself from Instagram","url":"https://techcrunch.com/2025/07/01/threads-gets-its-own-dms-as-app-distances-itself-from-instagram/","date":1751385540,"author":"Sarah Perez","guid":179008,"unread":true,"content":"<article>At launch, Threads DMs will support one-on-one chats, preset emoji reactions, the ability to report spam, and mute DMs (as on Instagram). Other features, like group messaging, inbox filters, and more advanced message controls, will arrive in a later release.</article>","contentLength":258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Teaching AI to Say \"I Don't Know\": A Four-Step Guide to Contextual Data Imputation","url":"https://hackernoon.com/teaching-ai-to-say-i-dont-know-a-four-step-guide-to-contextual-data-imputation?source=rss","date":1751385485,"author":"Impute","guid":179070,"unread":true,"content":"<p>(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);</p><p>(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).</p><p>We represent the missing data mechanism as a conditional distribution of M given X, which is parameterized by an unknown ϕ, as follows.</p><p>\\\nIn the literature, the following three standard mechanisms for missing data are defined [21].</p><p>\\\n<strong>Missing completely at random (MCAR).</strong> An MCAR case occurs when the probability that a value of a variable is missing is independent of the variable itself and any other variables, expressed as follows.</p><p>\\\nIn MCAR, the missingness probability depends neither on the missing variable nor on the observed variables.</p><p>\\\n The probability that the value of a variable is missing only depends on the observed values of other variables XO. Thus, the missingness is independent of the missing variables and the missing value is predictable from the observed variables, formalized as follows.</p><p>\\\n<strong>Missing not at random (MNAR).</strong> This case corresponds to missing mechanisms that are neither MCAR nor MAR. In MNAR, the reason for a value to be missing, can depend on other variables, but also on the value that is missing.</p><p>\\\nUnlike MAR, the missingness in MNAR cannot be predicted only from the observed variables. There is a no general method of handling MNAR missing data properly [14].</p><p>\\\nOften the reasons for missing data is ignored when the missingness is due to MCAR or MAR, thus imputation methods can be simplified [33]. For this reason, the majority of research covers the cases where missing data are of the MAR or the MCAR type.</p><h3>2.3 Generating Missing Values</h3><p>We constructed synthetic datasets with up to 30% missing values by applying the following three missingness mechanisms on complete datasets: MCAR, MAR and MNAR. The implementations of these mechanisms are modified from [20].</p><p>\\\n. It was introduced by randomly removing 30% of the observations from each feature.</p><p>\\\n. First, we select all observations within the 30-th percentile range of an independent feature (usually the first column in the dataset). Then, we randomly remove 60% observations from each corresponding (dependent) feature.</p><p>\\\n. We remove the observations of a feature if the observations fall within the 30-th percentile range of the feature value.</p><p>Figure 1 illustrates the CLAIM process, which encompasses four stages: (1) constructing a contextualized natural language dataset, (2) generating suitable descriptors for</p><p>\\\nmissing values, (3) creating a missingness-aware contextualized dataset, and (4) adapting an LLM for downstream tasks. We detail these stages below.</p><p>\\\n<strong>Constructing a Contextualized Natural Language Dataset.</strong> We construct a contextualized natural language dataset from a numeric dataset X containing missing values. The objective is to generate contextually suitable description of each attribute and its measures in natural language. For instance, a record from the UCI Wine dataset [12] with numeric input and output attributes is contextualized as follows: <em>“The alcohol content in the wine is 12.47. The level of malic acid in the wine is 1.52 … The class of the wine is classified as class 1 wine.”</em>[1] This step converts numeric values into detailed descriptions, preparing the dataset for embedding missing value descriptors.</p><p>\\\n<strong>Generating Suitable Descriptors for Missing Values.</strong> Unlike conventional imputation methods that estimate missing values from observed data using numerical methods, we utilize contextually-relevant descriptors of missing values for imputation. We generate these descriptors by a conversational LLM (e.g., OpenAI’s ChatGPT-3.5 [2]). We prompt the LLM with a dataset description and instruct it to generate missing value descriptors, such as: <em>“For any missing attribute values, suggest a descriptor for the missing data that I can place in those cells.”</em> This method relies on the LLM’s extensive knowledge base to produce appropriate missing value descriptors. A list of feature-specific contextually relevant missing-value descriptors for selected datasets are provided in the Appendix.</p><p>\\\n<strong>Creating a Missingness-Aware Contextualized Dataset.</strong> We construct the missingness aware contextualized natural language dataset Xmissingness_aware by replacing the missing values with the generated descriptors. This process ensures that each data instance is aware of its missing attributes, thus capable of improving the LLM’s ability to learn from incomplete data by providing explicit context. Furthermore, we use distinct descriptors for separate features in the dataset that contain missing values, thereby implicitly informing an LLM to handle the missingness of each feature in a contextually-suitable way for improving the performance of the downstream task.</p><p>\\\n<strong>Adapting an LLM for Solving Downstream Tasks.</strong> The final step involves finetuning a pre-trained LLM with the missingness-aware, contextually-rich dataset. We incorporate specific task instructions and strategies for handling missing data into the fine-tuning process. For instance, for classification tasks, we might include instructions like: <em>“Predict the class based on the given measurements. Use the context provided by missing value descriptors to inform your prediction.”</em></p><p>\\\nThis structured approach, from transforming datasets to fine-tuning LLMs, signifies a comprehensive method for addressing data missingness through the capabilities of LLMs.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2405.17712\">available on arxiv</a> under CC by 4.0 Deed (Attribution 4.0 International) license.</p><p>[1] the Python script used for contextualization is provided in the Supplementary Material.</p>","contentLength":5713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trump’s prospective TikTok buyer reportedly includes Oracle, Blackstone, a16z","url":"https://techcrunch.com/2025/07/01/trumps-prospective-tiktok-buyer-reportedly-includes-oracle-blackstone-a16z/","date":1751385002,"author":"Amanda Silberling","guid":179007,"unread":true,"content":"<article>After pushing the TikTok sale date back yet again, President Donald Trump said in an interview on Sunday that he had found a potential buyer for the ByteDance-owned platform.</article>","contentLength":174,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Automated My Content Side Hustle with Notion, ChatGPT, and Zapier — Here's the Exact Workflow","url":"https://hackernoon.com/i-automated-my-content-side-hustle-with-notion-chatgpt-and-zapier-heres-the-exact-workflow?source=rss","date":1751384982,"author":"The Crypto Quill","guid":179069,"unread":true,"content":"<h2>Intro: The Dream of Passive Productivity</h2><p>Everybody's going on about AI and automation. Does it really work, though, if you are an individual creator who wants to create a side income?</p><p>I experimented: I set up an automated system to write, schedule, and post my content solely with ChatGPT, Notion, and Zapier. My aim was straightforward — to find out whether or not I could save time, generate passive income, and cut down on content creation burnout.</p><p>No smoke-and-mirrors. No \"just believe me\" assertions.</p><p>This is day-by-day how I did it, what I cabled up, and <strong>what actually happened in 7 days</strong>.</p><h2>Step 1: Content Ideation with ChatGPT</h2><p>“<em>Generate title variations, outline blog posts, and create tweet threads.”</em></p><p><strong>I gave ChatGPT one prompt:</strong><em>“Give me 10 viral content ideas about using automation to earn passive income with Notion and Zapier.”</em></p><p><strong>ChatGPT brought back these gems:</strong></p><ul><li>\"How I Built a Passive Income Engine Using Just AI and No-Code Tools\"</li><li>\"The Lazy Creator's Guide to Automating a Content Business\"</li><li>\"How to Earn While You Sleep With Notion + Zapier Workflows\"</li></ul><p>I selected two and used ChatGPT to outline the blog post structure, complete with H2 headers and CTA lines.</p><h2>Step 2: Organizing in Notion</h2><p> Notion \\n   Content calendar, automation trigger base, and task manager.</p><p><strong>I created a content database in Notion with these columns:</strong></p><ul><li>Status (Idea, Drafting, Done)</li><li>Platform (Medium, Gumroad, HackerNoon)</li><li>Output (Tweet, Post, Newsletter)</li></ul><p>**Why Notion?\n\\  Because it plays *really* well with Zapier.</p><p>I tagged content by status, and every time I updated a status to “Done,” it triggered the automation in Step 3.</p><h2>Step 3: Zapier Automation</h2><p> Zapier \\n   Automate cross-posting and link distribution.</p><p><strong>Here’s the automation flow I built:</strong></p><ul><li> New “Done” status in Notion</li><li> Auto-post summary + link to Twitter (via X)</li><li> Add to newsletter draft in ConvertKit</li><li> Log in Airtable for analytics</li></ul><ul><li> I produced 3 times as much content by outsourcing formatting and idea generation. </li><li> My content got consistent, owing to Notion+Zapier oversight. </li><li><strong>Low effort, actual reach:</strong> I constructed an auto-distribution loop in under 2 hours.</li></ul><ul><li><strong>Zapier's basic plan is too limiting</strong> – just 100 tasks/month. Must upgrade. </li><li> – Zapier would not tweet if it was done too often. </li><li><strong>Affiliate link clickthrough</strong> was underperforming without strong CTAs or images.</li></ul><ul><li>Webhooks to automate Substack formatting</li><li>Canva API to generate auto thumbnails</li><li>A webhook that triggers when I  a keyword (using Voiceflow + Zapier)</li></ul><h2>Takeaway: It's Real, But It’s Not Magic</h2><p>AI won't  — but it can . If you're a solo content creator and you want to launch a content business, then running admin and distribution automates creative hours.</p><p><strong>But only if your system is based on receipts, not hopes.</strong></p><p>Have you tried automating your side hustle? What’s in your stack? I respond to every comment — especially the ones that break my workflow.</p><p>📧 <strong>You can reach me at [debestnext@gmail.com]</strong></p>","contentLength":2895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HN Slop: AI startup ideas generated from Hacker News","url":"https://www.josh.ing/hn-slop","date":1751383905,"author":"coloneltcb","guid":180364,"unread":true,"content":"<div>© 2025 Just Joshing, LLC. All rights reserved.</div>","contentLength":47,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434938"},{"title":"The Last Rank We Need? QDyLoRA's Vision for the Future of LLM Tuning","url":"https://hackernoon.com/the-last-rank-we-need-qdyloras-vision-for-the-future-of-llm-tuning?source=rss","date":1751383804,"author":"Model Tuning","guid":179068,"unread":true,"content":"<p>\\\n<strong>A. Supplementary Material</strong></p><p>QDyLoRA offers an efficient and effective technique for LoRA-based fine-tuning LLMs on downstream tasks. Eliminating the need for fine-tuning multiple models to find the optimal LoRA rank and offering the possibility of fine-tuning larger LLMs are two main advantages of QDyLoRA. The experimental results demonstrated that the optimal rank for QDyLoRA can be surprisingly low, yet it consistently outperforms QLoRA. QDyLoRA provides greater flexibility for deploying LLMs in various contexts and represents a promising step towards making fine-tuning large language models more accessible and efficient.</p><p>While the 4-bit QDyLoRA exhibits notable performance, it falls short of achieving the performance levels of full precision fine-tuning. One possible solution could be dynamic quantized DyLoRA (DyQDyLoRA), in which the quantization level could also vary during finetuning. In particular, the finetuning strategy can dynamically switch between different quantization levels based on a predefined learning feedback. Additionally, further research is required to investigate the impact of LoRA's scalar and the range of underlying ranks in QDyLoRA.</p><p>Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255.</p><p>\\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p><p>\\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p><p>\\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.</p><p>\\\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale pretrained language models. Nature Machine Intelligence, 5(3):220–235.</p><p>\\\nAli Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. 2022. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650.</p><p>\\\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2021. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366.</p><p>\\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p><p>\\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR.</p><p>\\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p><p>\\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</p><p>\\\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023. Open-assistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327.</p><p>\\\nSe Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee. 2022. Alphatuning: Quantization-aware parameterefficient adaptation of large-scale pre-trained language models. arXiv preprint arXiv:2210.03858.</p><p>\\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965.</p><p>\\\nXiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. Webglm: Towards an efficient webenhanced question answering system with human preferences. arXiv preprint arXiv:2306.07906.</p><p>\\\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. 2021. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprint arXiv:2110.07577.</p><p>\\\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005.</p><p>\\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.</p><p>\\\nMojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. 2022. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint arXiv:2210.07558.</p><p>\\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.</p><p>Table 4 provides an overview of the hyperparameters and experimental configurations employed in this study, which are crucial configurations that determine various aspects of the training process and model behavior in this study. Common key parameters across the experiments include the choice of optimizer, Adam-Beta2 value, maximum gradient norm, and warmup ratio, which collectively influence how the model adjusts its weights during training. LoRA-specific parameters such as LoRA dropout probability, maximum LoRA rank, and alpha value control the behavior of LoRA layers. Additionally, double quantization and quantization type impact the precision of numerical representations within the model, which are considered the same as baselines. Learning rate scheduling and weight decay contribute to the optimization process, helping to prevent overfitting and stabilize training. Random seeds ensure reproducibility, while the specified GPU determines the hardware used for training. Each model configuration, whether for the Web-GLM, GSM8k, or the specific experiment outlined in Table 1, features parameters tailored to the characteristics of the dataset and the computational resources available. These hyperparameters collectively shape the training process, ultimately influencing the performance and effectiveness of the models in the study.</p><h2>A.2 Generated Text Quality</h2><p>To describe the quality of texts generated by QDyLoRA, a sample query taken from GSM8k is fed to Falcon-40b trained by QDyLoRA. Below, we report the generated answers for different LoRA ranks.</p><p>\\\n: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market?</p><p>\\\n: Janet has 16 - 3 - 4 = «16-3-4=7»7 eggs left. She sells 7 - 16 = «7-16=9»9 eggs at the</p><p>\\\n\\\nfarmers’ market. She makes 9 * 2 =«9*2=18»18 at the farmers’ market every day.</p><p>\\\n She eats 3 eggs for breakfast and bakes 4 muffins per day, so she uses 3 + 4 = «3+4=7»7 eggs per day. She has 16 - 7 = «16- 7=9»9 eggs left over each day. She sells these eggs at the farmers’ market for $2 per egg, so she makes 2 * 9 = «2*9=18»18 per day at the market.</p><p>\\\n: She eats 3 eggs for breakfast and bakes 4 muffins per day, so she uses 3+4 = «3+4=7»7 eggs per day. She sells 16-7 = «16-7=9»9 eggs per day for $2 per egg. She makes 9*2 = «9*2=18»18 per day at the market.</p><p>\\\n: Janet’s ducks lay 16 eggs per day and she eats 3 for breakfast every morning so she has 16-3=13 eggs left. Janet bakes 4 muffins every day and she uses 4 eggs so she has 13-4=9 eggs left. Janet sells the remaining eggs at the farmers’ market for $2 each so she makes 9*2=«9*2=18»18 dollars a day. Janet makes 18 dollars a day at the farmers’ market.</p><p>(1) Hossein Rajabzadeh, University of Waterloo and Huawei Noah’s Ark Lab (hossein.rajabzadeh@uwaterloo.ca);</p><p>(2) Mojtaba Valipour, University of Waterloo (mojtaba.valipour@uwaterloo.ca);</p><p>(3) Tianshu Zhu, Huawei Noah’s Ark Lab (tianshu.zhu@huawei.com);</p><p>(4) Marzieh Tahaei, Huawei Noah’s Ark Lab (marzieh.tahaei@huawei.com);</p><p>(5) Hyock Ju Kwon, (hjkwon@uwaterloo.ca);</p><p>(6) Ali Ghodsi, (ali.ghodsi@uwaterloo.ca);</p><p>(7) Boxing Chen, Huawei Noah’s Ark Lab (boxing.chen@huawei.com);</p><p>(8) Mehdi Rezagholizadeh, Huawei Noah’s Ark Lab (mehdi.rezagholizadeh@huawei.com).</p><p>:::info\nThis paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.</p>","contentLength":9159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Grammarly acquires AI email client Superhuman","url":"https://techcrunch.com/2025/07/01/grammarly-acquires-ai-email-client-superhuman/","date":1751383503,"author":"Ivan Mehta","guid":179006,"unread":true,"content":"<article>In its announcement, Grammarly said it wants to build AI agents for emails using Superhuman's tech. </article>","contentLength":100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The electric Hummer is almost outselling the F-150 Lightning","url":"https://techcrunch.com/2025/07/01/the-electric-hummer-is-almost-outselling-the-f-150-lightning/","date":1751383366,"author":"Sean O'Kane","guid":179005,"unread":true,"content":"<article>GM's EVs were popular in the second quarter, a complete contrast to the struggles by Ford, Tesla, Hyundai, and Kia to grow electric vehicle sales.</article>","contentLength":146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proton Joins Antitrust Lawsuit Against Apple's App Store Practices","url":"https://apple.slashdot.org/story/25/07/01/0917211/proton-joins-antitrust-lawsuit-against-apples-app-store-practices?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751383200,"author":"msmash","guid":179012,"unread":true,"content":"Encrypted communications provider Proton has joined an antitrust lawsuit against Apple, filing a legal complaint that claims the company's App Store practices harm developers, consumers, and privacy. The Switzerland-based firm joined a group of Korean developers who sued Apple in May rather than filing a separate case. \n\nProton asked the US District Court for Northern California to require Apple to allow alternative app stores, expose those stores through its own App Store, permit developers to disable Apple's in-app payment system, and provide full access to Apple APIs. The company added a privacy-focused argument to typical antitrust complaints, contending that Apple's pricing model particularly penalizes companies that refuse to harvest user data. Developers of free apps typically sell user data to cover costs, while privacy-focused companies like Proton must charge subscriptions for revenue, making Apple's commission cuts more burdensome.","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta adds business voice calling to WhatsApp, explores AI-powered product recommendations","url":"https://techcrunch.com/2025/07/01/meta-adds-business-voice-calling-to-whatsapp-explores-ai-powered-product-reccomendations/","date":1751382900,"author":"Ivan Mehta","guid":179004,"unread":true,"content":"<article>WhatsApp announced it's introducing the ability for large businesses to reach customers through voice calls, which will allow the app to explore the use of AI-powered voice agents.</article>","contentLength":180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask HN: Who is hiring? (July 2025)","url":"https://news.ycombinator.com/item?id=44434576","date":1751382096,"author":"whoishiring","guid":179306,"unread":true,"content":"Please state the location and include REMOTE for remote work, REMOTE (US)\nor similar if the country is restricted, and ONSITE when remote work is  an option.<p>Please only post if you personally are part of the hiring company—no\nrecruiting firms or job boards. One post per company. If it isn't a household name,\nexplain what your company does.</p><p>Please only post if you are actively filling a position and are committed\nto responding to applicants.</p><p>Commenters: please don't reply to job posts to complain about\nsomething. It's off topic here.</p><p>Readers: please only email if you are personally interested in the job.</p><p>Don't miss these other fine threads:</p>","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434576"},{"title":"Ask HN: Who wants to be hired? (July 2025)","url":"https://news.ycombinator.com/item?id=44434574","date":1751382096,"author":"whoishiring","guid":180363,"unread":true,"content":"Share your information if you are looking for work. Please use this format:<pre><code>  Location:\n  Remote:\n  Willing to relocate:\n  Technologies:\n  Résumé/CV:\n  Email:\n</code></pre>\nPlease only post if you are personally looking for work. Agencies, recruiters, job boards,\nand so on, are off topic here.<p>Readers: please only email these addresses to discuss work opportunities.</p>","contentLength":355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434574"},{"title":"What If Your Blog Had a Product Manager? (Hint: It’s You)","url":"https://hackernoon.com/what-if-your-blog-had-a-product-manager-hint-its-you?source=rss","date":1751382005,"author":"Editing Protocol","guid":179067,"unread":true,"content":"<p>Most bloggers focus on publishing content—but the best ones treat their blog like a product. That means thinking strategically: defining your audience, setting goals, building systems, and iterating based on feedback.</p><p>Whether you’re writing to grow a personal brand, build a business, or simply share ideas, applying product management principles to your blog can help you stay focused, intentional, and impactful.</p><p>Here’s how to step into the role of <strong>Product Manager of your blog</strong>—and start running your content like a pro.</p><p>:::tip\nAlready an expert? Share your learnings using <a href=\"https://app.hackernoon.com/new?template=blog-strategy-reflection\">this template</a>!</p><h2>Step 1: Formulate a VISION for your career in content</h2><p>How do you want to see yourself growing and operating as a writer and blogger?</p><p>Outline the specific challenges your writing is intended to solve—who it’s for, why it matters, and when they’ll need it. Go with the first thoughts that come to mind, and don’t be afraid to get really specific. \\n </p><p>:::info\n when you try to speak to everyone, you end up speaking to no one.</p><h3><strong>Here’s a content strategy vision template to help guide your thinking:</strong></h3><h2><strong>Step 2: Document Your Ideal Reader PERSONAS</strong></h2><p>This is a crucial exercise in empathizing with your readers—understanding their pain points and knowing what they need so your content can directly address those needs.</p><p>Creating at least 3–5 ideal reader personas helps you refine not only the tone, branding, and design of your content, but also your overall content strategy, calendar, and monetization opportunities.</p><p>:::info\nIf you don’t yet have access to analytics about your readers, that’s okay—start with educated guesses. Try quick long-tail keyword research by typing potential questions into Google and reviewing the suggested queries and related searches. Tools like Ahrefs, Answer the Public, or even your own search history can help. Ideally, you’ll use Google Analytics to uncover a wealth of information about the audience already engaging with your work.</p><h2><strong>Step 3: Pick Your PROTO-PERSONA and Start Detailing</strong></h2><p>From your 3–5 ideal reader personas, choose one to represent the majority of your target audience. Then use your imagination (and research) to flesh out the following details:</p><ul><li> What do you know about this person? (Age, education, location, job, background)</li><li> Do they need to earn more money? Change careers? Master a skill? Be the first to know something? Build a side hustle?</li><li> Are they introverted or extroverted? Where do they hang out online and offline? What are their routines? How do they learn—visually, audibly, by reading?</li><li> Do they want to start a business? Learn a tool or software? Grow an audience? Improve time management?</li></ul><p>By putting all this together, you’ll have a clear map of who you’re creating content for—a smart foundation for planning and writing.</p><h2><strong>Step 4: Sketch Out Your Reader Journey Map</strong></h2><p>This step helps define your content distribution strategy by putting yourself in your reader’s shoes and mapping out the path they take toward having their needs met through your content.</p><p>Start with your vision statement and your readers’ goals. What might they be typing into Google right now? Make a list of relevant search terms or questions your content could answer.</p><ul><li>How will my ideal audience discover my content? At <a href=\"https://hackernoon.com\">HackerNoon</a>, for example, we get a lot of direct traffic (people typing the URL directly or bookmarking us) and organic traffic (people searching and finding our stories). That tells us our brand is strong and our content answers real questions.</li><li>How would they most enjoy engaging with my content? Does your post need a YouTube embed? Could you repurpose it for social media? Should it become a newsletter, podcast, or ebook?</li></ul><h2><strong>Step 5: Define Your Content Strategy, Goals, and Focus</strong></h2><p>This final step is about clarity. Define your long-term strategy, your unique value as a creator, and potential blockers to success. It’s where you turn your content creation into a career plan.</p><h3>Start with this question:</h3><blockquote><p><strong>What would wild success look like?</strong></p></blockquote><p>Write down outcomes for the following time frames:</p><ul></ul><h3>Next, write your six-month goal as a statement, as though it’s already happened:</h3><blockquote><p><em>“In the last six months, I published 100,000 words and created 3 months’ worth of reading time. I also launched a podcast / niche TikTok / LinkedIn community, built my personal site, and got my first 100 newsletter signups.”</em></p></blockquote><h3>Then, play the pessimist for a second and ask the hard questions:</h3><ul><li>What are the potential blockers to my success?</li><li>What are my current challenges?</li><li>What do I need to prioritize to overcome them?</li><li>How can I turn daily struggles into #relatablecontent?</li></ul><h3>And lastly, ask yourself:</h3><blockquote><p>What sets me apart from other creators in my niche?</p></blockquote><ul><li>What’s the most meaningful thing I could focus on to rise above the noise?</li><li>What unique perspectives and experiences do I bring?</li><li>Which creators could I collaborate with to reach a more relevant audience?</li></ul><p>:::tip\n<strong>👉 Ready to put this into practice?</strong></p><p>By treating your blog like a product, you're not just publishing content—you’re building a career. A roadmap. A strategy for your ideas.</p><p>Start small, iterate often, and remember: <strong>you’re the product manager of your writing life.</strong></p>","contentLength":5149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CLAIM: A Contextual Language Model for Accurate Imputation of Missing Tabular Data","url":"https://hackernoon.com/claim-a-contextual-language-model-for-accurate-imputation-of-missing-tabular-data?source=rss","date":1751381878,"author":"Impute","guid":179066,"unread":true,"content":"<p>(1) Ahatsham Hayat, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (aahatsham2@huskers.unl.edu);</p><p>(2) Mohammad Rashedul Hasan, Department of Electrical and Computer Engineering, University of Nebraska-Lincoln (hasan@unl.edu).</p><p>\\\n. This paper introduces the Contextual Language model for Accurate Imputation Method (CLAIM), a novel strategy that capitalizes on the expansive knowledge and reasoning capabilities of pre-trained large language models (LLMs) to address missing data challenges in tabular datasets. Unlike traditional imputation methods, which predominantly rely on numerical estimations, CLAIM utilizes contextually relevant natural language descriptors to fill missing values. This approach transforms datasets into natural language contextualized formats that are inherently more aligned with LLMs’ capabilities, thereby facilitating the dual use of LLMs: first, to generate missing value descriptors, and then, to fine-tune the LLM on the enriched dataset for improved performance in downstream tasks. Our evaluations across diverse datasets and missingness patterns reveal CLAIM’s superior performance over existing imputation techniques. Furthermore, our investigation into the effectiveness of context-specific versus generic descriptors for missing data highlights the importance of contextual accuracy in enhancing LLM performance for data imputation. The results underscore CLAIM’s potential to markedly improve the reliability and quality of data analysis and machine learning models, offering a more nuanced and effective solution for handling missing data.</p><blockquote><p><strong><em>‘Well! I’ve often seen a cat without a grin,’ thought Alice; ‘but a grin without a cat! It’s the most curious thing I ever saw in all my life!’</em></strong></p><p><em>Lewis Carroll, Alice’s Adventures in Wonderland (1865)</em></p></blockquote><p>\\\nA compelling real-world example of how context-unaware estimation of missing data can defy reality and compromise the integrity of downstream tasks is highlighted in [35]. This account describes a scenario where a predictive machine learning (ML) model, developed to process tabular demographic data including individuals’ ages, faced challenges due to missing age entries. The imputation strategy employed involved substituting missing age values with zeros—a common default for initializing integers in several programming languages. This approach inadvertently led the model to categorize individuals with unspecified ages as “toddlers”, resulting in aberrant model behavior. Numerous instances echoing this type of bias in ML models, resulting from context-unaware imputation of missing data, are reported in the literature [35,14,38,43,34,1].</p><p>\\\nThese incidents prompt a critical inquiry into more sophisticated and reality-congruent methods for estimating missing tabular data. While simple statistical replacements such as the mean or median might suffice under the assumption of a normal distribution, predictive ML techniques like k-Nearest Neighbors (k-NN), random forest (RF), or even deep learning (DL)-based generative models offer alternative strategies [20,13,45,8]. These ML/DL methods typically presuppose that missingness in an attribute correlates with observable values in other features. However, this raises fundamental questions: What if the missing data is independent of observed values? Or if the absence of data is influenced solely by unobserved variables? In scenarios where missingness is not attributable to external factors or other observed data, the challenge then becomes how to accurately estimate the missing values. To date, no single imputation method has proven universally effective, underscoring the complexity and variety of missing data scenarios encountered in practice [20].</p><p>\\\nThis paper introduces a novel approach, leveraging the capabilities of pre-trained large language models (LLMs) [6,9,39,26], to innovatively address the challenge of missing data in tabular datasets. Our method, the <strong>Contextual Language model for Accurate Imputation Method (CLAIM)</strong>, diverges significantly from traditional imputation techniques that predominantly estimate missing values through numerical methods. Instead, CLAIM harnesses LLMs’ expansive knowledge [28,29] and reasoning capabilities [9,42,4] in a : initially, it employs LLMs to generate contextually relevant natural language descriptors for missing values, effectively transforming datasets into natural language contextualized formats. This transformation is crucial, as it aligns the data with the inherent strengths of LLMs, making it more amenable to their processing capabilities.</p><p>\\\nSubsequently, these enriched datasets serve as the foundation for fine-tuning LLMs to enhance performance in downstream tasks (e.g., classification), showcasing a unique and effective use of language models beyond their conventional applications. By incorporating contextually relevant descriptors for missing data, CLAIM not only addresses the variability and specificity inherent in data across different domains but also adeptly navigates the complexities introduced by various missingness mechanisms. Through this innovative integration of LLMs into the data imputation process, CLAIM aims to deliver a more nuanced, accurate, and reliable method for data recovery, essential for improving the quality of subsequent data analysis and machine learning tasks.</p><p>\\\nTo assess the effectiveness of CLAIM, we undertake a comprehensive analysis across three standard missing data mechanisms—MCAR (Missing Completely at Random), MAR (Missing at Random), and MNAR (Missing Not at Random) [30], and comparing CLAIM against a wide range of existing imputation methods spanning single and multiple imputation techniques, non-ML and ML methods, and discriminative and generative ML approaches. Our empirical studies, aimed at evaluating the impact of CLAIM on LLM-based downstream classification tasks, are guided by two principal research questions (RQs):</p><p>\\\n How effective is CLAIM in imputing missing values across the distinct missingness mechanisms (MCAR, MAR, and MNAR) and how does it compare with existing imputation methods in terms of accuracy and robustness across varied datasets and missing data scenarios?</p><p>\\\n How does the choice of phrasing for missingness descriptors in CLAIM affect the performance of LLM-based downstream tasks?</p><p>\\\nThe main contributions of this work are multifaceted. Firstly, CLAIM represents a departure from traditional imputation methods by using LLMs to generate context specific descriptors for missing data, establishing a new benchmark in data imputation. Secondly, through extensive empirical evaluation, we demonstrate CLAIM’s superior performance over existing methods across varied datasets and missingness patterns. Lastly, our analysis of context-specific versus generic descriptors provides key insights into optimizing LLM performance for imputation tasks, highlighting the significance of contextual accuracy. Collectively, these contributions advance data preprocessing techniques and open novel pathways for applying LLMs in complex data science challenges.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2405.17712\">available on arxiv</a> under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":7217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NIH-Funded Science Must Now Be Free To Read Instantly","url":"https://science.slashdot.org/story/25/07/01/0827211/nih-funded-science-must-now-be-free-to-read-instantly?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751380800,"author":"msmash","guid":178965,"unread":true,"content":"Starting today, researchers funded by the US National Institutes of Health (NIH) will be required to make their scientific papers available to read for free as soon as they are published in a peer-reviewed journal. That's according to the agency's latest public-access policy, aimed at making federally funded research accessible to taxpayers. From a report: Established under former US president Joe Biden, the policy was originally set to take effect on 31 December for all US agencies, but the administration of Biden's successor, Donald Trump, has accelerated its implementation for the NIH, a move that has surprised some scholars. That's because, although the Trump team has declared itself a defender of taxpayer dollars, it has also targeted programmes and research projects focused on equity and inclusion for elimination. And one of the policy's main goals is to ensure equitable access to federally funded research. \n\nThe move means that universities will have less time to advise their researchers on how to comply with the policy, says Peter Suber, director of the Harvard Open Access Project in Cambridge, Massachusetts. There is usually \"some confusion or even some non-compliance after a new policy takes effect, but I think universities will eventually get on top of that,\" he says.","contentLength":1299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Faster, More Accurate IoT Security: A Quantitative Analysis of the CUMAD Framework","url":"https://hackernoon.com/faster-more-accurate-iot-security-a-quantitative-analysis-of-the-cumad-framework?source=rss","date":1751380788,"author":"Hypothesis","guid":179065,"unread":true,"content":"<p>(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);</p><p>(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);</p><p>(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).</p><p>In this section we perform evaluation studies to investigate the performance of CUMAD using the publicdomain N-BaIoT dataset [8]. In order to better understand the evaluation studies, we will first describe the dataset, in particular, the features of the data points contained in the dataset. We will also compare the performance of CUMAD with that of the N-BaIoT scheme (which is the name for both the dataset and the corresponding scheme on detecting compromised IoT devices) [8].</p><h3>5.1. Dataset, Features, and CUMAD System Setup</h3><p>\\\nN-BaIoT contains both benign and (Mirai and Bashlite) attack traffic of 9 commercial IoT devices, including two doorbells (Danmini and Ennio), an Ecobee thermostat, three baby monitors (different models from Provision and Philips), two SimpleHome security cameras, and a Samsung webcam. Benign IoT device traffic was collected immediately after the corresponding IoT device was connected to the experimental testbed. Care was taken to ensure that various representative normal operations and behaviors of IoT devices were collected into the benign dataset.</p><p>\\\nIn the N-BaIoT dataset, each data point corresponds to an arrived packet, and contains 115 statistical features, which together represent a behavioral snapshot that describes the context of the corresponding packet when it arrives at the data collection point. The snapshot contains the source and destination device information, the protocol information, among others. More specifically, the 115 features were extracted in the following manner. For each arriving packet, a total of 23 features were collected at different levels of aggregation (see Table 1), including features aggregated at source IP address level, at source MAC and IP addresses level, at level of channel (source and destination IP addresses), and at socket level (source and destination IP addresses and port numbers). These 23 features were extracted in a sliding window fashion, over 5 time windows of 100ms, 500ms, 1.5sec, 10sec, and 1min, respectively, generating a total of 115 features for each data point.</p><p>\\\nWe use the Keras sequential model as the foundation for our development of the autoencoder [12]. The model’s input dimension is set to match the number of features in the dataset (that is, 115). To ensure effective compression, we implement three hidden layers within the encoder. These layers progressively reduce the dimensions to 87, 58, 38, and 29, respectively, with the last one (29) being the dimension of the output layer of the encoder, that is, the dimension of the obtained code. Conversely, the decoder component mirrors the dimensions of the encoder layers in the reverse order, starting from 38. By employing compression and decompression in the encoder and decoder layers, we effectively eliminate redundant information from the features of the input data points. To optimize training performance, we utilize the Adam optimizer, and the mean square error is used as the reconstruction error (objective function of the model).</p><p>\\\nSPRT requires four user-defined parameters in order to compute the upper and lower bounds A and B (see Eq. (3)), as well as the step function for computing Λn following each observation (see Eq. (1)). The desired values for both the false positive rate and the false negative rate (represented by α and β, respectively) are typically very small. In this study we set both α and β to 0.01. Ideally, the parameter θ indicates the true probability of an observation being classified as an anomaly, from either a benign or compromised IoT device. We determine the values for θ0 and θ1 through our preliminary studies, and set them to 0.2 and 0.8, respectively.</p><p>Table 2 shows the performance of CUMAD in detecting IoT devices, in terms of accuracy, recall, and F1 score [12]. From the table we can see that CUMAD achieves superior performance in all three metrics. For example, for 5 of the IoT devices, CUMAD is able to detect all the compromised cases (see the column of Recall). CUMAD is also able to detect vast majority of the compromised cases for the remaining two of the IoT devices, with recall scores of 0.999 and 0.994. Considering both detection precision of attack and benign traffic, we can see that CUMAD also performs very well, with an accuracy score ranging from 0.955 to 0.995 for all 7 IoT devices. The F1 scores, which is a weighted average of the precision and recall scores of a model, also confirm that CUMAD performs well in detecting compromised cases.</p><p>\\\nFigure 4 shows the false positive rates of an autoencoder based anomaly detection scheme and CUMAD. As shown in the figure, the false positive rates of the autoencoder-based anomaly detection scheme for the 7 IoT devices range from 0.77% to 11.22%, while the false positive rates of CUMAD range from 0.014% to 2.067%. On average the autoencoder based anomaly detection scheme has about 3.57% false positive rate, while the false positive rate of CUMAD is about 0.5%, which represents about 7 times performance improvement in terms of false positive rate for CUMAD over the autoencoder-based anomaly detection scheme.</p><p>\\\n\\\nFor performance comparison, we also include in the table the performance results of the N-BaIoT scheme, with the same evaluation studies setup. We can see from the table that CUMAD and N-BaIoT performs comparably in terms of all three-performance metrics. However, N-BaIoT works on a fixed window size. Table 2 shows that N-BaIoT requires a relatively large window size, ranging from 20 to 82 (column with name Window Size). In contrast, CUMAD works in an online fashion and does not requires such a fixed window size. Table 2 shows the average number of observations required for CUMAD to reach a detection (column with name Mean Size); we can see from the table that it takes on average less than 5 observations for CUMAD to make a detection of a compromised case, much quicker than NBaIoT. In order to have a better understanding of the number of observations for CUMAD to make a detection of a compromised case, Figure 5 shows the cumulative distribution function (CDF) of required observations for CUMAD to make a detection for all the 7 IoT devices. We can see from the figure that the vast majority of detection requires less than 10 observations for all 7 IoT devices.</p><p>\\\n\\\nIn summary, compared to simple anomaly detection schemes such as the ones only based on autoencoders, CUMAD can greatly reduce the false positive rates, making CUMAD much more attractive than simple anomaly detection schemes in the real-world deployment. Compared with window-based schemes such as N-BaIoT, CUMAD requires much less observations to reach a detection, and thus can detect compromised IoT devices much quicker.</p><p>In this paper we have developed CUMAD, a cumulative anomaly detection framework for detecting compromised IoT devices. CUMAD employs an unsupervised neural network autoencoder to classify whether an individual input data point is anomalous or normal. CUMAD also incorporates a statistical tool sequential probability ratio test (SPRT) to accumulate sufficient evidence to detect if an IoT device is compromised, instead of directly relying on individual anomalous input data points. CUMAD can greatly improve the performance in detecting compromised IoT devices in terms of false positive rate compared to the methods only relying on individual anomalous input data points. In addition, as a sequential method, CUMAD can quickly detect compromised IoT devices. Evaluation studies based on public-domain IoT dataset N-BaIoT confirmed the superior performance of CUMAD.</p><p>[1] Yin Minn Pa Pa, Shogo Suzuki, Katsunari Yoshioka, Tsutomu Matsumoto, Takahiro Kasama, and Christian Rossow. Iotpot: analysing the rise of iot compromises. In Proceedings of the 9th USENIX Conference on Offensive Technologies, pages 9–9, 2015.</p><p>\\\n[2] Michael Fagan, Katerina Megas, Karen Scarfone, and Matthew Smith. Foundational cybersecurity activities for iot device manufacturers. Technical report, National Institute of Standards and Technology, May 2020.</p><p>\\\n[3] Mohammed Ali Al-Garadi, Amr Mohamed, Abdulla Khalid Al-Ali, Xiaojiang Du, Ihsan Ali, and Mohsen Guizani. A survey of machine and deep learning methods for internet of things (iot) security. IEEE Communications Surveys &amp; Tutorials, 22(3):1646–1685, 2020.</p><p>\\\n[4] Andrew A Cook, Goksel Mısırlı, and Zhong Fan. Anomaly detection ¨ for iot time-series data: A survey. IEEE Internet of Things Journal, 7(7):6481–6494, 2019.</p><p>\\\n[5] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):1–58, 2009.</p><p>\\\n[6] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.</p><p>\\\n[7] Abraham Wald. Sequential Analysis. John Wiley &amp; Sons, Inc, 1947.</p><p>\\\n[8] Yair Meidan, Michael Bohadana, Yael Mathov, Yisroel Mirsky, Asaf Shabtai, Dominik Breitenbacher, and Yuval Elovici. Nbaiot—network-based detection of iot botnet attacks using deep autoencoders. IEEE Pervasive Computing, 17(3):12–22, 2018.</p><p>\\\n[9] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1–38, 2021.</p><p>\\\n[10] Erol Gelenbe and Mert Nakıp. Traffic based sequential learning during botnet attacks to identify compromised iot devices. IEEE Access, 10:126536–126549, 2022.</p><p>\\\n[11] Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N Asokan, and Ahmad-Reza Sadeghi. D¨ıot: A federated self-learning anomaly detection system for iot. In 2019 IEEE 39th International conference on distributed computing systems (ICDCS), pages 756–767. IEEE, 2019.</p><p>\\\n[12] Francois Chollet. Deep learning with Python. Simon and Schuster, 2021.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2404.13690\">available on arxiv</a> under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":10226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Redefining IoT Threat Detection: The Power of Cumulative Analysis in the CUMAD Framework","url":"https://hackernoon.com/redefining-iot-threat-detection-the-power-of-cumulative-analysis-in-the-cumad-framework?source=rss","date":1751380392,"author":"Hypothesis","guid":179064,"unread":true,"content":"<p>(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);</p><p>(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);</p><p>(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).</p><h2>3.2. Sequential Probability Ratio Test</h2><p>Sequential probability ratio test (SPRT) is a simple yet powerful statistical tool that has found applications in many different domains, in particular, fault detection or quality control [7]. SPRT is a variant of the traditional probability ratio test for testing under what distribution (or with what distribution parameters), it is more likely to have the observed sequence of samples. Unlike traditional probability ratio test that requires a pre-defined fixed number of samples to carry out the test, SPRT works in an online fashion; it updates the corresponding statistical measure as samples arrive sequentially, and can conclude when sufficient samples have arrived to reach a decision. In its simplest form, SPRT is a statistical method to test a simple null hypothesis against a simple alternative hypothesis. In the following we will more formally describe the operation of SPRT.</p><p>\\\nLet y denote a Bernoulli random variable with an unknown parameter θ, and let yi , for i = 1, 2, . . . denote the corresponding successive observations of y. SPRT can be used to test a simple null hypothesis H0 that θ = θ0 against a simple alternative hypothesis H1 that θ = θ1, more specifically,</p><p>\\\nAs a simple and powerful statistical tool, SPRT possesses a few critical and desired properties that lead to the wide-spread application of the technique in many different domains. First, the false positive and false negative rates of SPRT can be specified by user-desired error rates, which in turn control the thresholds of the model. Second, it has been proved that, among all sequential and non-sequential probability ratio testing algorithms, SPRT minimizes the expected number of observations to reach a decision with no greater errors. Put in another way, on average SPRT can reach a conclusion quickly compared to other probability ratio testing algorithms.</p><p>In this section we will first discuss the considered network model, where CUMAD will be deployed, and then we will present the design of the CUMAD framework.</p><p>Figure 2 illustrates the conceptual network model, where CUMAD is deployed. As shown in the figure, in order for CUMAD to carry out its task to detect compromised IoT devices in a network, CUMAD needs to have access to the network traffic associated with the IoT devices in the network. Depending on the deployment scenarios of CUMAD in the network and the corresponding network architecture, there can be a few different ways for CUMAD to obtain the corresponding network traffic of IoT devices. In essence, CUMAD as a network-based solution can be deployed in a similar way as network-based intrusion detection systems.</p><p>\\\n\\\nIn the current design of CUMAD, (statistical) features from raw network traffic will be extracted and fed to CUMAD for detecting compromised IoT devices. Each input data point fed to CUMAD comprises these extracted features, and can be summarized at different levels of granularity of network traffic, such as packets, flows, and time windows. These features will capture the network behavioral characteristics of the corresponding IoT devices. In Section 5 we will discuss the network traffic features contained in the public-domain N-BaIoT dataset when we perform evaluation studies on CUMAD [8].</p><h2>4.2. CUMAD: Cumulative Anomaly Detection</h2><p>Figure 3 illustrates the high-level architecture of the CUMAD framework. CUMAD consists of two main components: an anomaly detection component (ADC) and a cumulative anomaly component (CAC). Assuming the model has been properly trained (will be discussed shortly), given an input data point with the corresponding features, the main responsibility of ADC is to classify an input data point as either normal or anomalous. After the classification of the input data point, the result is passed to the second component (CAC), which will maintain a cumulative view of the network traffic behavior of the corresponding IoT device, by sequentially merging the individual classification results into the view. When sufficient evidence on an IoT device has been collected to indicate that it has been compromised, an alert will be generated. In the following we will describe each component in details, both in model training and deployment to detect compromised IoT devices.</p><p>\\\nWe note that different types of IoT devices perform drastically different functionalities, and in addition, we would like to detect which IoT device is compromised, we need to develop a separate CUMAD model for each IoT device and monitor their network traffic behaviors separately using their corresponding CUMAD models. Therefore, the following discussions are for one IoT device. We note that, although there are vastly diverse types of IoT devices on the Internet, autoencoder is a powerful neural network that is capable of learning different models. Therefore, we are able to build diverse autoencoder models, one for each IoT device, despite their vastly different network traffic behaviors of</p><p>\\\n\\\nthese IoT devices. In addition, IoT devices also provide us with unique opportunities in establishing the models of normal behaviors, compared to traditional computer systems. In particular, each IoT device only performs a few well-defined simple functionalities in an autonomous or semiautonomous fashion, with very limited user interactions after the initial device configuration and setup. This makes it simpler to establish a model of normal behaviors in carrying out anomaly detection.</p><p>\\\n<strong>4.2.1. Model Training and Setup.</strong> Before CUMAD can be used to monitor network traffic to detect compromised IoT devices, we need to train a CUMAD system for each IoT device so that it can learn the normal model of the device. During the training stage of a deployed CUMAD system, it is critical that we should only feed normal (benign) network traffic of the device to the system. This can be done, for example, when an IoT device is first deployed in the network. In order to minimize false positives during the detection stage, it is also important that CUMAD has a reasonably complete view of all the normal network traffic behavior of the device.</p><p>\\\n\\\nAs discussed above, the premise of using an autoencoder as an anomaly detection mechanism is that, although it can effectively reconstruct data points that are similar to the data points that it has seen previously during the training stage, it in general performs poorly to reconstruct data points that substantially differ from the training data. This is manifested in large reconstruction errors. Therefore, we will use the reconstruction error as the anomaly score, and when the anomaly score is greater than the pre-defined threshold, we classify the corresponding input data point as an anomalous sample.</p><p>\\\n\\\nThe parameters α and β are the user-desired false positive rate and false negative rate, respectively. They normally have small values for all practical applications, for example, in the range 0.01 to 0.05. The initial value of Λn in Eq. (1) is set to 0 during the setup stage of the system. The functionality of the Alert module is to generate proper alert to inform system administrators of the detection of a compromised IoT device. Other actions can also be taken based on the local security policies, for example, informing proper agents to isolate the compromised IoT device.</p><p>\\\n After the model has been trained and the required parameters have been set for the CUMAD system, it can be used to monitor network traffic to detect if the corresponding IoT device has been compromised. In the following we describe the basic steps of a CUMAD system in carrying out the detection task (see Algorithm 1).</p><p>\\\n\\\ngenerated to indicate a normal data point. The output of the Detector module is then passed to the SPRT module to determine if sufficient evidence has been accumulated to make a decision regarding the nature of the IoT device (compromised or normal; line 10 of the algorithm). SPRT updates the probability ratio measure Λn according to Eq. (1), as the 0 (normal data point) and 1 (anomalous data point) output sequence of the Detector module arrives (lines 13 to 18). After the value of Λn is updated for each input data point, SPRT compares the value of Λn with the two boundaries A and B to determine if a decision can be made (lines 20 to 29). When the value of Λn hits or crosses the upper bound B, SPRT will conclude that the alternative hypothesis H1 is true, that is, the IoT device has been compromised. In this case, SPRT will inform the Alert module the detection of an compromised IoT device. Proper alert will be generated and corresponding system administrators will be informed. In addition, from this time on, it is not necessary for CUMAD to monitor the IoT device anymore, until proper actions have been taken to clean up or remove the device.</p><p>\\\nWhen the value of Λn is equal to or smaller than the lower bound A, SPRT reaches the conclusion that H0 is true, that is, the IoT device is not compromised. From the viewpoint of detecting compromised IoT devices, this conclusion is less interesting in that we cannot terminate the monitoring of the device as we have done when a compromised IoT device is detected. A normal IoT device may become compromised at a later time. Therefore, in this case, we will reset the state of SPRT to restart the monitoring of the IoT device, in particular, we will reset the value of Λn to zero. If a decision cannot be reached at this time (line 28), SPRT will simply wait for additional input data points and repeat the same procedure.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2404.13690\">available on arxiv</a> under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":10009,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"They Hired Me to Steal a Shopping Cart Full of Human DNA 🧬 Darknet Diaries Ep. 160: Greg","url":"https://www.youtube.com/watch?v=YV5XVcNLHzs","date":1751380242,"author":"Jack Rhysider","guid":178972,"unread":true,"content":"<article>Greg Linares (AKA Laughing Mantis) shares his hunts for Zero Day exploits, the triple(?) agent working at Microsoft, how he became the youngest hacker ever arrested in Arizona, and that time a client hired him to steal human DNA.\n\nFollow Greg on Twitter: https://x.com/Laughing_Mantis.\n\nVisit https://darknetdiaries.com/episode/160 for a list of sources, full transcripts, and to listen to all episodes.</article>","contentLength":403,"flags":null,"enclosureUrl":"https://www.youtube.com/v/YV5XVcNLHzs?version=3","enclosureMime":"","commentsUrl":null},{"title":"David George from a16z on the future of going public at TechCrunch Disrupt 2025","url":"https://techcrunch.com/2025/07/01/david-george-on-the-future-of-going-public-at-techcrunch-disrupt-2025/","date":1751380200,"author":"TechCrunch Events","guid":178957,"unread":true,"content":"<article>David George of a16z joins the Going Public stage at TechCrunch Disrupt 2025, Moscone West in San Francisco, from October 27–29. Register now.</article>","contentLength":144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solving the IoT's \"Boy Who Cried Wolf\" Problem: From Individual Alerts to Cumulative Certainty","url":"https://hackernoon.com/solving-the-iots-boy-who-cried-wolf-problem-from-individual-alerts-to-cumulative-certainty?source=rss","date":1751380063,"author":"Hypothesis","guid":179063,"unread":true,"content":"<p>(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);</p><p>(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);</p><p>(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).</p><p>The problem of anomaly detection has been studied in many different application domains and many techniques have been proposed, based on statistical inference, data mining, signal processing, and recently machine learning, among others. We note that in the literature of anomaly detection, anomalies have been classified into three categories: point anomaly, contextual anomaly, and collective anomaly [5]. However, they are all concerned with the detection of individual anomalous events, which are different from the cumulative anomaly we consider in this paper. In cumulative anomaly we are more concerned with the cause of anomalous events (for example, compromised IoT device), instead of individual anomalous events. As a consequence, we need to accumulate sufficient evidence (individual anomalous events) to reach a conclusion (for example, if an IoT device is compromised) in cumulative anomaly detection.</p><p>\\\nGiven the importance of improving IoT security, many security attack detection techniques have been proposed, including various ML-based solutions [3], [9]. However, some of them required the training data of both benign and attack traffic. They cannot detect new security attacks. Others developed anomaly detection based schemes to detect anomalous traffic originated from IoT devices. However, as we have discussed in Section 1, they often trigger a large number of false alerts, rendering them unusable in detecting compromised IoT devices in the real-world deployment.</p><p>\\\nIn [10], Gelenbe and Nakip developed an online scheme CDIS to detect compromised IoT devices based on autoassociative learning. However, the design of CDIS was tailored to Mirai botnet, and may not be effective to detect other types of compromised IoT devices. In addition, CDIS still only targeted individual anomalous events, instead of cumulative anomaly detection as we perform in this paper. The authors of [11] developed a federated self-learning based scheme D¨IoT to detect compromised IoT devices, where local security gateways communicate with remote IoT Security Service to build a more comprehensive normal traffic model of IoT devices. In order to further reduce the false alerts generated by the aggregated anomaly detection model, a window-based scheme was adopted, where anomaly alarm was triggered only if the fraction of anomalous packets was greater than a pre-defined threshold value. In [8], Meidan et al. presented an autoencoder-based anomaly detection system N-BaIoT to detect compromised IoT devices. N-BaIoT also tried to reduce the number of false alerts triggered by the pure anomaly detection system using a window-based scheme with a majority vote to reach a decision.</p><h2>3. Background on Autoencoder and SPRT</h2><p>In this section we provide the necessary background on autoencoder and sequential probability ratio test (SPRT) for understanding the development of the proposed CUMAD framework. We refer interested readers to [6] and [7], respectively, for the detailed treatment on these two topics.</p><p>Autoencoder is an unsupervised neutral network that aims to reconstruct the input at the output. Figure 1 illustrates a simple standard (undercomplete) autoencoder.</p><p>\\\n\\\nAn autoencoder can be considered as consisting of two components: an encoder f and an decoder g. Given input data x, the encoder function f maps x to a latent-space representation, or code h, that is h = f(x). Using the corresponding code h as the input, the decoder function g tries to reconstruct the original input x at its output x ′, that is, x′ = g(h). Combining both the encoder function and decoder function together, we have x′ = g(f(x)). Let L(x, x′) be the reconstruction error, that is, the difference between x and x′. The autoenceder aims to minimize L(x, x ′). We note that there are different definitions of L(x, x′) and one of the most common definitions is the mean squared errors (MSE). We note that in the example autoencoder of Figure 1, both the encoder and decoder have only one hidden layer. This is only for illustration purpose. In reality they can have many hidden layers, depending on the specific application requirement.</p><p>\\\nAutoencoders have been traditionally used in applications of dimensionality reduction and feature learning, by focusing on the compressed code of an autoencoder, which holds the latent-space representation of the original data. On the other hand, autoencoders also possess a few desired properties, making them an attractive candidate for anomaly detection. For example, an autoencoder is able to extract the salient features of the original data to remove dependency in the original data. More importantly, an autoencoder can only learn the properties or distributions of the data that it has seen during the training stage, that is, the data points in the training dataset. It excels at reconstructing data that are similar to the training data, but performs poorly on data that are very different from the training data, in terms of the reconstruction error L(x, x′).</p><p>\\\nThis is an appealing property of autoencoders in the application of anomaly detection. For example, in the context of detecting compromised IoT devices, we can establish the normal behavioral model of an IoT device using an autoencoder by training it with benign network traffic before the device has been compromised. We can continue monitoring the IoT device by passing the corresponding network traffic of the device into the trained model. If the reconstruction error is no greater than a pre-specified threshold, we consider the corresponding network traffic to be benign. When the reconstruction error is greater than the threshold, we claim that the network traffic is anomalous.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2404.13690\">available on arxiv</a> under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":6162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Build a Modular Selenium + Cucumber Framework in Java","url":"https://hackernoon.com/how-to-build-a-modular-selenium-cucumber-framework-in-java?source=rss","date":1751379665,"author":"Rama Mallika Kadali","guid":179062,"unread":true,"content":"<article>Learn how to build a scalable and modular test automation framework using Selenium, Cucumber, and Java. This guide covers folder structure, CI/CD integration, Page Object Model, Allure reporting, and best practices for long-term maintainability and team collaboration.</article>","contentLength":268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How CUMAD Accumulates Evidence to Unmask Compromised IoT Devices","url":"https://hackernoon.com/how-cumad-accumulates-evidence-to-unmask-compromised-iot-devices?source=rss","date":1751379354,"author":"Hypothesis","guid":179061,"unread":true,"content":"<p>(1) Md Mainuddin, Department of Computer Science, Florida State University, Tallahassee, FL 32306 (mainuddi@cs.fsu.edu);</p><p>(2) Zhenhai Duan, Department of Computer Science Florida State University Tallahassee, FL 32306 (duan@cs.fsu.edu);</p><p>(3) Yingfei Dong, Department of Electrical Engineering, University of Hawaii Honolulu, HI 96822 USA (yingfei@hawaii.edu).</p><p>\\\n<strong>—IoT devices fundamentally lack built-in security mechanisms to protect themselves from security attacks. Existing works on improving IoT security mostly focus on detecting anomalous behaviors of IoT devices. However, these existing anomaly detection schemes may trigger an overwhelmingly large number of false alerts, rendering them unusable in detecting compromised IoT devices. In this paper we develop an effective and efficient framework, named CUMAD, to detect compromised IoT devices. Instead of directly relying on individual anomalous events, CUMAD aims to accumulate sufficient evidence in detecting compromised IoT devices, by integrating an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem. CUMAD can effectively reduce the number of false alerts in detecting compromised IoT devices, and moreover, it can detect compromised IoT devices quickly. Our evaluation studies based on the public-domain N-BaIoT dataset show that CUMAD can on average reduce the false positive rate from about 3.57% using only the autoencoder-based anomaly detection scheme to about 0.5%; in addition, CUMAD can detect compromised IoT devices quickly, with less than 5 observations on average.</strong></p><p>In recent years Internet of Things (IoT) devices have been increasingly integrated into our daily lives and our society, with notable example environments such as smart homes, healthcare, transportation, and power grid. On one hand, this rapid development helps to improve the quality and efficiency of our daily lives. On the other hand, this same development also poses potentially unprecedented security and privacy challenges on the Internet, given that most of these IoT devices are low-cost systems with limited computation, memory, and energy resources. These devices often lack proper built-in security mechanisms to protect themselves and are vulnerable to various security attacks.</p><p>\\\nMany security attacks targeting or based on IoT devices have been reported in the past [1]. In response to the growing problems of IoT security, government agencies such as US NIST have developed many recommendations that manufacturers should adopt to mitigate the security risks associated with IoT devices [2]. In addition, many research efforts have been carried out to improve IoT security, including both proactive approaches to enhancing security mechanisms of IoT devices and more reactive solutions to monitor IoT device behaviors to detect rogue or infected IoT devices [3].</p><p>\\\nAlthough some of the recommendations, for example, avoiding default common credentials, are relatively easy to be incorporated into IoT device manufacturing and certainly help mitigate IoT security risks, IoT devices are still fundamentally vulnerable to security attacks. As low-cost systems, IoT devices are inherently constrained in resources to support advanced security mechanisms. In addition, from the perspectives of both manufacturers and users, there are often conflicting objectives of IoT device usability and security, which often discourage the adoption of advanced security mechanisms in IoT devices.</p><p>\\\nGiven these constraints of deploying advanced security mechanisms on IoT devices, network-based solutions have attracted a great amount of research efforts in recent years [3]. In particular, many machine learning (ML) based methods have been developed in detecting anomalous network behaviors of IoT devices [3]. (In this paper we use the term ML to refer to both traditional machine learning algorithms such as SVM and deep learning (DL) algorithms such as RNN.) However, most existing solutions only targeted the problem of anomaly detection in IoT devices [4], instead of detecting compromised IoT devices. Although detecting individual anomalies is of critical importance in certain application domains [5], we note that these solutions may not be directly translated into the detection of compromised IoT devices. Given the large amount of network traffic, even a small false positive rate of an anomaly detection method can often translate into a large number of false alerts, rendering the detection method unusable in detecting compromised IoT devices in the real-world deployment.</p><p>\\\nIn this paper we develop an effective and efficient framework to detect compromised IoT devices, named CUMAD (cumulative anomaly detection). In essence, CUMAD integrates an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem [6], [7]. In CUMAD, the normal behavior of each IoT device is learnt and modeled by an autoencoder. During the training of an autoencoder model, it learns a latent space representation of the training data. More importantly, due to the nature of autoencoder, it excels at reconstructing inputs that are similar to the data used in training the model, but performs poorly when the new data is very different from the training data, manifested as large reconstruction errors. Although autoencoder has been mainly used in dimensionality reduction and feature learning in the past, in recent years it has also attracted a great amount of interests in anomaly detection in many different application domains.</p><p>\\\nInstead of focusing on individual anomalous events detected by autoencoder, CUMAD aims to accumulate sufficient evidence to detect if an IoT device has been compromised. In CUMAD, the output of the autoencoder-based anomaly detection subsystem is fed into an SPRT-based sequential hypothesis testing subsystem. Unlike traditional probability ratio test methods that require a pre-defined fixed number of observations to reach a decision, SPRT works in an online manner and updates as observations arrive sequentially. SPRT reaches a conclusion whenever sufficient evidence has been observed. Therefore, SPRT can make a decision quickly (and consequently, CUMAD can detect compromised IoT devices quickly).</p><p>\\\nIn this paper we develop the CUMAD framework, and we also evaluate the performance of CUMAD using a public-domain IoT dataset N-BaIoT [8], which contains both benign and (Mirai and Bashlite) attack traffic of IoT devices. Our evaluation studies show that CUMAD can greatly improve the performance in detecting IoT devices in terms of false positive rates, for example, compared to the simple autoencoder-based anomaly detection system, CUMAD on average reduces the false positive rate from about 3.57% to 0.5%, representing about 7 times performance improvement in terms of false positive rate of the systems. In addition, CUMAD can detect a compromised IoT device quickly, with less than 5 sequential observations on average. We note that although both autoencoder and SPRT have been proposed in developing anomaly detection systems before, to our knowledge, we are the first to integrate the two techniques to detect compromised IoT devices, instead of being used separately for anomaly detection. In addition, we are the first to introduce the notion of cumulative anomaly in detecting compromised IoT devices (see Section 2 for more details).</p><p>\\\nThe remainder of the paper is organized as follows. In Section 2 we discuss related work. We present the background on autoencoder and SPRT in Section 3. We describe the design of CUMAD in Section 4, and evaluate its performance in Section 5. We conclude the paper in Section 6.</p><p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2404.13690\">available on arxiv</a> under CC by 4.0 Deed (Attribution 4.0 International) license.</p>","contentLength":7870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Industries Adopting Rust","url":"https://www.youtube.com/watch?v=zRgtK2DsEXM","date":1751379309,"author":"Let's Get Rusty","guid":178971,"unread":true,"content":"<article>If you love coding in Rust but aren’t sure how to turn that passion into a full-time career, this video is for you. We’re diving into the real-world opportunities for Rust developers, the industries that are actively adopting Rust, the skills they’re looking for, and the exact steps you can take to land your first Rust job.\n\nJoin the Rust Live Accelerator: https://letsgetrusty.com/join\n\nChapter:\n0:00 Intro\n0:43 Security Industry\n2:56 Backend Infrastructure\n4:00 Embedded\n5:26 Web3\n8:06 AI</article>","contentLength":498,"flags":null,"enclosureUrl":"https://www.youtube.com/v/zRgtK2DsEXM?version=3","enclosureMime":"","commentsUrl":null},{"title":"Feasibility study of a mission to Sedna - Nuclear propulsion and solar sailing","url":"https://arxiv.org/abs/2506.17732","date":1751378891,"author":"speckx","guid":179071,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434062"},{"title":"Why Agentic AI Isn’t Pure Hype (And What Skeptics Aren’t Seeing Yet)","url":"https://www.kdnuggets.com/why-agentic-ai-isnt-pure-hype-and-what-skeptics-arent-seeing-yet","date":1751378450,"author":"Bala Priya C","guid":178930,"unread":true,"content":"<article>A developer's take on why agentic AI systems are actually useful and not just another buzzword.</article>","contentLength":95,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/bala-agentic-ai-hype.jpeg","enclosureMime":"","commentsUrl":null},{"title":"Grammarly acquires Superhuman","url":"https://www.reuters.com/business/grammarly-acquires-email-startup-superhuman-ai-platform-push-2025-07-01/","date":1751378430,"author":"thm","guid":179011,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433994"},{"title":"Future Wireless Comms Could Process Data in Midair","url":"https://spectrum.ieee.org/wireless-communication-over-air-processing","date":1751378403,"author":"Michelle Hampson","guid":178959,"unread":true,"content":"<p>AirComp approaches do more than transmit data from point A to point B</p>","contentLength":69,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNTkwNS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2MzYyMzg1M30.XXiRdtScBG-Ox39QOpbhpoAjmKTB9e0NHXiQJBHOaQc/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Automakers Clash With India Over 'Aggressive' Emission Limits","url":"https://hardware.slashdot.org/story/25/07/01/081225/automakers-clash-with-india-over-aggressive-emission-limits?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751378400,"author":"msmash","guid":178964,"unread":true,"content":"India's automakers are opposing the government's proposal to cut car emissions by 33% from 2027, calling the target \"too aggressive\" in a formal submission to the power ministry. \n\nThe Society of Indian Automobile Manufacturers warned the plan risks billions of rupees in penalties and threatens future investments in the $137-billion auto sector. The proposal represents more than twice the pace of India's previous emission reduction target and forms part of the third phase of Corporate Average Fuel Efficiency norms first introduced in 2017. The industry body wants a more gradual 15% reduction target and opposes different standards for small versus heavy vehicles.","contentLength":670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real Python: Implementing the Factory Method Pattern in Python","url":"https://realpython.com/courses/factory-method-pattern/","date":1751378400,"author":"","guid":178958,"unread":true,"content":"<p>The book describes design patterns as a core design solution to reoccurring problems in software and classifies each design pattern into <a href=\"https://en.wikipedia.org/wiki/Software_design_pattern#Classification_and_list\">categories</a> according to the nature of the problem. Each pattern is given a name, a problem description, a design solution, and an explanation of the consequences of using it.</p><p>The GoF book describes Factory Method as a creational design pattern. Creational design patterns are related to the creation of objects, and Factory Method is a design pattern that creates objects with a common <a href=\"https://realpython.com/python-interface/\">interface</a>.</p><p>This is a recurrent problem that <strong>makes Factory Method one of the most widely used design patterns</strong>, and it’s very important to understand how it works and know how to apply it.</p><p><strong>By the end of this video course, you’ll</strong>:</p><ul><li>Understand the  of </li><li>Recognize  to use  in your applications</li><li>Know how to  and  by using the pattern</li><li>Be able to  where  is the appropriate design pattern</li><li>Know how to choose an <strong>appropriate implementation</strong> of </li><li>Understand how to <strong>implement a reusable, general purpose solution</strong> of </li></ul>","contentLength":1019,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mo Jomaa breaks down IPO prep for founders on the Scale Stage at TechCrunch All Stage","url":"https://techcrunch.com/2025/07/01/mo-jomaa-breaks-down-ipo-prep-for-founders-on-the-scale-stage-at-techcrunch-all-stage/","date":1751378400,"author":"TechCrunch Events","guid":178926,"unread":true,"content":"<article>Mo Jomaa of CapitalG, will lead a session, “What to Think About Now If You Want to IPO Someday” at TechCrunch All Stage on July 15 in Boston. Register now.</article>","contentLength":159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Performance & Power Of The Low-Cost EPYC 4005 \"Grado\" vs. Original EPYC 7601 Zen 1 Flagship CPU","url":"https://www.phoronix.com/review/amd-epyc-4005-server","date":1751378400,"author":"Michael Larabel","guid":178970,"unread":true,"content":"<article>For those on very long server upgrade cycles, typically just running the hardware until failure or consider buying second-hand servers that are generations old for lower up-front cost, today's unique article is for you with quantifying a first-generation EPYC server compared to today's entry-level EPYC processors in performance and power efficiency. With the fascinating AMD EPYC 4005 \"Grado\" budget-friendly server processors I was curious how well they would stack up against AMD's original flagship EPYC processor, the AMD EPYC 7601 \"Naples\" processor from the Zen 1 era. Can an entry-level brand new Grado server processor with dual channel DDR5 memory outpace an original EPYC server with twice the core/thread counts and eight channel DDR4 server memory? Yes, with huge gains in performance and power efficiency.</article>","contentLength":820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building the Unbreakable Contract: A Pipeline for AI-Powered Vulnerability Classification and Repair","url":"https://hackernoon.com/building-the-unbreakable-contract-a-pipeline-for-ai-powered-vulnerability-classification-and-repair?source=rss","date":1751378339,"author":"Blockchainize Any Technology","guid":179060,"unread":true,"content":"<p>\\\nTo achieve high-quality results in training our framework utilizing a RandomForestClassifier and LLMs for classification and repair (Fig. 1), several essential features must be incorporated.</p><p>\\\nA source code column (“contract source”) is necessary to run Slither and the LLMs. However, since the datasets consistently excluded source code, a web scraping algorithm that employed the “contract address” column would be necessary to obtain source code from Etherscan and generation (see subsection D.). In order to account for source code that could not be scraped through Etherscan, the dataset (200,000 contracts) was reduced to 2500 rows.</p><p>\\\nSlither was then run on the newly acquired source code (see subsection B.), adding columns “vulnerability”, “confidence”, and “impact”. Slither occasionally failed to provide any vulnerabilities, totalling 474 failed contracts (80% successful output rate). To account for this, the dataset was reduced again to 2,000 smart contracts. Of the dataset, 400 were labeled malicious, and 1,600 were labeled non-malicious. Table I visualizes a segment of the finalized dataset.</p><p>\\\nSlither is a static code analyzer, which checks the smart contracts for vulnerabilities without executing the contract. Slither’s initial input comes from the Solidity Abstract Syntax Tree (AST) generated by the Solidity compiler from the contract source code. The smart contract is then simplified into an intermediate representation called SlithIR. This intermediate representation is compared to current industry standards, and Slither outputs vulnerabilities. Slither leads the industry in smart contract vulnerability detection, outperforming other static code analyzers in almost every metric, as shown in Table II. This, coupled with our Random Forest Classifier, ensures high accuracy in detecting vulnerable smart contracts.</p><p>\\\nAfter importing and running all 89 basic detectors provided by the API, we added each contract’s vulnerabilities to the dataset as a list of Slither’s natural language names with empty lists denoting contracts Slither deemed safe.</p><p>\\\n<em>C. Data Issues and Generation</em></p><p>\\\nWhen it came to data collection, specific issues were encountered. Our biggest issue, extracting source code, proved to be a challenging task. For instance, in a dataset that bytecode was given, we were unsuccessful in decompiling that code into analyzable source code as we were unaware of the decompiler’s limits. We also struggled to find additional malicious source code to train a model on, as our dataset only included 150 malicious contracts. To overcome this, we implemented OpenAI’s GPT 3.5 Turbo to generate malicious source code. Initial attempts were barred by GPT 3.5’s ethical limitations (Fig. 2). However, after jailbreaking GPT 3.5 with prompt engineering [18], GPT 3.5 would produce malicious source code that could be repaired by the model.</p><p>\\\nThe variability of the dataset made it difficult to generate Slither vulnerabilities for smart contracts, so a BLANK-step approach was used. The primary issue was the 100+ versions all contracts were written in combined with the limited backward compatibility of Solidity — i.e., version 0.4.11 could run on a compiler of version 0.4.26 but not a compiler of version 0.5.0+. Addressing this required modifying each contract to read ”pragma solidity ≥{version}”, creating five different scripts, and running each script on the entire dataset with one of five following Solidity versions: 0.4.26, 0.5.17,</p><p>\\\n0.6.12, 0.7.6, or 0.8.21, with Slither vulnerabilities of scripts that could not be compiled recorded as null, and those that could be recorded with the English name of the vulnerability, obtained from parsing the returned json. Combining these lists resulted in the final list of Slither vulnerabilities for the 75% of smart contracts for which this method yielded results.</p><p>\\\nEach detector class includes the detector’s confidence and impact levels. After creating a key-value pair of each detector’s English name and their confidence plus impact, this list was used to create confidence and impact lists for all vulnerabilities for each smart contract.</p><p>\\\nVarious models were implemented to classify smart contract maliciousness. Ultimately, RandomForestClassifier (RFC) provided the highest accuracy after pre-processing the finalized dataset.</p><p>\\\nRFC is unable to train on the dataset as provided by webscraping, generation, and Slither processing due to the abundance of unnecessary string-based features. So, unnecessary features are dropped, and necessary features are processed for RFC. For example, “confidence” and “vulnerability” retain a weaker correlation to “malicious” in comparison to “impact”, so to avoid convoluting the model, both are dropped. Thus, “contract source” and “impact” remain as the classifying features and “malicious” as the target label.</p><p>\\\nAs all columns are still either string or boolean data types, RFC is still unable to train on the dataset. “contract source” was tokenized using the CountVectorizer (CV) tool from the sci-kit-learn library. “malicious” and “impact” were encoded into usable numeric values by mapping dictionaries. Since “impact” contained more than two possible outputs, unlike “malicious”, the outputs of “impact” were scaled from 0-4.</p><p>\\\nAfter the tokenized and encoded columns are concatenated, RFC’s numeric prerequisite is fulfilled.</p><p>\\\nThe data is then split into a train-test split of 0.6-0.4 and randomized before RFC fits to the train set and predicts on the test set. Accuracy and confusion are evaluated in .</p><p>\\\n<em>E. Large Language Models (LLMs)</em></p><p>\\\n: We incorporated multiple Large Language Models to repair the smart contracts after they had been identified as malicious with our two-layered frameworks. The best results came from the Llama-2-7B model, which can be found on Hugging Face. This model finished training in July 2023. Our finetuning process took place about three weeks later. The Llama-2-7B model has become very popular due to its low number of parameters and reliability, leading to a less memory-intensive alternative to other LLMs in the industry.</p><p>\\\nThe finetuning process took place on Google Colab using the T4 chip, which carries 16 GB of VRAM. However, Llama2-7B’s weights themselves fill this limit (7b * 2 bytes = 14). This also does not include any weights, optimizers, or gradients. Thus to run Llama-2-7B and be able to run it without memory restrictions on a platform like Google Colab, we will use parameter-efficient-finetuning (PEFT). Specifically, we will use QLoRa (Efficient Finetuning of Quantized LLMs), using 4-bit precision instead of the normal 16-bit precision. This quantization process allows for finetuning on Colab while also ensuring that the precision of the model is adequate. This is because when saving the 4-bit model, we also save the QLoRa adapters, which can be used with the model.</p><p>\\\nMoreover, Llama-2-7B is open source meaning the model is available to be downloaded and used locally. Traditional data privacy concerns with LLMs are therefore nullified because all data is processed on the local machine, not in a 3rd party server. This bodes well for smart contracts as many execute agreements with sensitive information and large sums of money. Llama-2-7B provides the benefits and accuracy of an advanced LLM while also providing the security and versatility neccesary for blockchain technology.</p><p>\\\nThe Llama-2-7B model was fine-tuned on fifty smart contracts that were once malicious and then repaired, using a supervised learning approach. These smart contracts were collected in the data collection mentioned above. Specifically, the source code was tokenized and embedded, using the quantization outlined previously. The model was trained over 100 steps, with training loss consistently decreasing with every step(as shown in figure 3).</p><p>\\\nThe supervised fine-tuning process allowed the model to understand the relationships between malicious source code and the same source code that had been repaired to emulate that with any other contract.</p><p>\\\n: We also utilized OpenAI’s API to use GPT-3.5-Turbo to repair vulnerabilities. OpenAI is one of the most well known names in the industry with applications such as DALL -E and ChatGPT. Specifically, while all GPT models are optimized to generate code, GPT-3.5-Turbo is the best combination of performance and efficiency. Moreover, by utilizing a ”chat bot”, we were able to use prompt engineering to create a prompt with the best possible performance. Directly querying GPT-3.5-Turbo to repair malicious code was unsuccessful. Similar to the generation of malicious smart contracts, GPT-3.5-Turbo had a reluctance to work with malicious source code (Fig. 4).</p><p>\\\nThus prompt engineering was utilized to circumvent this problem.</p><p>\\\nFirst, the use of the word ”malicious” needed to be removed. While we were looking for our LLM to repair malicious smart contracts, GPT-3.5 Turbo was instead asked to help us “fix vulnerable smart contracts”.</p><p>\\\nWe then used Chain of Thought Techniques in order for the model to elaborate on what changes it made and why. This led to a more accurate source code output and more vulnerabilities repaired. Additionally, this provided more information for the</p><p>\\\nuser as the specific vulnerabilities in the malicious smart contract were highlighted and explained.</p><p>\\\nUltimately, our prompt(Fig. 5) used Slither’s source code and vulnerabilities to prompt GPT 3.5 Turbo to repair the smart contracts. While Slither also outputs impact level and confidence on those vulnerabilities, we found incorporating these into the prompt hurt the model’s ability to output repaired source code or even source code that could be compiled. Essentially, using other Slither outputs led to overfitting. This prompt was also used with the Llama-2-7B model outlined above in order to create uniformity across outputs. In both models, the prompt allowed for the generation of repaired source code while also generating details that explained any changes and provided explanation.</p><p>\\\nIn conclusion, we ended with two primary models to repair source code. First, the Llama-2-7B, which had been finetuned specifically for repairing smart contracts. Second was the utilization of GPT-3.5-Turbo which learned to repair smart contracts through CoT prompt engineering.</p><p>(1) Abhinav Jain, Westborough High School, Westborough, MA and contributed equally to this work (jain3abhinav@gmail.com);</p><p>(2) Ehan Masud, Sunset High School, Portland, OR and contributed equally to this work (ehanmasud2006@gmail.com);</p><p>(3) Michelle Han, Granite Bay High School, Granite Bay, CA (michellehan2007agt@gmail.com);</p><p>(4) Rohan Dhillon, Lakeside School, Seattle, WA (rohand25@lakesideschool.org);</p><p>(5) Sumukh Rao, Bellarmine College Preparatory, San Jose, CA (sumukhsf@gmail.com);</p><p>(6) Arya Joshi, Robbinsville High School, Robbinsville, NJ (arya.joshi@gmail.com);</p><p>(7) Salar Cheema, University of Illinois, Champaign, IL (salarwc2@illinois.edu);</p><p>(8) Saurav Kumar, University of Illinois, Champaign, IL (sauravk4@illinois.edu).</p><p>:::info\nThis paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.</p>","contentLength":11224,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Will VR Integration for Remote Work Become Permanent?","url":"https://hackernoon.com/will-vr-integration-for-remote-work-become-permanent?source=rss","date":1751377909,"author":"Allan Grain","guid":179059,"unread":true,"content":"<p>If there’s been any seismic shift in the workplace over the last few decades, it’s manifested in the way remote work has become a cornerstone of modern business operations.</p><p>Many organizations today have teams distributed over several geographical areas and bridging the gap is a challenge. Phone calls, emails and even Zoom calls simply are not sufficient to replicate the in-office experience.</p><p>The question now is whether technology will advance to the point that virtual reality (VR) solutions can revolutionize remote collaboration and allow companies to operate multiple locations in a VR environment.</p><p>The best part of VR is that it provides a three-dimensional immersive space where employees can interact as if they were physically present. The addition of AI would probably assist in making these virtual environments even more realistic. This type of innovation would certainly address the limitations of traditional video conferencing as seen with Zoom or Microsoft Teams for instance. These technologies might be good for certain uses, but they lack the depth and engagement of face-to-face interaction.</p><p>One of the fun aspects of VR is the ability to create virtual offices – digital workspaces in which employees, represented by lifelike avatars, can collaborate in real time. AI algorithms power these avatars, enabling them to mimic natural human behaviors, such as gestures, facial expressions, and even tone of voice.</p><p>A terrific emerging technology that has existed for a while now but is reaching perfection today is real-time language translation. Language barriers have long been a challenge for global teams. AI-driven translation tools, such as those powered by natural language processing (NLP), are now being integrated into VR platforms. These tools are now able to translate speech in real time, displaying subtitles or dubbing voices in a user's native language. For instance, a team member in Japan can speak in Japanese, while a colleague in Brazil hears the conversation in Portuguese, all within a shared VR environment. This capability allows employees from various geographical regions to collaborate on projects even if they do not speak the same language. This is groundbreaking.</p><p>The virtual environment can also be altered to accommodate one-on-one meetings or large boardroom meetings. Users can even alter the room to accommodate client presentations.</p><p>AI is a VR environment is useful for scheduling, note taking, or action-item tracking. Transcription tools are easily integrated into a VR environment so that conversations and discussion can be transcribed, with key points summarized and follow-up tasks assigned in real-time. AI and VR working seamlessly together can function as a high-speed virtual assistant capable of performing multiple tasks at once with high accuracy and minimal input.</p><p>Several companies are already pioneering AI-VR integration for remote work. <a href=\"https://forwork.meta.com/nl/en/horizon-workrooms/\">Meta's Horizon Workrooms</a>, for example, allows teams to collaborate in virtual meeting rooms with spatial audio and interactive whiteboards.</p><p>Similarly, <a href=\"https://www.spatial.io/\">Spatial</a>, a VR collaboration platform, uses AI to create cross-platform compatibility, allowing users on different devices—VR headsets, laptops, or smartphones—to work together seamlessly.</p><p>Firms like <a href=\"https://www.accenture.com/nl-en\">Accenture</a> have adopted AI-VR solutions to host global meetings and training sessions. <a href=\"https://www.accenture.com/se-en/insights/technology/going-beyond-extended-reality\">Accenture's \"Nth Floor\"</a> virtual campus enables employees to attend workshops, network with colleagues, and even explore virtual replicas of physical offices.</p><p>Beyond corporate settings, AI combined with VR is making waves in education and healthcare. Universities now use virtual classrooms to connect students across continents. In healthcare, VR simulations allow medical professionals to practice surgeries or collaborate on patient care plans remotely, improving access to expertise.</p><p>Unfortunately, despite its potential, AI-VR integration for remote work still faces several major hurdles before it can be widely adopted. Cost and accessibility remain significant barriers, as high-quality VR headsets and AI-powered software are highly expensive. VR devices like the Oculus Quest 3 or Apple Vision Pro might become more affordable, but their price prevents widespread adoption.</p><p>While challenges remain, ongoing innovations will pave the way for a future where virtual offices imitate their physical counterparts sufficiently enough that companies will integrate them permanently as part of the workday.</p><p>Tomorrow’s workplace promises to be more immersive and perhaps even more fun. </p>","contentLength":4530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Novel Pipeline for Classifying and Repairing Smart Contracts at Scale","url":"https://hackernoon.com/a-novel-pipeline-for-classifying-and-repairing-smart-contracts-at-scale?source=rss","date":1751377650,"author":"Blockchainize Any Technology","guid":179058,"unread":true,"content":"<p>(1) Abhinav Jain, Westborough High School, Westborough, MA and contributed equally to this work (jain3abhinav@gmail.com);</p><p>(2) Ehan Masud, Sunset High School, Portland, OR and contributed equally to this work (ehanmasud2006@gmail.com);</p><p>(3) Michelle Han, Granite Bay High School, Granite Bay, CA (michellehan2007agt@gmail.com);</p><p>(4) Rohan Dhillon, Lakeside School, Seattle, WA (rohand25@lakesideschool.org);</p><p>(5) Sumukh Rao, Bellarmine College Preparatory, San Jose, CA (sumukhsf@gmail.com);</p><p>(6) Arya Joshi, Robbinsville High School, Robbinsville, NJ (arya.joshi@gmail.com);</p><p>(7) Salar Cheema, University of Illinois, Champaign, IL (salarwc2@illinois.edu);</p><p>(8) Saurav Kumar, University of Illinois, Champaign, IL (sauravk4@illinois.edu).</p><p>\\\n<strong>—Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither’s vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and finetuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repaired contracts shows that all retain functionality, indicating that the proposed method is appropriate for automatic batch classification and repair of vulnerabilities in smart contracts.</strong></p><p>As we delve into the crucial role smart contracts play in the global blockchain, it becomes increasingly imperative that we understand the severity of cyberattacks that exploit weak code. 2018 saw $23.5 million worth of cryptocurrencies stolen from the Bancor network due to the compromise of a wallet used to upgrade smart contracts, sparking controversy online over the safety of decentralized exchange and smart contract systems [16]. More recently, in 2020, a hacker drained Harvest Finance of $24 million by implementing a smart contract that manipulated the share values of the vaults [17]. The common theme across these hacks is that vulnerabilities within smart contracts were exploited to steal millions of dollars, highlighting the importance of strengthening smart contracts to prevent vulnerabilities from arising.</p><p>\\\nSmart contracts provide a secure platform for transactions without the need for a trusted intermediary. For this reason, they have become increasingly common in blockchain applications. But because most blockchain applications prevent users from editing smart contracts after they have been deployed, there is a need for analysis tools that can accurately and precisely determine the vulnerabilities of smart contracts. Although most tools rely on expert-developed frameworks, recent research has begun developing deep learning models that can evaluate a smart contract’s vulnerability. However, most existing deep learning models fail to provide helpful feedback on a smart contract’s vulnerabilities — instead, they determine whether or not a smart contract is vulnerable.</p><p>\\\nDLVA [1] introduces a three-step approach involving mapping bytecode to high-dimensional vectors, classifying vectors based on training data, and using neural networks to infer vulnerable contracts. However, a significant weakness in this approach was the high false positive rate during the prediction process. Similarly, MRN-GCN [5] utilizes deep learning with a nest contract graph capturing syntactic and semantic information, enabling the classification of vulnerable functions, but like [1], retained mixed recall percentages ranging from 98.18% to 79.59%. The authors of [3] take a different approach by proposing peer-to-peer voting and reward-and-slash mechanisms to mitigate and discourage malicious behavior in smart contracts.</p><p>\\\nLarge Language Models (LLMs) models prove to be exceptional in performing complex tasks. The authors of [8] demonstrated the capabilities of various LLMs in identifying vulnerabilities in DeFi smart contracts with F1-scores significantly higher than random baselines, which has the potential arXiv:2309.07841v1 [cs.CR] 14 Sep 2023 to be improved by the tool enhancement framework developed in [4]. Prompt engineering allows LLMs to be substantially enhanced. One powerful LLM prompt engineering method involves Chain of Thought (CoT) prompting [2] that significantly improves the ability of LLMs to perform complex reasoning. In eight CoT exemplars, [2] achieves an accuracy of 56.9 on PaLM-540B in the GSM8K benchmark, demonstrating an accuracy improvement of 39. However, the paper chooses to rely solely on CoT, neglecting fine-tuning entirely. In a similar implementation, the authors of [7] present a framework that improves upon CoT by transferring advanced reasoning abilities from large models to smaller ones through knowledge distillation, resulting in improved question-answering performance. In another scenario, [6] utilized prompt engineering by giving ChatGPT specific information, such as the translation’s purpose and target audience, leading to industry standard translation quality.</p><p>\\\nA comprehensive survey [11] described the current landscape of smart contract security, identifying eight core defense methods across 133 models. This finding underscores the complexity of the field but also reveals limitations. One limitation is seen in applying automated smart contract tools to DeFi systems [12]. Surprisingly, these tools only detected 8% of attacks, indicating a challenge with intricate vulnerabilities. Addressing this, [13] evaluated five smart contract detection tools, focusing on three types of vulnerabilities. [13]’s analysis determined that different detection models have varying strengths and weaknesses, suggesting a combination of methods may be more effective. Furthermore, this notion is corroborated by [9] and [10], which both utilize Multi-Task Learning, a combination method that leverages concurrent learning and optimization of multiple tasks. Notably, [14] advances this methodology by using an approach that blends K-means clustering and LSTM networks with a universal sentence encoder. This approach understood the smart contract code’s semantic meaning, outperforming baseline models.</p><p>\\\nMoreover, current work regarding repairing smart contracts has been shown to be reliable. For example, [19] utilizes a framework called ContractFix to repair vulnerabilites with 94% accuracy. ContractFix was based around static code analyzers and focused on repairing broken patches. Similarly, [15] utilizes a tool, Elysium, to repair patches in bytecode for seven vulnerabilities. However, this paper improves on these frameworks in two main ways. First, our framework is built on LLMs which allow for a more robust repairing process, that is adaptable to zero-day vulnerabilities. Secondly, we work directly with source code, which is a novel approach to repair vulnerabilities.</p><p>\\\nThese existing methods have been shown to work well in vulnerability detection across various situations with relatively little statistical error. However, we show that existing vulnerability detection methods face the following problems: 1) lack of a broad approach, 2) little detail on specific errors, 3) high false positive evaluations, and 4) lack of a direct repair framework. To address all these problems, we propose a novel pipeline. The pipeline first utilizes Slither and a RandomForestClassifier to detect and provide specific vulnerabilities within smart contract source code. After filtering out non-malicious contracts, two LLMs, GPT-3.5-Turbo and a fine-tuned Llama-2-7b generation model, each repair the vulnerable smart contract source code. The repaired contract is then evaluated by Slither against its vulnerable counterpart, assessing the effectiveness of the repair.</p><p>\\\nThe rest of this paper is outlined as follows: Section II details our novel pipeline approach that utilizes two layers for vulnerability detection: Slither and RandomForestClassifier, to classify vulnerable smart contracts and two LLM models (Llama-2-7B and GPT-3.5-Turbo) to repair them. Section III exhibits the results of our approach in comparison to existing methods. Section IV provides a conclusion.</p><p>:::info\nThis paper is  under ATTRIBUTION-NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL license.</p>","contentLength":8780,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Converting a large mathematical software package written in C++ to C++20 modules","url":"https://arxiv.org/abs/2506.21654","date":1751377616,"author":"vblanco","guid":179305,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433899"},{"title":"Why You Can't Miss SpoonOS's Developer Call S1 for AI Innovation","url":"https://hackernoon.com/why-you-cant-miss-spoonoss-developer-call-s1-for-ai-innovation?source=rss","date":1751377312,"author":"Ishan Pandey","guid":179057,"unread":true,"content":"<p>What if artificial intelligence could operate with the transparency and decentralized control of blockchain technology? <a href=\"https://spoonai.io/\">SpoonOS</a>, described as an infrastructure developer for the sentient economy, is initiating its first coordinated community effort, Developer Call S1, to address this question. The goal is to encourage developers to explore the core functionalities of SpoonOS, an agentic operating system designed for Web3. This initiative aims to validate the <a href=\"https://spoonai.io/\">SpoonOS</a> infrastructure and identify individuals who will contribute to the operating system's evolution. It represents a structured approach with defined objectives, a timeline, and incentives for participation, along with comprehensive documentation.</p><p>\\\nFor those unfamiliar with the terminology, Web3 refers to the next iteration of the internet, emphasizing decentralization, blockchain technologies, and token-based economics. Unlike the current internet, where large corporations often control data and platforms, Web3 aims to give users more control over their data and digital interactions. Within this context, AI agents are autonomous entities that perceive their environment, act to achieve specific goals, and can improve their performance through learning. SpoonOS positions itself at the intersection of these two transformative fields, providing a framework for developers to create these intelligent, on-chain agents.</p><p>Developer Call S1 is specifically designed for Web3 developers engaged in building with AI agents. The program provides participants with resources to understand the SpoonOS technology stack. This includes examining the SpoonOS codebase on GitHub, which offers clear instructions for environment setup, building a first AI agent, mastering command line interface (CLI) tools, and integrating with built-in Model Context Protocol (MCP) servers. The MCP serves as an interface connecting SpoonOS to various data sources, both decentralized and traditional.</p><p>\\\nBeyond the GitHub resources, developers gain access to a \"Spoon Cookbook,\" which contains end-to-end tutorials. These tutorials cover topics such as API key configuration, setting up agent memory, and constructing reproducible AI agents. Additionally, the \"Spoon Toolkit\" details third-party integrations, including on-chain data feeds and decentralized storage. On-chain data feeds provide real-time information directly from a blockchain, enabling AI agents to react to events as they happen on the network. Decentralized storage, in contrast to centralized cloud storage, distributes data across a network of computers, enhancing security and resilience by removing a single point of failure. SpoonOS operates on NEO's Layer-1 blockchain, a foundational blockchain that provides the underlying infrastructure for these scalable Web3 applications. This integration is presented as a means for developers to build, deploy, and manage AI agents using intuitive tools and interfaces.</p><h3>Cultivating an Agent Ecosystem: Beyond DevCall S1</h3><p>\\\nDeveloper Call S1 is a foundational element of SpoonOS's broader strategy to expand its AI agent ecosystem. Upcoming initiatives include a global hackathon and collaborative learning programs. The intent behind these efforts is to empower developers to create versatile AI agents capable of leveraging the full capabilities of the SpoonOS platform. The vision is to lay the groundwork for the next generation of AI agents, which are expected to play a significant role in the emerging \"sentient economy.\"</p><p>\\\nThe \"sentient economy\" refers to a conceptual ecosystem where intelligent agents can autonomously interact, learn, and contribute to value creation within Web3 networks. This implies a future where AI agents are not merely tools, but active participants in economic processes, making decisions and executing transactions independently based on real-time data and learned behaviors. SpoonOS aims to provide the robust and integrated environment necessary for such an economy to flourish.</p><p>\\\nThe launch of SpoonOS's Developer Call S1 marks a significant step towards a more integrated future for AI and blockchain. The emphasis on providing comprehensive tools and structured learning pathways suggests a genuine commitment to developer adoption, which is crucial for any new platform's success. The concept of a \"sentient economy\" fueled by autonomous AI agents operating on a decentralized infrastructure is compelling. It suggests a shift from passive data consumption to active, intelligent participation within digital economies.</p><p>\\\nHowever, the realization of such a vision depends heavily on the practical utility and scalability of these AI agents. The complexity of integrating AI models with the immutable and often resource-intensive nature of blockchain transactions presents challenges. The success of SpoonOS will hinge on how effectively its toolkit simplifies this complexity for developers, allowing them to build agents that are not only intelligent but also efficient and secure. The ability for these agents to interact seamlessly across various data sources, both on-chain and off-chain, will be a key differentiator. It remains to be seen how broadly the developer community will embrace this new paradigm, but the initial push with incentivized calls and comprehensive resources provides a strong starting point. The development of a truly \"sentient economy\" will require continued innovation and collaboration from a diverse set of builders, and SpoonOS's approach seems designed to foster that environment.</p><p>\\\nDon’t forget to like and share the story!</p><p>:::tip\n<em>This author is an independent contributor publishing via our&nbsp;. HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO</em></p>","contentLength":5725,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Your Next Smart-Home Device Hunts Mosquitoes: Meet Bzigo Iris","url":"https://hackernoon.com/why-your-next-smart-home-device-hunts-mosquitoes-meet-bzigo-iris?source=rss","date":1751376520,"author":"Jon Stojan Journalist","guid":179056,"unread":true,"content":"<article>Bzigo Iris is the world’s first AI-powered mosquito detector that uses infrared and laser tech to locate mosquitoes indoors without harmful chemicals. Founded in 2019, Bzigo offers a smart, health-conscious alternative to sprays and zappers—sending real-time alerts so you can finally sleep bite-free. Visit bzigo.com to learn more.\n\n</article>","contentLength":338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Little Pepe (LILPEPE) Breezes Past 3rd Presale Stage in Record Time","url":"https://hackernoon.com/little-pepe-lilpepe-breezes-past-3rd-presale-stage-in-record-time?source=rss","date":1751376253,"author":"Kashvi Pandey","guid":179055,"unread":true,"content":"<article>Little Pepe (LILPEPE) just completed its third presale stage faster than expected, raising $2.57M in Stage 4. Backed by its own Layer 2 blockchain and a $777K giveaway, the token offers real utility beyond meme hype. With rising prices and strong demand, LILPEPE is quickly becoming 2025’s breakout meme coin.\n\n</article>","contentLength":313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stablecoins, Smart Contracts and The Rise of More Intelligent Cash","url":"https://hackernoon.com/stablecoins-smart-contracts-and-the-rise-of-more-intelligent-cash?source=rss","date":1751376006,"author":"Paul Quickenden","guid":179054,"unread":true,"content":"<blockquote><p><strong>Fintechs already have the talent, the ingenuity and after a decade of challenger success with innovations like Wise’s borderless accounts, Stripe’s one-click checkout and Revolut’s multi-currency wallets - the credibility to reshape finance on a global stage. Those breakthroughs gave consumers faster payments and slicker front-ends; but let’s call that Act I.</strong></p></blockquote><p>\\\nAct II is unfolding now: money itself is becoming programmable, composable and borderless. Stablecoins can settle in seconds, smart contracts can execute “if-this-then-that” logic without humans in the loop and tokenised assets can move 24/7 across jurisdictions. The rails we lay today - whether they’re open, interoperable and secure or fragmented and proprietary - will determine not just how fast value moves tomorrow, but how fairly it’s distributed and who gets to participate in the next wave of financial innovation.</p><p>\\\nThe challenge to builders today is this: if your vision begins and ends with speeding up domestic payments or making bill-splitting cuter, you’re missing the bigger prize. Programmable money rewrites the playbook for remittances, trade, treasury and even machine-to-machine transactions. Are we building for that horizon - or solving tomorrow’s problems with yesterday’s stack?</p><h2><strong>What’s the biggie with blockchain?</strong></h2><p>Behind the scenes, blockchain infrastructure is becoming the default layer for secure, real-time, permissionless value transfer. J.P. Morgan’s Onyx platform, for instance, has settled more than US$1 trillion in tokenised intrabank payments on permissioned chains.</p><p>\\\nIf you’re designing for the next five years, on-chain rails offer instant settlement, embedded logic and global composability that legacy systems simply can’t match. No, it won’t replace every system but it is rapidly becoming the preferred foundation for anything that needs to be fast, transparent and interoperable. This isn’t about riding the crypto wave but about recognising a smarter way to move value - one that’s auditable, modular and increasingly composable.</p><p>\\\nIf you’re designing for the next five years, building  blockchain gives you access to speed, programmability and network effects that simply don’t exist in traditional architecture. We need to shift our thinking from ‘What is blockchain for?’ to ‘What’s already working, and how does it fit into my stack?’</p><h2><strong>Could stablecoins be the new fuel behind an AI-driven economy?</strong></h2><p>At Stripe’s 2025 Sessions conference, co-founder Patrick Collison didn’t mince his words: “There are not one, but two, gale-force tailwinds… reshaping the economic landscape around us: AI and stablecoins.” In a world increasingly run by AI’s, programmable stablecoins are the natural currency and could become the financial fuel behind an AI-driven economy.</p><p>\\\nStripe has just rolled out stablecoin-powered financial accounts to businesses in 101 countries which allow companies to hold, receive, and send stablecoins and, crucially, to hedge against inflation and plug directly into the global economy. All of this without needing a traditional bank account. And it's not just Stripe, Worldpay, Mastercard, X and even Apple are exploring how to turbocharge their business using Stablecoins.</p><p>\\\nThis isn’t theory; this is infrastructure, and entrepreneurs in high-inflation countries can now operate in stable, digital USD. What’s more, startups can pay contractors in stablecoins, skip wire transfer fees and move capital in minutes via lower-cost cross-border transactions.</p><p>\\\nStablecoins have quietly moved from the fringes to becoming the go-to method for paying global contractors (instantly, with zero FX fees), cross-border commerce (settling USD payments in mere seconds), accessing savings (even in inflation-prone regions) and plugging into on-chain treasury and lending protocols (without needing a bank account).</p><p>\\\nSo if you’re building a payments platform, now is the time to ask yourself: can my product evolve with how digital finance is  being used? If the answer is no - it might be worth reconsidering your architecture because what works here must scale there.</p><h2><strong>That sounds like “money, but faster.” What else is cool?</strong></h2><p>Think of smart contracts as ‘if this, then that’ logic for money - basically code that executes financial actions automatically when certain conditions are met. There’s no middleman, no lag and no manual work. This is where money stops just moving and starts </p><p>\\\nImagine if paying contractors happened the moment work is verified; if refunds were triggered instantly when goods didn’t arrive; if cross-border trade ran on rules baked into your code, not your paperwork.</p><p>\\\nThis is already happening globally - programmable finance means stablecoins that plug into automated workflows and APIs that react to real-world events. The question now moves on from <strong><em>can we move money faster?</em></strong> .. to <strong><em>can our money move smarter?</em></strong></p><p>What separates good from game-changing companies is vision. The winners in fintech will be the ones who design for what’s , not just what’s  right now.</p><p>If you believe that money will be programmable… \\n If you see that global commerce demands truly global rails… \\n If you know that the rules of infrastructure are being rewritten in real-time right now…</p><p>… then now’s the moment to double down. Build with stablecoins. Plug into on-chain rails. Create APIs that are blockchain-ready. It’s time to think like the internet: borderless, open and lightning fast.</p><blockquote><p>Let’s not just build for today’s money; let’s help define tomorrow’s.</p></blockquote>","contentLength":5573,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Expert Generalists need specialists (and LLMs)","url":"https://martinfowler.com/articles/expert-generalist.html#ExpertGeneralistsStillNeedSpecialists","date":1751375820,"author":"Martin Fowler","guid":178927,"unread":true,"content":"<p>While we've spent this article praising the Expert Generalist, Unmesh,\n      Gitanjali, and I simultaneously do not deny the value of specialist\n      knowledge. To be the most efficient, a team needs some specialist skill.\n      We've also observed that Expert Generalist capabilities are considerably\n      more valuable when working with LLMs.</p>","contentLength":346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US Senate removes controversial ‘AI moratorium’ from budget bill","url":"https://techcrunch.com/2025/07/01/us-senate-removes-controversial-ai-moratorium-from-budget-bill/","date":1751375268,"author":"Rebecca Bellan","guid":178894,"unread":true,"content":"<article>After going back and forth over the provision, Sen. Marsha Blackburn offered an amendment to strip the provision alongside Sen. Maria Cantwell.&nbsp;</article>","contentLength":145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Docker MCP Catalog: the Secure Way to Discover and Run MCP Servers","url":"https://www.docker.com/blog/docker-mcp-catalog-secure-way-to-discover-and-run-mcp-servers/","date":1751375060,"author":"Nuno Coracao","guid":178895,"unread":true,"content":"<p>The Model Context Protocol (MCP) ecosystem is exploding. In just weeks, our Docker MCP Catalog has surpassed , validating that developers are hungry for a <a href=\"https://www.docker.com/products/mcp-catalog-and-toolkit/\">secure way to run MCP servers</a>. Today, we’re excited to share major updates to the Docker MCP Catalog, including enhanced discovery features and our new open submission process. With hundreds of developers already requesting to publish their MCP servers through Docker, we’re accelerating our mission to make <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">containerized MCP servers</a> the standard for secure AI tool distribution.</p><p>The rapid adoption of MCP servers also highlights a critical problem — the current practice of running them via npx or uvx commands exposes systems to unverified code with full host access, not to mention dependency management friction. In this post, we’ll explain why Docker is investing in the MCP ecosystem, showcase the new catalog capabilities, and share how you can contribute to building a more secure foundation for AI applications.</p><p><strong>Figure 1: The new Docker MCP Catalog, built for easier discovery.</strong></p><h2>Why Docker is building the MCP Catalog</h2><h3>The security issues in MCP distribution</h3><p>Every time a developer runs npx -y @untrusted/mcp-server or uvx some-mcp-tool, they’re making a dangerous trade-off: convenience over security. These commands execute arbitrary code directly on the host system with full access to:</p><ul><li>Environment variables and secrets</li></ul><p>Some MCP clients limit environment variable access, but even that is not a universal practice. This isn’t sustainable. As MCP moves from experimentation to production, we need a fundamentally different approach.</p><p>Docker has spent over a decade solving exactly these problems for cloud-native applications. We’ve built the infrastructure, tools, and trust that developers rely on to run billions of containers in production. Now, we’re applying these same principles to the MCP ecosystem.</p><p>When you run an MCP server from our Catalog, you get:</p><ul><li> verifying the image hasn’t been tampered with</li><li><strong>Software Bill of Materials (SBOMs)</strong> documenting every component</li><li> from your host system</li><li> to only what the server actually needs</li></ul><p>This isn’t about making life harder for developers—it’s about making security the path of least resistance.</p><h2>Introducing the enhanced MCP Catalog</h2><p>We’ve reimagined the MCP Catalog to make it more accessible and easier to navigate. You can still access the MCP Catalog from Docker Hub and the MCP Toolkit in Docker Desktop just like before, or <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">go straight to the MCP catalog</a>. We’ve gone beyond generic container image listings by building features that help you quickly find the right MCP servers for your AI applications.&nbsp;&nbsp;</p><p>: MCP servers are organized by what they actually do:</p><ul><li>Data Integration (databases, APIs, file systems)</li><li>Development Tools (IDEs, code analysis, testing)</li><li>Communication (email, Slack, messaging platforms)</li><li>Productivity (task management, calendars, note-taking)</li><li>Analytics (data processing, visualization, reporting)</li></ul><p>: Find servers by capability, tools, GitHub tags, and categories — not just by name.</p><p>: Every catalog entry clearly shows whether it’s Docker-built (with transparent build signing and verification) or community-built (containerized and maintained by the publisher).</p><p><strong>Figure 2: Discover MCP servers by use cases.</strong></p><h3>How we classify MCP Servers: Built by Docker vs. community-built</h3><p>: When you see “Built by Docker,” you’re getting our complete security treatment. We control the entire build pipeline, providing cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning.</p><p>: These servers are packaged as Docker images by their developers. While we don’t control their build process, they still benefit from container isolation, which is a massive security improvement over direct execution.</p><p><strong>Tiers serve important roles</strong>: Docker-built servers demonstrate the gold standard for security, while community-built servers ensure we can scale rapidly to meet developer demand. Developers can change their mind after submitting a community-built server and opt to resubmit it as a Docker-built server.</p><p><strong>Figure 3: An example of Built by Docker MCP Server.</strong></p><h2>Open for MCP server submission: Join the secure MCP movement</h2><p>Starting today, we’re opening our submission process to the community. Whether you’re an individual developer or an enterprise team, you can feature your MCP servers on the Docker MCP Catalog. By publishing through our catalog, you’re not just distributing your MCP server — you’re helping establish a new security standard for the entire ecosystem while getting your MCP tools available to millions of developers already using Docker via Docker Hub and Docker Desktop. Your containerized server becomes part of the solution, demonstrating that production-ready AI tools don’t require compromising on security.&nbsp;</p><h3>How to submit your MCP server</h3><ol><li> – Package your MCP server as a Docker image</li><li> – Opt for Docker-built (we handle the build) or community-built (you build and maintain it)</li></ol><p>We’re committed to a fast, transparent review process. Quality MCP servers that follow our security guidelines will be published quickly, helping you reach Docker’s 20+ million developer community.</p><p>ClickHouse is one of the first companies to take advantage of Docker’s MCP Catalog, and they opted for the Docker-built tier to ensure maximum security. Here’s why they chose to partner with Docker:</p><p><a href=\"https://clickhouse.com/\" rel=\"nofollow noopener\" target=\"_blank\"></a><em>, we deliver the fastest analytics database – open-source, and designed for real-time data processing and analytics at scale. As agentic AI becomes more embedded in modern applications, developers are using the ClickHouse MCP server to support intelligent, data-driven workflows that demand low latency, high concurrency, and cost efficiency.</em><em>To make it easier for developers to deploy these workloads, we’re featuring </em><a href=\"https://hub.docker.com/mcp/server/clickhouse/overview\" rel=\"nofollow noopener\" target=\"_blank\"></a><em> on Docker’s MCP Catalog, which provides </em><strong><em>a powerful way to reach 20M+ developers</em></strong><em> and makes it easier for Docker users to discover and use our solution. </em><strong><em>We opted for “Built by Docker” with the highest security standard</em></strong><em>, including cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning. Together with Docker, developers can run ClickHouse MCP Server with confidence, knowing it’s secured, verified, and ready for their agentic applications.” – </em>Tanya Bragin, VP of Product and Marketing Clickhouse</p><p>We’re preparing for the future of cloud-native AI applications. Remote MCP servers will enable:</p><ul><li>Managed MCP services that scale automatically</li><li>Shared capabilities across teams without distributing code</li><li>Stricter security boundaries for sensitive operations</li></ul><h3>Integration with the official MCP registry</h3><p>We’re actively collaborating with the MCP community on the upcoming official registry. Our vision is complementary:</p><ul><li>The official registry provides centralized discovery – the “yellow pages” of available MCP servers</li><li>Docker provides the secure runtime and distribution for those listings</li><li>Together, we create a complete ecosystem where discovery and security work hand-in-hand</li></ul><p>The explosive growth of our MCP Catalog, 1 million pulls and hundreds of publisher requests, tells us developers are ready for change. They want the power of MCP, but they need it delivered securely.</p><p>By establishing containers as the standard for MCP server distribution, we’re not trying to own the ecosystem — we’re trying to secure it. Every MCP server that moves from npx execution to containerized deployment is a win for the entire community.</p><ul><li><strong>Explore the enhanced MCP Catalog</strong>: <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">Visit the MCP Catalog</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to discover MCP servers that solve your specific needs securely.</li><li><strong>Use and test hundreds of MCP Servers</strong>: <a href=\"https://www.docker.com/products/docker-desktop/\">Download Docker Desktop</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to download and use any MCP server in our catalog with your favorite clients: Gordon, Claude, Cursor, VSCode, etc</li><li>: Star our repository and watch for updates on the MCP Gateway release and remote server capabilities.</li></ul><p>Together, we’re building more than a catalog — we’re establishing the secure foundation that the MCP ecosystem needs to grow from experimental tool to production-ready platform. Because when it comes to AI applications, security isn’t optional. It’s fundamental.</p>","contentLength":8137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US Government Takes Down Major North Korean 'Remote IT Workers' Operation","url":"https://yro.slashdot.org/story/25/06/30/2236218/us-government-takes-down-major-north-korean-remote-it-workers-operation?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1751374800,"author":"BeauHD","guid":178898,"unread":true,"content":"An anonymous reader quotes a report from TechCrunch: The U.S. Department of Justice announced on Monday that it had taken several enforcement actions against North Korea's money-making operations, which rely on undercover remote IT workers inside American tech companies to raise funds for the regime's nuclear weapons program, as well as to steal data and cryptocurrency. As part of the DOJ's multi-state effort, the government announced the arrest and indictment of U.S. national Zhenxing \"Danny\" Wang, who allegedly ran a years-long fraud scheme from New Jersey to sneak remote North Korean IT workers inside U.S. tech companies. According to the indictment, the scheme generated more than $5 million in revenue for the North Korean regime. [...]\n \nFrom 2021 until 2024, the co-conspirators allegedly impersonated more than 80 U.S. individuals to get remote jobs at more than 100 American companies, causing $3 million in damages due to legal fees, data breach remediation efforts, and more. The group is said to have run laptop farms inside the United States, which the North Korean IT workers could essentially use as proxies to hide their provenance, according to the DOJ. At times, they used hardware devices known as keyboard-video-mouse (KVM) switches, which allow one person to control multiple computers from a single keyboard and mouse. The group allegedly also ran shell companies inside the U.S. to make it seem like the North Korean IT workers were affiliated with legitimate local companies, and to receive money that would then be transferred abroad, the DOJ said.\n \nThe fraudulent scheme allegedly also involved the North Korean workers stealing sensitive data, such as source code, from the companies they were working for, such as from an unnamed California-based defense contractor \"that develops artificial intelligence-powered equipment and technologies.\"","contentLength":1878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Genesis AI launches with $105M seed funding from Eclipse, Khosla to build AI models for robots","url":"https://techcrunch.com/2025/07/01/genesis-ai-launches-with-105m-seed-funding-from-eclipse-khosla-to-build-ai-models-for-robots/","date":1751374612,"author":"Marina Temkin","guid":178893,"unread":true,"content":"<article>Genesis AI, which aims to build a foundational model for powering all kinds of robots, has emerged from stealth with $105M in seed funding.</article>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I built the tool I wished existed for moving Stripe between countries","url":"https://www.stripemove.com/","date":1751374370,"author":"felphos","guid":179224,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433429"},{"title":"Canonical Decides To Double Down On Their Investment In Java For Ubuntu","url":"https://www.phoronix.com/news/Canonical-Double-Down-Java","date":1751374337,"author":"Michael Larabel","guid":178903,"unread":true,"content":"<article>Ubuntu maker Canonical has decided to \"double down\" their investment in OpenJDK Java for Ubuntu Linux...</article>","contentLength":104,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Spegel, a Terminal Browser That Uses LLMs to Rewrite Webpages","url":"https://simedw.com/2025/06/23/introducing-spegel/","date":1751374182,"author":"simedw","guid":178963,"unread":true,"content":"<p>TL;DR Spegel is a proof-of-concept terminal web browser that feeds HTML through an LLM and renders the result as markdown directly in your terminal.</p><p>Two weekends ago, after my family had gone to sleep, I found myself unsupervised with a laptop and an itch to build something interesting. A couple of hours later, I had a minimal web browser running in my terminal (no JavaScript, GET requests only) that transformed web content based on my custom prompts.</p><p>Then, a few days later, Google released Gemini 2.5 Pro Lite, significantly faster inference speed, suddenly my little weekend hack became a tad more practical.</p><p>Adapting content to suit individual needs isn’t a new idea, think about translating books or summarising lengthy articles. However, this used to be slow and expensive. LLMs have changed this dramatically, making these transformations quick and easy.</p><p>Spegel (\"mirror\" in Swedish) lets you explore web content through personalized views using your own prompts. A single page can have multiple views, maybe one simplifying everything down to ELI5 or another highlighting key actions. It's entirely up to you and your prompting skills. </p><p>Sometimes you don't want to read through someone's life story just to get to a recipe.\n<img alt=\"Recipe Example\" src=\"https://simedw.com/2025/06/23/introducing-spegel/images/recipe_example.png\"><small>A previous version of this screenshot showed an incorrect recipe on the right. That was due to a bug where large websites got truncated. Thanks to everyone who pointed it out!</small></p><div><pre><code></code></pre></div><p>The pipeline is straightforward.</p><p>Spegel fetches HTML content, processes it through an LLM using prompts stored in a config file (~/.spegel.toml), and outputs markdown rendered via Textual. Prompts and views can be adjusted live during a browsing session.</p><p>This was my first experience using Textual for a TUI, and it's been delightful, possibly too delightful, as I found myself adding a few unnecessary interface elements just because it was easy.</p><p>One gotcha was ensuring only completed lines (ending in newline characters) were streamed; otherwise, the markdown renderer would parse incomplete markdown and fail to recover formatting</p><div><pre><code></code></pre></div><p>There are a lot of great terminal browsers out there, Lynx and Links2 are close to my heart. There are also modern attempts like Browsh that can even render graphs using half-block Unicode characters (▄█). </p><p>Spegel isn’t meant to replace these, it’s more of an exploration or proof-of-concept. It currently doesn't support POST requests (though I have some ideas on handling  elements by creating on-the-fly UIs).</p><p>But most modern websites aren’t designed with terminal browsing in mind. They rely on CSS and JS, making them cumbersome in small terminal windows, full of clutter and noise. Spegel tries to clear away distractions, providing content tailored more closely to your needs.</p><p>Spegel is still in the early stages, so expect some rough edges, but it’s usable and kind of fun to play with.</p><p>Then just run it with a URL:</p><div><pre><code>spegelsimedw.com</code></pre></div><p>Don't forget to configure your own , (<a href=\"https://github.com/simedw/spegel/blob/main/example_config.toml\">example</a>)</p><p>Want to check out the source or contribute? It’s all on GitHub:</p>","contentLength":2997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433409"},{"title":"Show HN: Jobs by Referral: Find jobs in your LinkedIn network","url":"https://jobsbyreferral.com/","date":1751374026,"author":"nicksergeant","guid":179223,"unread":true,"content":"<div><p data-slot=\"text\">JobsByReferral analyzes your professional network to find job openings at companies where you have connections. Get referred by people you already know and dramatically increase your chances of landing interviews.</p></div>","contentLength":213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433386"},{"title":"The Gap Strikes Back: Now Stylable","url":"https://css-tricks.com/the-gap-strikes-back-now-stylable/","date":1751373758,"author":"Patrick Brosset","guid":178896,"unread":true,"content":"<p>Four years ago, I wrote an article titled <a href=\"https://css-tricks.com/minding-the-gap/\">Minding the “gap”</a>, where I talked about the CSS  property, where it applied, and how it worked with various CSS layouts.</p><p>At the time, I described how easy it was to evenly space items out in a flex, grid, or multi-column layout, by using the  property. But, I also said that  the gap areas was much harder, and I shared a workaround.</p><p>However, workarounds like using extra HTML elements, pseudo-elements, or borders to draw separator lines tend to come with drawbacks, especially those that impact your layout size, interfere with assistive technologies, or pollute your markup with style-only elements.</p><p>Today, I’m writing again about layout gaps, but this time, to tell you all about a new and exciting CSS feature that’s going to change it all. <strong>What you previously had to use workarounds for, you’ll soon be able to do with just a few simple CSS properties that make it easy, yet also flexible, to display styled separators between your layout items.</strong></p><p>There’s already a <a href=\"https://www.w3.org/TR/css-gaps-1/\" rel=\"noopener\">specification draft for the feature</a> you can peruse. At the time I’m writing this, it is available in Chrome and Edge 139 behind a flag. But I believe it won’t be long before we turn that flag on. I believe other browsers are also very receptive and engaged.</p><p>Displaying decorative lines between items of a layout can make a big difference. When used well, these lines can bring more structure to your layout, and give your users more of a sense of how the different regions of a page are organized.</p><h3>Introducing CSS gap decorations</h3><p>If you’ve ever used a multi-column layout, such as by using the  property, then you might already be familiar with gap decorations. You can draw vertical lines between the columns of a multi-column layout by using the  property:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">article {\n  column-width: 20rem;\n  column-rule: 1px solid black;\n}</code></pre><p>The CSS gap decorations feature builds on this to provide a more comprehensive system that makes it easy for you to draw separator lines in other layout types.</p><p>For example, the draft specification says that the  property also works in flexbox and grid layouts:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">.my-grid-container {\n  display: grid;\n  gap: 2px;\n  column-rule: 2px solid pink;\n}</code></pre><p>No need for extra elements or borders! The key benefit here is that the decoration happens in CSS only, where it belongs, with no impacts to your semantic markup.</p><p>The CSS gap decorations feature also introduces a new  property for drawing lines between rows:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">.my-flex-container {\n  display: flex;\n  gap: 10px;\n  row-rule: 10px dotted limegreen;\n  column-rule: 5px dashed coral;\n}</code></pre><p>But that’s not all, because the above syntax also allows you to define multiple, comma-separated, line style values, and use the same  function that CSS grid already uses for row and column templates. This makes it possible to define different styles of line decorations in a single layout, and adapt to an unknown number of gaps:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">.my-container {\n  display: grid;\n  gap: 2px;\n  row-rule:\n    repeat(2, 1px dashed red),\n    2px solid black,\n    repeat(auto, 1px dotted green);\n}</code></pre><p>Finally, the CSS gap decorations feature comes with additional CSS properties such as , , , , and , which make it possible to precisely customize the way the separators are drawn, whether they overlap, or where they start and end.</p><p>Currently, the CSS gap decorations feature is only available in Chromium-based browsers.</p><p>The feature is still early in the making, and there’s time for you all to try it and to provide feedback that could help make the feature better and more adapted to your needs.</p><p>If you want to try the feature today, make sure to use Edge or Chrome, starting with version 139 (or another Chromium-based browser that matches those versions), and enable the flag by following these steps:</p><ol><li>In Chrome or Edge, go to .</li><li>In the search field, search for <strong>Enable Experimental Web Platform Features</strong>.</li></ol><h3>Using CSS gap decorations</h3><p>Let’s build a simple web page to learn how to use the feature. Here is what we’ll be building:</p><p>The above layout contains a header section with a title, a navigation menu with a few links, a main section with a series of short paragraphs of text and photos, and a footer.</p><p>We’ll use the following markup:</p><pre rel=\"HTML\" data-line=\"\"><code markup=\"tt\">&lt;body&gt;\n&lt;header&gt;\n  &lt;h1&gt;My personal site&lt;/h1&gt;\n&lt;/header&gt;\n&lt;nav&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"#\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"#\"&gt;Blog&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"#\"&gt;About&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"#\"&gt;Links&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;main&gt;\n  &lt;article&gt;\n    &lt;p&gt;...&lt;/p&gt;\n  &lt;/article&gt;\n  &lt;article&gt;\n    &lt;img src=\"cat.jpg\" alt=\"A sleeping cat.\"&gt;\n  &lt;/article&gt;\n  &lt;article&gt;\n    &lt;p&gt;...&lt;/p&gt;\n  &lt;/article&gt;\n  &lt;article&gt;\n    &lt;img src=\"tree.jpg\" alt=\"An old olive tree trunk.\"&gt;\n  &lt;/article&gt;\n  &lt;article&gt;\n    &lt;p&gt;...&lt;/p&gt;\n  &lt;/article&gt;\n  &lt;article&gt;\n    &lt;p&gt;...&lt;/p&gt;\n  &lt;/article&gt;\n  &lt;article&gt;\n    &lt;p&gt;...&lt;/p&gt;\n  &lt;/article&gt;\n  &lt;article&gt;\n    &lt;img src=\"strings.jpg\" alt=\"Snow flakes falling in a motion blur effect.\"&gt;\n  &lt;/article&gt;\n&lt;/main&gt;\n&lt;footer&gt;\n  &lt;p&gt;© 2025 Patrick Brosset&lt;/p&gt;\n&lt;/footer&gt;\n&lt;/body&gt;</code></pre><p>We’ll start by making the  element be a grid container. This way, we can space out the , , , and  elements apart in one go by using the  property:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">body {\n  display: grid;\n  gap: 4rem;\n  margin: 2rem;\n}</code></pre><p>Let’s now use the CSS gap decorations feature to display horizontal separator lines within the gaps we just defined:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">body {\n  display: grid;\n  gap: 4rem;\n  margin: 2rem;\n \n  row-rule: 1rem solid #efefef;\n}</code></pre><p>This gives us the following result:</p><p>We can do a bit better by making the first horizontal line look different than the other two lines, and simplify the  value by using the  syntax:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">body {\n  display: grid;\n  gap: 4rem;\n  margin: 2rem;\n \n  row-rule:\n    1rem solid #efefef,\n    repeat(2, 2px solid #efefef);\n}</code></pre><p>With this new  property value, we’re telling the browser to draw the first horizontal separator as a  thick line, and the next two separators as  thick lines, which gives the following result:</p><p>Now, let’s turn our attention to the navigation element and its list of links. We’ll use flexbox to display the links in a single row, where each link is separated from the other links by a gap and a vertical line:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">nav ul {\n  display: flex;\n  flex-wrap: wrap;\n  gap: 2rem;\n  column-rule: 2px dashed #666;\n}</code></pre><p>Very similarly to how we used the  property before, we’re now using the  property to display a dashed  thick separator between the links.</p><p>Our example web page now looks like this:</p><p>The last thing we need to change is the  element and its paragraphs and pictures. We’ll use flexbox again and display the various children in a wrapping row of varying width items:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">main {\n  display: flex;\n  flex-wrap: wrap;\n  gap: 4rem;\n}\n\n\nmain &gt; * {\n  flex: 1 1 200px;\n}\n\n\nmain article:has(p) {\n  flex-basis: 400px;\n}</code></pre><p>In the above code snippet, we’re setting the  element to be a wrapping flex container with a  gap between items and flex lines. We’re also making the items have a flex basis size of  for pictures and  for text, and allowing them to grow and shrink as needed. This gives us the following result:</p><p>Let’s use CSS gap decorations to bring a little more structure to our layout by drawing  thick separator lines between the rows and columns of the layout:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">main {\n  display: flex;\n  flex-wrap: wrap;\n  gap: 4rem;\n  row-rule: 2px solid #999;\n  column-rule: 2px solid #999;\n}</code></pre><p>This gives us the following result, which is very close to our expected design:</p><p>The last detail we want to change is related to the vertical lines. We don’t want them to span across the entire height of the flex lines but instead start and stop where the content starts and stops.</p><p>With CSS gap decorations, we can easily achieve this by using the  property to fine-tune exactly where the decorations start and end, relative to the gap area:</p><pre rel=\"CSS\" data-line=\"\"><code markup=\"tt\">main {\n  display: flex;\n  flex-wrap: wrap;\n  gap: 4rem;\n  row-rule: 2px solid #999;\n  column-rule: 2px solid #999;\n  column-rule-outset: 0;\n}</code></pre><p>The  property above makes the vertical column separators span the height of each row, excluding the gap area, which is what we want:</p><p>There’s more to the feature and I mentioned a couple more CSS properties earlier</p><ul><li>, which lets you control which of the decorations, rows or columns, appear above the other ones.</li><li> / , which sets the behavior of the decoration lines at intersections. In particular, whether they are made of multiple segments, which start and end at intersections, or single, continuous lines.</li></ul><p>Because the feature is new, there isn’t MDN documentation about it yet. So to learn more, check out:</p><p>The Edge team has also created an <a href=\"https://microsoftedge.github.io/Demos/css-gap-decorations/playground.html\" rel=\"noopener\">interactive playground</a> where you can use visual controls to configure gap decorations.</p><p>And, of course, the reason this is all implemented behind a flag is to elicit feedback from developers like you! If you have any feedback, questions, or bugs about this feature, I definitely encourage you to open a new ticket on the <a href=\"https://issues.chromium.org/issues/new?template_issue=422768750&amp;component=1456721\" rel=\"noopener\">Chromium issue tracker</a>.</p>","contentLength":8779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"xAI raises $10B in debt and equity","url":"https://techcrunch.com/2025/07/01/xai-raises-10b-in-debt-and-equity/","date":1751373354,"author":"Ram Iyer","guid":178892,"unread":true,"content":"<article>Elon Musk's AI company, xAI, has raised $5 billion in debt and $5 billion in equity, Morgan Stanley said on Monday. </article>","contentLength":116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a Website with Sphinx and Markdown","url":"https://www.blog.pythonlibrary.org/2025/07/01/creating-a-website-with-sphinx-and-markdown/","date":1751372880,"author":"Mike","guid":178904,"unread":true,"content":"<p><a href=\"https://www.sphinx-doc.org/en/master/\">Sphinx</a> is a Python-based documentation builder. The Python documentation is written using Sphinx. The Sphinx project supports using ReStructuredText and Markdown, or a mixture of the two. Each page of your documentation or website must be written using one of those two formats.</p><p>In this tutorial, you will learn how to use Sphinx to create a documentation site. Here is an overview of what you’ll learn:</p><ul><li>Making Markdown work in Sphinx</li><li>Building your Sphinx site</li><li>Adding content to your site</li></ul><p>Let’s start by installing all the packages you need to get Sphinx working!</p><p>You will need the following packages to be able to use Sphinx and Markdown:</p><p>You should install these package in a Python virtual environment. Open up your terminal and pick a location where you would like to create a new folder. Then run the following command:</p><pre data-enlighter-language=\"generic\">python -m venv NAME_OF_VENV_FOLDER</pre><p>Once you have the virtual environment, you need to activate it. Go into the&nbsp; folder and run the activate command in there.</p><p>Now you can install the dependencies that you need using pip, which will install them to your virtual environment.</p><p>Here’s how to install them using pip:</p><pre data-enlighter-language=\"generic\">python -m pip install myst-parser sphinx</pre><p>Once your packages are installed, you can learn how to set up your site!</p><p>Now that your packages are installed, you must set up your Sphinx website. To create a barebones Sphinx site, run the following command inside your virtual environment:</p><pre data-enlighter-language=\"generic\">sphinx-quickstart NAME_OF_SITE_FOLDER</pre><p>It will ask you a series of questions. The Sphinx documentation recommends keeping the source and build folders separate. Otherwise, you can set the other fields as needed or accept the defaults.</p><p>You will now have the following tree structure in your SITE_FOLDER:</p><p>You will work with the files and directories in this structure for the rest of the tutorial.</p><p>The next step on your Sphinx journey is to enable Markdown support.</p><h2>Making Markdown Work in Sphinx</h2><p>Go into the  directory and open the  file in your favorite Python IDE. Update the&nbsp; and the&nbsp; variables to the following (or add them if they do not exist):</p><pre data-enlighter-language=\"python\">extensions = ['myst_parser']\n\nsource_suffix = ['.rst', '.md']</pre><p>These changes tell Sphinx to use the Myst parser for Markdown files. You also leave ReStructuredText files in there so that your Sphinx website can handle that format.</p><p>You now have enough of your site available to build it and ensure it works.</p><h2>Building Your Sphinx Site</h2><p>You can now build a simple site with only an index page and the auto-generated boilerplate content. In your terminal, run the following command in the root of your Sphinx folder:</p><pre data-enlighter-language=\"generic\">sphinx-build -M html .\\source\\ .\\build\\</pre><p>The HTML files will be created inside the  folder. If you open the index page, it will look something like this:</p><p>Good job! You now have a Sphinx website!</p><p>Now you need to add some custom content to it.</p><h2>Adding Content to Your Site</h2><p>You can add ReStructuredText or Markdown files for each page of your site.&nbsp; using the  section:</p><pre data-enlighter-language=\"generic\">.. toctree::\n   :maxdepth: 2\n   :caption: Contents:\n\n   SUB_FOLDER/acknowledgments.md\n   doc_page1.md\n   OTHER_FOLDER/sub_doc_page1.md</pre><p>Let’s add some real content. Create a new file called&nbsp; in the root folder that contains the&nbsp; file. Then enter the following text in your new Markdown file:</p><pre data-enlighter-language=\"md\"># Python: All About Decorators\n\nDecorators can be a bit mind-bending when first encountered and can also be a bit tricky to debug. But they are a neat way to add functionality to functions and classes. Decorators are also known as a “higher-order function”. This means that they can take one or more functions as arguments and return a function as its result. In other words, decorators will take the function they are decorating and extend its behavior while not actually modifying what the function itself does.\n\nThere have been two decorators in Python since version 2.2, namely **classmethod()** and **staticmethod()**. Then PEP 318 was put together and the decorator syntax was added to make decorating functions and methods possible in Python 2.4. Class decorators were proposed in PEP 3129 to be included in Python 2.6. They appear to work in Python 2.7, but the PEP indicates they weren’t accepted until Python 3, so I’m not sure what happened there.\n\nLet’s start off by talking about functions in general to get a foundation to work from.\n\n## The Humble Function\n\nA function in Python and in many other programming languages is just a collection of reusable code. Some programmers will take an almost bash-like approach and just write all their code in a file with no functions. The code just runs from top to bottom. This can lead to a lot of copy-and-paste spaghetti code. Whenever two pieces of code do the same thing, they can almost always be put into a function. This will make updating your code easier since you’ll only have one place to update them.</pre><p>Make sure you save the file. Then, re-run the build command from the previous section. Now, when you open the  file, you should see your new Markdown file as a link that you click on and view.</p><p>Sphinx is a powerful way to create documentation for your projects. Sphinx has many plugins that you can use to make it even better. For example, you can use <a href=\"https://www.sphinx-doc.org/en/master/man/sphinx-apidoc.html\">sphinx-apidoc</a> to automatically generate documentation from your source code using the autodoc extension.</p><p>If you are an author and you want to share your books online, Sphinx is a good option for that as well. Having a built-in search functionality makes it even better. Give Sphinx a try and see what it can do for you!</p>","contentLength":5461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mike Driscoll: Creating a Website with Sphinx and Markdown","url":"https://www.blog.pythonlibrary.org/2025/07/01/creating-a-website-with-sphinx-and-markdown/","date":1751372880,"author":"","guid":178929,"unread":true,"content":"<p><a href=\"https://www.sphinx-doc.org/en/master/\">Sphinx</a> is a Python-based documentation builder. The Python documentation is written using Sphinx. The Sphinx project supports using ReStructuredText and Markdown, or a mixture of the two. Each page of your documentation or website must be written using one of those two formats.</p><p>In this tutorial, you will learn how to use Sphinx to create a documentation site. Here is an overview of what you’ll learn:</p><ul><li>Making Markdown work in Sphinx</li><li>Building your Sphinx site</li><li>Adding content to your site</li></ul><p>Let’s start by installing all the packages you need to get Sphinx working!</p><p>You will need the following packages to be able to use Sphinx and Markdown:</p><p>You should install these package in a Python virtual environment. Open up your terminal and pick a location where you would like to create a new folder. Then run the following command:</p><pre>python -m venv NAME_OF_VENV_FOLDER</pre><p>Once you have the virtual environment, you need to activate it. Go into the&nbsp; folder and run the activate command in there.</p><p>Now you can install the dependencies that you need using pip, which will install them to your virtual environment.</p><p>Here’s how to install them using pip:</p><pre>python -m pip install myst-parser sphinx</pre><p>Once your packages are installed, you can learn how to set up your site!</p><p>Now that your packages are installed, you must set up your Sphinx website. To create a barebones Sphinx site, run the following command inside your virtual environment:</p><pre>sphinx-quickstart NAME_OF_SITE_FOLDER</pre><p>It will ask you a series of questions. The Sphinx documentation recommends keeping the source and build folders separate. Otherwise, you can set the other fields as needed or accept the defaults.</p><p>You will now have the following tree structure in your SITE_FOLDER:</p><p>You will work with the files and directories in this structure for the rest of the tutorial.</p><p>The next step on your Sphinx journey is to enable Markdown support.</p><h2>Making Markdown Work in Sphinx</h2><p>Go into the  directory and open the  file in your favorite Python IDE. Update the&nbsp; and the&nbsp; variables to the following (or add them if they do not exist):</p><pre>extensions = ['myst_parser']\n\nsource_suffix = ['.rst', '.md']</pre><p>These changes tell Sphinx to use the Myst parser for Markdown files. You also leave ReStructuredText files in there so that your Sphinx website can handle that format.</p><p>You now have enough of your site available to build it and ensure it works.</p><h2>Building Your Sphinx Site</h2><p>You can now build a simple site with only an index page and the auto-generated boilerplate content. In your terminal, run the following command in the root of your Sphinx folder:</p><pre>sphinx-build -M html .\\source\\ .\\build\\</pre><p>The HTML files will be created inside the  folder. If you open the index page, it will look something like this:</p><p>Good job! You now have a Sphinx website!</p><p>Now you need to add some custom content to it.</p><h2>Adding Content to Your Site</h2><p>You can add ReStructuredText or Markdown files for each page of your site.&nbsp; using the  section:</p><pre>.. toctree::\n   :maxdepth: 2\n   :caption: Contents:\n\n   SUB_FOLDER/acknowledgments.md\n   doc_page1.md\n   OTHER_FOLDER/sub_doc_page1.md</pre><p>Let’s add some real content. Create a new file called&nbsp; in the root folder that contains the&nbsp; file. Then enter the following text in your new Markdown file:</p><pre># Python: All About Decorators\n\nDecorators can be a bit mind-bending when first encountered and can also be a bit tricky to debug. But they are a neat way to add functionality to functions and classes. Decorators are also known as a “higher-order function”. This means that they can take one or more functions as arguments and return a function as its result. In other words, decorators will take the function they are decorating and extend its behavior while not actually modifying what the function itself does.\n\nThere have been two decorators in Python since version 2.2, namely **classmethod()** and **staticmethod()**. Then PEP 318 was put together and the decorator syntax was added to make decorating functions and methods possible in Python 2.4. Class decorators were proposed in PEP 3129 to be included in Python 2.6. They appear to work in Python 2.7, but the PEP indicates they weren’t accepted until Python 3, so I’m not sure what happened there.\n\nLet’s start off by talking about functions in general to get a foundation to work from.\n\n## The Humble Function\n\nA function in Python and in many other programming languages is just a collection of reusable code. Some programmers will take an almost bash-like approach and just write all their code in a file with no functions. The code just runs from top to bottom. This can lead to a lot of copy-and-paste spaghetti code. Whenever two pieces of code do the same thing, they can almost always be put into a function. This will make updating your code easier since you’ll only have one place to update them.</pre><p>Make sure you save the file. Then, re-run the build command from the previous section. Now, when you open the  file, you should see your new Markdown file as a link that you click on and view.</p><p>Sphinx is a powerful way to create documentation for your projects. Sphinx has many plugins that you can use to make it even better. For example, you can use <a href=\"https://www.sphinx-doc.org/en/master/man/sphinx-apidoc.html\">sphinx-apidoc</a> to automatically generate documentation from your source code using the autodoc extension.</p><p>If you are an author and you want to share your books online, Sphinx is a good option for that as well. Having a built-in search functionality makes it even better. Give Sphinx a try and see what it can do for you!</p>","contentLength":5461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elon Musk’s Starlink Adds $750 Congestion Charge","url":"https://www.techdirt.com/2025/07/01/elon-musks-starlink-adds-750-congestion-charge/","date":1751372820,"author":"Karl Bode","guid":178902,"unread":true,"content":"<p>Low-Earth Orbit satellite broadband services like Starlink have their uses, but will always be dealing with capacity constraints. That means higher prices, weird restrictions, and, as of November 2024, a $100 “<a href=\"https://www.techdirt.com/2024/10/01/capacity-crunch-causes-musks-starlink-to-sock-users-with-100-congestion-charge/\">congestion charge</a>” for a service that’s already too expensive for many of the rural Americans who could most benefit.</p><p>It didn’t take long for that “congestion charge” to soar to $500 in some areas. Now it’s <a href=\"https://www.pcmag.com/news/starlink-imposes-eye-popping-demand-surcharge-for-new-sign-ups-in-this?mc_cid=d6432109c8&amp;mc_eid=eb527a594e\">already risen as high as $750</a> in states like Washington as Starlink is forced to try and deter users in some markets from using the increasingly congested network:</p><blockquote><p><em>“The change can crank up the starting price simply to own the Starlink dish on a residential plan to $1,099.”</em></p></blockquote><p>Other parts of the country see no congestion charge, but there’s no guarantee that they won’t see one down the line as the network subscribership grows. It’s also very likely the company will increasingly have to resort to doing things like throttling higher definition videos, or engaging in other network management tricks to try and keep the service semi-reliable.</p><p>You might recall that Republicans and Elon Musk <a href=\"https://www.techdirt.com/2022/09/19/musks-starlink-says-its-unfair-the-fcc-pulled-886-million-in-subsidies-musk-claims-he-doesnt-want-anyway/\">threw a hissy fit</a> a few years ago when the Biden FCC prioritized “future-proof” fiber and higher-capacity 5G services over Starlink in previous government subsidy programs, (correctly) expressing concerns that the service lacked the capacity to provide consistently reliable speeds on the taxpayer dime. </p><p>These are all things Republican Elon Musk ass kissers either don’t know, or don’t care about as they work to reward their billionaire benefactor. It will be up to their constituents to figure it out later. But money redirected to Starlink is money redirected to cheaper and better broadband alternatives, including <a href=\"https://www.techdirt.com/2025/02/24/arpa-is-quietly-funding-cheap-50-65-a-month-community-owned-gigabit-fiber-access-to-long-neglected-neighborhoods/\">super cheap gigabit fiber access</a> and <a href=\"https://www.techdirt.com/2024/10/02/new-map-shows-community-broadband-networks-are-exploding-in-u-s/\">community-owned and operated broadband networks</a>. </p><p>So again, Starlink is a nice step up if you’re in the middle of nowhere, lack any other connectivity options, can afford it, and don’t care about its potential environmental impact. But it shouldn’t be taking priority in terms of taxpayer subsidies. Unless, of course, you only care about kissing Elon Musk’s ass and don’t  about the constituents you claim to serve.</p>","contentLength":2216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building Support Structures • Flavia Circiumaru & Hannes Lowette • GOTO 2025","url":"https://www.youtube.com/watch?v=Uk9CwG9a_C4","date":1751371263,"author":"GOTO Conferences","guid":178905,"unread":true,"content":"<article>This interview was recorded for GOTO Unscripted. #GOTOcon #GOTOunscripted\nhttps://gotopia.tech\n\nRead the full transcription of this interview here:\nhttps://gotopia.tech/articles/386\n\nFlavia Circiumaru - Software Engineer at FundApps @flaviacirciumaru7502 \nHannes Lowette - Principal Consultant at Axxes, Monolith Advocate, Speaker &amp; Whiskey Lover @Belenar82 \n\nRESOURCES\nFlavia\nhttps://x.com/FlavsFA\nhttps://github.com/flaviacirciumaru\nhttps://www.linkedin.com/in/flavia-circiumaru\n\nHannes\nhttps://bsky.app/profile/hanneslowette.net\nhttps://twitter.com/hannes_lowette\nhttps://github.com/Belenar\nhttps://linkedin.com/in/hanneslowette\n\nLinks\nhttps://youtu.be/wEq0RnHHE8g\nhttps://youtu.be/eaATm4rFqrE\nhttps://youtu.be/X3AHdo34gWM\nhttps://youtu.be/_R_Vc17mxNE\nhttps://youtu.be/j2AQ9eTZ3-0\nhttps://youtu.be/zmA5fhV-FGk\nhttps://youtu.be/Cx_vijTm24w\n\nDESCRIPTION\nWhen FundApps exploded from 20 to 50+ engineers in just one year, their old \"everyone does support\" model crumbled spectacularly. Software engineer Flavia Circiumaru and host Hannes Lowette dive deep into how this London-based regulatory compliance company transformed from a support-free-for-all to a streamlined powerhouse.\n\nFlavia Circiumaru reveals the behind-the-scenes story of building a three-desk support system that turned their support team into the ultimate connector, bridging customers, developers, and business teams like never before. From battling context-switching nightmares to creating communication magic, Flavia and Hannes explore actionable insights on scaling support without losing your sanity.\n\nThe twist? Their ultimate goal is to make support so seamless that dedicated teams won't even be needed. Talk about engineering your way out of a job!\n\nTIMECODES\n00:00 Intro\n02:30 From support chaos to structure\n09:36 3 desks, 1 goal: Better support engineering\n20:09 The challenge of multi-tenant support at scale\n32:12 Future aspirations &amp; advice\n39:23 Outro\n\nRECOMMENDED BOOKS\nJacqui Read • Communication Patterns • https://amzn.to/3E37lvv\nSaleem Siddiqui • Learning Test-Driven Development • https://amzn.to/35OMb3n\nBen Goldacre • Bad Science • https://amzn.to/4lqNLKN\nBen Goldacre • Bad Pharma • https://amzn.to/3G15Mz7\nJeet Pattanaik • Ethics in AI • https://amzn.to/4jXyQXw\n\nhttps://bsky.app/profile/gotocon.com\nhttps://twitter.com/GOTOcon\nhttps://www.linkedin.com/company/goto-\nhttps://www.instagram.com/goto_con\nhttps://www.facebook.com/GOTOConferences\n#SupportStructures #SupportEngineering #Communication #DevTeams #TodayInTech #FlaviaCirciumaru #HannesLowette\n\nCHANNEL MEMBERSHIP BONUS\nJoin this channel to get early access to videos &amp; other perks:\nhttps://www.youtube.com/channel/UCs_tLP3AiwYKwdUHpltJPuA/join\n\nLooking for a unique learning experience?\nAttend the next GOTO conference near you! Get your ticket at https://gotopia.tech\nSign up for updates and specials at https://gotopia.tech/newsletter\n\nSUBSCRIBE TO OUR CHANNEL - new videos posted almost daily.\nhttps://www.youtube.com/user/GotoConferences/?sub_confirmation=1</article>","contentLength":3036,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Uk9CwG9a_C4?version=3","enclosureMime":"","commentsUrl":null},{"title":"10 GitHub Awesome Lists for Data Science","url":"https://www.kdnuggets.com/10-github-awesome-lists-for-data-science","date":1751371218,"author":"Abid Ali Awan","guid":178867,"unread":true,"content":"<article>Most popular educational resource list on GitHub for Python, R, SQL, analytics, machine learning, datasets, and more.</article>","contentLength":117,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/awan_10_github_awesome_lists_data_science_1.png","enclosureMime":"","commentsUrl":null},{"title":"Why Extracting Text From PDFs Still Feels Like a Hack–And the Legacy Design that Keeps AI Stuck","url":"https://hackernoon.com/why-extracting-text-from-pdfs-still-feels-like-a-hack-and-the-legacy-design-that-keeps-ai-stuck?source=rss","date":1751371216,"author":"Paolo Perrone","guid":179053,"unread":true,"content":"<p>Devs working with LLMs run into document parsing constantly. And every few months, there’s a new wave of hype (or frustration) around the PDF problem. During those moments, it’s not unusual to see software folks venting about how one file format became such a massive headache. But the struggle isn’t new.</p><p>\\\nLong before LLMs entered the picture, entire SaaS businesses were built around managing the messiness of PDFs. And for good reason, it’s a format that was never designed for the kind of structured, machine-readable access we now expect.</p><p>\\\nWhen software becomes as widespread as Adobe Acrobat and the PDF format, it starts to feel like a permanent part of the landscape. It’s easy to forget that behind that ubiquity were real design decisions, constraints, and tradeoffs made by real engineers solving real problems. Problems that, over time, evolved and became the roots of today’s pain.</p><p>\\\nYes, PDFs are frustrating. But they weren’t born broken. In fact, they were a surprisingly elegant solution for their time.</p><p>\\\nSo, let’s zoom out. This story takes a step back to explore the origins of the PDF format: how it came to be, what problems it set out to solve, and how the decisions made in the early 90s still ripple through today’s stack. The goal: to understand not just the “why is this so hard?”, but also the “how did we get here?”</p><h2>Back to the ‘80s, from paper to&nbsp;pixels.</h2><p>The shift had begun. Personal computers were exploding in popularity, and paper documents were no longer the default. Software like VisiCalc, WordStar, WordPerfect, and early Microsoft Word marked the dawn of a new way to write, edit, and share.</p><p>By the late ’80s, PC suites had all but killed off the typewriter. Executives could tweak reports minutes before a meeting. Analysts were running “what-if” scenarios in spreadsheets. Teachers were printing tests on the fly. Engineers replaced drafting tables with digital blueprints.</p><p>\\\nIncreasingly, documents became the new workplace. Not just the end product, but where the work actually happened.</p><h2>The ‘90s and the birth of the&nbsp;PDF.</h2><p>In the early 1990s, the rise of PC-based word processing and electronic file sharing solved many problems, while introducing new ones. Every computer had its own fonts, printer drivers, and layout quirks. A report that looked perfect on one machine could print as a jumbled mess on another. Sharing files became a gamble.</p><p>\\\nTo fix this, in 1991 Adobe co-founder John Warnock and his team launched a project codenamed “Camelot” to create a truly universal document format. The result was the PDF, a file that embedded fonts, graphics, and page layout all in one place. This “digital paper” guaranteed that documents looked exactly the same everywhere, whether on Windows, Mac, or any printer.</p><p>\\\nBy bundling every font, image, and layout detail into a single file, PDFs let users share documents without surprises, and what you see on screen is printed exactly the same everywhere. Adobe made the free Acrobat Reader available in 1994, and within five years, PDF became the go-to format for everything from product manuals and corporate reports to government forms and academic papers. </p><p>\\\nBy the early 2000s, “export as PDF” was a one-click option in almost every authoring tool, and organizations across industries embraced it for distribution, archiving, and compliance. And it’s still the standard today.</p><p>The very thing that made PDFs so appealing (their promise of pixel-perfect fidelity) also introduced a hidden trade-off: it locked content into a rigid, print-first structure.</p><p>\\\nBeneath every flawless page was essentially a digital snapshot, built to mimic what came out of a printer. Headings, tables, paragraphs, none of it had semantic meaning. To a computer, it was just coordinates and text boxes scattered across a canvas.</p><p>\\\nAt first, this didn’t matter. But as documents moved from desktops to web browsers, mobile screens, and automated pipelines, the cracks began to show. Want to extract clean data? Reflow text on a phone? Understand document structure? Suddenly, what looked clean to humans became a mess for machines. \\n  <img src=\"https://cdn.hackernoon.com/images/null-td033hx.png\" alt=\"Ideal vs. canvas: why PDF feels uniquely hostile\"></p><h2>Tagged PDF and Other Modernization Attempts</h2><p>Adobe wasn’t blind to the problem. Tagged PDF (introduced in 2001 and later formalized in PDF/UA for accessibility) adds an HTML-like logical structure. It never became universal, but it is mandated for accessible government documents and widely used in large-enterprise workflows. Other milestones, such as PDF/A for long-term archiving, XMP metadata support, and the 2008 hand-off of the spec to ISO, show steady efforts to modernize the format. Still, broad adoption lagged; tagging is invisible to most users, tedious for creators, and often stripped out by careless export settings.</p><p>\\\nA whole ecosystem of SaaS tools popped up to bridge this gap. You see it in heavyweights like DocuSign, in the many web-based PDF editors such as DocHub, and in open-source libraries like Poppler, which developers depend on just to pull text out of PDFs. </p><p>\\\nThat’s also why the big cloud players are all throwing serious AI muscle at this problem: AWS with Textract, Google with Document AI, and Microsoft with Azure AI Document Intelligence. The market emerged, products followed, and plenty of revenue flowed. Adobe, whether we like it or not, changed the game.</p><h2>The Rise of AI-Native PDF&nbsp;Handling</h2><p>When ChatGPT hit, the “PDF problem” exploded. Companies scrambled to feed their data into LLMs, only to hit a wall: most of that valuable info was locked away inside PDFs.</p><p>\\\nAt first, the goal was simple: just extract clean text for Retrieval-Augmented Generation (RAG). But that quickly proved too basic. Without layout awareness, text from columns got scrambled, tables turned into nonsense, images got ignored, and important context disappeared.</p><p>\\\nModern Document AI now trains models to understand a document’s visual and logical layout: identifying titles, paragraphs, tables, and images. So, AI can reference information, skip repeated headers/footers, and grasp the overall structure.</p><p>\\\nThis AI stack reveals the full extent of the mess we’re dealing with. What should be straightforward data extraction now requires multiple specialized layers:</p><ul><li><p>Layout analysis to understand document structure,</p></li><li><p>OCR to extract text from images and scanned documents,</p></li><li><p>VLM orchestration to coordinate these different AI components.</p></li></ul><p>Each layer adds latency, potential errors, and compute cost. The irony is staggering: we’re using some of the most advanced AI models ever built to solve a problem that stems from a 30-year-old decision to treat documents like photographs.</p><p>\\\nWhile PDFs have gradually evolved, their print-first DNA keeps piling costs onto every modern workflow. Structured formats, scanned or photographed, do introduce some of the same hurdles, but PDF’s design amplifies the pain.</p><p>We can’t scrap decades of PDFs overnight, but we can avoid repeating history. For new content, choose born-digital formats that preserve semantics by default:</p><ul><li>Markdown-derived standards for technical docs,</li><li>or DOCX/OOXML when Office compatibility is a must.</li></ul><p>\\\nWhen a fixed-layout file is unavoidable, export with full tags and metadata intact; some authoring tools now automate this. Government procurement rules that require PDF/UA compliance are a positive precedent. Similar pressure from enterprises on vendors and regulators can push tagging from “nice-to-have” to “table stakes.”</p><p>\\\nLong term, open standards like W3C’s Portable Web Publication or EPUB 3, along with upcoming containerized JSON-based formats, promise fidelity without sacrificing structure. Supporting these in mainstream authoring tools (and educating users to adopt them) will spare the next generation from writing vision models just to pull text out of a contract.</p><p>\\\nThe story of PDFs proves that early design choices echo for decades. The lesson isn’t to vilify the engineers who solved 1991’s problem; it’s to recognize that today’s “good enough” shortcuts become tomorrow’s costly handcuffs. Let’s embed semantics at the source, back open, machine-readable standards, and ensure the next wave of document tech is built for humans and machines alike.</p><p>\\\nFor teams already dealing with legacy formats, tools like  offer an Open-Source API-based pipeline to convert complex documents into structured, chunked formats tailored for LLM and RAG workflows, available both as hosted endpoints or self-managed infrastructure.</p><h2>Struggling to Grow Your Audience as a Tech Professional?</h2><p><a href=\"https://techaudienceaccelerator.substack.com/\">The Tech Audience Accelerator</a> is the go-to newsletter for tech creators serious about growing their audience. You’ll get the proven frameworks, templates, and tactics behind my 30M+ impressions (and counting).</p>","contentLength":8753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cuba’s Power Grid Nears Total Failure","url":"https://spectrum.ieee.org/cuba-energy-crisis","date":1751371205,"author":"Ricardo Torres","guid":178865,"unread":true,"content":"<p>Decades of neglect and subpar fuel leave Cuba’s energy infrastructure in crisis</p>","contentLength":81,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTExNTM5Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2NTc3MTgyMn0.MpArvxigPLjNqXMmdCRIIb_2Han8CkTmfg2U1UC9dj8/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Gentoo Releases Updated Install Media Based On KDE Plasma 6.3 + Linux 6.12 LTS","url":"https://www.phoronix.com/news/Gentoo-Linux-July-2025","date":1751371200,"author":"Michael Larabel","guid":178873,"unread":true,"content":"<article>The Gentoo Linux project ended the month of June by releasing new install media...</article>","contentLength":82,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Blockchain Is Redefining Financial Inclusion for the Unbanked in 2025","url":"https://hackernoon.com/how-blockchain-is-redefining-financial-inclusion-for-the-unbanked-in-2025?source=rss","date":1751371190,"author":"Vision NP","guid":179052,"unread":true,"content":"<p>In 2025, the global economy will continue to rapidly digitize. Do you know another secret? Yet over 1.4 billion people remain unbanked, according to the latest World Bank data. These people lack access to traditional financial services due to systemic barriers, such as identity issues, geographic isolation, high service fees, and distrust in centralized institutions. On the other hand, blockchain networks have a decentralized nature with the potential for financial inclusion. Bitcoin appeared to be the main enabler of decentralization. Traditional financial systems are bound by gatekeeping, bureaucracy, and geography, but blockchain offers a trustless, borderless, and programmable financial layer. It can turn a smartphone into a bank, digital ID provider, and loan officer, all in one.</p><p>\\\nThe diagram below explains how blockchain technology transforms the financial inclusion of an unbanked population.</p><p>\\\nThis study explores how blockchain tackles financial exclusion through the latest innovations.</p><h3><strong>1. Own Your Online Identity—No Central Authority Required</strong></h3><p>One of the key obstacles to unbanking is its verifiable identity. <a href=\"https://en.wikipedia.org/wiki/Decentralized_identifier\">Decentralized Identity (DID</a> blockchain systems (<a href=\"https://polygon.technology/blog/introducing-polygon-id-zero-knowledge-own-your-identity-for-web3\">Polygon ID</a> and <a href=\"https://world.org/blog/world/proof-of-personhood-what-it-is-why-its-needed\">Proof of Personhood of Worldcoin</a>) enable users to create tamper-resistant digital identities without the help of central authorities. They are owned by the users themselves and cryptographically authenticated, meaning that they can be applied to any finance platform. The DID enables one to.</p><ul><li>Participate in global markets</li></ul><p>The effective implementation of a DID can be determined using a real-life example in Bhutan. In the middle of 2024, <a href=\"https://www.biometricupdate.com/202408/bhutans-self-sovereign-digital-id-switches-to-polygon-blockchain\">Bhutan migrated</a> its national self-sovereign digital identity operating on the Hyperledger Indy to the Polygon blockchain and started using the previously adopted <a href=\"https://docs.credebl.id/docs\">CREDEBL protocol</a>, which is an open-source, UN-approved verifiable credentials system.</p><p>\\\nIt is very likely that you also ask yourself why it is important. Bhutan is one of the first nations in the world to fully implement a sovereign identity system and provide all citizens with ownership and control of their digital credentials. Moving to Polygon implies a faster, safer, and higher capacity to have a broad, nationwide scope of use.</p><h3><strong>2. Banking Without Banks? Stablecoins Make It Possible</strong></h3><p>Centralized traditional banks often charge high fees for account maintenance, remittances, and FX conversion. However, if we locate Stablecoins like USDC, cUSD (Celo Dollar), and GHO (Aave's stablecoin), they provide a USD-pegged, low-volatility alternative that can be sent and received globally, instantly, and at nearly zero cost.</p><p>Through mobile wallets, such as Trust Wallet, users in remote areas can</p><ul><li><p>Save in a stable currency</p></li><li><p>Avoid local currency inflation</p></li></ul><p>In Latin America, <a href=\"https://www.chainalysis.com/blog/latin-america-cryptocurrency-geography-report-2022-preview/\">stablecoins are rapidly growing</a>, as in the case of Venezuela, where stablecoins are used in 34 % of cases, and Argentina, where they constitute approximately 61.8 % of crypto activity, far more than in Brazil (59.8 %) and Mexico (18 %). In low-banking, high-inflation contexts, mobile-first users are also turning to dollar-pegged tokens, such as USDT and USDC, as an easily accessible and reliable source of savings, transactions, and even remittances when normal banks are weakened by rampant inflation and rising costs of using bank services.</p><h3><strong>3. Microfinance and DeFi Lending</strong></h3><p>The decentralized Finance (DeFi) protocols Goldfinch, Aave Arc, and Maple Finance currently target real-world assets (RWA) and under-collateralized lending, giving credit to new establishments and individuals in emerging markets.</p><ul><li><p>Accessible without a credit history</p></li></ul><p>With blockchain, a street vendor can seek out a loan through a foreign investor to whom smart contracts apply and are repaid through the blockchain.</p><p>\\\nThe fact that Goldfinch has successfully lent more than USD 100 + million to real-life businesses, and most prominently within the Sub-Saharan Africa market, exemplifies the issue of using blockchain to match global capital to the underbanked. Goldfinch offers accessible and sustainable financing to underserved members of the community by utilizing the USDC, local partnerships, and smart contracts to offer scalable, accountable, and impactful financing to the underserved.</p><h3><strong>4. Remittances Without Middlemen</strong></h3><p>The world remittance market, with an annual turnover of more than 800 billion, is currently monopolized by intermediaries such as the Western Union, charging sums up to 10 percent in fees. Smart contract-based remittance systems such as Ramp Network, Xoom using the USDC, and OnFinality have minimized the cost to be nearly free and settle within minutes rather than days.</p><p>\\\nOne of the real-world examples for this category is that in the Philippines, <a href=\"https://coins.ph/blog/coins-ph-brings-cheaper-and-faster-remittances-for-all/\"> is revolutionizing remittances</a> by using blockchain and stablecoins such as USDC and PHPC to enable near-instant, low-cost cross-border transfers.</p><p>\\\nTraditional services charge 6-7% and create a delay by taking time, but Coins.ph processes remittances in minutes with fees as low as 0.1%. Recently, it handled over  in remittances. This allowed unbanked users to reach local cash-in partners.</p><p>\\\nThis is the same case as Chipper Cash, the African platform that runs on <a href=\"https://ripple.com/ripple-press/crypto-enabled-payments-in-africa/\">Ripple to make cross-border remittances</a> through cryptocurrency simple and relatively cheap, which is a good example of how blockchain can remove middlemen and raise financial inclusion.</p><h3><strong>5. Local Economies Powered by Blockchain</strong></h3><p>New blockchain-backed economies are becoming strong alternatives to conventional markets as substitutes for community currencies and economies of rewards, which are dependent on banks and centralized authorities. In such ecosystems, users have access to earn, save, spend, and accumulate wealth completely inside blockchain-native infrastructure, in many cases, only with a mobile phone.</p><ul><li><p> promote environmental action through token incentives.</p></li><li><p> and  enable direct aid and grants without bureaucratic delay.</p></li><li><p> integrated blockchain wallets with mobile money services, such as M-Pesa, in Africa.</p></li></ul><p>These projects present a new trend: blockchain is no longer reshaping finance, but rather democratizing access to value, redefining aid, and a local, inclusive economy. Blockchain native systems allow the elimination of mediators and the provision of communities that are disadvantaged in the global economy with resilient financial infrastructure by allowing programmable trust. Efforts such as <a href=\"https://spacecoin.org/\">Spacecoin</a> to decentralize the Internet and finance show how blockchain can overcome the digital and financial divides. By paying Internet bills with crypto, generally no more than a dollar or two per month, disadvantaged communities develop an on-chain credit history that can lead them to loans and other financial services without going through banks.</p><h3><strong>6.Key Challenges to Watch &amp; Attempt to Tackle Them</strong></h3><p>Owing to the transformational power of the blockchain, there are real-world obstacles to its use in disadvantaged communities. Problematic issues should be addressed in a context-aware and user-centric manner to help create meaningful financial inclusion.</p><ul><li><p>A significant portion of the unbanked population does not have smartphones or access to a stable Internet connection, which reduces their exposure to dApps. This gap is bridged by mobile-first instruments and SMS-based wallets such as .</p></li><li><p>Interfaces are difficult to understand and deal with, and they contain unknown cryptographic concepts. Getting new users on board is assisted by local education initiatives and simplified apps such as .</p></li><li><p>Using stablecoins puts users at regulatory and issuer risks. Asset-backed or regional stablecoins can be made more resilient.</p></li><li><p>The use of technical terms and the lack of proper localization made mass adoption impossible. To address this, platforms should enhance the user interface and experience with a user-friendly multilingual interface.</p></li><li><p>Losing private keys or becoming victims of fraud can be disastrous. Therefore, emerging safer models include social recovery wallets (e.g., Trust Wallet has a feature to back up your key in Google Drive) and local custodianship models.</p></li><li><p>Vague or antagonistic legislation halts the advancement in certain countries like Bhutan, which are leading positive exponents by adopting controlled Blockchain ID structures.</p></li><li><p>The expensive mining costs for PoW-based blockchains, such as Bitcoin, and network congestion, restrict their use. To tackle this problem, there are other scalable and cheap options, including chains such as Polygon, Celo, and Solana.</p></li></ul><p>These are not only technical issues, but also ethical. Addressing these problems presents the challenge of working together, and doing so involves developers, policymakers, educators, and communities. It is only after that time that the blockchain can open the door to fair access to financial tools.</p><p><strong>🔗 Blockchain + AI: Smarter Financial Access</strong></p><p>Artificial intelligence (AI)-based products and services are becoming increasingly popular as of 2025, and the future seems even brighter if blockchain is combined with AI for effective services. Some real-world examples, such as Worldcoin and the Human Protocol, combine blockchain with AI to assess reputational credit scores. This allows lenders to evaluate unbanked users based on the following criteria:</p><ul></ul><p>This AI + blockchain combination completely upends conventional credit scoring regimes in low-data settings by relying on on-chain modes of behavior, peer-to-peer recommendations, digital identity solutions, and decentralized reputation systems to create dynamic, immutable credit profiles. The new system can offer inclusive, real-time risk assessment, whereas traditional systems use credit rating agencies that have not changed since the 80s and fixed financial history; millions of unbanked people will be able to be supplied with fair credit, insurance, and other financial instruments.</p><p>\\\n<strong>🏦 Central Bank Digital Currencies (CBDCs)</strong></p><p>With DeFi developing organically, it is being approached by the government. Over 130 nations are in the trial or implementation of CBDCs. When combined with blockchain wallets, CBDCs can</p><ul><li>Enable state benefits without banks</li><li>Distribute UBI (universal basic income)</li><li>Track inflation-resistant micro-savings</li></ul><p>The e-CNY, eNaira, and India digital rupees are already under testing for merchant payments and cross-border payouts.</p><p>\\\n<strong>🌍 Local Economies, Global Rails</strong></p><p>Kenyan community projects, such as , or a Latin American community project, such as , enable communities to create locally built digital currencies that are backed to stabilize other ecoins, such as cUSD or DAI. These currencies are frequently issued to local DAOs or mobile applications and are tasked with reflecting the actual economic transactions in the community.</p><p>\\\nThese systems: \\n • Mobilize microtrade without money: allow local businesses, farmers, and vendors to enable them to receive payments digitally, even in places where the traditional banking system is not well connected. \\n • Pay people to do good things, such as tree planting, education, recycling, or volunteering--turn people into communal tokens that anybody can spend in the community.</p><p>• Establish circular economies founded on activities tied in which value is generated and used in the local area, hence disregarding the need to rely on outside aid or unstable fiat currencies.</p><p>\\\nFor example, the Sarafu Network in Kenya, initiated by Grassroots Economics, has provided over 60,000 users with services and transfer of goods using digital credits printed in the community, with a total number of transacted dollars so far, being more than $ 3 million. Members are credited with credit vouchers in exchange for labor or production, which can be used in the network for food, transport, and other basic necessities. The given model not only creates financial stability but also develops better social bonds, more economic collaboration, and local development, which are also strengthened by blockchain technology that works under the surface.</p><h2><strong>Final Thoughts: A Financial Reset</strong></h2><p>Blockchain is not just a technological innovation, it is also a catalyst for economic empowerment and financial justice. Blockchain allows people in an underserved community to be empowered about their financial future, irrespective of geographical locations and backgrounds, by breaking the traditional rungs of identity, access, and trust.</p><p>\\\nLow-fee remittances made AirBnB-killer project stable ecoins, microcredit without collateral provided by the DeFi protocols, secure documents and data on the blockchain, and decentralized identity systems that are replacing formal KYC, all of which are already here and radically developing.</p><p>\\\nWe are in a new financial age where no one is forgotten and banking comes not with a privilege but a right as governments, developers, and communities collaborate to create ethical, inclusive, and clear blockchain-based solutions.</p><p>\\\nThe next billion users will not just join Web3; they will help shape it. In the future, the most powerful bank may not have walls. It may simply be a phone, key, or a chain.</p>","contentLength":13010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Crisis to Security - How DePIN Can Solve Tonga's Cybersecurity Challenges","url":"https://hackernoon.com/from-crisis-to-security-how-depin-can-solve-tongas-cybersecurity-challenges?source=rss","date":1751370901,"author":"Edwin Liava'a","guid":179051,"unread":true,"content":"<p><em>A comprehensive response to recent data breaches and a roadmap for digital sovereignty</em></p><h2>The Wake-Up Call - Recent Cybersecurity Breaches in Tonga</h2><p>The recent leak of patient information online has sent shockwaves through Tonga's government and healthcare systems, highlighting a critical vulnerability that threatens not just individual privacy, but national security itself. When hackers can access sensitive patient data and potentially disrupt essential services, it becomes clear that Tonga's current digital infrastructure is inadequately prepared for modern cyber threats.</p><p>This breach represents more than just a technical failure, it's a symptom of a fundamental problem with centralized, vulnerable systems that rely on foreign infrastructure and proprietary software. For vulnerable groups, including patients, the elderly, and those requiring critical medical services, these security failures can have life-threatening consequences.</p><p><strong>The time for half measures is over. Tonga needs a revolutionary approach to cybersecurity, one that puts sovereignty, resilience, and unbreachable security at its core.</strong></p><h2>The DePIN Solution - Building an Unhackable Government Network</h2><p>Decentralized Physical Infrastructure Network (DePIN) represents a paradigm shift from vulnerable centralized systems to a distributed, blockchain-based architecture that leverages the proven security model of Bitcoin, a network that has operated for 16 years without ever being successfully hacked or compromised.</p><h3>Why Bitcoin Level Security Matters</h3><ul><li> January 3, 2009</li><li> 16 Years</li><li> Never compromised or hacked since inception</li><li> Imperfect networks and imperfect human aspects</li></ul><p>Bitcoin has stood the test of time as the most secure decentralized network precisely because it was designed to be resilient against the very types of attacks that are currently plaguing traditional government systems.</p><h2>The Journey to Digital Sovereignty - Tonga's DePIN Transformation Story</h2><p>Picture this i.e. It's a Monday morning in Nuku'alofa, and instead of waking up to news of another devastating cyberattack, government workers across Tonga are logging into the most secure digital infrastructure in the Pacific. This isn't a distant dream, it's the inevitable destination of our DePIN journey, and here's how we get there.</p><h3>Chapter 1: The Foundation - Building Our Digital Fortress at MEIDECC</h3><p><em>\"Every revolution begins with a single spark.\"</em></p><p>Our story begins where all great transformations do, with a proof of concept that will forever change how Tonga thinks about digital security. Picture the Tonga Ministry of Meteorology, Energy, Information, Disaster Management, Environment, Climate Change and Communications (MEIDECC) building, currently vulnerable to the same attacks that just compromised patient data across the country. But within six months, this same building will house the most secure government network node in the Pacific.</p><p><strong>The First Month - Discovery and Vision</strong></p><p>The transformation begins quietly, almost invisibly. Our technical team walks through MEIDECC's corridors, laptops in hand, conducting what looks like a routine network audit. But this is no ordinary assessment, we're mapping every cable, every server, every potential entry point that hackers could exploit. In conference rooms late into the evening, security experts spread out network diagrams like military strategists planning a campaign.</p><p>By week three, the real work begins. While government employees continue their daily routines, our developers are crafting something revolutionary in the background, a blockchain architecture specifically designed for Tonga's unique needs. Picture lines of code flowing across screens, each function carefully designed to prevent the exact type of breach that recently exposed sensitive patient information. This isn't just software development; it's digital nation building.</p><p><strong>Months Two and Three - The Technical Renaissance</strong></p><p>Cafe Escape, Friends, Post Coffee and other coffee spots around MEIDECC start recognizing our <a href=\"https://pasifika.xyz/\">Pasifika Web3 Tech Hub</a> team as regulars. Day and night, they're building Tonga's digital sovereignty one line of code at a time. They're implementing proof-of-authority consensus mechanisms that ensure only trusted government nodes can validate transactions. They're creating smart contracts that will make document forgery impossible. They're designing multi-signature authorization systems that require multiple government officials to approve any sensitive operation.</p><p>Meanwhile, the real magic is happening in the government's commitment to energy independence. Picture construction crews arriving at MEIDECC with solar panels and battery systems. These aren't just renewable energy installations, they're the foundation of a network that can operate completely independently of Tonga's power grid. When the next cyclone hits and the lights go out across Nuku'alofa, MEIDECC's blockchain nodes will continue humming along, powered by the Pacific sun.</p><p><strong>Months Four and Five - Hardening the Fortress</strong></p><p>As solar panels gleam on MEIDECC's roof, inside the building, another transformation is taking place. Hardware security modules, the digital equivalent of bank vaults, are being installed to protect the cryptographic keys that secure Tonga's data. Multi-layered firewalls are configured like digital moats around the government's most sensitive information. Intrusion detection systems are deployed like digital sentries, watching for any sign of malicious activity.</p><p>The real-time monitoring systems we install don't just detect threats—they learn from them. Every attempted intrusion, every suspicious packet, every anomalous behavior pattern becomes data that makes the system smarter and more resilient.</p><p><strong>Month Six - The Proof is in the Testing</strong></p><p>Picture this scene i.e. In a secure facility in Nuku'alofa, ethical hackers, some of the best cybersecurity experts money can hire, are unleashing every attack they can imagine against MEIDECC's new blockchain infrastructure. They try DDoS attacks that would cripple traditional systems. They attempt the same social engineering tactics that compromised the patient data. They simulate the power outages and network failures that have historically left government systems vulnerable.</p><p>And the system holds. Every attack is deflected, every vulnerability is absent, every backup system activates flawlessly. The blockchain continues processing transactions, the solar power keeps systems operational, and the monitoring systems log every attempted intrusion for future analysis.</p><h3>Chapter 2: The Expansion - From Spark to Wildfire</h3><p><em>\"Success is not a destination, but a journey of continuous transformation\"</em></p><p>By month seven, word is spreading through government circles about something remarkable happening at MEIDECC. While other ministries struggle with basic cybersecurity, MEIDECC operates with the confidence of an organization whose data is truly secure. Ministers start asking questions: \"How can we get this level of protection?\"</p><p><strong>The Ministry Awakening - Months Seven and Eight</strong></p><p>Our story now expands beyond MEIDECC's walls. Picture assessment teams walking through the Ministry of Health, where the recent patient data breach occurred. They're not just looking at current vulnerabilities, they're envisioning a future where patient records are stored on an immutable blockchain, where access requires cryptographic authorization from multiple medical professionals, where even the most sophisticated hackers find only impenetrable walls.</p><p>At the Ministry of Education, we see the potential for student records that can never be falsified, academic credentials that are instantly verifiable anywhere in the world, and research data that remains secure even as it's shared with international partners.</p><p>The Ministry of Finance presents the most compelling case for blockchain implementation. Picture budget allocations recorded on an immutable ledger, government expenditures tracked with transparent smart contracts, and financial reporting that citizens can verify themselves. This isn't just cybersecurity—it's the foundation of unprecedented government transparency.</p><p><strong>Months Nine Through Twelve - The Network Effect</strong></p><p>The real magic happens when individual ministry nodes start connecting to form Tonga's unified government blockchain network. Picture dedicated blockchain nodes being installed in each ministry building, each one a fortress in its own right, but collectively forming an impregnable digital kingdom.</p><p>These aren't isolated systems, they're nodes in a living network that grows stronger with each addition. When the Health Ministry node communicates with the Finance Ministry node to process a medical equipment purchase, the transaction is validated by multiple nodes across the government network. When the Education Ministry needs to verify a student's eligibility for government assistance, the request flows through encrypted channels that make eavesdropping impossible.</p><p><strong>The Tonga Cable Ltd Partnership - A Strategic Alliance</strong></p><p>As our network grows, a crucial partnership emerges with Tonga Cable Ltd. Picture TCL not just as an internet service provider, but as the strategic gatekeeper of Tonga's digital sovereignty. Through the dedicated infrastructure of TongaIX, Tonga's own internet exchange point (IXP), government data never has to leave Tongan soil to travel between ministries.</p><p>This means that when a doctor at Vaiola Hospital accesses a patient's records, that data travels through Tongan infrastructure, controlled by Tongan engineers, secured by Tongan cryptography. No foreign servers, no vulnerable international links, no opportunity for external interference.</p><p><em>\"True sovereignty is not given, it is built, byte by byte, block by block\"</em></p><p><strong>Months Thirteen Through Eighteen - The Great Migration</strong></p><p>Picture government offices across Tonga undergoing a quiet but profound transformation. The familiar Windows interfaces that have dominated government desktops for decades are being replaced by sleek, secure Linux distributions designed specifically for government use. Microsoft Office gives way to LibreOffice, but this is more than just a software change, it's a declaration of digital independence.</p><p>Government workers receive training not just on new software, but on a new philosophy of digital security. They learn to think like cybersecurity professionals, to recognize threats before they materialize, to operate in an environment where every click is part of a larger security ecosystem.</p><p>This is where the PasifikaOS vision comes alive, not just for Tonga, but as a model for Pacific digital independence that other island nations will soon seek to emulate.</p><p><strong>Months Sixteen Through Twenty - The Backup Revolution</strong></p><p>While Tonga has historically been at the mercy of submarine cable failures, our DePIN solution includes a revolutionary backup strategy. Picture Starlink ground stations being installed at each ministry location, not as primary connections, but as instantly available backup links that activate automatically when traditional connectivity fails.</p><p>The next time a ship's anchor damages Tonga's submarine cable (and it will happen), government operations continue without interruption. Critical communications flow through Low Earth Orbit satellites, blockchain transactions continue processing, and essential services remain available to citizens.</p><p><strong>Months Eighteen Through Twenty Four - The Digital Government</strong></p><p>In the final phase of our journey, Tonga doesn't just have secure government systems, it has a completely reimagined digital government that serves as a model for the world. Picture citizens receiving digital identity credentials stored on the blockchain, making identity theft virtually impossible. Elections conducted through cryptographically secure voting systems that provide both complete transparency and absolute privacy.</p><p>Government procurement happens through smart contracts that automatically release payments when delivery conditions are met, eliminating corruption and ensuring efficient use of public funds. Land registry records become immutable blockchain entries that resolve property disputes instantly and prevent fraudulent land claims.</p><p>Budget tracking becomes a real-time, public dashboard where every citizen can see exactly how their tax money is being spent, with smart contracts automatically flagging unusual expenditures for review.</p><p><em>\"In the calm after the storm, we realize that we built not just a network, but a legacy\"</em></p><p>Picture this scene twelve months from today i.e. Across the Pacific, another island nation experiences a massive cyberattack that brings down government services for weeks. Citizens can't access health records, schools can't verify student credentials, and government operations grind to a halt.</p><p>But in Tonga, government workers arrive at their offices that same morning to find their systems operating normally. The blockchain network continues processing transactions, the solar powered infrastructure hums along independently, and the automated monitoring systems haven't detected so much as a suspicious packet.</p><p>More importantly, when news of the attack reaches Tonga, government officials don't respond with fear, they respond with confidence. They know their systems are built on the same cryptographic foundations that have protected Bitcoin for sixteen years without a single successful hack. They know their data is distributed across multiple nodes, their communications are end-to-end encrypted, and their backup systems can operate independently of any external infrastructure.</p><p>This isn't just the end of our implementation story, it's the beginning of Tonga's emergence as a digital sovereignty leader in the Pacific. Other nations start sending delegations to study Tonga's DePIN implementation. International cybersecurity experts cite Tonga's government network as a model for resilient infrastructure design. And most importantly, Tongan citizens begin to trust their government's digital services in ways they never could before.</p><p><strong>\"Ko e 'Otua mo Tonga ko hotau tofi'a\" - God and Tonga are our inheritance</strong></p><p>Our journey from vulnerability to invincibility isn't just a technical transformation, it's a story of national pride, digital sovereignty, and the courage to build something completely new rather than patch something fundamentally broken. In twenty four months, we don't just solve Tonga's cybersecurity problems, we eliminate them entirely, creating a digital infrastructure that becomes Tonga's gift to the Pacific and the world.</p><h2>Technical Architecture: Layer-by-Layer Security</h2><h3>Layer 1: Physical Infrastructure</h3><ul><li><strong>Solar-powered mini-grids:</strong> Energy independence eliminates grid vulnerability</li><li> No single point of failure</li><li> Military-grade encryption at the hardware level</li></ul><h3>Layer 2: Network Security</h3><ul><li> All communication encrypted end-to-end</li><li> Efficient, secure packet routing</li><li> Government traffic stays within Tonga</li><li> Independent satellite connectivity</li></ul><h3>Layer 3: Blockchain Consensus</h3><ul><li> Government controlled validator nodes</li><li><strong>Multi-signature authorization:</strong> Multiple approvals required for critical actions</li><li> All transactions permanently recorded and verifiable</li><li> Automated execution of government processes</li></ul><h3>Layer 4: Application Security</h3><ul><li> Every user and device must be verified</li><li> Permissions based on job function and clearance level</li><li> 24/7 surveillance of all network activity</li><li><strong>Automated incident response:</strong> Immediate isolation of compromised systems</li></ul><h2>Addressing Current Vulnerabilities</h2><h3>Problem 1: Centralized Data Storage</h3><p> Single point of failure makes entire systems vulnerable  Distributed storage across multiple blockchain nodes ensures no single point of failure</p><h3>Problem 2: Foreign Infrastructure Dependency</h3><p> Reliance on foreign systems creates sovereignty and security risks  100% Tongan-controlled infrastructure with local decision-making</p><h3>Problem 3: Proprietary Software Vulnerabilities</h3><p> Closed-source software contains unknown security flaws  Open-source transparency allows for continuous security auditing</p><h3>Problem 4: Inadequate Backup Systems</h3><p> System failures can cause extended outages  Multiple redundancy layers including solar power and satellite backup</p><h3>Problem 5: Insufficient Cyber Threat Response</h3><p> Slow response to security incidents  Real-time monitoring with automated threat response and isolation</p><h2>Economic Benefits: Security That Pays for Itself</h2><ul><li><strong>Licensing fee elimination:</strong> Save approximately TOP 150,000 annually</li><li> Solar power saves estimated TOP 35,000 per year</li><li> 99.9% uptime eliminates productivity losses</li></ul><h3>Long-term Economic Impact</h3><ul><li> Full system cost recovery within 3-4 years</li><li> Compared to traditional cybersecurity solutions</li><li> Technical expertise remains in Tonga</li></ul><ul><li> Avoid costs of incident response and reputation damage</li><li> Meet international data protection standards</li><li><strong>Insurance premium reduction:</strong> Demonstrable security measures reduce coverage costs</li></ul><h2>Implementation Roadmap: Making It Happen</h2><ol><li><strong>Conduct comprehensive security audit</strong> of current government systems</li><li><strong>Assemble technical implementation team</strong> with blockchain and cybersecurity expertise</li><li> for MEIDECC proof of concept</li><li><strong>Begin stakeholder engagement</strong> across all government ministries</li></ol><h3>Short-term Goals (3-6 Months)</h3><ol><li><strong>Deploy MEIDECC pilot program</strong> with full blockchain infrastructure</li><li><strong>Demonstrate measurable security improvements</strong> through testing and monitoring</li><li><strong>Train core technical staff</strong> on DePIN management and maintenance</li><li> for other government ministries</li></ol><h3>Medium-term Objectives (6-18 Months)</h3><ol><li><strong>Roll out to all major ministries</strong> with full blockchain node deployment</li><li><strong>Implement Tonga Cable Ltd integration</strong> for complete network control</li><li><strong>Deploy solar power infrastructure</strong> for energy independence</li><li><strong>Begin open-source migration</strong> away from vulnerable proprietary systems</li></ol><h3>Long-term Vision (18-24 Months)</h3><ol><li><strong>Achieve complete digital sovereignty</strong> with 100% Tongan-controlled infrastructure</li><li><strong>Deploy advanced blockchain services</strong> for citizens and businesses</li><li><strong>Establish Tonga as a model</strong> for Pacific Island digital independence</li><li><strong>Create exportable solution</strong> for other developing nations</li></ol><h2>Why This Solution Will Succeed</h2><h3>Proven Technology Foundation</h3><p>Built on Bitcoin's 16 year track record of unbreachable security, the DePIN solution leverages battle tested blockchain technology that has never been successfully compromised.</p><h3>Tailored for Tonga's Needs</h3><p>Unlike generic cybersecurity solutions, this system is specifically designed for Tonga's unique challenges, including natural disaster resilience, energy independence, and sovereignty requirements.</p><p>The modular design allows for gradual implementation and expansion, while the economic benefits ensure long-term sustainability without ongoing foreign dependency.</p><h3>Future Proof Architecture</h3><p>Web3 compatibility ensures that Tonga's infrastructure will remain cutting edge as digital technologies continue to evolve.</p><h2>Call to Action: Securing Tonga's Digital Future</h2><p>The recent cybersecurity breaches are not just isolated incidents, they are warnings of what happens when a nation's digital infrastructure is inadequately protected. The cost of inaction is measured not just in compromised data, but in lost sovereignty, economic damage, and risks to vulnerable populations.</p><p><strong>The DePIN solution offers Tonga a unique opportunity to leapfrog traditional cybersecurity approaches and implement a truly revolutionary system that provides:</strong></p><ul><li> that has never been breached</li><li><strong>Complete digital sovereignty</strong> with 100% local control</li><li> through cost savings and local capacity building</li><li> through multiple redundancy layers</li><li> with emerging Web3 technologies</li></ul><ol><li><strong>Government Leadership Commitment:</strong> Secure high-level government buy-in for the DePIN transformation</li><li> Pursue both local and donor funding options for the initial implementation</li><li> Recruit and train the technical expertise needed for deployment</li><li> Build public support for digital sovereignty initiatives</li></ol><p><strong>\"Ko e 'Otua mo Tonga ko hotau tofi'a\" - God and Tonga are our inheritance</strong></p><p>The time has come for Tonga to claim its digital inheritance, a secure, sovereign, and sustainable technological infrastructure that protects its people and preserves its independence for generations to come.</p><p>The full technical specifications, architecture diagrams, and detailed budget breakdowns are available in our comprehensive pitch deck presentation. This interactive presentation includes:</p><ul><li>Complete network architecture visualization</li><li>Department-by-department blockchain node configuration</li><li>Solar power infrastructure layouts</li><li>Business continuity and disaster recovery planning</li><li>Detailed budget analysis with cost comparisons</li><li>Timeline and implementation roadmaps</li></ul>","contentLength":20347,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Iranian Blackout Affected Misinformation Campaigns","url":"https://www.schneier.com/blog/archives/2025/07/iranian-blackout-affected-misinformation-campaigns.html","date":1751368071,"author":"Bruce Schneier","guid":178866,"unread":true,"content":"<p>Dozens of accounts on X that promoted Scottish independence <a href=\"https://www.scottishdailyexpress.co.uk/news/politics/iranian-pro-scottish-independence-accounts-35450209\">went dark</a> during an internet blackout in Iran.</p><p>Well, that’s one way to identify fake accounts and misinformation campaigns.</p>","contentLength":184,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mesa 25.2 Should Have Initial Vulkan Support In Good Shape For NVIDIA Blackwell","url":"https://www.phoronix.com/news/Mesa-25.2-NVK-Blackwell","date":1751366651,"author":"Michael Larabel","guid":178843,"unread":true,"content":"<article>The Mesa 25.2 release that will likely be out as stable in August should have nice initial support for the newest NVIDIA Blackwell GPUs, namely used by the GeForce RTX 50 series, with the NVK open-source driver for Vulkan usage...</article>","contentLength":230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scientists identify culprit behind biggest-ever U.S. honey bee die-off","url":"https://www.science.org/content/article/scientists-identify-culprit-behind-biggest-ever-u-s-honeybee-die","date":1751366123,"author":"pseudolus","guid":179010,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432467"},{"title":"NVIDIA Confirms 580 Linux Driver Is The Last For Maxwell / Pascal / Volta","url":"https://www.phoronix.com/news/NVIDIA-580-Linux-Driver-Last-HW","date":1751365920,"author":"Michael Larabel","guid":178842,"unread":true,"content":"<article>NVIDIA previously warned CUDA users that CUDA 12.x is the last for Maxwell, Pascal, and Volta GPUs. NVIDIA overnight now officially confirmed that the Maxwell / Pascal / Volta GPU support is going to end in their Linux driver with the upcoming NVIDIA R580 Linux driver series...</article>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cloudflare to introduce pay-per-crawl for AI bots","url":"https://blog.cloudflare.com/introducing-pay-per-crawl/","date":1751365227,"author":"scotchmi_st","guid":178870,"unread":true,"content":"<div><h2>A changing landscape of consumption&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#a-changing-landscape-of-consumption\" aria-hidden=\"true\"></a></div><p>Many publishers, content creators and website owners currently feel like they have a binary choice — either leave the front door wide open for AI to consume everything they create, or create their own walled garden. But what if there was another way?</p><p>At Cloudflare, we started from a simple principle: we wanted content creators to have control over who accesses their work. If a creator wants to block all AI crawlers from their content, they should be able to do so. If a creator wants to allow some or all AI crawlers full access to their content for free, they should be able to do that, too. Creators should be in the driver’s seat.</p><p>After hundreds of conversations with news organizations, publishers, and large-scale social media platforms, we heard a consistent desire for a third path: They’d like to allow AI crawlers to access their content, but they’d like to get compensated. Currently, that requires knowing the right individual and striking a one-off deal, which is an insurmountable challenge if you don’t have scale and leverage.&nbsp;</p><div><h2>What if I could charge a crawler?&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#what-if-i-could-charge-a-crawler\" aria-hidden=\"true\"></a></div><p>We believe your choice need not be binary — there should be a third, more nuanced option: <b>You can charge for access.</b> Instead of a blanket block or uncompensated open access, we want to empower content owners to monetize their content at Internet scale.</p><div><h2>Introducing pay per crawl</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#introducing-pay-per-crawl\" aria-hidden=\"true\"></a></div><p><a href=\"https://www.cloudflare.com/paypercrawl-signup/\">Pay per crawl</a>, in private beta, is our first experiment in this area.&nbsp;</p><p>Pay per crawl integrates with existing web infrastructure, leveraging HTTP status codes and established authentication mechanisms to create a framework for paid content access.&nbsp;</p><p>Each time an AI crawler requests content, they either present payment intent via request headers for successful access (<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/200\"></a>), or receive a  response with pricing. Cloudflare acts as the Merchant of Record for pay per crawl and also provides the underlying technical infrastructure.</p><div><h3>Publisher controls and pricing</h3><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#publisher-controls-and-pricing\" aria-hidden=\"true\"></a></div><p>Pay per crawl grants domain owners full control over their monetization strategy. They can define a flat, per-request price across their entire site. Publishers will then have three distinct options for a crawler:</p><ul><li><p> Grant the crawler free access to content.</p></li><li><p> Require payment at the configured, domain-wide price.</p></li><li><p> Deny access entirely, with no option to pay.</p></li></ul><p>An important mechanism here is that even if a crawler doesn’t have a billing relationship with Cloudflare, and thus couldn’t be charged for access, a publisher can still choose to ‘charge’ them. This is the functional equivalent of a network level block (an HTTP  response where no content is returned) — but with the added benefit of telling the crawler there could be a relationship in the future.&nbsp;</p><p>While publishers currently can define a flat price across their entire site, they retain the flexibility to bypass charges for specific crawlers as needed. This is particularly helpful if you want to allow a certain crawler through for free, or if you want to negotiate and execute a content partnership outside the pay per crawl feature.&nbsp;</p><p>To ensure integration with each publisher’s existing security posture, Cloudflare enforces Allow or Charge decisions via a rules engine that operates only after existing WAF policies and bot management or bot blocking features have been applied.</p><div><h3>Payment headers and access</h3><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#payment-headers-and-access\" aria-hidden=\"true\"></a></div><p>As we were building the system, we knew we had to solve an incredibly important technical challenge: ensuring we could charge a specific crawler, but prevent anyone from spoofing that crawler. Thankfully, there’s a way to do this using <a href=\"https://developers.cloudflare.com/bots/concepts/bot/verified-bots/web-bot-auth/\"></a> proposals.</p><ul><li><p>Generating an Ed25519 key pair, and making the <a href=\"https://datatracker.ietf.org/doc/html/rfc7517\"></a>-formatted public key available in a hosted directory</p></li><li><p>Registering with Cloudflare to provide the URL of your key directory and user agent information.</p></li></ul><p>Once registration is accepted, crawler requests should always include , , and  headers to identify your crawler and discover paid resources.</p><pre><code>GET /example.html\nSignature-Agent: \"https://signature-agent.example.com\"\nSignature-Input: sig2=(\"@authority\" \"signature-agent\")\n ;created=1735689600\n ;keyid=\"poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U\"\n ;alg=\"ed25519\"\n ;expires=1735693200\n;nonce=\"e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg==\"\n ;tag=\"web-bot-auth\"\nSignature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:</code></pre><p>Once a crawler is set up, determination of whether content requires payment can happen via two flows:</p><h4>Reactive (discovery-first)</h4><p>Should a crawler request a paid URL, Cloudflare returns an <code>HTTP 402 Payment Required</code> response, accompanied by a  header. This signals that payment is required for the requested resource.</p><pre><code>HTTP 402 Payment Required\ncrawler-price: USD XX.XX</code></pre><p>&nbsp;The crawler can then decide to retry the request, this time including a  header to indicate agreement to pay the configured price.</p><pre><code>GET /example.html\ncrawler-exact-price: USD XX.XX </code></pre><p>Alternatively, a crawler can preemptively include a  header in its initial request.</p><pre><code>GET /example.html\ncrawler-max-price: USD XX.XX</code></pre><p>If the price configured for a resource is equal to or below this specified limit, the request proceeds, and the content is served with a successful  response, confirming the charge:</p><pre><code>HTTP 200 OK\ncrawler-charged: USD XX.XX \nserver: cloudflare</code></pre><p>If the amount in a  request is greater than the content owner’s configured price, only the configured price is charged. However, if the resource’s configured price exceeds the maximum price offered by the crawler, an  response is returned, indicating the specified cost. &nbsp;Only a single price declaration header,  or , may be used per request.</p><p>The  or  headers explicitly declare the crawler's willingness to pay. If all checks pass, the content is served, and the crawl event is logged. If any aspect of the request is invalid, the edge returns an <code>HTTP 402 Payment Required</code> response.</p><p>Crawler operators and content owners must configure pay per crawl payment details in their Cloudflare account. Billing events are recorded each time a crawler makes an authenticated request with payment intent and receives an HTTP 200-level response with a  header. Cloudflare then aggregates all the events, charges the crawler, and distributes the earnings to the publisher.</p><div><h2>Content for crawlers today, agents tomorrow&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#content-for-crawlers-today-agents-tomorrow\" aria-hidden=\"true\"></a></div><p>At its core, pay per crawl begins a technical shift in how content is controlled online. By providing creators with a robust, programmatic mechanism for valuing and controlling their digital assets, we empower them to continue creating the rich, diverse content that makes the Internet invaluable.&nbsp;</p><p>We expect pay per crawl to evolve significantly. It’s very early: we believe many different types of interactions and marketplaces can and should develop simultaneously. We are excited to support these various efforts and open standards.</p><p>For example, a publisher or new organization might want to charge different rates for different paths or content types. How do you introduce dynamic pricing based not only upon demand, but also how many users your AI application has? How do you introduce granular licenses at internet scale, whether for training, inference, search, or something entirely new?</p><p>The true potential of pay per crawl may emerge in an agentic world. What if an agentic paywall could operate entirely programmatically? Imagine asking your favorite deep research program to help you synthesize the latest cancer research or a legal brief, or just help you find the best restaurant in Soho — and then giving that agent a budget to spend to acquire the best and most relevant content. By anchoring our first solution on , we enable a future where intelligent agents can programmatically negotiate access to digital resources.&nbsp;</p><p>Pay per crawl is currently in private beta. We’d love to hear from you if you’re either a crawler interested in paying to access content or a content creator interested in charging for access. You can reach out to us at <a href=\"https://www.cloudflare.com/paypercrawl-signup/\"><u>http://www.cloudflare.com/paypercrawl-signup/</u></a> or contact your Account Executive if you’re an existing Enterprise customer.</p>","contentLength":8099,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432385"},{"title":"Framework 12, AMD Strix Halo & Linux Kernel Improvements Were Most Popular In June","url":"https://www.phoronix.com/news/June-2025-Highlights","date":1751365066,"author":"Michael Larabel","guid":178841,"unread":true,"content":"<article>Over the course of last month on Phoronix were 240 original news articles written by your's truly as well as another 24 Linux hardware reviews / multi-page featured benchmark articles. On top of that last month also marked the 21st birthday of Phoronix.com...</article>","contentLength":259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: ToplingDB - A Persistent Key-Value Store for External Storage","url":"https://github.com/topling/toplingdb","date":1751364479,"author":"rockeetterark","guid":179511,"unread":true,"content":"<p>As the creator of TerarkDB (acquired by ByteDance in 2019), I have developed ToplingDB in recent years.</p><p>ToplingDB is forked from RocksDB,   where   we have replaced almost all components with more efficient alternatives(db_bench shows ToplingDB is about ~8x faster than RocksDB):</p><p>* MemTable: SkipList is replaced by CSPP(Crash Safe Parallel Patricia trie), which is 8x faster.</p><p>* SST: BlockBasedTable is replaced by ToplingZipTable, implemented by searchable compression algo, it is very small and fast, typically less than 1μs per lookup:</p><pre><code>  * Keys/Indexes are compressed   using NestLoudsTrie(a multi-layer nesting LOUDS succinct trie).\n\n  * Values in a SST are compressed   together with better zip ratio than zstd, and can unzip by a single value at 1GB/sec.\n\n  * BlockCache is no longer needed, double caching(BlockCache &amp; PageCache) is avoided\n</code></pre>\nOther hotspots are also improved:<p>* Flush MemTable to L0 is omited, greatly reducing write amp and is very friendly for large(GB) MemTable</p><pre><code>  * MemTable   serves as the index of Key to \"value position in WAL log\"\n\n  * Since WAL file content almost always in page cache, thus value content can be efficiently accessed by mmap\n\n  * When Flush happens, MemTable is dumpped as an SST and WAL is treated as a blob file\n\n    * CSPP MemTable use integer index instead of physical pointers, thus in-memory format is exactly same with in-file format\n</code></pre>\n* Prefix cache for searching candidate SSTs and prefix cache for scanning by iterators<pre><code>  * Caching fixed len key prefix into an array, binary search it as an uint array\n</code></pre>\n* Distributed compaction(superior replacement to rocksdb remote compaction)<pre><code>  * Gracefully support MergeOperator, CompactionFilter, PropertiesCollector...\n\n  * Out of the box, development efforts are significantly reduced\n\n  * Very easy to share compaction service on spot instances for many DB nodes\n</code></pre>\nUseful Bonus Feature:<p>* Config by json/yaml: can config almost all features</p><p>* Optional embeded WebView: show db structures in web browser, refreshing pages like animation</p><p>* Online update db configs by http</p><p>MySQL integration, ToplingDB has integrated into MySQL by MyTopling, which is forked from MyRocks with great improvements, like improvements of ToplingDB on RocksDB:</p><p>* WBWI(WriteBatchWithIndex): like MemTable, SkipList is replace with CSPP, 20x faster(speedup is more than MemTable).</p><p>* LockManager &amp; LockTracker: 10x faster</p><p>* Encoding &amp; Decoding: 5x faster</p><p>MyRocks has many disadvantages compared to InnoDB, while MyTopling outperforms InnoDB at almost all aspect - excluding feature differences.</p><p>We have create ~100 PRs for RocksDB, in which ~40 were accepted. Our PRs are mostly \"small\" changes, since big changes are not likely accepted.</p><p>ToplingDB has been deployed in numerous production environments.</p>","contentLength":2756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432322"}],"tags":[]}