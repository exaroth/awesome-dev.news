{"id":"2TQQJ2bU7mR6LpUoekpTCZLWVm2v","title":"DEV Community","displayTitle":"Dev.to","url":"https://dev.to/feed/","feedLink":"https://dev.to/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":188,"items":[{"title":"Study Shows Spatial Label Noise Severely Impacts AI Object Detection Performance","url":"https://dev.to/mikeyoung44/study-shows-spatial-label-noise-severely-impacts-ai-object-detection-performance-3h4i","date":1739782176,"author":"Mike Young","guid":1704,"unread":true,"content":"<ul><li>This paper investigates the impact of label noise, specifically spatial noise, on instance segmentation models.</li><li>Instance segmentation is a computer vision task that involves identifying and delineating individual objects within an image.</li><li>The researchers examine how different types and levels of label noise affect the performance of instance segmentation models.</li><li>They propose a new benchmark dataset and evaluation protocol to standardize the study of label noise in instance segmentation.</li></ul><h2>\n  \n  \n  Plain English Explanation\n</h2>","contentLength":522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"You Don't Know Java\" 💘 The Philosophy of Unit Test Naming","url":"https://dev.to/chuck1sn/you-dont-know-java-the-philosophy-of-unit-test-naming-1j2c","date":1739781803,"author":"Chuck1sn","guid":1703,"unread":true,"content":"<p>Programs are written primarily for humans to read, and only incidentally for machines to execute.</p><blockquote><p>Programs must be written for people to read, and only incidentally for machines to execute.</p></blockquote><p>The first step in writing code is naming, and the same applies to unit tests.</p><p>How can you make the names of your unit tests understandable? You need to reflect three elements in the naming, abbreviated as the WWW principle.</p><ul><li>What are you testing? (what)</li><li>Under what conditions are you testing? (when)</li><li>What behavior do you expect? (want)</li></ul><div><pre><code></code></pre></div><p>Breaking down the name into three parts:</p><ul><li><code>createVerifyGetSubjectJwt</code> reflects the object being tested.</li><li> indicates the condition under which the test is conducted.</li><li><code>shouldReturnTrueAndGetExpectIdentify</code> describes the expected behavior.</li></ul><p>This is a good name—one that a business person can understand.</p><p>The first hurdle is over; now let's implement the test logic. In fact, with the WWW naming design, the logic implementation already has a clear path. Like naming, test logic also has principles to follow, commonly known as the AAA principle.</p><p>Let's continue with a <a href=\"https://github.com/ccmjga/mjga-scaffold/blob/main/src/test/java/com/zl/mjga/integration/mvc/SignMvcTest.java\" rel=\"noopener noreferrer\">test case</a> from the MJGA scaffolding:</p><div><pre><code></code></pre></div><p>This is the preparation phase of the test. In this phase, you need to construct some code that provides context, such as inserting data, building objects, or even more complex mocks and stubs. In the example, we constructed the SignInDto object for subsequent use and assumed the return value of the signIn method—all in preparation for the subsequent test.</p><p>This is the execution phase, corresponding to  in the example. The API is just a bit complex here, so you can think of it as a single line of code.</p><p>You need to clearly state what result you expect. In the example, this corresponds to <code>.andExpect(status().isOk());</code>.</p><p>Often, you may need to write some test methods to verify whether a certain logic can run without errors, as the method itself has no return value. This is easy to solve using .</p><div><pre><code></code></pre></div><p>These coding philosophies are independent of the language and framework you use. Any unit test is written with this mindset. More content on unit testing will be shared in the future.</p><ul><li>I am Chuck1sn, a developer long committed to promoting the modern JVM ecosystem.</li><li>Your replies, likes, and bookmarks are the motivation for my continuous updates.</li><li>A simple triple action (like, comment, share) means a lot to me and is greatly appreciated!</li><li>Follow my account to receive article updates as soon as they are published.</li></ul><p>PS: All the code examples above can be found in the <a href=\"https://github.com/ccmjga/mjga-scaffold/blob/main/README_EN.md\" rel=\"noopener noreferrer\">Github repository</a>. If it helps, please give it a Star—it's a great encouragement to me. Thank you!</p>","contentLength":2566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Define and Use Custom Breakpoints in Tailwind CSS","url":"https://dev.to/rowsanali/how-to-define-and-use-custom-breakpoints-in-tailwind-css-3m5g","date":1739781369,"author":"Rowsan Ali","guid":1702,"unread":true,"content":"<p>Tailwind CSS is a utility-first CSS framework that provides a powerful way to build responsive designs. One of its standout features is the ability to define breakpoints, which allow you to apply styles conditionally based on the screen size. While Tailwind comes with a set of default breakpoints, you might find yourself needing custom breakpoints to better suit your project's requirements. In this blog post, we'll explore how to define and use custom breakpoints in Tailwind CSS.</p><h2>\n  \n  \n  Understanding Tailwind's Default Breakpoints\n</h2><p>Before diving into custom breakpoints, let's take a quick look at Tailwind's default breakpoints:</p><ul></ul><p>These breakpoints are used to apply responsive styles. For example, you can apply a style that only takes effect on medium screens and above using the  prefix:</p><div><pre><code>\n  This div is red on small screens and blue on medium screens and above.\n</code></pre></div><h2>\n  \n  \n  Defining Custom Breakpoints\n</h2><p>Tailwind allows you to define custom breakpoints by modifying the  configuration in your  file. This is particularly useful if your design requires breakpoints that differ from the default ones.</p><h3>\n  \n  \n  Step 1: Modify </h3><p>To define custom breakpoints, you need to edit the  file. If you don't have this file yet, you can create it by running:</p><p>Once you have the file, you can define your custom breakpoints like this:</p><div><pre><code></code></pre></div><p>In this example, we've added an  breakpoint for extra small devices and a  breakpoint for very large screens.</p><h3>\n  \n  \n  Step 2: Using Custom Breakpoints in Your HTML\n</h3><p>Once you've defined your custom breakpoints, you can use them just like the default ones. For example:</p><div><pre><code>\n  This div changes color based on the screen size.\n</code></pre></div><p>In this example, the background color of the  will change depending on the screen size, using the custom breakpoints we defined.</p><h3>\n  \n  \n  Step 3: Extending Default Breakpoints\n</h3><p>If you only need to add a few custom breakpoints without replacing the default ones, you can extend the  object instead of redefining it:</p><div><pre><code></code></pre></div><p>This way, you keep the default breakpoints and only add the new ones you need.</p><h2>\n  \n  \n  Practical Example: Responsive Typography\n</h2><p>Let's look at a practical example where custom breakpoints can be useful. Suppose you want to create a responsive typography system where the font size changes based on the screen size.</p><div><pre><code>\n  Responsive Typography\n</code></pre></div><p>In this example, the font size of the  element will increase as the screen size grows, using both default and custom breakpoints.</p><p>Custom breakpoints in Tailwind CSS provide a flexible way to create responsive designs that align perfectly with your project's requirements. By modifying the  file, you can define breakpoints that cater to specific screen sizes, ensuring your design looks great on all devices.</p><p>Whether you're adding a new breakpoint for a unique device size or extending the default breakpoints, Tailwind's configuration system makes it easy to tailor the framework to your needs. With custom breakpoints, you can take full control of your responsive design and create a seamless user experience across all screen sizes.</p>","contentLength":3017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"React.js vs React Native: Differences, Benefits, Features & More","url":"https://dev.to/trretatechnolabs/reactjs-vs-react-native-differences-benefits-features-more-4p50","date":1739780779,"author":"Rachel Ritchay","guid":1701,"unread":true,"content":"<p>When it comes to building scalable and high-performing web and mobile applications, <strong>React.js and React Native</strong> are two of the most popular frameworks people turn to.</p><p>Choosing between these two options is super important for those responsible for making final decisions, as it will significantly impact the performance, user experience, and overall development cost of your service or product.</p><p>Whether you want to create an interactive web platform or a high-performing mobile app, understanding the key differences between React.js and React Native will help you make an informed decision for your project.</p><p>This guide reveals all about their unique features, benefits, and how they compare to each other. So, without further ado, let’s learn more about both technologies.</p><p>React.js, often just called React, is an open-source JavaScript library created by Meta (formerly known as Facebook) for building user interfaces, primarily for web applications. This technology is the second most used framework in the community, with <a href=\"https://www.statista.com/statistics/1124699/worldwide-developer-survey-most-used-frameworks-web/\" rel=\"noopener noreferrer\">39.5%</a> of users worldwide (recorded as of 2024).</p><h3><strong>Key features of React.js:</strong></h3><ul><li> Its virtual DOM optimizes rendering, ensuring faster updates.</li><li><strong>Component-Based Architecture:</strong> This allows you to develop reusable UI components, thus making it easier to manage and scale applications.</li><li> Its declarative approach simplifies the process of creating dynamic UIs by describing how the UI should look at any given state.</li><li> Allows you to write HTML-like code in JavaScript, making it easier to define UI elements directly within JavaScript code.</li><li><strong>Unidirectional Data Flow:</strong> Ensures data flows in a single direction, improving code maintainability and debugging.</li><li> Extensive ecosystem of tools, libraries, and community resources to enhance functionality.</li></ul><p>Developed by Meta, React Native is a framework based on React.js principles. It is specifically designed for building cross-platform mobile apps - iOS and Android – and provides a closer-to-native app experience.</p><h2><strong>Key Features of React Native:</strong></h2><ul><li><strong>Cross-Platform Development:</strong> Write code once and deploy it on both iOS and Android, thus reducing development time and costs.</li><li> Renders using actual native components, providing performance close to native apps.</li><li> Allows you to instantly see changes without restarting the app, thus improving productivity.</li><li> By utilizing JavaScript, it empowers your development teams to efficiently build and maintain both iOS and Android apps.</li><li> Benefit from a large number of libraries and community contributions.</li><li><strong>Third-Party Plugin Support:</strong> Supports third-party plugins to extend functionality, including access to native device features like the camera and GPS.</li></ul><h2><strong>React.js vs React Native: A Detailed Comparison</strong></h2><p>To help you decide which technology is right for your project, refer to this detailed comparison of React.js and React Native across key areas:</p><ul><li>It focuses on building dynamic web applications that run in the browser.</li><li>React.js uses JavaScript, HTML, and CSS to create interactive UIs.</li><li>This technology is ideal for developing single-page applications (SPAs) or complex web platforms.</li><li>According to <a href=\"https://tsh.io/state-of-frontend/#frameworks\" rel=\"noopener noreferrer\">State of Frontend’s survey report 2024</a>, 69.9% of developers used React.js to develop modern web applications.This makes it a key technology for business owners and founders to consider when planning their digital products.</li><li>Fast iteration and updates with reusable components help speed up development and improve scalability.</li></ul><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7pkd94ll7dnii3dgjzo6.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7pkd94ll7dnii3dgjzo6.png\" alt=\"Image description\" width=\"800\" height=\"483\"></a>\nOverview of React.js Framework - <a href=\"https://www.researchgate.net/figure/Overview-of-the-REACT-framework_fig1_348212386\" rel=\"noopener noreferrer\">Source</a></p><ul><li>Designed for <a href=\"https://trreta.com/mobile-development\" rel=\"noopener noreferrer\">mobile app development</a>, React Native allows your development team to write code that works on both iOS and Android.</li><li>It uses JavaScript and JSX to create native apps, rendering them into native codes for optimal performance.</li><li>It’s a cross-platform solution, meaning you can write code once and deploy it anywhere, thus saving your development time and resources.</li></ul><ul><li>It uses Virtual DOM to efficiently update only the necessary parts of your website. This means your app can handle complex interactions and data changes without slowing down.</li><li>React's speed and efficiency make it ideal for crafting websites that constantly change and adapt, such as e-commerce stores with real-time inventory updates or social media platforms with live feeds.</li><li>If you are building complex web applications like dashboards or social media platforms, choose React.js for delivering fast, smooth experiences.</li></ul><p>Representation of how React.js’ Virtual DOM works - <a href=\"https://vengithiyagarajan.medium.com/%EF%B8%8F-visualization-virtual-dom-in-react-js-d151c8a05fa\" rel=\"noopener noreferrer\">Source</a></p><ul><li>As mentioned before, React Native provides near-native performance for mobile apps by utilizing native components (iOS or Android).</li><li>While this cross-platform framework is fast, it doesn’t fully match the speed of native apps built with Swift (iOS) or Kotlin (Android), especially for complex apps.</li><li>However, for most business apps, like e-commerce and social networks, it provides unmatched performance and a seamless user experience.</li></ul><ul><li>Thanks to its modular component system, you can create interactive, responsive web UIs that will work across browsers.</li><li>React.js works seamlessly with popular design libraries like Material-UI and Bootstrap. This means you can quickly add stylish and professional-looking components to your app, without starting things from scratch.React.js is best suited for web apps where flexibility and a fast user experience are crucial.</li><li>With React’s wide support for styling frameworks, businesses can create customizable and scalable designs without extra effort.</li></ul><ul><li>Its native mobile UI components ensure that your app looks and feels like a native app on iOS and Android.</li><li>It does provide some unique UI elements, but you may have to adjust designs according to different screen sizes and resolutions.</li><li>You get a consistent user experience across mobile devices. This makes it an excellent choice for apps that require seamless navigation and responsive design. </li></ul><h3><strong>4. Community and Ecosystem</strong></h3><ul><li>React's thriving community and ecosystem provide your team access to a wealth of third-party libraries and frameworks, which contributes to accelerated development time and ensures enhanced application capabilities.</li><li>Your development team can use a range of libraries for everything - from data handling to UI elements.</li><li>With regular updates, strong community contributions, and comprehensive documentation, you can rely on this technology for long-term support.</li></ul><ul><li>While React Native’s ecosystem isn’t as large as React.js, it still includes a broad range of mobile-specific libraries and tools.</li><li>More mobile-specific resources are being added continuously, which means you can easily build fast, high-performing mobile apps.</li></ul><h3><strong>5. Cost and Resource Management</strong></h3><ul><li>Since it’s used primarily for <a href=\"https://trreta.com/web-development\" rel=\"noopener noreferrer\">web development</a>, you will require less initial investment.</li><li>Your development team can quickly transition to React.js, thus reducing training and hiring costs.</li><li>React's component-based architecture and extensive library support streamline the development process, leading to faster time-to-market and lower maintenance costs. This makes it an economical solution for building web applications.</li></ul><ul><li>Building React Native apps requires developers with expertise in mobile platforms, which may lead to higher hiring costs compared to web developers.</li><li>However, its ability to share a single codebase across iOS and Android platforms can significantly reduce your development time and costs.</li><li>It also reduces the need for separate teams for each platform. If you want to cut down on mobile development costs, React Native is the clear winner.</li></ul><h2><strong>React.js vs React Native: The Comparison Table</strong></h2><p>React.js is ideal for business owners and CTOs who want to build scalable, high-performance web applications. If your product is primarily web-based and you want to take advantage of a large pool of developers, React.js is the perfect tech solution.</p><ul><li>You want to build a dynamic, responsive web application.</li><li>You are working on a project that focuses on user interactivity.</li><li>You are considering a single-platform solution (web-only).</li><li>Your app requires fast development and deployment with minimal overhead.</li></ul><h2><u><strong>When to Choose React Native?</strong></u></h2><p>For businesses aiming to create cross-platform mobile apps with near-native performance, React Native is the go-to framework. It allows you to reduce development time and costs by targeting both iOS and Android simultaneously while maintaining high performance and a great user experience.</p><ul><li>You need to build mobile apps for both iOS and Android.</li><li>You want to save time and resources by reusing code across platforms.</li><li>You aim to provide a near-native experience for your users.</li><li>You are focused on mobile-first development and need faster deployment.</li></ul><h2><strong>React.js vs React Native: Which is the Best Technology?</strong></h2><p>Choosing between React.js and React Native completely depends on what is your long-term business goal. </p><p>If you’re focused on web development, React.js is the best solution, with a rich ecosystem and stable performance. \nIf your business is expanding into mobile, React Native offers an efficient way to create high-quality mobile applications without doubling the cost of development for both iOS and Android.<p>\nIf you are contemplating choosing between both technologies, you can reach out to </p><a href=\"https://trreta.com/\" rel=\"noopener noreferrer\">Trreta</a>. We specialize in both <a href=\"https://trreta.com/reactjs-development\" rel=\"noopener noreferrer\">React.js</a> and React Native, helping businesses like yours pick the right framework and build scalable, high-performance solutions. </p><p>Whether you're looking for a web platform or a mobile app, our React experts will guide you through the development process. <a href=\"https://calendly.com/trreta/letstalk\" rel=\"noopener noreferrer\">Schedule a consultation</a> today to discuss how we can help you achieve your business goals with the latest in React technology.</p><h3><strong>1. What are the key differences between React.js and React Native?</strong></h3><p>While both technologies share the same core principles of component-based architecture and virtual DOM, their target platforms and development approaches differ significantly.</p><ul><li>React.js is used for building web applications, while React Native is used for mobile app development. </li><li>React.js works with HTML, CSS, and JavaScript, whereas React Native uses JavaScript to interact with native components.</li></ul><h2><strong>2. Can existing web applications benefit from migrating to React.js?</strong></h2><p>Absolutely! Migrating to React.js can significantly improve performance, enhance user interactivity, and streamline development, especially for complex, dynamic web applications. </p><h2>\n  \n  \n  3. Is it cost-effective to use React Native for small mobile apps?\n</h2><p>Yes, React Native is highly cost-effective for small apps, as it allows code reuse across platforms (iOS and Android), reducing development time and costs. However, for extremely simple apps, the initial setup cost and learning curve might outweigh the benefits. </p><h3>\n  \n  \n  4. How do updates and maintenance differ between React.js and React Native?\n</h3><ul><li>Updates typically involve library updates, which affect the core functionality.</li><li>Maintenance involves keeping up with the latest library versions and addressing potential security vulnerabilities.</li></ul><ul><li>Updates can involve both library and platform-specific changes (iOS, Android).</li><li>Maintenance requires keeping track of multiple platforms, ensuring compatibility, and addressing platform-specific issues.</li></ul>","contentLength":10999,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"preparing the usb-stick for file-transfer","url":"https://dev.to/linomanzz/preparing-the-usb-stick-for-file-transfer-3enm","date":1739780721,"author":"linoman-zz","guid":1700,"unread":true,"content":"<p>command-line: very funny - the copy process is without any (!) effect - zero results copied!?</p><p>i run this command - to copy data from my notebook to a usb-stick</p><p>`sudo rsync -av --progress /home/ubuntu/Dokumente /media/ubuntu/sdb/</p><p>see a part of the copied data</p><p>`Dokumente/new_literatur/theologie/petra_jeckel_schöpfungspoetik_/mirjamjekel_0408.jpg\n             0 100%    0,00kB/s    0:00:00 (xfr#51206, to-chk=2/59613)<p>\nDokumente/new_literatur/theologie/petra_jeckel_schöpfungspoetik_/mirjamjekel_0409.jpg</p>\n              0 100%    0,00kB/s    0:00:00 (xfr#51207, to-chk=1/59613)<p>\nDokumente/new_literatur/theologie/petra_jeckel_schöpfungspoetik_/mirjamjekel_0410.jpg</p>\n              0 100%    0,00kB/s    0:00:00 (xfr#51208, to-chk=0/59613)</p><p>sent 4.694.015.209 bytes  received 1.024.752 bytes  46.256.551,34 bytes/sec\ntotal size is 4.688.989.106  speedup is 1,00`</p><p>well believe me - i have no (!) results on the USB-Stick!!!!</p><p>i did some checks: see this Output here<code>\nubuntu@T420s:~$ ls -al /media/ubuntu/\ndrwxr-x---+ 6 root root 4096 Feb 16 18:18 .<p>\ndrwxr-xr-x  5 root root 4096 Feb  8 23:53 ..</p>\ndrwxr-xr-x  2 root root 4096 Feb 14 21:02 C449-8570<p>\ndrwxr-xr-x  6 root root 4096 Feb 15 11:59 C449-8571</p>\ndrwxr-xr-x  3 root root 4096 Feb 15 18:27 ed5dd6d0-c358-4a93-a4d4-67d3aa31fed1<p>\ndrwxr-xr-x  6 root root 4096 Feb 15 21:02 sdb</p>\nubuntu@T420s:~$</code></p><p>well this is interesting - how to proceed now - how to prepare the stick to get ready for the data saving process</p><p>well to be frank - i did a lot of errors - while i thought that this is enough</p><p>sudo rsync -av --progress /home/ubuntu/Dokumente /media/ubuntu/sdb/</p><p>well i guess that i have to create a partition on the drive?</p><p>and i will have to create a filesystem on the partition?</p><p>i look forward to har from you</p>","contentLength":1727,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Guide to LLM Training, Fine-Tuning, and RAG","url":"https://dev.to/scrapfly_dev/guide-to-llm-training-fine-tuning-and-rag-2jpi","date":1739780312,"author":"Scrapfly","guid":1699,"unread":true,"content":"<p>Large Language Models (LLMs) are revolutionizing how we interact with technology, powering everything from chatbots to content generation tools. But understanding how to make these models work for you requires knowing the difference between training, fine-tuning, and Retrieval-Augmented Generation (RAG).</p><p>This article will break down each concept, highlighting their strengths, weaknesses, and common use cases, providing a comprehensive guide to harnessing the power of LLMs.</p><p>LLM training is the process of teaching a large language model to understand and generate human language. This process involves feeding the model massive amounts of text data and using algorithms to adjust the model's parameters so that it can accurately predict the next word in a sequence.</p><h3>\n  \n  \n  Key Components of LLM Training\n</h3><p>Understanding the core elements of LLM training is crucial for grasping how these powerful models learn and function. Let's break down the key components:</p><ul><li><p><p>\nDuring pretraining, the model is exposed to large-scale datasets such as Common Crawl, Wikipedia, and other publicly available text sources. This phase helps the model learn grammar, facts, and reasoning abilities.</p></p></li><li><p><strong>Reinforcement Learning with Human Feedback (RLHF):</strong><p>\nAfter pretraining, the model is fine-tuned using human feedback to align its outputs with desired behaviors. This step ensures the model generates more accurate and contextually appropriate responses.</p></p></li></ul><h3>\n  \n  \n  Why Full LLM Training Is Not Feasible\n</h3><p>Training an LLM from scratch provides control and customization but is highly impractical for most organizations due to:</p><ul><li> Requires thousands of GPUs/TPUs, running for weeks, costing millions in compute power.</li><li> Training requires terabytes of high-quality text, posing challenges in collection, cleaning, and legal compliance.</li><li> Needs specialized AI infrastructure with high-speed networking, scalable storage, and cloud resources.</li><li> Requires AI researchers, ML engineers, and data scientists, making in-house development unrealistic for most companies.</li></ul><p>Due to these barriers, most businesses opt for fine-tuning pre-trained models instead of full-scale training.</p><p>Now that you understand the fundamentals of full LLM training, let's explore the more practical approach of fine-tuning.</p><p>Fine-tuning LLM is the process of adapting a pretrained LLM to perform specific tasks or cater to particular domains. Unlike full training, fine-tuning requires significantly fewer resources and can be done with smaller, task-specific datasets.</p><h3>\n  \n  \n  Fine-Tuning: A Cost-Effective Alternative\n</h3><p>Fine-tuning is a cost-effective way to customize large language models. However, it is only possible with open-source models like <a href=\"https://www.llama.com/\" rel=\"noopener noreferrer\">LLaMA</a> and <a href=\"https://www.deepseek.com/\" rel=\"noopener noreferrer\">deepseek</a>, as closed-source models generally do not allow fine-tuning. This makes it an attractive option, enabling organizations to tailor AI models to their needs without the high costs of building one from scratch.</p><p>There are different techniques used for fine-tuning an LLM. Each has its own advantages and disadvantages.</p><ul><li><p><strong>LoRA (Low-Rank Adaptation):</strong><p>\nLoRA is a lightweight fine-tuning method that modifies only a small subset of the model’s parameters. This approach is cost-effective and efficient, making it ideal for organizations with limited resources.</p></p></li><li><p><p>\nIn this method, the model is trained on custom datasets tailored to specific tasks. For example, a company might fine-tune an LLM using its internal documentation to create an AI assistant for employees.</p></p></li></ul><p>Fine-tuning unlocks various practical applications across different industries. Let's explore some common use cases where fine-tuning proves to be particularly beneficial.</p><ul><li> Fine-tuning LLMs with company-specific data to create personalized virtual assistants.</li><li><strong>Domain-Specific Chatbots:</strong> Adapting models for industries like legal, medical, or customer support to provide accurate and relevant responses.</li></ul><p>Now that you understand fine-tuning, let’s see how Retrieval-Augmented Generation (RAG) can further enhance LLM capabilities.</p><h2>\n  \n  \n  What is RAG (Retrieval-Augmented Generation)?\n</h2><p>Retrieval-Augmented Generation (RAG) is an advanced technique that enhances Large Language Models (LLMs) by dynamically retrieving external data at runtime. Unlike fine-tuning, which modifies the model’s parameters, RAG keeps the model unchanged and instead augments prompts with relevant, real-time information.</p><p>Instead of relying solely on pre-trained knowledge, RAG retrieves external information such as documents, web pages, or databases before generating a response. This process allows LLMs to stay up-to-date, context-aware, and factually accurate without requiring periodic model updates.</p><p>RAG stands out from other LLM enhancement techniques due to its ability to dynamically fetch information rather than relying on static training data. Key advantages include:</p><ul><li><strong>Real-Time Knowledge Updates:</strong> RAG enables LLMs to access up-to-date data without retraining, making it ideal for dynamic industries like news, finance, and healthcare.</li><li><strong>No Need for Extensive Fine-Tuning:</strong> Since RAG works by retrieving external data, it eliminates the need for resource-intensive fine-tuning processes.</li><li> Businesses can connect RAG to various external data sources, such as APIs, document repositories, or web scrapers, to tailor responses to their needs.</li><li><strong>Overcomes Training Limitations:</strong> Traditional LLMs have a knowledge cutoff and token constraints; RAG mitigates these issues by fetching only the most relevant information when needed.</li></ul><p>RAG is particularly useful in scenarios where real-time information retrieval is crucial. Some common applications include:</p><ul><li><strong>AI Chatbots with Live Data:</strong> Chatbots enhanced with RAG can fetch the latest news, stock prices, or support documentation, ensuring users receive up-to-date responses.</li><li><strong>Legal and Medical Research Assistants:</strong> RAG-powered assistants can pull information from legal databases or medical journals to provide context-specific insights.</li><li><strong>Web-Scraped Data Integration:</strong> Businesses can combine web scraping with RAG to integrate external data into AI responses, ensuring the most relevant and recent information is always available.</li></ul><p>Now, let’s explore how to seamlessly integrate web data into your LLM workflows using RAG and web scraping techniques.</p><h2>\n  \n  \n  Enhancing LLMs with RAG and Web Scraping Using ScrapFly\n</h2><p>It's common for web scraping tools to send HTTP requests to web pages in order to retrieve their data as HTML. However, utilizing web scraping as the RAG data source, we have to extract the web data in a format that LLMs understand, either as Text or Markdown.</p><p>Here's how to use web scraping for RAG models using OpenAI. First, <a href=\"https://platform.openai.com/api-keys/\" rel=\"noopener noreferrer\">get your OpenAI key</a> and use the following code:</p><div><pre><code></code></pre></div><p>Here, we start by creating a , a component required by the RAG model. It splits the documents into a set of chunks, sets the relationship between their text, and saves them into memory.</p><p>You can learn more about How to Power-Up LLMs with Web Scraping and RAG in our dedicated article:</p><p>How to Power-Up LLMs with Web Scraping and RAG</p><p>We'll explain how to use LLM and web scraping for RAG applications.</p><p>Below are quick answers to common questions about LLM training, fine-tuning, and RAG.</p><h3>\n  \n  \n  What is the difference between fine-tuning and RAG?\n</h3><p>Fine-tuning modifies the LLM's parameters using custom data, while RAG enhances LLM responses with real-time, external data without changing the model itself. Fine-tuning is used to modify the LLM's behavior, while RAG modifies the knowledge.</p><h3>\n  \n  \n  When should I use LoRA for fine-tuning?\n</h3><p>Use LoRA when you have limited computational resources, need to fine-tune quickly, and want to minimize the number of trainable parameters.</p><h3>\n  \n  \n  What are the limitations of RAG?\n</h3><p>RAG cannot deeply modify the LLM's behavior. For knowledge modification, it's primarily limited by the token size constraints of the LLM model, which restricts the amount of external data that can be incorporated into a single prompt at any given time.</p><p>This article provided a comprehensive overview of three critical techniques for leveraging Large Language Models (LLMs): full training, fine-tuning, and Retrieval-Augmented Generation (RAG). Here's a recap of the key takeaways:</p><ul><li><p> While powerful, it's often impractical for most organizations due to high costs, massive data requirements, and specialized infrastructure.</p></li><li><p> A more accessible approach, enabling you to customize pre-trained LLMs for specific tasks with significantly fewer resources. LoRA, in particular, offers a lightweight and efficient method.</p></li><li><p><strong>Retrieval-Augmented Generation (RAG):</strong> A dynamic technique that enhances LLM responses with real-time data, keeping knowledge current without the need for retraining.</p></li></ul><p>Here's a handy table to illustrate this:</p><p>Retrieval-Augmented Generation (RAG)</p><p>Requires access to GPUs/TPUs, but significantly fewer than full training.</p><p>Minimal computational resources needed.</p><p>Requires thousands of GPUs/TPUs, running for weeks.</p><p>Needs task-specific datasets, which are smaller and more manageable.</p><p>Uses existing knowledge bases or documents, no need for extensive datasets.</p><p>Needs terabytes of high-quality text data.</p><p>Requires some AI infrastructure but less demanding than full training.</p><p>Basic infrastructure suffices.</p><p>Requires specialized AI infrastructure with high-speed networking and scalable storage.</p><p>Requires knowledge in machine learning and model fine-tuning.</p><p>Easier to implement with less specialized knowledge.</p><p>Requires a team of AI researchers, ML engineers, and data scientists.</p><p>Understanding the nuances of each approach full training, fine-tuning, and RAG is crucial for effectively and efficiently harnessing the power of LLMs in your projects.</p>","contentLength":9592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Share your success stories in Green Software","url":"https://dev.to/gaelduez/share-your-success-stories-in-green-software-ep4","date":1739779702,"author":"Gael DUEZ","guid":1698,"unread":true,"content":"<p>And one last thing, several recent episodes of the <a href=\"https://greenio.tech\" rel=\"noopener noreferrer\">Green IO podcast</a> 🎙️ have focused on development, feel free to give them a listen. :headphone:</p>","contentLength":149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Upcoming Scenario of India’s E-commerce Market: An Analysis for 2030","url":"https://dev.to/silverclouding_ads_0fb803/the-upcoming-scenario-of-indias-e-commerce-market-an-analysis-for-2030-55ch","date":1739779684,"author":"SilverClouding Ads","guid":1697,"unread":true,"content":"<p>Introduction\nAccording to the statistical report, the Indian e-commerce market’s industry is achieving a milestone by the end of 2030. Integration of the internet and smartphones is the answer. Over 895 million people are using the internet widely to serve their purposes till June 2023. The most important thing is that 55% of connections were in urban areas, and most of them were wireless. Again, smartphone users are expected to reach 1.1 billion by 2025 and US$ 1 trillion by 2030. This rapid growth of internet use already started transforming India’s e-commerce sector. </p><p>This article provides a comprehensive overview of the current e-commerce market. It also explains the key growth drivers. Here, you will also explore emerging trends and challenges to reaching US$325 billion by 2030.</p><p>India’s E-commerce Market Situation\nAccording to India’s Market 2025, e-commerce can reach more and more customers across the world. It also influences traditional retailers to innovate and adopt new ideas, and the Internet economy will reach $1 trillion by 2030. Businesses from Tier-2, Tier-3, and Tier-4 cities started to follow data-driven patterns by the ISB Institute of Data Science and Ecom Express. It affects the shopping experience and makes the supply chain infrastructure perfect. It boosts consumer behavior significantly toward businesses. India’s e-commerce market data says the market size will be $325 billion by 2030. </p><p>E-commerce market growth is also influenced by rapid urbanization, fluctuating consumer preferences, and rising maintenance costs. Amazon, Flipkart, and Reliance Digital-like popular brands follow this concept and continue to shape the e-commerce market landscape. So, overcoming logistical barriers, cybersecurity threats, and regulatory intricacies for India’s e-commerce market is vital to ensuring sharp e-commerce growth.</p><p>Key Drivers of Growth by 2030\nIn this competitive business era, several key drivers are there to influence growth. Here are some of these drivers that people need to know. </p><p>a. Digital Penetration and Smartphone Usage\nOver a billion people prefer mobile phones for shopping today. Smartphones make it more hassle-free and accessible to consumers. Whether it is in urban or rural areas, this is an ongoing trend everywhere. This penetration is one of the key drivers of the e-commerce market. </p><p>b. Technological Advancements\nThe invention of AI-driven technology gives a more personalized shopping experience. It boosts customer engagement. Integration of machine learning, big data analytics, and blockchain offers smooth operations in the e-commerce market. </p><p>c. Government Policies and Support\nThese days, the government also takes initiatives like Digital India, Make in India, and Startup India. It gives a blow to the online retail ecosystem. Thus, businesses started to enter the digital marketplace with this idea and ensure sharp growth.</p><p>d. Logistics and Infrastructure Evolution\nOptimized delivery in an urban location is also possible with smart warehouses and AI-powered supply chain management. Thus, most online businesses start investing in logistics hubs. The constant connectivity will increase operational efficiency in India’s market in 2025.</p><p>e. Digital Payment Utilization\nThe huge adoption of UPT and mobile wallets makes transactions simple and fast. It also increases users’ reliability and dependency worldwide. Therefore, more businesses have decided to participate in online commerce.</p><p>Emerging Trends in E-commerce\nKnowing the trends and e-commerce market conditions is always beneficial for businesses. This section elaborates on some popular trends in e-commerce. </p><p>a. Hyperlocal and Social Commerce\nToday, Instagram shopping, Meesho, and WhatsApp businesses are getting popular. These online platforms have started reshaping the retail landscape effectively. Using these hyperlocal social commerce platforms drives sales directly. Following this trend is the way to engage more customers in a business. </p><p>b. Sustainable Practices \nSustainability is another thing that helps improve business efficiency. They include everything from eco-friendly packaging to carbon-neutral shipping and more. Most importantly, conscious consumers trust brands to maintain sustainability and ensure satisfaction.</p><p>c. Subscription-Based Models\nOTT streaming services are a part of online marketing and have become a top trend. It will not get fed with time. Top platforms ensure their subscription models are gaining popularity. Amazon Prime, Netflix, and Flipkart Plus are renowned OTT platforms in this regard. These platforms ensure exclusive benefits. So, follow a suitable subscription-based model to enhance customer loyalty and repeat purchases.</p><p>d. Augmented Reality (AR) and Virtual Reality (VR)\nAR and VR technologies also influence the shopping experience. How? It enables virtual try-ons and allows customers immersive store visits. Most businesses, i.e., fashion, beauty, and furniture, prefer to implement this technology and bring new growth. </p><p>e. Cross-border E-commerce\nIndian brands also influence the international market. Global e-commerce platforms and improved logistics solutions with cross-border trade. It gives access to Indian sellers to a wider customer base.</p><p>Challenges to Overcome \nFDI policies and data privacy regulations are changing rapidly and upgrading day by day. It brings hassle for the businesses. Thus, e-commerce businesses must know these changes. Doing this is the key to avoiding any future hassle.<p>\nCybersecurity is another vital part and has become a concern for businesses. India’s e-commerce market also needs this security for cyber threat protection.</p>\nCost-effective and last-mile delivery is challenging. It is more difficult in rural areas. Thus, e-commerce businesses should focus on this issue to ensure uninterrupted business satisfaction.<p>\nFuture Projections and Market Size</p>\nIndia’s e-commerce market experiences exponential growth and is projected to reach $1 trillion by 2030. This high-rise business market also brings new opportunities, and the tentative market size will be $325 billion by 2030. E-commerce has become a gateway for businesses to reach more customers beyond categories and geographical boundaries. It not only influences businesses but gives consumers a satisfying experience.</p><p>Customers can experience unparalleled convenience and accessibility to a range of products with a few clicks. They can compare prices for their preferred items, which gives them more comfort. This flexibility allows the e-commerce market to grow rapidly and influence the overall Indian economy. </p><p>Conclusion\nIn conclusion, businesses need to support the e-commerce landscape that is expanding exponentially. It supports a tech-savvy nature and robust investment for propelling the growth of e-commerce in India. Consumers can also experience flexibility while keeping faith in this ongoing trend. It is better to be prepared to address challenges associated with online e-commerce marketing and drive prosperity across the nation.</p>","contentLength":7045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Unlocking Multilingual Speech: The Future of AI Translation Models\"","url":"https://dev.to/gilles_hamelink_ea9ff7d93/unlocking-multilingual-speech-the-future-of-ai-translation-models-52ec","date":1739779632,"author":"Gilles Hamelink","guid":1696,"unread":true,"content":"<p>In an increasingly interconnected world, the ability to communicate across languages is no longer a luxury but a necessity. Have you ever found yourself lost in translation, struggling to convey your thoughts in a foreign tongue? Or perhaps you've wished for seamless conversations with friends or colleagues from different linguistic backgrounds? Welcome to the transformative realm of AI-powered multilingual speech models—where barriers dissolve and understanding flourishes. In this blog post, we will embark on an enlightening journey through the rise of artificial intelligence in language translation, exploring how these advanced systems work their magic behind the scenes. You'll discover not only the remarkable benefits they offer—such as real-time communication and enhanced accessibility—but also the challenges that developers face in perfecting these technologies. As we gaze into the future trends shaping AI language technology, you'll gain insights into its real-world applications that are revolutionizing industries and personal interactions alike. Join us as we unlock the potential of multilingual speech and redefine what it means to connect globally!</p><p>The emergence of advanced AI models, particularly the OWLS suite, has significantly transformed language translation and multilingual speech recognition. These models range from 0.25B to 18B parameters and are pre-trained on extensive datasets encompassing 150 languages. By leveraging neural scaling laws, researchers can predict performance improvements as model size increases, demonstrating that larger models yield better results in diverse linguistic contexts. This advancement addresses challenges such as orthographic opacity and code-switching, which often hinder accurate translations.</p><p>Research indicates that increased model size enhances capabilities in recognizing speech across various languages while also improving translation accuracy. Notably, the OWLS models exhibit superior performance even with low-resource languages where traditional systems struggle. Furthermore, these open-source tools foster collaboration within the research community by allowing for continuous improvement and adaptation to new linguistic data sets.</p><p>The societal implications of deploying large-scale multilingual models cannot be overlooked; ethical considerations regarding bias and representation must guide their development to ensure equitable access to language technology globally. As we witness this rise in AI-driven translation capabilities, it is essential to remain vigilant about both its potential benefits and inherent challenges.</p><p>Multilingual speech models, such as the OWLS suite, leverage neural scaling laws to enhance performance across diverse languages. These models range from 0.25B to 18B parameters and are pre-trained on extensive multilingual datasets encompassing 150 languages. The study reveals that increasing model size significantly improves automatic speech recognition (ASR) and translation tasks by effectively managing linguistic diversity and complexities like code-switching. Furthermore, it emphasizes the importance of data variety in training; a richer dataset leads to better generalization across low-resource languages.</p><h2>Key Insights into Model Scaling</h2><p>The research highlights how larger models yield substantial benefits in real-world applications, particularly when addressing challenges such as orthographic opacity—where written language does not clearly represent spoken sounds—and recognizing mixed-language utterances. Additionally, it discusses the uneven distribution of advantages among different languages due to varying resource availability for training data. By introducing OWLS models capable of evaluating mondegreen capabilities and facilitating in-context learning, this work underscores the necessity for robust architectures tailored for multilingual processing while also considering ethical implications associated with large-scale AI systems development.# Benefits of Advanced AI Translation</p><p>Advanced AI translation models, such as the OWLS suite, offer significant advantages in multilingual speech recognition and translation. These models utilize neural scaling laws to enhance performance by increasing both model size and training data diversity. With capabilities spanning 150 languages, they address complex linguistic contexts effectively. Larger models demonstrate superior accuracy in challenging scenarios like code-switching and orthographic opacity, making them invaluable for real-world applications.</p><h2>Enhanced Performance Metrics</h2><p>The study reveals that larger parameter sizes correlate with improved outcomes across various tasks. This is particularly evident in low-resource language processing where traditional methods struggle. The OWLS models also facilitate better semantic understanding through advanced context learning mechanisms, ensuring translations are not only accurate but contextually relevant.</p><p>By leveraging extensive datasets during training, these advanced systems can adapt to diverse dialects and regional variations within languages—ultimately leading to more inclusive communication solutions globally. Furthermore, their open-source nature promotes collaborative research efforts aimed at refining these technologies further while addressing ethical considerations surrounding large-scale AI implementations in society.# Challenges Facing AI in Multilingualism</p><p>AI systems face significant challenges when dealing with multilingualism, particularly in speech recognition and translation. One of the primary issues is the varying availability of training data across languages; many low-resource languages lack sufficient datasets, leading to subpar model performance. Furthermore, complexities such as orthographic opacity—where written forms do not correspond neatly to spoken language—and code-switching—switching between languages within a conversation—complicate processing tasks. The OWLS models highlight these difficulties by demonstrating that larger models can improve recognition capabilities but may still struggle with less common linguistic contexts.</p><h2>Data Diversity and Model Scaling</h2><p>The relationship between model size and data diversity plays a crucial role in overcoming these challenges. Larger models benefit from increased parameters that allow for better generalization across diverse linguistic structures; however, they require extensive and varied training datasets to realize their full potential. This uneven distribution of resources means that while high-performance solutions exist for widely spoken languages, many others remain underserved. Thus, addressing these disparities is essential for developing robust multilingual AI systems capable of effective communication across all languages.</p><p>The future of AI language technology is poised for significant advancements, particularly with the development of multilingual speech recognition and translation models like OWLS. These models leverage neural scaling laws to enhance performance across diverse linguistic contexts, showcasing a trend towards larger model sizes that yield better accuracy in tasks such as automatic speech recognition (ASR) and machine translation (MT). The increasing availability of extensive multilingual datasets covering over 150 languages allows researchers to train more robust systems capable of handling challenges like code-switching and orthographic opacity. Furthermore, open-source initiatives promote collaboration within the research community, accelerating innovation.</p><h2>Scaling Models for Enhanced Performance</h2><p>As we look ahead, the emphasis on scaling both model size and training data will continue to shape AI language technologies. Larger models not only improve performance metrics but also address disparities among low-resource languages by providing tailored solutions that consider unique linguistic features. Additionally, advancements in compute efficiency will enable these sophisticated models to operate effectively even under constrained resources. This evolution underscores the importance of ethical considerations surrounding large-scale model training while aiming for inclusivity across various dialects and cultural contexts.</p><p>In summary, ongoing research into scalable architectures promises transformative impacts on how machines understand and generate human language—ultimately fostering greater accessibility through improved communication tools globally.# Real-World Applications of Multilingual Speech</p><p>Multilingual speech recognition and translation models, such as the OWLS suite, have transformative applications across various sectors. In customer service, these models enable businesses to provide support in multiple languages simultaneously, enhancing user experience and satisfaction. Healthcare systems utilize multilingual capabilities for patient interactions, ensuring accurate communication regardless of language barriers. Educational platforms benefit from real-time translation services that facilitate learning among diverse student populations.</p><h2>Industry-Specific Implementations</h2><p>In the travel industry, multilingual speech technology assists travelers by providing instant translations during conversations with locals or when navigating foreign environments. Furthermore, content creators leverage these advanced AI tools to generate localized marketing materials efficiently. The entertainment sector also sees significant advantages; subtitling and dubbing processes are streamlined through automated translations powered by robust multilingual models.</p><p>These applications underscore the importance of scaling laws in developing effective ASR (Automatic Speech Recognition) and ST (Speech Translation) systems capable of handling a wide array of linguistic contexts while addressing challenges like code-switching and low-resource languages effectively. As industries continue to adopt these technologies, their impact on global communication will only grow stronger.</p><p>In conclusion, the evolution of AI in language translation marks a significant milestone in bridging communication gaps across cultures. As multilingual speech models become increasingly sophisticated, they harness advanced algorithms to provide seamless and accurate translations that cater to diverse linguistic needs. The benefits of these technologies are profound, enhancing global connectivity and accessibility while fostering collaboration in various sectors such as business, education, and healthcare. However, challenges remain—such as dialectal nuances and contextual understanding—that require ongoing research and innovation. Looking ahead, we can anticipate exciting trends like real-time translation capabilities and greater integration with everyday devices. Ultimately, the future of AI-driven multilingual speech holds immense potential for transforming how we interact globally, making it essential for individuals and organizations alike to embrace this technological advancement for a more interconnected world.</p><h3>1. What is the significance of AI in language translation?</h3><p>AI has revolutionized language translation by enabling faster, more accurate translations across multiple languages. It leverages machine learning algorithms to understand context and nuances in different languages, making communication easier and breaking down language barriers.</p><h3>2. How do multilingual speech models function?</h3><p>Multilingual speech models utilize deep learning techniques to process and analyze spoken language data from various languages simultaneously. They are trained on large datasets that include diverse linguistic patterns, allowing them to recognize and translate speech in real-time while maintaining contextual integrity.</p><h3>3. What are the benefits of advanced AI translation technologies?</h3><p>Advanced AI translation technologies offer numerous benefits including improved accuracy, reduced turnaround time for translations, support for a wider range of languages, enhanced user experience through natural-sounding translations, and increased accessibility for non-native speakers.</p><h3>4. What challenges does AI face in achieving effective multilingualism?</h3><p>Challenges facing AI in multilingualism include handling dialects and regional variations accurately, managing low-resource languages with limited training data, ensuring cultural sensitivity during translations, addressing privacy concerns related to data usage, and overcoming biases present in training datasets.</p><h3>5. What future trends can we expect in AI language technology?</h3><p>Future trends may include the development of more sophisticated neural networks capable of understanding emotional tone or intent behind words; greater integration with augmented reality (AR) tools; advancements toward real-time conversational translators; improvements in voice recognition accuracy across diverse accents; and an emphasis on ethical considerations surrounding bias reduction within translation systems.</p>","contentLength":12970,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flutter vs. Kotlin: A Comprehensive Guide for 2025 Mobile Strategies","url":"https://dev.to/trretatechnolabs/flutter-vs-kotlin-a-comprehensive-guide-for-2025-mobile-strategies-4n14","date":1739779518,"author":"Rachel Ritchay","guid":1695,"unread":true,"content":"<p>Customer experience continues to dominate mobile app development practices for responsive, scalable, and efficient applications. To achieve this, choosing the right technology stack becomes essential as it is a strategic decision and not a technical one.</p><p>Understanding the difference between Flutter and Kotlin will help you avoid inefficiencies, compatibility issues, and scaling challenges in app performance. Flutter, Google’s cross-platform framework, is known for its spontaneous development cycles and customizable UI widgets.</p><p>On the other hand, Kotlin is a programming language perfect for Android-native app development. This blog on Flutter vs Kotlin compares their unique features, performance benchmarks, and use cases, helping you align your choice with your 2025 mobile strategy.</p><p><a href=\"https://flutter.dev/\" rel=\"noopener noreferrer\">Flutter</a> is an open-source framework for developing cross-platform applications (mobile, web, and desktop) using a single codebase. Powered by <a href=\"https://dart.dev/\" rel=\"noopener noreferrer\">Dart</a>, Flutter offers pixel-perfect UI widgets to build responsive, natively compiled, visually attractive applications.</p><p>Its ability to update user interfaces in real-time and ensure consistent performance across devices makes it a top choice for modern app development.</p><ul><li> Facilitates real-time updates during development, accelerating time-to-market.</li><li> Offers extensive libraries of plugins and tools for complex projects.</li><li> Backed by an active global community, minimizing dependency on proprietary technologies.</li><li><strong>Less Development Time &amp; Cost:</strong> A single codebase reduces overhead for cross-platform development. Build once, deploy everywhere!</li></ul><p><a href=\"https://kotlinlang.org/\" rel=\"noopener noreferrer\">Kotlin</a> , created by JetBrains and endorsed by Google, is an open-source programming language for Android app development. It is known for its seamless Java interoperability, concise syntax, and ability to reduce code duplication.</p><p>With its flexibility and clean coding practices, Kotlin serves as an ideal choice for businesses prioritizing efficiency, scalability, and faster time-to-market for Android-first or modular apps.</p><ul><li> Provides native-like performance by leveraging direct platform integrations</li><li><strong>Reuse of Shared Business Logic:</strong> Reduces redundancy for cross-platform projects by sharing core logic across platforms.</li><li> Reduces code duplication, ensuring scalable applications.</li><li> Offers businesses greater control by integrating with existing tools and workflows.</li></ul><p>When comparing Flutter vs Kotlin, the decision often depends on your project’s requirements. Understanding their unique strengths and use cases allows you to align your choice with your business's 2025 mobile strategy.</p><h2><strong>Flutter vs. Kotlin: Choosing the Right Tech Stack for Your Mobile App Development</strong></h2><p>Let's check out some of the main differences between the two app development frameworks.</p><p>The learning curve for Kotlin and Flutter depends on several factors that need to be considered.</p><ul><li> Understanding Flutter requires learning Dart, a unique syntax that differs from the available programming languages like Java, C++, and Javascript. However, if you prefer using Flutter, commencing your first project will take a lot less time.</li></ul><p>It employs JIT(Just-in-Time) and AOT(AHead-of-Time) compilation techniques for app performance and ensures faster load times to run apps smoothly. Moreover, its straightforward installation process takes only 30 minutes, enabling teams to collaborate on cross-platform app development projects using a single codebase. </p><ul><li> Kotlin offers a syntax that is near-identical for developers familiar with Java. It is also known as an advanced version of Java. Its concise structure reduces the need for repetitive code, especially for the development process for Android-native applications.</li></ul><p>However, Kotlin requires separate codebases for each platform, increasing development time and cost for businesses aiming to build cross-platform applications.</p><p>In contrast, Flutter’s single codebase makes learning easier for cross-platform projects. Additionally, Kotlin’s strengths lie in Android native app development, but it lacks the maturity and speed that Flutter brings to businesses. </p><p>The performance of Flutter vs Kotlin depends on several key factors that influence app functionality and user experience:</p><ul><li> Dart is Flutter's core programming language, making it an absolute solution for businesses, as there is no need for a Javascript bridge to allow app interactions. It uses the same codebase for frontend and backend, offering faster animation speeds.</li></ul><p>It natively compiles with ARM code, boosting mobile app performance. Moreover, the UI rendered by its own engine allows for smoother app performance at 60 frames per second. Flutter is extensively helpful for complex, resource-intensive applications.</p><ul><li> Unlike Flutter, Kotlin uses the resources provided by its native platform, such as an Android ecosystem. It does not require a separate engine for rendering, as it uses platform-specific APIs.</li></ul><p>Kotlin's multithreading capabilities and Java interoperability offer flexibility in the existing codebase. However, regarding cross-platform development, Kotlin requires additional effort, such as employing Kotlin Multiplatform, which enables code sharing but does not match Flutter's unified codebase approach.</p><h3><strong>3. Integration with Third-party Libraries and Tools</strong></h3><p>In today’s customer-centric app development environment, integrating your digital product with the most efficient APIs and plugins is necessary. The tough competition of Flutter vs Kotlin plays a crucial role in shaping your app’s future.</p><ul><li> Flutter’s integration capabilities are centered around its plugin ecosystem, accessible through platforms like <a href=\"https://pub.dev/\" rel=\"noopener noreferrer\">pub.dev</a>. It houses an extensive library of ready-to-use packages. This simplifies adding functionalities such as payment gateways, analytics, and device-specific features.</li></ul><p>Moreover, its community-driven approach regularly updates plugins, allowing businesses to develop customized plugins if the existing one does not meet their needs.</p><ul><li> Kotlin integrates with a vast ecosystem of Java’s libraries and tools because of its interoperability with Java. Businesses focusing on Android-specific app development projects or certain Java-based functionalities can rely on Kotlin.</li></ul><p>In contrast, Flutter offers perfectly aligned collaboration with your projects, providing UI-rich functionalities and reducing development overhead. However, Kotlin focuses on integrating features that are available on Android-native apps.</p><h3><strong>4. User Interface Development</strong></h3><p>User interface(UI) is the first thing that a user comes across when using your app, making it a critical factor to consider. Here’s why:</p><ul><li> Flutter is a widget-oriented cross-platform framework that offers numerous options to customize your UI. Moreover, its “hot reload” feature allows you to make changes to the user interface and experience the interactions in real-time.</li></ul><p>Flutter’s Skia rendering engine ensures that the UI is consistent on both Android and iOS, regardless of device-specific differences. As a UI-focused framework, Flutter is a robust choice for businesses aiming for visually consistent apps.</p><ul><li> Kotlin, a general-purpose programming language, relies on Android’s built-in UI components. It ensures highly reliable and updated UIs but can lack designing for complex projects compared to Flutter’s widget-based system.</li></ul><p>Moreover, Kotlin Multiplatform lacks a unified widget framework like Flutter, making it unreliable for cross-platform app development. This means you must build separate UI for Android and iOS.</p><p>You must understand the popularity and cost differences to better debate Flutter vs Kotlin. It will help you determine which tech stack best fits your app budget and target audience.</p><ul><li><p> Flutter has gained significant traction because it can create cross-platform apps with a single codebase. Its open-source nature eliminates licensing fees, while the strong developer community ensures ongoing support and updates. The framework serves as an excellent choice for businesses seeking cost efficiency.</p></li><li><p> Kotlin is highly popular among businesses focused on Android app development because it is Google's preferred language for Android development. It collaborates seamlessly with existing Java systems and minimizes overhead costs.</p></li></ul><p>Both technologies are free to use, but the overall cost will depend on your project's complexity and your development team's expertise. Also, the Kotlin SDK only offers semi-native features, so some additional expenses may be involved.</p><p>Comprehensive documentation allows your business teams to resolve unknown issues that increase market downtime.</p><ul><li><p> Flutter’s official resources cover everything to get you started with advanced techniques, allowing businesses to resolve issues and build robust apps. All this is done with the help of strong community support and detailed examples for each instance.</p></li><li><p> Kotlin’s documentation focuses primarily on language syntax, platform-specific implementation, and integration with Android Studio. While comprehensive for Android-native development, it lacks Flutter's unified cross-platform focus.</p></li></ul><p>Businesses relying on Kotlin may need additional resources for cross-platform features, especially when integrating with non-Android platforms.</p><h3><strong>7. Scope in Backend Development</strong></h3><p>Backend development is the key to smooth application processing. Let's check how Kotlin vs Flutter serves the business's intended purpose.</p><ul><li> Flutter is traditionally a frontend framework; however, its integration capabilities with backend technologies are growing. Using tools like <a href=\"https://firebase.google.com/\" rel=\"noopener noreferrer\">Firebase</a>, REST APIs, or GraphQL, Flutter simplifies connecting the app’s front end to robust backend systems.</li></ul><p>For businesses looking to streamline both frontend and backend, Flutter enables efficient development through third-party plugins and libraries that handle backend functionalities.</p><ul><li> Kotlin, being a programming language, offers a more direct scope for backend development. It can be used with frameworks like <a href=\"https://ktor.io/\" rel=\"noopener noreferrer\">Ktor</a> and <a href=\"https://spring.io/\" rel=\"noopener noreferrer\">Spring Boot</a> to build server-side applications, APIs, and microservices.</li></ul><p>Kotlin's seamless integration with JVM-based tools and libraries makes it a strong choice for businesses seeking a unified tech stack for front-end (Android-native) and back-end development.</p><h3><strong>8. Code Reusability and Maintenance</strong></h3><p>The reuse of code plays a crucial role in reducing app deployment and development time. Let’s check which of the two frameworks makes it to the top in such a case.</p><ul><li><p> Flutter’s banner approach says: write once, deploy everywhere, meaning you can write the code once and deploy it to build apps for multiple platforms. This unified approach makes it easier to implement updates and fix issues. It also supports ongoing maintenance practices for efficient app performance.</p></li><li><p> Kotlin focuses on clean coding practice and modular structures. However, you still need to write a separate UI code for both Android and iOS. As a result, Kotlin may not provide effortless development time and code reusability.  </p></li></ul><p>Flutter’s edge lies in its ability to align cross-platform workflows, making it an efficient solution for businesses prioritizing scalability and cost-effectiveness.</p><h3><strong>9. Technical Support &amp; Community</strong></h3><ul><li><p> Flutter has a large, active community that offers constant app development support. Numerous platforms like <a href=\"https://stackoverflow.com/\" rel=\"noopener noreferrer\">Stack Overflow</a> and <a href=\"https://github.com/\" rel=\"noopener noreferrer\">GitHub</a> are filled with resources. These resources make it easier to resolve challenges during development. You can also rely on regular updates from Google, ensuring the framework remains robust and future-proof.</p></li><li><p> Kotlin benefits from strong support, particularly for Android-native apps, due to its integration with <a href=\"https://www.jetbrains.com/\" rel=\"noopener noreferrer\">JetBrains</a> and Google. Its community is smaller compared to Flutter but highly skilled. While resources are focused on Android development, Kotlin still provides reliable support for businesses targeting Android-first apps.</p></li></ul><ul><li><p> Flutter’s single codebase significantly reduces development and maintenance costs. Its consistent updates ensure long-term reliability and the ability to create feature-rich apps. These feature-rich capabilities are expenses that come along with the framework, reducing the overall costs of using Flutter for app development.</p></li><li><p> Kotlin provides excellent long-term ROI for Android-native projects due to its efficiency and seamless integration with Java. \nHowever, for businesses focusing on cross-platform development, the need for separate codebases may increase initial costs, potentially affecting ROI in the long run compared to Flutter.</p></li></ul><ul><li><p> Flutter serves as an ideal solution for businesses targeting specific audiences. Whether it’s mobile, web, or desktop, Flutter adapts flawlessly. Its widget-based architecture also allows for complete UI customization so that you can optimize your app to meet unique market demands without high overhead.</p></li><li><p> Kotlin provides flexibility for Android-first businesses, allowing deep integration with platform-specific APIs and Java-based systems. While it supports cross-platform development through Kotlin Multiplatform, it’s best suited for businesses primarily delivering top-tier Android-native experiences.</p></li></ul><h2><u><strong>Flutter vs. Kotlin: Use Case Table</strong></u></h2><p>This table for Flutter vs Kotlin is designed to highlight specific scenarios where each framework excels. It explicitly ties features to real-world use cases so business owners can make informed decisions based on their unique app needs.</p><h2><u><strong>Why Flutter is Better than Kotlin?</strong></u></h2><p>Flutter is a strong contender for businesses needing cost-efficient and visually engaging cross-platform applications. Its single codebase approach and tools like hot-reload ensure faster prototyping and scalable app development.</p><ul><li><p>Build Instant MVPs and High-Performance Apps\nFlutter enables businesses to develop MVPs rapidly with its real-time hot-reload feature. It native ARM compilation offers the high performance needed for complex applications.</p></li><li><p>UI Experience and Design\nFlutter provides widget-based architecture, ensuring fully customizable and consistent UI designs across devices.</p></li><li><p>Cross-platform App Development\nFlutter reduces overhead for businesses targeting multiple platforms, ensuring faster development cycles and lower maintenance costs without compromising quality.</p></li></ul><h2><strong>Flutter vs. Kotlin: Which Technology Stack is The Future of Mobile App Development?</strong></h2><p>Your digital product goals can help you choose between the two frameworks. Nowadays, users prefer those apps that can serve their intent on multiple devices. Flutter’s single codebase and customizable widgets can help you with simultaneous development on both iOS and Android, boosting your app’s visibility.</p><p>On the other hand, Kotlin’s native capabilities shine for Android-first apps, leveraging Java interoperability. Moreover, as we look ahead, Flutter continues to gain momentum with its active developer community and improvements in performance and tooling. </p><p>Kotlin can be a top choice for Android-native development while expanding its reach through Kotlin Multiplatform. However, it still is a growing framework with a rather less powerful community and resources than Flutter.</p><h2><strong>Ready to Build Your Next Mobile App with Flutter?</strong></h2><p>Choosing the right framework is key to building a successful mobile app. Both Flutter and Kotlin offer unique advantages, whether it’s Flutter’s cross-platform efficiency or Kotlin’s seamless integration with Android. The choice ultimately depends on your project goals, audience, and technical requirements.</p><p>If you’re leaning towards Flutter, <a href=\"https://trreta.com/\" rel=\"noopener noreferrer\">Trreta</a> can help you leverage its full potential. Our expert <a href=\"https://trreta.com/mobile-development\" rel=\"noopener noreferrer\">Flutter development</a> team specializes in creating high-performance, visually stunning apps that work seamlessly across platforms. Partner with us to turn your app idea into a reality with speed, precision, and innovation.</p><p><a href=\"https://trreta.com/contact\" rel=\"noopener noreferrer\">Contact us</a> today to kickstart your Flutter app development journey!</p><h3><strong>1. What factors should I consider when choosing between Flutter and Kotlin for my app?</strong></h3><p>When deciding, evaluate your app's complexity, platform requirements, and team expertise. Flutter excels with its single codebase, reducing costs and time for cross-platform apps, while Kotlin is ideal for Android-native apps requiring platform-specific functionality.</p><h3><strong>2. Can I use both Flutter and Kotlin in the same project?</strong></h3><p>Yes, combining Flutter for UI and Kotlin for backend logic is possible. This hybrid approach suits scenarios requiring Android-native functionalities while maintaining Flutter's cross-platform flexibility, ensuring seamless integration of specific features or custom platform requirements.</p><h3><strong>3. Which technology is better for long-term scalability: Flutter or Kotlin?</strong></h3><p>Flutter offers long-term scalability for cross-platform apps due to its unified codebase and active community. Kotlin is better suited for scaling Android-native apps.</p><h3><strong>4. How do Flutter and Kotlin compare in terms of cost and time efficiency?</strong></h3><p>Flutter reduces development costs and time with its single codebase for multiple platforms, which is ideal for cross-platform apps. Kotlin, while efficient for Android-native development, often requires separate efforts for iOS, increasing project expenses and timelines.</p><h3><strong>5. Are there any industries where Flutter or Kotlin is the preferred choice?</strong></h3><p>Flutter is popular for industries needing cross-platform apps, such as e-commerce, healthcare, or startups. Kotlin is preferred in Android-centric sectors like gaming, finance, and enterprise apps, offering native performance and platform-specific features.</p>","contentLength":17320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Netplan: Set static IP address","url":"https://dev.to/sertxudev/netplan-set-static-ip-address-2kdh","date":1739779466,"author":"Sergio Peris","guid":1694,"unread":true,"content":"<p>In order to set a static IP address to an interface using Netplan, we have to edit the desired Netplan yaml file.</p><p>Here is an example for configuring the interface  with a static IP address.</p><div><pre><code></code></pre></div><p>Here we're setting the IP  as the static IP,  as the gateway,  and  as the DNS servers.</p><p>To apply the changes, we should run the command:</p>","contentLength":323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Simplest Eight Queens Code You’ll Find Online","url":"https://dev.to/esproc_spl/the-simplest-eight-queens-code-youll-find-online-28ko","date":1739779392,"author":"Judy","guid":1693,"unread":true,"content":"<p>The Eight Queens problem is a classic and well-known problem. Specifically, place eight queens on an 8x8 chessboard so that no two queens can attack each other, which means no two queens can be on the same row, the same column, or the same diagonal. The question is: how many solutions are there?</p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fve6uiishjj2nnqbzh2uc.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fve6uiishjj2nnqbzh2uc.png\" alt=\"Image description\" width=\"800\" height=\"389\"></a><a href=\"https://try.esproc.com/splx?3NH\" rel=\"noopener noreferrer\">try.DEMO</a>\nConsider each row of an N*N chessboard as a sequence. Number the cells in each column sequentially from 1 to N. Each queen must be placed in a unique row, with one queen per row. A1 specifies the number of queens, N, to be placed. A2 initializes the position of the queen in each row to 0. C1 defines the index of the current row being checked, i, starting from row 1.</p><p>A2 starts executing a loop, which terminates when the row index i is not greater than 0. B2 moves the queen in the current row to the next position. B3 determines if the column position of the queen in the current row is greater than N; if so, it indicates all positions in the current row have been tried. In this case, remove the queen in the current row, and return to the previous row to continue adjusting the queen’s position. In line 4, for the case of only one row, there’s no need to check for validity; proceed directly to placing the queen on the second row. If N is 1, then directly proceed to the subsequent checks.</p><p>A5 obtains the position of the queen in the current row. C5 obtains the positions of the queens in all preceding rows.</p><p>B6 determines if the current queen is on the same column or diagonal as any previously placed queen; if so, it is invalid. In this case, proceed to determine the next position.</p><p>If the current queen’s position is not in conflict with any previously placed queens, then increment the row index i in B7, and determine if all queens have been placed correctly. If so, record the positions of all queens in C7 and continue to check if there is other valid position; otherwise, start placing the next queen. In SPL, using A.concat@c() can concatenate the members of a sequence to a comma-separated string.</p><p>Once the loop ends, all possible solutions can be seen in C7.</p><p>The total number of members in C7 sequence is the number of solutions. If there is no solution, then both C7 and A8 will contain null values.</p><p>It is also possible to recursively call a subroutine to attempt to place the queen in the next row:<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fr3nkliorq3fmlm4bubyn.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fr3nkliorq3fmlm4bubyn.png\" alt=\"Image description\" width=\"800\" height=\"276\"></a><a href=\"https://try.esproc.com/splx?3VU\" rel=\"noopener noreferrer\">try.DEMO</a>\nA3 defines a subroutine queen(a, r), where the sequence a records the positions of every queen, and r represents the row where the queen is currently placed. B3 determines if all queens have been placed correctly. If true, record the current solution in B1 and return. B4 loops through each column in the current row. C4 determines if the current position is on the same column or diagonal as any queen already placed in previous rows; if so, it indicates the current position is invalid, and the next position is checked. If the current queen’s position is valid, record this position in a, and proceed to place the queen in the next row.</p><p>C1 calls a subroutine to begin placing queens starting with the first row. After execution, the placement results can be obtained in B1, and the total number of solutions can be obtained in D1.</p><p>In SPL, subroutines can be called recursively, making the code written in this way more concise and easier to read.</p>","contentLength":3282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why It’s Worth Outsourcing Your Rails Upgrades","url":"https://dev.to/railsup_sedin/why-its-worth-outsourcing-your-rails-upgrades-5cbk","date":1739778559,"author":"Raisa Kanagaraj","guid":1671,"unread":true,"content":"<p>Upgrading to the latest version of Ruby on Rails is not just a technical task—it’s a strategic decision that impacts your platform's stability, security, and scalability. As your business grows, your application evolve, and staying current with the latest version of Ruby on Rails ensures you’re leveraging cutting-edge features while avoiding vulnerabilities in older versions. However, Rails upgrades are often complex, time-consuming, and fraught with challenges.</p><p>In this blog post, we’ll explore why outsourcing your Rails upgrade to experts can save you time, reduce risks, and allow your team to focus on what they do best: delivering value through feature development.</p><h2>\n  \n  \n  The Challenges of Rails Upgrades\n</h2><p>Before diving into the benefits of outsourcing, let’s first address the common pain points associated with Rails upgrades :</p><p>Upgrading from one Rails version to another involves more than simply updating dependencies. It requires testing every aspect of your application to ensure compatibility with the new framework. This process can take weeks or even months, depending on the size and complexity of your codebase.</p><h3>\n  \n  \n  Risk of Breaking Changes:\n</h3><p>Each Rails version introduces changes—some minor, others significant—that may break existing functionality. For example, deprecated methods, altered APIs, or updated database schemas could lead to unexpected errors if not handled properly.</p><h3>\n  \n  \n  Gems Compatibility Issues:\n</h3><p>Many Rails applications rely heavily on third-party gems (libraries). When upgrading, ensuring that all gems are compatible with the new Rails version becomes a critical step. Without proper tools like RailsUp , identifying incompatible gems can be tedious and error-prone.</p><p>Your internal engineering team likely has deep domain expertise focused on building features and solving business-specific problems. Pulling them away from these tasks to handle an upgrade means losing valuable momentum on innovation and delivery.</p><p>If your team hasn’t performed a Rails upgrade before, they’ll need to learn the ropes on the job. This trial-and-error approach increases the likelihood of mistakes, delays, and frustration.</p><h2>\n  \n  \n  3 Scenarios Where Outsourcing Makes Sense\n</h2><h3>\n  \n  \n  1. If Your Team Lacks Experience with Rails Upgrades\n</h3><p>The Ruby on Rails ecosystem is vast, and its documentation for upgrades spans over 100,000 words. Navigating this guide without prior experience can overwhelm even seasoned developers.</p><p>If your team hasn’t tackled a Rails upgrade before, they’ll face a steep learning curve. They’ll need to pause their regular work to study the intricacies of the upgrade process, test extensively, and troubleshoot unforeseen issues. This DIY approach is inefficient and risky, as it increases the chances of introducing regressions or breaking critical functionality.</p><p>By outsourcing to <a href=\"https://railsfactory.com/\" rel=\"noopener noreferrer\">Rails upgrade experts</a> like RailsFactory, you gain access to consultants who live and breathe Rails upgrades. Our team has honed its skills across multiple Rails versions and industries, enabling us to execute upgrades efficiently and with minimal disruption. We have developed tools like RailsUp to identify potential gem conflicts early, reducing the risk of surprises during deployment.</p><p>Moreover, our process isn’t just about completing the upgrade; it’s about empowering your team for future success. We provide clear documentation, training sessions, and insights that help your developers maintain the upgraded system confidently.</p><h3>\n  \n  \n  2. If Your Team Is Focused on Feature Development\n</h3><p>Feature development drives business growth, yet it often grinds to a halt during major Rails upgrades. The sweeping changes required by an upgrade demand significant attention from your engineers, pulling them away from their primary responsibilities.</p><p>For instance, migrating to a newer API introduced in the latest Rails version might require rewriting parts of your codebase. If other team members aren’t aware of these changes, they might inadvertently revert progress, causing further delays and frustration.</p><p>Outsourcing your Rails upgrade allows your team to stay laser-focused on feature development. While we handle the heavy lifting of the upgrade, your engineers can continue shipping products and innovating within your core competencies. This division of labor maximizes productivity and minimizes downtime.</p><p>Additionally, our sequential upgrade methodology ensures smooth transitions between Rails versions. Rather than jumping directly to the latest version of Ruby on Rails, we incrementally move through intermediate versions, ensuring each step is stable before proceeding. This approach reduces the risk of catastrophic failures and makes the entire process more manageable.</p><h3>\n  \n  \n  3. If You Could Use a Fresh Perspective\n</h3><p>Sometimes, an outside perspective can uncover inefficiencies or opportunities for improvement that internal teams overlook. At RailsFactory, our consultants bring fresh eyes to your codebase, processes, and workflows. During the upgrade process, we often identify areas where performance can be optimized, technical debt reduced, or modern practices implemented.</p><p>For example, we might suggest refactoring legacy code that’s become difficult to maintain or recommend adopting new Rails features that align with your business goals. These insights not only enhance the quality of your application but also empower your team to adopt better practices moving forward.</p><p>Our goal is to leave your team stronger than we found it. By sharing knowledge, best practices, and actionable recommendations, we ensure that your organization benefits long after the upgrade is complete.</p><h2>\n  \n  \n  Why Choose RailsFactory for Your Rails Upgrade?\n</h2><p>When it comes to Rails Upgrade services , RailsFactory stands out for several reasons:</p><p> With nearly 20 years of experience, we’ve worked with some of the largest and most complex Rails applications in the world. Our track record speaks for itself—we’ve helped companies like General Electric successfully navigate challenging upgrades.</p><p> Our <a href=\"https://www.railsup.io/\" rel=\"noopener noreferrer\">gems compatibility checker tool</a>, RailsUp, is a game-changer for Rails upgrades. It automates the tedious task of verifying gem compatibility, saving time and reducing errors. No more manual checks or guesswork — RailsUp provides accurate, actionable insights at scale.</p><p> We follow a well-defined process tailored to each client’s unique needs. From initial assessment to final deployment, we ensure transparency, communication, and collaboration throughout the project.</p><p><strong>Focus on Business Outcomes:</strong> We don’t just deliver a technically sound upgrade; we align our efforts with your business objectives. Whether it’s improving performance, enhancing security, or enabling new features, we prioritize outcomes that drive real value.</p><p> Our relationship doesn’t end once the upgrade is complete. We offer ongoing support, training, and consultation to ensure your team feels confident managing the upgraded system.</p><p>Upgrading your Rails application is essential for staying competitive in today’s fast-paced digital landscape. However, attempting to manage this complex process internally can strain resources, delay feature development, and introduce unnecessary risks.</p><p>Outsourcing your Rails upgrade to Rails experts allows you to sidestep these challenges. With their deep expertise, advanced tools like RailsUp , and commitment to excellence, they make the upgrade process seamless and stress-free. </p><p>Let us handle the technical heavy lifting so your team can focus on driving innovation and delivering exceptional user experiences.</p><p>Ready to take the next step? Contact us today for a free consultation and discover how our <a href=\"https://railsfactory.com/rails-upgrade-services/\" rel=\"noopener noreferrer\">Rails Upgrade services</a> can transform your application—and your business.</p>","contentLength":7760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I Built Schedulicious: A Meal Planning Web App","url":"https://dev.to/czhoudev/how-i-built-schedulicious-a-meal-planning-web-app-576a","date":1739777960,"author":"Chloe Zhou","guid":1670,"unread":true,"content":"<p>I joined <a href=\"https://www.chingu.io/\" rel=\"noopener noreferrer\">Chigu</a> during my job search because I wanted to stay productive, sharpen my skills, and gain more hands-on experience working in a team. </p><p>Job hunting can be a unpredictable process, and instead of waiting around, I saw Chigu as the perfect opportunity to build something meaningful while collaborating with others. </p><p>Looking back, it turned out to be one of the best decisions I made—not only did I improve my technical and teamwork skills, but I also got to experience the full process of bringing a project from 0 to 1.</p><p>The project is a  designed to help managers efficiently create weekly menus for employees. </p><ul><li><strong>One-Click Menu Generation</strong> – Instead of manually selecting meals, managers can generate a balanced menu with a single click.</li><li> – The system ensures that each day’s dish is unique, preventing repetition throughout the week.</li><li> – If the results aren’t satisfactory, a “regenerate” button allows them to create a new menu instantly.</li><li> – Users can save and track meal plans in PDF or Excel format for easy reference.</li></ul><p>Each team receives a product spec with predefined features, but it’s up to us to break them down, plan the implementation, and bring the product to life. </p><p>Our team was unique—no project manager, product owner, or UI/UX designer—just developers. Seeing the opportunity to lead in the absence of a project manager, I volunteered to take on the role. It felt like a perfect challenge to organize the team and drive the project forward.</p><p>I kicked off the project by planning the initial meeting: setting an agenda, defining each team member’s tasks, and ensuring we were aligned on the outcomes. Thanks to this preparation, the meeting was productive, and we defined our MVP features and roles for the upcoming sprint.</p><h4>\n  \n  \n  Building the Product Backlog\n</h4><p>In sprint 2, I took on the responsibility of creating a Product Backlog, which is usually handled by a Product Owner. I broke down the MVP features into epics, user stories, and tasks, creating templates to ensure everyone was aligned. </p><p>This process made me realize how essential the backlog is as a roadmap for the team. It wasn’t easy—understanding each feature, defining clear acceptance criteria, and avoiding repetition was a challenge. But as I worked through it, I gained a clearer understanding of how each feature impacts the end user. </p><p>One instance where I exercised product thinking was when I adjusted the feature flow based on my own understanding of user needs and how they would interact with the product to ensure a smooth experience.</p><p>Unlike in more structured environments where backlogs are typically fixed, I had the freedom to adapt the “HOW” during development, tailoring the features to better suit user needs. This process was both challenging and rewarding, sharpening my product thinking and development skills.</p><p>For this project, we didn’t have to worry about a backend since we were working solely with a dish API. Therefore, I chose  as the core of the tech stack, paired with  as the build tool to ensure fast development and smooth hot reloading. </p><p>For state management, I initially considered  and , ultimately choosing  for the following reasons:</p><ul><li>Our use case was simple but involved sharing state across multiple routes (e.g., allergies and weekly menus).</li><li>Zustand provides built-in middleware for local storage persistence, which saved us time and effort in implementing our own solution.</li><li>Out of the box, Zustand offers better performance, as it updates only the relevant parts of the state without unnecessary re-renders.</li><li>Zustand is easier to scale. Should we need to manage more complex states, such as loading or error handling, or handle more sophisticated logic in the future, it can grow with the project.</li></ul><p>For this MVP, I aimed to create a clean, modern, and easy-to-navigate UI that would allow users to start using the tool immediately without the need for a sign-in or sign-up process. The goal was to minimize friction and make the user experience as seamless as possible. </p><p>A bright color palette, chosen for its ability to convey a sense of openness and clarity, sets the tone for the UI, thanks to our talented UI/UX designer. She laid the foundation for the design, and I tailored the UX to suit the users’ needs.</p><h4>\n  \n  \n  Designing the Swap Feature\n</h4><p>One of the key features I focused on was ensuring users could easily swap dishes if they weren’t happy with their selection. </p><p>Initially, I considered allowing users to search through a large list of dishes, which could be intuitive but might not be the most efficient way to save their time. Instead, I designed a modal popup that presents five recommended replacement dishes. </p><p>These dishes are randomly generated from the available dish database, ensuring no repeats with the current weekly menu. Offering five options strikes a balance between providing enough variety and avoiding overwhelming the user. I believe this design choice helps users make quick, informed decisions without limiting their options. </p><p>Here’s the UI in action:<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fmedia4.giphy.com%2Fmedia%2Fv1.Y2lkPTc5MGI3NjExbHkzY3h0Z3o0eDl1eG92MDAwMzRvcmtycHc0eGc5eTVidmtkbjVtaSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw%2FVJlquVE33e0io1POhP%2Fgiphy.gif\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fmedia4.giphy.com%2Fmedia%2Fv1.Y2lkPTc5MGI3NjExbHkzY3h0Z3o0eDl1eG92MDAwMzRvcmtycHc0eGc5eTVidmtkbjVtaSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw%2FVJlquVE33e0io1POhP%2Fgiphy.gif\" width=\"480\" height=\"230\"></a></p><h3>\n  \n  \n  Overcoming Challenges and Delivering the Project\n</h3><p>Just two sprints into the project, our team faced some challenges when two members became less active, and our UI/UX designer, who was also expected to contribute to development, had to step back. </p><p>By the time we were ready to dive into the development phase, it was just me. Although I briefly considered quitting, I decided to push forward and deliver the project on my own. </p><p>I focused on core feature development and refining the user experience. Despite the challenges, I was able to successfully complete the product!</p><p>I shared my full journey on Twitter.</p><p>\n\n  // Detect dark theme\n  var iframe = document.getElementById('tweet-1890735259190440170-433');\n  if (document.body.className.includes('dark-theme')) {\n    iframe.src = \"https://platform.twitter.com/embed/Tweet.html?id=1890735259190440170&amp;theme=dark\"\n  }\n\n\n\n </p><p>Special thanks to @numulaa for laying the foundation for the design, and you can find her work here on <a href=\"https://github.com/numulaa\" rel=\"noopener noreferrer\">GitHub</a>.</p><p>I’m excited to share that this project is open source! You can explore the code, contribute, or just check it out on <a href=\"https://github.com/chloezhoudev/schedulicious\" rel=\"noopener noreferrer\">GitHub</a>. If you found it useful or interesting, feel free to give it a star ⭐—it would mean a lot!</p><p>Try the live <a href=\"https://schedulicious.vercel.app/\" rel=\"noopener noreferrer\">demo</a> or watch the YouTube walkthrough to see it in action! </p><p>💬 Got feedback? I’d love to hear your thoughts! Feel free to open an issue on GitHub or drop me a message. </p><p>I’ll continue improving and iterating on it—stay tuned for updates! 🚀</p>","contentLength":6492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Leverage IP Address Ping for Better Network Management","url":"https://dev.to/swiftproxy_residential/how-to-leverage-ip-address-ping-for-better-network-management-1lil","date":1739777869,"author":"Swiftproxy - Residential Proxies","guid":1669,"unread":true,"content":"<p>Ever wondered why some websites load faster than others, or why your video calls sometimes lag? The culprit often comes down to your ping—an essential measure of network performance. Let’s break it down.</p><h2>\n  \n  \n  Understanding Ping and Its Importance\n</h2><p>Ping tests the availability of devices on a network and measures the time it takes to send data from your device to a target server and back. It's an indispensable tool for diagnosing network problems and gauging connection speed and stability. \nDespite its simplicity, ping is a powerhouse. Whether you're using Windows, macOS, or Linux, you can run a ping test. And there’s no need to worry about complicated software—ping can also be performed via scripts or online services. But let’s dive into the ways you can use it effectively.</p><h2>\n  \n  \n  Different Methods for Running a Ping Test\n</h2><p>Ping works by sending Internet Control Message Protocol (ICMP) packets to an IP address or domain and waits for a reply. This exchange measures round-trip time, providing insights into network performance. Here’s a quick look at how to run ping tests:\nThis is the most basic, yet powerful option. Available on almost every operating system, a simple command (ping [IP address]) will give you an instant snapshot of your connection. You can tweak it for more detailed results—like packet size or retry counts—based on your needs.\nFor those who need more than just a ping, there are a variety of network tools. These offer additional features like route tracing and port scanning, along with a user-friendly interface for analyzing results over time.\nIf you’re running multiple tests across devices or need to automate pinging, scripts (in Python or Bash) can help. This is especially useful for handling large-scale testing or collecting data for analysis.\nFor the non-technical crowd, online ping tests are a free, convenient option. No software installation required—just visit a website, enter your target address, and let the test run.</p><h2>\n  \n  \n  Why Regular Ping Checks Matter\n</h2><p>Ping isn’t just for the pros. Whether you’re troubleshooting slow connections or checking if your network is up to snuff, a ping test can provide crucial insights.<strong>Diagnosing Connection Speed</strong>\nPing can be the first indicator of network issues. High ping values can signal problems like network congestion or weak connectivity. It’s especially helpful when slow-loading websites or video lags are giving you headaches. If your ping is high, that’s where to start.\nGot trouble accessing a specific website? A ping test can tell you whether it’s your connection or a server issue. Low ping means the server’s fine—high ping or packet loss? That’s on you.<strong>Speeding Up Downloads and Streaming</strong>\nMany times, websites offer multiple download mirrors or CDNs. A quick ping test will show you which one is the fastest, so you can get the best connection possible.\nNot all internet service providers are created equal. If you’re getting slow speeds or constant disconnects, use ping to test and compare different ISPs. It’s a simple, no-nonsense way to make an informed decision.</p><h2>\n  \n  \n  How Many Ping Packets Should You Send\n</h2><p>Ping tests can be as brief or as thorough as you like. The number of packets you send will impact the depth of your analysis. For a quick check, 3 to 10 packets should give you a decent idea of the connection. If you’re troubleshooting network issues, consider sending 20 to 50 packets to get a better picture of fluctuations in response time. For server testing or gaming performance, you may want to send 50 to 100 packets to understand the overall stability.\nBe mindful, though—more packets mean a longer test. Too many on a slow connection might overload the network, so tailor your test based on your needs.</p><h2>\n  \n  \n  How to Conduct a Ping Check\n</h2><p>Ready to test your connection? Here’s how to use our easy online tool:</p><ol><li> Visit our ping checker page and type in the target address.</li><li> Choose the server you want to ping.</li><li> Choose how many packets to send.</li><li> Get results in a minute (or more, depending on packets).</li></ol><p>Here’s what the numbers really mean when you check your ping.</p><ul><li> The fastest reply from the server. Lower is better.</li><li> The usual time it takes to establish communication.</li><li> The longest time taken. Long delays? Time for troubleshooting.\n</li><li> Ideal. No interruptions in your connection.</li><li> You’ve got problems. Even a small amount can affect performance, especially for gaming or video calls.</li></ul><ul><li> Fast. Perfect for gaming or HD streaming.</li><li> Good for most activities.</li><li> Noticeable delays, especially in games or calls.</li><li> Major lag, disconnections, or poor performance.</li></ul><p>Ping tests can reveal issues, but they don’t always tell the whole story. If your ping is high or you’re getting packet loss, it’s time to look at other factors like your router, cables, or even your ISP. The results of a ping test should guide you, but for serious issues, don’t hesitate to call in the pros.</p><p>Regularly testing your ping, including performing an <a href=\"https://www.swiftproxy.net/?ref=devto\" rel=\"noopener noreferrer\">IP address</a> ping, isn’t just for troubleshooting. It’s a useful tool for maintaining a smooth and efficient network, ensuring optimal performance whether you’re working from home, gaming, or streaming. The next time your connection feels off, check your IP address ping to address any issues.</p>","contentLength":5288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In which programming language are you working on!","url":"https://dev.to/develooper_nand_8b94c9e5d/in-which-programming-language-are-you-working-on-3pge","date":1739777839,"author":"Develooper nand","guid":1668,"unread":true,"content":"<p>Reflecting on my programming journey, I realize it has been a transformative experience filled with challenges, learning, and growth. It all began a few years ago when I stumbled upon a coding tutorial while searching for ways to automate mundane tasks. Little did I know that this would spark a passion that would shape my career and personal projects.</p>","contentLength":353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To-do: the wrong approach - again...(localStorage)","url":"https://dev.to/codecara/to-do-the-wrong-approach-againlocalstorage-5b3h","date":1739777586,"author":"CodeCara","guid":1667,"unread":true,"content":"<p><em>(Note: this should have been posted on 16/02/25)</em></p><h2>\n  \n  \n  Things I've worked on/completed since my last post…\n</h2><p>I’ve been working on <strong>implementing local storage for the To-do project.</strong></p><p>: trying to go back to saved files in VS code, creating a new branch to try something for localStorage - <strong>trying to be careful with resets etc as this caused me a long time delay earlier in the project</strong>. I only know the parts of git that I need to use - i.e. I’m not an expert</p><p><strong>I again had the issue of having unexpected results when I added a parameter to a function, but forgot to add the corresponding one to the function when attached to an event listener elsewhere</strong>. I only found this error by console logging the element I was trying to manipulate and seeing that the reference to it in the debugger was a different elements, so obviously this displaces things somehow.</p><p><strong>Trying to implement code to add DOM elements for local storage was hard</strong> - I tried to turn my exisiting code into a function to use, which casued many scoping problems, which I was struggling to fix for a while.</p><p>As I was experimenting with things trying to implement localStorage, <strong>I realised that my DOM was being rendered using an array of stored DOM elements (whose data was previously populated by the data array), whereas I would be using a data object for local storage</strong>.  I realised that this was probably <strong>a fundamental design flaw(!)</strong>, but I just did it in the first way that came to me initially, not taking local storage into account as I had no experience with that previously.   </p><p><strong>I decided that I needed to alter my current structure to build the DOM  from a data object, rather than  building from an object containing DOM elements</strong>.  This was tricky at first, but I eventually managed to do it. I then had a number of issues with removing the correct childNodes, which was frustrating, but the solution was to just step back and go through everything slowly with the debugger.</p><p><strong>I (sort of) figured out how to set/get things from local storage and how to serialize and de-serialize the data</strong> (converting to strings to save to local storage and then converting those strings back to objects when necessary).</p><p><strong>I had a minor issue with some IIFEs, but just did some experimenting with my own basic functions to figure out the answer</strong>, which I got in the end.</p><p>As I completely took time pressure off myself, <strong>I am not as phased now when something goes wrong and am just working through it bit-by-bit</strong> - thankfully, I do not find ‘going wrong’ as overwhelming as I did earlier in the course.</p><h2>\n  \n  \n  Things I've learnt/need to improve on...\n</h2><p>Even though it caused me a lot more problems which I needed to fix, <strong>I am glad that figured out that the way I had built my DOM initially was probably something to avoid in future</strong> as it seemed not to be the ideal way when it comes to dealing with local storage, so hopefully I can keep this in mind for future projects.</p><h2>\n  \n  \n  Plan for the forthcoming week...\n</h2><p><strong>I  to finish local storage</strong>, but we’ll see…</p><p>At the moment of writing, I am trying to now fix my edit and delete function for the new structure I implemented as detailed above.</p>","contentLength":3128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"React vs Angular – Which One To Choose in 2025?","url":"https://dev.to/pagepro_agency/react-vs-angular-which-one-to-choose-in-2025-44ec","date":1739777389,"author":"Chris Lojniewski","guid":1666,"unread":true,"content":"<h2><strong>Angular and React in Web Development</strong></h2><p>Some people say a comparison of Angular and React for front-end development is pointless. While both are made with JavaScript, , and <strong>Angular is a TypeScript-based framework</strong>.&nbsp;</p><p>However, I think you can and even should compare them, as both can deliver great apps, and support even the most demanding digital business objectives. Both had pretty big updates in 2024, which made comparing them even more important. So today we will go over the <a href=\"https://pagepro.co/blog/7-reasons-why-you-should-be-using-react/?swcfpc=1\" rel=\"noopener noreferrer\">benefits of React.js</a> and Angular. I’ll make a classic head-to-head comparison to <strong>help you decide if you should use Angular or React</strong>.</p><p>React is  developed at Facebook (now Meta) and released in 2013. This open-source project has&nbsp;gained popularity for its <strong>ability to build fast, responsive, and beautiful user interfaces</strong>. With its component-based architecture, React also allows developers to efficiently create reusable UI elements, <strong>making it easier to manage complex applications</strong>.&nbsp;</p><p>It combines development simplicity (thanks to small pieces of code called React components) and a strong focus on user experience. React is the <strong>View layer of the MVC model</strong>, which gives you almost total freedom in choosing Model and Controller libraries. <strong>React comes with a set of browser extensions and developer tools.</strong></p><p><strong>The library’s&nbsp;evolution continues with React 19</strong>, which introduces features like Actions for handling pending states, new hooks for state management, and enhanced Server Components for better server-side rendering and static site generation.</p><h3><strong>Disadvantages and Advantages of React</strong></h3><div><table><tbody><tr><td colspan=\"1\" rowspan=\"1\"><p>The high pace of development</p></td></tr></tbody></table></div><p>Facebook isn’t the only company taking advantage of React and its possibilities. React applications and websites work for all companies, no matter the size.</p><ul><li><p><strong>Single-Page Applications (SPAs)</strong> – React’s virtual DOM and component-based structure make it ideal for SPAs that require fast updates without full-page reloads. Example: </p></li><li><p><strong>Mobile Development (React Native)</strong> – React’s ecosystem includes React Native, allowing developers to build cross-platform mobile applications with a shared codebase. Example: </p></li><li><p> – React’s reusability and ecosystem support (e.g., with Next.js for SEO) make it useful for e-commerce sites. Example: </p></li></ul><p>Angular is  created by Google for building web applications. While React is based on JavaScript, . And unlike React, Angular is a full-fledged  framework (Model-View-Controller) so once you learn it well, you won’t need other solutions.</p><p>Angular is an  and a result of rewriting AngularJS, which was the first version of the framework.</p><p>: Google&nbsp;stopped supporting&nbsp;AngularJS on the 31st of December 2021 and support fully further Angular development:</p><blockquote><p><strong>We’re incredibly proud of the work that has been done with AngularJS and the way it evolved into its successor, Angular, which empowers an ecosystem of millions of developers.</strong></p><p><cite>Mark Thompson, Developer Advocate on Angular Team at Google</cite><a href=\"https://blog.angular.io/discontinued-long-term-support-for-angularjs-cc066b82e65a\" rel=\"noopener noreferrer\"></a></p></blockquote><h3><strong>Disadvantages and Advantages of Angular</strong></h3><p>You already have learned about React’s pros and cons, so it’s time to find out why it’s a great idea to choose Angular (or not).</p><div><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Simpler Development with Standalone Components</p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Material Design-like interface</p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Seamless updates thanks to Angular CLI</p></td></tr></tbody></table></div><p>Angular applications and websites are used across numerous industries and organizations.</p><ul><li><p><strong>Enterprise-Grade Applications</strong> – Angular offers opinionated structure, TypeScript support, and built-in features (like dependency injection) making it great for enterprise applications. Example: </p></li><li><p><strong>Progressive Web Apps (PWAs)</strong> – Angular provides built-in support for PWAs, and is mobile-friendly for web applications. Example: </p></li><li><p><strong>Large-Scale SaaS Platforms</strong> – Framework’s&nbsp;built-in features like HTTP services, state management, and CLI tools make it suitable for large SaaS platforms. Example: </p></li></ul><p>Now, let’s look at how Angular and React compare to one another.</p><h2><strong>Key Differences Between Angular and React: Head-to-Head Comparison</strong></h2><div><table><tbody><tr></tr><tr></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Full backward compatibility</p></td></tr><tr></tr></tbody></table></div><h2><strong>React vs Angular: Library vs Framework</strong></h2><p>It might seem like <strong>Angular is a better option, as it’s a full-stack MVC</strong> (Model-View-Controller)&nbsp;framework that doesn’t require any additional libraries to be complete, which is not like React. Although it is dependent on third-party additions, it’s not necessarily a disadvantage. <strong>React gives you almost unlimited possibilities</strong> if it comes to building user experience, and what’s more, you can also be quite flexible in choosing which libraries you want to work with.</p><h4>\n  \n  \n  React For Projects Requiring High Flexibility and Custom User Experience\n</h4><p>React outperforms Angular when <strong>flexibility and customizability</strong> are top priorities. As an independent view library, React allows for the creation of dynamic user experiences with the choice to integrate a variety of libraries according to project needs. It’s ideal for projects where <strong>building a unique user experience is crucial</strong>, and React developer tools let teams choose what’s best for each aspect of their application.</p><h4>\n  \n  \n  Angular For Projects Needing a Full-Stack Solution\n</h4><p>Angular can be the better choice if you’re looking for <strong>a comprehensive, out-of-the-box solution</strong>. It provides a wide array of built-in functionalities without the need for additional libraries. Angular is particularly suitable for projects where having <strong>a cohesive framework that covers everything from frontend to backend</strong> (assuming you are using it in conjunction with other full-stack capabilities) is advantageous.</p><h2><strong>Angular vs React: Performance</strong></h2><h3><strong>DOM Handling: Real vs. Virtual DOM</strong></h3><p>The fundamental difference in how these technologies handle Document Object Model (DOM) updates impacts performance.</p><ul><li><p>Angular uses a real DOM, meaning every update requires scanning and mutating the entire HTML tree. This works well for applications that don’t change frequently but can slow down performance in highly dynamic UIs. Angular 19 introduces built-in control flow, optimizing template rendering, and improves server-side rendering, which is more efficient for enterprise-scale applications.</p></li><li><p>Meanwhile, React uses a virtual DOM, which only updates the changed elements, making UI updates much faster. This optimization allows React apps to perform better in scenarios where frequent, real-time data updates occur. React 19 boosted UI responsiveness with Actions, which streamline asynchronous rendering, and Server Components, which shift computation to the backend.&nbsp;</p></li></ul><p>Some features of React introduce  for UI responsiveness:</p><ul><li><p>, which allows React to prepare multiple UI versions simultaneously, preventing the app from freezing during heavy rendering tasks. It also helps ensure smooth UI transitions even under high load.</p></li><li><p> implements a priority-based update system, meaning critical UI updates happen first. Pausing and resuming rendering tasks it enabled reduces unnecessary delays.</p></li></ul><ul><li> is a modern rendering engine introduced to reduce build times and app size. It supports the lazy loading of components, improving performance in large applications.</li></ul><h3><strong>React vs Angular: Data binding</strong></h3><p>Angular uses&nbsp;, though one-way is also possible. Two-way data binding means that whenever you change any element of the interface, your model state changes automatically, too.</p><p>React uses , which renders the change in the interface model only after the model state is updated first. In one-way data binding, <strong>the data flow is unidirectional</strong>, which gives developers more flexibility and control over mobile and web apps. Whenever you change the UI components, <strong>the model state stays the same with no changes</strong>.&nbsp;</p><h4>\n  \n  \n  Angular For Applications with Rare Updates\n</h4><p>Angular fits projects that <strong>focus on single-page applications</strong> where the content does not change very often. Since&nbsp;Angular uses the real DOM, updates are handled efficiently in a structured manner. Angular excels&nbsp;for applications where the <strong>data structure is relatively static</strong>, and stability and reliability matter more than fast content changes.</p><h4>\n  \n  \n  React For Highly Dynamic Applications with Frequent Content Updates\n</h4><p>React library is better suited for <strong>applications that require frequent updates</strong> and dynamic content changes. React’s virtual DOM implementation allows it to quickly identify changes between the current and previous states of the HTML, updating only what’s necessary. This results in <strong>improved performance and faster loading times</strong>, making React faster than Angular and ideal for interactive user interfaces where the data or content changes regularly.</p><h2><strong>Angular vs React: Learning curve</strong></h2><p>Since <strong>React is a lightweight UI library</strong>, its learning curve is much simpler than in Angular. The list of things you have to get familiar with is quite short: JSX, a router library, and a state management library. Also, <strong>using React requires knowledge of writing components, managing internal state, and using props</strong>.&nbsp;</p><p>As it’s a complete MVC framework, <strong>it’s not as easy to learn Angular</strong>. The list of things to study is much longer than for React.js:</p><ul></ul><p>And that’s just a start, which means <strong>Angular has a steeper learning curve</strong> for most developers.&nbsp;</p><p>Compared to Angular, <strong>it’s easier to learn React</strong>, as it has a lower learning curve and more resources available, as we will find out in the next section.</p><h2><strong>React vs Angular: Popularity</strong></h2><p>The popularity of any particular Javascript framework or web technology can be a great <strong>source of information and insights</strong>. Looking at these stats will help you answer questions like:</p><ul><li><p>Is this a serious technology?</p></li><li><p>Is there a demand for this technology?</p></li><li><p>Will it be easy to find and hire developers?</p></li><li><p>How big is the community?</p></li><li><p>What kind of help can I expect from the community?</p></li></ul><p><strong>ReactJS is much more popular than Angular</strong>. Since 2019 the number of questions about Angular has decreased, and React had a small drop since 2022.</p><p><strong>Source: Stack Overflow Trends</strong></p><h3><strong>Most Popular Web Frameworks and Technologies Stack Overflow 2024 Survey</strong></h3><p>For the last few years,  among the most common web frameworks and technologies according to the Stack Overflow Survey, while .</p><p><strong>Source: Stack Overflow 2024 Survey</strong></p><h3><strong>Angular vs ReactJS on GitHub</strong></h3><p>React.js, without a doubt. While the Angular community is strong, <strong>React is a popular choice</strong>. Its developers are more numerous thanks to the library’s widespread adoption, which means a larger pool of experienced devs and more community-driven solutions available.</p><h2><strong>React vs Angular: Templates</strong></h2><p>Angular framework uses templates based on an extended version of HTML with Angular directives. <strong>The syntax of those directives is complex and sophisticated,</strong> making it one more thing to learn from the basics.</p><p>With React all you need is JavaScript knowledge. A JSX is a genius component made from markup and JavaScript logic in the same file. Thanks to the use of XML-like language, <strong>you can write your markup in your JavaScript code</strong>, so everything is in one place, and the code completion works better.</p><h4>\n  \n  \n  React For Developer Experience and Productivity\n</h4><p><strong>React and JSX might be preferred for their simplicity and the unified way of handling logic and markup</strong>. The ability to use JavaScript for both can make development faster and more intuitive, especially for those already proficient in JavaScript.</p><h4>\n  \n  \n  Angular For Structured Development and Complex Applications\n</h4><p><strong>Angular could be the better choice for projects that benefit from a clear separation between templates and logic</strong>. Its sophisticated templating system is powerful for building complex, large-scale applications, albeit with a steeper learning curve.</p><h4>\n  \n  \n  React For Flexibility vs. Structure\n</h4><p><strong>If your project values flexibility and a less opinionated approach, React’s JSX is the winner</strong>. For projects requiring a more structured and comprehensive framework, Angular’s template system comes out ahead.</p><h2><strong>Angular vs React.js: Testing</strong></h2><p> (sometimes together with Enzyme – a JavaScript testing utility). Jest has a powerful mocking library, doesn’t require any configuration, and is included in every React project. However, nowadays <strong>using react-testing-library is more common and practiced</strong>.</p><p> downloads all the tools you need for testing with a Jasmine test framework. Angular has multiple features, such as isolation of the unit of code, and was created with testability in mind, but <strong>many Angular developers find the output from Jasmine rather difficult to read</strong>.</p><h4>\n  \n  \n  React For Ease of Setup and Use\n</h4><p>React might have an edge due to Jest’s zero-configuration setup and the intuitive approach of the React Testing Library. This combination <strong>makes it easier for developers to write and maintain tests</strong>, especially for those who are new to testing.</p><h4>\n  \n  \n  Angular For a Behavior-Driven Development Approach\n</h4><p>Angular with Jasmine is powerful for developers who prefer a more traditional, behaviour-driven approach to writing tests. <strong>It offers comprehensive testing capabilities</strong>, albeit with a steeper learning curve in interpreting test results.</p><h4>\n  \n  \n  React For Developer Preference and Community Support\n</h4><p>React’s testing tools are widely adopted and have strong community support, which can be advantageous for finding resources and troubleshooting issues. Angular’s Jasmine also has good support, but <strong>its complexity might be a barrier for some developers</strong>.</p><h4>\n  \n  \n  Angular For Integration with Framework\n</h4><p>Angular’s testing tools are designed to work seamlessly with its architecture, making it a solid choice for testing Angular-specific features. <strong>React’s tools, while flexible, are more generalized and not tied to any specific architectural patterns.</strong></p><h2><strong>React or Angular: Which to Choose for Your Project?</strong></h2><p>Deciding between Angular and React for web development projects is not a straightforward choice, as both frameworks are capable of delivering high-quality applications. <strong>Your final choice should depend on the specific needs of the project, the team’s expertise, and the desired development experience.</strong></p><p><strong>Angular is best suited to build complex applications</strong>, as it&nbsp;provides a structured environment that includes everything developers need. Its use of TypeScript improves code quality and maintainability and an opinionated approach to architecture ensures consistency across projects. This makes Angular better than React for enterprise-level applications requiring a standardized development process.</p><p>Meanwhile, <strong>React stands out for its flexibility and simplicity of integrating JavaScript logic with the UI</strong>. The component-based architecture of React lets developers reuse UI components, which promotes a more efficient&nbsp;development process. React’s virtual DOM implementation addresses performance challenges effectively, making it ideal for applications that require frequent dynamic updates.</p><ul><li><p>: React’s virtual DOM makes it well-suited for applications where performance and responsiveness are critical.</p></li><li><p>: It allows us to tailor the frontend experience precisely to our project’s needs without being constrained by a framework’s limitations.</p></li><li><p><strong>Cross-Platform Development</strong>: With React Native, we can extend our web development expertise into building native mobile applications, assuring a unified development experience.</p></li><li><p>: The React ecosystem is vast and keeps growing. This provides us with a wealth of libraries, tools, and resources that improve our development capabilities and lets us stay at the forefront of web and mobile app development.</p></li><li><p>: React is designed to be simple, which makes it a pleasure to work with. Our developers can focus on creating innovative solutions instead of wrestling with the framework.</p></li></ul><p>To sum up, while Angular is a powerful framework for building sophisticated web applications, <strong>performance capabilities, flexibility, and cohesive development experience</strong>.&nbsp; This is especially true when transitioning between web and mobile app development. Thanks to that, we can deliver exceptional applications that meet our client’s needs while staying true to our expertise.</p>","contentLength":15738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Pre and Post Middleware in Mongoose with NestJS & TypeScript 🚀","url":"https://dev.to/abhivyaktii/understanding-pre-and-post-middleware-in-mongoose-with-nestjs-typescript-3acf","date":1739777284,"author":"Abhinav","guid":1665,"unread":true,"content":"<p>Mongoose provides powerful  and  middleware hooks that allow you to execute logic before or after an operation (e.g., save, find, remove). These hooks are useful for <strong>data validation, transformation, logging, security</strong>, and more 🔐.</p><p>During my recent endeavor to migrate the entire existing codebase to NEST, I discovered a set of tools that proved invaluable.</p><p>In this blog, we'll explore  in Mongoose using .</p><h2><strong>What is Middleware in Mongoose?</strong> 🤔\n</h2><p>Middleware (also called hooks) in Mongoose allows you to run functions  or  certain operations on documents.</p><p>Middleware can be applied to:</p><ul><li>, , </li><li>, , , </li></ul><h2><strong>Setting Up NestJS with Mongoose</strong> ⚡\n</h2><p>First, install Mongoose and its TypeScript types in a NestJS project:</p><div><pre><code>npm  @nestjs/mongoose mongoose\nnpm  @types/mongoose\n</code></pre></div><p>Next, let's create a  model with pre and post middleware.</p><h2><strong>Using  Middleware in NestJS</strong> 🔒\n</h2><h3><strong>Example 1: Hashing Password Before Saving</strong></h3><p>A common use case for  middleware is <strong>hashing a password before saving it</strong> to the database.</p><div><pre><code></code></pre></div><ul><li>The  middleware runs  saving a user.</li><li>It checks if the  field has been modified.</li><li>If modified, it hashes the password using .</li><li>Finally, the  function is called to continue the save process.</li></ul><h2><strong>Using  Middleware in NestJS</strong> 📣\n</h2><h3><strong>Example 2: Logging User Creation</strong></h3><p>We can use  to log when a new user is created.</p><div><pre><code></code></pre></div><ul><li>The  middleware executes  a user is successfully saved.</li><li>It logs the created user's username.</li></ul><h2><strong>Using  Middleware for Query Operations</strong> 🔍\n</h2><h3><strong>Example 3: Auto-Populating a Reference Before Querying</strong></h3><p>Let's say a  model has an  field referencing the  model. We can automatically populate the  field before fetching a post.</p><div><pre><code></code></pre></div><ul><li> runs before executing a  query.</li><li> ensures that the  field is .</li></ul><h2><strong>Using  Middleware for Query Operations</strong> 📊\n</h2><h3><strong>Example 4: Logging After a User is Found</strong></h3><p>We can use  middleware to log user retrievals.</p><div><pre><code></code></pre></div><ul><li> runs  a document is found.</li><li>If a user is found, it logs their username.</li></ul><h2><strong>Pre and Post Middleware for Deleting Documents</strong> 🗑️\n</h2><h3><strong>Example 5: Cleaning Up Related Data Before Deleting a User</strong></h3><p>If a user is deleted, we might want to <strong>remove their associated posts</strong>.</p><div><pre><code></code></pre></div><ul><li> runs before deleting a user.</li><li>It removes all posts associated with the user.</li></ul><p>Mongoose middleware ( and ) is a powerful tool for <strong>extending document behavior</strong> in a NestJS application. Some common use cases include:</p><p>✅ Hashing passwords before saving users<p>\n✅ Auto-populating referenced fields</p><p>\n✅ Logging events after CRUD operations</p><p>\n✅ Cleaning up related data before deleting documents  </p></p><p>By leveraging these hooks, you can enhance <strong>data integrity, security, and performance</strong> in your NestJS applications.</p><p>Would you like to see middleware used in a real-world NestJS app? Let me know in the comments! 🚀</p>","contentLength":2612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How a Proxy for Using ChatGPT Can Improve Privacy","url":"https://dev.to/swiftproxy_residential/how-a-proxy-for-using-chatgpt-can-improve-privacy-3p0c","date":1739777164,"author":"Swiftproxy - Residential Proxies","guid":1664,"unread":true,"content":"<p>Ever tried accessing ChatGPT only to find that your region has thrown up a wall? Frustrating, right? But don’t worry, there’s a workaround. With a proxy for using ChatGPT, you can easily bypass government restrictions, protect your privacy, and access ChatGPT seamlessly. Whether you're using Windows’ built-in proxy feature or the more advanced Proxifier app, I’ll show you exactly how to set things up. Let’s break it down.</p><h2>\n  \n  \n  Configuring a Proxy on Windows for ChatGPT Access\n</h2><p>It’s easier than you think to get started with a proxy on Windows. Here’s how you do it:</p><ol><li><p> Go ahead and type “Proxy” into the Windows search bar. Select either “Change proxy settings” or “Change manual proxy server settings,” based on what pops up.</p></li><li><p> In the settings window, look for the “Use a proxy server” option and click the “Set up” button next to it.</p></li><li><p> Flip the “Use proxy server” switch to “on,” and input the address and port number of the proxy server. If there are sites you’d prefer not to route through the proxy (like YouTube or Facebook), add them to the exceptions field.</p></li><li><p> Click “Save,” and you’re good to go. Your system will now route internet traffic through the proxy, including ChatGPT.</p></li><li><p> Open your browser, and you’ll be prompted to enter your proxy username and password. Once you do, you’ll have access to ChatGPT.</p></li><li><p> If you ever need to disconnect, just return to the proxy settings and flip the “Use a proxy server” switch to “off.”</p></li></ol><h2>\n  \n  \n  Using Proxifier for More Control\n</h2><p>Looking for more customization? Proxifier is a great option. It’s a tool that gives you control over which traffic goes through the proxy and which doesn’t. This way, you can route only ChatGPT through the proxy, keeping the rest of your internet traffic untouched.</p><p>Here’s how to set up Proxifier:</p><ol><li><p> Open the app and click on the “Proxy Servers” icon.</p></li><li><p> Click “Add…”, choose the connection protocol (HTTPS, SOCKS4, or SOCKS5), then input your proxy’s address and port.</p></li><li><p><strong>Authenticate (If Needed):</strong> If your proxy requires authentication, check the “Enable” box and enter your credentials.</p></li><li><p> Once everything is filled out, click “OK” to confirm.</p></li></ol><h2>\n  \n  \n  Configuring Proxifier to Work with ChatGPT\n</h2><p>Now that you’ve added the proxy, let’s apply it to your browser for seamless access to ChatGPT:</p><ol><li><p><strong>Create a Proxification Rule:</strong> In the “Proxification Rules” section, click “Add…”. Name the new rule, then in the “Applications” field, specify the executable file of the browser you want to use (e.g., Chrome.exe or Edge.exe). You can list multiple apps, separated by semicolons.</p></li><li><p> Under “Actions,” select the <a href=\"https://www.swiftproxy.net/?ref=devto\" rel=\"noopener noreferrer\">proxy</a> you just added, and hit “OK.”</p></li><li><p> Your new rule will appear under “Proxification Rules.” You can activate or deactivate it by checking or unchecking the box next to the rule.</p></li></ol><h2>\n  \n  \n  Why Free Proxies Aren’t the Best Option\n</h2><p>Free proxy servers often come with their own set of problems. You might experience slower speeds, connection drops, or even the proxy being blocked if too many users are accessing it at once. To avoid these issues, it’s best to opt for a paid proxy that offers a unique IP address for your use. This will ensure faster speeds, more reliable connections, and greater security when accessing ChatGPT.</p><p>Accessing ChatGPT from restricted regions doesn’t have to be a headache. With either Windows’ built-in proxy feature or Proxifier, you can bypass any blocks and get right back to work. Just remember, free proxies often bring more trouble than they’re worth. For smooth, reliable access, it’s worth investing in a paid proxy.</p>","contentLength":3634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Full-Stack Developer Roadmap👨‍💻","url":"https://dev.to/franklin_mn/full-stack-developer-roadmap-1mnf","date":1739776789,"author":"Franklin MN","guid":1663,"unread":true,"content":"<p>Become job ready as a full stack Java devevloper with this 4–6 months roadmap, the below roamap covers Frontend, Backend and deployment.</p><ul><li>Duration: 4–6 months (2–4 hours/day)</li><li>Key Skills: Backend, Frontend, Deployment, Testing</li></ul><h2>\n  \n  \n  🎯Phase 1: Java (1.5&nbsp;Months)\n</h2><ul><li>Variables, Data Types, Operators</li><li>Loops (for, while, do-while)</li></ul><p><strong>⌛Week 2: Object-Oriented Programming (OOP)</strong></p><ul><li>Classes, Objects, Constructors</li><li>Encapsulation, Inheritance, Polymorphism</li></ul><p><strong>⌛Week 3: Collections &amp; Exception Handling</strong></p><ul><li>Lists, Sets, Maps (ArrayList, HashMap, TreeSet)</li><li>Try-Catch Blocks, Custom Exceptions</li></ul><p><strong>⌛Week 4: File Handling &amp; JDBC</strong></p><ul><li>Reading/Writing Files (FileReader, BufferedReader)</li><li>Connecting Java with MySQL (JDBC)</li></ul><h2>\n  \n  \n  🎯Phase 2: Spring &amp; Spring Boot (2&nbsp;Months)\n</h2><ul><li>Dependency Injection (DI) &amp; Inversion of Control (IoC)</li></ul><p><strong>⌛Week 6: Spring Boot Basics</strong></p><ul><li>Setting up Spring Boot Project</li><li>RESTful APIs (@GetMapping, @PostMapping)</li></ul><p><strong>⌛Week 7: Spring MVC &amp; JPA</strong></p><ul><li>MVC Architecture (Controller, View, Model)</li><li>Spring Data JPA (@Entity, @Repository, @Service)</li><li>CRUD Operations (Create, Read, Update, Delete)</li></ul><p><strong>⌛Week 8: Spring Security &amp; Authentication</strong></p><ul><li>Role-Based Access Control</li></ul><h2>\n  \n  \n  🎯Phase 3: Frontend (JavaScript + React) (1.5&nbsp;Months)\n</h2><p><strong>⌛Week 9: JavaScript, HTML &amp; CSS Basics</strong></p><ul><li>HTML Elements, Forms, Tables</li><li>CSS Flexbox, Grid, Animations</li><li>JavaScript (let, const, function, fetch())</li></ul><p><strong>⌛Week 10: JavaScript Advanced Concepts</strong></p><ul><li>ES6+ Features (Arrow Functions, Promises, async/await)</li><li>DOM Manipulation (document.getElementById(), addEventListener())</li></ul><ul><li>State Management (useState, useEffect)</li><li>React Router (Navigation)</li></ul><ul><li>Context API, Redux (State Management)</li><li>Integrating React with Spring Boot (Fetching Data)</li></ul><h2>\n  \n  \n  🎯Phase 4: Git, Integration, Deployment &amp; Testing (1&nbsp;Month)\n</h2><p><strong>⌛Week 13: Git &amp; Version Control</strong></p><ul><li>Git Basics (git init, git commit, git push)</li><li>GitHub &amp; Branching (git checkout, git merge)</li></ul><p><strong>⌛Week 14: Backend &amp; Frontend Integration</strong></p><ul><li>Connecting React Frontend with Spring Boot Backend</li></ul><ul><li>Deploying Spring Boot API (Heroku, AWS, or DigitalOcean)</li><li>Deploying React App (Netlify, Vercel, or Firebase)</li></ul><p><strong>⌛Week 16: Testing &amp; Debugging</strong></p><ul><li>JUnit &amp; Mockito (Unit Testing)</li></ul><p>🍎 After completing this roadmap, build a Resume with some projects to land on a job.\n🍓 Try for remote jobs or start freelancing.<p>\n🍇 If you are good enough of the topics you have learned, you can start teaching to others.</p>\n🍊 After this, you can explore DevOps, Microservices, or Cloud Computing, Blockchain.</p>","contentLength":2401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Use Nuxt 3 Middleware for Authentication and Permissions","url":"https://dev.to/jacobandrewsky/how-to-use-nuxt-3-middleware-for-authentication-and-permissions-5d9b","date":1739776558,"author":"Jakub Andrzejewski","guid":1662,"unread":true,"content":"<p>Nuxt 3 is a powerful framework for building Vue.js applications, offering a streamlined way to manage routing, server-side rendering (SSR), and middleware. One of its key features is middleware, which allows developers to control access to pages and perform tasks such as authentication and permission checks. </p><p>In this article, we'll explore how to implement authentication and permissions using Nuxt 3 middleware effectively.</p><h2>\n  \n  \n  🤔 What is Middleware in Nuxt 3?\n</h2><p>Middleware in Nuxt 3 is a function that runs before rendering a page or layout. It can be used for various purposes, such as authentication, logging, or modifying requests. Middleware can be applied globally, per layout, or on specific pages.</p><p>You can read more about it <a href=\"https://nuxt.com/docs/guide/directory-structure/middleware\" rel=\"noopener noreferrer\">here</a>.</p><h2>\n  \n  \n  🟢 Setting Up Authentication Middleware\n</h2><p>To protect specific routes and ensure only authenticated users can access them, you can create an authentication middleware.</p><p>In your Nuxt 3 project, navigate to the  directory and create a new file called :</p><div><pre><code></code></pre></div><p>This middleware checks if the user is logged in by accessing the authentication state from a Pinia store. If the user is not logged in, they are redirected to the login page.</p><p>You can apply this middleware to specific pages by adding it to the  property in the page component:</p><div><pre><code></code></pre></div><p>Alternatively, you can apply it globally by naming it  and placing it in the  folder.</p><h2>\n  \n  \n  🟢 Implementing Role-Based Access Control (RBAC)\n</h2><p>In addition to authentication, you might want to restrict access based on user roles (e.g., admin, editor, user). Here’s how you can create a role-based permission middleware.</p><p>Create a new file  inside the  folder:</p><div><pre><code></code></pre></div><p>This middleware checks if the user has the required role to access certain routes. If not, they are redirected to a “Forbidden” page.</p><p>Use the middleware in specific pages that require role-based access:</p><div><pre><code></code></pre></div><p>To ensure both authentication and authorization checks are performed, you can apply multiple middleware functions to a page:</p><div><pre><code></code></pre></div><p>If you would like to learn more about Vue, Nuxt, JavaScript or other useful technologies, checkout VueSchool by clicking this <a href=\"https://vueschool.io/courses?friend=baroshem\" rel=\"noopener noreferrer\">link</a> or by clicking the image below:</p><p>It covers most important concepts while building modern Vue or Nuxt applications that can help you in your daily work or side projects 😉</p><p>Nuxt 3 middleware provides a simple yet powerful way to handle authentication and permissions in your application. By leveraging middleware, you can enforce user authentication, implement role-based access control, and ensure secure navigation within your Nuxt 3 app.</p><p>By following these steps, you can build a secure and scalable authentication system tailored to your application's needs.</p><p>Take care and see you next time!</p><p>And happy coding as always 🖥️</p>","contentLength":2717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My LeetCode Experience","url":"https://dev.to/vaibhav423/my-leetcode-experience-1jm7","date":1739776435,"author":"Vaibhav Bahuguna","guid":1661,"unread":true,"content":"<p>🔥 <strong>My LeetCode Experience: From Fibonacci to Dynamic Programming</strong> 🔥  </p><p>When I first started tackling LeetCode problems, even the basics like  and  felt like climbing a mountain. But with consistent practice and a structured approach, I began to see patterns, improve my problem-solving skills, and build confidence. Over time, I explored advanced topics like , , , , and , solidifying my understanding of data structures and algorithms.  </p><p>Here’s how I transformed my LeetCode journey from overwhelming to empowering—and how you can too!  </p><h3>\n  \n  \n  🚀 <strong>My Process for Solving LeetCode Problems</strong></h3><p>1️⃣ </p><ul><li>First, read to understand the problem.\n</li><li>Second, read to identify inputs, outputs, and edge cases.\n</li></ul><p>2️⃣ <strong>Brainstorm multiple approaches</strong></p><ul><li>Think about brute force first, then optimize.\n</li><li>Consider time and space complexity for each approach.\n</li></ul><ul><li>Visualize the flow: inputs → processing → outputs.\n</li><li>Anticipate edge cases (e.g., empty inputs, large datasets).\n</li></ul><p>4️⃣ </p><ul><li>Pseudocode or step-by-step plan before coding.\n</li><li>This helps avoid getting stuck mid-implementation.\n</li></ul><ul><li>Start with the simplest approach, then refine.\n</li><li>Keep the code clean and readable.\n</li></ul><ul><li>After solving, reflect: What worked? What didn’t?\n</li><li>Could the solution be more efficient?\n</li></ul><p>7️⃣ <strong>Explore alternative solutions</strong></p><ul><li>Look at other submissions or discuss with peers.\n</li><li>Learn from optimized solutions to improve your skills.\n</li></ul><h3>\n  \n  \n  🔥 <strong>Lessons Learned Along the Way</strong></h3><p>✅ <strong>Consistency beats intensity</strong></p><ul><li>Solving a few problems daily is better than cramming.\n</li><li>Small, consistent efforts compound over time.\n</li></ul><p>✅ <strong>Pattern recognition is key</strong></p><ul><li>Most problems fall into familiar categories (e.g., sliding window, two pointers, DP).\n</li><li>Recognizing these patterns speeds up problem-solving.\n</li></ul><p>✅ <strong>Failure isn’t the end—it’s a lesson</strong></p><ul><li>Debugging and learning from mistakes is part of the process.\n</li><li>Every error is an opportunity to grow.\n</li></ul><p>A couple of months ago, seeing a wall of errors was frustrating. Today? It’s just another puzzle to solve. Progress isn’t always about getting the right answer immediately—it’s about learning how to think.  </p><p>I used to struggle alone, thinking I had to figure out everything myself. But I’ve realized that struggling endlessly isn’t a badge of honor—. Ask for help, debug, optimize, and repeat.  </p><p>Every problem solved is a step forward, and I’m here for the grind. 💪🔥  </p><h3>\n  \n  \n  🛠️ <strong>Tools and Resources That Helped Me</strong></h3><ul><li>: Great for understanding alternative approaches.\n</li><li>: Structured problem sets and video explanations.\n</li><li><strong>Cracking the Coding Interview</strong>: A classic book for mastering DSA.\n</li><li>: Visualizing algorithms made concepts click for me.\n</li></ul><ul><li>, ranging from Easy to Hard.\n</li><li>:\n\n<ul><li>Dynamic Programming (Knapsack, Coin Change)\n</li><li>Graph Traversal (BFS, DFS)\n</li><li>Tree Algorithms (BST, Trie, Heaps)\n</li><li>Sliding Window, Two Pointers, and more!\n</li></ul></li></ul><p>LeetCode isn’t just about acing interviews—it’s about building a problem-solving mindset. Whether you’re just starting or already deep into your journey, remember: <strong>progress takes time, but every step counts</strong>.  </p><p>Keep grinding, keep learning, and most importantly, enjoy the process!  </p><p>What’s your LeetCode journey been like? Share your tips, struggles, and wins in the comments below! Let’s grow together. 💬👇  </p>","contentLength":3221,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Handle Memory Leaks in Node.js: Debugging and Optimization Techniques","url":"https://dev.to/rabindratamang/how-to-handle-memory-leaks-in-nodejs-debugging-and-optimization-techniques-4a5i","date":1739776376,"author":"rabindratamang","guid":1660,"unread":true,"content":"<p>Ever had your Node.js app suddenly crash or slow down for no apparent reason? You might be dealing with a . These sneaky issues can cause your app to consume more and more memory until it finally runs out, leading to <strong>out-of-memory (OOM) errors</strong> and performance problems.</p><p>If your app processes , like fetching logs from OpenSearch or handling millions of requests, memory management is . In this guide, we'll break down how to <strong>identify, debug, and optimize</strong> memory leaks in a way that's easy to understand—no PhD in computer science required!</p><h2><strong>1. How to Spot a Memory Leak</strong></h2><p>Before we fix anything, we need to  there’s a problem. Here are some telltale signs:</p><ul><li><strong>Memory usage keeps increasing</strong> even when your app should be idle</li><li><strong>Frequent garbage collection (GC) cycles</strong> slowing things down</li><li><strong>Your app crashes with an OOM error</strong></li><li><strong>Performance gets worse over time</strong></li></ul><h3><strong>1.1 Quick Memory Check with Process Metrics</strong></h3><p>Want a quick way to keep tabs on memory usage? Try this:</p><div><pre><code></code></pre></div><p>This logs memory usage every 5 seconds—perfect for catching unexpected spikes.</p><h3><strong>1.2 Debugging with Chrome DevTools</strong></h3><p>Chrome DevTools isn’t just for frontend debugging! You can use it to profile your Node.js app too:</p><ol><li>Start your app with :\n</li></ol><ol><li>Open Chrome and go to .</li><li>Take a  and look for objects that aren’t getting cleared.</li></ol><p>Need a deeper dive? Take a heap dump and inspect it:</p><div><pre><code></code></pre></div><p>Open the snapshot in  or  to see what's clogging up memory.</p><h2><strong>2. Debugging Memory Leaks</strong></h2><p>Found a leak? Here’s how to .</p><h3><strong>2.1 Analyzing Heap Snapshots</strong></h3><p>Heap snapshots help you find <strong>objects that should be gone but aren’t</strong>. Look for:</p><ul><li> (if using server-side rendering)</li><li><strong>Global variables holding onto data</strong></li></ul><h3><strong>2.2 Tracking Garbage Collection</strong></h3><p>To see when Node.js runs garbage collection, start your app with:</p><div><pre><code>node  your-app.js\n</code></pre></div><p>This logs , helping you spot when memory isn’t being freed.</p><h3><strong>2.3 Try  for an Easy Debugging Experience</strong></h3><p> gives a high-level overview of memory leaks and performance bottlenecks:</p><div><pre><code>npx clinic doctor  node your-app.js\n</code></pre></div><p>It generates a detailed report, so you don’t have to manually sift through logs.</p><h2><strong>3. Preventing Memory Leaks</strong></h2><p>Once your app is running smoothly, let’s !</p><h3><strong>3.1 Stream Large Data Instead of Buffering</strong></h3><p>Handling big logs or data dumps? Avoid loading everything into memory at once.</p><h4>\n  \n  \n  ❌ <strong>Bad (Buffers Entire Data in Memory)</strong></h4><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3><strong>3.2 Clean Up Event Listeners</strong></h3><p>Forgetting to remove event listeners can cause major memory leaks!</p><div><pre><code></code></pre></div><h3><strong>3.3 Use  for Better Memory Management</strong></h3><p> helps avoid memory leaks by allowing automatic garbage collection:</p><div><pre><code></code></pre></div><p>If your app eats up too much memory, set a :</p><div><pre><code>node 4096 your-app.js\n</code></pre></div><p>This prevents <strong>uncontrolled memory usage</strong> from taking down your server.</p><p>Memory leaks in Node.js can be frustrating, but <strong>they don’t have to be a mystery</strong>. By using <strong>heap snapshots, garbage collection tracking, and memory monitoring tools</strong>, you can <strong>identify, debug, and optimize</strong> memory usage.</p><ul><li>Keep an eye on memory with </li><li>Use <strong>Chrome DevTools, heapdump, and clinic.js</strong> for debugging</li><li>Optimize memory with <strong>streams, WeakMap, and event listener cleanup</strong></li><li>Set limits with </li></ul><p>By following these best practices, you’ll keep your <strong>Node.js app fast, stable, and memory-efficient</strong>. 🚀</p><p>💬 <strong>Have you ever struggled with a memory leak? Share your experience in the comments!</strong></p>","contentLength":3155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"API Debugging is Really A Piece of Cake🎂","url":"https://dev.to/apilover/api-debugging-is-really-a-piece-of-cake-3n77","date":1739775658,"author":"Wanda","guid":1649,"unread":true,"content":"<p>API debugging is a crucial skill for backend developers, playing a pivotal role in the development cycle. Even small mistakes can lead to significant issues, but with the right tools, debugging doesn't need to be difficult or time-consuming. By using the right tools, developers can debug smarter, not harder, and deliver high-quality code more efficiently.</p><p>This is where  comes in. Apidog is an all-in-one API development tool that streamlines the entire API lifecycle—from designing and testing to debugging and documenting. In this article, we’ll explore how Apidog can help backend developers debug APIs more efficiently and with ease. Plus, we’ll dive into , a plugin for IntelliJ IDEA that integrates Apidog’s powerful debugging features directly into your IDE.</p><h2>\n  \n  \n  Why API Debugging is Vital for Backend Developers\n</h2><p>APIs are the backbone of communication between services, allowing applications to exchange data. Without effective debugging, however, APIs can become fragile, leading to system failures, performance issues, or inaccurate data exchanges.</p><p>For backend developers, debugging APIs means ensuring that each request and response behaves as expected. This includes:</p><ul><li> to prevent bigger issues down the line.</li><li> to ensure they match the expected structure and return the correct data.</li><li><strong>Troubleshooting network issues</strong> like connectivity problems that affect how APIs interact with other systems.</li></ul><p>Effective debugging accelerates development, reduces downtime, and ensures that your APIs perform optimally. Let’s see how Apidog can help streamline this process.</p><h2>\n  \n  \n  How Apidog Simplifies API Debugging for Backend Developers\n</h2><p>Apidog offers a complete suite of debugging tools that integrate smoothly into your API development workflow. Whether you're debugging a single API endpoint or troubleshooting complex API interactions, Apidog equips you with everything you need to identify, fix, and optimize your API responses.</p><h3>\n  \n  \n  1. <strong>Easier API Request Debugging</strong></h3><p>Apidog makes debugging easier by showing both the  you're sending and the  you receive from the API server. You can view responses in various formats, including , , and even  or  compressed content. This simplifies troubleshooting and makes it more intuitive.</p><ul><li><strong>Automatic Response Validation</strong>: Apidog automatically checks the response based on your API specifications, ensuring that the response structure is as expected.</li><li>: View request history to track and fix issues from past requests.</li><li>: Adjust the interface layout to suit your needs, making it easier to analyze requests and responses.</li></ul><h3>\n  \n  \n  2. <strong>Handle Dynamic Requests with Flexibility</strong></h3><p>Backend developers often work with APIs that involve dynamic parameters, authentication tokens, or complex data. Apidog lets you  on the fly without disrupting the API schema. You can easily adjust headers, parameters, and even request bodies, giving you flexibility during debugging and testing.</p><ul><li>: Apidog lets you tweak requests directly in the debugging interface, making it simple to test different parameters.</li><li>: Use dynamic values and scripts to modify requests in real time.</li><li>: Extract useful data from responses and incorporate it into your workflow.</li></ul><h3>\n  \n  \n  3. <strong>Debug Directly in Your IDE with Apidog Fast Request</strong></h3><p>While Apidog is a powerful standalone tool, many developers prefer to debug directly within their Integrated Development Environment (IDE) for convenience and speed. That’s where  comes in.</p><p> is a plugin for IntelliJ IDEA that integrates Apidog’s debugging features directly into your IDE. With this plugin, backend developers can send, debug, and validate API requests without switching between multiple tools.</p><h2>\n  \n  \n  Using Apidog Fast Request in IntelliJ IDEA\n</h2><p>Apidog Fast Request simplifies the debugging process by keeping everything in one place—inside your IntelliJ IDEA environment. Here’s how you can use it for more efficient API debugging:</p><h3>\n  \n  \n  Step 1: Install the Apidog Fast Request Plugin\n</h3><p>To get started, install the Apidog Fast Request plugin from the  or directly within IntelliJ IDEA:</p><ul><li>Navigate to <strong>File &gt; Settings &gt; Plugins</strong> in IntelliJ IDEA.</li><li>Search for “Apidog Fast Request” and click .</li></ul><h3>\n  \n  \n  Step 2: Automatically Detect Endpoints\n</h3><p>Once installed, Apidog Fast Request will automatically scan your Springboot or Kotlin project and detect all API endpoints. It lists these endpoints in the right panel, making it easy to test each one.</p><ul><li><strong>Automatic Endpoint Detection</strong>: Apidog Fast Request identifies endpoints without requiring extra annotations or configurations.</li><li><strong>Automatic Parameter Population</strong>: It auto-populates parameters based on the detected endpoints, ensuring you don’t miss important values.</li></ul><h3>\n  \n  \n  Step 3: Debug API Requests with One Click\n</h3><p>Apidog Fast Request makes debugging effortless. Once your endpoints are detected, you can send requests with a single click and instantly view the response in the  tab. You can also modify requests, manage cookies, and track request histories, all without leaving your IDE.</p><ul><li><strong>Send Requests with One Click</strong>: Once set up, you can send requests quickly and efficiently without leaving your IDE.</li><li>: Easily track past requests for troubleshooting.</li><li>: Handle local cookies directly within your IDE for smooth API communication.</li></ul><p><strong>Pro tip: Upload your APIs from IDEA to Apidog to automatically generate API documentation and publish it online for direct use. <a href=\"https://fastrequest.apidog.com/?utm_source=dev.to&amp;utm_medium=wanda&amp;utm_content=debugging-piece-of-cakes\">Learn more here.</a></strong></p><h2>\n  \n  \n  Why Use Apidog Fast Request for API Debugging?\n</h2><p>Apidog offers several advantages that can significantly improve your API debugging process:</p><ul><li>: Apidog Fast Request is available to all developers, including those using the <strong>Community Edition of IntelliJ IDEA</strong>.</li><li>: From automatic response validation to request modification, Apidog Fast Request covers all your debugging needs.</li><li>: Debugging is quicker and more intuitive, with minimal context switching and maximum productivity.</li></ul><p>In the fast-paced world of backend development, effective debugging is essential. Apidog equips developers with the tools needed to debug smarter, not harder. Whether you prefer working within Apidog’s intuitive interface or debugging directly within IntelliJ IDEA using , you can streamline your API testing and troubleshooting process.</p><p>By automating common debugging tasks, Apidog helps you focus on what truly matters—building robust, high-performance APIs. Start using <a href=\"https://app.apidog.com/user/login?utm_source=dev.to&amp;utm_medium=wanda&amp;utm_content=debugging-piece-of-cakes\">Apidog</a> and <a href=\"https://plugins.jetbrains.com/plugin/25925-apidog-fast-request--auto-detect-endpoints-http-rest-client?utm_source=dev.to&amp;utm_medium=wanda&amp;utm_content=debugging-piece-of-cakes\">Apidog Fast Request</a> today!</p>","contentLength":6348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Power Platform - The Managed Delivery Process","url":"https://dev.to/wyattdave/power-platform-the-managed-delivery-process-1fo6","date":1739775446,"author":"david wyatt","guid":1648,"unread":true,"content":"<p>Not to be confused with Managed Environments, the Managed Delivery Process is at its core how you implement ALM (Application Lifecycle Management). So what does that mean, well you think of it as how your organisation moves a solution from development to production, and all the steps in-between. </p><p>Some will argue that you don't need ALM, you can simply edit in production/dev is your production. The Default environment in the Power Platform supports this approach, so you can see Microsoft didn't want to follow the normal process, why you may ask? </p><p>Well that's because ALM slows down development, worst of all it requires the developer to have additional knowledge base, and additional people involved, not what you want when you want Citizen Developers to drive adoption. So then you ask why have ALM?</p><p>Well there are a few good reasons, but mainly they are:</p><ul><li>Protects production by ensuring testing</li><li>Mitigates risk of developers having access to production data</li><li>Meets certain government regulations like SOX</li><li>Ensures stability by separating prod from the developer</li><li>Ensures sustainability through coding standards and documentation</li></ul><p>So you can see the value, and the challenges it creates, and that's why your Managed Delivery Process has to be secure yet streamlined, In-depth but not challenging, quick yet detailed.</p><p>Key to this is automated deployments (aka Pipelines) as this is the tool you use to deliver your Managed Deliveries, and it shows you what Microsoft's vision of Managed Delivery vs what I recommend.</p><p>Looking at Microsoft's Power Platform Pipelines you can see that this is more on the 'Default' side then ALM side. It works on a Dev/Test/Prod approach, but it does not detach the Developer from Prod. They still own the solution, and they still use their connections.</p><p>There are variations of this, with delegated deployments added that enable a SPN to own the solutions in Test and Prod (there is delegated to Service Accounts as well, but they can't deploy anything with connections so is not fit for purpose), but this generates issues with licenses, connection errors, and doesn't cover the connections (which are still the developers, so they still need access to production data).</p><p>Additionally interestingly there is no default approval process, this has to be added through flows. You can tell this was built to enable this, but it's also true to say its not prebuilt, let alone on by default.</p><p>The approach I recommend is to go with full compliant ALM, and then move all the complexity to the Automated Deployment.</p><p>So we have Dev/Test/Prod, but this time Service Accounts own the solutions in Test and Prod. The developer can have read only access, but they can not edit it, not even environment variables.</p><p>The next step is we need stage gates to ensure compliance, and there are a few of them:</p><ul><li>Intake: is there a ROI, is there a plan to sustain the solution</li><li>Arch Review: is the solution viable, is the right tech used (are there new systems/changes in the future that would impact it)</li><li>Security: is the solution secure and low risk</li><li>Impact: does it impact any other system, are required licenses/ dev environments available in connected systems</li><li>UAT approval: does someone separate from the dev team agree it passes all testing</li><li>Code Review: does the code meet required org standards, is it documented</li><li>Change Approval: does the business impacted by the solution approve changes</li></ul><p>So we need a process/system that can track all of these stage gates, recording who approves what. This creates a robust audit trail, ensuring we meet all government regulations (I can't imagine Microsoft's version complying with <a href=\"https://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act\" rel=\"noopener noreferrer\">SOX</a>).</p><p>The final part of the puzzle is ensuring that each environment is used correctly, as Microsoft didn't create different Dev/Test/Prod environments we have to (and yes I know there are Developer Environments, but they are not really different, they can still act like a production environment).</p><p>For dev we have an automation that periodically turns off all flows that have not been edited in 24 hours. This stops dev becoming pseudo prod.</p><p>Test we ensure that the pipeline first checks the solution has been through required stage gates (like Impact, Arch, Security and Impact). We also should have a automation that deletes all solutions that have not been updated in 14 days. Finally we should have reporting that shows length of time for every solution in Test, anything there for over 28 days should be investigated.</p><p>If you would like to get notified every new blog (I also do a few in the Power Platform Community), subscribe below</p>","contentLength":4538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Smart Way to Find Duplicates in an Array (No Extra Space!)","url":"https://dev.to/mehta0007/the-smart-way-to-find-duplicates-in-an-array-no-extra-space-3ol1","date":1739774956,"author":"Ankit","guid":1647,"unread":true,"content":"<p>When I first saw  — Find All Duplicates in an Array, my initial thought was:\n“Okay, I’ll just use a set to track occurrences.”</p><p>But then I came across Index Marking, and it blew my mind! 🤯 This method lets you find duplicates in O(n) time using O(1) extra space — without sorting or extra data structures.</p><p>\nInstead of storing visited numbers in a set, we mark them directly in the array:</p><p>1️⃣ Treat each number as an index.\n2️⃣ Flip the sign of the number at that index (to mark it).<p>\n3️⃣ If we encounter a negative number, it means the index has been visited before → it’s a duplicate!</p></p><p>\n📌 Given array: [4, 3, 2, 7, 8, 2, 3, 1]</p><p>Read 4 → Mark Index 3 (nums[3] = -7)\nRead 3 → Mark Index 2 (nums[2] = -2)<p>\nRead 2 → Mark Index 1 (nums[1] = -3)</p>\nRead 7 → Mark Index 6 (nums[6] = -3)<p>\nRead 8 → Mark Index 7 (nums[7] = -1)</p>\nRead 2 → Index 1 It's already negative → Duplicate!<p>\nRead 3 → Index 2 It's already negative → Duplicate!</p>\nWhy This Works<p>\n✔️ O(n) time complexity (each number is processed once)</p>\n✔️ O(1) space complexity (modifies input array instead of using extra space)<p>\n✔️ Avoids sorting (which takes O(n log n) time)</p></p><p>\nThis trick is a game-changer for space-efficient duplicate detection! If you haven’t tried it yet, give it a shot next time you see a similar problem.</p>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Why Every Beginner (Especially Non-Tech Students) Should Learn Git","url":"https://dev.to/masaudahmod/why-every-beginner-especially-non-tech-students-should-learn-git-4ib3","date":1739774940,"author":"Mr Masaud","guid":1646,"unread":true,"content":"<p><strong>🚀 Why Should Beginner Programmers (Especially Non-Tech Students) Learn Git?</strong>\nIf you're new to programming and not from a tech background, you might wonder:<p>\n_\"Why should I learn Git? Isn't it for advanced developers?\"</p>\n_*<em>Well, Git is not just for experienced programmers—it's a must-have tool for beginners too! Here’s why:\n*</em>\n**🔹 What is Git?<p>\n**Git is a version control system that helps you track changes in your code, collaborate with others, and prevent accidental data loss. Think of it as Google Docs for your code, where you can:</p>\n✅ Save different versions of your project\n✅ Work on projects without fear of breaking something<p>\n✅ Collaborate with developers worldwide</p></p><h2>\n  \n  \n  🔹 How to Install Git Locally &amp; Set Up a Global Git Account\n</h2><p> Install Git on Your System\n🔹** Windows Users**<a href=\"https://git-scm.com/downloads\" rel=\"noopener noreferrer\">Git’s official website</a>\n2️⃣ Run the downloaded .exe file and follow the installation process<p>\n3️⃣ Choose \"Git Bash\" during installation (this will help you run Git commands)</p></p><p>🔹** Mac Users**\n1️⃣ Open Terminal and type:</p><p>(If you don’t have Homebrew, install it first from <a href=\"https://brew.sh/\" rel=\"noopener noreferrer\">brew.sh</a>)</p><p>🔹** Linux Users**\n1️⃣ Open Terminal and type:</p><div><pre><code>sudo apt install git  # For Ubuntu/Debian  \nsudo dnf install git  # For Fedora \n</code></pre></div><p> Verify Git Installation\nAfter installation, open Command Prompt (Windows) / Terminal (Mac &amp; Linux) and run:</p><p>_If it returns something like git version 2.x.x, your installation was successful. ✅\n_<p>\n**Step 3: **Set Up Git Account Globally in Local Machine</p>\nNow, link Git with your personal identity (GitHub account).</p><p>1️⃣ Configure Your Name &amp; Email (Globally)\nUse your GitHub email and name:</p><div><pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your-email@example.com\"\n</code></pre></div><p>Verify your setup by running:</p><div><pre><code>git config --global --list\n</code></pre></div><p>2️⃣ Generate SSH Key &amp; Connect GitHub (Optional but Recommended)\nThis step lets you push code to GitHub without entering your password every time.</p><div><pre><code>ssh-keygen -t rsa -b 4096 -C \"your-email@example.com\"\n</code></pre></div><p>Press Enter multiple times (default options are fine). Then, copy the SSH key to GitHub:</p><p>Copy the output and paste it into GitHub under:\n👉 GitHub Settings → SSH and GPG keys → New SSH key</p><h2>\n  \n  \n  🔹 Why is Git Important for Beginners?\n</h2><p>1️⃣ Mistakes are normal – Git helps you roll back to a previous working version if you mess up.\n2️⃣ Backup for your projects – You won’t lose progress if your computer crashes.<p>\n3️⃣ Learn team collaboration – Most companies use Git for software development.</p>\n4️⃣ Showcase your work – Platforms like GitHub act as a portfolio for developers.</p><p>🔹 Basic Git Commands Every Beginner Should Know\n💻 git init → Start a new Git project<p>\n💻 git add . → Add all changes to be tracked</p>\n💻 git commit -m \"Your message\" → Save changes with a message<p>\n💻 git push origin main → Upload your project to GitHub</p>\n💻 git clone [repo URL] → Download a project from GitHub<p>\n💻 git pull origin main → Get the latest updates from a project</p></p><p>🔹 Discussion: What Confuses You About Git?\nWhat part of Git feels difficult or confusing? Drop your questions below &amp; let’s discuss! 👇🔥</p>","contentLength":3127,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"useEffect: The Hook That Keeps You Guessing (And Refreshing)","url":"https://dev.to/umangmittal/useeffect-the-hook-that-keeps-you-guessing-and-refreshing-3i0p","date":1739774363,"author":"Umang Mittal","guid":1645,"unread":true,"content":"<p>Hi my developer community!</p><p>Let's be honest, useEffect is one the most used hook in the React Eco-system. We all have at some point in our development life used this hook for running some side-effects.</p><p>Let's learn some advanced things about the useEffect.</p><p>We all know that useEffect is used to trigger any side-effect in your React component.</p><p>The syntax of useEffect is:</p><div><pre><code>import { useEffect } from 'react';\n\nuseEffect(callbackFn, dependencyArr)\n</code></pre></div><p>Well, in my experience in taking interviews I asked this question a lot. What would happen if I don't pass any value in the second argument of the useEffect?</p><p>And mostly candidates say, \"The callback function will run infinitely\". <strong>That is not the correct answer</strong>. And it also shows how much experience they have with the useEffect hook.</p><p>I have one console log in the component's useEffect and one outside of the effect. Notice that there is not dependency array in the useEffect hook.</p><p>And here is the output:<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F6qp6lv7gmyjkkfvzyrlc.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F6qp6lv7gmyjkkfvzyrlc.png\" alt=\"Image description\" width=\"565\" height=\"582\"></a></p><p>As you can see there is no infinite logs. That means useEffect hook only runs on every render when there is no dependency array.</p><p>Sometimes, we attach an event listener on the mount of a component. But have you wondered what happens when the component unmounts. Does that event listener gets destroyed?</p><p>To demonstrate, let's attach a keyup listener on the mount of child component and unmounts the child component after 3 seconds.</p><p><strong>CAUTION: For demonstration purpose, I have removed the Strict Mode provided in React to demonstrate the deployed version of outputs.</strong></p><p>Now, even after the child component unmounts, the event listener is still active and I can still see the logs when I press any key. That proves the fact that event listeners will still mount even when the component itself unmount.</p><p>To resolve this we have a cleanup function in useEffect. This will run before the callbackFn and on unmount of the component (I know sounds confusing but I will explain).</p><p>Some Dev's say it will run only when the component unmounts which is not entirely true. According to React dev docs:\nReact calls your setup and cleanup functions whenever it’s necessary, which may happen multiple times:</p><div><pre><code>1. Your setup code runs when your component is added to the page (mounts).\n2. After every re-render of your component where the dependencies have changed:\n    - First, your cleanup code runs with the old props and state.\n    - Then, your setup code runs with the new props and state.\n    - Your cleanup code runs one final time after your component is removed from the page (unmounts).\n</code></pre></div><p>So, if you have given some dependencies in the dependencyArr, then the cleanup function is called before it runs the side-effect again and in case of empty dependency array it will run on unmount.</p><p>Now the basic things out of the way, let's understand the execution order of useEffect in a Parent-Child relationship.</p><p>In this code, as you can see we have console logs out of the useEffect and inside useEffects as well. So, try to solve it without looking at the solution.<strong>Note: I have disabled React.StrictMode to make the console look cleaner, but if won't make any difference in the order of execution.</strong></p><p>Hmm.., If you think the Parent's useEffect should've run first, here you got the answer.</p><p><em>React prioritizes effects in the child components before the parent to ensure that effects affecting the child’s layout or behavior are handled first. This helps to avoid potential issues like the parent reacting to changes in the child that have not yet been fully applied.</em></p><p>And what would be the order of the cleanup functions, is it the same or different?\nLet's tweak our little example to see the result:</p><p>So, Now we have two useEffect's in both the components. I introduced a count variable just to re-render the parent and child and passed that as a prop to child component.</p><p>And yes, the order of the useEffects being registered also matters. If both the side-effects needs to run, the one that registers first will run first.</p><p>Let's see what would be the execution of this. As an exercise, you can think of a solution first before looking at the console.</p><p>what we've understood so far seems to be working here as well. First we have both parent's and child's outer console's and then useEffect console's. Child's first and then parent's.</p><p>Now, let's see what happens when you update the state. In our case, clicking on the button.</p><p>I am going to put a console log \"Button clicking event\" to segregate the output of before the button clicked and after the button is clicked.</p><p>As you can see, Parent and child render comes first.\nNext up is the cleanup functions. And yes, in the reverse order again.<p>\nAfter the cleanup functions, we again have the side-effects running similarly.</p></p><p><strong>Note: For the simplicity, I've removed the React.StrictMode</strong></p><p>I guess that's it for the post, I hope you have gained some extra knowledge of how React works by attaching events and the order of execution for useEffect by reading this post.</p><p>Thanks for reading, I'll code a bug next time.\nThat's Umang, signing off.</p>","contentLength":4975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SQL vs Programming Languages: Can SQL be Considered a Programming Language?","url":"https://dev.to/katiek/sql-vs-programming-languages-lets-explore-why-sql-isnt-considered-a-programming-language-hl2","date":1739774306,"author":"Katie","guid":1644,"unread":true,"content":"<p>This article tries to solve the never ending debate of whether SQL is a programming language or not. As the name goes, SQL; is a structured query language that helps manage, clean and manipulate data that is stored in relational databases. With SQL, one has the ability to update records, query data, identify relations between sets of data, and much more. However, it is important to note that SQL helps one do this on a condition that the data being worked on is in a relational structure. SQL is limited to accessing and managing datasets using specified application domains. </p><p>On the other hand, traditional programming languages such as python are general purpose programming languages that have the ability to write code that the computer can execute to perform specific functions. \"A programming language is a formal set of instructions used to produce a variety of machine code outputs, such as software applications, websites, or system operations.\" All these are categorized as general purpose languages which sets them apart from domain specific languages.</p><p>Now that we clearly understand what is SQL and its ability and what a programming language is, is it right to classify SQL as a programming language? \nSQL fits the description of a scripting language and traditional programming languages are general purpose and meet the requirements of a programming language. </p>","contentLength":1376,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Install an SSL Certificate on an Ubuntu Server Running a Go Application","url":"https://dev.to/judypage/how-to-install-an-ssl-certificate-on-an-ubuntu-server-running-a-go-application-4g9p","date":1739774274,"author":"Judy Page","guid":1643,"unread":true,"content":"<p>If you are looking to secure your go application with an SSL certificate to protect your sensitive data and ensure encrypted communication to build user trust. </p><p>Without having an SSL certificate your application is vulnerable to cyber attacks, data interception and third-party unauthorized access. </p><p>In this article, we will take you through the step-by-step process of installing an SSL certificate on the Ubuntu server for a Go application. \nSpecifically, we will focus on using a certificate from Porkbun, a reliable domain and SSL provider, to enhance your application's security and encryption. </p><p>By following these steps you can guarantee a safer and more secure online experience for your users.</p><p><strong>How to install an SSL certificate on an Ubuntu server running on a Go application</strong></p><p>\n1: Log in to your Porkbun account and locate the SSL certificate section<p>\n2: Now you have to download the SSL certificate bundle which includes:</p>(private key) (public key)(certificate)</p><p> Upload the SSL files to the server\nTransfer your SSL files to your Ubuntu server using <a href=\"https://www.cerberusftp.com/blog/comparing-scp-vs-sftp-which-is-better/\" rel=\"noopener noreferrer\">SCP or SFTP</a></p><div><pre><code>scp private.key.pem public.key.pem domain.cert.pem user@your-server:/home/user/\n\n</code></pre></div><p>Move them to a secure location:</p><div><pre><code>sudo mkdir -p /etc/ssl/api.example.xyz/\nsudo mv private.key.pem /etc/ssl/api.example.xyz/\nsudo mv public.key.pem /etc/ssl/api.example.xyz/\nsudo mv domain.cert.pem /etc/ssl/api.example.xyz/\n</code></pre></div><div><pre><code>sudo chmod 600 /etc/ssl/api.example.xyz/*\nsudo chown root: root /etc/ssl/api.example.xyz/*\n\n</code></pre></div><p> Configure the Go application for HTTPS</p><p>Since our Go application runs on its own without Apache or Nginx, we need to set up TLS directly in the Go code.</p><div><pre><code>package main\n\nimport (\n\"log\"\n\"net/http\"\n)\nfunc main() {\nhttp.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n w.Write([]byte(\"Hello, Secure API!\"))\n})\nerr := http.ListenAndServeTLS(\":443\",\n \"/etc/ssl/api.example.xyz/domain.cert.pem\",\n \"/etc/ssl/api.example.xyz/private.key.pem\",\n nil)\nif err != nil {\n log.Fatal(err)\n}\n}\n</code></pre></div><p> Allow HTTPS in Firewall\nIf you’re using ufw (Uncomplicated Firewall), allow traffic on <a href=\"https://www.ssl2buy.com/wiki/port-443\" rel=\"noopener noreferrer\">port 443 (HTTPS)</a>:</p><div><pre><code>sudo ufw allow 443/tcp\nsudo ufw reload\n</code></pre></div><p> Restart the Go Application\nRun the Go application again to apply the changes:</p><p> Testing your  SSL Installation\nYou can test your SSL installation using curl:</p><div><pre><code>curl -v https://api.example.xyz \n\n</code></pre></div><p>Now your Go application is secured with SSL</p>","contentLength":2320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding API Types and Protocols: A Beginner’s Guide","url":"https://dev.to/apilover/understanding-api-types-and-protocols-a-beginners-guide-7ld","date":1739774119,"author":"Wanda","guid":1642,"unread":true,"content":"<p>APIs (Application Programming Interfaces) are the backbone of modern software development, enabling applications to communicate, share data, and extend functionality. But not all APIs are created equal. Choosing the right type and protocol can make or break your project’s efficiency, security, and scalability. Let’s break down the key API types, architectures, and use cases to help you make informed decisions.  </p><blockquote><p><strong>PRO TIP: If you're new to API development, <a href=\"https://apidog.com/?utm_source=dev.to&amp;utm_medium=wanda&amp;utm_content=api-types\">Apidog</a> is a fantastic tool to kickstart your journey. Its user-friendly interface and all-in-one functionality—from designing and documenting to testing and mocking APIs—make it a seamless choice for beginners. Whether you're building APIs from scratch or need a simpler way to manage existing ones, Apidog simplifies the process and helps you get results faster, without the steep learning curve of more complex tools.</strong></p></blockquote><h2>\n  \n  \n  The 4 Core Types of Web APIs\n</h2><p>APIs are categorized by their intended scope and audience. Here’s how they differ:  </p><h3>\n  \n  \n  1. <strong>Public APIs (Open/External APIs)</strong></h3><ul><li>: Available to any external developer or business.\n</li><li>: Encourage third-party integrations (e.g., Google Maps API, Twitter API).\n</li><li>: Moderate authentication (API keys, OAuth).\n</li><li>: Often pay-per-call or freemium models.\n</li></ul><ul><li>: Exclusive access for authorized external partners.\n</li><li>: B2B integrations (e.g., CRM systems sharing data with approved vendors).\n</li><li>: Strict authentication (JWT, mutual TLS) and role-based access.\n</li><li>: Partners pay for services, not API usage directly.\n</li></ul><h3>\n  \n  \n  3. <strong>Private APIs (Internal APIs)</strong></h3><ul><li>: Internal use within an organization.\n</li><li>: Connecting internal systems (e.g., HR + payroll).\n</li><li>: Historically lax, but modern practices enforce OAuth or API gateways.\n</li></ul><ul><li>: Combine multiple APIs into a single interface.\n</li><li>: Streamlining complex workflows (e.g., an e-commerce checkout that calls payment, inventory, and shipping APIs).\n</li><li>: Reduces round trips, improves performance.\n</li></ul><h2>\n  \n  \n  API Protocols: REST, SOAP, and RPC\n</h2><p>APIs rely on protocols to structure communication. Here’s how the big three stack up:  </p><h3><strong>1. REST (Representational State Transfer)</strong></h3><ul><li>: Architectural (resource-based).\n</li><li>: JSON, XML, HTML, plain text.\n</li><li>:\n\n<ul><li>Stateless, cacheable, and client-server separated.\n</li><li>Uses standard HTTP methods (, , , ).\n</li></ul></li><li>: Public APIs (mobile/web apps), scalable services.\n</li><li>: GitHub’s REST API.\n</li></ul><h3><strong>2. SOAP (Simple Object Access Protocol)</strong></h3><ul><li>: Protocol (XML-based).\n</li><li>: Strict standards with four message components: envelope, header, body, fault.\n</li><li>:\n\n<ul><li>Built-in security (WS-Security), ACID compliance, error handling.\n</li><li>Works over HTTP, SMTP, TCP/IP.\n</li></ul></li><li>: Enterprise systems (banking, healthcare), internal/partner APIs.\n</li><li>: PayPal’s SOAP API.\n</li></ul><h3><strong>3. RPC (Remote Procedure Call)</strong></h3><ul><li>: Protocol (action-based).\n</li><li>:\n\n<ul><li>: Lightweight, text-only.\n</li><li>: Supports images, charts, and stricter security.\n</li></ul></li><li>:\n\n<ul><li>Invokes remote functions like local ones.\n</li><li>Simple but limited in data types and security.\n</li></ul></li><li>: Internal microservices, IoT devices.\n</li><li>: Slack’s RPC-based Real-Time Messaging API.\n</li></ul><h2>\n  \n  \n  REST vs. SOAP vs. RPC: Quick Comparison\n</h2><div><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><h2>\n  \n  \n  Choosing the Right API: Key Considerations\n</h2><ol><li><ul><li>Clear docs with code samples are non-negotiable. Swagger/OpenAPI specs are gold.\n</li></ul></li><li><ul><li>Can developers integrate your API in minutes? Provide SDKs, sandboxes, and tutorials.\n</li></ul></li><li><ul><li>REST’s caching beats SOAP’s verbosity. RPC is fast but not for heavy data.\n</li></ul></li><li><ul><li>Public APIs need OAuth 2.0; SOAP suits industries needing WS-Security.\n</li></ul></li><li><ul><li>Versioning (e.g., ) prevents breaking changes.\n</li></ul></li></ol><ol><li><ul><li>: Twitter’s API for tweet automation.\n</li></ul></li><li><ul><li>: PayPal’s API for B2B transactions.\n</li></ul></li><li><ul><li>: Private (REST/RPC).\n</li><li>: Netflix’s internal service communication.\n</li></ul></li><li><ul><li>: Expedia combines hotel, flight, and car rental APIs.\n</li></ul></li></ol><p>APIs are tools—pick the right one for the job. Need simplicity and scale? Go REST. Handling sensitive transactions? SOAP’s rigor shines. Building internal tools? RPC or private REST APIs might suffice. Always prioritize security, documentation, and developer experience.  </p><p>As systems grow more interconnected, mastering API design isn’t optional—it’s essential. Now go build something awesome. 🚀  </p>","contentLength":4050,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simplifying Request Body Handling in Next.js 15","url":"https://dev.to/saiful7778/simplifying-request-body-handling-in-nextjs-15-2l8a","date":1739773558,"author":"Saiful Islam","guid":1641,"unread":true,"content":"<p>When working with API routes in , handling different types of request payloads (, , and ) can be frustrating. Developers often write repetitive code to extract data from , , or .</p><p>To  this process, we’ll create a  that make request body handling more structured and error-proof. These functions will also integrate with the  function we built in our <a href=\"https://dev.to/saiful7778/building-a-robust-api-response-helper-in-nextjs-15-4aeg\">previous blog</a> to maintain consistent API responses.</p><h2>\n  \n  \n  Why Use a Request Body Helper?\n</h2><p>❌ <strong>Manually parsing request bodies</strong> in every API route leads to redundant code.\n❌ <strong>Missing or invalid payloads</strong> can cause unexpected runtime errors.\n❌ <strong>Inconsistent error messages</strong> make debugging difficult.</p><p>✅ With reusable helpers, we ensure ,  error handling, and better DX (Developer Experience).</p><h2>\n  \n  \n  Building the Request Body Helper in Next.js 15\n</h2><h3>\n  \n  \n  Step 1: Creating the  Class\n</h3><p>First, let’s define a  to handle different request body errors:</p><div><pre><code></code></pre></div><p>📌 This allows us to throw  when request bodies are missing or invalid.</p><h3>\n  \n  \n  Step 2: Creating Utility Functions for Parsing Request Bodies\n</h3><p>Now, we’ll write functions to handle different request body types.</p><div><pre><code></code></pre></div><p>✅ Each function automatically throws an error if the required payload is missing or invalid.\n✅ Works with TypeScript for better type safety.</p><h3>\n  \n  \n  Step 3: Integrating with the  Function\n</h3><p>In our <a href=\"https://dev.to/saiful7778/building-a-robust-api-response-helper-in-nextjs-15-4aeg\">previous blog</a>, we built a reusable  function to ensure . Now, let’s use it in an API route.</p><div><pre><code></code></pre></div><p>📌 Now, all API routes will follow a structured error-handling pattern!</p><p>With this helper function setup:</p><p>✔️  when handling request payloads.\n✔️ <strong>Standardized error handling</strong> using .\n✔️  with .</p><p>If you haven’t read the <a href=\"https://dev.to/saiful7778/building-a-robust-api-response-helper-in-nextjs-15-4aeg\">previous blog</a>, I recommend checking it out first to see how  works!</p><p>💡 What’s your approach to handling API request payloads? Let me know in the comments!</p>","contentLength":1787,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I got AWS Certified in 2 Weeks","url":"https://dev.to/bagelbomb/how-i-got-aws-certified-in-2-weeks-531m","date":1739773484,"author":"Tanner Iverson","guid":1640,"unread":true,"content":"<p>I passed the AWS Certified Cloud Practitioner exam after studying for 2 weeks, with no prior experience. Here's how, and why.</p><p>Several months ago, my manager asked me to work on getting cloud certifications. I fully intended to do so, but because my team doesn't use AWS directly and because I had more interesting projects to work on (not to mention my full-time workload), I lacked the motivation to do it.</p><p>I thought, \"I'll just finish what I'm working on now and get back to the AWS stuff before my yearly evaluation. I have plenty of time.\"</p><p>On January 3rd, I received a notification that self-evaluations had opened and were due on January 24th, 3 weeks later.</p><p>At that point, I had worked on an AWS course for about 3 hours. That's 10% of the course. I quickly realized that I was not going to finish the course in time. </p><p>I had to either accept the fact that I would disappoint my manager and receive a worse evaluation than I could have, or figure out how to get certified in less than 3 weeks.</p><p>I started by continuing the AWS course (<a href=\"https://www.udemy.com/course/aws-certified-cloud-practitioner-new/\" rel=\"noopener noreferrer\">Ultimate AWS Certified Cloud Practitioner CLF-C02</a>; highly recommend it), until I realized that I spend about the same amount of time on things like taking notes and the hands-on exercises as I do on watching the course videos. That means that 15 hour course would end up taking 30 hours. I didn't have time to finish the course, let alone to prepare for and take the exam.</p><p>I needed something to teach me just enough about every topic in the exam, without diving too deep. So I looked for cheat sheets, and found <a href=\"https://sathittham.medium.com/aws-cloud-practitioner-certification-cheat-sheet-1191b36137a8\" rel=\"noopener noreferrer\">this one</a>. I wasn't expecting to find what I needed immediately or all in one place, but this ended up being exactly that. I was able to read through it in one sitting, and I learned a lot.</p><p>I read through that cheat sheet several times, sometimes reading the sections in reverse order to avoid primacy bias, and Googling anything I felt like I needed more information on.</p><p>I also skimmed through the <a href=\"https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf\" rel=\"noopener noreferrer\">exam guide</a> and the short descriptions in the Services dropdown menu on the AWS Console.</p><p>After doing this for a few days, I felt ready to start taking some practice tests.</p><p>Then I took the more realistic practice test at the end of the AWS course. I also failed that one, but not by much (63% out of 70% for passing). I was getting better.</p><p>The instructor for the AWS course also has a <a href=\"https://www.udemy.com/course/practice-exams-aws-certified-cloud-practitioner/\" rel=\"noopener noreferrer\">separate course</a> with 6 additional practice tests. Since my company pays for Udemy subscriptions, I made sure to take advantage of this. I took the first test... and passed!</p><p>I took the second and third tests and passed those as well. I also went back and took the free test again, and passed it too.</p><p>Throughout this whole process, I made sure to read the explanations for the questions I got wrong, and even some that I got right (anything I was unsure about), and to reread the cheat sheet as needed.</p><p>I would've liked to take the rest of the tests I had access to, but I was running out of time and now that I was consistently passing the practice tests, I felt confident enough to take the real one.</p><p>I scheduled the exam for January 17th, 1 week before my self-imposed deadline, and 2 weeks after I started studying (I wanted to leave a gap in case it took a while to get the score report). On the day of the exam, I read the cheat sheet one more time, and then I gave it my best shot. I passed with a score of 881!</p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fb9lnouhg07dib3o27ica.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fb9lnouhg07dib3o27ica.png\" alt=\"Jurassic Park - never thought if you should meme\" width=\"620\" height=\"337\"></a>\nJust because you  pass the AWS Certified Cloud Practitioner exam after studying for 2 weeks, doesn't mean you  (you probably shouldn't). Many things could have gone wrong, and at many points, I thought I wouldn't be able to do it.</p><p>This is also an entry-level exam; I would definitely advise against following this process on more advanced exams. And AWS is a very good thing to be knowledgeable in as a web developer; it deserves to be learned \"properly\".</p><p>That being said, I'm proud of what I was able to do, and I thought I'd share my experience in case it can help anyone else in their learning journey. Thanks for reading!</p>","contentLength":3956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Setting Up a Custom Domain with AWS Amplify Without Using AWS Route 53","url":"https://dev.to/binaryibex/attaching-a-custom-domain-to-aws-amplify-without-using-aws-route-53-2k0k","date":1739773134,"author":"Chetan Dhongade","guid":1639,"unread":true,"content":"<p>1) Make sure you have your site deployed on AWS Amplify. For demonstration purposes, I have deployed a simple index.html page with the content shown below. </p><p>2) Go to the Hosting section in the sidebar, where you will see the Custom Domain option. Click on it, and then click on Add Domain. </p><p>3) On the Add Domain screen, enter the domain or subdomain name. After that, select Manual configuration and click on the Configure domain button.</p><p>4) On the next screen, keep everything at the default settings and click on Add Domain.</p><p>5) Wait for 5 minutes, and you will receive 3 CNAMEs that you need to set in your domain's DNS manager. </p><p>6) My domain registrar is Porkbun, and I have set the CNAMEs as follows:</p><p>7) You may need to wait a few minutes for DNS propagation, depending on your domain registrar. In my case, it took just 3 minutes.</p><p>8) After that, the domain activation process will begin, and within 7-10 minutes, you will be able to visit the site using your custom domain. In my case, it was <a href=\"https://app.davesidewalk.com\" rel=\"noopener noreferrer\">https://app.davesidewalk.com</a>.</p><p>Once the domain activation is complete, your custom domain will be fully functional. You can now access your site seamlessly with your new domain. If you face any issues, ensure the DNS settings are correct and allow time for propagation. Enjoy using your custom domain with AWS Amplify!</p>","contentLength":1306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Father - an NPM package development tool.","url":"https://dev.to/ramunarasinga-11/father-an-npm-package-development-tool-15dp","date":1739771091,"author":"Ramu Narasinga","guid":1630,"unread":true,"content":"<p>In this article, we review a tool called <a href=\"https://github.com/umijs/father\" rel=\"noopener noreferrer\">Father</a>. I pulled this below information from the Father repository <a href=\"https://github.com/umijs/father\" rel=\"noopener noreferrer\">README.md</a>. Although, their README is in Chinese, I will be submitting a PR with a docs translation in the near future.</p><div><pre><code></code></pre></div><p>defineConfig is imported from ‘father’. This is even when I had to search for “father” npm package.</p><p>Father is an NPM package development tool that helps developers efficiently and reliably develop NPM packages, generate build artifacts, and publish them. It offers the following key features:</p><ul><li><p>: Supports both  and  build modes — ESModule and CommonJS outputs use Bundless mode, while UMD outputs use Bundle mode.</p></li><li><p>s: Bundle mode uses k as the build engine, while Bundless mode supports C, allowing flexible configuration switching.</p></li><li><p>n: Supports generating s type definitions for TypeScript modules, whether for source code builds or dependency pre-bundling.</p></li><li><p>: All output types support persistent caching, enabling faster incremental builds.</p></li><li><p>: Checks for common pitfalls in NPM package development to ensure more stable releases.</p></li><li><p>: Adds commonly used engineering capabilities to projects, such as setting up Jest for testing.</p></li><li><p>: Provides out-of-the-box dependency pre-bundling to improve the stability of Node.js frameworks/libraries and prevent issues caused by upstream dependency updates (experimental).</p></li></ul><p>Use  to quickly create a new  project:</p><div><pre><code></code></pre></div><p>This template includes only the basic configuration. For additional configuration options, refer to the  documentation.</p><p>To build the project, run:</p><p>After the build completes, check the  folder to see the generated output.</p><blockquote><p><a href=\"https://github.com/umijs/father/blob/master/docs/config.md\" rel=\"noopener noreferrer\"></a><em>is in Chinese language, I have submitted a PR with English translations.</em></p></blockquote><div><pre><code></code></pre></div><p> is used to specify the prebundle output directory.  is used to specify the target platform for the build output.</p><p>Hey, my name is Ramu Narasinga. I study large open-source projects and create content about their codebase architecture and best practices, sharing it through articles, videos.</p>","contentLength":1947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Your Website Needs JavaScript Minification & Compression Now: Boost Performance Instantly!","url":"https://dev.to/dct_technologyprivatelimited/why-your-website-needs-javascript-minification-compression-now-boost-performance-instantly-4i1n","date":1739770404,"author":"DCT Technology","guid":1629,"unread":true,"content":"<p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzby2ntttzqi5bih6bdd.jpg\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fbzby2ntttzqi5bih6bdd.jpg\" alt=\"Image description\" width=\"800\" height=\"800\"></a>\nWhen it comes to improving your website’s performance, one of the most impactful yet often overlooked techniques is JavaScript minification and compression. </p><p>In this post, we'll explore why these two processes matter, how to do them effectively, and how they can lead to significant performance improvements for your website. </p><p><strong>What is JavaScript Minification?</strong></p><p>JavaScript minification is the process of removing unnecessary characters (like spaces, line breaks, and comments) from your code without changing its functionality. This reduces the file size, making it faster to download and execute. </p><p><strong>Tools to Minify JavaScript</strong></p><p>To minify JavaScript, there are several excellent tools you can use: </p><p>UglifyJS: A widely used JavaScript compressor that works well with large files.  </p><p>Terser: Another powerful minifier, particularly useful for modern JavaScript and ES6+ syntax.  </p><p>By running your JavaScript files through these tools, you'll dramatically reduce the size of your code, speeding up load times and improving overall user experience. </p><p><strong>Gzip vs. Brotli Compression: What’s the Difference?</strong></p><p>Compression takes your minified files and reduces them even further, making them faster to transfer over the web. But which compression method should you use? Let’s break it down: </p><p>: It's widely supported by all browsers, and it works well for most websites. </p><p>: It's not the most efficient option for all file types, and performance can be a little slower compared to newer methods. </p><p>: Brotli often results in better compression ratios, meaning even smaller file sizes for faster load times. It's especially effective for text-based content like JavaScript and CSS. </p><p>: While it's supported by most modern browsers, it might not work on every device or browser version (but it's becoming more widespread). </p><p><strong>How to Enable Compression on Your Website</strong></p><p>Both Gzip and Brotli are easy to implement on your server. Here’s how: </p><p>For Gzip: You can enable Gzip compression on your server by adding this code to your .htaccess file (if you're using Apache): AddOutputFilterByType DEFLATE text/plain text/html text/xml text/css application/x-javascript application/javascript </p><p>: Most modern servers (like Nginx or Apache) support Brotli out of the box. </p><p>Here’s a simple Nginx config snippet to enable Brotli: load_module /usr/share/nginx/modules/ngx_http_brotli_filter_module.so; \nbrotli on; </p><p>Check out these compression resources to understand how to implement compression for faster load times. </p><p><strong>Real-World Performance Improvements</strong></p><p>You might be wondering: How much faster will my site be with these optimizations? </p><p>Minifying JavaScript: A well-minified JavaScript file can reduce its size by up to 60%, leading to a 20-30% reduction in load time. </p><p>Compression (Gzip or Brotli): When combined with minification, compression can reduce your file sizes even more, cutting load times by another 20-30%. </p><p>In real-world terms, this means faster page loads, lower bounce rates, and a better user experience. Websites with fast load times also enjoy better SEO rankings. </p><p>Ready to boost your site’s performance? Start by minifying your JavaScript and enabling compression on your server. </p><p>Not only will this improve your site's speed, but it will also enhance user satisfaction, making your site stand out from the competition. </p><p>If you need expert help with optimizing your website’s performance, contact <a href=\"//www.dctinfotech.com\">DCT Technology</a> for professional web development, IT consulting, and more! </p><p>What’s your experience with minifying and compressing JavaScript? Do you prefer Gzip or Brotli? </p><p>Drop a comment below, and let’s discuss the best ways to improve website performance. </p>","contentLength":3624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streamlining AWS Deployments: Jenkins & Terraform in Action with the 2048 Game","url":"https://dev.to/pravesh_sudha_3c2b0c2b5e0/streamlining-aws-deployments-jenkins-terraform-in-action-with-the-2048-game-4e14","date":1739770062,"author":"Pravesh Sudha","guid":1628,"unread":true,"content":"<blockquote><p>Automate Your AWS Deployments: Master Jenkins, Terraform, and Docker with a Fun Twist!</p></blockquote><p>Welcome to the world of DevOps! Today, we're diving into an exciting project where we <strong>automate AWS infrastructure using Jenkins and Terraform</strong> while deploying the  on our server.</p><p>We’ll kick off by provisioning a , which will act as our host machine. On this instance, we’ll <strong>install Docker, Jenkins, Terraform, and the AWS CLI</strong>. Next, we’ll configure Terraform to <strong>provision an EC2 instance, create an S3 bucket for Terraform state and logs, and define IAM policies</strong> following the <strong>Principle of Least Privilege</strong> for security.</p><p>Once the infrastructure is ready, we’ll <strong>containerize the 2048 game using Docker and expose it via Nginx on port 80</strong>. Finally, we’ll <strong>automate everything using Jenkins</strong>, setting up a  that:<p>\n✅ Pulls Terraform code from GitHub</p><p>\n✅ Applies the Terraform configuration</p><p>\n✅ SSHs into the EC2 instance</p><p>\n✅ Restarts services automatically on changes</p></p><p>By the end of this blog, you’ll have a <strong>fully automated AWS deployment pipeline</strong>, giving you hands-on experience with <strong>Terraform, Jenkins, AWS, and containerized applications</strong>. Let's get started! 🚀</p><p>Before diving into the project, ensure you have a foundational understanding of the following concepts:</p><p>✅  – Since we’ll be provisioning infrastructure on AWS, familiarity with <strong>EC2, IAM Roles, AWS CLI, and S3</strong> will be helpful.</p><p>✅ <strong>Jenkins &amp; Terraform Fundamentals</strong> – We’ll use  for infrastructure provisioning and  to automate the deployment process, so understanding <strong>how Terraform manages cloud resources</strong> and <strong>how Jenkins pipelines work</strong> is essential.</p><p>✅  – Since we’ll <strong>containerize the 2048 game</strong> using Docker, you should have a basic grasp of <strong>Docker images, containers, and networking</strong>.</p><p>If you’re new to any of these, don’t worry! This guide will walk you through the steps, but having prior knowledge will make things smoother. 🚀</p><h2><strong>💡 Setting Up the AWS EC2 Instance &amp; Installing Required Tools</strong></h2><h3><strong>Step 1: Launch the EC2 Instance</strong></h3><p>1️⃣ Go to  and click . as: <code>2048-Jenkins-Terraform-Automation</code>. as the operating system. as the instance type.. or create a new one to connect via SSH..</p><h3><strong>Step 2: Connect to the Instance</strong></h3><p>Once the instance is running:<p>\n🔹 Select the instance → Click </p> → Navigate to . and paste it into your terminal:</p><div><pre><code>ssh  your-key.pem ubuntu@your-instance-ip\n</code></pre></div><p>🔹 You are now connected to your EC2 instance! 🎉</p><h3><strong>Step 3: Install Required Tools</strong></h3><div><pre><code>apt update apt  docker.io\nsystemctl  docker\nusermod  docker  newgrp docker\ndocker </code></pre></div><div><pre><code>apt update apt  fontconfig openjdk-17-jre\njava wget  /usr/share/keyrings/jenkins-keyring.asc https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key\n |  /etc/apt/sources.list.d/jenkins.list  /dev/null\napt update apt  jenkins\nsystemctl  jenkins\n</code></pre></div><p>🔹 <strong>Retrieve Jenkins Initial Admin Password:</strong></p><div><pre><code> /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre></div><div><pre><code>apt update apt  gnupg software-properties-common\nwget  https://apt.releases.hashicorp.com/gpg | gpg  |  /usr/share/keyrings/hashicorp-archive-keyring.gpg  /dev/null\nlsb_release  |  /etc/apt/sources.list.d/hashicorp.list\napt update apt  terraform\nterraform </code></pre></div><div><pre><code>curl \nunzip awscliv2.zip\n ./aws/install\naws </code></pre></div><h3><strong>Step 4: Configure Security Group &amp; IAM Role</strong></h3><ul><li><p>Navigate to <strong>EC2 → Security Groups → Edit Inbound Rules</strong>.</p></li><li><ul><li> (or restrict to your IP for security).</li></ul></li></ul><p>✅ <strong>Create IAM Role for EC2 Access:</strong><strong>IAM → Roles → Create Role</strong>. and attach the following permissions:</p><ul><li><p> and assign it to your EC2 instance by going to the  →  → . Select  and click update role.</p></li></ul><h3><strong>Step 5: Configure AWS CLI Authentication</strong></h3><p>To authenticate Terraform and Jenkins with AWS, configure AWS CLI with an :</p><p>🔹 <strong>IAM → Users → Select your user</strong>. → Click .</p><p>3️⃣ Copy and store the  &amp; .</p><ul><li><p><em>Your AWS region (e.g., us-east-1)</em></p></li></ul><p>🔹 <strong>Verify AWS CLI is working:</strong></p><div><pre><code>aws ec2 describe-instances\n</code></pre></div><p>You should see details of your  instance. 🎯</p><h2><strong>💡 Provisioning AWS Infrastructure &amp; Deploying the 2048 Game using Terraform &amp; Docker</strong></h2><p>Now, we will  the  and set up the necessary infrastructure using .</p><h4><strong>1️⃣ Clone the Repository &amp; Configure Terraform</strong></h4><p>Start by cloning the repository and navigating to the Terraform setup directory:</p><div><pre><code>git clone https://github.com/Pravesh-Sudha/2048-game.git\n2048-game/terraform-setup\n</code></pre></div><p>Inside this directory, you’ll find a <a href=\"http://main.tf\" rel=\"noopener noreferrer\"></a> file. Adjust the Terraform configuration according to your needs:</p><p>Once the changes are done, save the file.</p><h4><strong>2️⃣ Initialize &amp; Apply Terraform Configuration</strong></h4><p>Now, initialize Terraform and provision the required AWS resources:</p><div><pre><code>terraform init       \nterraform plan       \nterraform apply </code></pre></div><p>Once Terraform completes execution, it will display the  of the newly created EC2 instance.</p><h4><strong>3️⃣ Connect to the EC2 Instance</strong></h4><p>After waiting  for the instance to be ready, use SSH to connect:</p><div><pre><code>ssh  key-name.pem ubuntu@&lt;public-ip-of-the-new-instance&gt;\n</code></pre></div><h4><strong>4️⃣ Install Docker &amp; Create a Dockerfile</strong></h4><p>Once connected, install  using:</p><div><pre><code>apt update\napt  docker.io\nsystemctl docker\nsystemctl start docker\n</code></pre></div><p>Now, create a  and paste the following content:</p><div><pre><code>apt-get update\napt-get  curl zip nginx\n\n /etc/nginx/nginx.conf\n\ncurl  /var/www/html/master.zip  https://codeload.github.com/gabrielecirulli/2048/zip/master\n /var/www/html  unzip master.zip 2048-master/ 2048-master master.zip\n\n</code></pre></div><h4><strong>5️⃣ Build &amp; Run the Docker Container</strong></h4><p>Now, build and run the Docker container:</p><div><pre><code>docker build  2048-game docker run  80:80 2048-game\n</code></pre></div><p>Open the  of the EC2 instance in your browser:</p><div><pre><code>http://&lt;public-ip-of-the-new-instance&gt;\n</code></pre></div><p>🎉  Your  is now running inside an <strong>NGINX-powered Docker container</strong> on your AWS EC2 instance.</p><h2>\n  \n  \n  💡 <strong>Automating the Deployment with Jenkins Pipeline</strong> 🚀\n</h2><p>Now that we’ve manually tested our application, it's time to automate the entire process! We'll create a  that will:</p><p>✅  from our GitHub repository <a href=\"https://github.com/Pravesh-Sudha/2048-game\" rel=\"noopener noreferrer\">2048-game</a><strong>Initialize and approve the Terraform configuration</strong> using credentials<strong>SSH into the newly provisioned EC2 instance</strong><strong>Install Docker on the instance</strong> for our 2048 game<strong>Build and run the Docker container</strong><strong>Access the application via the public IP</strong> of the deployed instance</p><p>Sounds interesting, right? Let's get started!</p><p>To get Jenkins Password, run the following command and log into Jenkins on port :</p><div><pre><code>sudo cat /var/lib/jenkins/secrets/initialAdminPassword\n</code></pre></div><p>Your Jenkins will be available at <strong>http://&lt;public-ip-address-of-ec2-instance&gt;:8080</strong></p><h3><strong>Step 1: Install Required Jenkins Plugins</strong></h3><p>First, navigate to  and install the following plugins:</p><p>These plugins are essential for handling SSH connections and executing Terraform commands within Jenkins.</p><h3><strong>Step 2: Configure Credentials in Jenkins</strong></h3><p>Now, we need to store our AWS credentials and SSH key securely.</p><ol><li><p>Go to <strong>Manage Jenkins → Credentials → Global credentials → Add credentials</strong></p></li><li><p>Add the following credentials:</p></li></ol><div><pre><code>- AWS_ACCESS_KEY_ID → Your AWS access key (Secret text)\n\n- AWS_SECRET_ACCESS_KEY → Your AWS secret access key (Secret text)\n\n- EC2_SSH_KEY → Upload your `.pem` key file (Secret file)\n</code></pre></div><p>These credentials will be used in our pipeline for authentication and deployment.</p><h3><strong>Step 3: Create a New Jenkins Pipeline</strong></h3><ol><li><p>Enter the name: </p></li><li><p>Select  and paste the repository URL:</p><pre><code>https://github.com/Pravesh-Sudha/2048-game\n</code></pre></li><li><p>Scroll down to the  section:</p></li></ol><div><pre><code>- Select Pipeline script from SCM\n\n- In Source, select Git and provide the same repository URL\n\n- Specify the branch as `main`\n</code></pre></div><h3><strong>Step 4: Run the Jenkins Pipeline</strong></h3><p>Now, click  and watch as Jenkins:<strong>Provisions an EC2 instance</strong> using Terraform<strong>SSHs into the instance and installs Docker</strong><strong>Builds the Docker image and runs the container</strong><strong>Outputs the Public IP of the running application</strong></p><p>At the end of the pipeline console logs, you'll see a <strong>link to your deployed 2048 game</strong>. Click on it, and you'll see the application running! 🎮✨</p><h3><strong>Step 5: (Optional) Add a Webhook for Auto-Deployments</strong></h3><p>If you want the deployment to trigger automatically whenever you push new changes to GitHub, you can set up a . However, since we're launching a new instance every time, it’s  in this setup.</p><p>In this blog, we successfully automated the deployment of the  using <strong>Jenkins, Terraform, and Docker</strong>. We started by setting up a , configuring AWS credentials, provisioning an EC2 instance with Terraform, and deploying our application inside a Docker container. By the end, we had a fully functional  that could spin up infrastructure and deploy the game with a single click! 🚀</p><p>This project demonstrated how powerful <strong>Infrastructure as Code (IaC)</strong> and  can be in modern DevOps workflows. Instead of manually provisioning servers and deploying applications, we let Jenkins handle the entire process—<strong>making deployments faster, consistent, and scalable.</strong></p><p>While this pipeline works great, here are some possible improvements: – Destroy AWS resources after deployment to optimize costs. – Use tools like  to track performance. – Use <strong>IAM roles and least privilege access</strong> for better security.<strong>Implement Blue-Green Deployments</strong> – Deploy updates without downtime.</p><p>By continuously refining our CI/CD pipeline, we move towards <strong>efficient, scalable, and production-ready deployments</strong>. I hope this blog helped you understand the <strong>automation behind deploying applications on AWS</strong>. Stay tuned for more DevOps projects! 💡🔧</p><p><strong>Have any questions or suggestions? Drop them in the comments below!</strong> 💬</p>","contentLength":9103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to manage clusters in Alibaba Cloud Elasticsearch for beginners","url":"https://dev.to/a_lucas/how-to-manage-clusters-in-alibaba-cloud-elasticsearch-for-beginners-3lm3","date":1739769996,"author":"A_Lucas","guid":1627,"unread":true,"content":"<p>Managing clusters in Alibaba Cloud Elasticsearch involves overseeing the resources and configurations that power your search and analytics engine. You ensure that you can effectively manage clusters to operate efficiently, scale seamlessly, and remain secure. Effective cluster management offers several benefits: <a href=\"https://www.alibabacloud.com/blog/alibaba-cloud-elasticsearch-performance-optimization_597092\" rel=\"noopener noreferrer\">faster query speeds through optimized configurations</a>, improved processing power with high-spec servers, and elastic scaling for additional disk space or node upgrades. Security also improves as clusters stay isolated in VPCs, access is controlled via whitelists, and role-based access control strengthens identity verification. This tutorial simplifies the process, making it accessible even if you're just starting out.</p><h2><strong>Setting Up Your Alibaba Cloud Elasticsearch Cluster</strong></h2><h3><strong>Prerequisites for Cluster Setup</strong></h3><h4><strong>Alibaba Cloud account and permissions</strong></h4><p>Before creating an Elasticsearch cluster, you need an <a href=\"https://www.alibabacloud.com/help/en/es/user-guide/create-an-alibaba-cloud-elasticsearch-cluster\" rel=\"noopener noreferrer\">Alibaba Cloud account</a>. You can register for one through the official registration page. Ensure that your account has completed real-name verification. This step is essential for accessing Alibaba Cloud services.</p><p>You also need the necessary permissions to manage resources. If you are part of a team, confirm that your account has the required roles assigned.</p><h4><strong>Understanding billing options</strong></h4><div><table><thead><tr></tr></thead><tbody><tr><td>Requires an upfront fee and is more cost-effective  for long-term use.</td></tr><tr><td>Charged hourly, suitable for short-term use or  testing, and can be released at any time.</td></tr></tbody></table></div><p>Choose the billing method that aligns with your project needs. For example, if you are testing or experimenting with the ELK stack, the pay-as-you-go option provides flexibility.</p><h3><strong>Steps to Create an Alibaba Cloud Elasticsearch Cluster</strong></h3><h4><strong>Choosing the right Elasticsearch version</strong></h4><p>When creating an Elasticsearch instance, select a version compatible with your application. Alibaba Cloud Elasticsearch supports multiple versions, ensuring flexibility for different use cases. Always choose the latest stable version for optimal performance and security.</p><h4><strong>Selecting instance types and storage</strong></h4><p>Alibaba Cloud Elasticsearch <a href=\"https://alibaba-cloud.medium.com/alibaba-cloud-elasticsearch-whats-new-and-latest-features-48ee771cb28\" rel=\"noopener noreferrer\">separates storage from computing</a>, reducing costs and improving performance. Select an instance type based on your workload. For example, a high-spec instance is ideal for data-intensive tasks.</p><p>The following table shows how cluster configurations affect <a href=\"https://www.alibabacloud.com/blog/alibaba-cloud-elasticsearch-performance-optimization_597092\" rel=\"noopener noreferrer\">response times</a>:</p><div><table><thead><tr><th><strong>Average RT for 10 concurrent retrievals</strong></th><th><strong>Average RT for 50 concurrent retrievals</strong></th><th><strong>Average RT for 100 concurrent retrievals</strong></th><th><strong>Average RT for 200 concurrent retrievals</strong></th></tr></thead><tbody></tbody></table></div><h3><strong>Initial Cluster Configurations</strong></h3><h4><strong>Configuring cluster name and region</strong></h4><p>When building an Elasticsearch cluster, assign a unique name to identify it easily. Choose a region close to your users to minimize latency. Specifying a zone is unnecessary during setup, simplifying the process.</p><h4><strong>Setting up IP whitelists for access control</strong></h4><p><a href=\"https://www.alibabacloud.com/blog/securing-your-elasticsearch-clusters-comprehensive-access-control-methods_601326\" rel=\"noopener noreferrer\">IP whitelists enhance security</a> by restricting access to specific IP addresses. Add your host's IP address to the whitelist to enable public network access. This step prevents unauthorized access and ensures your data remains secure.</p><p> Regularly update your IP whitelist to reflect changes in your network configuration.</p><h2><strong>Managing Clusters in Alibaba Cloud Elasticsearch</strong></h2><h3><strong>Adding and Removing Nodes</strong></h3><h4><strong>When and why to scale your cluster</strong></h4><p>Scaling your cluster ensures it can handle increasing workloads or optimize resource usage. You should add nodes when your cluster experiences high traffic, increased data ingestion, or slow query responses. Removing nodes may be necessary to reduce costs or reallocate resources. Maintaining at least two data nodes is essential for reliability. For multi-zone clusters, balance the number of nodes across zones to enhance stability.</p><h4><strong>Steps to add or remove nodes</strong></h4><p>Follow these steps to manage nodes effectively:</p><p>Use the Alibaba Cloud console to add nodes to your cluster.</p><p>Ensure the new nodes have sufficient resources, such as memory and disk space.</p><p>Elasticsearch automatically redistributes shards across the new nodes</p><p>Verify that the cluster has enough resources to handle the workload after node removal.</p><p>Use the GET _cat/indices?v command to check resource usage.</p><p>Adjust the number of replica shards to avoid allocation errors.</p><h4><strong>Vertical scaling (upgrading instance types)</strong></h4><h4><strong>Horizontal scaling (adding more nodes)</strong></h4><p>Horizontal scaling adds more nodes to distribute the workload. Elasticsearch's distributed architecture simplifies this process. This method is suitable for applications with varying traffic patterns or high availability needs. Horizontal scaling minimizes downtime and enhances workload distribution.</p><div><table><thead><tr></tr></thead><tbody><tr><td>Adapts to varying traffic with the ability to add or  remove resources.</td></tr><tr><td>Efficient for consistent high-resource demand by  boosting existing capabilities.</td></tr><tr><td>Suitable for applications designed to run on  multiple servers.</td></tr><tr><td>Facilitates less downtime, ideal for high  availability needs.</td></tr><tr><td>Excels in distributing workloads across multiple  nodes.</td></tr></tbody></table></div><h3><strong>Configuring Security Settings</strong></h3><p>IP whitelists restrict access to specific IP addresses, enhancing security. Regularly update your whitelist to reflect changes in your network. Use the Alibaba Cloud console to add or remove IP addresses. This step ensures only authorized users can access your cluster.</p><h4><strong>Enabling alert features for monitoring</strong></h4><p>Enable alert features to monitor your cluster's health and performance. Alerts notify you of critical events, such as high CPU usage or abnormal cluster statuses. Configure alerts for key metrics like disk usage and cluster health. This proactive approach helps you address issues before they escalate, ensuring your cluster operates efficiently.</p><h2><strong>Monitoring and Maintaining Your Cluster</strong></h2><h3><strong>Monitoring Cluster Health</strong></h3><h4><strong>Using the Alibaba Cloud console</strong></h4><p>Monitoring your cluster's health ensures optimal performance and stability. The Alibaba Cloud console provides tools to simplify this process:</p><p><a href=\"https://trendmicro.com/cloudoneconformity/knowledge-base/alibaba-cloud/AlibabaCloud-ACK/cluster-check.html\" rel=\"noopener noreferrer\"></a>: Automates health checks     to identify and resolve issues proactively. This feature minimizes downtime and enhances stability.</p><p><a href=\"https://www.alibabacloud.com/en/product/cloud-monitor?_p_lc=1\" rel=\"noopener noreferrer\"></a>: Tracks metrics and     detects service availability. It enables you to monitor resource usage and health status while setting alarms for critical metrics.</p><p>These tools help you stay informed about your cluster's condition and act promptly when issues arise.</p><h4><strong>Key metrics to track (e.g., CPU, memory, disk usage)</strong></h4><p>Tracking key metrics is essential for maintaining your Elasticsearch cluster. Use the following table to understand the most critical metrics:</p><div><table><tbody><tr><td>Indicates overall health (Red, Yellow, Green).</td></tr><tr><td>Total number of nodes, including successful and  failed nodes.</td></tr><tr><td>Percentage of JVM heap memory used.</td></tr><tr><td>Percentage of CPU used by Elasticsearch.</td></tr><tr><td>Percentage of disk space used.</td></tr><tr><td>Number of active shards in the cluster.</td></tr><tr><td>Ratio of cache hits to total requests.</td></tr></tbody></table></div><p>Monitoring these metrics helps you identify bottlenecks and optimize resource usage.</p><h4><strong>Creating, deleting, and optimizing indices</strong></h4><p>Efficient index management improves query performance and reduces resource consumption. Follow these <a href=\"https://www.alibabacloud.com/blog/alibaba-cloud-elasticsearch-performance-optimization_597092\" rel=\"noopener noreferrer\">best practices</a>:</p><p>1)Create one type per index and separate indexes for data with different fields. This approach avoids large index queries.</p><p>2)Merge read-only indexes into larger segments to reduce fragmentation and memory usage.</p><p>3)Disable historical data indexes that are not queried. This saves JVM memory.</p><p>4)Use batch requests for better performance. Commit 5 MB to 15 MB of data at a time.</p><p>These practices ensure your indices remain optimized and manageable.</p><h4><strong>Setting up automated snapshots</strong></h4><p><a href=\"https://www.alibabacloud.com/help/en/es/user-guide/data-backup-overview\" rel=\"noopener noreferrer\">Automated snapshots</a> protect your data and simplify recovery. Follow these steps to set up snapshots:</p><p>3)Use Snapshot Lifecycle Management (SLM) to automate snapshot handling     and retention.</p><p>This setup ensures your data remains secure and recoverable.</p><h4><strong>Restoring data from backups</strong></h4><p>Restoring data from backups is straightforward. You can restore snapshots to the original cluster or a different one using a shared OSS repository. Follow these steps:</p><p>1)Register a snapshot repository on Alibaba Cloud OSS.</p><p>2)Create automatic or manual snapshots of your data.</p><p>3)Use SLM for automated snapshot handling.</p><p>4)Restore data to the desired cluster.</p><p>This process ensures reliable data recovery and management.</p><h2><strong>Troubleshooting Common Issues in Alibaba Cloud Elasticsearch</strong></h2><h3><strong>Resolving Connection Issues</strong></h3><h4><strong>Diagnosing access problems</strong></h4><div><table><tbody><tr><td>The Elasticsearch cluster cannot be accessed over  the Internet.</td><td>Ensure the IP address of your device is whitelisted  and check network connectivity using ping or telnet commands.</td></tr><tr><td>The Elasticsearch cluster cannot be accessed over an  internal network.</td><td>Verify that the client is in the same VPC and test  connectivity with ping commands.</td></tr><tr><td>The Elasticsearch cluster is unhealthy.</td><td>Check the cluster's health status using the GET  _cat/health?v command and monitor resource usage.</td></tr></tbody></table></div><h4><strong>Fixing IP whitelist misconfigurations</strong></h4><p>Misconfigured IP whitelists can block access to your cluster. Follow these steps to fix the issue:</p><p>2)Add the private IP address of your host to the cluster's private IP     address whitelist for internal access.</p><p>3)Configure a whitelist for your host's IP address for Kibana access,     ensuring both public and private IPs are accounted for.</p><p> Regularly update your whitelist to reflect changes in your network configuration.</p><h3><strong>Addressing Performance Problems</strong></h3><h4><strong>Solutions for high resource usage</strong></h4><p>Expand the cluster by adding more nodes or upgrading existing ones.</p><p>Distribute bulk requests into smaller batches to reduce CPU strain.</p><p>Cancel long-running searches using the tasks management API.</p><p>Avoid resource-intensive searches, such as fuzzy or wildcard queries.</p><p> Optimizing your cluster's configuration ensures efficient resource utilization.</p><h4><strong>Common causes of indexing failures</strong></h4><p>Node capacity issues during spikes in queries or write requests.</p><p>Memory overload caused by excessive index cache usage.</p><p>Low-spec cluster configurations.</p><p>Disk usage exceeding 85%, preventing new shard allocation.</p><h4><strong>Steps to resolve indexing issues</strong></h4><p>Follow these steps to fix indexing problems:</p><p>1)Run the POST /Index name/_cache/clear?fielddata=true command to clear     the cache for indexes.</p><p>2)Use the GET /_cat/indices?v command to check shard distribution.</p><p>3)Reduce write concurrency and delete invalid indexes to free up     resources.</p><p>4)Upgrade the cluster configuration if issues persist..</p><p>Managing clusters in Alibaba Cloud Elasticsearch becomes easier when you follow a structured approach. Start by creating an account, <a href=\"https://www.alibabacloud.com/blog/how-to-create-an-alibaba-cloud-elasticsearch-cluster-and-log-on-to-the-kibana-console_600201\" rel=\"noopener noreferrer\">configuring your cluster</a>, and ensuring proper access control. Use elastic scaling to handle growing workloads and <a href=\"https://www.alibabacloud.com/blog/alibaba-cloud-elasticsearch-performance-optimization_597092\" rel=\"noopener noreferrer\">optimize performance with high-spec servers</a>. Regular monitoring is essential for maintaining cluster health and preventing downtime. Proactive maintenance saves resources and ensures smooth operations. To deepen your knowledge, explore Alibaba's documentation on creating clusters, managing access, and using API operations. These resources help you master Elasticsearch cluster management and unlock its full potential.</p><h3><strong>What is the first step in installing Elasticsearch on Alibaba Cloud?</strong></h3><p>You need to create an Alibaba Cloud account and complete real-name verification. Afterward, access the Alibaba Cloud console to start creating a general-purpose business edition instance. This instance serves as the foundation for installing Elasticsearch and managing your cluster.</p><h3><strong>How do you begin working with ELK on Alibaba Cloud?</strong></h3><p>Start by installing the ELK stack. This involves installing Elasticsearch, Logstash, and Kibana. Each component plays a role in data ingestion, storage, and visualization. Ensure you install the required Elasticsearch client to interact with your cluster effectively.</p><h3><strong>What are the benefits of installing Kibana with Elasticsearch?</strong></h3><p>Kibana provides a user-friendly interface for visualizing data stored in Elasticsearch. It simplifies data analysis by offering dashboards, charts, and graphs. Installing Kibana enhances your ability to monitor and manage your Elasticsearch cluster.</p><h3><strong>How do you troubleshoot issues when accessing the Alibaba instance?</strong></h3><p>Check your IP whitelist settings to ensure your device's IP address is authorized. Verify network connectivity using ping or telnet commands. If problems persist, review your cluster's health status and resource usage through the Alibaba Cloud console.</p><h3><strong>What is the role of Logstash in the ELK stack?</strong></h3><p>Logstash processes and transforms data before sending it to Elasticsearch. Installing Logstash allows you to collect data from various sources, filter it, and store it efficiently. This ensures your Elasticsearch cluster receives clean and structured data.</p>","contentLength":12388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serverless vs. Kubernetes: Choosing the Right Architecture for Your Cloud Applications","url":"https://dev.to/ibne_sabidsaikat_1443282/serverless-vs-kubernetes-choosing-the-right-architecture-for-your-cloud-applications-342o","date":1739769939,"author":"Ibne sabid saikat","guid":1626,"unread":true,"content":"<p>Introduction\nChoosing between Serverless computing and Kubernetes (K8s) is a crucial decision for cloud architects and developers. Both architectures provide scalability, automation, and flexibility, but they serve different use cases. In this blog, we’ll compare Azure Functions (Serverless) and Azure Kubernetes Service (AKS), helping you determine the best fit for your application.</p><p>Understanding Serverless Computing\nServerless computing abstracts away infrastructure management, allowing developers to focus on writing code. In Azure, the primary serverless solution is Azure Functions.</p><p>Key Characteristics of Serverless\nEvent-driven execution: Functions execute in response to triggers (HTTP requests, database changes, message queues, etc.).<p>\nFully managed scaling: Azure automatically scales functions up or down based on demand.</p>\nPay-per-execution pricing: You only pay for the compute resources used during execution.<p>\nNo server management: Developers don’t worry about provisioning or maintaining servers.</p>\nWhen to Use Serverless (Azure Functions)<p>\n✅ Real-time event processing (e.g., IoT data ingestion, real-time analytics)</p>\n✅ API backends for lightweight web and mobile apps<p>\n✅ Automation and task scheduling (e.g., CRON jobs, background tasks)</p>\n✅ Low-traffic workloads with unpredictable scaling needs</p><p>Limitations of Serverless\n❌ Cold start delays for infrequently used functions<p>\n❌ Limited execution time (Azure Functions have a default timeout of 5 minutes)</p>\n❌ Less control over environment and networking</p><p>Understanding Kubernetes (AKS)\nKubernetes is a container orchestration platform designed for managing containerized applications at scale. Azure Kubernetes Service (AKS) simplifies the deployment, management, and scaling of Kubernetes clusters in Azure.</p><p>Key Characteristics of Kubernetes\nContainer-based deployment: Applications are packaged as containers and managed across a cluster.<p>\nScalability and load balancing: Automatically scales applications based on demand.</p>\nHigh availability and fault tolerance: Ensures workloads are resilient and distributed across nodes.<p>\nSupports microservices architecture: Ideal for complex applications with multiple components.</p>\nWhen to Use Kubernetes (AKS)<p>\n✅ Microservices-based applications with multiple interdependent services</p>\n✅ Long-running applications that require persistent workloads<p>\n✅ Custom networking, security, and infrastructure control</p>\n✅ Multi-cloud and hybrid cloud deployments</p><p>Limitations of Kubernetes\n❌ Operational complexity (Managing a Kubernetes cluster requires DevOps expertise)<p>\n❌ Higher costs (Compute resources are always running, even if not in use)</p>\n❌ Longer setup time compared to serverless</p><p>Serverless vs. Kubernetes: A Side-by-Side Comparison\nFeatureServerless (Azure Functions)Kubernetes (AKS)ScalabilityAuto-scales instantlyManual or auto with HPACost ModelPay-per-useAlways-on compute costsManagementFully managed by AzureRequires operational effortStartup TimeCold start delays possibleAlways runningBest ForLightweight, event-driven appsLong-running, complex applications</p><p>Real-World Use Cases\nCase Study: An E-Commerce Platform<p>\nServerless for Payments &amp; Notifications: The platform uses Azure Functions to handle payment processing and order notifications, scaling automatically based on incoming traffic.</p>\nKubernetes for Core Services: The product catalog, user authentication, and recommendation engine run on AKS to maintain high availability and handle thousands of concurrent users.<p>\nHow to Choose the Right Architecture</p>\nAsk yourself these questions:</p><p>Do I need persistent workloads? → Choose Kubernetes\nIs my application event-driven with intermittent traffic? → Choose Serverless<p>\nDo I want minimal infrastructure management? → Choose Serverless</p>\nDo I need fine-grained control over networking and security? → Choose Kubernetes\nThere’s no one-size-fits-all approach. If you’re building lightweight, event-driven applications, Azure Functions is a great choice. If you need full control over infrastructure, AKS is the way to go. By understanding the strengths and limitations of both, you can make an informed decision for your cloud applications.</p><p>What’s your experience with Serverless and Kubernetes? Share your thoughts in the comments!</p>","contentLength":4260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Most Popular Backend Languages: Choose Wisely, Code Powerfully","url":"https://dev.to/web_dev-usman/most-popular-backend-languages-choose-wisely-code-powerfully-169o","date":1739769841,"author":"Muhammad Usman","guid":1625,"unread":true,"content":"<p>, , , , , , , —there are so many different options, and it can be overwhelming to figure out which one to actually pick. So today, let's break down the most popular backend languages and which one of them is the is best for you.</p><p>Let's dive right in with . Now, Java is ideal for the enterprise-level developer. Think about building large-scale enterprise software like banking or financial services. Now, Java can be used for a ton of different things, but typically you'll be doing . You may be working on Android apps or dealing with .</p><p>Now,  are always in high demand, especially in the enterprise sector, and compared to a lot of other languages, they have very high job stability. Overall, you should choose Java for its enterprise-grade applications due to its robust performance, scalability, and vast ecosystems of libraries and frameworks.</p><p>So now we move on to . Now, Python is typically best for newcomers due to its readability and simplicity. It's used for web applications, , , and automation. Now, you can use Python for a ton of different things, but typically when it comes to getting a job with Python, you need to be really good in a specific framework or area or have some other languages under your belt as well.</p><p>If you're just looking for some hobby projects, Python is great to pick up, but if you want a long-term career, you need to be really good in a specific area or potentially learn some other languages as well. Now, overall, you should opt for Python if you value developer productivity and versatility with extensive libraries for web development, data analysis, and machine learning.</p><p>So now we move on to . JavaScript, or , is best for those of you that want to be full-stack developers. That's because, with a single language, you can write code on both the front end and the back end. JavaScript can be used in a lot of different places, but typically it's going to be used for building web applications—in this case, web backends or real-time applications like chat apps.</p><p>With that in mind, if you're a , you're going to have a lot of prospects for startups or earlier-stage companies because they're typically using JavaScript for their entire stack. They don't want to hire seven different developers that know all these different languages—they want one that can write on both the front end and the back end. Overall, you should select JavaScript or Node.js for full-stack development in real-time applications, benefiting from a non-blocking I/O model and a unified language across the  and the .</p><p>So now we can move on to . Now, Ruby is for the —those of you that want to push products to market as quickly as possible. It's really popular in the e-commerce space, with websites like Shopify, for content management, and for database web </p><p>Ruby is obviously used with , and anyone who's really good in Ruby is always going to have job prospects because a lot of tech firms are constantly looking for Ruby on Rails devs. With that in mind, you should use Ruby on Rails for  and startup environments where time to market and convention over configuration are prioritized.</p><p>So now we can discuss the dreaded . Now, everyone loves to hate on PHP, but the truth is it's very useful, and it's really used all over the web. It's very popular for small business websites, blogs—think things like WordPress, which is completely built on PHP. That means that it's in pretty high demand. A lot of new developers aren't learning it, and if you know PHP, you're going to be able to pick up a lot of jobs, especially servicing old sites, which a lot of companies need, and working with platforms like WordPress.</p><p>It's big for e-commerce, and it actually is pretty fast in terms of building web apps. Overall, you should go with PHP for a wide range of web development needs, particularly effective in content management systems and sites with dynamic content.</p><p>So now we get into . Now, C is ideal for those of you invested in the <code>Microsoft ecosystem—Windows</code>, , etc. It's very popular for enterprise-grade level backends, building games (think frameworks like Unity), and for desktop applications as well.  always has high job demand. People are always looking for experienced C developers—again, especially in that enterprise sector where you're building larger and larger applications. Now, overall, you should choose C for robust desktop and enterprise applications within the Microsoft ecosystem, leveraging the  extensive capabilities.</p><p>So now we get into Go, or , Go is used for building highly performant and scalable systems. It's really used for building distributed systems, , and large-scale cloud services. It's increasingly popular in the data streaming space as well as the cloud services space and really for those that need the utmost performance. Overall, you should pick Go for concurrent applications and microservices where performance and efficiency are critical, thanks to its lightweight goroutines and out-of-the-box scalability.</p><p>Finally, we get to . Now, this is ideal for  that need absolute control over system resources without compromising on performance, especially when safety and concurrency are of utmost importance. Now, Rust can be used for so many different things—I'm just going to list them out here: network services, simulation engines for things like virtual reality platforms, game engines, and really anything that's performance-critical.</p><p>You can build so many different types of  in Rust, and a lot of people are migrating to Rust now because of how much they love the language. Rust is constantly growing in demand, especially when it comes to high-performance computing, and it's also used for things like cryptocurrency platforms and embedded systems. Overall, you should select Rust for system-level programming where safety and performance are paramount, offering memory safety without garbage collection.</p><p>So there you have it. Those are the eight <code>most popular backend languages</code> I covered in this list. There are some more, but these are the most popular, and hopefully, this helped you make the decision on which one you want to pick. If you want to become a software developer.</p><p><strong>Thanks for reading to the end, and have a good day! 🚀</strong></p><p>Let’s grow, learn, and build amazing things together!</p><p>Don’t forget to ,  it to your list, and . </p><p>Stay connected with me on my other platforms:</p>","contentLength":6349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stay ahead in web development: latest news, tools, and insights #72","url":"https://dev.to/urbanisierung/stay-ahead-in-web-development-latest-news-tools-and-insights-72-4g65","date":1739769430,"author":"Adam","guid":1624,"unread":true,"content":"<p>Signup <a href=\"https://weeklyfoo.com\" rel=\"noopener noreferrer\">here</a> for the newsletter to get the weekly digest right into your inbox.</p><p>weeklyfoo #72 is here: your weekly digest of all webdev news you need to know! This time you'll find 46 valuable links in 6 categories! Enjoy!</p><ul><li><a href=\"https://www.lesswrong.com/posts/KFJ2LFogYqzfGB3uX/how-ai-takeover-might-happen-in-2-years?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">How AI Takeover Might Happen in 2 Years</a>: I’m not a natural “doomsayer.” But unfortunately, part of my job as an AI safety researcher is to think about the more troubling scenarios. by Joshua Clymer</li></ul><ul><li><a href=\"https://github.com/nizzyabi/Mail0?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Mail0</a>: open source gmail alternative by nizzy</li><li><a href=\"https://github.com/upstash/jstack?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">JStack</a>: Build seriously fast, lightweight and end-to-end typesafe Next.js apps by jstack.app</li><li><a href=\"https://stocksnap.io/?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">StockSnap.io</a>: Beautiful Free Stock Photos. New CC0 images added daily, free from copyright restrictions. by stocksnap.io</li><li><a href=\"https://github.com/unjs/consola?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Consola</a>: Elegant Console Logger for Node.js and Browser by UnJS</li><li><a href=\"https://github.com/rajibola/human-regex?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Human Regex</a>: Human-friendly regular expression builder with English-like syntax. by Ridwan Ajibola</li><li><a href=\"https://localsend.org/?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">LocalSend</a>: Share files to nearby devices. Free, open-source, cross-platform. by Tien Do Nam</li><li><a href=\"https://a0.dev/?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">a0.dev</a>: Like v0 but for React Native apps. by a0.dev</li><li><a href=\"https://github.com/ohmjs/ohm?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Ohm</a>: A library and language for building parsers, interpreters, compilers, etc. by ohmjs.org</li><li><a href=\"https://github.com/bkrem/react-d3-tree?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">React D3 Tree</a>: React component to create interactive D3 tree graphs by Ben Kremer</li><li><a href=\"https://github.com/xiaolin/react-image-gallery?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">React Image Gallery</a>: React carousel image gallery component with thumbnail support by Xiao Lin</li><li><a href=\"https://github.com/daviddarnes/share-button?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Share Button</a>: A Web Component to share web pages using the native OS sharing options by David Darnes</li><li><a href=\"https://github.com/visprex/visprex?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Visprex</a>: Visualise your CSV files in seconds without sending your data anywhere by visprex.com</li><li><a href=\"https://postspark.app?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">PostSpark</a>: Beautify Screenshots by Dan</li><li><a href=\"https://github.com/fullsoak/fullsoak?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">FullSoak</a>: a no-build TypeScript fullstack SSR-first framework for developing fast web applications with a shallow learning curve by fullsoak</li><li><a href=\"https://litur.app/?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Litur</a>: Find, collect and organize all the colors around you. by litur.app</li><li><a href=\"https://www.phase.com?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Phase</a>: Simple Animation For Product Designers by phase.com</li><li><a href=\"https://github.com/prazzon/flexbox-labs?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Flexbox Labs</a>: A web app for creating flexible layouts with the power of CSS Flexbox. by Praise Ogunleye</li><li><a href=\"https://github.com/leaverou/style-observer?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Style Observer</a>: Run JS when a CSS property changes. Any CSS property. by Lea Verou</li><li><a href=\"https://github.com/branchseer/oxidase?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Oxidase</a>: Transpiles TypeScript at the Speed of Parsing by branchseer</li><li><a href=\"https://github.com/t3dotgg/unduck?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Unduck</a>: A fast, local-first \"search engine\" for !bang users by Theo Browne</li></ul><ul><li><a href=\"https://tabboo.xyz/?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">TabBoo</a>: Add random jumpscares to sites you're trying to avoid by tabboo.xyz</li></ul><ul><li><a href=\"https://deno.com/blog/intro-to-wasm?utm_source=weeklyfoo&amp;utm_medium=web&amp;utm_campaign=weeklyfoo-72&amp;ref=weeklyfoo\" rel=\"noopener noreferrer\">Intro to Wasm in Deno</a>: JavaScript is a scripting language—distant from the machine code your CPU actually consumes. But JavaScript has a way to execute binary machine code, or something close to it, called WebAssembly. WebAssembly, or Wasm, is a low-level, portable binary format that runs at near-native speeds in the browser. by Andy Jiang, David Sherret</li></ul><p>Want to read more? Check out the full article <a href=\"https://weeklyfoo.com/foos/foo-072/\" rel=\"noopener noreferrer\">here</a>.</p>","contentLength":2565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The this Keyword in JavaScript 🤯","url":"https://dev.to/rating_rai_/the-this-keyword-in-javascript-2fbn","date":1739769195,"author":"Rating Rai","guid":1623,"unread":true,"content":"<p>The this Keyword in JavaScript 🤯\n🔹 What is this?<p>\nThe this keyword refers to the object that is currently executing the function. Its value depends on how and where a function is called.</p></p><p>🔹 Different Cases of this:</p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fiufikb92ua6ujmisv6w7.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fiufikb92ua6ujmisv6w7.png\" alt=\"Image description\" width=\"800\" height=\"565\"></a>\n✅ Key Takeaways:<p>\n✔ this behaves differently in arrow functions</p>\n✔ Depends on the calling context<p>\n✔ In strict mode (\"use strict\"), this in the global scope is undefined</p></p><p>🔥 Challenge: Can you explain the difference between this in an arrow function vs a regular function? Let's discuss in the comments!</p>","contentLength":529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sports on Glance: Revolutionizing How American Fans Experience Sports on Their Lock Screen on Android","url":"https://dev.to/adambrooks1231/sports-on-glance-revolutionizing-how-american-fans-experience-sports-on-their-lock-screen-on-2m2a","date":1739769142,"author":"Adam Brooks","guid":1622,"unread":true,"content":"<p>The way American sports fans consume their favorite games is undergoing a remarkable transformation, thanks to Glance—an innovative feature integrated into Android lock screens. With Glance Sports, fans can access real-time updates, personalized scores, and breaking news directly from their lock screen. This feature seamlessly transforms your lock screen into a dynamic hub for sports, making it easier than ever to stay connected to the action.</p><h2>\n  \n  \n  Real-Time Hockey Updates at Your Fingertips\n</h2><p>Imagine keeping up with every thrilling moment of a hockey game without switching apps or opening browsers. During a recent matchup where the Capitals extended their point streak to 10 games by holding off the Penguins, fans enjoyed instant updates through Glance. Instead of manually refreshing apps, hockey enthusiasts saw real-time scores, live player stats, and key game moments—like goals, penalties, and power plays—right on their lock screen.\nThis seamless access to sports information eliminates distractions and keeps fans informed without disrupting their routines. Glance enhances the fan experience by delivering timely and engaging updates at a glance, directly on the lock screen.</p><h2>\n  \n  \n  Stay Ahead with NBA Trade Intelligence\n</h2><p>The basketball world thrives on trade rumors and breaking news, and Glance ensures you're always in the loop. Whether the Nets are asking a steep price for Cam Johnson or the Pistons are considering a strategic shift, fans can stay updated in real-time. With Glance Sports, your Android lock screen becomes a window into these developments, providing instant notifications and relevant details as they unfold.\nThis immediacy allows basketball enthusiasts to track their favorite teams and players effortlessly, ensuring they never miss a beat in the fast-paced NBA world.</p><h2>\n  \n  \n  Seamless Integration with Android Lock Screens\n</h2><p>What sets Glance apart is its effortless integration with Android lock screens. Unlike traditional sports apps that require downloads, constant updates, and manual navigation, Glance is already pre-installed on many Android devices. From the moment you activate it, Glance Sports begins delivering curated content tailored to your interests, right on your lock screen.\nThis innovative feature saves time and eliminates the need for app-switching, making it a highly convenient way to stay informed. Whether you're tracking a live game or catching up on trade rumors, Glance ensures that your lock screen becomes an essential tool for staying connected to the sports world.</p><h2>\n  \n  \n  Personalization and Enhanced Fan Engagement\n</h2><p>Glance goes beyond generic updates by offering a personalized experience. By learning from your interactions, it adapts to your preferences, ensuring that the most relevant content appears on your lock screen.\nFor example, if you’re a die-hard hockey fan following the Capitals or an NBA enthusiast tracking trade rumors, Glance Sports will prioritize updates about those teams or players. Over time, this feature fine-tunes its recommendations, ensuring that your lock screen delivers precisely what you care about most.<p>\nThis intelligent customization extends beyond scores and news. Whether it's in-depth player profiles, team statistics, or injury updates, Glance Sports ensures you get a holistic view of your favorite sports. The result? A lock screen that’s informative and uniquely tailored to your passions.</p></p><h2>\n  \n  \n  Beyond the Scores: A Comprehensive Sports Experience\n</h2><p>While many sports platforms focus solely on scores, Glance offers a broader perspective. It provides a complete picture of the sporting world, from player stats and team updates to management decisions and injury reports. This depth of coverage makes Glance an invaluable companion for serious sports fans, all from the convenience of your lock screen.</p><h2>\n  \n  \n  The Advantage of Pre-Installation\n</h2><p>One of the standout features of Glance is its pre-installed nature on many Android devices. There’s no need to search app stores, manage downloads, or worry about updates. Glance is ready to go the moment you activate your phone. This ease of access makes Glance an ideal solution for anyone looking to stay connected to the sports world without added hassle.</p><h2>\n  \n  \n  Real-Time Analysis and Insights\n</h2><p>Glance doesn’t just stop at delivering news—it provides expert analysis and insights directly on your lock screen. When significant events like major trade rumors or game-winning plays occur, Glance offers context and commentary to help you understand their broader implications. This added layer of depth ensures that fans don’t just stay informed but also gain valuable insights into the world of sports.</p><h2>\n  \n  \n  A Socially Connected Sports Experience\n</h2><p>With Glance, your lock screen becomes more than just an information hub—it’s also a gateway to the sports community. Fans can see reactions to major plays, share their thoughts, and stay connected to the larger conversation surrounding their favorite teams and leagues. This integration of social engagement enhances the overall fan experience, all without needing to switch between multiple apps or platforms.</p><p>Glance is redefining how American sports fans engage with their favorite games through its innovative integration on Android lock screens. By delivering real-time updates, personalized content, and in-depth insights, Glance Sports transforms your lock screen into a dynamic and interactive sports hub. This seamless, pre-installed feature eliminates the need for additional apps and ensures that fans stay connected effortlessly. For anyone looking to revolutionize how they consume sports content, Glance is the ultimate game-changer—right there on your Android lock screen.</p>","contentLength":5729,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CDK Drag Drop: CdkDrag cannot be dropped back to origin DropList in Stacked DropList","url":"https://dev.to/remruat_thanga_9693aabc12/cdk-drag-drop-cdkdrag-cannot-be-dropped-back-to-origin-droplist-in-stacked-droplist-k3l","date":1739768705,"author":"Remruat Thanga","guid":1621,"unread":true,"content":"<p>I'm working on the idea of 2 dimensional drag and drop.<a href=\"https://stackblitz.com/~/github.com/NanobyteRuata/drag-drop-test?file=src/app/app.component.html\" rel=\"noopener noreferrer\">Stackblitz</a></p><p>Currently, I'm trying out stacked droplists (one row droplist and multiple column droplists).</p><p>I have a list for rows. Each row has an array of children which I consider as columns.</p><p>I want to be able to drag a column and be able to drop it in the parent row list, sibling column lists or back into the origin column list.</p><p>Everything is working as expected EXCEPT dragging back into the origin column list.</p><p>I have tried including/excluding the the origin column list id in the [cdkDropListConnectedTo] but it doesn't seem to solve the issue.</p><p>As far as I have debugged, (cdkDragEntered) is not even triggered on the origin column list's dropList.</p>","contentLength":703,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recursion in Java","url":"https://dev.to/neelakandan_ravi_2000/recursion-in-java-27pd","date":1739768000,"author":"Neelakandan R","guid":1615,"unread":true,"content":"<p>In Java, Recursion is a process in which a function calls itself directly or indirectly is called recursion and the corresponding function is called a recursive function.</p><p><u><strong>case Condition in Recursion</strong></u></p><p>In the recursive program, the solution to the base case is provided and the solution to the bigger problem is expressed in terms of smaller problems.</p><div><pre><code>int fact(int n)\n{\n    if (n &lt; = 1) // base case\n        return 1;\n    else    \n        return n*fact(n-1);    \n}\n</code></pre></div><p>In the above example, the base case for n &lt; = 1 is defined and the larger value of a number can be solved by converting it to a smaller one till the base case is reached. </p><p>The idea is to represent a problem in terms of one or more smaller sub-problems and add base conditions that stop the recursion. For example, we compute factorial n if we know the factorial of (n-1). The base case for factorial would be n = 0. We return 1 when n = 0</p><div><pre><code>// Factorial using recursion\nclass GFG {\n\n    // recursive method\n    int fact(int n)\n    {\n        int result;\n\n        if (n == 1)\n            return 1;\n        result = fact(n - 1) * n;//ex-4 mean 4*3*2*1\n        return result;//ex-4 mean ans--24\n    }\n}\n\n// Driver Class\nclass Recursion {\n\n    // Main function\n    public static void main(String[] args)\n    {\n        GFG f = new GFG();\n\n        System.out.println(\"Factorial of 3 is \"\n                           + f.fact(3));\n        System.out.println(\"Factorial of 4 is \"\n                           + f.fact(4));\n        System.out.println(\"Factorial of 5 is \"\n                           + f.fact(5));\n    }\n}\n\n</code></pre></div><p>Factorial of 3 is 6\nFactorial of 4 is 24</p>","contentLength":1604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Set Is Just Built Different 😤💪","url":"https://dev.to/mattlewandowski93/set-is-just-built-different-484f","date":1739767796,"author":"Matt Lewandowski","guid":1614,"unread":true,"content":"<p>The title says it all 😤. Let's talk about JavaScript's most slept-on data structure: . Everyone's out here using arrays and objects, but Set? Set is just built different ✨.</p><p>I'll be real with you - I used to skip over Set entirely. \"Just use an array,\" I thought. Or \"objects are faster for lookups.\" But the more I worked with large-scale applications like <a href=\"https://kollabe.com?utm_source=devto\" rel=\"noopener noreferrer\">Kollabe</a>, the more I realized: Set is the secret sauce that combines the best of both worlds.</p><h2>\n  \n  \n  7 Times Set Proves It's Built Different 😤\n</h2><h3>\n  \n  \n  1: Lightning Fast Lookups AND Clean Syntax\n</h3><p>Sets give you object-speed lookups with array-like syntax. It's literally the best of both worlds:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  2: Automatic Deduplication That Just Works\n</h3><p>Ever had to remove duplicates from an array? Set makes it trivial:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  3: Set Operations That Make Sense\n</h3><p>Want to find common elements or differences between collections? Set operations are actually readable:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  4: Type-Safe Enums That Actually Work\n</h3><p>TypeScript enums are... controversial. But Sets can give you type-safe, runtime-safe enums:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  5: Memory Efficient Large Collections\n</h3><p>Sets are more memory efficient than objects for large collections of unique values:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  6: WeakSet: The Memory Management Secret Weapon\n</h3><p>Like WeakMap, WeakSet lets you store object references without preventing garbage collection:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  7: The Ultimate Event Tracker\n</h3><p>Sets are perfect for tracking complex states in event systems:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Performance Showdown 🏃‍♂️\n</h2><p>Let's be real about performance. Here's how Set stacks up:</p><div><pre><code></code></pre></div><p>The results? Set is consistently faster than Array.includes() and nearly as fast as object property lookups. But unlike objects, it maintains insertion order and has a clean API. Built different indeed 😤.</p><p>Set plays incredibly well with TypeScript:</p><div><pre><code></code></pre></div><p>Let's keep it 💯 - Set isn't always the answer:</p><ol><li>When you need to serialize data frequently (JSON.stringify)</li><li>When you need index-based access (use arrays)</li><li>When you need key-value pairs (use Map)</li><li>When memory is extremely constrained (arrays might be better)</li></ol><p>Set is just built different. It combines the best parts of arrays and objects into something uniquely powerful. From lightning-fast lookups to automatic deduplication, from clean set operations to memory efficiency, Set proves itself as an essential tool in modern JavaScript.</p><p>Next time you reach for an array or object, ask yourself: \"Could Set handle this better?\" The answer might surprise you 😤💪.</p><p>P.S. If you're looking for a place to practice these Set operations in a real project, check out <a href=\"https://kollabe.com?utm_source=devto\" rel=\"noopener noreferrer\">Kollabe</a>. It's where I learned just how clutch Set can be for real-time collaboration!</p>","contentLength":2630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamically Modifying CloudFront Origin for Country-Specific and A/B Testing","url":"https://dev.to/aws-builders/dynamically-modifying-cloudfront-origin-for-country-specific-and-ab-testing-2k53","date":1739766846,"author":"Avinash Dalvi","guid":1613,"unread":true,"content":"<p>As a dedicated developer, I'm always on the lookout for more efficient ways to implement feature development. However, it's equally important to ensure that the features I build are genuinely beneficial for users. This is where A/B testing comes into play—it helps validate what truly resonates with users.</p><p>Previously, I was familiar with A/B testing using Google Analytics and custom logic to direct traffic to specific features. However, I sought a more seamless and scalable approach that didn't rely on additional infrastructure. As re:Invent approached, my focus wasn't solely on DevOps updates; I was actively searching for innovations that could simplify A/B testing and enhance feature development.</p><p>This led me to explore whether AWS had introduced any new capabilities to streamline this process. When I came across the recent update to Amazon CloudFront, allowing origin modifications using CloudFront Functions, I realised its potential in dynamically routing users based on key attributes like location or device type.</p><p>Amazon CloudFront recently introduced support for modifying the origin using , enabling developers to dynamically route requests to different origins based on user attributes such as country, device type, or other request headers. This feature unlocks new possibilities for global content delivery, including country-specific websites, A/B testing, and device-based content optimization.</p><p>This update immediately caught my attention as I was in the middle of preparing for the re:Cap event. It felt like the perfect opportunity to explore its real-world applications.</p><h2>\n  \n  \n  The Story Behind This Update\n</h2><p>As part of AWS re:Invent preparations, AWS announced a Pre-re:Invent update on November 21, 2024, introducing new CloudFront capabilities. In preparation for the re:Cap event on January 6, 2025, I explored these new capabilities to showcase them in demos. When I came across this update, I decided to do a deep dive into  and explore how it could be leveraged for . This led me to experiment with different use cases like country-based content delivery, A/B testing, and device-specific optimisations. Through this exploration, I realized how impactful this update could be for developers optimising global content distribution.</p><h2>\n  \n  \n  Why Modify CloudFront Origin Dynamically?\n</h2><p>Traditionally, CloudFront distributions have a fixed origin (an S3 bucket, EC2 instance, or any HTTP endpoint). However, with CloudFront Functions, we can dynamically select the origin based on request attributes, such as:</p><ul><li><p> Serve country-specific content from different S3 buckets or servers.</p></li><li><p> Route a percentage of traffic to different versions of a website or application.</p></li><li><p> Serve optimised content for mobile or desktop users.</p></li></ul><p>CloudFront Functions allow lightweight JavaScript-based logic to run at the  stage, enabling modifications to the request before it reaches the origin. One key function is <code>request.updateRequestOrigin()</code>, which allows us to change the request origin dynamically.</p><h2>\n  \n  \n  Example: Changing Origin Based on User’s Country\n</h2><p>For a country-specific website, we can use <code>CloudFront-Viewer-Country</code> header to decide which S3 bucket or server should serve the request.</p><h3>\n  \n  \n  Step 1: Create a CloudFront Function\n</h3><p>You can create a CloudFront Function from the AWS Management Console or AWS CLI. Below is a sample function to modify the origin based on the user’s country.</p><h3>\n  \n  \n  Step 2: Deploy the CloudFront Function\n</h3><ol><li><p><strong>Go to AWS CloudFront Console</strong> → Select </p></li><li><p> and name it (e.g., <code>ModifyOriginBasedOnCountry</code>)</p><p>You can choose any runtime but preferred one to use latest runtime.</p></li><li><p><strong>Paste the JavaScript code</strong> and publish the function</p><pre><code></code></pre></li><li><p> with the desired CloudFront distribution at the  stage.</p></li><li><p>Make sure to publish function whenever make changes.</p></li></ol><h3>\n  \n  \n  Step 3: Test the Function\n</h3><p>To test country-specific content delivery, you can use any VPN-based proxy site like <a href=\"https://proxyium.com/#google_vignette\" rel=\"noopener noreferrer\">https://proxyium.com</a> to browse the website from different locations. This will allow you to verify if the CloudFront function is correctly routing requests based on the user's country. Or Use  to simulate requests from different countries by setting the <code>CloudFront-Viewer-Country</code> header.</p><div><pre><code>curl  https://your-cloudfront-domain.com\n</code></pre></div><p>If implemented correctly, users from different regions will be routed to the appropriate origin. Same for can be tested for device specific using browser device testing under developer tool or test it on different physical devices.</p><p>Modify the function to route a percentage of traffic to different origins for testing purposes.</p><div><pre><code>var randomValue  Math.randomrandomValue &lt; 0.5\n    request.updateRequestOrigin domainName: </code></pre></div><h3><strong>2. Device-Based Content Routing</strong></h3><p>Use the  header to route requests based on mobile or desktop access.</p><div><pre><code>var userAgent  headers[.value.toLowerCaseuserAgent.includes\n    request.updateRequestOrigin domainName: </code></pre></div><p>With the new  functionality in CloudFront Functions, developers can build  and  content delivery strategies. Whether it's serving country-specific content, conducting A/B tests, or optimising content for different devices, this feature brings greater flexibility to AWS CloudFront.</p><p>🚀 <strong>Start leveraging CloudFront Functions today to optimise your content delivery!</strong></p><p>I hope this blog helps you to learn. Feel free to reach out to me on my Twitter handle <a href=\"https://dev.to@AvinashDalvi_\">@AvinashDalvi_</a> or leave comment on the blog. Stay tuned for more learning.</p>","contentLength":5351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nguyên lý Controller trong GRASP Pattern","url":"https://dev.to/hcmute_project_988df1c63c/nguyen-ly-controller-trong-grasp-pattern-36k3","date":1739766663,"author":"HCMUTE Project","guid":1612,"unread":true,"content":"<p> là một mẫu thiết kế trong  (General Responsibility Assignment Software Patterns) nhằm xác định đối tượng nào sẽ xử lý các yêu cầu từ người dùng trong hệ thống. Đây là một lớp trung gian, đóng vai trò điều phối giữa giao diện người dùng và các lớp nghiệp vụ.</p><h2>\n  \n  \n  2. Vai trò của Controller\n</h2><p>Controller chịu trách nhiệm:</p><ul><li>Nhận yêu cầu từ người dùng và gọi các phương thức phù hợp trên các lớp nghiệp vụ.</li><li>Giữ cho các lớp nghiệp vụ không phụ thuộc vào giao diện người dùng, giúp dễ bảo trì và mở rộng.</li><li>Kiểm soát luồng dữ liệu và đảm bảo tuân thủ nguyên tắc  và .</li></ul><h2>\n  \n  \n  3. Nguyên tắc thiết kế Controller hiệu quả\n</h2><ul><li>Controller chỉ nên tập trung vào điều phối luồng xử lý, không nên thực hiện quá nhiều chức năng nghiệp vụ.</li><li>Không chứa logic liên quan đến giao diện người dùng hoặc truy xuất dữ liệu.</li></ul><h3>\n  \n  \n  b. Giảm Coupling với Business Classes\n</h3><ul><li>Controller không nên chứa logic nghiệp vụ chi tiết mà chỉ chuyển tiếp yêu cầu đến các lớp thích hợp.</li><li>Tuân theo nguyên tắc  () để tránh gọi trực tiếp các phương thức từ nhiều lớp khác nhau.</li></ul><h3>\n  \n  \n  c. Định danh theo Use Case\n</h3><ul><li>Thông thường, Controller được đặt tên theo , ví dụ: , .</li><li>Điều này giúp dễ hiểu và duy trì hơn so với các tên chung chung như .</li></ul><p>Giả sử trong hệ thống đặt cược, có  \"Đặt cược\" () với các bước:</p><ol><li>Hiển thị thông tin cuộc đua ()</li><li>Chọn cuộc đua ()</li><li>Nhập thông tin đặt cược ()</li></ol><p>Thay vì để lớp  xử lý việc hiển thị chi tiết cuộc đua, chúng ta sẽ sử dụng  để điều phối yêu cầu:</p><div><pre><code>public class PlaceBetHandler {\n    private RaceService raceService;\n    private BetService betService;\n\n    public PlaceBetHandler(RaceService raceService, BetService betService) {\n        this.raceService = raceService;\n        this.betService = betService;\n    }\n\n    public void displayRaceDetails(int raceId) {\n        Race race = raceService.getRaceById(raceId);\n        System.out.println(race.getDetails());\n    }\n\n    public void enterBet(int raceId, double amount) {\n        betService.placeBet(raceId, amount);\n    }\n}\n</code></pre></div><h2>\n  \n  \n  5. <strong>Lợi ích của Controller trong GRASP</strong></h2><ul><li><strong>Tách biệt giao diện người dùng và lớp nghiệp vụ</strong>, giúp hệ thống linh hoạt hơn khi thay đổi UI hoặc backend.</li><li>, vì mọi logic điều phối tập trung trong một lớp duy nhất.</li><li><strong>Cải thiện tính tái sử dụng</strong>, vì các lớp nghiệp vụ không bị ràng buộc với giao diện cụ thể.</li></ul><p>Mẫu thiết kế  trong  giúp xây dựng hệ thống có kiến trúc rõ ràng, dễ bảo trì và mở rộng. Khi áp dụng, cần kết hợp với các nguyên tắc <strong>High Cohesion, Low Coupling</strong> và  để đảm bảo hiệu quả thiết kế.</p>","contentLength":2991,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Learn MERN Stack Development: An Introduction to Full-Stack JavaScript","url":"https://dev.to/lekshmi_525/why-learn-mern-stack-development-an-introduction-to-full-stack-javascript-130b","date":1739766584,"author":"Lekshmi","guid":1611,"unread":true,"content":"<p>In the world of web development, staying ahead of the curve means adopting the latest technologies that not only simplify your workflow but also provide powerful and scalable solutions. Among the most popular and efficient tech stacks is the MERN Stack – a powerful combination of four JavaScript-based technologies: MongoDB, Express.js, React.js, and Node.js. Each of these technologies plays a crucial role in developing dynamic, full-stack applications, making it one of the most desirable skill sets for developers today.</p><p>But why should you learn MERN Stack Development? In this article, we explore the reasons that make MERN an attractive choice for modern web development.</p><p><strong>1. Unified Language Across the Stack</strong></p><p>One of the most compelling reasons to learn MERN stack development is the ability to use JavaScript for both client-side and server-side programming. Traditionally, full-stack developers had to juggle multiple languages – like HTML, CSS, and JavaScript for the front-end, and PHP, Python, or Ruby for the back-end. With MERN, JavaScript is the only language required across the entire application, providing a streamlined experience.</p><p>By using a single language, you can avoid the overhead of switching between different languages and syntax. This reduces complexity, improves productivity, and allows for better code consistency throughout the application.</p><p><strong>2. Full-Stack Development with Scalability</strong></p><p>The MERN stack is designed for developing full-stack applications, meaning developers can build everything from the front-end to the back-end, along with a database, all in one place. The stack provides a complete set of tools for creating dynamic, interactive, and responsive web applications.</p><p>MongoDB serves as the NoSQL database, providing flexibility in storing data, especially for applications that require quick scalability. It stores data in JSON-like formats, which fits perfectly with JavaScript.</p><p>Express.js is a web application framework for Node.js, which simplifies the routing and handling of HTTP requests, enabling developers to easily build RESTful APIs and handle various server-side tasks.</p><p>React.js is a powerful JavaScript library for building user interfaces. It enables developers to create fast, interactive, and component-based user interfaces with minimal effort. React’s virtual DOM feature ensures efficient rendering of changes, offering superior performance.</p><p>Node.js is the back-end framework that allows developers to build scalable, event-driven, and high-performance applications using JavaScript. It’s built on Chrome’s V8 JavaScript engine, which makes it fast and efficient for handling concurrent requests.</p><p>Together, these technologies allow for the development of highly scalable applications, whether for small businesses or large enterprises.</p><p>As JavaScript remains one of the most popular programming languages in the world, knowledge of the MERN stack has become an essential skill for web developers. MERN is used by many top companies, including Netflix, Uber, and Instagram, for building modern web applications.</p><p>Consequently, developers proficient in the MERN stack are in high demand across the job market.\nLearning the MERN stack opens up numerous career opportunities, whether you are looking for a front-end, back-end, or full-stack development role. Companies are increasingly looking for developers who can work with this stack due to its versatility and ability to handle both small and large-scale projects.</p><p>MERN Stack development emphasizes speed and efficiency, both in terms of application performance and the development process. Here’s how each technology in the stack contributes:</p><ul><li><p>React.js makes it easy to build components that automatically update and re-render based on changes in state. This significantly reduces the time and effort required to keep the UI in sync with the data.</p></li><li><p>Node.js enables asynchronous, non-blocking operations, which means the server can handle multiple requests simultaneously without waiting for one to finish before starting the next. This results in faster response times, making it perfect for building real-time applications.</p></li><li><p>MongoDB allows for easy data modeling, particularly when dealing with large amounts of unstructured or semi-structured data. Its flexible schema-less nature means developers don’t need to worry about predefined data structures.</p></li></ul><p>Because of these efficiencies, MERN is well-suited for developing modern, high-performance applications that meet the demands of today’s fast-paced web ecosystem.</p><p><strong>5. Active Community and Open-Source Resources</strong></p><p>The MERN stack benefits from strong community support and a wealth of open-source resources. With each of the four technologies being open-source, developers have access to a vast pool of tools, libraries, and documentation to help them during the development process.</p><p>React, Node.js, MongoDB, and Express have large communities of developers continuously improving and expanding the tools, frameworks, and resources that make development faster and easier. Additionally, there are plenty of tutorials, forums, and GitHub repositories where you can collaborate, learn, and get answers to your questions.</p><p><strong>6. Real-Time Application Development</strong></p><p>The MERN stack is highly effective for building real-time applications such as messaging platforms, social networks, and live updates in collaborative tools. With Node.js’ event-driven architecture and the powerful front-end capabilities of React, developers can easily build applications that require real-time interaction, such as chat apps or live data streaming.</p><p><strong>7. Familiarity and Flexibility</strong></p><p>JavaScript is one of the most widely-used programming languages, making it relatively easy to learn and understand, even for beginners. MERN’s use of familiar JavaScript frameworks ensures a smoother learning curve and greater flexibility when working on various aspects of the development process.</p><p>For developers who are already comfortable with JavaScript, adopting the MERN stack is a natural progression. Furthermore, as the stack is modular, you can work with the components individually if needed, allowing you to integrate them with other technologies or frameworks without much friction.</p><p>Conclusion\nIn conclusion, learning MERN Stack development provides a comprehensive, efficient, and scalable solution for full-stack web development. With JavaScript at the core, developers can work across the front-end and back-end seamlessly, building robust and dynamic web applications. The stack’s popularity in the industry, combined with its ease of use, speed, and performance, makes it an ideal choice for anyone looking to become a proficient full-stack developer. If you are a resident of Kerala, there is a top notch <a href=\"https://edure.in/\" rel=\"noopener noreferrer\">MERN Stack Development Course in Kozhikode</a> that you can check out.</p><p>Whether you’re starting from scratch or looking to enhance your skills, mastering the MERN stack, especially the <a href=\"https://edure.in/\" rel=\"noopener noreferrer\">MERN Stack Development Course in Kozhikode</a> can open the door to exciting career opportunities, innovative projects, and a deeper understanding of modern web development.</p>","contentLength":7080,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Test FastAPI Efficiently with Requestly API Client","url":"https://dev.to/requestly/how-to-test-fastapi-efficiently-with-requestly-api-client-5ahn","date":1739766211,"author":"Dinesh Thakur","guid":1610,"unread":true,"content":"<p><a href=\"https://fastapi.tiangolo.com/\" rel=\"noopener noreferrer\">FastAPI</a>  is a    that makes building high-performance APIs a breeze. It’s designed to be fast, efficient, and developer-friendly, so you can create APIs with minimal code while keeping things scalable and reliable.</p><p>But once your API is up and running, testing it is just as important as building it. You need to make sure everything works as expected. While there are many ways to test APIs, the    makes the process smooth and hassle-free. It lets you send requests, tweak request data, and debug issues—all in one place.</p><p>In this article, we’ll dive into how you can use Requestly to test your FastAPI endpoints efficiently and why it might just be a better option than traditional testing methods.</p><h2><strong>Why and When to Use FastAPI</strong></h2><p><a href=\"https://fastapi.tiangolo.com/\" rel=\"noopener noreferrer\">FastAPI</a>  is becoming a popular choice for API development due to its  <strong>speed, simplicity, and modern features</strong>. But when should you use  <a href=\"https://fastapi.tiangolo.com/\" rel=\"noopener noreferrer\">FastAPI</a>  over other web frameworks like Flask or Django? Let’s break it down.</p><ol><li>  –  <a href=\"https://fastapi.tiangolo.com/\" rel=\"noopener noreferrer\">FastAPI</a>  is one of the fastest Python web frameworks, thanks to its asynchronous capabilities and efficient request handling.</li><li>  – With a clean and intuitive syntax, developers can build APIs quickly with minimal code.</li><li><strong>Automatic Data Validation</strong>  – FastAPI uses    to validate request and response data, reducing the chances of errors.</li><li>  – Unlike Flask, which requires additional libraries for async programming, FastAPI natively supports asynchronous operations.</li><li>  – With type hints in Python, FastAPI ensures better code quality and catches potential issues early.</li></ol><ul><li><strong>When you need high-speed API performance</strong>  – If your application requires handling a large number of concurrent requests efficiently, FastAPI is a great choice.</li><li><strong>For real-time applications</strong>  – FastAPI is ideal for WebSockets, chat applications, live data streaming, and similar use cases.</li><li><strong>If you prefer modern Python features</strong>  – With support for type hints, async/await, and automatic data validation, FastAPI is perfect for developers who want to leverage the latest Python capabilities.</li><li><strong>When working with microservices</strong>  – FastAPI is lightweight and integrates well with containerized environments, making it a strong candidate for microservices-based architectures.</li><li><strong>For machine learning and AI applications</strong>  – Many AI and ML engineers use FastAPI to expose their models as APIs due to its speed and ease of integration.</li></ul><p>If you’re building a modern, scalable, and high-performance API, FastAPI is one of the best frameworks to consider.</p><h2><strong>How to Test FastAPI with Requestly</strong></h2><p>Testing is essential to ensure your app behaves as expected. While  <a href=\"https://fastapi.tiangolo.com/\" rel=\"noopener noreferrer\">FastAPI</a>  provides some built-in tools for testing,    offers a more powerful and flexible way to test, debug, and modify API requests effortlessly.</p><h3><strong>Setting Up Your FastAPI Application</strong></h3><p>Before testing, ensure you have a running FastAPI application. Here’s a simple example of a FastAPI app:</p><div><pre><code></code></pre></div><p>Run this FastAPI app using:</p><h3><strong>Installing Requestly API Client</strong></h3><h3><strong>Sending API Requests Using Requestly</strong></h3><ul><li>Open    navigate to API client</li></ul><ul><li>Here create a  .</li></ul><ul><li>Click    to see the response.</li></ul><p>Requestly makes it easy to simulate different testing scenarios:</p><ul><li><strong>Changing Query Parameters</strong>  – Modify request parameters dynamically to test different cases.</li><li>  – Test authentication or different content types by adding/modifying headers.</li><li>  – Easily test POST or PUT requests by sending JSON data in the request body.</li><li>  – Modify request responses to test how your application handles failures.</li></ul><p>Using  <a href=\"https://requestly.com/products/api-client/\" rel=\"noopener noreferrer\">Requestly API Client</a>, you can efficiently test and debug your FastAPI endpoints without relying solely on code-based testing tools</p><p>FastAPI is a powerful and efficient web framework for building APIs in Python, offering high performance and ease of use. However, testing your API is just as important as building it to ensure it works as expected in real-world scenarios.</p><p>By using Requestly, developers can test FastAPI endpoints more efficiently, simulate different scenarios, and debug issues quickly—all within an intuitive interface. If you’re looking for a hassle-free way to test your APIs,  <strong>Requestly is a great tool to add to your workflow</strong>.</p><p>Try Requestly today and streamline your FastAPI testing process! 🚀</p>","contentLength":4139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build a Guitar Scale Visualization App with React, TypeScript, and Next.js","url":"https://dev.to/radzion/build-a-guitar-scale-visualization-app-with-react-typescript-and-nextjs-1ben","date":1739763151,"author":"Radzion Chachura","guid":1609,"unread":true,"content":"<p>In this post, we'll create an app for guitarists to visualize scales on the fretboard using React, TypeScript, and NextJS. You can check out the final result <a href=\"https://pentafret.com\" rel=\"noopener noreferrer\">here</a> and explore the codebase <a href=\"https://github.com/radzionc/guitar\" rel=\"noopener noreferrer\">here</a>. To kick things off, we'll use the <a href=\"https://github.com/radzionc/radzionkit\" rel=\"noopener noreferrer\">RadzionKit</a> starter, which provides a comprehensive set of components and utilities designed to streamline React app development and boost productivity.</p><h2>\n  \n  \n  Motivation and Inspiration\n</h2><p>I've always had a guitar and often come up with riffs and melodies, but I struggled to develop them further because I didn’t know any music theory. To change that, I picked up a book called  to learn the basics. One of the first topics it covers is the layout of notes on the fretboard, along with scales and pentatonics. This inspired me to build an app where you can visualize all of this on a fretboard, making it easy to explore different patterns in one convenient place.</p><h2>\n  \n  \n  Features of the Guitar Scale Visualization App\n</h2><p>Our app will feature two main views: a home page that displays all the notes on the fretboard and a scale page where you can select a root note and a scale. On the scale page, you'll also have the option to toggle between viewing the full scale or its pentatonic version.</p><h2>\n  \n  \n  Ensuring SEO-Friendly Pages\n</h2><p>While this app could be built as a React single-page application, that approach would negatively impact SEO. Instead, it's better to create proper pages for each pattern. We have two options: server-side rendering or generating static pages. Since the app will only require around 200 pages, generating static pages is a great choice. It's cost-effective because we don't need to pay for a server, and the pages can be served through a CDN for free or minimal cost.</p><h2>\n  \n  \n  Implementing the Home Page\n</h2><p>Let's start by implementing the home page, which we'll name .</p><div><pre><code></code></pre></div><p>We'll wrap the home page with a  component, which uses the  and  CSS utilities from <a href=\"https://github.com/radzionc/radzionkit\" rel=\"noopener noreferrer\">RadzionKit</a>. These utilities ensure the content has responsive horizontal padding, a maximum width of 1600px, and vertical padding of 80px.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Adding Meta Tags and Titles\n</h2><p>Next, we need to create a proper title to improve our chances of ranking on Google. We'll display an  element for the main heading and include a  component to set the page's title and description meta tags.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Leveraging Context and AI Assistance for Better Results\n</h2><p>To generate that copy, I used ChatGPT. However, for AI to truly understand the product, it needs context. To address this, I always maintain a file called  that contains all the raw information about the project. Whenever I need to ask the AI something, I start by sharing this context first. While it takes time to write and maintain initially, it saves a lot of effort in the long run. You don’t have to explain the project repeatedly, and with better context, the AI delivers more accurate and relevant results.</p><div><pre><code>You will be helping me with tasks related to this product. Read more about it below and reply with \"Yes\" if you understand the product.\n\nThis app allows you to view scales and pentatonics on a guitar fretboard. At the top of the page, there are three controls:\n Select the root note of the scale. Options include all 12 notes.\n Select the scale. Options include Major, Minor, Blues, Dorian, Mixolydian, Phrygian, Harmonic Minor, or Melodic Minor.\n Choose whether to view the whole scale or just the pentatonic version.\n\nBelow the controls, you will see the fretboard with the notes of the selected scale. The fretboard consists of 15 frets with open notes and 6 strings. Each note is outlined with a distinct color and labeled with the note name inside the circle.\n\nWhen the pentatonic scale is selected, the app also displays 5 pentatonic patterns. Each pattern is shown on a dedicated fretboard, progressing from the first to the fifth pattern.\n\nThe URL pattern is . For example,  displays the pentatonic scale with E as the root note in the minor scale.\n\nOn the index page , the app shows all the notes on the fretboard.\n\n\n\nThe app code is contained within a TypeScript monorepo. It is built with NextJS. The app does not use server-side rendering and instead relies on static site generation.\n</code></pre></div><h2>\n  \n  \n  Building the Fretboard Component\n</h2><p>The content of our  is straightforward: a fretboard displaying all the notes. We use the  component as the container and pass the notes as its children.</p><div><pre><code></code></pre></div><p>We use the  component as the container for our fretboard. It's a flexbox row element with a fixed height and a  position.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Positioning the Neck and Frets\n</h3><p>Other static parameters, like the neck height, are stored in a single source of truth within the  file.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Representing Fret Markers\n</h3><p>The first child of the  component is the  component, which acts as a placeholder for the open notes. It has a fixed width equal to the size of a note plus the offset.</p><p>Next is the  component, which also has a fixed width but includes a background color to visually highlight the beginning of the fretboard.</p><div><pre><code></code></pre></div><p>The  component occupies the remaining space, features a background color to contrast with the page background, and uses a  position to enable absolute positioning of its child components.</p><p>Next, we iterate over the visible frets and render a  component for each one. Variables like the number of visible frets and string count are stored in a separate configuration file. This file also includes other essential parameters, such as the total number of frets, the tuning of the strings, and the thickness of each string.</p><div><pre><code></code></pre></div><p>To represent a fret, we simply render a line with a width of 1px. To position an element by its center on the horizontal axis, we use the <code>PositionAbsolutelyCenterVertically</code> component from <a href=\"https://github.com/radzionc/radzionkit\" rel=\"noopener noreferrer\">RadzionKit</a>.</p><div><pre><code></code></pre></div><p>To calculate the  position, we use the  utility function. This function returns the start and end positions of a fret based on its index and the total number of frets. To make the fretboard look more realistic, we ensure the frets get progressively closer together as they move up the neck.</p><div><pre><code></code></pre></div><p>To enhance realism, we also display fret markers. We use the <code>PositionAbsolutelyCenterVertically</code> component to position the markers on the fretboard and  to determine the start and end positions of each fret. Since the fret position is represented as a generic  type, we can easily calculate the center of the interval using the  utility function.</p><div><pre><code></code></pre></div><p>Depending on a marker's type, we render either a single dot or double dots. To handle this, we use the  component from <a href=\"https://github.com/radzionc/radzionkit\" rel=\"noopener noreferrer\">RadzionKit</a>, which allows us to conditionally render different components based on the union type of the  prop. This approach serves as an excellent alternative to traditional switch statements.</p><p>To decide which frets should display markers, we use the  utility function. This function returns an array of  objects, each containing the fret index and the type of marker to render.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Rendering Guitar Strings and Notes\n</h3><p>Finally, we render the guitar strings. For vertical positioning, we use the <code>PositionAbsolutelyCenterHorizontally</code> component. To enhance realism, we use a <code>repeating-linear-gradient</code> to create a pattern that simulates the texture of real guitar strings.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Implementing the Scale Page\n</h2><p>With the  component complete, we return to the  component to display the actual notes. Here, we iterate over each string and visible fret. Starting with the note of the open string, we add the fret index to calculate the note's position on the chromatic scale. We also determine whether the note is natural or sharp/flat. By setting the  property to , we make sharp/flat notes less visually prominent.</p><h3>\n  \n  \n  Managing Scale State and URL Updates\n</h3><p>We represent each note with a number:  is 0,  is 1, and so on. To generate names for the notes, we use a minor scale pattern and iterate over it. For each step in the pattern, if the step equals two, it indicates a sharp note between the two natural notes, so we include it in the array. To determine if a note is natural, we check if its name has a length of 1.</p><div><pre><code></code></pre></div><p>Our  component supports three kinds: , , and . The  kind is used to highlight the root note of the scale. To position a note, we pass the string index and fret index. If the  is , it indicates an open string.</p><div><pre><code></code></pre></div><p>As with other fretboard elements, we rely heavily on constants from the config files to calculate the note's position. The <code>PositionAbsolutelyByCenter</code> component helps us position the note precisely by its center.</p><p>With all the notes displayed on the fretboard, we can now move on to the scale page. This page uses a dynamic route with three parts: the scale type, the root note, and the scale name. The scale type can be either  or , the root note is one of the 12 notes, and the scale name corresponds to one of the predefined scales.</p><div><pre><code></code></pre></div><p>We represent a scale as an array of steps, where each step indicates the number of semitones between two notes. Most scales, except the blues scale, have seven steps, while pentatonic scales have five steps.</p><p>To manage the selected configuration, we use a  type. This state is stored in a React context. Using the  utility from <a href=\"https://github.com/radzionc/radzionkit\" rel=\"noopener noreferrer\">RadzionKit</a>, we create both a provider and a hook for accessing the state seamlessly.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Generating Static Pages for Better SEO\n</h3><p>When the user changes the scale type, root note, or scale, we need to redirect them to the new URL. For this, we use the  hook. This hook retrieves the current scale state and the router instance. It returns a callback that updates the URL based on the new scale state. To construct the URL, we use the  utility function.</p><p>To make the scale page URL more readable, we convert the numeric note into a URI-friendly format. The sharp symbol is replaced with , and the note is converted to lowercase. To reverse this process and convert the URI note back to a numeric note, we use the  utility function.</p><div><pre><code></code></pre></div><p>Since each scale has its own static page, we need to generate them. To achieve this, we use the  and  functions from Next.js. The  function generates all possible paths by combining the scale types, root notes, and scales. The  function extracts the scale type, root note, and scale from the URL and provides them as props to the page.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Working with Scale Patterns\n</h2><p>The  component receives the scale state as props and passes it to the . This allows child components to easily access the scale state without the need for prop drilling.</p><div><pre><code></code></pre></div><p>At the top of the page, we display controls that let the user select the scale they want to view. These controls include the root note, scale, and scale type, presented in that order. The  component organizes these controls within a flexbox row.</p><div><pre><code></code></pre></div><p>To display the root note and scale selectors, we use the  component from <a href=\"https://github.com/radzionc/radzionkit\" rel=\"noopener noreferrer\">RadzionKit</a>. You can learn more about its implementation <a href=\"https://radzion.com/blog/selector\" rel=\"noopener noreferrer\">here</a>.</p><div><pre><code></code></pre></div><p>To toggle between the scale and pentatonic views, we use the  component from <a href=\"https://github.com/radzionc/radzionkit\" rel=\"noopener noreferrer\">RadzionKit</a>.</p><div><pre><code></code></pre></div><p>Next, we display the title, which follows the same principles as the home page title. However, on this page, the text is dynamically generated based on the selected scale.</p><div><pre><code></code></pre></div><p>Based on the selected scale type, we retrieve the appropriate pattern from either the  or  records. We then use the  utility function to determine the notes of the scale.</p><div><pre><code></code></pre></div><p>Next, we iterate over the strings and frets, rendering a note only if it belongs to the scale. If the note is a root note, we pass the  kind to highlight it.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Displaying Pentatonic Patterns\n</h2><p>When displaying the pentatonic scale, we also render the five pentatonic patterns, as it's a common practice to learn them individually.</p><div><pre><code></code></pre></div><p>We create a separate section on the page for this, complete with an  title. Within the section, we iterate over the five patterns using the  component.</p><div><pre><code></code></pre></div><p>The approach is similar to the full scale, but this time we render only two notes per string, shifting the pattern by one note with each iteration. This allows us to display all five patterns on individual fretboards within the section.</p><p>With this app, guitarists can easily visualize scales, explore patterns, and better understand the fretboard. By combining React, TypeScript, and Next.js, we created a dynamic and SEO-friendly tool that serves as both an educational resource and a practice companion. Happy playing!</p>","contentLength":12091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"#145 — Insert Blank Row when Meeting with Data Change","url":"https://dev.to/judith677/145-insert-blank-row-when-meeting-with-data-change-19f","date":1739761861,"author":"Judith-Excel-Sharing","guid":894,"unread":true,"content":"<h2>\n  \n  \n  Problem description &amp; analysis:\n</h2><p>Here below is a data table:</p><p>: We want to insert one blank row when the values in two adjacent rows change, the result is as follows:</p><p>Use  and enter the following code:</p><div><pre><code>=spl(\"=?1.conj().group@o(~).(~|[null]).conj().new(~:_1)\",A1:A9)\n</code></pre></div><p>Download  for FREE and boost your working speed with !!! 🚀✨⬇️</p>","contentLength":340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ECS FinHacks: Scaling Microservices with AWS ECS Fargate and RDS","url":"https://dev.to/aws-builders/ecs-finhacks-scaling-microservices-with-aws-ecs-fargate-and-rds-1fnh","date":1739761199,"author":"Ravindra Singh","guid":893,"unread":true,"content":"<p>If you've ever struggled with questions like:</p><ul><li>How do I securely deploy my containerized application on AWS?</li><li>How do I integrate ECS, Fargate, PostgreSQL, and AWS security services?</li><li>How can I ensure high availability while keeping costs under control?</li></ul><p>Then this blog is for you.</p><p>In this blog post, we will explore how to deploy a Node Js Microservice in AWS ECS Fargate with connectivity to Amazon RDS (PostgreSQL). This architecture ensures high availability, security, and scalability while leveraging fully managed AWS services.</p><ul><li> Eliminates the need to manage EC2 instances.</li><li> Supports automatic scaling based on demand.</li><li> Integrates with AWS IAM, Security Groups, and VPC.</li><li> Pay only for the resources used.</li><li> Works seamlessly with AWS services like RDS, S3, and CloudWatch.</li></ul><p><strong>Advanced Architecture Benefits:</strong></p><ol><li>ECS Capacity Provider(ECS Fargate SPOT + ECS Fargate)</li></ol><p>\nThe architecture follows AWS best practices by leveraging containerized workloads on ECS (Fargate), a multi-AZ database layer (PostgreSQL), and various AWS security and monitoring services.</p><ul><li>ECS Fargate: Fully managed container orchestration.</li><li>Amazon RDS (PostgreSQL): Managed relational database service.</li><li>AWS ALB (Application Load Balancer): Distributes traffic among ECS tasks.</li><li>AWS Secrets Manager: Stores database credentials securely.</li><li>AWS CloudWatch: Monitors logs and metrics.</li><li>AWS Route 53: Domain Name System (DNS) for routing traffic.</li><li>AWS WAF (Web Application Firewall): Protects against common web threats.\nAWS Config: Tracks and records AWS configuration changes.</li><li>AWS CloudTrail: Logs all API requests for auditing.</li><li>AWS CloudWatch Alarms: Triggers notifications based on metrics.</li><li>VPC Endpoint: Enables secure, private connectivity to AWS services.</li><li>KMS (Key Management Service): Encrypts data at rest and in transit.</li><li>ENI (Elastic Network Interface): Provides network connectivity for ECS tasks.</li><li>Health Check &amp; Route 53 Health Check: Ensures high availability by monitoring service health.</li></ul><p><strong>2. Step-by-Step Breakdown of the AWS Architecture</strong>\nLet's dive deeper into how each AWS service fits into the architecture.</p><p><strong>2.1 Networking &amp;&nbsp;Security</strong><strong>VPC (Virtual Private Cloud):</strong></p><ul><li>A private and secure network for hosting all resources.\nContains public and private subnets for better isolation.</li></ul><p><strong>AWS WAF (Web Application Firewall):</strong></p><ul><li>Protects against common attacks like SQL injection and XSS.</li></ul><ul><li>Detects and alerts on security threats.</li></ul><p><strong>AWS KMS (Key Management Service):</strong></p><ul><li>Encrypts sensitive data, database records, and API secrets.</li></ul><p><strong>2.2 Load Balancing &amp; Traffic&nbsp;Routing</strong></p><ul><li>Provides global DNS resolution and failover routing.</li></ul><p><strong>Application Load Balancer (ALB):</strong></p><ul><li>Distributes traffic to ECS containers.</li><li>Performs health checks and ensures high availability.</li></ul><p><strong>Amazon ECS (Elastic Container Service):</strong></p><ul><li>Manages containerized workloads with Fargate &amp; Spot instances.</li></ul><p><strong>Fargate (On-demand &amp; Spot):</strong></p><ul><li>Serverless compute for containers, reducing management overhead.\nSpot pricing optimizes costs by using spare AWS capacity.</li></ul><p><strong>Task Definitions &amp; IAM Roles:</strong></p><ul><li>Defines how containers run within ECS.</li><li>IAM Roles ensure secure communication between services.</li></ul><p><strong>Amazon RDS (PostgreSQL Multi-AZ):</strong></p><ul><li>High availability using a Master-Replica setup.</li><li>KMS encryption ensures data security.</li></ul><ul><li>Logs container performance, database health, and API requests.\n</li><li>Tracks infrastructure changes and compliance.</li></ul><p><strong>3. Scalability &amp; High Availability</strong>\nThis architecture ensures scalability at multiple levels:<p>\n✅ ECS Auto-scaling: Dynamically adjusts the number of running containers based on load.</p>\n✅ Database Auto-scaling: Supports read replicas for handling increased query loads.<p>\n✅ Multi-AZ Deployment: Ensures uptime even if one availability zone fails.</p>\n✅ ALB Health Checks: Automatically reroutes traffic in case of failure.<p>\nThis combination allows applications to handle traffic spikes without downtime.</p></p><p><strong>4. Security Best Practices</strong>\nSecurity is a top priority, and this architecture follows best practices:<p>\n🔐 IAM Roles &amp; Policies: Grant the least privilege access to services.</p>\n🔐 WAF &amp; GuardDuty: Blocks malicious requests and detects threats.<p>\n🔐 KMS Encryption: Protects database and sensitive data.</p>\n🔐 Secrets Manager: Manages database credentials securely.<p>\nBy implementing these security layers, the architecture remains resilient against cyber threats.</p></p><p><strong>5. Cost Optimisation Strategies</strong>\nAWS provides multiple ways to reduce costs while maintaining performance.<p>\n💰 Fargate Spot: Uses AWS's spare capacity for containerized workloads, reducing costs by up to 70%.</p>\n💰 Reserved Instances for PostgreSQL: Locks in lower pricing for predictable workloads.<p>\n💰 Auto-scaling Policies: Ensures you only pay for what you use.</p>\n💰 EFS Infrequent Access Storage: Saves money on unused storage.<p>\nBy leveraging these strategies, you can run a cost-efficient architecture without sacrificing performance.</p></p><p>\n👉🏻 To resolve below error.</p><p>If you are using a VPC endpoint for ECR, please enable private DNS in the VPC endpoint.</p><ol><li>Use the following command to create the ECR repositories.\n</li></ol><div><pre><code>aws ecr create-repository --repository-name nodejs-api --endpoint-url https://api.ecr.ap-south-1.amazonaws.com\n</code></pre></div><p>👉🏻 <code>endpoint url will get from https://api.ecr.ap-south-1.amazonaws.com</code></p><p>Navigate to VPC Endpoints and select the API URL.</p><p>👉🏻 Amazon ECS tasks hosted on Fargate using platform version 1.4.0 or later require both Amazon ECR VPC endpoints and the Amazon S3 gateway endpoints.<a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html\" rel=\"noopener noreferrer\"></a></p><p>\nBuilding a scalable, secure, and cost-effective AWS architecture doesn't have to be complicated. By integrating ECS, Fargate, PostgreSQL, and AWS security services, you can:<p>\n✅ Achieve high availability and fault tolerance</p>\n✅ Protect your workloads with advanced security measures<p>\n✅ Optimize cloud costs using AWS best practices</p>\nThis architecture provides a blueprint for running production-grade applications in AWS. Whether you're scaling a startup or managing enterprise workloads, these principles will help you build a robust cloud infrastructure.</p><p>Reference:&nbsp;\nIf you prefer a video tutorial to help guide you through the setup of Scaling Microservices with AWS ECS Fargate and RDS</p>","contentLength":6007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week's AI News Updates (Feb 13, 2025) 🚀","url":"https://dev.to/h_metacode_74e90df0ee5da6/this-weeks-ai-news-updates-feb-13-2025-4b0l","date":1739760321,"author":"Metacode","guid":892,"unread":true,"content":"<h5>\n  \n  \n  ✅ Google has released its new AI-powered image generation tool, Whisk, making it available in South Korea\n</h5><h5>\n  \n  \n  ✅ EU mobilizes $200 billion in AI race against US and China\n</h5><h5>\n  \n  \n  ✅ ByteDance Unveils Goku to Take on Google’s Luma and OpenAI’s Sora\n</h5><h5>\n  \n  \n  ✅ OpenAI partners with Korea’s Kakao after inking SoftBank Japanese JV\n</h5><h5>\n  \n  \n  ✅ AI researchers at Stanford created an open rival to OpenAI’s o1 ‘reasoning’ model for under $50\n</h5><h5>\n  \n  \n  ✅ Amazon reportedly gears up to release next-gen Alexa\n</h5><h5>\n  \n  \n  ✅ GitHub Copilot brings mockups to life by generating code from images\n</h5><h5>\n  \n  \n  ✅ Meta launches new program to improve speech and translation AI\n</h5><h5>\n  \n  \n  ✅ OpenAI is expanding its footprint to Germany\n</h5><h5>\n  \n  \n  ✅ ChatGPT’s advanced AI costs $200/mo. It’s now free for Windows users\n</h5>","contentLength":838,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/_7bc44b7af58b2d5a7471/-4ele","date":1739758898,"author":"小野道玄","guid":885,"unread":true,"content":"<h2>How (and How NOT) to Name Rails Models Beyond the Obvious</h2>","contentLength":57,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ConfigMaps vs Secrets: Secure Configuration Management in Kubernetes","url":"https://dev.to/olamyde/configmaps-vs-secrets-secure-configuration-management-in-kubernetes-4jj5","date":1739758877,"author":"olamide odufuwa","guid":884,"unread":true,"content":"<p>In modern software development, Kubernetes has become a cornerstone for container orchestration. With its rise, proper configuration management in Kubernetes environments has become a critical task for developers and DevOps practitioners. ConfigMaps and Secrets are two essential components in Kubernetes used for storing configuration data. However, a common pitfall is using ConfigMaps to hold sensitive information, like passwords or API keys, which exposes systems to security vulnerabilities. This article will explore why ConfigMaps are unsuitable for sensitive data, how Kubernetes Secrets provide a more secure alternative, and the best practices for managing sensitive information in Kubernetes. By understanding these elements, teams can safeguard their applications and infrastructures, reducing the risk of data breaches and unauthorized access.</p><p><strong>Understanding ConfigMaps and Their Limitations</strong></p><p>ConfigMaps in Kubernetes are used to decouple configuration artifacts from image content, providing a way to easily change configurations that do not impact application code. They store non-sensitive data, such as environment configurations, feature flags, or external resource URLs. While useful, ConfigMaps store data in plaintext. This means any data within a ConfigMap can be accessed and read without the need for decryption, posing a significant security risk if sensitive data is stored. Given the volume of configurations that need managing, it may be tempting to use ConfigMaps for everything. However, storing sensitive information here is a practice that should be avoided to prevent vulnerabilities.</p><p><strong>Kubernetes Secrets: A Secure Alternative</strong></p><p>For securely handling sensitive data, Kubernetes Secrets provide an encrypted solution. Secrets are designed to manage small amounts of sensitive data, such as passwords, tokens, or keys. Unlike ConfigMaps, Kubernetes encrypts Secrets at rest and in transit. This means that the data stored in a Secret cannot be easily accessed or exposed by unauthorized parties. Kubernetes Secrets offer additional security features such as tighter access controls, including role-based access control (RBAC), ensuring that only authorized pods and users can access them. By utilizing Secrets, organizations can significantly reduce the risk of accidental exposure and adhere to best security practices.</p><p><strong>Injecting Secrets into Pods</strong></p><p>Kubernetes provides flexible mechanisms to inject Secrets into running pods, making it straightforward to use sensitive data in applications. Secrets can be mounted as files within a pod’s filesystem or injected as environment variables. When mounted as files, each underlying file inside the mounted directory represents a key-value pair from the Secret. This is particularly useful for applications that expect configurations to be provided via files. Alternatively, by injecting as environment variables, Secrets can be accessed through the application’s runtime environment, providing another layer of abstraction. These methods offer developers multiple ways of integrating Secrets depending on their application’s architecture and requirements.</p><p><strong>Best Practices for Managing Kubernetes Secrets</strong></p><p>Effective management of Kubernetes Secrets involves implementing best practices to enhance security. First, regularly audit permissions to ensure that only essential services and users have access to Secrets. Employ automated tools to monitor and rotate credentials stored in Secrets routinely. Additionally, consider encrypting the data in Secrets before placing it in Kubernetes and using tools like HashiCorp Vault for an added encryption layer. Enforce strict network policies to limit exposure and ensure that the Secret’s lifecycle management is integrated into your CI/CD pipeline for seamless updates. By following these practices, the security and integrity of Sensory data within Kubernetes environments can be significantly enhanced.</p><p>In conclusion, while ConfigMaps serve as a convenient tool for managing non-sensitive configurations in Kubernetes, they are ill-suited for handling sensitive data due to their lack of encryption and exposure to unauthorized access. Kubernetes Secrets provide a secure alternative, offering encryption at rest and in transit, along with robust access controls. By injecting Secrets into pods via environment variables or mounted files, developers can securely manage sensitive information without compromising application security. Adopting best practices in managing Secrets, such as regular auditing and credential rotation, further enhances the protection of sensitive data, helping to fortify an organization’s Kubernetes deployment.</p>","contentLength":4657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No More Hardcoded Secrets: Automatic Database Credential Rotation with Vault, AKS and Postgres🔐","url":"https://dev.to/poojan18/no-more-hardcoded-secrets-automatic-database-credential-rotation-with-vault-aks-and-postgres-1nmn","date":1739758755,"author":"Poojan Mehta","guid":878,"unread":true,"content":"<p>In <a href=\"https://dev.to/poojan18/secrets-management-101-a-technical-approach-with-aks-terraform-and-vault-284p\">Part 1 of this series</a>, we set up HashiCorp Vault in an AKS cluster using Terraform, configured ExternalSecrets, and demonstrated how to fetch secrets from Vault's KV engine into Kubernetes.</p><p>Now, let's take it a step further. Static credentials are risky—they can be leaked, misused, or forgotten.🤯To mitigate this, Vault provides Dynamic Secrets, allowing credentials to be generated on-demand, time-bound, and auto-revoked after expiration.</p><p>✅In this article, we’ll deploy  in our AKS cluster using Helm, explore Vault's database secrets engine to generate short-lived credentials, set  and  to natively sync those credentials in the cluster. </p><h3>\n  \n  \n  👉🏻 1) Setup PostgreSQL using Helm:\n</h3><p>We will use the <a href=\"https://github.com/bitnami/charts/tree/main/bitnami/postgresql\" rel=\"noopener noreferrer\">bitnami helm chart</a>, and use the default parameters for the sake of simplicity. Fine-tuning parameters can be added in a separate  file and applied with the installation command.</p><div><pre><code>helm my-release oci://registry-1.docker.io/bitnamicharts/postgresql </code></pre></div><p>This will generate the default Postgres user password and store it as a secret in the cluster. Use the below-mentioned command to fetch the password in decoded format.</p><div><pre><code>kubectl get secret  default my-release-postgresql  | </code></pre></div><blockquote><p>📝Note: The configured password will be ignored on a new installation in case the previous PostgreSQL release was deleted through the helm command. In that case, old PVC will have an old password, and setting it through helm won't take effect. Deleting persistent volumes (PVs) will solve the issue.</p></blockquote><p>Up next, let's create a  user in the database which we will be using for all the interactions between Postgres and Vault. Since we are aiming at credential rotations, keeping the root user intact will ensure we never lose access to the database.</p><div><pre><code>\nkubectl  my-release-postgresql-0  /bin/bash\n\n\npsql  127.0.0.1  postgres  5432 \nCREATE USER vault WITH PASSWORD \nGRANT ALL PRIVILEGES ON DATABASE postgres TO vault\nALTER USER vault WITH SUPERUSER</code></pre></div><p>We can see the vault user is created<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffxwop52lnnjsbbz584wc.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffxwop52lnnjsbbz584wc.png\" alt=\"Create Vault User\" width=\"800\" height=\"347\"></a></p><h3>\n  \n  \n  👉🏻 2) Create Database Secret Engine from Vault UI:\n</h3><p>The Vault database secrets engine dynamically generates database credentials based on configured roles, eliminating the need to hardcode credentials. It supports various databases through plugins and allows for both dynamic and static roles.</p><blockquote><p>Vault's  assigns a Time To Live (TTL) to dynamic secrets and tokens, ensuring they are valid for a specified period. Once the TTL expires, Vault can revoke the secret or token, necessitating periodic lease renewals by clients to maintain access.</p></blockquote><p>Navigate to the Vault UI and create a new secrets engine.<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fiysdgora4a7fojbqfek1.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fiysdgora4a7fojbqfek1.png\" alt=\"New database secrets engine\" width=\"800\" height=\"642\"></a></p><p>Give an appropriate name and adjust the default lease TTL and Max lease TTL if needed. <a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwrmwb4b4o0ddj4t1xyd5.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwrmwb4b4o0ddj4t1xyd5.png\" alt=\"Secrets engine configuration\" width=\"800\" height=\"296\"></a></p><p>Next, select the database plugin as  and pass the connection URL <code>postgresql://{{username}}:{{password}}@localhost:5432/database-name</code></p><p>💡Here, Postgresql is the connection method, username and password of the Vault user created in the previous step, and IP address of the ClusterIP service created while Postgres installation followed by the database as Postgres. </p><p>The reason for using private clusterIP is that the database is running the same cluster and can be accessed through the service connected with the statefulset. </p><blockquote><p>To further enhance the security, TLS configuration can also be added in this step. </p></blockquote><p>🤐Let's configure the role for this connection. Dynamic roles generate unique, time-limited database credentials for each service request. In contrast, static roles map Vault roles to existing database usernames, and Vault manages automatic password rotation for these static credentials.</p><blockquote><p>⚠️ Static roles are not recommended for root credentials as rotating them will no longer keep the authentication between Vault and Postgres.</p></blockquote><p>This is the main part. The role is attached to the database connection and it generates a dynamic username and password with 10 minutes of validity (Default TTL) and max TTL of 1 day. </p><blockquote><p>We have kept the validity to only 10 minutes to verify the rotation. Based on the sensitivity of the application, the TTL duration can be adjusted. </p></blockquote><p>Here,  and  consist of SQL queries which would be performed when new credential request is triggered. In our example, it will create a database role with password and expiry and grant  privileges on the given table(s). At the time of expiry, it will revoke all the permissions and drop the user. </p><p>As shown in the screenshot below, on clicking \"Generate Credentials\" from the vault UI, the short-lived credentials are displayed one-time.</p><p>Now let's log in with this user and verify if the granted permissions got applied or not. As shown below, it shows temporary users with their expiry dates added as attributes, and any other operation than  would not work.</p><p>Okay, so far so good. But how to natively fetch these secrets in the cluster?🤔🤔 There comes the  in the picture. </p><h3>\n  \n  \n  👉🏻 3) Configure VaultDynamicSecret resource:\n</h3><p>Since we already have  set up to natively fetch credentials from external secret stores like Vault, integrating these dynamic credentials with the same would make it more flexible to use with other Kubernetes resources through native secrets. </p><p>By default, the externalSecrets resource we used in part 1 only supports KV (Key-value) secret engine. Here, we will be using  which allow to generate values from the given source. Generators can be defined as a custom resource and re-used across different ExternalSecrets.</p><p>The VaultDynamicSecret generator specifically integrates with HashiCorp Vault to retrieve dynamic secrets directly from Vault's database secrets engine.</p><div><pre><code></code></pre></div><p>This resource definition creates a , which dynamically retrieves database credentials from HashiCorp Vault. </p><p>The credentials are fetched from the specified path <code>(/database/creds/dynamicrole)</code> in Vault using the  method. \nThe Vault server address and authentication are provided, with the token stored in a Kubernetes secret named vault-token within the external-secrets namespace.</p><p>This is one side of the bridge. On the next side, let's create  resource to connect with  resource and fetch the short-lived credentials natively.</p><p>Here, a native Kubernetes secret named  will be created keeping the refresh interval to 1 hour and referencing data source from the generator we created in the last step.</p><div><pre><code></code></pre></div><p>Ready.! Let's deploy the resources :)</p><p>Let's verify the changes by describing the secret resource in the namespace . Two keys are stored with a username and password in it. This action is supported by the creation statement declared in the role attached to the database secrets engine. </p><p>How can we move ahead without verifying the rotation?😮‍💨 Run the below commands to fetch the decoded values of the secrets at intervals of 10 minutes and see the changes in action.</p><div><pre><code>\nkubectl get secret db-credentials  external-secrets  | \nkubectl get secret db-credentials  external-secrets  | </code></pre></div><p>And it worked. We got rotated credentials with fine-grained privileges which expire at every TTL duration. </p><p>If you are still reading, thanks for making it to the end.🥹 We've taken secrets management to the next level by introducing dynamic database credentials with Vault! Instead of relying on boring static, long-lived passwords, we now have ephemeral credentials that are automatic, time-bound with built-in rotation and seamlessly injected into Kubernetes pods via ExternalSecrets.! Wohoooo 🥳</p><p>This means no more hardcoded database passwords, reduced security risks from leaked credentials, and no manual rotation headaches—everything is automated! 🚀</p><p>With this, our Kubernetes workloads are now safer, more scalable, and fully automated in handling sensitive data.</p><p>💡 Let’s Connect!\nThanks for reading! 🎉 I hope this guide helped you understand dynamic secrets and automated credential rotation in Kubernetes. If you have feedback, suggestions, or want to discuss more, feel free to reach out! 💬</p><p>Find me on <a href=\"https://www.linkedin.com/in/poojanmehta18/\" rel=\"noopener noreferrer\">LinkedIn</a> and check out the full project on <a href=\"https://github.com/poojan1812/secrets-management\" rel=\"noopener noreferrer\">GitHub</a>. Let’s build smarter, more secure cloud solutions together! 🚀</p>","contentLength":7952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Terraform Tree Structure: Boost Cloud Efficiency and Management","url":"https://dev.to/olamyde/terraform-tree-structure-boost-cloud-efficiency-and-management-4il0","date":1739757965,"author":"olamide odufuwa","guid":877,"unread":true,"content":"<p>The complexity of managing cloud infrastructure has grown significantly, prompting the need for efficient tools that simplify this process. Whether it’s deploying applications or configuring resources, the way we handle infrastructure has drastically evolved. One of the remarkable tools in this space is Terraform, known for enabling Infrastructure as Code (IaC). Within Terraform, the concept of tree structure plays a critical role in organizing configuration files and modules effectively. This article delves into the essential elements of Terraform tree structure, exploring its significance and how it contributes to streamlined and scalable infrastructure management. By understanding its framework, developers and IT professionals can optimize their cloud environments, ensuring better performance, reduced errors, and enhanced collaboration across teams.</p><p><strong>Understanding Terraform Overview</strong></p><p>In its essence, Terraform is an open-source tool developed by HashiCorp that facilitates the automation of infrastructure management across different service providers. At the heart of Terraform lies , allowing users to specify what their infrastructure should look like without detailing the steps to reach that state. With a robust plugin ecosystem, Terraform’s modular and cohesive architecture enables the development of reusable code. This proves invaluable in setting up cloud environments, from low-level components like VPCs and subnets to high-level features such as application deployments. Understanding these elements sets the scene for leveraging the tree structure to its full potential.</p><p><strong>Importance of Tree Structure in Terraform</strong></p><p>The tree structure within Terraform plays a pivotal role in organizing and managing configuration files systematically. This structured approach allows teams to split environments into distinct and easily manageable segments. By adopting a tree-like representation of configurations, developers can effortlessly navigate through complex setups, tracing resource dependencies and shared modules. Such a setup not only simplifies debugging but also enhances  and reusability. By structuring directories correctly, teams can implement consistent workflows and automate deployments, driven by clear and logical directory hierarchies, facilitating seamless collaboration.</p><p><strong>Optimizing Resource Management</strong></p><p>Achieving optimal resource management in Terraform relies heavily on configuring an efficient tree structure. The integration of modules allows for reusable, independent blocks of configurations that can be easily integrated into various projects. This reusability is crucial when managing multi-account cloud deployments, where managing state becomes more complex. By adhering to a well-planned tree structure, organizations can manage multiple states across diverse environments effectively, automating the provisioning and management of resources. Moreover, standardized structures aid in instilling best practices, fostering an environment where resources and budgets are optimally utilized.</p><p><strong>Best Practices for Implementing Tree Structures</strong></p><p>To leverage the full potential of Terraform’s tree structure, it’s essential to adhere to certain best practices. Start by defining clear directory namespaces, separating configurations logically using environment-specific paths, and incorporating state files pertinent to specific environments. Ensuring consistency in module development without duplicating code is another key practice. Build a library of reusable modules for frequently used infrastructure patterns. It’s also beneficial to document each part of the structure, enabling easier onboarding of team members and simplifying maintenance tasks. Consistently revisiting and refining your tree structure can provide adaptability and long-term operational efficiency.</p><p>In conclusion, the tree structure in Terraform is more than an organization tool; it’s a blueprint for managing infrastructure with precision and efficiency. As detailed in this article, the adoption of a tree structure facilitates clear organization, modular code development, and optimal resource management, all of which are crucial in modern infrastructure management. By adhering to best practices, professionals can significantly reduce errors, enhance scalability, and foster a cohesive development environment. As cloud deployments grow in complexity, understanding and implementing a well-defined tree structure becomes indispensable for maintaining control and achieving successful outcomes in diverse IT landscapes.</p>","contentLength":4542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LangGraph Subgraphs: A Guide to Modular AI Agents Development","url":"https://dev.to/sreeni5018/langgraph-subgraphs-a-guide-to-modular-ai-agents-development-31ob","date":1739757960,"author":"Seenivasa Ramadurai","guid":876,"unread":true,"content":"<p>LangGraph’s  feature is a game-changer for building complex AI workflows. It helps break down large workflows into smaller, modular components, making them more manageable, reusable, and maintainable. If you're looking to improve the structure of your AI applications, understanding when and how to use subgraphs is essential.  </p><p>Let’s dive into why subgraphs matter and how to integrate them into your LangGraph projects.  </p><h2>\n  \n  \n  When Should You Use Subgraphs?\n</h2><p>Subgraphs shine in the following scenarios:  </p><p> – When multiple agents need to collaborate, subgraphs help organize logic for each agent or team.  </p><p> – Have a set of nodes you frequently use? Define them as a subgraph for seamless integration across multiple workflows.  </p><p> – Different teams can work on separate subgraphs, allowing for independent development and testing.  </p><p> – For intricate AI workflows, breaking them into subgraphs keeps your architecture clean and modular.  </p><h2>\n  \n  \n  How to Add Subgraphs to Your LangGraph Project\n</h2><p>There are two primary ways to integrate subgraphs into a parent graph:  </p><h3>\n  \n  \n  1️⃣ Adding a Compiled Subgraph as a Node**\n</h3><p>Best when the parent graph and subgraph share state keys and don’t require state transformation.  </p><div><pre><code></code></pre></div><h2>\n  \n  \n  What Does the above Code Do?\n</h2><p>The core of the code revolves around LangGraph's StateGraph framework, which is used to model and execute workflows. The workflow here involves two key steps:</p><p>Tavily Search: The subgraph takes a user-provided query and makes a request to the Tavily API for search results (limited to 3 results). Each result contains a URL and content. These results are then formatted into a readable string.</p><p>Processing Results: The processed search results are returned and combined with the original query, providing a clear output for the user.</p><p>The main graph consists of a simple node that modifies the input query and passes it to the subgraph, which handles the search and result formatting. The results are then returned as the output, showing both the search query and the formatted search results.</p><p>Once the workflow is defined, the code compiles it into a graph and visualizes it in two formats: ASCII and PNG. These visuals give us a clear view of the flow from the start node, through the subgraph, to the final result. The images are saved as sreeni_subgraph.png and sreeni_Main_and_subgraph.png, providing a useful representation of the process for documentation or debugging purposes.</p><p>Finally, the graph is executed with an example query (\"NVIDIA\"). The result is printed, showing the combined query and processed search results. This demonstrates how LangGraph can handle dynamic inputs and return structured outputs, all while maintaining clear visibility into the process through the generated graphs.</p><p>This workflow illustrates just one of the many ways LangGraph can be used to create organized, visual, and easy-to-debug workflows that interact with external APIs and process data in a structured manner.</p><h3>\n  \n  \n  2️⃣ Using a Node Function to Invoke the Subgraph**\n</h3><p>Ideal when the parent graph and subgraph have different state schemas, requiring state transformation.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Best Practices for Using Subgraphs\n</h2><p> – Ensure the parent graph and subgraph share at least one state key for smooth communication.  </p><p> – When state structures differ, use a node function to manage the transformation.  </p><p> – Think modular! Reusable subgraphs keep your workflows efficient and scalable.  </p><p> – Define precise input and output schemas to ensure seamless integration with parent graphs.  </p><p>By leveraging  in LangGraph, you can build AI systems that are more <strong>scalable, organized, and flexible</strong>. Whether you’re constructing multi-agent workflows or breaking down complex processes, subgraphs offer a structured approach to handling AI workflow challenges.  </p>","contentLength":3811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I've been studying Python for a few weeks now. Scrolling through tiktok today an influencer in DevSec said learn Linux along with language. I run a Mac M1. VM? external SSD? I'm learning fast but this was the first worthy question on social platform.","url":"https://dev.to/sqrrl_lee_0d379e32ac90a8/ive-been-studying-python-for-a-few-weeks-now-scrolling-through-tiktok-today-an-influencer-in-j0e","date":1739757641,"author":"Sqrrl_Lee","guid":883,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deploy-Kubernetes-using-ConfigMaps-and-Helm","url":"https://dev.to/enriqueaguila/deploy-kubernetes-using-configmaps-and-helm-2j4m","date":1739757075,"author":"Enrique Aguilar Martinez","guid":882,"unread":true,"content":"<p>Creating well-structured, declarative, and reusable deployments within Kubernetes is an essential skill that increases productivity, using a manifest file, which is used to declare cluster resources, along with Helm (v3), which can be used to template the manifests. helm</p><p>What we are going to implement:\nAn application on a Kubernetes cluster using: <p>\na. ConfigMaps : To manage application settings (e.g. environment variables, configurations). </p>\nb. Helm : As a package manager to declaratively define, install, and manage the application. </p><p>Install Chart on Kubernetes : \n• Use Helm to install the application on the cluster. \n• Check the status of the application and its configuration. <p>\n• Update or remove the deployment as needed. Benefits </p>\n• Configuration management : <p>\nConfigMaps allows you to separate configuration from code. </p>\n• Simplified installation and upgrade : <p>\nHelm makes it easy to manage complex applications on Kubernetes. </p>\n• Declarative : <p>\nYou define the desired state, and Kubernetes and Helm take care of getting there. </p>\nFor recording \nFor configuration. \nTo define and install applications on Kubernetes efficiently.</p><p>\nAn NGINX web server container will be configured to redirect incoming HTTP requests downstream to another container running a custom FLASK-based web application. <p>\nThe NGINX web server container will use the publicly available nginx:1.13.7 image. The FLASK-based web application container will be based on a custom Docker image that you will need to build first.</p></p><p>a-Connect to IDE port 8080 of web-based containers \nb-Perform image compilation and create namespace</p><p>\nIn this step you will create a custom Docker image containing a ﬂaskapp file. Based on a web application, the custom Docker image will later be deployed to the Kubernetes cluster. <p>\nBefore you deploy within the cluster, you will be shown how to create a new namespace resource and how to set it as the default namespace in which the remaining deployment takes place. </p></p><p>List the contents of the ﬂaskapp directory. In the terminal run the following command:</p><p>\nPerform a Docker build to create a custom Docker image. In the terminal run the following command:</p><div><pre><code>docker build -t cloudacademydevops/flaskapp .\n</code></pre></div><p>Check for the presence of the newly created Docker image. In the terminal run the following command:</p><p>Create a custom kikis namespace inside the Kubernetes cluster. In the terminal run the following command:</p><p>Switch to the new kikis namespace. In the terminal run the following command:</p><div><pre><code>kubectl config set-context --current --namespace=kikis\n</code></pre></div><p><strong>Step 4 deploy Nginx ConﬁgMap</strong></p><p>ConfigMaps allow you to decouple configuration artifacts from image content to maintain portability of containerized applications. </p><p>Change to the k8s directory and list its contents. In the terminal, run the following command:</p><p>The updated nginx.configmap.yaml manifest file is already in this repo.</p><p>Apply the updated nginx.configmap.yaml file to the K8s cluster: this will create a new ConﬁgMap resource. \nIn the terminal run the following command:</p><div><pre><code>kubectl apply -f nginx.configmap.yaml\n</code></pre></div><p>List all ConfigMap resources in the K8s cluster. In the terminal run the following command:</p><p>At this step you have updated, saved, and applied the nginx.configmap.yaml manifest file to the K8s cluster.</p><p>This created a new ConfigMap containing the NGINX configuration that will be mounted in the NGINX container.</p><p><strong>Step 5 Apply Implementation</strong>\nDeployments represent a set of multiple identical pods and have the ability to automatically replace instances that fail or become unresponsive.</p><p>In this repo the deployment.yaml manifest file is already updated.</p><p>Let's apply the updated deployment.yaml file to the K8s cluster, this will create a new Deployment file. In the terminal run the following command:</p><div><pre><code>kubectl apply -f deployment.yaml\n</code></pre></div><p>Let's number all the deployments in the K8s cluster. In the terminal run the following command:</p><p>List all the pods in the K8s cluster. In the terminal run the following command:</p><p>Extract the name of the frontend pod and store it in a variable called POD_NAME. \nEcho it back to the terminal. In the terminal run the following commands:</p><div><pre><code>POD_NAME=`kubectl get pods -o jsonpath='{.items[0].metadata.name}'`echo$POD_NAME\n</code></pre></div><p>Now use the kubectl describe command to get a detailed report on the current status of the frontend pod. In the terminal run the following command:</p><div><pre><code>kubectl describe pod $POD_NAME\n</code></pre></div><p>Use the docker images command to confirm the correct Docker image tag for the FLASK-based web application you created in step 1 of the lab. \nIn the terminal, run the following command:</p><div><pre><code>docker images | grep cloudacademydevops\n</code></pre></div><p>List all deployment resources in the K8s cluster. In the terminal run the following command:</p><p>Expose the frontend deployment. This will create a new Service that will allow calling frontend pods through a stable cluster network VIP address. </p><p>In the terminal run the following command:</p><div><pre><code>kubectl expose deployment frontend --port=80 --target-port=80\n</code></pre></div><p>List the new frontend service resource created above. In the terminal run the following command:</p><p>Extract the IP address of the frontend service cluster and store it in a variable called FRONTEND_SERVICE_IP. Echo it back to the terminal. In the terminal run the following commands:</p><div><pre><code>FRONTEND_SERVICE_IP=`kubectl get service/frontend -o jsonpath='{.spec.clusterIP}'` echo$FRONTEND_SERVICE_IP\n</code></pre></div><p>Test the frontend service by sending a curl request to it. In the terminal run the following command:</p><div><pre><code>curl -i http://$FRONTEND_SERVICE_IP\n</code></pre></div><p>Retest the frontend service by sending a new curl request to it. In the terminal run the following command:</p><div><pre><code>curl -i http://$FRONTEND_SERVICE_IP\n</code></pre></div><p>Let’s now examine the logs associated with sending traffic through the NGINX web server. To start, list the current pods within the cluster. In the terminal, run the following command:</p><p>Extract the name of the frontend pod and store it in a variable called FRONTEND_POD_NAME. Echo it back to the terminal. In the terminal run the following commands:</p><div><pre><code>FRONTEND_POD_NAME=`kubectl get pods --no-headers -o custom-columns=\":metadata.name\"` echo$FRONTEND_POD_NAME\n</code></pre></div><p>Perform a directory listing directly inside the NGINX container by listing the contents of the /var/log/nginx directory. In the terminal run the following command:</p><div><pre><code>kubectl exec -it $FRONTEND_POD_NAME -c nginx -- ls -la /var/log/nginx/\n</code></pre></div><p>Use the kubectl logs command to examine the NGINX log generated by the curl commands executed above. In the terminal, run the following command:</p><div><pre><code>kubectl logs $FRONTEND_POD_NAME nginx\n</code></pre></div><p>Use the kubectl logs command to examine the FLASK log generated by the curl commands executed above. In the terminal, run the following command:</p><div><pre><code>kubectl logs $FRONTEND_POD_NAME flask\n</code></pre></div><p><strong>Step 6 Create Helm Project</strong></p><p>To start, navigate up one directory to the directory:</p><p>Use the helm create command to generate a new Helm chart project named test-app. In the terminal, run the following command:</p><p>Use the tree command to render the directory structure on the screen. In the terminal run the following command:</p><p>Use the helm template command to convert Helm templates into a single deployable Kubernetes manifest file. In the terminal, run the following command:</p><p>In this step, you learned how to use the Helm method</p><p><strong>Step 7 Update the deployment to use Helm templates.</strong></p><p>In this scenario, the provided application Helm chart has been configured to create a deployment that launches a single pod containing two containers. The first container is an NGINX web server container that sends incoming HTTP requests to a second container running a FLASK-based web application (based on the Docker image you created earlier).</p><p>Start by deleting all previous resources started within the cluster. \nUse the kubectl delete command to delete the previously created deployment, pod, and service resources. In the terminal run the following command:</p><div><pre><code>kubectl delete deploy,pods,svc --all\n</code></pre></div><p>In this repo the Helm values.yaml file and the deployment.yaml file are already modified.\nUse the helm template command to generate a deployable manifest file. In the terminal, run the following command:</p><p>With this command you ONLY generated the deployable manifest, it has not been deployed to the cluster yet, this is useful to validate the resulting manifest BEFORE actually deploying it.</p><p>This time you will perform the actual deployment to the cluster. To do this, you will again use the helm template command to generate a deployable manifest file, piping the output (manifest) directly to the kubectl apply command. \nIn the terminal run the following command:</p><div><pre><code>helm template kikis ./app | kubectl apply -f -\n</code></pre></div><p>Examine the current services available within the cluster. In the terminal run the following command:</p><p>Extract the IP address of the cloudacademy-app service cluster and store it in a variable named KIKIS_APP_IP. Echo it back to the terminal. </p><p>In the terminal run the following commands:</p><div><pre><code>KIKIS_APP_IP=`kubectl get service/kikis-app -o jsonpath='{.spec.clusterIP}'`\necho $KIKIS_APP_IP\n</code></pre></div><p>Test the kikis-app service by sending a curl request to it. In the terminal run the following command:</p><div><pre><code>curl -i http://$KIKIS_APP_IP\n</code></pre></div><div><pre><code>cp app/values.yaml app/values.dev.yaml \ncp app/values.yaml app/values.prod.yaml\n</code></pre></div><p>Perform a dev redeployment to the cluster by referencing the values.dev.yaml Values ​​file within the helm template command, run the following command:</p><div><pre><code>helm template kikis -f ./app/values.dev.yaml ./app | kubectl apply -f -\n</code></pre></div><p>Retest the cloudacademy-app service by sending another curl request to it. In the terminal run the following command:</p><div><pre><code>curl -i http://$KIKIS_APP_IP\n</code></pre></div><p>Perform a prod redeployment back to the cluster by referencing the values.prod.yaml Values ​​file within the helm template command. In the terminal run the following command:</p><div><pre><code>helm template kikis -f ./app/values.prod.yaml ./app | kubectl apply -f -\n</code></pre></div><p>Retest the cloudacademy-app service by sending it another curl request. In the terminal, run the following command:</p><div><pre><code>curl -i http://$KIKIS_APP_IP\n</code></pre></div><p>Use the helm command to package the application into a graph. In the terminal, run the following command:</p><p>Perform a directory listing to confirm that the graph was created successfully. In the terminal run the following command:</p><p>Before installing the new chart, revert (remove) all previously deployed resources in Helm. In the terminal, run the following command:</p><div><pre><code>helm template kikis -f ./app/values.prod.yaml ./app | kubectl delete -f -\n</code></pre></div><p>Install the new graphic. In the terminal run the following command:</p><div><pre><code>helm install kikis-app app-0.1.0.tgz\n</code></pre></div><p>List all current versions of Helm for the current namespace context. In the terminal run the following command:</p><p>Show the deployment, pod, and service resources that were created due to the Helm chart installation. In the terminal, run the following command:</p><div><pre><code>kubectl get deploy,pods,svc\n</code></pre></div><p>Test the kikis-app service by sending a curl request to it. In the terminal run the following command:</p><div><pre><code>curl -i http://$KIKIS_APP_IP\n</code></pre></div><p>The deployment consisted of a single pod containing two containers. \nThe first container was an NGINX web server container that redirected incoming HTTP requests to a second container running a FLASK-based web application, based on the Docker image you created in the first lab step. We used yaml as a values ​​file to externalize runtime variables, ensuring that your Helm templates are reusable across different environments, in this case dev and prod environments, via the values.dev.yaml and values.prod.yaml files respectively.<p>\nVisit my repo to have access to the code:</p><a href=\"https://github.com/EnriqueAguila/Deploy-Kubernetes-using-Con-gMaps-and-Helm/tree/main\" rel=\"noopener noreferrer\">https://github.com/EnriqueAguila/Deploy-Kubernetes-using-Con-gMaps-and-Helm/tree/main</a></p><p>Enrique Aguilar Martinez\nCloud Engineer</p>","contentLength":11556,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"learn Data Structure and Probleme-Solving","url":"https://dev.to/ammardeche/learn-data-structure-and-probleme-solving-59e4","date":1739755746,"author":"Ammar Aissa DECHE","guid":875,"unread":true,"content":"<p>Hi everyone! Today I started learning data structures and problem-solving. I want to master these topics to join popular communities like LeetCode and Codewars. My first lecture today was about adding an element at the end of an array</p>","contentLength":234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tipos de Case na Programação: Como Escolher o Melhor para seu Código","url":"https://dev.to/alexandrefreire/tipos-de-case-na-programacao-como-escolher-o-melhor-para-seu-codigo-4ol7","date":1739754803,"author":"Alexandre Freire","guid":874,"unread":true,"content":"<p>Se você é programador, provavelmente já se deparou com diferentes estilos de escrita para nomear variáveis, funções e classes. Esses padrões são chamados de naming case e desempenham um papel fundamental na legibilidade e padronização do código.</p><p>Vamos explorar os principais tipos de case usados na programação, quando utilizá-los e como escolher o melhor para seu projeto.</p><p>Os naming cases são convenções de nomenclatura usadas para formatar nomes de variáveis, funções, classes e constantes. Seguir um padrão adequado melhora a organização do código e facilita a colaboração em equipe.</p><p>A seguir, veremos os estilos mais comuns e suas aplicações.</p><p>📌  As palavras são unidas sem separadores, e a primeira letra da primeira palavra é minúscula, enquanto as demais iniciam com maiúscula.</p><p>📌  Variáveis e funções em linguagens como JavaScript, Java, TypeScript e Swift.</p><div><pre><code>let nomeUsuario = \"João\";  \nfunction calcularTotalCompra() { /* código */ }  \n</code></pre></div><ul><li>Muito usado em linguagens populares.</li><li>Fácil de ler quando há poucas palavras.</li><li>Evita caracteres especiais, tornando o código mais limpo.</li></ul><ul><li>Pode ser difícil de ler em nomes muito longos.</li></ul><p>📌  Similar ao camelCase, mas a primeira letra também é maiúscula.</p><p>📌  Nome de classes, componentes em React e .NET.</p><div><pre><code>class UsuarioCadastro { /* código */ }  \n</code></pre></div><ul><li>Padrão recomendado para classes e objetos.</li><li>Fácil de identificar elementos de alto nível no código.</li></ul><ul><li>Pode ser confundido com camelCase se não houver um padrão bem definido no projeto.</li></ul><p>📌  As palavras são separadas por underscores (_) e todas as letras são minúsculas.</p><p>📌  Python, bancos de dados (SQL) e algumas configurações de arquivos.</p><div><pre><code>nome_usuario = \"João\"  \ndef calcular_total_compra():  \n    # código  \n</code></pre></div><ul><li>Excelente legibilidade, especialmente para nomes longos.</li><li>Muito usado em linguagens como Python e SQL.</li></ul><ul><li>Em muitas linguagens modernas (JavaScript, Java, C#, Swift), o uso do snake_case foge do padrão adotado pela comunidade, o que pode prejudicar a padronização do código.</li></ul><p>📌  Uma variação do snake_case, mas todas as letras são maiúsculas.</p><p>📌  Nomenclatura de constantes em diversas linguagens.</p><div><pre><code>#define TAMANHO_MAXIMO 100  \nconst MAX_USERS = 500;  \n</code></pre></div><ul><li>Fácil de identificar constantes no código.</li><li>Padrão adotado na maioria das linguagens.</li></ul><ul><li>Não indicado para variáveis comuns, pois dificulta a leitura.</li></ul><p>📌  As palavras são separadas por traços (-) e todas as letras são minúsculas.</p><p>📌  Nomes de arquivos, URLs e algumas configurações de frameworks web.</p><div><pre><code>minha-variavel  \nhttps://meusite.com/pagina-inicial  \n</code></pre></div><ul><li>Padrão ideal para URLs e arquivos.</li><li>Amplamente utilizado no desenvolvimento web.</li></ul><ul><li>Não pode ser usado em variáveis de código-fonte, pois o hífen é interpretado como um operador de subtração.</li></ul><h2>\n  \n  \n  Qual Naming Case Usar no Seu Projeto?\n</h2><p>A escolha do naming case depende da linguagem e das convenções do time. Aqui estão algumas recomendações gerais:</p><p>🔹 JavaScript, Java, Swift → Use camelCase para variáveis e funções.\n🔹 C#, React, .NET → Use PascalCase para classes e componentes.<p>\n🔹 Python, SQL → Prefira snake_case para variáveis e nomes de tabelas.</p>\n🔹 Constantes em qualquer linguagem → Use SCREAMING_SNAKE_CASE.<p>\n🔹 Nomes de arquivos e URLs → Utilize kebab-case.</p></p><p>Escolher a convenção certa para nomear variáveis e funções ajuda a manter um código limpo e organizado. Além disso, seguir um padrão melhora a colaboração entre desenvolvedores e evita erros de interpretação.</p><p>Agora que você conhece os principais naming cases, qual deles você mais usa? Deixe seu comentário abaixo! 👇💬</p>","contentLength":3595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Testing Strategies: Essential Tips and Tricks for Developers","url":"https://dev.to/d_thiranjaya_6d3ec4552111/testing-strategies-essential-tips-and-tricks-for-developers-h2f","date":1739754557,"author":"Pawani Madushika","guid":856,"unread":true,"content":"<h2>\n  \n  \n  Advanced Testing Strategies for Modern Development (2025)\n</h2><p>In the rapidly evolving software development landscape, rigorous testing plays a pivotal role in ensuring the reliability, performance, and security of applications. This article explores cutting-edge testing strategies and advanced techniques poised to revolutionize development practices in 2025 and beyond.</p><h2>\n  \n  \n  Latest Advanced Techniques\n</h2><p><strong>1. AI-Driven Test Generation and Analysis</strong></p><p>Artificial intelligence (AI) empowers test automation tools to generate nuanced and comprehensive test cases, significantly reducing manual effort. AI algorithms analyze codebase and user behavior patterns to identify potential risks and coverage gaps.</p><p>Model-based testing leverages mathematical models to represent the system's behavior and generate test cases. This approach enables thorough exploration of complex systems and reduces the need for extensive manual test creation.</p><p>Chaos engineering introduces controlled failures to systems to test their resilience and uncover potential vulnerabilities. By injecting random errors or outages, developers can assess the system's ability to recover and maintain stability under extreme conditions.</p><p><strong>1. Real-Time Performance Monitoring</strong></p><p>Modern testing tools integrate real-time monitoring capabilities, providing developers with actionable insights into performance bottlenecks. They track key metrics such as latency, memory usage, and resource consumption to facilitate rapid debugging and optimization.</p><p><strong>2. Load and Stress Testing with Cloud-Based Tools</strong></p><p>Cloud-based load and stress testing platforms enable developers to simulate realistic traffic patterns and test the system's ability to scale and handle high user loads. This helps identify performance issues that may not be evident in traditional testing environments.</p><p><strong>3. Database Performance Optimization</strong></p><p>Advanced techniques for database performance optimization include indexing strategies, caching, and query optimization. Developers can leverage tools like EXPLAIN and Query Profiles to analyze database queries and identify areas for improvement.</p><h2>\n  \n  \n  Modern Development Workflow\n</h2><p><strong>1. Test-Driven Development (TDD) with Modern Frameworks</strong></p><p>TDD frameworks encourage developers to write tests before implementing functionality. Modern frameworks like  and  automate test setup and assertion handling, fostering a rigorous and efficient development cycle.</p><p><strong>2. Continuous Integration and Continuous Delivery (CI/CD) Integration</strong></p><p>CI/CD pipelines seamlessly integrate testing into the development workflow. Automated tests are executed on code commits, providing real-time feedback and reducing the risk of merging untested code into the production environment.</p><p><strong>3. Deployment Considerations</strong></p><p>Advanced testing strategies extend to deployment considerations. End-to-end testing tools simulate real-world scenarios to validate the functionality and performance of the system in the target deployment environment.</p><p> (with Selenium 4.0)</p><p>Selenium WebDriver remains a powerful tool for automated web testing. Selenium 4.0 introduces headless testing, improved multi-browser support, and enhanced reporting capabilities.</p><p>Cypress is a modern testing framework that combines ease of use with extensive features. Its snapshot testing ensures visual consistency, while its time-travel debugger allows developers to step through tests and instantly view the changes in the UI.</p><p>K6 is an open-source load testing tool specifically designed for high-performance applications. It offers cloud-based execution, real-time monitoring, and in-depth performance analysis.</p><ul><li>Advanced AI-driven testing techniques revolutionize test generation and analysis.</li><li>Model-based testing and chaos engineering enhance system resilience and coverage.</li><li>Real-time performance monitoring and cloud-based testing ensure optimal system performance.</li><li>CI/CD integration and deployment considerations optimize the testing workflow and minimize production risks.</li><li>Modern tools like Selenium WebDriver, Cypress, and K6 empower developers with robust testing capabilities.</li></ul><p>By embracing these advanced testing strategies, developers can elevate their testing practices, ensure the quality and reliability of their applications, and stay at the forefront of modern development trends.</p>","contentLength":4259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Share authentication cookies between ASP.NET 4.x and ASP.NET Core","url":"https://dev.to/roicp/share-authentication-cookies-between-aspnet-4x-and-aspnet-core-47gj","date":1739753629,"author":"Rodrigo Ipaves de Campos Pinto","guid":855,"unread":true,"content":"<p>I've been working with ASP.NET 4.x and ASP.NET Core for a while, and I've always noticed some unusual differences between them.</p><p>The migration process from ASP.NET 4.x to ASP.NET Core can be challenging, depending on how the source code is designed or organized, the size of the application, and the packages used.</p><p>I'm going to present a technique for implementing a side-by-side migration process that ensures both applications continue to run.</p><p>Let's begin with a simple ASP.NET MVC 4.8 application configured for forms authentication.</p><p>Normally, the authentication cookie is created by calling FormsAuthentication.SetAuthCookie(username, rememberMe); after a successful login, and it is destroyed by calling FormsAuthentication.SignOut() upon logout. This setup is supported by a small configuration in the web.config file using the authentication tag.</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Using browser DevTools, you can locate the ASP.NET authentication cookie generated during the forms authentication login process.<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftlolgzgkujsdu6apbf2d.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftlolgzgkujsdu6apbf2d.png\" alt=\"ASPXAUTH Cookie\" width=\"800\" height=\"51\"></a></p><p>However, to share the authentication cookie between the applications, we need to change the way this cookie is generated.</p><p>First, remove the web.config configuration. Let's go back to the original code without the authentication tag.</p><div><pre><code></code></pre></div><p>Now, let's install some OWIN packages.</p><ul><li>Microsoft.Owin.Host.SystemWeb</li><li>Microsoft.Owin.Security.Interop</li><li>Microsoft.Owin.Security.Cookies</li></ul><p>Next, create a startup class.</p><div><pre><code></code></pre></div><p>Add a configuration to AntiForgeryToken on Global.asax in the Application_Start.</p><div><pre><code></code></pre></div><p>And finally, the AccountController should create the auth cookie.</p><div><pre><code></code></pre></div><p>Let's dive into the ASP.NET Core 9 part.</p><p>In Program.cs, configure the authentication to use cookies.</p><div><pre><code></code></pre></div><p>Here's our ASP.NET Core cookie 😉</p><p>It's time to see if everything worked.</p><p>Create a new controller with an action decorated with the [Authorize] attribute (do this in both applications).</p><div><pre><code></code></pre></div><p>Update the _Layout.cshtml file to link the routes in both apps.</p><div><pre><code>&lt;ul class=\"navbar-nav flex-grow-1\"&gt;\n    @if (Request.IsAuthenticated)\n    {\n        &lt;li&gt;\n            &lt;a href=\"#\"&gt;Hello, @User.Identity.Name!&lt;/a&gt;\n        &lt;/li&gt;\n        &lt;li&gt;\n            @using (Html.BeginForm(\"Logout\", \"Account\", FormMethod.Post, new { id = \"logoutForm\", @class = \"navbar-form\" }))\n            {\n                @Html.AntiForgeryToken()\n                &lt;button type=\"submit\" class=\"btn btn-link navbar-btn\"&gt;Logout&lt;/button&gt;\n            }\n        &lt;/li&gt;\n    }\n    else\n    {\n        &lt;li&gt;\n            @Html.ActionLink(\"Login\", \"Login\", \"Account\", new { area = \"\" }, new { @class = \"nav-link\" })\n        &lt;/li&gt;\n    }\n\n    &lt;li&gt;@Html.ActionLink(\"Home\", \"Index\", \"Home\", new { area = \"\" }, new { @class = \"nav-link\" })&lt;/li&gt;\n    &lt;li&gt;@Html.ActionLink(\"Local Private\", \"Index\", \"Private\", new { area = \"\" }, new { @class = \"nav-link\" })&lt;/li&gt;\n    &lt;li&gt;\n        &lt;a class=\"nav-link\" href=\"https://localhost:7102/Private\"&gt;External Private&lt;/a&gt;\n    &lt;/li&gt;\n&lt;/ul&gt;\n</code></pre></div><div><pre><code>&lt;ul class=\"navbar-nav flex-grow-1\"&gt;\n    @if (User.Identity.IsAuthenticated)\n    {\n        &lt;li class=\"nav-item\"&gt;\n            &lt;span class=\"navbar-text\"&gt;Hello, @User.Identity.Name!&lt;/span&gt;\n        &lt;/li&gt;\n        &lt;li class=\"nav-item\"&gt;\n            &lt;form id=\"logoutForm\" asp-controller=\"Account\" asp-action=\"Logout\" method=\"post\" class=\"form-inline\"&gt;\n                @Html.AntiForgeryToken()\n                &lt;button type=\"submit\" class=\"nav-link btn btn-link\" style=\"display:inline; padding:0;\"&gt;Logout&lt;/button&gt;\n            &lt;/form&gt;\n        &lt;/li&gt;\n    }\n    else\n    {\n        &lt;li class=\"nav-item\"&gt;\n            &lt;a class=\"nav-link\" asp-controller=\"Account\" asp-action=\"Login\"&gt;Login&lt;/a&gt;\n        &lt;/li&gt;\n    }\n\n    &lt;li class=\"nav-item\"&gt;\n        &lt;a class=\"nav-link text-dark\" asp-area=\"\" asp-controller=\"Home\" asp-action=\"Index\"&gt;Home&lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=\"nav-item\"&gt;\n        &lt;a class=\"nav-link text-dark\" asp-area=\"\" asp-controller=\"Home\" asp-action=\"Privacy\"&gt;Privacy&lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=\"nav-item\"&gt;\n        &lt;a class=\"nav-link text-dark\" asp-area=\"\" asp-controller=\"Private\" asp-action=\"Index\"&gt;Local Private&lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li&gt;\n        &lt;a class=\"nav-link text-dark\" href=\"https://localhost:44301/Private\"&gt;External Private&lt;/a&gt;\n    &lt;/li&gt;\n&lt;/ul&gt;\n</code></pre></div><p>Of course, sharing the DataProtectionKey via the local file system isn't ideal.</p><p>There are many remote and distributed alternatives available, but for demonstration purposes, we'll use Valkey/Redis.</p><p>Now, let's create and run a container with Valkey to store the data protection key.</p><div><pre><code>docker run -d --rm -p \"6379:6379\" --name share-valkey valkey/valkey:alpine\n</code></pre></div><p>(it could be any Valkey/Redis server that you have access)</p><p>Back to ASP.NET 4.8 application</p><p>Install the package Microsoft.AspNetCore.DataProtection.StackExchangeRedis</p><p>And change the Startup class</p><div><pre><code></code></pre></div><p>Don't worry about maintaining the DirectoryInfo because the builder won't generate the DataProtectionKey file anymore.</p><p>And in the ASP.NET Core application, configure Data Protection to use the same remote key storage.</p><p>Install the package Microsoft.AspNetCore.DataProtection.StackExchangeRedis</p><div><pre><code></code></pre></div><p>Running the applications, we can see that the DataProtectionKey is being used from Valkey/Redis.</p><p>I know there's a lot of code, but in the end, it's pretty straightforward once you understand how to configure the OWIN cookie.</p><p>Let's review some key points.</p><ul><li>Install the following packages:\n\n<ul><li>Microsoft.Owin.Host.SystemWeb</li><li>Microsoft.Owin.Security.Cookies</li><li>Microsoft.Owin.Security.Interop</li><li>Microsoft.AspNetCore.DataProtection.StackExchangeRedis</li></ul></li><li>Create the Startup class to configure the cookie settings</li><li>Update the AccountController to properly create and destroy the authentication cookie</li><li>Configure AntiForgeryToken in the Application_Start method of Global.asax</li></ul><ul><li>Install the package:\n\n<ul><li>Microsoft.AspNetCore.DataProtection.StackExchangeRedis</li></ul></li><li>Configure Cookie Authentication:\n\n<ul><li>Add cookie authentication to the WebApplicationBuilder.</li><li>Call  in the pipeline.</li></ul></li><li>Modify how the Account controller creates and destroys the authentication cookie</li></ul><ul><li>The cookie parameters must be the same in both applications.</li><li>Do not expose the DataProtectionKey 🔥</li></ul><p>The source code can be found <a href=\"https://github.com/roicp/SharedAuthCookie\" rel=\"noopener noreferrer\">here</a></p><p>Thanks, and keep making awesome code!</p>","contentLength":5974,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Always keep this hacking technique in mind","url":"https://dev.to/blue_byte/always-keep-this-hacking-technique-in-mind-4mi0","date":1739753595,"author":"Blue Byte","guid":854,"unread":true,"content":"<p>Imagine that during an engagement you come across a static website, with only an image on the home page and standard files from a frontend framework. You have already reviewed them and found no sensitive information, no route or anything relevant. You have also found (maybe while analyzing HTTP headers) that the server is behind the Cloudfront CDN. End of the line, right?</p><p>Also imagine that, by intuition, you suspect that there is an AWS S3 bucket behind the CDN, serving static files. The problem is that you don't have the name of this bucket to be able to test it. </p><p>The good news is that by causing signature inconsistencies and causing errors in handling requests, the server can leak this information.</p><div><pre><code>POST / HTTP/2\nHost: flow.redacted.com\nUser-Agent: Mozilla/5.0\nContent-Type: application/x-www-form-urlencoded\nContent-Length: 9\n\nlogging=1\n</code></pre></div><p>When sending the above request, the response received was something like:</p><div><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;Error&gt;\n    &lt;Code&gt;SignatureDoesNotMatch&lt;/Code&gt;\n    &lt;Message&gt;The request signature we calculated does not match the signature you provided. Check your key and signing method.&lt;/Message&gt;\n    &lt;AWSAccessKeyId&gt;REDACTED&lt;/AWSAccessKeyId&gt;\n\n    &lt;StringToSign&gt;\n        POST \n\n        application/x-www-form-urlencoded\n\n        x-amz-date:REDACTED\n        /redacted-bucket-name/index.htm?logging\n    &lt;/StringToSign&gt;\n\n    &lt;SignatureProvided&gt;REDACTED&lt;/SignatureProvided&gt;\n    &lt;StringToSignBytes&gt;REDACTED&lt;/StringToSignBytes&gt;\n    &lt;RequestId&gt;REDACTED&lt;/RequestId&gt;\n    &lt;HostId&gt;REDACTED&lt;/HostId&gt;\n&lt;/Error&gt;\n</code></pre></div><p>And no, this bucket was not \"fully exposed\", but it was configured so that any authenticated user (from any account) on AWS could access it.</p><div><pre><code>aws s3 sync s3://redacted-bucket-name .\n</code></pre></div><p>From then on some of juicy info was found! After obtaining the AWS keys (a process that I will not describe here, but I can only say that it is sad when they are in files inside a public bucket), with the  command in the <a href=\"https://github.com/RhinoSecurityLabs/pacu\" rel=\"noopener noreferrer\">Pacu framework</a> it is possible to access the account in the browser.</p><p>And this was another case of a security misconfiguration gone terrible!</p>","contentLength":2081,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Check out what I just built.","url":"https://dev.to/fstrube/check-out-what-i-just-built-2kj7","date":1739753123,"author":"Franklin Strube","guid":873,"unread":true,"content":"<p>I needed a way to give a new UI framework a test-drive, so I decided to build a quick little PDF generator. Those of you who know me know that I love me some PDFs. I ended up with <a href=\"https://franklinstrube.com/tools/markdown-to-pdf\" rel=\"noopener noreferrer\">this little gem</a>. </p><p>Give it a whirl, feedback welcome 👇</p>","contentLength":235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Te Invito a Crear un Sistema Operativo para Revivir Hardware Obsoleto","url":"https://dev.to/rickyperrone/un-sistema-operativo-para-revivir-el-hardware-obsoleto-una-revolucion-en-sostenibilidad-y-d3","date":1739752963,"author":"Ángel Ricardo Baez","guid":872,"unread":true,"content":"<h2>\n  \n  \n  Un Sistema Operativo para Hardware Obsoleto: Una Revolución en Sostenibilidad y Rendimiento\n</h2><p>En un mundo donde la obsolescencia de hardware es una constante, surge una nueva propuesta para cambiar la forma en que usamos la tecnología. Este proyecto tiene como objetivo desarrollar un sistema operativo innovador diseñado específicamente para revivir computadoras antiguas, optimizándolas para que sigan siendo útiles, funcionales y eficientes. No se trata de un \"Mac en miniatura\" ni de una versión reducida de ningún sistema conocido, sino de una nueva experiencia de usuario que aprovecha al máximo los recursos limitados del hardware obsoleto.</p><h2>\n  \n  \n  ¿Por qué es importante este proyecto?\n</h2><p>La obsolescencia planificada y el constante avance de la tecnología generan un ciclo en el que millones de dispositivos son desechados antes de tiempo. Este proyecto nace con la idea de romper este ciclo, brindando a las computadoras antiguas una nueva oportunidad para ser útiles y productivas. La sostenibilidad no solo es una tendencia, sino una necesidad urgente, y este sistema operativo tiene el propósito de contribuir a reducir la contaminación electrónica y la necesidad de adquirir nuevo hardware constantemente.</p><h2>\n  \n  \n  Un Sistema Operativo que Respeta el Hardware\n</h2><p>Este sistema operativo no es solo ligero, sino que está diseñado para ofrecer una experiencia robusta, adaptándose a las limitaciones de hardware y optimizando el rendimiento. A diferencia de las soluciones actuales que se centran en sistemas pesados o complicados, esta propuesta busca ofrecer:</p><p>Rendimiento optimizado para hardware más antiguo, aprovechando cada recurso disponible sin sobrecargar el sistema.</p><p>Interfaz intuitiva y fácil de usar, sin la complejidad de las interfaces modernas que requieren más recursos.</p><p>Fiabilidad y estabilidad, ofreciendo un sistema confiable para tareas cotidianas y profesionales sin problemas de lentitud o errores frecuentes.</p><h2>\n  \n  \n  ¿Cómo se diferencia de otras opciones?\n</h2><p>Este sistema operativo no busca replicar otras plataformas, sino ofrecer una solución verdaderamente nueva para quienes necesitan una alternativa que no dependa de la compra constante de nuevos dispositivos. Al basarse en principios de eficiencia y longevidad, el sistema está diseñado para hacer más con menos, al contrario de lo que proponen muchos sistemas operativos actuales que requieren hardware moderno y costoso.</p><h2>\n  \n  \n  Una Filosofía de Generosidad y Sostenibilidad\n</h2><p>Mi enfoque hacia este proyecto está influenciado por los valores de Zarathustra y el Zoroastrismo, que promueven la generosidad y la bondad, pero rechazan la ambición egoísta. Nunca he perseguido el dinero ni el éxito personal como motivación principal, sino que mi objetivo siempre ha sido contribuir al bienestar de los demás. Este proyecto refleja esa misma filosofía: hacer el bien por el bien, sin esperar nada a cambio.</p><p>En un mundo que a menudo está impulsado por el consumismo y la búsqueda del beneficio personal, mi visión es ofrecer una alternativa honesta y sostenible, donde la tecnología se utilice para mejorar la vida de las personas, no para enriquecerse a costa de ellas. Este sistema operativo no es solo una herramienta técnica, sino también una manifestación de valores que creo que son esenciales para un futuro más equitativo y saludable para todos.</p><h2>\n  \n  \n  Un Proyecto sin Fines de Lucro\n</h2><p>Es importante destacar que este proyecto no tiene fines de lucro. Mi objetivo no es ganar dinero con él, sino ofrecer una solución real a un problema que me ha tocado profundamente. Este es un esfuerzo por y para la comunidad, para mejorar la vida de quienes tienen hardware antiguo y evitar que termine en la basura. Mi misión es crear un sistema operativo que pueda impactar positivamente a todos, sin ninguna motivación económica detrás.</p><h2>\n  \n  \n  ¿Quién está detrás de este proyecto?\n</h2><p>Este proyecto no solo nace de una idea técnica, sino de una visión personal que ha sido moldeada por mi vida, mis experiencias y mis desafíos. Soy Angel Baez, un hombre de 45 años, con cuatro hijos, dos de los cuales tienen autismo, y he enfrentado obstáculos que no solo han probado mi resiliencia, sino que también me han enseñado la importancia de dar nuevas oportunidades. Actualmente, vivo en una situación difícil, pero no he dejado que eso me impida soñar con un futuro mejor.</p><p>Mi vida me ha mostrado que, aunque el mundo a menudo valora lo nuevo y lo brillante, lo viejo y lo olvidado también tiene un valor inmenso. Este proyecto es mi manera de contribuir, de dar algo de mí mismo para mejorar el acceso a la tecnología, hacerla más accesible para todos y, sobre todo, de ofrecer una segunda oportunidad a lo que muchos consideran obsoleto.</p><p>Si crees en el poder de darle una nueva vida a lo viejo y quieres ser parte de esta revolución, te invitamos a unirte a la causa. Ya sea como desarrollador, diseñador, o simplemente como alguien que apoya la idea, tu ayuda es valiosa para crear una alternativa tecnológica más sostenible y accesible para todos.</p><p>¡Juntos podemos darle una segunda oportunidad a la tecnología que ya tenemos!</p>","contentLength":5169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Amazon CloudWatch Costs for High-Traffic Lambda Functions with Advanced Logging Controls","url":"https://dev.to/matheusdasmerces/optimizing-amazon-cloudwatch-costs-for-high-traffic-lambda-functions-with-advanced-logging-controls-5406","date":1739752200,"author":"Matheus das Mercês","guid":853,"unread":true,"content":"<p>Are you happy with your CloudWatch bill? If you have one or more high-traffic Lambda functions, you pay for their duration and number of requests per month. However, there is a chance these functions are also increasing your CloudWatch costs - and this is why you need Advanced Logging Controls.</p><p>In this article, I explain how to optimize CloudWatch costs while respecting compliance, and leveraging a simple AWS Systems Manager Automation Runbook to achieve full control of your logs.</p><p>CloudWatch log groups have two storage classes and their costs vary from region to region, but according to the <a href=\"https://aws.amazon.com/cloudwatch/pricing/\" rel=\"noopener noreferrer\">CoudWatch pricing page</a>, in  (eu-west-1):</p><ul><li> class, where you can have all log group features, like a subscription filter, for example, you pay  per GB. </li><li> class, where features are limited and you can not have a subscription filter in place and the data can only be queried using Log Insights, the cost is  per GB.</li></ul><p>Knowing that you pay per GB for the data ingested into a log group, how can you determine the size of a message?</p><p>The short answer is doing that before sending it to CloudWatch is not very handy and there are situations where you can not or do not want to reduce its size. Instead, you can control what kind of data is ingested into log groups, when, and for how long - and this is where the benefits are.</p><p>Whenever your Lambda function sends logs to a log group, it uses the CloudWatch sub-feature of . Under the hoods, your Lambda uses the PutLogEvents API from CloudWatch, generating a metric called  for the Standard Class and  for Infrequent-Access. Then, based on these two metrics, AWS creates your bill (more about monitoring these metrics later in this article).</p><p>To put this into perspective, let's imagine that you have a high-traffic Lambda function that executes  a day. I've separated some examples, including the number of bytes that message can generate and how much they can cost you at scale.</p><p>Application logs are custom messages generated by your Lambda function. For instance, you want to log the event received, or debug messages you add to your code along the way, and so on.</p><p>Imagine that your high-traffic Lambda function handles API Gateway requests. For some reason, you want to log the event received. Let's have a look at an example of that payload:</p><div><pre><code>{\n  \"resource\": \"/my/path\",\n  \"path\": \"/my/path\",\n  \"httpMethod\": \"GET\",\n  \"headers\": {\n    \"header1\": \"value1\",\n    \"header2\": \"value1,value2\"\n  },\n  \"multiValueHeaders\": {\n    \"header1\": [\n      \"value1\"\n    ],\n    \"header2\": [\n      \"value1\",\n      \"value2\"\n    ]\n  },\n  \"queryStringParameters\": {\n    \"parameter1\": \"value1,value2\",\n    \"parameter2\": \"value\"\n  },\n  \"multiValueQueryStringParameters\": {\n    \"parameter1\": [\n      \"value1\",\n      \"value2\"\n    ],\n    \"parameter2\": [\n      \"value\"\n    ]\n  },\n  \"requestContext\": {\n    \"accountId\": \"123456789012\",\n    \"apiId\": \"id\",\n    \"authorizer\": {\n      \"claims\": null,\n      \"scopes\": null\n    },\n    \"domainName\": \"id.execute-api.us-east-1.amazonaws.com\",\n    \"domainPrefix\": \"id\",\n    \"extendedRequestId\": \"request-id\",\n    \"httpMethod\": \"GET\",\n    \"identity\": {\n      \"accessKey\": null,\n      \"accountId\": null,\n      \"caller\": null,\n      \"cognitoAuthenticationProvider\": null,\n      \"cognitoAuthenticationType\": null,\n      \"cognitoIdentityId\": null,\n      \"cognitoIdentityPoolId\": null,\n      \"principalOrgId\": null,\n      \"sourceIp\": \"IP\",\n      \"user\": null,\n      \"userAgent\": \"user-agent\",\n      \"userArn\": null,\n      \"clientCert\": {\n        \"clientCertPem\": \"CERT_CONTENT\",\n        \"subjectDN\": \"www.example.com\",\n        \"issuerDN\": \"Example issuer\",\n        \"serialNumber\": \"a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1:a1\",\n        \"validity\": {\n          \"notBefore\": \"May 28 12:30:02 2019 GMT\",\n          \"notAfter\": \"Aug  5 09:36:04 2021 GMT\"\n        }\n      }\n    },\n    \"path\": \"/my/path\",\n    \"protocol\": \"HTTP/1.1\",\n    \"requestId\": \"id=\",\n    \"requestTime\": \"04/Mar/2020:19:15:17 +0000\",\n    \"requestTimeEpoch\": 1583349317135,\n    \"resourceId\": null,\n    \"resourcePath\": \"/my/path\",\n    \"stage\": \"$default\"\n  },\n  \"pathParameters\": null,\n  \"stageVariables\": null,\n  \"body\": \"Hello from Lambda!\",\n  \"isBase64Encoded\": false\n}\n</code></pre></div><p>This message generates  bytes. If this Lambda executes an average of 5 million a day, by the end of the month, the size in GB will be  (<em>1903 x 5000000 = 8.86GB a day, times 31 = 274.66GB</em>).</p><p>Looking at the pricing for the Standard class, by the end of the month, this single log from a single Lambda cost you  ().</p><p>If you think this is an undesired cost - and somehow you want to keep logging these messages for debugging purposes, you need Advanced Logging Controls.</p><p>System logs are log messages generated by the Lambda service by default. For instance, Lambda reports data with the , , , and  and  time. Even if your Lambda code does not generate any application logs, by default, the system logs will always appear in your log group.</p><p>Let's have a look at a report message generated by a  Lambda function in plain text (a cold Lambda has a different report message):</p><div><pre><code>START RequestId: 1716e630-0997-4bd6-aae3-0f681ef1e69c Version: $LATEST\nEND RequestId: 1716e630-0997-4bd6-aae3-0f681ef1e69c\nREPORT RequestId: 1716e630-0997-4bd6-aae3-0f681ef1e69c Duration: 1.80 ms Billed Duration: 2 ms Memory Size: 128 MB Max Memory Used: 68 MB\n</code></pre></div><p>These logs created by one warm Lambda execution generate  bytes. If this Lambda executes an average of 5 million a day (not considering cold starts), by the end of the month, the size in GB will be  (<em>262 x 5000000 = 1.22GB a day, times 31 = 37.82GB</em>).</p><p>Looking at the pricing for the Standard class, by the end of the month, this report messages from a single Lambda cost you  (). </p><p>Not a fortune, right? The important question to ask yourself is, do you need these log messages? Commonly, you would want to analyze the duration, and memory consumed for a Lambda to reduce costs related to Lambda execution, for example. But if you don't actively look at these messages, it is wise to change them to optimize your costs at scale. </p><h3>\n  \n  \n  Measuring the size of a log message\n</h3><p>How can you calculate the size, in bytes, of a message sent to CloudWatch, as I did for the previous examples?</p><p>You can use the same query I've used in CloudWatch Log Insights:</p><div><pre><code>FIELDS @message, ingested_bytes\n#| filter @message like 'REPORT'\n| STATS sum(strlen(@message)) AS ingested_bytes\n</code></pre></div><blockquote><p>: Querying data using Insights can also be expensive. For each GB of data scanned, you will pay $0.0057 (eu-west-1). Make sure you run this query in a log group that does not retain much data or only for a short time window.</p></blockquote><h2>\n  \n  \n  Finding the perfect balance\n</h2><p>There is always a balance between  and . This is especially true in large organizations, where there might be constraints related to:</p><ul><li>Retention period, as in how long the data must be stored to be compliant with the company's policies</li><li>The type of data ingested, which is more often related to the company's technical guidelines for specific AWS services</li></ul><p>The important aspect of this trade-off is . By clearly identifying these constraints alongside the analysis of cost reduction, you have better arguments to start a negotiation that benefits all parties involved.</p><h2>\n  \n  \n  Setting up a retention policy\n</h2><p>Knowing how long you have to keep the data in a log group, you can start by setting up a retention policy for your log groups.</p><p>Less expensive than the  metric, there is another metric called  metric, which is the amount of time that the data is stored - which AWS also charges you for. The cost for the Ireland region is  per GB compressed (0.15 compression ratio for each uncompressed byte).</p><p>Having a retention policy can help your application optimize some costs and also be more sustainable. Especially when using  (AWS Cloud Development Kit) as your IaC tool (Infrastructure as Code), when the default configuration from a log group is to never expire the data stored. If you don't use it after some period, it's best to set up a retention policy.</p><p>Below you can find an example, in TypeScript, of how to change the CDK default when creating a Log Group and setting the retention policy you define:</p><div><pre><code>new LogGroup(this, 'MyLogGroup', {\n  logGroupName: '/aws/lambda/advanced-logging-control',\n  retention: RetentionDays.ONE_WEEK,\n})\n</code></pre></div><p>One common mistake when developing a Lambda function using  (whether you use or not TypeScript features) is to log everything using the  method. In order to fully leverage <strong>Advanced Logging Controls</strong> and be able to manipulate the log level, it is important to use the correct methods when logging with JavaScript.</p><p>I have tested all JavaScript  methods when sending to a CloudWatch log group, and I separated a few examples from the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/console\" rel=\"noopener noreferrer\">console object documentation</a> and how they match the Lambda application log-level configuration. Depending on the log level you set, different console methods will affect your CloudWatch log group. When using them properly, you make your Lambda function ready to switch the log level on the fly:</p><ol><li> level shows messages from all JavaScript console methods, except for .</li><li> level shows messages from , , and .</li><li> level shows messages from  only.</li><li> level shows messages from all JavaScript console methods.</li><li> level shows messages from all JavaScript console methods.</li><li> level does not show any messages from any JavaScript console method.</li></ol><p>Using the proper JavaScript console methods for specific situations not only makes your Lambda function ready to use Advanced Logging Controls but also helps your JavaScript comply with .</p><h2>\n  \n  \n  Switching log level on the fly\n</h2><p>Now, let's get down to business and see how to natively switch the log level of your Lambda function when needed.</p><p>I've created a very simple Lambda function with a few examples of using the JavaScript console methods:</p><div><pre><code>export const handler = async (\n    event: any,\n): Promise&lt;string&gt; =&gt; {\n    console.log('Received event:', JSON.stringify(event, null, 2));\n    console.info('Info: Processing event');\n    console.debug('Debug: Event details', event);\n    console.warn('Warning: This is a sample warning message');\n    console.error('Error: This is a sample error message');\n    console.assert(false, 'Assert: This is a sample assert message');\n\n    return 'Hello World!';\n};\n</code></pre></div><p>Using CDK, I set the System Log Level to WARN and the Application Log Level to ERROR:</p><div><pre><code>new NodejsFunction(this, 'AdvancedLoggingControlFunction', {\n  functionName: 'advanced-logging-control',\n  entry: 'src/hello-world/handler.ts',\n  handler: 'handler',\n  //set application log level to ERROR and system log level to WARN\n  applicationLogLevelV2: ApplicationLogLevel.ERROR,\n  systemLogLevelV2: SystemLogLevel.WARN,\n  //logging format must be set to JSON\n  loggingFormat: LoggingFormat.JSON,\n});\n</code></pre></div><blockquote><p>: When changing the System Log Level and Application Log Level, the logging format must be set to JSON (defaults to Text).</p></blockquote><p>This means that I am <strong>overriding CDK's default configuration</strong> for logging and reducing the messages sent to the CloudWatch log group. According to the explanation in the session above, now only console.error methods will show up in my CloudWatch log group.</p><p>If I execute this Lambda function, this is what I have in my CloudWatch Log Group:</p><p>Note that there are no System Log Messages (for instance, Lambda report) because I set the System Log Level to WARN, and only the console.error method affects the Log Group because the Application Log Level is  only.</p><p>By only logging what you need, you can significantly reduce the costs of the  metric for a high-traffic Lambda Function and stop  your log group.</p><p>However, there are going to be situations where you want your Lambda function to be more , to debug something, or to have extra information on your log group. Instead of always logging everything since the start, you might want to have the  possible and change it afterward, by updating your Logging Configuration to DEBUG () when needed:</p><div><pre><code>aws lambda update-function-configuration \\\n  --function-name advanced-logging-control \\\n  --logging-config LogFormat=JSON,ApplicationLogLevel=DEBUG\n</code></pre></div><blockquote><p>: By not specifying the System Log Level in the parameters, AWS will automatically set it to INFO.</p></blockquote><p>After the change, executing the Lambda function now gives us more messages in the log group:</p><p>As mentioned before, the DEBUG level will show messages from all console JavaScript methods.</p><p>Executing a CLI command every time you need to debug something is not very handy. More than that, you might want to execute this change in a more controlled and auditable way, especially when you do not have/must not have elevated privileges to do so.</p><p>To improve operational excellence, I have created a simple System Manager Automation Runbook, where this change can be executed in the AWS environment without the need to manually update the Lambda configuration using your IAM permissions. Instead, I have created an  by the SSM Automation Runbook. Plus, by doing that, the changes can be recorded by  for compliance reasons.</p><p>More importantly, you want to analyze the Lambda function  and reset it to the previous configuration to avoid extra costs with the  metric.</p><p>First of all, defining the IAM role to be assumed by the Automation:</p><div><pre><code>const automationIamRole = new Role(this, 'AutomationIamRole', {\n  assumedBy: new ServicePrincipal('ssm.amazonaws.com'),\n});\n\nautomationIamRole.addToPolicy(\n  new PolicyStatement({\n    actions: [\n      'lambda:GetFunctionConfiguration',\n      'lambda:UpdateFunctionConfiguration',\n    ],\n    resources: [`arn:aws:lambda:${Aws.REGION}:${Aws.ACCOUNT_ID}:function:*`],\n  })\n);\n</code></pre></div><blockquote><p>: Achieving the  can be a challenge because the Automation does not know upfront what Lambda function will be manipulated. Do you have an idea of how can this be solved? Let me know in the comments!</p></blockquote><p>The Automation Runbook definition in CDK looks like this:</p><div><pre><code>new CfnDocument(this, 'ModifyLambdaLogLevelDocument', {\n  documentType: \"Automation\",\n  name: 'ModifyLambdaLogLevelDocument',\n  documentFormat: \"YAML\",\n  updateMethod: \"NewVersion\",\n  content: {\n    schemaVersion: \"0.3\",\n    description: \"Modify the log level of a Lambda function temporarily. After 10 minutes, the log level will be reset to the original value.\",\n    assumeRole: automationIamRole.roleArn,\n    parameters: {\n      FunctionName: {\n        type: \"String\",\n        description: \"The name of the Lambda Function\",\n      },\n      LogLevel: {\n        type: \"String\",\n        description: \"The log level to set\",\n        allowedValues: [\n          \"DEBUG\",\n          \"INFO\",\n          \"WARN\",\n        ],\n      },\n      Reason: {\n        type: \"String\",\n        description: \"The reason for the change\",\n      },\n    },\n    mainSteps: [\n      {\n        name: \"GetCurrentLoggingConfig\",\n        action: \"aws:executeAwsApi\",\n        inputs: {\n          Service: \"Lambda\",\n          Api: \"getFunctionConfiguration\",\n          FunctionName: \"{{FunctionName}}\",\n        },\n        outputs: [\n          {\n            Name: \"CurrentLoggingConfig\",\n            Selector: \"$.LoggingConfig\",\n            Type: \"StringMap\",\n          },\n        ],\n      },\n      {\n        name: \"ModifyLogLevel\",\n        action: \"aws:executeAwsApi\",\n        inputs: {\n          Service: \"Lambda\",\n          Api: \"updateFunctionConfiguration\",\n          FunctionName: \"{{FunctionName}}\",\n          Description: \"Update log level to {{LogLevel}}\",\n          LoggingConfig: {\n            ApplicationLogLevel: \"{{LogLevel}}\",\n            LogFormat: \"JSON\",\n            SystemLogLevel: \"{{LogLevel}}\",\n          },\n        },\n      },\n      {\n        name: \"Wait10Minutes\",\n        action: \"aws:sleep\",\n        inputs: {\n          Duration: \"PT10M\",\n        },\n      },\n      {\n        name: \"ResetLogLevel\",\n        action: \"aws:executeAwsApi\",\n        inputs: {\n          Service: \"Lambda\",\n          Api: \"updateFunctionConfiguration\",\n          FunctionName: \"{{FunctionName}}\",\n          Description: \"Reset log level to original value\",\n          LoggingConfig: \"{{GetCurrentLoggingConfig.CurrentLoggingConfig}}\",\n        },\n      }\n    ]\n  },\n});\n</code></pre></div><p>This document executes the following steps:</p><ol><li>Get the <strong>current logging configuration</strong> for the provided Lambda function, saving it in a variable. This value will be used in the last step to reset the logging configuration to its original value.</li><li>Executes the API call to update the Lambda logging configuration with the . It accepts DEBUG, INFO, and WARN, which are more important levels to change on the fly.</li><li>Sleep for <strong>10 minutes before changing it back</strong>. If 10 minutes is not enough time to debug, consider increasing the time or receiving the duration as a parameter in the document.</li><li>Reset the log level to its .</li></ol><blockquote><p>: It is also possible to add an <a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-action-approve.html\" rel=\"noopener noreferrer\">approval step</a> to be approved by one of your colleagues, although I have not included that in this example. </p><p>The full example, including the Lambda function and the SSM Automation Runbook can be found <a href=\"https://github.com/matheusdasmerces/lambda-advanced-logging-control\" rel=\"noopener noreferrer\">here</a>.</p></blockquote><h2>\n  \n  \n  Monitoring CloudWatch costs\n</h2><p>Using the **Cost Explorer **console can give you an overview of the  and  metrics and how their costs increase/decrease over time. </p><p>You can select, under the \"Usage Type\" filter both of the metrics:</p><p>Managing CloudWatch costs efficiently is crucial, especially for high-traffic Lambda functions. By implementing Advanced Logging Controls, you can significantly reduce unnecessary log ingestion and storage costs while maintaining compliance.</p><ul><li>Monitor metrics like DataProcessing-Bytes and TimedStoraged-BytesHrs to track expenses.</li><li>Ensure logs are only stored for the necessary period to avoid excessive storage fees.</li><li>Leverage Lambda’s built-in log levels to filter out unnecessary logs and avoid polluting CloudWatch.</li><li>Utilize AWS Systems Manager Automation Runbooks to temporarily adjust log levels when debugging, without requiring constant manual intervention.</li><li>Use Cost Explorer to track trends and make informed decisions on further optimizations.</li></ul><p>By adopting these practices, you can strike the right balance between compliance and cost efficiency, ensuring that your CloudWatch bills remain manageable while still providing the insights your applications need.</p>","contentLength":18002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Revolutionize AI with AttentionSmithy: Modular Transformer Customization Unleashed!\"","url":"https://dev.to/gilles_hamelink_ea9ff7d93/revolutionize-ai-with-attentionsmithy-modular-transformer-customization-unleashed-2og5","date":1739751049,"author":"Gilles Hamelink","guid":852,"unread":true,"content":"<p>In the rapidly evolving landscape of artificial intelligence, staying ahead of the curve can feel like an insurmountable challenge. Are you grappling with rigid AI models that stifle creativity and innovation? Do you yearn for a way to tailor your AI solutions to meet specific needs without getting lost in technical jargon? Enter AttentionSmithy—a groundbreaking tool designed to revolutionize how we approach modular transformers in AI customization. Imagine wielding the power to sculpt your own unique transformer architecture, fine-tuning it effortlessly to fit your project’s demands while unleashing unprecedented levels of efficiency and performance. In this blog post, we will delve into what makes AttentionSmithy a game changer in the world of AI, explore its key features that promise to enhance your development experience, and guide you through real-world applications transforming industries today. Whether you're an experienced developer or just starting out on your AI journey, you'll discover invaluable insights that empower you not only to adapt but also thrive in this dynamic field. Join us as we embark on this exciting exploration—your gateway to mastering modular transformer customization awaits!</p><p>AttentionSmithy represents a significant advancement in the field of artificial intelligence, particularly for those working with transformer architectures. This modular software package simplifies the customization process by breaking down complex components such as attention modules, feed-forward networks, normalization layers, and positional encodings into reusable building blocks. Users can prototype and adapt various transformer models without needing extensive coding expertise. The ability to experiment with different positional encoding strategies—like sinusoidal or learned embeddings—enhances flexibility and facilitates meaningful comparisons between methods.</p><h2>Core Advantages of AttentionSmithy</h2><p>One of the standout features of AttentionSmithy is its capacity to enable specialized experiments across diverse application domains while minimizing labor-intensive framework manipulation. Researchers benefit from enhanced model performance through straightforward experimentation with architectural variations while maintaining code readability. Validation studies confirm that this tool significantly boosts adaptability in fields like natural language processing and bioinformatics. Furthermore, it supports extendable attention mechanisms such as Longformer and Big Bird, addressing limitations inherent in traditional approaches and paving the way for future advancements within transformer-based models.</p><p>Modular transformers represent a significant advancement in the customization and flexibility of AI models. AttentionSmithy, a pioneering software package, allows users to easily modify transformer architectures by breaking down complex components into reusable building blocks. This modularity enables researchers to experiment with various attention mechanisms, feed-forward networks, normalization layers, and positional encodings without requiring extensive coding expertise. By facilitating independent activation or deactivation of different encoding strategies—such as sinusoidal or learned embeddings—AttentionSmithy enhances model adaptability across diverse applications like natural language processing and bioinformatics.</p><h2>Advantages of Modular Design</h2><p>The core advantage lies in its ability to streamline experimentation while preserving code readability. Researchers can explore architectural variations that align closely with their specific needs without getting bogged down by labor-intensive framework adjustments. Validation studies have shown that using AttentionSmithy significantly improves model performance through tailored configurations suited for specialized tasks. Furthermore, ongoing development aims to incorporate emerging transformer variants while ensuring ease of use remains paramount—a crucial factor as the field continues evolving rapidly towards more sophisticated AI solutions.</p><p>AttentionSmithy stands out due to its modular architecture, which allows users to customize transformer models effortlessly. Its key components—attention modules, feed-forward networks, normalization layers, and positional encodings—are designed as reusable building blocks. This structure simplifies the prototyping process for various AI applications without requiring extensive coding expertise.</p><h2>Customization and Flexibility</h2><p>One of the most significant features is the ability to experiment with different positional encoding strategies like sinusoidal or learned embeddings independently. Users can activate or deactivate these methods for comparative analysis, enhancing their understanding of model behavior in diverse contexts. Furthermore, AttentionSmithy's validation studies confirm its effectiveness in improving performance across domains such as natural language processing and bioinformatics.</p><p>The software's design anticipates future developments by supporting emerging transformer variants while maintaining clarity and usability. Researchers are encouraged to contribute new positional encoding strategies and explore specialized applications within this framework. Additionally, extendable attention mechanisms like Longformer enhance efficiency by addressing limitations inherent in traditional methods. Overall, AttentionSmithy empowers researchers with a robust platform for innovation in AI technologies.</p><p>To begin using AttentionSmithy, first download the software package from its official repository. Installation is straightforward; simply follow the provided instructions for your operating system. Once installed, familiarize yourself with the modular components such as attention modules and feed-forward networks. The intuitive interface allows users to customize transformer architectures without extensive coding knowledge.</p><h3>Experimenting with Positional Encodings</h3><p>Start by exploring different positional encoding strategies like sinusoidal or learned embeddings. You can activate or deactivate these options independently to observe their impact on model performance. This flexibility enables you to prototype various configurations tailored to specific AI applications seamlessly.</p><h3>Leveraging Documentation and Community Support</h3><p>Utilize the comprehensive documentation available within AttentionSmithy for guidance on implementation and best practices. Engage with the community through forums or social media platforms where experienced users share insights and troubleshooting tips, enhancing your learning experience while contributing back by sharing your findings in return.# Real-World Applications: Transforming Industries with AI</p><p>AttentionSmithy is revolutionizing various industries by simplifying the customization of transformer architectures, which are pivotal in fields like natural language processing (NLP) and bioinformatics. Its modular design allows users to experiment with different attention mechanisms and positional encoding strategies, enabling tailored solutions for specific applications without extensive coding expertise. For instance, businesses can leverage AttentionSmithy's capabilities to enhance customer service chatbots or optimize data analysis processes in healthcare.</p><h2>Industry-Specific Use Cases</h2><p>In finance, AttentionSmithy can be utilized for sentiment analysis on market trends through NLP models that interpret news articles and social media posts. In healthcare, it aids in predictive analytics by analyzing patient data patterns more effectively. The adaptability of this software not only boosts model performance but also accelerates research cycles across diverse sectors—facilitating rapid prototyping and deployment of AI-driven solutions that meet unique industry demands while maintaining high standards of accuracy and efficiency.# Join the Revolution: Community and Support for Users</p><p>AttentionSmithy fosters a vibrant community that encourages collaboration among users, researchers, and developers. This modular software package not only simplifies transformer architecture customization but also provides extensive support resources. Users can access forums, documentation, and tutorials to enhance their understanding of various components like attention modules and positional encodings. By actively participating in discussions or contributing code enhancements, individuals can share insights that drive innovation within the platform.</p><h2>Engaging with the Community</h2><p>The community aspect is vital for continuous improvement; it allows users to report issues, suggest features, or seek assistance on specific challenges they encounter while using AttentionSmithy. The collaborative environment ensures that best practices are shared widely—facilitating knowledge transfer across different application domains such as natural language processing and bioinformatics. Additionally, regular webinars and workshops hosted by experienced practitioners provide opportunities for skill development and networking within this growing ecosystem.</p><p>By joining this revolution of user-centric support systems in AI development through AttentionSmithy, you position yourself at the forefront of technological advancements while gaining invaluable expertise from fellow enthusiasts dedicated to pushing boundaries in research and application.</p><p>In conclusion, AttentionSmithy stands at the forefront of AI innovation by offering a groundbreaking approach to modular transformer customization. This tool empowers users to tailor AI models specifically to their needs, fostering creativity and efficiency in various applications across industries. The key features of AttentionSmithy not only enhance user experience but also streamline workflows, making it accessible for both seasoned developers and newcomers alike. As we delve into real-world applications, it's evident that this technology has the potential to revolutionize sectors such as healthcare, finance, and education by enabling more precise data analysis and decision-making processes. By joining the vibrant community surrounding AttentionSmithy, users can access invaluable support while contributing to an evolving landscape of artificial intelligence. Embracing this revolutionary tool is essential for anyone looking to harness the full power of AI today and in the future.</p><h3>1. What is AttentionSmithy and how does it change the landscape of AI?</h3><p>AttentionSmithy is an innovative platform designed for modular transformer customization in artificial intelligence. It allows users to tailor transformer models according to specific needs, enhancing flexibility and performance in various applications. This capability represents a significant advancement in AI development, enabling more personalized and efficient solutions.</p><h3>2. What are modular transformers, and why are they important for customization?</h3><p>Modular transformers are advanced neural network architectures that can be easily modified or extended with different components or modules. They allow developers to customize their models by integrating specialized functionalities without starting from scratch each time. This adaptability makes them crucial for addressing diverse challenges across industries.</p><h3>3. What key features should I know about AttentionSmithy?</h3><p>AttentionSmithy boasts several standout features including:\n- : Simplifies the process of customizing transformer models.\n- <strong>Extensive Library of Modules</strong>: Offers a wide range of pre-built modules that can be integrated into existing models.\n- : Provides access to forums and resources where users can share insights and get assistance.\nThese features make it easier for both beginners and experienced developers to leverage its capabilities effectively.</p><h3>4. How can I start using AttentionSmithy today?</h3><p>To get started with AttentionSmithy, visit the official website where you can find documentation, tutorials, and download options for the software. Registration may be required to access certain features or community support channels which will help guide you through your initial setup.</p><h3>5. In what real-world applications is AttentionSmithy being utilized?</h3><p>AttentionSmithy's customizable transformers have been applied across various sectors such as healthcare (for predictive analytics), finance (for fraud detection), natural language processing (for chatbots), and many others. Its versatility enables businesses to enhance their operations by implementing tailored AI solutions that meet specific industry demands efficiently.</p>","contentLength":12546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3 layers of nextjs prefetch","url":"https://dev.to/filip_kudla/3-layers-of-nextjs-prefetch-38lm","date":1739750629,"author":"Filip Kudla","guid":851,"unread":true,"content":"<p>This post is sponsored by next.js... but in a bit different way. After spending a few hours connecting the right dots right after shipping the middleware feature on production and quick rollback 😬</p><p>Don't know about most of the devs but for me, it was a \"shocker\". Our service throughput skyrocketed right after my initial merge. It was about a 2x or 3x increase. Simply caused by the middleware file.</p><p>There is a gotcha that I want to mention at the very beginning of this article. Such \"issues\" are available on <strong>production app build only</strong>. I mean link prefetch \"feature\" in the . It's a bit cumbersome and good to know ahead.</p><blockquote><p>just to visualize this for you - you don't need to thank</p></blockquote><p>I will call it a beginner-level prefetch.</p><p>You can create a simple wrapper around the  component, such as a  and opt out of this behaviour. The whole logic is to set  as the default value in your component and then replace all usage of default .</p><div><pre><code>import Link from 'next/link'\n\nexport const CustomLink = ({prefetch = false, ...props}) =&gt; {\n  return (\n    &lt;Link\n      prefetch={prefetch}\n      {...props}\n    /&gt;\n  )\n}\n</code></pre></div><p>That's not all! A different behaviour depends on whether you use the  or  directory.</p><p><a href=\"https://nextjs.org/docs/pages/api-reference/components/link#prefetch\" rel=\"noopener noreferrer\">For  routing</a> has a  value by default in next v15. Setting to false will work fine but there is one more disclaimer. Prefetch behaviour on hover can't be disabled with .</p><blockquote><p>Note: Of course, you can use the native  tag to disable prefetching completely but this will work only for external links.</p></blockquote><blockquote><p>Again, this might be outdated pretty fast so make sure to check the official docs</p></blockquote><p>I have found another place where you can minimize the middleware prefetch requests to the server. In middleware configuration itself!</p><p>There is a great <a href=\"https://nextjs.org/docs/pages/building-your-application/routing/middleware#matcher\" rel=\"noopener noreferrer\">matcher</a> feature where we can exclude requests with prefetch headers. Let's consider the code snippet below:</p><div><pre><code></code></pre></div><p>Explanation step-by-step:</p><ol><li>Note that Regexp is a negation</li><li>It will work for not listed potential routes such as our pages</li><li>Normally it would match both prefetch and non-prefetch requests but we have added a requirement to match only those without a \"prefetch\" header.</li></ol><p>This would work just fine, except for one more case - prefetch on hover. If you are fine with it you can probably pause here.</p><h2>\n  \n  \n  Ultimate layer 3 - experimental  flag\n</h2><p>You might wonder - why it's not just optional to prefetch, right? Well, you can but we need a closer look into the source code.</p><p>Middleware comes with 2 prefetch options -  and . The second one is set by default and it \"always prefetches\". While the first one prefetches only \"when the href explicitly matches an SSG route\" which should be fine to prefetch the static generated sites.</p><blockquote><p>Note: You have completely disabled prefetches if you are still using .</p><p>Note: If you have just a few static sites, you probably don't need to configure 2nd option to exclude prefetches. It shouldn't generate a big load on your servers.</p></blockquote><h2>\n  \n  \n  Why does it generate all the load on our server\n</h2><p>The answer is simple, with the  file we were running additional operations for ALL our requests like images, prefetches and pages which are treated as additional load. This takes time and resources so our application will scale.</p><p>I couldn't understand why prefetch is not happening when you are not using nextjs middleware file. Without it, it's completely normal behaviour. Looks like  is directly connected with this feature and mentioned load from the previous point.</p><p>There it is. 3 layers to avoid prefetch while using middleware. I can say that you might live with the 2nd option without any issues but it's also good to know how to completely shut down \"prefetch\".</p><p>Also, I can see the 3rd layer as part of the docs. I need to think about some PR for that 👀 </p>","contentLength":3671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Daily JavaScript Challenge #JS-104: Compute Spiral Order Traversal of a Matrix","url":"https://dev.to/dpc/daily-javascript-challenge-js-104-compute-spiral-order-traversal-of-a-matrix-amb","date":1739750440,"author":"DPC","guid":850,"unread":true,"content":"<p>Hey fellow developers! 👋 Welcome to today's JavaScript coding challenge. Let's keep those programming skills sharp! </p><p>: Medium: Matrix Traversal</p><p>Given a 2D matrix, write a function that returns all elements of the matrix in spiral order (clockwise direction).</p><ol><li>Test it against the provided test cases</li><li>Share your approach in the comments below!</li></ol><ul><li>How did you approach this problem?</li><li>Did you find any interesting edge cases?</li><li>What was your biggest learning from this challenge?</li></ul><p>Let's learn together! Drop your thoughts and questions in the comments below. 👇</p><p><em>This is part of our Daily JavaScript Challenge series. Follow me for daily programming challenges and let's grow together! 🚀</em></p>","contentLength":674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spring's new Support Policy Updates","url":"https://dev.to/michelle_sebek_/springs-new-support-policy-updates-203","date":1739750001,"author":"michelle sebek","guid":841,"unread":true,"content":"<h2>\n  \n  \n  Understanding the Latest Spring Support Policy Updates\n</h2><p>As developers, one of the key aspects of maintaining long-term, stable applications is understanding the support policies of the frameworks and tools we use. For Spring developers, this is especially crucial, given the framework’s widespread adoption in enterprise environments. To help ensure smooth project maintenance, the Spring team has recently announced updates to its Support Policy, providing a clearer roadmap for developers regarding the lifecycle and support of Spring releases.</p><p>In this article, we’ll break down what these updates mean, why they’re important, and how they’ll impact your Spring projects.</p><h2><strong>What’s New in the Spring Support Policy?</strong></h2><p>The Spring Support Policy Updates aim to provide more transparency and clarity around which versions of Spring are actively supported, when support ends, and what you can expect in terms of security fixes, bug fixes, and feature updates. Here are some of the key changes:</p><p><strong>1. Updated Long-Term Support (LTS) Guidelines</strong>\nThe new support policy places a greater emphasis on Long-Term Support (LTS) releases, ensuring that these versions receive extended maintenance and security updates for a longer period. LTS releases are particularly important for enterprise applications that require a stable foundation for many years, often with minimal changes.</p><p>With this update, the Spring team has clarified the expected LTS lifespan for each release and detailed how security patches and critical bug fixes will be handled throughout the support period.</p><p><strong>2. Streamlined Version Lifecycle</strong>\nTo avoid confusion around support timelines, the Spring team has made the version lifecycle more predictable. From now on, major versions of Spring will have clearly defined end-of-life dates, and developers will know exactly when a version will transition from active support to end-of-life (EOL).</p><p>This change ensures that teams can plan upgrades and migrations well in advance, preventing any unexpected disruptions due to versions falling out of support.</p><p><strong>3. Improved Security &amp; Maintenance Updates</strong>\nAs security threats evolve and new vulnerabilities are discovered, keeping frameworks up-to-date is crucial. The new support policy outlines the process for delivering timely security updates and maintenance fixes. It also clarifies the level of support available for both major and minor releases.</p><p>For example, minor releases will continue to receive security patches for an extended period, while major versions will only receive support during their active maintenance window. This change allows Spring developers to plan more effectively for long-term security without scrambling for fixes on deprecated versions.</p><p><strong>4. Clearer Communication of Version Support</strong>\nOne of the standout updates is the improved communication around which Spring versions are currently supported, which are transitioning out of support, and which have reached end-of-life (EOL). With regular updates to the support policy, developers will be able to stay on top of important lifecycle milestones and avoid common pitfalls that come from using unsupported versions.</p><h2>\n  \n  \n  Why Are These Updates Important?\n</h2><p>For enterprise developers and teams managing mission-critical applications, understanding the support policy is crucial to ensuring application stability and security over time. Here are a few reasons why these updates matter:</p><p><strong>1. Better Planning for Upgrades</strong>\nWith clearer support timelines, development teams can now plan their upgrade paths with greater confidence. No longer will you have to guess when a Spring version will no longer be supported. You’ll have a clear roadmap of when to start considering an upgrade to newer versions and which releases are safe to stay on for the long term.</p><p>\nSecurity is always top of mind for developers, especially when working on applications that handle sensitive data. The updated policy ensures that developers have the necessary resources to stay on top of security vulnerabilities and patching requirements. It provides a predictable schedule for when security updates will be delivered, which helps prevent potential security breaches due to unsupported software.</p><p><strong>3. Easier Project Maintenance</strong>\nFor teams with complex, long-lived applications, maintaining a consistent environment over time can be challenging. With the new policy in place, you’ll have better clarity about which versions are actively supported, allowing you to make informed decisions about managing project dependencies and planning for long-term maintenance.</p><h2><strong>What Does This Mean for You?</strong></h2><p>If you’re already using Spring in your project, these support policy updates will help you make more informed decisions about which versions to use and when to plan your upgrades. If you’re still on an older version, now is a great time to review your support options and ensure your Spring-based project is running on a supported version.</p><p><strong>Here are a few actions you can take right now:</strong></p><ul><li><p>Review your current Spring version and check its support status using the updated policy guidelines.</p></li><li><p>Plan your upgrade strategy based on the new support timelines.</p></li><li><p>Consider moving to an <a href=\"https://www.vmware.com/products/app-platform/tanzu-spring\" rel=\"noopener noreferrer\">LTS version</a> if you’re on a non-LTS release and need long-term support.</p></li><li><p>Stay informed about security and maintenance updates through official Spring channels.</p></li></ul><p>The updated Spring Support Policy is designed to make the Spring framework even more reliable and predictable for developers building enterprise-grade applications. With clearer timelines, enhanced security updates, and more transparent communication, developers can focus more on building innovative features and less on worrying about version lifecycles.</p><p>By staying informed and aligning your projects with the latest support policies, you’ll enable that your applications are always running on a secure and stable foundation.</p>","contentLength":5846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human Regex Gets TypeScript's Magic Typing 🚀","url":"https://dev.to/rajibola/human-regex-gets-typescripts-magic-typing-i7n","date":1739749350,"author":"Ridwan Ajibola","guid":840,"unread":true,"content":"<p>Two weeks ago, I released the first public version of <a href=\"https://www.npmjs.com/package/human-regex\" rel=\"noopener noreferrer\">Human Regex</a>, and I’m thrilled to share that it now leverages TypeScript’s magic typing to take regex building to the next level! This update not only brings a host of bug fixes but also introduces a smarter, safer API for crafting regular expressions. Thanks to two new contributors—<a href=\"https://github.com/busyboxkitty\" rel=\"noopener noreferrer\">one</a> of whom contributed the powerful magic typing—<a href=\"https://github.com/rajibola/human-regex\" rel=\"noopener noreferrer\">Human Regex</a> has already gained 900+ downloads.</p><h2>\n  \n  \n  Why is Human Regex better than traditional regex in JS/TS?\n</h2><ul><li>English-like Syntax: Build regex patterns in a readable, chainable syntax that feels like natural language.</li><li>Magic Typing: Enforces valid regex construction at compile time, catching mistakes early and making your code safer.</li></ul><ul><li>📘 Explore the latest tutorial: Dive deeper into using Human Regex in your JS/TS projects by completing this blog post.</li></ul><p>\nStart by installing Human Regex in your project:</p><p>Then import the library in your TypeScript code:</p><div><pre><code></code></pre></div><p>\n  Match the exact word \"hello\" without worrying about special character escaping.</p><div><pre><code></code></pre></div><p>\n   Create a regex that matches only digits.</p><div><pre><code></code></pre></div><p>\n   Match one of multiple options using the  method.</p><div><pre><code></code></pre></div><p>\n   Check if a string contains a whitespace character.</p><div><pre><code></code></pre></div><p><strong>Escaping Special Characters:</strong>\n   The  method automatically escapes special characters for you.</p><div><pre><code></code></pre></div><p>\n   Build a regex to check for a valid HTTP/HTTPS protocol.</p><div><pre><code></code></pre></div><p>\n   Extract a substring using a named capture group.</p><div><pre><code></code></pre></div><p><strong>Matching Exactly Three Digits:</strong>\n   Ensure a string contains exactly three digits.</p><div><pre><code></code></pre></div><p>Human Regex includes smart typing and runtime checks to help you catch mistakes early. Here are two examples that intentionally trigger errors:</p><p>: <strong>Using .lazy() on an Empty Pattern</strong></p><div><pre><code></code></pre></div><p>: <strong>Invalid Unicode Letter Variant</strong></p><div><pre><code></code></pre></div><p>Human Regex transforms the complexity of traditional regex into a series of intuitive method calls. Whether you’re matching a simple literal, testing digits, or combining multiple methods for complex patterns, the library offers a clear and maintainable approach. Its robust type system and runtime checks help prevent common mistakes, making your code safer and easier to debug.</p><p>Have you explored <a href=\"https://www.npmjs.com/package/human-regex\" rel=\"noopener noreferrer\">Human Regex</a> in your projects? I’d love to hear your experiences or see your examples in the comments below!</p><p>You can also contribute by:</p><p>⭐ Starring the <a href=\"https://github.com/rajibola/human-regex\" rel=\"noopener noreferrer\">GitHub repo</a> to help others discover it\n🐞 Reporting bugs or requesting features<p>\n💡 Contributing code or documentation</p></p>","contentLength":2349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Overview Description","url":"https://dev.to/reii0x0/overview-description-3fh7","date":1739749256,"author":"Hoang Tran","guid":839,"unread":true,"content":"<p>Here you can find the overview of our system, the <strong><em>Coffee Shop Management Application</em></strong>. It is a comprehensive solution designed to streamline coffee shop operations by efficiently managing orders, inventory, and financial performance. It provides an intuitive interfaces for baristas, managers, and business owners to track key aspects of their shop's workflow.</p><p>The system our group aiming to create will be a business-oriented system, therefore its target users will be primary business owners, companies,... but it can also be used by people who want an application to help them with time management, or resources management.</p><ul><li><ul><li>Coffee shop owners, managers, and staff often struggle with managing multiple aspects of their business, including tracking orders, maintaining ingredient inventory, and monitoring financial performance. Manual record-keeping or outdated systems can lead to errors, inefficiencies, and lost revenue. This application aims to simplify these processes by providing a centralized, automated solution.</li></ul></li><li><ul><li>Users interact with the system daily to place and track customer orders, check ingredient stock, and review sales data. Baristas and cashiers require a fast and intuitive interface for quick order processing, while managers expect detailed reports and notifications for stock levels and profits. Users value a seamless, responsive, and user-friendly experience that integrates smoothly into their workflow.</li></ul></li><li><ul><li>The primary goal of using this application is to improve workflow efficiency, reduce operational errors, and optimize financial performance. For baristas, it ensures smoother order handling and ingredient tracking, while managers benefit from real-time insights and better decision-making. People that don't want anything with businesses can also use this application for enhancing personal experience, like better time management. Ultimately, the system helps coffee shop owners run their businesses more efficiently, improving customer satisfaction and profitability.</li></ul></li></ul><p>We are aiming to create an application for users that help them efficiently and intelligently managing their coffee business, which involves in a lot of processes that can easily be neglected or mistaken, therefore deal a enormous amount of harm to the business.\nWe will divide the functions of the system into two divisions,  and .</p><ul><li><ul><li>Customer Order Tracking: The system streamlines order management by allowing barista to process and track orders in real time. Orders can be modified or updated easily, ensuring accuracy and efficiency. This reduces wait times and improves customer satisfaction.</li><li>Ingredients Tracking: Keep track of ingredient stock levels to prevent shortages and avoid over-purchasing. The system notifies staff when supplies are running low, ensuring timely restocking. This helps reduce waste, optimize inventory management, and maintain smooth operations.</li><li>Profit Tracking: Monitor revenue, expenses, and net profit (the amount of money that a company has after all its expenses are paid.) through detailed financial insights. The system provides reports on daily  sales, high-demand products, and cost analysis. This allows business owners to make informed decisions and improve profitability.</li></ul></li><li><ul><li>Employee Management: Allows managers to assign roles, track work shifts, and monitor employee performance. This helps streamline scheduling and ensures efficient workflow in the coffee shop.</li><li>Sales Reports and Analytics: Provides insights into best-selling items, peak business hours, and customer trends. These reports help owners make data-driven decisions to improve sales and marketing strategies.</li><li>Customer Loyalty Program: Encourages repeat business by offering rewards, discounts, or points for frequent purchases. This helps build customer relationships and boost long-term revenue.</li><li>Table Management: Tracks table availability, reservations, and waiting list for dine-in customers. This improves seating efficiency and enhances the overall customer experience. </li><li>Order Customization: Allows customers to personalize their drinks by selecting options like extra shots, milk alternatives, or sugar levels. This ensures flexibility and better customer satisfaction.</li><li>Expense Tracking: Help business owners monitor operational costs, including rent, wages, and utilities. Keeping track of expenses allows for better budgeting and cost optimization.</li><li>Supplier Management: Maintains supplier details, order history, and restocking schedules. This ensures smooth inventory management and avoid supply chain disruptions.</li><li>Mobile and Tablet Compatibility: Ensures the system runs smoothly on different devices, allowing employees and managers to access features easily from anywhere in the shop.</li></ul></li></ul><h2>\n  \n  \n  5.Assumptions and Dependencies\n</h2>","contentLength":4728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 [Free Follow Opertunity]: Who can create the biggest loop? 🔁","url":"https://dev.to/mileswk/free-follow-opertunity-who-can-create-the-biggest-loop-3b2i","date":1739748624,"author":"MilesWK","guid":838,"unread":true,"content":"<p>There are many ways to create loops. There is the boring  way:</p><p>But that is boring, as mentioned above. I want to see if you can create a better to make something RUN FOREVER. </p><p><strong>🥳 Free Follow Opertunity:</strong>\nThere is a really hard challenge for a free follow! The first person to figure out a way to create a function that calls a function that calls the original function to create an endless loop, <strong>gets a shoutout, a lot of posts liked, and a follow!</strong></p>","contentLength":447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Involving the Right People in an Incident: All vs. Correct","url":"https://dev.to/miry/involving-the-right-people-in-an-incident-all-vs-correct-1p2a","date":1739748424,"author":"Michael Nikitochkin","guid":837,"unread":true,"content":"<h3>\n  \n  \n  Involving the Right People in an Incident: All vs. Correct\n</h3><p>It's been a while since I last wrote about incidents. Lately, I’ve been more focused on backend development in a Ruby and Crystal projects, but after handling a few recent incidents, I wanted to jot down my thoughts.</p><h3>\n  \n  \n  The Problem: Over-Involving Teams During an Incident\n</h3><p>It’s common for an Incident Commander to be paged when something isn’t working. As the Incident Commander, you might see reports from customers. Your responsibility is to identify contributing factors and bring the right people together to stop the bleeding.</p><p>However, the concept of bringing the \"correct\" people is sometimes misunderstood. Some Incident Commanders assume this means inviting  who might be remotely involved. They create massive video or audio calls, hoping someone will figure out the problem. While this might seem like a thorough approach, it often leads to frustration among teams who are pulled into the incident but have nothing to contribute. They end up waiting passively, leading to wasted time and effort.</p><p>This broad approach may give Incident Commanders a false sense of control—believing that if all teams are present, they’ve done everything possible. But in reality, each team may assume the issue lies elsewhere, leading to passive listening rather than active problem-solving.</p><h3>\n  \n  \n  The Consequences of Over-Involvement\n</h3><p>Bringing too many people into an incident can have several negative effects:</p><ul><li><strong>High-cost meetings with low productivity:</strong> More people in the call means more noise, more conflicting theories, and a harder time reaching a consensus.</li><li><strong>Blame-shifting and distraction:</strong> Each team might focus on their own long-standing issues rather than identifying the real root cause.</li><li><strong>Loss of the bigger picture:</strong> With too many perspectives, the core problem can become obscured, making it harder to pinpoint the actual failure.</li></ul><p>In complex systems, problems can be hidden under layers of dependencies, making a broad approach ineffective. It’s crucial to separate valid long-term concerns from immediate incident causes.</p><h3>\n  \n  \n  How to Solve an Incident Without Over-Involving Teams\n</h3><p>So, how can an Incident Commander solve an unknown issue efficiently without involving too many people, while still resolving the problem as quickly as possible?</p><ol><li> using all available tools. Start by narrowing down the issue to its closest impact point—typically where users are directly affected. Investigate progressively deeper into microservices and vendor solutions.</li><li> by building a timeline and reproducing the problem as closely as possible to the reported issue. This is often the hardest step, especially if the issue is intermittent or device-specific.</li><li><strong>Leverage observability tools</strong> across mobile, backend services, and database profiling. These tools should be a core part of every playbook.</li><li><strong>Identify the success lines</strong> in monitoring reports to determine possible mitigation steps.</li><li><strong>Engage teams incrementally</strong> — bring in only the necessary teams one at a time, verifying details with each and syncing on next steps before continuing the investigation. Even if details are shared in an incident channel, it's more effective to request targeted help in short bursts.</li><li><strong>Consult experts when needed</strong> — if someone has experience with a similar issue, involve them, but avoid defaulting to large group calls.</li><li><strong>Track multiple leads separately</strong> in different threads, summarizing findings regularly.</li><li><strong>Mitigation over resolution:</strong> Depending on the incident’s criticality, full resolution might not be immediate. Collaborate closely with 1-2 relevant teams to assess mitigation strategies before broadening involvement.</li><li><strong>Maintain focused escalation:</strong> Always escalate and page when necessary. Most people are willing to help, but ensure they have a clear role rather than keeping them in a call unnecessarily.</li></ol><h3>\n  \n  \n  Conclusion: All vs. Correct\n</h3><p>Should you bring  into an incident call? Or should you focus on identifying the  people? While including all teams might seem like a faster way to solve the problem, understanding the issue through observability tools and selectively involving the right teams is a more effective approach. This minimizes stress and improves resolution time.</p><p>Does this mean you should hesitate to escalate? Absolutely not—always escalate when necessary. People are generally willing to help, but ensure they have a clear role rather than keeping them in a call unnecessarily.</p><p>By shifting from an  approach to a ,  strategy, Incident Commanders can handle incidents more efficiently, reduce noise, and ensure faster recovery.</p><p>Of course, this isn’t something that can be perfected during an active incident. Understanding company structure, service dependencies, mitigation practices, and observability tools requires preparation. One of the best ways to improve is by reviewing past incidents and occasionally practicing simulated ones using exercises like .</p><p>And now, I trust you to make the right call!</p><p>Check out these resources to learn more:</p>","contentLength":5020,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I've never reposted but this is so useful I need to save it as more than just a bookmark.","url":"https://dev.to/samuel_fishback/ive-never-reposted-but-this-is-so-useful-i-need-to-save-it-as-more-than-just-a-bookmark-ifg","date":1739748288,"author":"Samuel Fishback","guid":836,"unread":true,"content":"<h2>10 Secret Git Commands That Will Save You 5+ Hours Every Week</h2>","contentLength":61,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flutter","url":"https://dev.to/oyoki_/flutter-4337","date":1739748276,"author":"Thomas MANIOC","guid":835,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/coolkid__96f5a08a6b113bf/-2p8i","date":1739747370,"author":"CoolKid !","guid":834,"unread":true,"content":"<h2>How We Accidentally Declared War on Our Own Database</h2>","contentLength":52,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pre-shaved Y-Axis: the case for preconfigured Rstudio workshop environments","url":"https://dev.to/lpmi13/pre-shaved-y-axis-the-case-for-preconfigured-rstudio-workshop-environments-1k9e","date":1739747292,"author":"Adam Leskis","guid":833,"unread":true,"content":"<p>You might think that not being an avid user of either Rstudio or the R programming language in general makes me unqualified to write about them, and you would be 100% correct; but it's my blog, so I'm gonna do it anyway!</p><p>Now that we're past my extremely obscure joke in the title related to data viz in R and yak shaving, I will tell you about how I had a lot of fun creating <a href=\"https://labs.iximiuz.com/playgrounds/custom-e21deb0c\" rel=\"noopener noreferrer\">a prepackaged playground</a> on <a href=\"https://labs.iximiuz.com\" rel=\"noopener noreferrer\">labs.iximiuz.com</a> to follow along with a <a href=\"https://www.cambridge.org/core/journals/journal-of-linguistic-geography/article/visualizing-map-data-for-linguistics-using-ggplot2-a-tutorial-with-examples-from-dialectology-and-typology/369F9643F85781AAAC0096D6BD146215\" rel=\"noopener noreferrer\">recent paper</a> presenting a tutorial for visualizing linguistics data using the R package ggplot2.</p><p>I'd like to briefly discuss  I did it, and also , in case you are interested in doing something similar.</p><p>While I may not be accustomed to using R for...well...anything, really...what I do have loads of experience in is listening to friends in the academic realm talk about how difficult it is to work with.</p><p>The dependency management is very difficult, it's nontrivial to upgrade Rstudio to a version that works with the packages that are needed for a given task, and even just setting up working environments on students' personal devices to take part in class work can be a multi-day job.</p><p>Imagine it didn't have to be this way...</p><p>Imagine that students in a workshop could open up any modern web browser, load the URL, and click one button to have a 100% deterministic working version of Rstudio loaded with all the packages needed for the activity...</p><p>Well, imagine no more (and you already knew this was possible if you read <a href=\"https://micromaterialsblog.wordpress.com/2025/01/13/turning-markdown-into-learning-publishing-a-challenge-on-labs-iximiuz-com/\" rel=\"noopener noreferrer\">my last post</a>), this is possible right now!</p><p>The very cool platform on <a href=\"https://labs.iximiuz.com\" rel=\"noopener noreferrer\">labs.iximiuz.com</a> has playgrounds that can be customized for this exact usecase and shared publicly. I know because I made one. Given the ability to run basically whatever you want in the ephemeral environments there, this is the perfect platform to create on-demand environments pre-configured with almost any software your heart desires.</p><p>To borrow a term from the testing and security domains, we can \"shift left\" and get the user to actually doing the thing they wanna do faster and more effectively. This is devops for education, and I'm here for it!</p><p>Because the Iximiuz platform is basically a free-form sandbox, the most difficult part was figuring out how to run the appropriate code in there. I knew it could be done, I just needed to connect all the fun lego blocks.</p><p>I started with the <a href=\"https://hub.docker.com/r/rocker/tidyverse\" rel=\"noopener noreferrer\">rocker/tidyverse</a> container image, pinned to base R version 4.4.1 (this was originally 4.4.2 since that's the latest, but I updated it after noticing the author specifying in GitHub that the base version of R they used was 4.4.1).</p><p>After firing that up locally with:</p><div><pre><code>docker run -it --rm -p 127.0.0.1:8787:8787 -e PASSWORD=rstudio rocker/tidyverse\n</code></pre></div><p>I had a running version of Rstudio at . Then it was a matter of just running the commands from the tutorial, seeing what broke and then fixing it by installing stuff via an iteratively growing Dockerfile.</p><p>Once that was all set, all I had to do was build the container locally and push to a public GitHub container registry, then add one line to customize a <a href=\"https://labs.iximiuz.com/playgrounds/docker\" rel=\"noopener noreferrer\">linux playground with Docker pre-installed</a> to run my container.</p><p>There's a super slick feature of the platform where you can basically create a new tab that accesses a port from the running container, and this can be configured to be available on startup.</p><p>So the entire flow for the user is</p><ol><li><p>Click \"Start\" for the playground</p></li><li><p>Wait for playground to be ready</p></li><li><p>Log in with the supplied username/password</p></li><li><p>Start running commands from the tutorial</p></li></ol><p>If you're interested in seeing the complete Dockerfile, that's <a href=\"https://github.com/danaroemling/mapping-for-linguists/blob/main/Dockerfile\" rel=\"noopener noreferrer\">here</a>.</p><p>This is a completely disposable learning environment where the users can have a running start without worrying about any annoying configuration challenges, and when they're done, they can just close the browser tab. They don't even need to know that Docker is a thing!</p><p>The yak is fully pre-shaved! The future is wow!</p>","contentLength":3833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Refactoring 023 - Replace Inheritance with Delegation","url":"https://dev.to/mcsee/refactoring-023-replace-inheritance-with-delegation-1c00","date":1739747070,"author":"Maxi Contieri","guid":832,"unread":true,"content":"<p><em>Transform your rigid inheritance into flexible delegations</em></p><blockquote><p>TL;DR: Replace restrictive inheritance hierarchies with flexible object delegation</p></blockquote><ul><li>Liskov substitution violation</li><li>Single Responsibility principle violation</li></ul><ol><li>Create a temporary field in the subclass for the superclass.</li><li>Update subclass methods to delegate calls.</li><li>Add delegation methods for inherited behavior.</li><li>Remove inheritance and update object creation.</li></ol><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>This refactoring is safe when done carefully and with proper testing.</p><p>You should ensure all delegated method signatures match exactly and maintain existing behavior.</p><p>The main risk comes from missing methods that need delegation or incorrectly implementing the delegation methods.</p><p>You gain the flexibility to change implementations at runtime and avoid the pitfalls of inheritance like tight coupling.</p><p>This refactoring improves the <a href=\"https://dev.to/mcsee/the-one-and-only-software-design-principle-3086\">Bijection</a> between code and reality by better modeling real-world relationships.</p><p>A robot doesn't inherit from a brain in the real world - it has a brain.</p><p>By replacing inheritance with delegation, you create a more accurate representation of the actual relationship between objects using the <a href=\"https://dev.to/mcsee/what-is-wrong-with-software-5pa\">MAPPER</a>.</p><p>The rewriting requires writing additional delegation methods.</p><p>If subclass logic relies too much on the superclass, delegation might increase boilerplate.</p><p>This article is part of the Refactoring Series.</p>","contentLength":1325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"12 Must-Have Next.js Libraries to Improve Your Workflow! 🚀","url":"https://dev.to/joodi/12-must-have-nextjs-libraries-to-boost-your-development-workflow-4p3d","date":1739746786,"author":"Joodi","guid":808,"unread":true,"content":"<p>👋 Hey developers! Today, I’m bringing you a curated list of 12 powerful Next.js libraries that will make your development process faster, smoother, and more efficient. Next.js is already an awesome framework for building scalable applications, but with these tools in your toolkit, you’ll boost your productivity and take your projects to the next level! 🏆</p><p>1️⃣ <strong>next-auth — Authentication Made Easy</strong> 🔐\nAuthentication can be a pain, but NextAuth.js makes it a breeze. Whether you're using Google, GitHub, Twitter, or a custom provider, NextAuth has you covered!<a href=\"https://next-auth.js.org/\" rel=\"noopener noreferrer\">https://next-auth.js.org/</a></p><ul><li>✅ Simplifies OAuth &amp; JWT-based authentication</li><li>📂 Supports session storage in databases</li><li>🔒 Built with security and scalability in mind</li></ul><p>2️⃣ <strong>@tanstack/react-query — Data Fetching Made Efficient</strong> ⚡\nEfficient data fetching and caching is a must, and React Query makes it effortless. It automatically caches, synchronizes, and updates your data in real-time! 🔄</p><ul><li>🛠️ Automatic caching &amp; background updates</li><li>🔄 Simplifies data synchronization</li><li>📜 Built-in pagination &amp; infinite scrolling</li></ul><p>3️⃣ <strong>next-seo — Boost Your SEO</strong> 📈\nIf you’re aiming for better visibility on search engines, Next SEO helps manage metadata, Open Graph, and structured data (JSON-LD) with ease.</p><ul><li>🏆 Improves SEO with structured metadata</li><li>🛣️ Seamless integration with dynamic routes</li><li>🐦 Supports Open Graph &amp; Twitter Cards</li></ul><p>4️⃣ <strong>next-sitemap — Auto-Generate Sitemaps</strong> 🗺️\nA well-structured sitemap is crucial for search engine indexing, and next-sitemap automates this process for all your routes!</p><ul><li>⚙️ Dynamic sitemap generation</li><li>🔍 Enhances search engine indexing</li><li>🌍 Supports multiple languages &amp; custom routes</li></ul><p>5️⃣ <strong>react-hook-form — Forms Made Simple</strong> 📋\nForms can be tricky and slow, but React Hook Form makes them lightweight, fast, and effortless to validate!</p><ul><li>⚡ Minimal re-renders &amp; high performance</li><li>🎨 Works with Material-UI, Tailwind CSS, and more</li><li>🚀 Built-in validation &amp; error handling</li></ul><p>6️⃣ <strong>next-i18next — Effortless Multilingual Support</strong> 🌍\nLooking to add multi-language support to your Next.js app? next-i18next is your go-to solution!</p><ul><li>📖 Easy localization with translations</li><li>🌏 Integrates seamlessly with i18next</li></ul><p>7️⃣ <strong>zod — Type-Safe Schema Validation</strong> 📜\nSchema validation is essential, and Zod guarantees your API routes and form data remain valid, with TypeScript-first support!</p><ul><li>⚙️ TypeScript-first approach</li><li>✅ Validates API inputs &amp; outputs</li><li>✨ Simple, expressive syntax</li></ul><p>8️⃣ <strong>next-translate — Lightweight i18n for Next.js</strong> 🌎\nIf you’re looking for an alternative i18n solution with a focus on performance, next-translate is a solid choice!</p><ul><li>📂 File-based translation management</li><li>🚀 Works on both server &amp; client</li></ul><p>9️⃣ <strong>next-pwa — Convert Your App into a PWA</strong> 📲\nWant to turn your Next.js app into a Progressive Web App (PWA)? next-pwa is the answer!</p><ul><li>💾 Enables caching &amp; offline support</li><li>🛠️ Supports service workers</li><li>🚀 Boosts performance &amp; engagement</li></ul><p>🔟 <strong>sharp &amp; next-optimized-images — Optimize Images for Performance</strong> 🖼️\nSharp and next-optimized-images ensure your images are compressed and optimized for better performance.</p><ul><li>⚡ Reduces bandwidth &amp; load time</li><li>🖼️ Supports WebP, AVIF, and more</li><li>🚀 Improves site speed &amp; SEO</li></ul><p>1️⃣1️⃣ <strong>Shadcn UI — UI Components Library</strong>\nA powerful, customizable UI library for Next.js, built on Radix UI. It offers a flexible set of components that integrate seamlessly with Next.js and Tailwind CSS. With full customization capabilities, developers can modify or extend components to match their design needs, making it a top choice for simplicity and adaptability.</p><p>1️⃣2️⃣ <strong>Zustand — Global State Management</strong>\nA lightweight state management library that simplifies handling global state in Next.js. It reduces boilerplate, supports asynchronous operations, and includes built-in state persistence. Zustand allows effortless state sharing across components without complex configurations or prop drilling.</p><p>🎯 Final Thoughts\nThe right tools can make your Next.js development way smoother. From authentication to SEO, performance, and internationalization, these libraries help you work smarter, not harder! 💡</p><p>🔥 Give them a try in your projects and level up your development!</p><p>I hope this was helpful, and I’d be happy to connect and follow each other!</p>","contentLength":4357,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Select Dropdown + Searchbar + Clearable (React & Shadcn)","url":"https://dev.to/vic_ong/select-dropdown-searchbar-clearable-react-shadcn-cl8","date":1739746051,"author":"Vic Ong","guid":807,"unread":true,"content":"<p>Shadcn provides a fantastic set of beautiful UI components right out of the box. One of the most commonly used components is a selector. However, the  component from shadcn (which is based on Radix UI) lacks certain features, such as search functionality and the ability to clear selected options.</p><p>In this guide, I'll be implementing a custom select dropdown component that supports searching and clearing options.</p><p>Let's start with a list of options:</p><div><pre><code></code></pre></div><p>First, I will create a basic dropdown using  and  that:</p><ul><li>Displays a list of options</li><li>Shows a checkmark for the selected option</li></ul><div><pre><code></code></pre></div><p>To enhance usability, I'll integrate  for built-in search capabilities and  to display a message when no results are found.</p><div><pre><code></code></pre></div><p>I also want to provide a button to clear the selected value when one is chosen.</p><div><pre><code></code></pre></div><p>So far so good, now I have a dropdown looking like so:</p><p>Now, for the last step, I can add a trigger to toggle open/close on the dropdown. I could just use a  for that.</p><div><pre><code></code></pre></div><p>With these enhancements, I've built a fully functional  component that supports searching and clearing selected options. I can now just use this component anywhere in my app like so:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Extend customizability (optional)\n</h2><p>In React, I can technically pass any  prop to a component so long as it renders as a functional component.</p><div><pre><code></code></pre></div><p>So, for the , I can extract the InputSelectTrigger out as a separate component to provide additional customizability</p><div><pre><code></code></pre></div><p>Now, I can also pass additional props to the  when I need it.</p><div><pre><code></code></pre></div>","contentLength":1445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feature Flags in React.js: Boosting Quality, Confidence, and Solving Marketing Challenges","url":"https://dev.to/jpbp/feature-flags-in-reactjs-boosting-quality-confidence-and-solving-marketing-challenges-4ho0","date":1739745814,"author":"João Pedro","guid":806,"unread":true,"content":"<p>Feature flags (or feature toggles) are a powerful technique in software development that allows teams to <strong>enable or disable features without deploying new code</strong>. In React.js, they provide a way to manage feature rollouts, improve code quality, increase deployment confidence, and even solve one of the hardest problems in computer science: marketing.</p><p>A feature flag is a conditional switch in your application that determines whether a feature is available to users. It enables dynamic control over functionality, allowing teams to toggle features on or off without requiring code changes or redeployments.</p><h3>\n  \n  \n  Benefits of Feature Flags in React.js\n</h3><ol><li>: Release features to a subset of users before a full rollout.</li><li>: Test different versions of a feature to see what performs best.</li><li>: Disable problematic features instantly.</li><li>: Deploy code without exposing unfinished features.</li><li>: Introduce features based on user segments, boosting adoption and revenue.</li></ol><h2>\n  \n  \n  Implementing Feature Flags in React.js\n</h2><h3>\n  \n  \n  1. Basic Implementation Using Context API\n</h3><p>You can use React's Context API to manage feature flags across your application.</p><div><pre><code></code></pre></div><p>Now, wrap your app with the :</p><div><pre><code></code></pre></div><p>Inside components, use the hook:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  2. Advanced Implementation with Remote Feature Flags\n</h3><p>For production-ready applications, using a remote feature flag management service like , , or  provides more flexibility.</p><p>Example using LaunchDarkly:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Feature Flags Solve the Hardest Problem in Computer Science: Marketing\n</h2><h3>\n  \n  \n  1. Gradual Feature Releases\n</h3><p>Instead of a big-bang launch that risks failure, companies can use feature flags to introduce new functionalities gradually. Marketing teams can align promotional campaigns with phased rollouts, ensuring customers see what’s being promoted.</p><h3>\n  \n  \n  2. Personalized User Experience\n</h3><p>By enabling features for specific user segments (e.g., premium users, early adopters), companies can drive engagement and upsell opportunities without bloating the UI with features that don’t appeal to every user.</p><h3>\n  \n  \n  3. Data-Driven Decision Making\n</h3><p>Feature flags enable A/B testing, allowing marketing teams to measure conversion rates and retention before committing to a full launch. Instead of relying on gut feelings, businesses can iterate based on real user behavior.</p><p>Feature flags in React.js provide a strategic advantage for development and marketing teams alike. They improve software quality by allowing incremental rollouts, reduce deployment risks, and enable data-driven marketing decisions. By integrating feature flags into your workflow, you ensure that your software evolves in a controlled, efficient, and user-centric way.</p>","contentLength":2641,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 13 Most-Starred Open Source Next.js Projects on GitHub ⭐","url":"https://dev.to/joodi/top-13-most-starred-open-source-nextjs-projects-on-github-4i6c","date":1739745240,"author":"Joodi","guid":805,"unread":true,"content":"<p>Hey, developers! 👋 Here’s a curated list of top Next.js open-source projects you shouldn't miss. Whether you're developing web apps, optimizing workflows, or diving into AI, these repositories are definitely worth a ⭐!</p><p>1️⃣ \n🔥 A powerful open-source alternative to Firebase, Supabase provides a full-fledged PostgreSQL database, real-time subscriptions, authentication, and storage—all in one platform. If you're building scalable web apps, this is a must-have!</p><p>2️⃣ \n🛫 An open-source project management tool built to track issues, epics, and roadmaps. With its user-friendly interface and seamless integration with GitHub, Slack, and Figma, it’s perfect for teams aiming to streamline their workflows.</p><p>3️⃣ \n📝 An open-source survey platform designed to collect user feedback. With its no-code editor and advanced analytics, it makes creating surveys and analyzing results effortless.</p><p>4️⃣ \n📊 A self-hosted, open-source CRM solution that gives you complete control over your data. Its intuitive interface, inspired by Notion, makes managing customer relationships simple and efficient.</p><p>5️⃣ \n🔗 A link management tool tailored for marketers. Create, share, and track shortened links with complete control, all while self-hosting the platform.</p><p>6️⃣ \n📅 A self-hosted open-source alternative to Calendly. With integrations for Google Calendar, Zoom, HubSpot, and more, it makes scheduling easy and efficient.</p><p>7️⃣ \n🔐 A secret management tool designed for teams. Securely store API keys, database credentials, and other sensitive data with role-based access control and versioning.</p><p>8️⃣ \n📅 A simple scheduling tool that allows users to create polls for group meetings—no sign-ups needed. It’s private, customizable, and easy to use.</p><p>9️⃣ \n📨 An AI-powered email app designed to help users achieve inbox zero with features like automated responses, email filtering, and subscription management.</p><p>🔟 \n🔑 An open-source API key management tool that simplifies and streamlines API authentication.</p><p>1️⃣1️⃣ \n📝 A self-hosted open-source document signing tool, ensuring privacy and transparency.</p><p>🔗 GitHub: github.com/documenso/documenso</p><p>1️⃣2️⃣ \n🤖 An open-source AI community platform for sharing, fine-tuning, and collaborating on AI models.</p><p>1️⃣3️⃣ \n💬 A no-code chatbot builder that allows you to create and embed advanced chatbots anywhere with ease.</p><p>That’s a wrap! These Next.js open-source projects are making a huge impact in the developer community. Go ahead, star them ⭐, contribute, and let’s create something amazing together! 💻🔥</p><p>I hope this was helpful, and I’d be happy to connect and follow each other!</p>","contentLength":2704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Ingestion with dlt - Week 3 Bonus","url":"https://dev.to/ccinaza/data-ingestion-with-dlt-week-3-bonus-25ok","date":1739745081,"author":"Blessing Angus","guid":804,"unread":true,"content":"<h2>\n  \n  \n  🧙‍♂️ Data Doesn’t Just Appear—Engineers Make It Happen!\n</h2><p>Have you ever opened a dataset and thought, “Wow, this is so clean and structured”? Well, someone worked really hard to make it that way! Welcome to data ingestion—the first step in any powerful data pipeline.</p><h3>\n  \n  \n  Why Data Pipelines Matter\n</h3><p>A data pipeline is more than just moving data from point A to point B. It ensures that raw, unstructured data becomes something usable, reliable, and insightful.</p><p>Here’s what happens under the hood:</p><p>1️⃣ Extract: Fetch data from APIs, databases, and files \n2️⃣ Normalize: Clean and structure messy, inconsistent formats <p>\n3️⃣ Load: Store it in data warehouses/lakes for analysis </p>\n4️⃣ Optimize: Use incremental loading to refresh data efficiently ⚡</p><h3>\n  \n  \n  Becoming the Data Magician 🧙‍♂️\n</h3><p>During our dlt workshop, we explored how to build scalable, self-maintaining pipelines that handle:</p><ul><li><p>Real-time and batch ingestion</p></li><li><p>Automated schema detection and normalization</p></li><li><p>Governance and best practices for high-quality data</p></li></ul><p>🚀 Key takeaway? If you want to work in data, mastering ingestion pipelines is a game-changer! Whether you’re dealing with messy JSON, SQL databases, or REST APIs, a strong pipeline ensures that data is always ready when you need it.</p><p>💬 What are your favorite tricks for handling messy data? Drop them in the comments! 👇</p><p>DataEngineering #DLT #ETL #BigData #Python #DataPipelines</p>","contentLength":1448,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"1KB Frontend Library: Final Bytes","url":"https://dev.to/fedia/1kb-frontend-library-final-bytes-1n0m","date":1739744629,"author":"Fedor","guid":803,"unread":true,"content":"<h2>\n  \n  \n  Recap: The Journey to a 1KB Frontend Library\n</h2><p>In the <a href=\"https://dev.to/fedia/1kb-frontend-library-5ef1\">previous article</a>, we set out to build a 1KB frontend library — a lean, no-frills toolkit that fits in just a few functions without any dependencies. Here’s what we covered:</p><ol><li>: For creating reactive state.</li><li>: For running side effects when state changes.</li><li>: For deriving reactive values.</li><li>: For declarative DOM rendering.</li></ol><p>These four functions gave us a solid foundation for building dynamic, efficient UIs without the need for build tools. But today, we’re introducing the fifth and final function —  — to unlock efficient list rendering. Let’s dive into how  works and why it’s the perfect finishing touch!</p><h2>\n  \n  \n  The Problem with Brute-Force Re-Rendering\n</h2><p>Signals allow us to pinpoint exactly which parts of the DOM depend on specific data changes, enabling us to rerender only the affected nodes or attributes.</p><p>However, when it comes to , the story changes. Lists often involve complex updates — adding, removing, or reordering items — that can’t be handled by simply updating individual nodes.</p><p>Let’s start with the basic way of rendering lists. Typically, we might use a  to iterate over an array and generate DOM elements. For example:</p><div><pre><code></code></pre></div><p>While this approach is straightforward, it has a significant downside: <strong>every update to the list results in a full re-render</strong>. Even if only one item in the list changes, the entire list is torn down and rebuilt. This can lead to unnecessary DOM manipulations, reflows, and degraded performance — especially as the list grows in size.</p><h2>\n  \n  \n  A Smarter Way to Render Lists with </h2><p>The  function is designed to solve this problem by introducing a . Instead of re-rendering the entire list,  compares the current state of the list with the new state and only updates the parts of the DOM that have changed. This approach minimizes DOM operations and <strong>preserves critical DOM states</strong> like user focus, text selection, and scroll position.</p><p>To implement the  function, we’re dusting off a  by <a href=\"https://github.com/paldepind/list-difference/\" rel=\"noopener noreferrer\">Simon Friis Vindum</a>, the creator of the widely respected  and  libraries. The resulting code is compact yet performant:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>The second argument to  is a function that returns a unique key for each item (e.g., ). These keys allow us to track which items have been added, removed, or moved, enabling precise updates to the DOM.</p><p>When the list updates,  creates a map of the old and new items using their keys. It then iterates through both lists simultaneously, comparing items based on their keys. The algorithm ensures that the DOM reflects the new list state with the fewest possible changes.</p><p>Unlike some old diffing algorithms,  allows you to use the entire item object as a key. This is possible because each leverages JavaScript’s Map under the hood, which can use objects as keys. For example:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  An Updated Example: A Todo App\n</h2><p>To showcase the power of the  function, here is an updated version of our Todo App example. Check out how it preserves DOM state like text selection.</p><h2>\n  \n  \n  Conclusion: A Complete 1KB Frontend Library\n</h2><p>Our journey to build a 1KB frontend library is now complete! With just  — , , , , and  — we’ve created a powerful, lightweight toolkit for building dynamic, efficient UIs. Clocking in at  minified and gzipped, this library proves that you don’t need megabytes of dependencies or complex build tools to create modern web applications.</p><p>And the best part? The entire code is provided below — ready for you to copy, paste, and start using right away.</p><div><pre><code></code></pre></div>","contentLength":3479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Relationships in MongoDB and Mongoose","url":"https://dev.to/harshm03/understanding-relationships-in-mongodb-and-mongoose-hl0","date":1739743208,"author":"Harsh Mishra","guid":802,"unread":true,"content":"<p>MongoDB, being a NoSQL database, handles relationships differently than relational databases like MySQL. Instead of foreign keys, it uses <strong>Embedded Documents (Denormalization)</strong> and <strong>References (Normalization)</strong> to establish connections between data.  </p><p>In this guide, we’ll explore:  </p><ul><li><strong>One-to-One (1:1) Relationships</strong></li><li><strong>One-to-Many (1:M) Relationships</strong></li><li><strong>Many-to-Many (M:N) Relationships</strong></li><li>How to define them in  and </li><li>How to query them and what the results look like\n</li></ul><p>In MongoDB, relationships can be represented in :</p><ol><li><strong>Embedded Documents (Denormalization)</strong></li><li><strong>References (Normalization)</strong></li></ol><p>Just like MySQL, MongoDB supports:</p><ul></ul><p>For each type, I’ll cover:</p><ul><li><strong>MongoDB Schema (Embedded &amp; Referenced)</strong></li><li><strong>How to Define it in Mongoose</strong></li></ul><h2><strong>2. One-to-One (1:1) Relationship</strong></h2><p>A  has , and a  belongs to only one .</p><h3><strong>Method 1: Using Embedded Document (Denormalization)</strong></h3><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3><strong>Method 2: Using References (Normalization)</strong></h3><h4><strong>MongoDB Schema (Separate Documents)</strong></h4><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h4><strong>Querying in Mongoose (Using )</strong></h4><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>3. One-to-Many (1:M) Relationship</strong></h2><p>A  has , but each  belongs to only one .</p><h3><strong>Method 1: Using Embedded Documents (Denormalization)</strong></h3><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3><strong>Method 2: Using References (Normalization)</strong></h3><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>4. Many-to-Many (M:N) Relationship</strong></h2><p>A  can enroll in , and a  can have .</p><h3><strong>Using References with a Junction Collection</strong></h3><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><table><thead><tr><th>References with </th></tr></thead><tbody><tr><td><code>{ profile: { bio: \"text\" } }</code></td><td><code>{ profile: ObjectId(\"profile_id\") }</code></td></tr><tr><td><code>{ posts: [ {title, content} ] }</code></td><td><code>{ posts: [ObjectId(\"post_id\")] }</code></td></tr><tr><td><code>{ courses: [ObjectId(\"course_id\")] }</code></td></tr></tbody></table></div><ul><li> are good for fast reads but increase duplication.</li><li><strong>References with </strong> keep data normalized but require joins.</li><li><strong>Mongoose makes relationships easy</strong> using .</li></ul>","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Running Laravel Reverb in the Background Using systemd","url":"https://dev.to/edgaras/running-laravel-reverb-in-the-background-using-systemd-3732","date":1739743200,"author":"Edgaras","guid":801,"unread":true,"content":"<p>If you're using Laravel's Reverb WebSocket server, managing it efficiently is key, especially in production environments. In this post, I’ll show you how to configure and run the Reverb WebSocket server in the background using . This approach ensures your WebSocket server starts with the system and restarts automatically in case of failure.</p><h2>\n  \n  \n  Step 1: Create the systemd Service File\n</h2><p>First, create a service file. This file tells systemd how to manage the server process.</p><p>Run the following command to create and edit the service file:</p><div><pre><code>sudo nano /etc/systemd/system/reverb.service\n</code></pre></div><p>Now, add the following configuration to the file:</p><div><pre><code>[Unit]\nDescription=Laravel Reverb WebSocket Server\nAfter=network.target\n\n[Service]\nExecStart=/usr/bin/php /var/www/example.test/artisan reverb:start --port=6001 --host=0.0.0.0\nRestart=always\nUser=www-data\nGroup=www-data\nWorkingDirectory=/var/www/example.test\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></div><h3>\n  \n  \n  Breaking Down the Parameters\n</h3><ul><li><ul><li>: Describes the service for your reference.</li><li>: Ensures the service starts only after the network is up.</li></ul></li><li><ul><li>: Specifies the command to start the Reverb server. Update the path (/var/www/example.test) to match your Laravel installation directory.</li><li>: Automatically restarts the service on failure.</li><li>: Runs the service as the  user and group, which is typical for web servers.</li><li>: Sets the working directory for the command, ensuring Laravel's environment is loaded correctly.</li></ul></li><li><ul><li><u>WantedBy=multi-user.target</u>: Ensures the service starts in the standard multi-user mode.</li></ul></li></ul><h2>\n  \n  \n  Step 2: Reload systemd and Enable the Service\n</h2><p>Once the service file is created, reload  to pick up the changes:</p><div><pre><code>sudo systemctl daemon-reload\n</code></pre></div><p>Next, enable the service so it starts automatically on boot:</p><div><pre><code>sudo systemctl enable reverb.service\n</code></pre></div><h2>\n  \n  \n  Step 3: Start the Service\n</h2><p>To start the service manually for the first time, use:</p><div><pre><code>sudo systemctl start reverb.service\n</code></pre></div><p>This will launch the Reverb WebSocket server in the background.</p><h2>\n  \n  \n  Step 4: Check the Service Status\n</h2><p>To verify that the service is running correctly, use:</p><div><pre><code>sudo systemctl status reverb.service\n</code></pre></div><p>This command provides detailed output about the current state of the service, including whether it’s active, running, or if there are errors.</p><p>If you need to stop the service, run:</p><div><pre><code>sudo systemctl stop reverb.service\n</code></pre></div><p>This immediately halts the WebSocket server without removing it from startup.</p><p>If you’ve made changes to the service or the Laravel application, restart it using:</p><div><pre><code>sudo systemctl restart reverb.service\n</code></pre></div><p>This stops the current service instance and starts a new one.</p><p>For a deeper understanding of  service configuration, <a href=\"https://dev.to/edgaras/creating-and-managing-custom-systemd-services-on-ubuntu-dkh\">read this post</a>.</p>","contentLength":2628,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mongoose with MongoDB","url":"https://dev.to/harshm03/mongoose-with-mongodb-4h0","date":1739742816,"author":"Harsh Mishra","guid":800,"unread":true,"content":"<p>Mongoose is an <strong>ODM (Object Data Modeling) library</strong> for  and . It provides a schema-based approach to working with MongoDB, allowing developers to define models with built-in validation, middleware, and query-building capabilities. This guide will take you through everything you need to know about using , from setup to advanced queries, relationships, and deployment.  </p><ol><li>\nWhy Use Mongoose with MongoDB?\n</li><li>\nSetting Up Mongoose with MongoDB\n</li><li>\nUnderstanding Mongoose Schema &amp; Models\n</li><li>\nCRUD Operations with Mongoose\n</li><li>\nQuerying Data in Mongoose\n</li><li>\nWorking with Relations in Mongoose\n</li><li>\nUsing Middleware in Mongoose\n</li><li>\nValidation and Custom Validators\n</li><li>\nWorking with Transactions\n</li><li>\nError Handling in Mongoose\n</li><li>\nDeploying Mongoose with MongoDB\n</li></ol><h2><strong>1. Introduction to Mongoose</strong></h2><p>Mongoose is an <strong>Object Data Modeling (ODM) library</strong> for MongoDB and Node.js. It provides:  </p><p>✅  for defining document structure. for data integrity. for handling pre/post operations. for efficient data retrieval. to model complex data structures.  </p><p>Mongoose makes working with MongoDB more structured and organized, making it a popular choice for Node.js developers.  </p><h2><strong>2. Why Use Mongoose with MongoDB?</strong></h2><p>MongoDB is a  that stores data in JSON-like documents. Mongoose provides a structured way to interact with MongoDB by offering:  </p><p>🔹  – Defines structure and types for documents. – Ensures data integrity before saving. – Makes queries more readable and efficient. – Allows logic before and after database actions. – Enables population and referencing of documents.  </p><h2><strong>3. Setting Up Mongoose with MongoDB</strong></h2><h3><strong>Step 1: Install Node.js and MongoDB</strong></h3><p>Ensure  and  are installed. You can download MongoDB from:<a href=\"https://www.mongodb.com/try/download/community\" rel=\"noopener noreferrer\">MongoDB Download</a></p><h3><strong>Step 2: Initialize a Node.js Project</strong></h3><div><pre><code>my-mongoose-app\nmy-mongoose-app\nnpm init </code></pre></div><h3><strong>Step 4: Connect Mongoose to MongoDB</strong></h3><div><pre><code></code></pre></div><h2><strong>4. Understanding Mongoose Schema &amp; Models</strong></h2><p>A  defines the structure of a document. A  is an instance of a schema that interacts with the database.  </p><div><pre><code></code></pre></div><h2><strong>5. CRUD Operations with Mongoose</strong></h2><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>6. Querying Data in Mongoose</strong></h2><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3><strong>Sorting &amp; Limiting Results</strong></h3><div><pre><code></code></pre></div><h2><strong>7. Working with Relations in Mongoose</strong></h2><h3><strong>One-to-Many Relationship (User → Posts)</strong></h3><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>8. Using Middleware in Mongoose</strong></h2><p>Mongoose supports  and  middleware hooks.  </p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>9. Validation and Custom Validators</strong></h2><p>Mongoose provides built-in validation like , , and .  </p><p>Example custom validator:</p><div><pre><code></code></pre></div><h2><strong>10. Working with Transactions</strong></h2><p>MongoDB supports transactions using .</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>12. Deploying Mongoose with MongoDB</strong></h2><p>MongoDB Atlas provides a <strong>cloud-based MongoDB database</strong>.  </p><div><pre><code>DATABASE_URL=\"mongodb+srv://your-user:password@cluster.mongodb.net/mydatabase\"\n</code></pre></div><div><pre><code></code></pre></div><p>Mongoose provides a <strong>structured, type-safe, and efficient</strong> way to interact with MongoDB. With <strong>schema validation, relationships, middleware, transactions, and error handling</strong>, it simplifies database interactions while ensuring reliability. 🚀</p>","contentLength":2753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SQL Coding","url":"https://dev.to/farlamo/sql-coding-2j7","date":1739742055,"author":"Faruk","guid":792,"unread":true,"content":"<p>In the realm of SQL database development, adhering to advanced coding practices is paramount for ensuring optimal performance, maintainability, and scalability. This article delves into sophisticated techniques and methodologies that seasoned database professionals employ to enhance SQL coding standards.</p><p>Advanced SQL Coding Practices</p><ol><li>Embrace Set-Based Operations\nSQL is inherently designed for set-based operations. Prioritizing set-based approaches over iterative constructs like cursors can lead to significant performance gains. Set-based operations allow the database engine to optimize query execution more effectively.</li></ol><p>Instead of processing rows individually:</p><div><pre><code>DECLARE @id INT\nDECLARE cursor_example CURSOR FOR\nSELECT id FROM Orders\nOPEN cursor_example\nFETCH NEXT FROM cursor_example INTO @id\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    -- Process each order\n    FETCH NEXT FROM cursor_example INTO @id\nEND\nCLOSE cursor_example\nDEALLOCATE cursor_example\n</code></pre></div><p>Opt for a set-based operation:</p><p><code>UPDATE Orders\nSET Processed = 1<p>\nWHERE OrderDate &lt; '2025-01-01'</p></code></p><ol><li>Leverage Common Table Expressions (CTEs) for Recursive Queries\nCTEs provide a more readable and manageable way to write recursive queries, especially when dealing with hierarchical <a href=\"https://databasesample.com/database/notepad-database\" rel=\"noopener noreferrer\">data</a> structures.</li></ol><p>Example:<code>WITH RecursiveCTE AS (\n    SELECT id, ParentID, Name, 1 AS Level\n    WHERE ParentID IS NULL\n    SELECT c.id, c.ParentID, c.Name, r.Level + 1\n    INNER JOIN RecursiveCTE r ON c.ParentID = r.id\nSELECT * FROM RecursiveCTE</code></p><ol><li>Implement Window Functions for Advanced Analytics\nWindow functions allow for performing calculations across a set of table rows related to the current row, enabling complex analytical tasks without the need for self-joins or subqueries.</li></ol><p>Example:<code>SELECT\n    EmployeeID,\n    AVG(Salary) OVER (PARTITION BY DepartmentID) AS AvgDeptSalary</code></p><ol><li>Optimize Index Usage\nEffective indexing is crucial for query performance. However, over-indexing can lead to increased storage requirements and slower write operations. Regularly analyze query performance and adjust indexes accordingly.</li></ol><p>Composite Indexes: Create indexes on multiple columns that are frequently used together in WHERE clauses.</p><p>Covering Indexes: Design indexes that include all the columns retrieved by a query to avoid accessing the table data.</p><p>Index Maintenance: Regularly rebuild or reorganize fragmented indexes to maintain their efficiency.</p><ol><li>Utilize Parameterized Queries to Prevent SQL Injection\nParameterized queries ensure that user inputs are treated as data rather than executable code, mitigating the risk of <a href=\"https://databasesample.com/database/apple-notes-database\" rel=\"noopener noreferrer\">SQL</a> injection attacks.</li></ol><p>Example:<code>-- Using T-SQL\nDECLARE @username NVARCHAR(50)<p>\nSET @username = 'user_input'</p>\nEXEC sp_executesql N'SELECT * FROM Users WHERE Username = @username', N'@username NVARCHAR(50)', @username</code></p><ol><li>Apply Proper Transaction Management\nEnsuring data integrity requires meticulous transaction management, especially in systems with high concurrency.</li></ol><p>Transaction Scope: Keep transactions as short as possible to reduce locking and potential contention.</p><p>Error Handling: Implement TRY...CATCH blocks to handle exceptions and ensure transactions are rolled back in case of errors.</p><p>Isolation Levels: Choose the appropriate isolation level based on the specific requirements to balance between data consistency and system performance.</p><ol><li>Regularly Update Statistics\nDatabase engines rely on statistics to generate optimal execution plans. Regularly updating these statistics ensures the query optimizer has accurate information.</li></ol><p>Example:<code>-- For SQL Server\nUPDATE STATISTICS dbo.Orders</code></p><ol><li>Avoid Using SELECT *\nExplicitly specifying the required columns in a SELECT statement reduces the amount of data transferred and can improve performance.</li></ol><p>Example:\n`-- Inefficient</p><p>-- Efficient\nSELECT OrderID, OrderDate, CustomerID FROM Orders`</p><ol><li>Monitor and Analyze Query Performance\nUtilize tools and techniques to monitor query performance and identify bottlenecks.</li></ol><p>Execution Plans: Analyze execution plans to understand how queries are executed and identify areas for improvement.</p><p>Performance Counters: <a href=\"https://databasesample.com/database/chkrootkit-database\" rel=\"noopener noreferrer\">Monitor</a> relevant performance counters to detect resource bottlenecks.</p><p>Query Profiling Tools: Use profiling tools to trace query execution and gather performance metrics.</p><ol><li>Implement Proper Error Handling and Logging\nRobust error handling ensures that exceptions are managed gracefully, and logging provides insights into system behavior and aids in troubleshooting.</li></ol><p>Example:<code>BEGIN TRY\n    -- Attempt to execute a query<p>\n    INSERT INTO Orders (OrderID, OrderDate) VALUES (1, '2025-02-17')</p>\nEND TRY\n    -- Handle the error<p>\n    DECLARE @ErrorMessage NVARCHAR(4000)</p>\n    SET @ErrorMessage = ERROR_MESSAGE()<p>\n    -- Log the error or take appropriate action</p>\nEND CATCH</code>\nBy integrating these advanced practices into your SQL development workflow, you can enhance the efficiency, security, and maintainability of your database systems.</p>","contentLength":4818,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative and Predictive AI in Application Security: A Comprehensive Guide","url":"https://dev.to/friendgrass7/generative-and-predictive-ai-in-application-security-a-comprehensive-guide-37c","date":1739741790,"author":"Coley Guerrero","guid":791,"unread":true,"content":"<p>AI is transforming the field of application security by enabling more sophisticated weakness identification, automated assessments, and even self-directed attack surface scanning. This write-up delivers an comprehensive narrative on how machine learning and AI-driven solutions function in AppSec, written for cybersecurity experts and executives alike. We’ll examine the development of AI for security testing, its current capabilities, obstacles, the rise of “agentic” AI, and future developments. Let’s begin our journey through the past, current landscape, and future of AI-driven AppSec defenses. </p><p>History and Development of AI in AppSec </p><p>Initial Steps Toward Automated AppSec \nLong before machine learning became a hot subject, cybersecurity personnel sought to mechanize security flaw identification. In the late 1980s, the academic Barton Miller’s pioneering work on fuzz testing showed the impact of automation. His 1988 class project randomly generated inputs to crash UNIX programs — “fuzzing” uncovered that 25–33% of utility programs could be crashed with random data. This straightforward black-box approach paved the groundwork for later security testing methods. By the 1990s and early 2000s, developers employed basic programs and tools to find common flaws. Early static scanning tools functioned like advanced grep, inspecting code for dangerous functions or embedded secrets. While these pattern-matching methods were useful, they often yielded many spurious alerts, because any code resembling a pattern was labeled regardless of context. </p><p>Evolution of AI-Driven Security Models \nDuring the following years, academic research and industry tools improved, moving from static rules to context-aware analysis. Data-driven algorithms slowly made its way into the application security realm. Early adoptions included neural networks for anomaly detection in system traffic, and Bayesian filters for spam or phishing — not strictly application security, but demonstrative of the trend. Meanwhile, static analysis tools improved with flow-based examination and control flow graphs to observe how inputs moved through an app. </p><p>A notable concept that arose was the Code Property Graph (CPG), combining syntax, control flow, and information flow into a comprehensive graph. This approach enabled more meaningful vulnerability assessment and later won an IEEE “Test of Time” honor. By depicting a codebase as nodes and edges, analysis platforms could pinpoint complex flaws beyond simple signature references. </p><p>In 2016, DARPA’s Cyber Grand Challenge exhibited fully automated hacking platforms — designed to find, confirm, and patch security holes in real time, minus human intervention. The winning system, “Mayhem,” combined advanced analysis, symbolic execution, and some AI planning to compete against human hackers. This event was a defining moment in self-governing cyber defense. </p><p>Major Breakthroughs in AI for Vulnerability Detection \nWith the growth of better algorithms and more labeled examples, AI security solutions has accelerated. Large tech firms and startups together have reached breakthroughs. One substantial leap involves machine learning models predicting software vulnerabilities and exploits. An example is the Exploit Prediction Scoring System (EPSS), which uses a vast number of data points to predict which CVEs will get targeted in the wild. This approach helps infosec practitioners prioritize the highest-risk weaknesses. </p><p>In detecting code flaws, deep learning networks have been supplied with massive codebases to spot insecure structures. Microsoft, Alphabet, and various entities have revealed that generative LLMs (Large Language Models) enhance security tasks by creating new test cases. For example, Google’s security team leveraged LLMs to produce test harnesses for open-source projects, increasing coverage and finding more bugs with less developer involvement. </p><p>Modern AI Advantages for Application Security </p><p>Today’s application security leverages AI in two major categories: generative AI, producing new outputs (like tests, code, or exploits), and predictive AI, analyzing data to highlight or project vulnerabilities. These capabilities reach every aspect of application security processes, from code review to dynamic scanning. </p><p>AI-Generated Tests and Attacks \nGenerative AI produces new data, such as attacks or code segments that reveal vulnerabilities. This is visible in AI-driven fuzzing. Classic fuzzing uses random or mutational data, while generative models can create more strategic tests. Google’s OSS-Fuzz team tried LLMs to write additional fuzz targets for open-source codebases, boosting vulnerability discovery. </p><p>In the same vein, generative AI can aid in building exploit scripts. Researchers cautiously demonstrate that LLMs enable the creation of proof-of-concept code once a vulnerability is known. On the attacker side, penetration testers may use generative AI to simulate threat actors. For defenders, teams use automatic PoC generation to better test defenses and implement fixes. </p><p>How Predictive Models Find and Rate Threats \nPredictive AI analyzes data sets to locate likely exploitable flaws. Rather than manual rules or signatures, a model can infer from thousands of vulnerable vs. safe software snippets, spotting patterns that a rule-based system would miss. This approach helps indicate suspicious constructs and predict the risk of newly found issues. </p><p>Prioritizing flaws is a second predictive AI application. The EPSS is one illustration where a machine learning model scores security flaws by the probability they’ll be attacked in the wild. This helps security teams focus on the top fraction of vulnerabilities that pose the highest risk. Some modern AppSec toolchains feed pull requests and historical bug data into ML models, predicting which areas of an product are most prone to new flaws. </p><p>Merging AI with SAST, DAST, IAST \nClassic SAST tools, DAST tools, and interactive application security testing (IAST) are more and more empowering with AI to upgrade throughput and effectiveness. </p><p>SAST analyzes source files for security defects in a non-runtime context, but often produces a slew of incorrect alerts if it doesn’t have enough context. AI helps by ranking findings and filtering those that aren’t actually exploitable, through smart data flow analysis. Tools for example Qwiet AI and others employ a Code Property Graph and AI-driven logic to judge exploit paths, drastically reducing the extraneous findings. </p><p>DAST scans a running app, sending test inputs and observing the responses. AI advances DAST by allowing autonomous crawling and adaptive testing strategies. The autonomous module can interpret multi-step workflows, modern app flows, and RESTful calls more accurately, raising comprehensiveness and decreasing oversight. </p><p>IAST, which instruments the application at runtime to log function calls and data flows, can yield volumes of telemetry. An AI model can interpret that data, spotting dangerous flows where user input affects a critical function unfiltered. By integrating IAST with ML, false alarms get removed, and only valid risks are shown. </p><p>Methods of Program Inspection: Grep, Signatures, and CPG \nToday’s code scanning systems commonly mix several approaches, each with its pros/cons: </p><p>Grepping (Pattern Matching): The most rudimentary method, searching for tokens or known patterns (e.g., suspicious functions). Fast but highly prone to wrong flags and missed issues due to no semantic understanding. </p><p>Signatures (Rules/Heuristics): Heuristic scanning where security professionals create patterns for known flaws. It’s good for common bug classes but limited for new or unusual weakness classes. </p><p>Code Property Graphs (CPG): A advanced semantic approach, unifying syntax tree, control flow graph, and data flow graph into one structure. Tools analyze the graph for critical data paths. Combined with ML, it can discover unknown patterns and cut down noise via flow-based context. </p><p>In practice, providers combine these strategies. They still use rules for known issues, but they supplement them with AI-driven analysis for deeper insight and ML for prioritizing alerts. </p><p>Securing Containers &amp; Addressing Supply Chain Threats \nAs companies shifted to cloud-native architectures, container and open-source library security gained priority. AI helps here, too: </p><p>Container Security: AI-driven image scanners scrutinize container files for known security holes, misconfigurations, or API keys. Some solutions assess whether vulnerabilities are actually used at runtime, diminishing the alert noise. Meanwhile, machine learning-based monitoring at runtime can detect unusual container activity (e.g., unexpected network calls), catching intrusions that traditional tools might miss. </p><p>Supply Chain Risks: With millions of open-source packages in npm, PyPI, Maven, etc., manual vetting is infeasible. AI can monitor package behavior for malicious indicators, exposing hidden trojans. Machine learning models can also evaluate the likelihood a certain component might be compromised, factoring in maintainer reputation. This allows teams to pinpoint the dangerous supply chain elements. In parallel, AI can watch for anomalies in build pipelines, verifying that only legitimate code and dependencies are deployed. </p><p>Challenges and Limitations </p><p>While AI offers powerful capabilities to application security, it’s no silver bullet. Teams must understand the problems, such as misclassifications, reachability challenges, algorithmic skew, and handling undisclosed threats. </p><p>Accuracy Issues in AI Detection \nAll AI detection deals with false positives (flagging non-vulnerable code) and false negatives (missing actual vulnerabilities). AI can reduce the former by adding context, yet it introduces new sources of error. A model might incorrectly detect issues or, if not trained properly, overlook a serious bug. Hence, expert validation often remains essential to verify accurate results. </p><p>Reachability and Exploitability Analysis \nEven if AI flags a problematic code path, that doesn’t guarantee hackers can actually access it. Assessing real-world exploitability is difficult. Some suites attempt symbolic execution to validate or dismiss exploit feasibility. However, full-blown exploitability checks remain rare in commercial solutions. Therefore, many AI-driven findings still require human judgment to label them urgent. </p><p>Data Skew and Misclassifications \nAI algorithms adapt from historical data. If that data is dominated by certain coding patterns, or lacks instances of emerging threats, the AI could fail to anticipate them. Additionally, a system might disregard certain languages if the training set concluded those are less prone to be exploited. Continuous retraining, inclusive data sets, and bias monitoring are critical to mitigate this issue. </p><p>Dealing with the Unknown \nMachine learning excels with patterns it has seen before. A wholly new vulnerability type can slip past AI if it doesn’t match existing knowledge. Attackers also work with adversarial AI to mislead defensive systems. Hence, AI-based solutions must adapt constantly. Some vendors adopt anomaly detection or unsupervised clustering to catch abnormal behavior that signature-based approaches might miss. Yet, even these anomaly-based methods can fail to catch cleverly disguised zero-days or produce red herrings. </p><p>The Rise of Agentic AI in Security </p><p>A recent term in the AI community is agentic AI — intelligent agents that don’t just produce outputs, but can take objectives autonomously. In AppSec, this means AI that can control multi-step operations, adapt to real-time feedback, and act with minimal human input. </p><p>What is Agentic AI? \nAgentic AI programs are given high-level objectives like “find security flaws in this system,” and then they determine how to do so: collecting data, performing tests, and shifting strategies based on findings. Implications are substantial: we move from AI as a utility to AI as an independent actor. </p><p>Offensive vs. Defensive AI Agents \nOffensive (Red Team) Usage: Agentic AI can conduct red-team exercises autonomously. Security firms like FireCompass advertise an AI that enumerates vulnerabilities, crafts exploit strategies, and demonstrates compromise — all on its own. Likewise, open-source “PentestGPT” or comparable solutions use LLM-driven logic to chain scans for multi-stage penetrations. </p><p><a href=\"https://rentry.co/3rdksey5\" rel=\"noopener noreferrer\">https://rentry.co/3rdksey5</a> (Blue Team) Usage: On the protective side, AI agents can survey networks and proactively respond to suspicious events (e.g., isolating a compromised host, updating firewall rules, or analyzing logs). Some security orchestration platforms are experimenting with “agentic playbooks” where the AI handles triage dynamically, rather than just using static workflows. </p><p>AI-Driven Red Teaming \nFully autonomous penetration testing is the ambition for many cyber experts. Tools that comprehensively discover vulnerabilities, craft attack sequences, and evidence them with minimal human direction are becoming a reality. Successes from DARPA’s Cyber Grand Challenge and new self-operating systems show that multi-step attacks can be orchestrated by autonomous solutions. </p><p>Risks in Autonomous Security \nWith great autonomy arrives danger. An autonomous system might accidentally cause damage in a live system, or an malicious party might manipulate the agent to mount destructive actions. Robust guardrails, sandboxing, and manual gating for risky tasks are critical. Nonetheless, agentic AI represents the next evolution in cyber defense. </p><p>Where AI in Application Security is Headed </p><p>AI’s role in AppSec will only expand. We anticipate major developments in the next 1–3 years and longer horizon, with new compliance concerns and responsible considerations. </p><p>Short-Range Projections \nOver the next handful of years, enterprises will embrace AI-assisted coding and security more frequently. Developer tools will include AppSec evaluations driven by AI models to highlight potential issues in real time. Machine learning fuzzers will become standard. Regular ML-driven scanning with agentic AI will supplement annual or quarterly pen tests. Expect upgrades in noise minimization as feedback loops refine learning models. </p><p>Threat actors will also leverage generative AI for phishing, so defensive filters must evolve. We’ll see phishing emails that are nearly perfect, necessitating new AI-based detection to fight machine-written lures. </p><p>Regulators and compliance agencies may start issuing frameworks for ethical AI usage in cybersecurity. For example, rules might require that organizations audit AI recommendations to ensure explainability. </p><p>Futuristic Vision of AppSec \nIn the long-range range, AI may reinvent DevSecOps entirely, possibly leading to: </p><p>AI-augmented development: Humans pair-program with AI that produces the majority of code, inherently enforcing security as it goes. </p><p>Automated vulnerability remediation: Tools that not only detect flaws but also patch them autonomously, verifying the safety of each solution. </p><p>Proactive, continuous defense: Automated watchers scanning apps around the clock, predicting attacks, deploying security controls on-the-fly, and dueling adversarial AI in real-time. </p><p>Secure-by-design architectures: AI-driven architectural scanning ensuring applications are built with minimal attack surfaces from the start. </p><p>We also expect that AI itself will be strictly overseen, with compliance rules for AI usage in high-impact industries. This might mandate explainable AI and auditing of training data. </p><p>AI in Compliance and Governance \nAs AI assumes a core role in AppSec, compliance frameworks will adapt. We may see: </p><p>AI-powered compliance checks: Automated auditing to ensure controls (e.g., PCI DSS, SOC 2) are met in real time. </p><p>Governance of AI models: Requirements that organizations track training data, show model fairness, and record AI-driven decisions for auditors. </p><p>Incident response oversight: If an AI agent conducts a defensive action, which party is responsible? Defining responsibility for AI misjudgments is a complex issue that legislatures will tackle. </p><p>Moral Dimensions and Threats of AI Usage \nIn addition to compliance, there are ethical questions. Using AI for insider threat detection can lead to privacy breaches. Relying solely on AI for critical decisions can be unwise if the AI is flawed. Meanwhile, criminals use AI to generate sophisticated attacks. Data poisoning and model tampering can disrupt defensive AI systems. </p><p>Adversarial AI represents a heightened threat, where attackers specifically target ML infrastructures or use generative AI to evade detection. Ensuring the security of ML code will be an critical facet of cyber defense in the future. </p><p>AI-driven methods have begun revolutionizing application security. We’ve discussed the foundations, modern solutions, obstacles, agentic AI implications, and forward-looking prospects. The main point is that AI functions as a powerful ally for AppSec professionals, helping spot weaknesses sooner, rank the biggest threats, and handle tedious chores. </p><p>Yet, it’s not infallible. False positives, training data skews, and zero-day weaknesses still demand human expertise. The constant battle between attackers and protectors continues; AI is merely the latest arena for that conflict. Organizations that incorporate AI responsibly — aligning it with expert analysis, robust governance, and regular model refreshes — are positioned to succeed in the continually changing landscape of AppSec. </p><p>Ultimately, the opportunity of AI is a safer digital landscape, where weak spots are detected early and remediated swiftly, and where defenders can combat the agility of attackers head-on. With ongoing research, collaboration, and growth in AI techniques, that vision may come to pass in the not-too-distant timeline.<a href=\"https://rentry.co/3rdksey5\" rel=\"noopener noreferrer\">https://rentry.co/3rdksey5</a></p>","contentLength":17951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prisma.js with MySQL","url":"https://dev.to/harshm03/prismajs-with-mysql-23f0","date":1739740686,"author":"Harsh Mishra","guid":790,"unread":true,"content":"<p>Prisma.js is a modern database toolkit that simplifies working with databases in Node.js and TypeScript applications. It offers a type-safe and intuitive way to interact with databases, making development faster and less error-prone. This comprehensive guide will cover everything you need to know about , from installation and setup to advanced database queries, migrations, and best practices.  </p><ol><li>\nIntroduction to Prisma.js\n</li><li>\nWhy Use Prisma with MySQL?\n</li><li>\nSetting Up Prisma with MySQL\n</li><li>\nUnderstanding Prisma Schema\n</li><li>\nRunning Database Migrations\n</li><li>\nCRUD Operations with Prisma Client\n</li><li>\nAdvanced Queries in Prisma\n</li><li>\nUsing Prisma with Express.js\n</li><li>\nDeployment Best Practices\n</li></ol><h2><strong>1. Introduction to Prisma.js</strong></h2><p>Prisma is an <strong>open-source ORM (Object-Relational Mapper)</strong> that simplifies database access in  applications. It provides:  </p><ul><li>: Auto-generated and type-safe database queries.\n</li><li>: A powerful migration tool for database schema changes.\n</li><li>: A GUI for viewing and managing database records.\n</li></ul><p>Prisma supports <strong>MySQL, PostgreSQL, SQLite, SQL Server, MongoDB, and CockroachDB</strong>.  </p><h2><strong>2. Why Use Prisma with MySQL?</strong></h2><p>Prisma is an excellent choice for working with  because it offers:  </p><p>✅ : Auto-generated TypeScript types for database models.: Uses a simple, declarative syntax.: Handles schema changes effortlessly.: Optimized queries and caching.: Easily switch between databases.  </p><h2><strong>3. Setting Up Prisma with MySQL</strong></h2><h3><strong>Step 1: Install Node.js and MySQL</strong></h3><p>Make sure you have  installed. Install MySQL and create a database:</p><div><pre><code></code></pre></div><h3><strong>Step 2: Initialize a Node.js Project</strong></h3><div><pre><code>my-prisma-app\nmy-prisma-app\nnpm init </code></pre></div><h3><strong>Step 3: Install Prisma and MySQL Client</strong></h3><div><pre><code>npm prisma \nnpm  @prisma/client mysql2\n</code></pre></div><h3><strong>Step 4: Initialize Prisma</strong></h3><p>This creates a  folder with a  file.  </p><h3><strong>Step 5: Configure Prisma for MySQL</strong></h3><p>Edit :</p><div><pre><code>generator client {\n  provider = \"prisma-client-js\"\n}\n\ndatasource db {\n  provider = \"mysql\"\n  url      = env(\"DATABASE_URL\")\n}\n</code></pre></div><h3><strong>Step 6: Set Up Environment Variables</strong></h3><div><pre><code>DATABASE_URL=\"mysql://root:password@localhost:3306/mydatabase\"\n</code></pre></div><h2><strong>4. Understanding Prisma Schema</strong></h2><p>The  file defines the database structure.  </p><p>Example schema for :</p><div><pre><code>model User {\n  id       Int      @id @default(autoincrement())\n  name     String\n  email    String   @unique\n  posts    Post[]\n}\n\nmodel Post {\n  id       Int      @id @default(autoincrement())\n  title    String\n  content  String?\n  author   User     @relation(fields: [userId], references: [id])\n  userId   Int\n}\n</code></pre></div><ul><li>: Defines a primary key.\n</li><li><code>@default(autoincrement())</code>: Auto-increment ID.\n</li><li>: Ensures unique values.\n</li><li>: Defines foreign key relationships.\n</li></ul><h2><strong>5. Running Database Migrations</strong></h2><p>Generate migration files:</p><div><pre><code>npx prisma migrate dev  init\n</code></pre></div><div><pre><code>npx prisma migrate deploy\n</code></pre></div><h2><strong>6. CRUD Operations with Prisma Client</strong></h2><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>7. Advanced Queries in Prisma</strong></h2><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>8. Using Prisma with Express.js</strong></h2><div><pre><code></code></pre></div><h2><strong>9. Working with Relations</strong></h2><p>Prisma supports <strong>one-to-one, one-to-many, and many-to-many</strong> relations.  </p><ul><li>One-to-Many (User → Posts)\n</li><li>Many-to-Many (Users ↔ Groups)\n</li></ul><p>Example many-to-many relationship:</p><div><pre><code>model User {\n  id    Int    @id @default(autoincrement())\n  name  String\n  groups Group[] @relation(\"UserGroups\")\n}\n\nmodel Group {\n  id    Int    @id @default(autoincrement())\n  name  String\n  users User[] @relation(\"UserGroups\")\n}\n</code></pre></div><div><pre><code></code></pre></div><p>Prisma.js with MySQL offers a <strong>type-safe, intuitive, and efficient</strong> way to manage databases. From <strong>migrations and CRUD operations to advanced queries and API integration</strong>, Prisma simplifies database management while improving performance and developer experience. 🚀</p>","contentLength":3387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Relationships in MySQL and Prisma","url":"https://dev.to/harshm03/understanding-relationships-in-mysql-and-prisma-1opc","date":1739740603,"author":"Harsh Mishra","guid":789,"unread":true,"content":"<p>In relational databases like MySQL, defining relationships between tables is essential for structuring data efficiently. There are three main types of relationships: , , and . Each type requires a specific table structure, foreign key constraints, and query approach.  </p><p>In this article, we will explore:  </p><ul><li>How to define relationships in MySQL with SQL table schemas.\n</li><li>How to query related data using SQL.\n</li><li>How to model the same relationships in .\n</li><li>How to retrieve related data using Prisma queries.\n</li></ul><p>By the end, you’ll have a solid understanding of how to manage relational data efficiently with . 🚀</p><p>In MySQL, relationships between tables are typically categorized into three main types:</p><ol></ol><p>For each relation, I will explain:  </p><ul><li><strong>Querying the Relationship in SQL</strong></li><li><strong>How to Define It in Prisma Schema</strong></li><li><strong>Querying the Relationship in Prisma</strong></li></ul><h2><strong>2. One-to-One (1:1) Relationship</strong></h2><p>A  can have , and a  belongs to only one .</p><div><pre><code></code></pre></div><h3><strong>Querying in MySQL (Fetching User with Profile)</strong></h3><div><pre><code></code></pre></div><div><table><tbody></tbody></table></div><h3><strong>Defining One-to-One in Prisma Schema</strong></h3><div><pre><code>model User {\n  id      Int      @id @default(autoincrement())\n  name    String\n  profile Profile?\n}\n\nmodel Profile {\n  id     Int    @id @default(autoincrement())\n  bio    String?\n  user   User   @relation(fields: [userId], references: [id])\n  userId Int    @unique\n}\n</code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>3. One-to-Many (1:M) Relationship</strong></h2><p>A  can have , but each  belongs to only one .</p><div><pre><code></code></pre></div><h3><strong>Querying in MySQL (Fetching User with Posts)</strong></h3><div><pre><code></code></pre></div><div><table><tbody></tbody></table></div><h3><strong>Defining One-to-Many in Prisma Schema</strong></h3><div><pre><code>model User {\n  id    Int     @id @default(autoincrement())\n  name  String\n  posts Post[]\n}\n\nmodel Post {\n  id      Int    @id @default(autoincrement())\n  title   String\n  content String?\n  user    User   @relation(fields: [userId], references: [id])\n  userId  Int\n}\n</code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h2><strong>4. Many-to-Many (M:N) Relationship</strong></h2><p>A  can enroll in , and a  can have .</p><div><pre><code></code></pre></div><h3><strong>Querying in MySQL (Fetching Students with Courses)</strong></h3><div><pre><code></code></pre></div><div><table><tbody></tbody></table></div><h3><strong>Defining Many-to-Many in Prisma Schema</strong></h3><div><pre><code>model Student {\n  id         Int       @id @default(autoincrement())\n  name       String\n  courses    Course[]  @relation(\"Enrollments\")\n}\n\nmodel Course {\n  id         Int       @id @default(autoincrement())\n  title      String\n  students   Student[] @relation(\"Enrollments\")\n}\n\nmodel Enrollment {\n  student   Student @relation(fields: [studentId], references: [id])\n  studentId Int\n  course    Course  @relation(fields: [courseId], references: [id])\n  courseId  Int\n\n  @@id([studentId, courseId])\n}\n</code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><table><thead><tr></tr></thead><tbody><tr><td> in second table</td><td><code>include: { profile: true }</code></td></tr><tr><td>Foreign key in child table</td></tr><tr><td>Junction table with two FKs</td><td><code>include: { courses: true }</code></td></tr></tbody></table></div><ul><li> uses  and  to define relationships.\n</li><li> uses a  to simplify relationship management.\n</li><li><strong>Querying relationships in Prisma</strong> is easy using .\n</li><li><strong>Output in Prisma is structured as JSON</strong>, making it easy to work with.\n</li></ul>","contentLength":2650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/numberonebot/-1a3n","date":1739740159,"author":"Alex Strelets","guid":788,"unread":true,"content":"<h2>I Built a Library to Visualize and Edit Audio Filters</h2>","contentLength":53,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cómo integrar azure advisor con react y node.js","url":"https://dev.to/danieljsaldana/como-integrar-azure-advisor-con-react-y-nodejs-4dbn","date":1739740067,"author":"Daniel J. Saldaña","guid":787,"unread":true,"content":"<p>Este tutorial explica la configuración básica de un endpoint (API) en Node.js y un componente controlador en React, que te permitirán:</p><ul><li> grupos de recursos de Azure por  (workspace o projectId).</li><li> recomendaciones de Azure Advisor y agruparlas por grupo de recursos.</li><li> los resultados con MongoDB.</li><li> las recomendaciones en tiempo real con paginación y filtrado.</li></ul><h2>\n  \n  \n  1. Descripción de la funcionalidad\n</h2><ol><li>Validar los parámetros (organization, workspace o projectId).</li><li>Conectarse a  usando tus credenciales y obtener:\n\n<ul><li> filtrados por etiquetas (workspace o projectId),</li><li> de Azure Advisor asociadas.</li></ul></li><li>Almacenar en MongoDB los resultados para que, si se piden otra vez, no sea necesario llamar de nuevo al servicio de Azure.</li></ol><p>El controlador de React, por su parte, consume el endpoint y muestra, en una interfaz amigable, las recomendaciones paginadas, categorizadas y con indicadores de impacto (High, Medium, Low).</p><h2>\n  \n  \n  2. Configuración de las credenciales de azure\n</h2><p>Para utilizar los SDK de Azure (por ejemplo, , , ), necesitas:</p><ul><li> de tu Directorio de Azure AD.</li><li> de la aplicación registrada en Azure AD.</li><li> que se genera para esa aplicación.</li></ul><p>Asegúrate de definirlos en tus variables de entorno como:</p><div><pre><code>AZURE_TENANT_ID=...\nAZURE_CLIENT_ID=...\nAZURE_CLIENT_SECRET=...\n\n</code></pre></div><p>Luego, en tu código, puedes leerlas y crear un :</p><div><pre><code>const TENANT_ID = import.meta.env.AZURE_TENANT_ID as string;\nconst CLIENT_ID = import.meta.env.AZURE_CLIENT_ID as string;\nconst CLIENT_SECRET = import.meta.env.AZURE_CLIENT_SECRET as string;\n\nconst credential = new ClientSecretCredential(TENANT_ID, CLIENT_ID, CLIENT_SECRET);\n\n</code></pre></div><h2>\n  \n  \n  3. Endpoint de api para consultar las recomendaciones\n</h2><p>A continuación, se muestra un ejemplo de un endpoint REST (solo maneja método GET) que:</p><ol><li> el uso de métodos y los parámetros necesarios.</li><li> datos en MongoDB (para cachear las recomendaciones).</li><li> por cada suscripción y, dentro de esta, por cada grupo de recursos que coincida con las  requeridas.</li><li> las recomendaciones de Azure Advisor y las filtra para cada grupo de recursos.</li><li> los resultados en un objeto JSON.\n</li></ol><div><pre><code>import { methodValidator } from '../../utils/methodValidator';\nimport { log } from '../../utils/logging';\nimport { ClientSecretCredential } from '@azure/identity';\nimport { SubscriptionClient } from '@azure/arm-subscriptions';\nimport { ResourceManagementClient } from '@azure/arm-resources';\nimport { AdvisorManagementClient } from '@azure/arm-advisor';\nimport getDatabase from '../../utils/mongoClient';\n\nconst TENANT_ID = import.meta.env.AZURE_TENANT_ID as string;\nconst CLIENT_ID = import.meta.env.AZURE_CLIENT_ID as string;\nconst CLIENT_SECRET = import.meta.env.AZURE_CLIENT_SECRET as string;\n\nconst credential = new ClientSecretCredential(TENANT_ID, CLIENT_ID, CLIENT_SECRET);\n\nconst getSubscriptions = async () =&gt; {\n  // ...\n};\n\nconst getResourceGroups = async (subscriptionId: string, requiredTags: Record&lt;string, string&gt;) =&gt; {\n  // ...\n};\n\nconst getAdvisorRecommendations = async (subscriptionId: string) =&gt; {\n  // ...\n};\n\nconst filterRecommendationsByResourceGroup = (recommendations: any[], resourceGroupName: string) =&gt; {\n  // ...\n};\n\nconst extractRecommendationDetails = (recommendations: any[]) =&gt; {\n  // ...\n};\n\nexport const GET = async ({ request }: { request: Request }) =&gt; {\n  const url = new URL(request.url);\n  const organization = url.searchParams.get('organization');\n  const workspace = url.searchParams.get('workspace');\n  const projectId = url.searchParams.get('projectId');\n\n  // Lógica:\n  // 1. Validar método y parámetros (organization, workspace o projectId).\n  // 2. Conectarse a la base de datos, verificar si hay resultados cacheados.\n  // 3. Obtener suscripciones, filtrar grupos de recursos, buscar recomendaciones.\n  // 4. Almacenar en caché si es la primera vez.\n  // 5. Responder en formato JSON al cliente.\n};\n\n</code></pre></div><p>En el código completo (que se muestra en tu ejemplo) puedes ver con detalle cada uno de los pasos (almacenamiento en , uso de ,  y ).</p><h2>\n  \n  \n  4. Controlador de react para mostrar recomendaciones\n</h2><p>Este componente, que podrías llamar , es un ejemplo de cómo  y desplegar la información en una tabla, incluyendo:</p><ul><li> por categoría (cost, security, highAvailability, etc.).</li><li> (página actual, total de páginas, ítems por página).</li><li> para el impacto (High, Medium, Low).</li><li> detallada de cada recomendación (problema y solución).\n</li></ul><div><pre><code>import React, { useEffect, useState } from 'react';\nimport { FaSpinner, FaChevronLeft, FaChevronRight } from 'react-icons/fa';\n\nfunction RecommendationDescription({ shortDescription }) {\n  // ...\n}\n\nexport default function AzureAdvisorController({ organization, workspace = null, projectId = null }) {\n  // 1. Estados (loading, error, recomendaciones).\n  // 2. Llamada a la API (useEffect).\n  // 3. Procesamiento de datos (filtrado, paginación, conteo de categorías).\n  // 4. Render de la tabla, paginación y controles de filtrado.\n}\n\n</code></pre></div><p>En el hook , se arma la URL en función de los parámetros y se hace  al endpoint:</p><div><pre><code>useEffect(() =&gt; {\n  let url = `/api/private/azure/advisor?organization=${encodeURIComponent(organization)}`;\n  if (workspace) {\n    url += `&amp;workspace=${encodeURIComponent(workspace)}`;\n  } else if (projectId) {\n    url += `&amp;projectId=${encodeURIComponent(projectId)}`;\n  }\n\n  fetch(url)\n    .then((resp) =&gt; resp.json().then((data) =&gt; [resp.ok, data]))\n    .then(([ok, data]) =&gt; {\n      if (!ok) throw new Error(data.error || 'Error fetching Advisor data');\n      // Procesar resultados y setear estado\n    })\n    .catch((err) =&gt; {\n      setError(err.message);\n      setLoading(false);\n    });\n}, [organization, workspace, projectId]);\n\n</code></pre></div><ul><li>Muestra un  si está cargando.</li><li>Muestra un  si falla la petición.</li><li>En caso de éxito, presenta una  con la información: categoría, impacto, campo afectado, valor afectado, grupo de recursos y fecha de la última actualización.</li></ul><h2>\n  \n  \n  5. Integración en tu sistema\n</h2><p>Para usar esta lógica en tu propio producto o aplicación:</p><ol><li><p> la estructura del endpoint de Node.js y ajusta:</p></li><li><p> el componente de React () en tu interfaz:</p></li><li><p> las tablas y la barra de impacto en el frontend:</p></li></ol><p>Este ejemplo demuestra cómo integrar las recomendaciones de  en una aplicación real, utilizando  para la comunicación con la API de Azure y  para la presentación de los datos. Se trata de una  que te permitirá centralizar todas las recomendaciones de tus recursos en la nube, filtrarlas y mostrarlas a tu equipo de forma clara y eficiente.</p><p>Si bien esta solución está incluida en un producto en desarrollo propio, <strong>puedes adaptar libremente</strong> tanto el endpoint como el controlador para ajustarlos a tu entorno. Con esta base, tendrás una vista consolidada de las optimizaciones que Azure Advisor ofrece, maximizando la confiabilidad, seguridad y rentabilidad de tus recursos en la nube.</p>","contentLength":6728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Visualización de costos de Azure en Astro: una solución práctica","url":"https://dev.to/danieljsaldana/visualizacion-de-costos-de-azure-en-astro-una-solucion-practica-40g6","date":1739740023,"author":"Daniel J. Saldaña","guid":786,"unread":true,"content":"<p>Durante el desarrollo del proyecto , una herramienta de código abierto para gestionar y optimizar despliegues de  , surgió una necesidad específica: <strong>identificar los costos asociados a recursos en Azure que estén etiquetados</strong> con información específica, como el nombre de una organización o un identificador de proyecto.</p><p>Las etiquetas en Azure permiten clasificar recursos, lo que facilita la administración de costos por equipo, proyecto o entorno. Para aprovechar esta capacidad, desarrollé una solución que permite:</p><ol><li><strong>Buscar resource groups (RG) que contienen una etiqueta específica</strong>.</li><li><strong>Obtener los costos de esos resource groups</strong>.</li><li> segmentados por grupo de recursos, ubicación y nivel de servicio.</li></ol><p>Esta solución se compone de:</p><ol><li> para consultar Azure y devolver datos de costos filtrados por etiquetas.</li><li> para visualizar los datos de manera clara y segmentada.</li></ol><h5>\n  \n  \n  ¿Qué ofrece esta solución?\n</h5><ul><li> : identifica y analiza solo los resource groups que contienen una etiqueta específica.</li><li> : consulta actualizada directamente desde Azure.</li><li> : muestra gráficos segmentados por:\n\n<ul><li>Nivel de servicio ().</li></ul></li><li> : guarda resultados en MongoDB para evitar consultas repetidas y mejorar el rendimiento.</li></ul><h5>\n  \n  \n  1. Crear el endpoint en Astro\n</h5><p>El endpoint se conecta a las APIs de Azure y realiza lo siguiente:</p><ol><li><strong>Busca todos los resource groups</strong> dentro de las suscripciones disponibles.</li><li><strong>Filtra aquellos resource groups que contengan una etiqueta específica</strong> , como ,  o .</li><li><strong>Obtiene los costos asociados a esos resource groups</strong> utilizando el servicio de cost management de Azure.</li><li><strong>Devuelve los datos en formato JSON</strong> para que puedan ser consumidos por el componente React.</li></ol><h5>\n  \n  \n  ¿Dónde crear el endpoint?\n</h5><ol><li>Crea un archivo en la siguiente ruta de tu proyecto Astro:\n</li></ol><div><pre><code>import { methodValidator } from \"../utils/methodValidator\";\nimport { log } from \"../utils/logging\";\nimport { ClientSecretCredential } from \"@azure/identity\";\nimport { SubscriptionClient } from \"@azure/arm-subscriptions\";\nimport { ResourceManagementClient } from \"@azure/arm-resources\";\nimport { CostManagementClient } from \"@azure/arm-costmanagement\";\nimport getDatabase from \"../utils/mongoClient\";\n\nconst TENANT_ID = import.meta.env.AZURE_TENANT_ID as string;\nconst CLIENT_ID = import.meta.env.AZURE_CLIENT_ID as string;\nconst CLIENT_SECRET = import.meta.env.AZURE_CLIENT_SECRET as string;\n\nconst credential = new ClientSecretCredential(\n    TENANT_ID,\n    CLIENT_ID,\n    CLIENT_SECRET,\n);\n\nconst getSubscriptions = async () =&gt; {\n    log(\"Fetching subscriptions...\", \"DEBUG\");\n    const client = new SubscriptionClient(credential);\n    const subscriptions = [];\n    for await (const subscription of client.subscriptions.list()) {\n        subscriptions.push(subscription);\n    }\n    log(`Fetched ${subscriptions.length} subscriptions.`, \"DEBUG\");\n    return subscriptions;\n};\n\nconst getResourceGroups = async (subscriptionId: string) =&gt; {\n    log(`Fetching resource groups for subscription: ${subscriptionId}`, \"DEBUG\");\n    const client = new ResourceManagementClient(credential, subscriptionId);\n    const resourceGroups = [];\n    for await (const rg of client.resourceGroups.list()) {\n        resourceGroups.push(rg);\n    }\n    log(\n        `Fetched ${resourceGroups.length} resource groups for subscription ${subscriptionId}.`,\n        \"DEBUG\",\n    );\n    return resourceGroups;\n};\n\nconst getCostData = async (\n    subscriptionId: string,\n    resourceGroupName: string,\n) =&gt; {\n    log(\n        `Fetching cost data for resource group: ${resourceGroupName} in subscription: ${subscriptionId}`,\n        \"DEBUG\",\n    );\n    const costClient = new CostManagementClient(credential);\n    const result = await costClient.query.usage(\n        `/subscriptions/${subscriptionId}/resourceGroups/${resourceGroupName}`,\n        {\n            type: \"ActualCost\",\n            timeframe: \"BillingMonth\",\n            dataset: {\n                granularity: \"None\",\n                aggregation: {\n                    totalCost: {\n                        name: \"PreTaxCost\",\n                        function: \"Sum\",\n                    },\n                },\n                grouping: [\n                    {\n                        type: \"Dimension\",\n                        name: \"ServiceName\",\n                    },\n                    {\n                        type: \"Dimension\",\n                        name: \"ResourceLocation\",\n                    },\n                    {\n                        type: \"Dimension\",\n                        name: \"MeterSubCategory\",\n                    },\n                ],\n            },\n        },\n    );\n    log(\n        `Fetched cost data for resource group ${resourceGroupName}: ${JSON.stringify(result)}`,\n        \"DEBUG\",\n    );\n    return result;\n};\n\nexport const GET = async ({ request }: { request: Request }) =&gt; {\n    const url = new URL(request.url);\n    const organization = url.searchParams.get(\"organization\");\n    const workspace = url.searchParams.get(\"workspace\");\n    const projectId = url.searchParams.get(\"projectId\");\n\n    log(\"Received GET request.\", \"DEBUG\");\n\n    if (!methodValidator(request, [\"GET\"])) {\n        log(\"Invalid method used. Only GET is allowed.\", \"ERROR\");\n        return new Response(JSON.stringify({ error: \"Method Not Allowed\" }), {\n            status: 405,\n            headers: { \"Content-Type\": \"application/json\" },\n        });\n    }\n\n    if (!organization) {\n        log(\"Organization parameter is missing.\", \"ERROR\");\n        return new Response(\n            JSON.stringify({ error: \"Organization is required.\" }),\n            {\n                status: 400,\n                headers: { \"Content-Type\": \"application/json\" },\n            },\n        );\n    }\n\n    if ((workspace &amp;&amp; projectId) || (!workspace &amp;&amp; !projectId)) {\n        log(\n            \"Invalid parameters: Provide either workspace or projectId, but not both.\",\n            \"ERROR\",\n        );\n        return new Response(\n            JSON.stringify({\n                error: \"Provide either workspace or projectId, but not both.\",\n            }),\n            {\n                status: 400,\n                headers: { \"Content-Type\": \"application/json\" },\n            },\n        );\n    }\n\n    try {\n        log(\"Checking database connection\", \"DEBUG\");\n        const db = await getDatabase();\n        const identifier = workspace || projectId || \"\";\n\n        if (db) {\n            log(\n                `Checking cached data for organization: ${organization}, identifier: ${identifier}`,\n                \"DEBUG\",\n            );\n            const collection = db.collection(\"azure_cost_data\");\n            const cachedData = await collection.findOne({\n                organization,\n                identifier,\n            });\n            if (cachedData) {\n                log(\n                    `Returning cached data for organization: ${organization}, identifier: ${identifier}`,\n                    \"DEBUG\",\n                );\n                return new Response(JSON.stringify(cachedData), {\n                    status: 200,\n                    headers: { \"Content-Type\": \"application/json\" },\n                });\n            }\n        } else {\n            log(\"Database not available, proceeding without caching.\", \"DEBUG\");\n        }\n\n        log(\"Fetching subscriptions...\", \"DEBUG\");\n        const subscriptions = await getSubscriptions();\n\n        const results: Record&lt;\n            string,\n            {\n                organization: string;\n                identifier: string;\n                totalAllRGs?: string;\n                totalAllRGsByLocation?: Record&lt;string, string&gt;;\n                totalAllRGsByTier?: Record&lt;string, string&gt;;\n                resourceGroups: Array&lt;{\n                    resourceGroupName: string;\n                    totalCost: string;\n                    costByCategory: Record&lt;string, string&gt;;\n                    costByLocation: Record&lt;string, string&gt;;\n                    costByTier: Record&lt;string, string&gt;;\n                }&gt;;\n            }\n        &gt; = {};\n\n        results[identifier] = { organization, identifier, resourceGroups: [] };\n\n        let totalCostSum = 0;\n        let costCurrency: string | null = null;\n        const totalCostByLocation: Record&lt;string, number&gt; = {};\n        const totalCostByTier: Record&lt;string, number&gt; = {};\n\n        for (const subscription of subscriptions) {\n            const subscriptionId = subscription.subscriptionId;\n            if (!subscriptionId) {\n                log(\"Subscription ID is undefined.\", \"ERROR\");\n                continue;\n            }\n            log(`Processing subscription: ${subscriptionId}`, \"DEBUG\");\n            const resourceGroups = await getResourceGroups(subscriptionId);\n\n            const matchingResourceGroups = resourceGroups.filter((rg: any) =&gt; {\n                const tags = rg.tags || {};\n                return (\n                    tags.organization === organization &amp;&amp;\n                    ((workspace &amp;&amp; tags.workspace === workspace) ||\n                        (projectId &amp;&amp; tags.projectId === projectId))\n                );\n            });\n            log(\n                `Found ${matchingResourceGroups.length} matching resource groups.`,\n                \"DEBUG\",\n            );\n\n            for (const rg of matchingResourceGroups) {\n                try {\n                    const resourceGroupName = rg.name;\n                    if (!resourceGroupName) {\n                        log(\n                            `Resource group name is undefined for subscription ${subscriptionId}`,\n                            \"ERROR\",\n                        );\n                        continue;\n                    }\n                    const costData = await getCostData(subscriptionId, resourceGroupName);\n                    log(\n                        `Cost data response for ${resourceGroupName}: ${JSON.stringify(costData)}`,\n                        \"DEBUG\",\n                    );\n                    if (!costData || !costData.columns || !costData.rows) {\n                        log(\"No columns or rows in cost data result.\", \"ERROR\");\n                        continue;\n                    }\n                    const costIndex = costData.columns.findIndex(\n                        (col: any) =&gt; col.name === \"PreTaxCost\",\n                    );\n                    const serviceNameIndex = costData.columns.findIndex(\n                        (col: any) =&gt; col.name === \"ServiceName\",\n                    );\n                    const locationIndex = costData.columns.findIndex(\n                        (col: any) =&gt; col.name === \"ResourceLocation\",\n                    );\n                    const tierIndex = costData.columns.findIndex(\n                        (col: any) =&gt; col.name === \"MeterSubCategory\",\n                    );\n                    const currencyIndex = costData.columns.findIndex(\n                        (col: any) =&gt; col.name === \"Currency\",\n                    );\n\n                    let rgTotalCost = 0;\n                    let currency = \"\";\n                    const costByCategory: Record&lt;string, number&gt; = {};\n                    const costByLocation: Record&lt;string, number&gt; = {};\n                    const costByTier: Record&lt;string, number&gt; = {};\n\n                    for (const row of costData.rows) {\n                        const cost = row[costIndex];\n                        const serviceName = row[serviceNameIndex];\n                        const resourceLocation = row[locationIndex];\n                        const tier = row[tierIndex] || \"No Tier Info\";\n                        currency = row[currencyIndex];\n                        rgTotalCost += cost;\n                        costByCategory[serviceName] =\n                            (costByCategory[serviceName] || 0) + cost;\n                        costByLocation[resourceLocation] =\n                            (costByLocation[resourceLocation] || 0) + cost;\n                        costByTier[tier] = (costByTier[tier] || 0) + cost;\n                    }\n\n                    if (!costCurrency) {\n                        costCurrency = currency;\n                    }\n\n                    totalCostSum += rgTotalCost;\n\n                    for (const [location, cost] of Object.entries(costByLocation)) {\n                        totalCostByLocation[location] =\n                            (totalCostByLocation[location] || 0) + cost;\n                    }\n\n                    for (const [tier, cost] of Object.entries(costByTier)) {\n                        totalCostByTier[tier] = (totalCostByTier[tier] || 0) + cost;\n                    }\n\n                    const formattedTotalCost = new Intl.NumberFormat(\"es-ES\", {\n                        style: \"currency\",\n                        currency: currency,\n                    }).format(rgTotalCost);\n\n                    const formattedCostByCategory: Record&lt;string, string&gt; = {};\n                    for (const [serviceName, cost] of Object.entries(costByCategory)) {\n                        formattedCostByCategory[serviceName] = new Intl.NumberFormat(\n                            \"es-ES\",\n                            {\n                                style: \"currency\",\n                                currency: currency,\n                            },\n                        ).format(cost);\n                    }\n\n                    const formattedCostByLocation: Record&lt;string, string&gt; = {};\n                    for (const [location, cost] of Object.entries(costByLocation)) {\n                        formattedCostByLocation[location] = new Intl.NumberFormat(\"es-ES\", {\n                            style: \"currency\",\n                            currency: currency,\n                        }).format(cost);\n                    }\n\n                    const formattedCostByTier: Record&lt;string, string&gt; = {};\n                    for (const [tier, cost] of Object.entries(costByTier)) {\n                        formattedCostByTier[tier] = new Intl.NumberFormat(\"es-ES\", {\n                            style: \"currency\",\n                            currency: currency,\n                        }).format(cost);\n                    }\n\n                    results[identifier].resourceGroups.push({\n                        resourceGroupName,\n                        totalCost: formattedTotalCost,\n                        costByCategory: formattedCostByCategory,\n                        costByLocation: formattedCostByLocation,\n                        costByTier: formattedCostByTier,\n                    });\n                } catch (error) {\n                    log(`Error processing resource group ${rg.name}: ${error}`, \"ERROR\");\n                }\n            }\n        }\n\n        if (results[identifier].resourceGroups.length === 0) {\n            log(\"No matching resource groups found.\", \"ERROR\");\n            return new Response(\n                JSON.stringify({ error: \"No matching resource groups found.\" }),\n                {\n                    status: 404,\n                    headers: { \"Content-Type\": \"application/json\" },\n                },\n            );\n        }\n\n        if (costCurrency) {\n            results[identifier].totalAllRGs = new Intl.NumberFormat(\"es-ES\", {\n                style: \"currency\",\n                currency: costCurrency,\n            }).format(totalCostSum);\n\n            const formattedTotalAllRGsByLocation: Record&lt;string, string&gt; = {};\n            for (const [location, cost] of Object.entries(totalCostByLocation)) {\n                formattedTotalAllRGsByLocation[location] = new Intl.NumberFormat(\n                    \"es-ES\",\n                    {\n                        style: \"currency\",\n                        currency: costCurrency,\n                    },\n                ).format(cost);\n            }\n            results[identifier].totalAllRGsByLocation =\n                formattedTotalAllRGsByLocation;\n\n            const formattedTotalAllRGsByTier: Record&lt;string, string&gt; = {};\n            for (const [tier, cost] of Object.entries(totalCostByTier)) {\n                formattedTotalAllRGsByTier[tier] = new Intl.NumberFormat(\"es-ES\", {\n                    style: \"currency\",\n                    currency: costCurrency,\n                }).format(cost);\n            }\n            results[identifier].totalAllRGsByTier = formattedTotalAllRGsByTier;\n        }\n\n        if (db) {\n            log(\n                `Caching data for organization: ${organization}, identifier: ${identifier}`,\n                \"DEBUG\",\n            );\n            const collection = db.collection(\"azure_cost_data\");\n            await collection.updateOne(\n                { organization, identifier },\n                { $set: results[identifier] },\n                { upsert: true },\n            );\n        }\n\n        return new Response(JSON.stringify(results[identifier]), {\n            status: 200,\n            headers: { \"Content-Type\": \"application/json\" },\n        });\n    } catch (error: unknown) {\n        log(\n            `Internal server error: ${error instanceof Error ? error.stack : String(error)}`,\n            \"ERROR\",\n        );\n        return new Response(JSON.stringify({ error: \"Internal server error\" }), {\n            status: 500,\n            headers: { \"Content-Type\": \"application/json\" },\n        });\n    }\n};\n\n</code></pre></div><ol><li><p><strong>Copia el código del endpoint</strong> en ese archivo. Este código manejará la búsqueda de resource groups etiquetados y la consulta de costos a Azure.</p></li><li><p><strong>Configura las variables de entorno</strong> en el archivo :</p></li><li><p><strong>Instala las dependencias necesarias</strong> para conectar con Azure y MongoDB:</p></li></ol><h5>\n  \n  \n  2. Crear el componente en React\n</h5><p>El componente React se encarga de:</p><ol><li> para obtener los costos de los resource groups filtrados por etiquetas.</li><li><strong>Mostrar el estado de carga y los posibles errores</strong>.</li><li><strong>Visualizar los datos en gráficos tipo </strong> usando  :\n\n<ul><li>Costos por resource group.</li><li>Costos por nivel de servicio ().</li></ul></li></ol><h5>\n  \n  \n  ¿Dónde crear el componente?\n</h5><ol><li>Crea el archivo del componente en:\n</li></ol><div><pre><code>import React, { useEffect, useState } from \"react\";\nimport { Doughnut } from \"react-chartjs-2\";\nimport { FaSpinner } from \"react-icons/fa\";\nimport { Chart as ChartJS, ArcElement, Tooltip, Legend } from \"chart.js\";\n\nChartJS.register(ArcElement, Tooltip, Legend);\n\nexport default function WorkspaceCostsController({ organization, workspace }) {\n    const [data, setData] = useState(null);\n    const [loading, setLoading] = useState(true);\n    const [error, setError] = useState(null);\n\n    const fetchData = async () =&gt; {\n        setLoading(true);\n        setError(null);\n        try {\n            const url = `/api/private/azureCost?organization=${organization}&amp;workspace=${workspace}`;\n            const response = await fetch(url);\n            if (!response.ok) {\n                const errorText = await response.text();\n                throw new Error(`Error fetching data: ${errorText}`);\n            }\n            const jsonData = await response.json();\n            setData(jsonData);\n        } catch (err) {\n            setError(err.message);\n        } finally {\n            setLoading(false);\n        }\n    };\n\n    useEffect(() =&gt; {\n        fetchData();\n    }, [organization, workspace]);\n\n    if (loading) {\n        return (\n            &lt;div className=\"flex justify-center items-center h-64\"&gt;\n                &lt;FaSpinner className=\"animate-spin text-2xl text-gray-900 dark:text-white\" /&gt;\n            &lt;/div&gt;\n        );\n    }\n\n    if (error) {\n        return &lt;p className=\"text-red-500\"&gt;Error: {error}&lt;/p&gt;;\n    }\n\n    if (!data) {\n        return (\n            &lt;p className=\"text-gray-500 dark:text-gray-400\"&gt;No data available&lt;/p&gt;\n        );\n    }\n\n    const {\n        totalAllRGs,\n        totalAllRGsByLocation,\n        totalAllRGsByTier,\n        resourceGroups,\n    } = data;\n\n    const isDarkMode = document.documentElement.classList.contains(\"dark\");\n    const textColor = isDarkMode ? \"#F9FAFB\" : \"#1F2937\";\n    const borderColor = isDarkMode ? \"#4B5563\" : \"#D1D5DB\";\n\n    const parseEuroValue = (val) =&gt; {\n        if (!val) return 0;\n        let numericString = val.replace(/[^\\d.,-]/g, \"\");\n        numericString = numericString.replace(\",\", \".\");\n        const numberValue = parseFloat(numericString);\n        return isNaN(numberValue) ? 0 : numberValue;\n    };\n\n    const formatEuro = (value) =&gt; {\n        return new Intl.NumberFormat(\"es-ES\", {\n            style: \"currency\",\n            currency: \"EUR\",\n        }).format(value);\n    };\n\n    const generateRandomPastelColor = () =&gt; {\n        const hue = Math.floor(Math.random() * 360);\n        const base = `hsl(${hue}, 70%, 80%)`;\n        const hover = `hsl(${hue}, 70%, 60%)`;\n        return { base, hover };\n    };\n\n    const generateDoughnutData = (obj) =&gt; {\n        const entries = Object.entries(obj || {});\n        const filteredEntries = entries.filter(\n            ([_, val]) =&gt; parseEuroValue(val) !== 0,\n        );\n\n        if (filteredEntries.length === 0) {\n            return null;\n        }\n\n        const labels = filteredEntries.map(([key]) =&gt; key);\n        const values = filteredEntries.map(([_, val]) =&gt; parseEuroValue(val));\n\n        const backgroundColors = [];\n        const hoverBackgroundColors = [];\n\n        labels.forEach(() =&gt; {\n            const { base, hover } = generateRandomPastelColor();\n            backgroundColors.push(base);\n            hoverBackgroundColors.push(hover);\n        });\n\n        return {\n            labels,\n            datasets: [\n                {\n                    data: values,\n                    backgroundColor: backgroundColors,\n                    hoverBackgroundColor: hoverBackgroundColors,\n                    borderColor: borderColor,\n                    borderWidth: 2,\n                },\n            ],\n        };\n    };\n\n    const locationData = totalAllRGsByLocation\n        ? generateDoughnutData(totalAllRGsByLocation)\n        : null;\n    const tierData = totalAllRGsByTier\n        ? generateDoughnutData(totalAllRGsByTier)\n        : null;\n\n    let rgData = null;\n    if (resourceGroups &amp;&amp; resourceGroups.length &gt; 0) {\n        const rgObj = {};\n        resourceGroups.forEach((rg) =&gt; {\n            if (rg.totalCost) {\n                rgObj[rg.resourceGroupName] = rg.totalCost;\n            }\n        });\n        rgData = generateDoughnutData(rgObj);\n    }\n\n    const doughnutOptions = {\n        responsive: true,\n        maintainAspectRatio: false,\n        layout: {\n            padding: {\n                top: 10,\n                bottom: 10,\n            },\n        },\n        plugins: {\n            legend: {\n                position: \"bottom\",\n                labels: {\n                    color: textColor,\n                },\n            },\n            tooltip: {\n                bodyColor: textColor,\n                titleColor: textColor,\n                backgroundColor: isDarkMode ? \"#374151\" : \"#ffffff\",\n                callbacks: {\n                    label: function (context) {\n                        const label = context.label || \"\";\n                        const value = context.parsed;\n                        return `${label}: ${formatEuro(value)}`;\n                    },\n                },\n            },\n        },\n    };\n\n    return (\n        &lt;div className=\"mt-6\"&gt;\n            &lt;div className=\"grid w-full grid-cols-1 gap-4 xl:grid-cols-3\"&gt;\n                &lt;div className=\"relative p-4 bg-white border border-gray-200 rounded-lg shadow dark:border-gray-700 dark:bg-gray-800 flex flex-col\"&gt;\n                    &lt;h3 className=\"text-lg font-medium text-gray-900 dark:text-white mb-2\"&gt;\n                        Cost by Resource Group\n                    &lt;/h3&gt;\n                    &lt;p className=\"text-sm text-gray-600 dark:text-gray-400 mb-4\"&gt;\n                        Cost breakdown by each resource group.\n                    &lt;/p&gt;\n                    &lt;div className=\"h-64\"&gt;\n                        {rgData ? (\n                            &lt;Doughnut data={rgData} options={doughnutOptions} /&gt;\n                        ) : (\n                            &lt;p className=\"text-gray-500 dark:text-gray-400\"&gt;No data&lt;/p&gt;\n                        )}\n                    &lt;/div&gt;\n                &lt;/div&gt;\n\n                &lt;div className=\"relative p-4 bg-white border border-gray-200 rounded-lg shadow dark:border-gray-700 dark:bg-gray-800 flex flex-col\"&gt;\n                    &lt;h3 className=\"text-lg font-medium text-gray-900 dark:text-white mb-2\"&gt;\n                        Cost by Location\n                    &lt;/h3&gt;\n                    &lt;p className=\"text-sm text-gray-600 dark:text-gray-400 mb-4\"&gt;\n                        Cost breakdown by resource region.\n                    &lt;/p&gt;\n                    &lt;div className=\"h-64\"&gt;\n                        {locationData ? (\n                            &lt;Doughnut data={locationData} options={doughnutOptions} /&gt;\n                        ) : (\n                            &lt;p className=\"text-gray-500 dark:text-gray-400\"&gt;No data&lt;/p&gt;\n                        )}\n                    &lt;/div&gt;\n                &lt;/div&gt;\n\n                &lt;div className=\"relative p-4 bg-white border border-gray-200 rounded-lg shadow dark:border-gray-700 dark:bg-gray-800 flex flex-col\"&gt;\n                    &lt;h3 className=\"text-lg font-medium text-gray-900 dark:text-white mb-2\"&gt;\n                        Cost by Tier\n                    &lt;/h3&gt;\n                    &lt;p className=\"text-sm text-gray-600 dark:text-gray-400 mb-4\"&gt;\n                        Cost breakdown by service tier.\n                    &lt;/p&gt;\n                    &lt;div className=\"h-64\"&gt;\n                        {tierData ? (\n                            &lt;Doughnut data={tierData} options={doughnutOptions} /&gt;\n                        ) : (\n                            &lt;p className=\"text-gray-500 dark:text-gray-400\"&gt;No data&lt;/p&gt;\n                        )}\n                    &lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    );\n}\n\n</code></pre></div><ol><li><p><strong>Copia el código del componente</strong> en ese archivo.</p></li><li><p>Instala las dependencias necesarias para los gráficos y los íconos de carga:</p></li></ol><h5>\n  \n  \n  3. Integrar el componente en una página de Astro\n</h5><p>Para mostrar el dashboard de costos en tu aplicación Astro:</p><ol><li><p>Crea una página Astro en:</p></li><li><p>Importa el componente y pásale los parámetros necesarios, como  y :</p></li></ol><h5>\n  \n  \n  4. Probar la implementación\n</h5><ol><li><p>Inicia el servidor de desarrollo con el siguiente comando:</p></li><li><p>Accede a la página en tu navegador, por ejemplo:</p></li><li><p>Deberías ver los gráficos que muestran los costos de los resource groups filtrados por la etiqueta especificada.</p></li></ol><p>Esta solución permite monitorear y visualizar los costos de Azure de una manera eficiente, enfocándose en aquellos resource groups que cumplen con criterios específicos a través de etiquetas. Esto facilita la gestión de recursos y la optimización de costos en proyectos complejos.</p><p>Si te interesa una herramienta integral para gestionar tus despliegues de Terraform y visualizar costos en Azure, te invito a explorar el proyecto , una plataforma de código abierto diseñada para optimizar tus operaciones de infraestructura.</p>","contentLength":26833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding and Installing Terraform: A Beginner's Guide","url":"https://dev.to/tjasper/understanding-and-installing-terraform-a-beginners-guide-dca","date":1739739661,"author":"Oluwaloseyi Oluwatofunmi Emmanuel","guid":785,"unread":true,"content":"<ul><li>Step-by-Step Guide to Install Terraform on Windows</li></ul><p>Terraform by HashiCorp is an open-source <strong>infrastructure as code (IaC)</strong> tool that lets us define, manage, and automate cloud infrastructure using a declarative (human-readable) configuration language called <strong>HashiCorp Configuration Language (HCL)</strong> across multiple cloud providers like Amazon Web Services (AWS), Azure, Google Cloud, and even on-premises data centers.</p><h2>\n  \n  \n  Key Features of Terraform\n</h2><p><strong>1. Manage any infrastructure:</strong> Terraform can interact with cloud platforms and multiple providers such as AWS, Azure, Google Cloud, Kubernetes, VMware, GitHub, and many more to manage resources.</p><p><strong>2. Standardize your deployment workflow:</strong></p><ul><li><p> We Define infrastructure in  files.</p></li><li><p> Set up the working directory and download provider plugins by running .</p></li><li><p> Preview changes Terraform will make to match your configuration by running . This stimulates better control over your environment.</p></li><li><p> Create or update infrastructure as defined. By simply running , you will be able to make the planned changes.</p></li></ul><p> With Terraforms remote state backends, you can securely share your state with your teammates and provide a stable environment for Terraform to run in.</p><p> You can connect Terraform to version control systems (VCSs) like GitHub, GitLab, and others, allowing you to manage changes to your infrastructure through version control, as you would with application code.</p><h2>\n  \n  \n  Step-by-Step Guide to Install Terraform on Windows\n</h2><p>Setting up Terraform is straightforward. Follow the steps below to install it on your operating system.</p><ul><li>Open PowerShell on your windows PC.</li><li>Install the Chocolatey software on your system with the code below\n</li></ul><div><pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n</code></pre></div><ul><li>Run the command below to install Terraform using Chocolatey.</li><li>Then allow Chocolatey to run the installation by answering 'y'.\n</li></ul><p>After placing Terraform in the correct directory, verify the installation by running:</p><p>Understanding the fundamentals of Terraform and successfully installing it prepares you to begin managing infrastructure as code. Terraform's declarative approach, multi-cloud support, and automation capabilities make it an extremely useful tool for DevOps and cloud engineers. To fully realize Terraform's potential, experiment with writing configuration files and running deployments.</p><p><strong>Thank you for reading my blog.😊😊</strong></p><p>If you need further assistance, feel free to reach out in the comment or hit me up on <a href=\"https://x.com/seyi_jasper\" rel=\"noopener noreferrer\">Twitter</a>. You can also follow me on <a href=\"https://github.com/oluwaloseyiT\" rel=\"noopener noreferrer\">GitHub</a>. My DM's are open and love discussions on cloud topics!😁</p><p><strong>Oluwatofunmi Emmanuel Oluwaloseyi</strong></p>","contentLength":2775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Avoiding Pitfalls in the Race to Build Super Apps.","url":"https://dev.to/ivan_assenov_c6e899f61741/avoiding-pitfalls-in-the-race-to-build-super-apps-3pe0","date":1739738800,"author":"Ivan Assenov","guid":778,"unread":true,"content":"<p>Before diving into the pitfalls, one might ask:</p><h2><strong>Wait, what exactly is a super app?</strong></h2><p>To understand this, we need to rewind to the meteoric rise of . This little Canadian company, led by its visionary co-founder , made waves with an ambitious concept. In his 2010 keynote speech at the , Lazaridis coined the term Super App and described it as follows:</p><p>\"<em>A Super App for BlackBerry smartphones is an app that delivers a compelling user experience by leveraging the unique capabilities of the BlackBerry platform. Super Apps are the kind of apps that people love and use every day because they offer such a seamless, integrated, contextualized, and efficient experience.</em>\"</p><p>This vision planted the seed for what we now recognize as  that seamlessly integrate diverse services into one ecosystem, providing unmatched value to users.\nSource: <a href=\"https://www.rimarkable.com/mike-lazaridis-introduces-blackberry-super-apps/?utm_source=chatgpt.com\" rel=\"noopener noreferrer\">RIMarkable</a></p><p>In other words, to put it simply, a  is an app that becomes an inseparable part of your daily life. Wherever you go, you use it—not because someone tells you to, but because it’s convenient. It has everything you need, exactly where you want it. It does everything you want, and most importantly, it works .</p><p>Sounds simple, right? . Building a super app has proven to be one of the most challenging feats in modern app development. While the concept seems straightforward, the reality is far more complex. Today, there are only a handful of true super apps, and they primarily fall into one category: <strong>social messaging platforms integrated with mobile payment systems</strong>.</p><p>Imagine a restaurant serving super apps on its menu. Surprisingly, they would look almost identical: instant messaging, short social interactions, and an integrated mobile payment system—often built and managed by the .</p><p>Another notable example is  (formerly Twitter) and its aspirations to become a super app. With approximately <strong>600 million monthly active users</strong>, X is making a determined push into the super app market. However, key ingredients are still missing: its  and a significantly improved . The rest? They already have it.</p><p>You get the idea: <strong>there’s nothing particularly groundbreaking about super apps so far</strong>. They seem tailor-made for , and perhaps a better name for them would be “”.</p><h2>\n  \n  \n  At their core, the formula is simple: Social app + Wallet = Super App.\n</h2><p>That equation is also where the risk lies: <strong>losing the next 20 years of mobile app development</strong> to a narrow, oversimplified idea of what a super app should be.\nThe argument here is that super apps may have <strong>nothing to do with social apps</strong> at all. In fact, they may—or may not—have only minimal ties to . If we look closer, this actually describes <strong>most modern business platforms today</strong>.</p><h2>\n  \n  \n  We all need a wallet, right? Until we don’t.\n</h2><p>But let’s take this one step at a time.\nWhile every person is unique, people also share common —sometimes so much so that entire groups can exhibit <strong>overlapping behaviors and needs</strong>.\nNow, imagine a workplace: a company juggling <strong>multiple apps, multiple tools, and endless systems</strong>. They just keep coming—each with its own security requirements, updates, and lost passwords. It’s a logistical nightmare.</p><p>Or take a hobby group, for example—whether it’s biking or skiing—where people share everything about their activities. Building a super mobile app tailored specifically for these groups, for their needs, seems like a logical idea, right?</p><h2>\n  \n  \n  But is anyone actually doing it?\n</h2><p>The reality is that creating and supporting such an app with  is extremely expensive. It’s a financial gamble that could <strong>bankrupt even the most ambitious developer</strong>.\nI still remember sitting in a company meeting where the idea of a super app began to float around. Everyone debated what it would include, feature by feature. Then, at the very end, my boss spoke up with a single question:</p><p>Very expensive. And a super app? It must be , right? After all, if it weren’t, <strong>someone would have built it already</strong>.</p><p>How about no?\nWhat if we flip the script and say, no, it doesn’t have to be that way?</p><p>What if we introduce the <a href=\"https://acenji.com\" rel=\"noopener noreferrer\">NoCode Super App Builder</a>, a tool that empowers anyone to create an <strong>endless number of super apps</strong>? A platform where users can  from an <strong>unlimited library of mini apps</strong>—the very tools and services already known in the market.</p><p>And because everything operates under the —on the same device, using the same protocols—building, distributing, and adjusting a  becomes easy.\nUsers can  which mini apps to upload and use, but here’s the key difference: these mini apps won’t be like the apps we know today. They can , and the user—or the app owner—can decide  that data flows and  it is shared.\nNow, we’re talking about a —working together seamlessly, under the same protocols, on the same device, using shared data.\nWho’s still talking about  or ? That’s yesterday’s conversation. We’re talking about the —a vast world of possibilities, all available and ready to use.</p><p>The answer lies in —and it has to be . Why? Because the moment we introduce humans with their low-code injections, we open the door to inconsistency. It will <strong>never work everywhere, every time</strong>. Bugs will appear, and it will collapse—just like BlackBerry did.\nFor this to succeed, it  be entirely . And over time, it must be .\nAt first, AI might handle  of the build, then , but I envision a future where  is driven by AI. A world where super apps are created, maintained, and scaled—seamlessly and autonomously.</p><h2>\n  \n  \n  Super App Builder Components\n</h2><p>So, what are the key components that this  (SAB) would include? with a library of around <strong>NoCode Unlimited Native Mobile Mini Apps</strong> for complete control of data flow for code coverage and reliability as needed\nWith SAB, users gain the power to create seamless, customizable, and robust  that function flawlessly across platforms.</p><h2>\n  \n  \n  NoCode Super App Operating System - SAB OS\n</h2><p>As a variation, we can take this concept a step further and create —an  for building super apps.\nFor this to become a reality, a few critical elements must fall into place:</p><ol><li>:\nThe Super App Builder must either be  or made  to encourage contributions. This is crucial because it requires a  to gain traction in the market and attract <strong>businesses, design agencies, and product teams</strong>.</li><li>:\nThe tool must be  from any —and by “database,” we mean <strong>anything related to data and how it is fetched into the system</strong>.\nSounds crazy, right? .</li></ol><p>Many businesses already have their infrastructure built around <strong>existing data and databases</strong>. Instead of forcing them to migrate, why not  to what’s already in place? SAB OS can , and  integrate with their existing data.\nThe data could operate in , or allow for  capabilities—offering flexibility and control based on business needs.\nThe biggest question remains: why haven’t established companies built this yet?<p>\nThe answer couldn’t be simpler: </p>—not just financially, but in terms of redefining who they are. Building a super app would mean reconstructing their business around a new identity, and for many, that cost is simply too high to bear.</p><p>Instead, the new trend should focus on <strong>super app no-code builders</strong> that come with one idea in mind: <strong>Build Your Vision with a Nocode Super App Builder</strong>.</p>","contentLength":7186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Built a Library to Visualize and Edit Audio Filters","url":"https://dev.to/numberonebot/i-built-a-library-to-visualize-and-edit-audio-filters-2mke","date":1739738720,"author":"Alex Strelets","guid":777,"unread":true,"content":"<p>Several years ago, I was deep-dived into reverse engineering the parameter system used in VAG (Volkswagen, Audi, Porsche, etc) infotainment units. I managed to decode their binary format for storing settings for each car type and body style. To explain it simply - the firmware contains equalizer configurations for each channel of the on-board 5.1 speaker system, calibrated based on cabin volume and other parameters, much like how home theater systems are configured (with specific gains, delays, limiters, etc).</p><p>I published this research for the car enthusiast community. While the interest was huge, the reach remained small since most community members weren't familiar with HEX editors. Only a few could really replicate what I documented. After some time, I built a web application that visualized these settings and allowed to unpack, edit and repack that data back into the binary format.</p><p>When I first tried to visualize audio filters with that project, I hit a wall. Most charting libraries are built for business data, all those \"enterprise-ready visualization solutions\". But  of them is designed for audio-specific needs. D3.js is the only real option here - it is powerful but requires days of digging through docs just to get basic styling right. And if you want interactive features like drag-and-drop? Good luck with that. (Fun fact: due to D3's multiple abstraction layers, just  filter calculations in DSSSP are 1.4-2x faster than D3's implementation).</p><p>Looking back, the original project was pretty messy and had a very specific focus. However, this experience led me to realize that the visualization library itself could have broader applications in audio processing software.</p><p>This insight drove me to develop a custom vector-based graph from the ground up using a modern React stack. The library focuses on one thing - audio filters. No unnecessary abstractions, no enterprise bloat, just fast and convenient (I hope!?) tools for professional audio software.</p><ul><li>Logarithmic frequency response visualization</li><li>Interactive biquad filter manipulation</li><li>Custom audio calculation engine</li><li>Drag-and-drop + Mouse wheel controls</li></ul><ul><li>Built with React + SVG (no Canvas)</li><li>Zero external dependencies besides React</li></ul><p>This is the first public release, landing page is missing, and the backlog is huge, and doc is incomplete.<em>(You know, there's never a perfect timing - I just had to stop implementing my ideas and make it community driven).</em></p><p>I'd love to see what you could build with these components. What's missing? What could be improved?</p><p>I'm still lacking the understanding of how it could gain some cash flow, while staying open-source. Any ideas?</p>","contentLength":2630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implementing Buttery-Smooth Infinite Scroll in Vue 3 with Composables","url":"https://dev.to/deondazy/implementing-buttery-smooth-infinite-scroll-in-vue-3-with-composables-3ak2","date":1739738433,"author":"Deon Okonkwo","guid":776,"unread":true,"content":"<p>In my <a href=\"https://dev.to/deondazy/how-to-combine-filters-sorting-and-infinite-scrolling-in-laravel-inertiajs-v2-and-vue-3-24a7\">previous guide</a> we covered combining filters, sorting, and infinite scrolling in Laravel, Inertia.js v2, and Vue 3. </p><p>In this guide, let's tackle a pure Vue 3 solution for infinite scrolling. </p><p>Think of this as the \"vanilla JavaScript\" approach to infinite scroll - more flexible, more powerful, and surprisingly elegant when done right.</p><p>Infinite scrolling seems simple until you consider:</p><ul><li>Network request management</li><li>Scroll position restoration</li></ul><p>Our composable will handle all these concerns while remaining flexible enough to drop into any component. It's like building a well-oiled machine that quietly does its job in the background.</p><p>First, let's set up our Laravel endpoint. We'll keep it simple:</p><div><pre><code></code></pre></div><ul><li>Classic pagination with Eloquent</li><li>Resource transformation for consistent API responses</li><li> meta field is our stop signal</li><li>Clean separation between HTML/JSON responses</li></ul><h2>\n  \n  \n  The Infinite Scroll Composable\n</h2><p>Here's our workhorse - the  composable. This is where the magic happens, think of it as your personal scroll concierge:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Using the Composable in Components\n</h2><p>Here's how you'd implement it in a Vue component:</p><div><pre><code>\n      Loading more posts...\n    Retry</code></pre></div><ol><li><p><p>\nThe sentinel div acts as a tripwire - when it enters the viewport, we trigger the next load. The observer options let you fine-tune when this happens.</p></p></li><li><p><p>\nAutomatic cancellation of in-flight requests prevents race conditions and memory leaks. Think of it as a \"last request wins\" system.</p></p></li><li><p><p>\nAll loading states, errors, and pagination data are handled internally, exposing clean reactive properties to your component.</p></p></li><li><p><p>\nNeed to add custom params? Use the </p> function. Different API structure? Adjust the meta key and data key response mapping.</p></li><li><p>Self-Cleaning\nProper observer and request cleanup on component unmount</p></li></ol><ol><li><p><p>\nDrop this composable into any component needing infinite scroll. Blog posts? Comments? Product listings? It just works.</p></p></li><li><p><p>\nThe Intersection Observer API is highly optimized, avoiding expensive scroll listeners. Combined with proper request cancellation, it's lightweight.</p></p></li><li><p><p>\nNetwork errors get handled gracefully with retry capability. Scroll position is maintained during loads.</p></p></li><li><p><p>\nAll complex logic lives in the composable, keeping components clean and focused on presentation.</p></p></li></ol><ol><li>\nAdjust based on your content height:\n</li></ol><div><pre><code></code></pre></div><ol><li>\nAdd exponential backoff to error handler:\n</li></ol><div><pre><code></code></pre></div><ol><li>\nAdd window resize listener to refresh observer:\n</li></ol><div><pre><code></code></pre></div><p>Want to enhance this further?</p><ul><li><p><p>\nAdd a scroll position memory system using the History API</p></p></li><li><p><p>\nImplement windowing for huge datasets</p></p></li><li><p><p>\nAdd a local cache layer using IndexedDB</p></p></li></ul><p>Inertia.js V2 offers great shortcuts (as shown in my <a href=\"https://dev.to/deondazy/how-to-combine-filters-sorting-and-infinite-scrolling-in-laravel-inertiajs-v2-and-vue-3-24a7\">previous guide</a>), but understanding the vanilla Vue 3 approach gives you ultimate flexibility. This composable is like building your own sports car - you control every aspect of the performance.</p><p>The beauty of this pattern is in its separation of concerns. The composable handles all the complex machinery, while your components stay clean and focused on what matters - displaying content beautifully.</p><p>Stay inspired, keep building!</p>","contentLength":2991,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neovim Tips to Accelerate Your Text Navigation","url":"https://dev.to/codingwithsphere/neovim-tips-to-accelerate-your-text-navigation-3p8f","date":1739737730,"author":"Sphere","guid":775,"unread":true,"content":"<p>Neovim has endless shortcuts and features. Even with a grasp on the basics, it can be difficult to really get a grasp on the hidden power of the editor. Here, my goal is to provide some tips that I believe are game changers when it comes to navigating text efficiently.</p><p>This is meant for individuals that are past the initial setup phase. You should have a decent understanding of how the configuration system works, and know about some of the basic shortcuts.</p><p>No, you don't have to  stop using . They have their uses. For example, if you are one line above or below,  and  make sense. If you are one character to the left or right, you could use  or . However, in any other situation  are inefficient.</p><h2>\n  \n  \n  What Should I Use Instead of h and l?\n</h2><p>h and l can be replaced with , , and . These three commands can almost fully replace moving left and right by one character. Traversing a line horizontally is significantly more efficient. Take this example:</p><div><pre><code></code></pre></div><p>You are at the  in , and you want to get to the  in . Normally, you'd have to type  eleven times! Instead, using , you can get there in 4.</p><p>We can do better, though. There exists , , and , which act similar to their respective lowercase keybindings, with one main difference. Instead of qualifying a word as a group of continuous characters not interrupted by symbols or white space, the capital versions treat every character that isn't white space as one word. This means that  is considered one word. Now, we can make it to \"Bar\" in just three key presses!</p><p>Anything beyond this can be a bit unreliable. We  use the  command followed by  to jump to the first occurrence of , and then press  to go to the desired one. This is very useful, but is not really a huge improvement, as it still takes three key presses to get to the target.</p><h2>\n  \n  \n  Using a Plugin for Ultimate Speed\n</h2><p>In the previous example, where the target is on the same line, it makes total sense to use  or . However, this is not always the case. Take this example:</p><div><pre><code></code></pre></div><p>Now we are at the  in  and we want to get to the  in . This is offset in two dimensions, making it difficult to determine the best way to get there.</p><p>We could just directly search up the argument with  followed by . This works, but isn't very precise. If  was used anywhere between the cursor and that target location, it would jump there first.</p><p>We can utilize <a href=\"https://github.com/folke/flash.nvim\" rel=\"noopener noreferrer\">flash.nvim</a> to help us out here. Instead of searching or spamming  and , we can press one keybinding (in my case, ), and then press the target character. We then get a jump list next to every occurrence of  on the screen, giving us a unique key to press to instantly teleport to the target location. This allows us to make it to the target in four key presses, despite being in a completely arbitrary location. It can be three as well, I just ran out of one-letter keybinding space.</p><p>Now honestly,  and  have a lot more practicality than  and . Jumping up and down by one line is extremely common. So, don't completely throw away the muscle memory...</p><p>However, if there are large jumps &gt; 2 lines, maybe there is a better approach. I'd like to tell you I used numbered jumps like , but that isn't true most of the time. Even though I have relative line numbers enabled, I find that looking to the left at the numbers pulls me out of a productive state. There must be some way to jump large blocks of text without having to look away from the text...</p><p>In situations where the target line is  visible, I like to use  and . These commands, which go up half a page and down half a page respectively, are extremely useful when I need to rapidly travel up and down a page.</p><p>If a target line is known and visible, I'll use a variety of techniques. If the location is near a  or any unique, easy to press character, I'll just use  followed by the character. Most of the time this works well. In unique situations, I'll use flash.nvim.</p><p> and  are very useful when highlighting or traveling by blocks of code, with some caveats. They travel up and down until the line is white space, which they stop on (and include!). This is great for highlighting entire functions that are visible on the screen. It's not so great for moving through code precisely. If you've worked on any code base with more than one developer, you'd know that spacing is dangerously optional. Some people don't even put spaces between their functions!!! In a code base like this,  and  become non-deterministic.</p><p>If you didn't know, there is a family of commands for performing an action in and around a text object in nvim. The general form is \"\". This is really useful in situations where your cursor is not at the beginning of a word, but you need to modify it entirely. It is even  useful when modifying blocks of code encapsulated in brackets. For example:</p><div><pre><code></code></pre></div><p>If I want to change the contents of the arguments, I can just type  to remove everything in the arg list and be in insert mode. You don't even have to be inside the brackets, quotes, etc. Nvim will search for the next occurence of the text object and perform the action on that.</p><p><a href=\"https://github.com/nvim-telescope/telescope.nvim\" rel=\"noopener noreferrer\">Telescope.nvim</a> is extremely popular for navigating between files in Neovim. But that's not all that it can do! Telescope.nvim comes with dozens of builtins that can be extremely useful. My two favorites are  and .</p><p>The first is extremely useful for jumping around a file. If an LSP is setup, it will find functions, type definitions, constants, and lots of other symbols that can then be searched for and jumped to. This is extremely useful, especially in files with over a thousand lines. Jumping to a function becomes trivial, even with so much noise.</p><p>The second is very useful when you can remember a snippet of a function, definition, etc., but don't know what file or where in the file.  allows you to search across files for a specific pattern. For example, I can search \"matrix\" in a game engine code base to find every instance where I typed \"matrix\".</p><h2>\n  \n  \n  Using Marks with Telescope.nvim\n</h2><p>Marks are no where near as popular as they should be. I see a lot of beginner tutorials talking about using , , and , and maybe  and , but never marks. When used correctly, they can be absolutely revolutionary to your workflow. Take this example:</p><div><pre><code></code></pre></div><p>We are modifying , but notice we need to change something with the  struct, which is hundreds of lines away. We can utilize previous shortcuts to jump a half-page up until we reach , correct our position, and make a modification. Or, we can even using Telescope with  to jump straight to Bar (good idea!). However, neither of these approach are perfectly seamless. Even in the Telescope approach, we have to type in a few characters until we find the bar struct. If this was a longer type like , it could take even more characters to identify it.</p><p>We can use marks to prevent doing this every time we need to jump back and forth. To make a mark, place a cursor where you want the mark to be, and then press  followed by a mark identifier. This should be a lower-case character.We can put a mark on  with  and a mark on  with . Then, while we're working on the function, we can press  to jump to , and  to jump back. We can even omit the mark on something, and press  to jump back to our last location before a jump.</p><p>So, we can setup marks quickly with Telescope, and then use the marks for ultimate speed from then on out. It looks like we've maxed out our navigation speed!</p><p>Snippets can massively boost productivity. Most languages have some built-in to the LSP. However, they must be non-intrusive and generic enough that everyone accepts them. Your setup is  setup, so you can make whatever you want. For example, I created a simple snippet for creating a struct in <a href=\"https://ziglang.org\" rel=\"noopener noreferrer\">zig</a>:</p><div><pre><code></code></pre></div><p>This was made for the <a href=\"https://github.com/L3MON4D3/LuaSnip\" rel=\"noopener noreferrer\">luasnip</a> plugin, which is most likely what you are using for snippets. It allows me to autocomplete \"stru\" with a skeleton struct implementation, that gives me convenience points to insert content and tab through.</p><h2>\n  \n  \n  Create Your Own Plugins/Utilities\n</h2><p>Plugins are not mystical pieces of software that take an experienced wizard to create--quite the opposite actually. Neovim uses lua because it enables more people to write a plugin than with the harder to use \"VimScript\" in vim. You can and should make plugins for your own needs.</p><p>No one knows you as well as you do. So, go make the thing you always dreamed of having in an editor!</p>","contentLength":8296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fixing Kafka Connectivity Issues Between Node.js and Docker Containers","url":"https://dev.to/rakshyak/fixing-kafka-connectivity-issues-between-nodejs-and-docker-containers-515b","date":1739737345,"author":"Rakshyak Satpathy","guid":774,"unread":true,"content":"<p>Apache Kafka is a powerful event streaming platform, but setting it up in a Docker environment while ensuring seamless connectivity with an external Node.js service can be challenging. This article explores two key challenges:</p><ol><li><strong>Connecting a Local Node.js Server as an External Host Machine with a Kafka Broker in Docker</strong></li><li><strong>Understanding and Fixing Docker DNS Resolution Issues for Local Machine Communication</strong></li></ol><p>We'll walk through the problems faced, their root causes, and step-by-step solutions.</p><h2><strong>Setting Up Kafka in a Docker Container</strong></h2><p>We are using Bitnami's Kafka image, which supports KRaft mode (Kafka's built-in metadata management) without requiring ZooKeeper.</p><h3><strong>1. Create a Docker Network</strong></h3><p>To ensure seamless communication between services, create a dedicated network:</p><div><pre><code>docker network create app-tier\n</code></pre></div><h3><strong>2. Run Kafka in KRaft Mode</strong></h3><div><pre><code>docker run  kafka-server  9092:9092  kafka-server  app-tier 0 controller,broker PLAINTEXT://:9092,CONTROLLER://:9093 CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT PLAINTEXT://192.168.1.10:9092 0@kafka-server:9093 CONTROLLER  \nbitnami/kafka:latest\n</code></pre></div><h3><strong>Understanding </strong></h3><ul><li><code>listeners=PLAINTEXT://:9092</code> → Kafka listens on all interfaces inside the container.</li><li><code>advertised.listeners=PLAINTEXT://192.168.1.10:9092</code> → External clients connect using the .</li></ul><p>This prevents Kafka from advertising its internal container IP, which would make it unreachable from the local machine.</p><h2><strong>Challenge 1: Connecting a Local Node.js Server to Docker Kafka</strong></h2><p>By default, a <strong>Docker container runs in an isolated network</strong>. This means  inside the container does not refer to the host machine’s localhost. If a Node.js service runs outside Docker, it cannot connect to Kafka using  unless Kafka explicitly advertises the .</p><h3><strong>Solution: Use the Host IP</strong></h3><p>Find your machine's IP address:</p><p>Then, update Kafka’s  to point to this IP.</p><h2><strong>Challenge 2: Docker DNS Resolution and Communication</strong></h2><p>Even after setting the correct , issues may arise if:</p><ul><li>Kafka and Node.js services are on different networks.</li><li>Docker’s DNS resolution prevents local services from reaching Kafka.</li></ul><h3><strong>Solution: Use Docker’s Built-in DNS</strong></h3><ul><li>Containers in the same network can resolve each other by name.</li><li>Use .</li><li>Use .</li></ul><h3><strong>Fixing Docker Name Resolution for Node.js</strong></h3><p>If your Node.js service runs , ensure it is in the same network:</p><div><pre><code>docker network connect app-tier nodejs-container\n</code></pre></div><div><pre><code></code></pre></div><h2><strong>Final Working Node.js Kafka Integration</strong></h2><h3><strong>Kafka Producer &amp; Consumer Code</strong></h3><div><pre><code></code></pre></div><p>By addressing <strong>Kafka’s networking challenges in Docker</strong>, we ensured seamless communication between a local Node.js service and a Kafka broker running in a container. Key takeaways:</p><p>✅ <strong>Use  to expose Kafka correctly</strong>\n✅ <strong>Leverage Docker DNS for intra-container communication</strong>\n✅ <strong>Ensure Kafka and Node.js services are in communication with the docker container kafka broker</strong></p>","contentLength":2717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/jeffque/-483i","date":1739737274,"author":"Jefferson Quesado","guid":773,"unread":true,"content":"<h2>Acho que podemos deixar o mobile first em 2024</h2><h3>Camilo Micheletto ・ Feb 15</h3>","contentLength":74,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surviving Kubernetes Pod Evictions: Managing Resources, Priorities, and Stability","url":"https://dev.to/nurudeen_kamilu/surviving-kubernetes-pod-evictions-managing-resources-priorities-and-stability-3jj1","date":1739737038,"author":"NURUDEEN KAMILU","guid":772,"unread":true,"content":"<p>Kubernetes is designed to orchestrate workloads efficiently across nodes, ensuring optimal resource utilization and workload reliability. However, when resource constraints arise, Kubernetes must make tough decisions—this is where pod eviction comes in. Understanding how Kubernetes evicts pods helps administrators optimize workload resilience and ensure high availability.</p><p>In this article, we will explore Kubernetes pod eviction mechanisms, diving into node-pressure eviction, API-driven eviction, pod priorities, pod preemption, and Quality of Service (QoS) classes. We will also examine how these factors interact to maintain cluster stability.</p><h3>\n  \n  \n  The Foundation: Quality of Service (QoS)\n</h3><p>At the heart of Kubernetes' eviction decisions lies the  (QoS) classification system. Every pod in Kubernetes is assigned one of three QoS classes:</p><ol><li>: A pod is assigned this QoS class if all of its containers have precisely defined resource (cpu and memory) requests and limits that are set equal to each other.</li><li>: This is the middle class of QoS; these pods have defined memory or CPU requests or limits for at least one of their containers.</li><li>: These pods have no resource requests or limits defined.</li></ol><p>Evictions can occur for multiple reasons, including resource constraints (e.g., memory pressure) and administrative actions (e.g., API-initiated deletions). Kubernetes provides structured mechanisms to handle evictions gracefully. There are two main categories of pod eviction:</p><ul><li><p>: Triggered automatically when a node experiences resource shortages.</p></li><li><p>: Initiated by a user or an external controller via the Kubernetes API.</p></li></ul><p>Let’s break down these eviction types and the factors influencing them.</p><h3>\n  \n  \n  Node-Pressure Eviction: Automatic Resource Management\n</h3><p>When a node in the cluster experiences resource pressure—such as low memory or disk space availability—Kubernetes triggers . This is a self-defense mechanism to prevent the node from becoming unresponsive or crashing. The <a href=\"https://kubernetes.io/docs/concepts/architecture/#kubelet\" rel=\"noopener noreferrer\">kubelet</a> monitors resource usage and, upon reaching critical thresholds, selects pods for eviction based on priority and QoS value.</p><h4>\n  \n  \n  Configuring Node-Pressure Eviction\n</h4><p>Node-pressure eviction can be configured by setting eviction thresholds in the kubelet configuration. Below is an example of how to configure eviction thresholds in the <a href=\"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration\" rel=\"noopener noreferrer\">Kubelet configuration file</a>:</p><div><pre><code></code></pre></div><p>Key configuration components:</p><ul><li>: When these are breached, the kubelet will immediately start evicting pods</li><li>: Pods are evicted only if the threshold is exceeded for a specified grace period</li><li>: Define how long the kubelet should wait before starting eviction</li><li><strong>Pressure Transition Period</strong>: Defines how long a node condition must persist before triggering eviction under pressure condition</li></ul><h4>\n  \n  \n  Factors Affecting Node-Pressure Eviction\n</h4><ol><li><p>Quality of Service (QoS) Class:</p><ul><li>Guaranteed: Highest priority—only evicted in extreme conditions.</li><li>Burstable: Evicted after BestEffort pods but before Guaranteed pods.</li><li>BestEffort: Lowest priority—first to be evicted.</li></ul></li><li><p>Pod Priority and Preemption: Higher-priority pods are less likely to be evicted, while lower-priority pods are targeted first. Pod priority can be defined by creating a <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass\" rel=\"noopener noreferrer\">PriorityClass</a> and specifying it in the pod specification ().</p></li></ol><h3>\n  \n  \n  API-Driven Eviction: The Manual Override\n</h3><p>Unlike node-pressure evictions, which are automatic,  occur when users or controllers explicitly request pod removal using the <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#create-eviction-pod-v1-core\" rel=\"noopener noreferrer\">Eviction API</a>.</p><h4>\n  \n  \n  Use Cases for API-Driven Eviction\n</h4><ul><li>: Scales down nodes by evicting pods before removing the node.</li><li><strong>Controllers (e.g., Deployment, ReplicaSet)</strong>: May trigger evictions to manage rolling updates.</li><li>: Operators can manually evict pods to redistribute workloads.</li></ul><p>API-driven evictions respect <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets\" rel=\"noopener noreferrer\">pod disruption budgets</a> (PDBs), ensuring that evictions do not impact availability beyond acceptable thresholds.</p><h3>\n  \n  \n  Pod Priority and Preemption: The Hierarchy of Importance\n</h3><p>Not all pods are created equal. Kubernetes allows users to assign priorities to pods, which influence eviction and scheduling decisions. Preemption priority determines which pods get evicted when a higher-priority pod needs to be scheduled. The <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/\" rel=\"noopener noreferrer\">Kubernetes scheduler</a> evaluates available nodes and determines if preempting existing pods would create enough resources for the new pod. </p><ul><li><strong>Higher-priority pods preempt lower-priority ones</strong> when no sufficient resources are available.</li><li><strong>Pods with the same priority are not preempted</strong>; Kubernetes looks for lower-priority alternatives.</li><li><strong>Preemption considers pod disruption budgets (PDBs)</strong> to minimize service disruptions.</li></ul><p>This ensures that critical workloads always have resources available.</p><p>Pod eviction in Kubernetes is a finely tuned process that balances resource availability, workload importance, and user-defined policies. By leveraging node-pressure eviction, API-driven eviction, pod priorities, pod preemption, and QoS classes, Kubernetes ensures that clusters remain stable and efficient, even under pressure. By understanding these mechanisms, you can optimize your workloads, ensure fair resource distribution, and prevent disruptions in your Kubernetes environments.</p><p><strong>So, the next time a pod is evicted, remember: it’s not a failure—it’s Kubernetes doing its job, mastering the art of letting go.</strong></p>","contentLength":5214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MongoDB Commands Categorized","url":"https://dev.to/harshm03/mongodb-commands-categorized-a24","date":1739736880,"author":"Harsh Mishra","guid":771,"unread":true,"content":"<p>MongoDB is a powerful NoSQL database known for its flexibility and scalability. Whether you are a beginner or an advanced user, mastering MongoDB commands will help you efficiently manage databases, collections, and documents. This guide categorizes MongoDB commands based on their usage, covering everything from basic operations to advanced database management.  </p><h2><strong>1. Basic MongoDB Commands</strong></h2><h3><strong>Starting and Stopping MongoDB</strong></h3><div><pre><code>systemctl start mongod\n</code></pre></div><div><pre><code>systemctl stop mongod\n</code></pre></div><div><pre><code>systemctl restart mongod\n</code></pre></div><div><pre><code>systemctl status mongod\n</code></pre></div><ul><li>Connect to a specific database:\n</li></ul><ul><li>Show available databases:\n</li></ul><ul><li>Show the currently selected database:\n</li></ul><p>MongoDB automatically creates a database when a document is inserted. However, you can explicitly select a database:</p><p>(This does not create the database until data is added.)</p><p>Deletes the currently selected database.</p><p>Displays all available databases.</p><h3><strong>Checking the Current Database</strong></h3><p>Displays the name of the currently selected database.</p><div><pre><code>db.createCollection</code></pre></div><p>Creates an empty collection.</p><p>Displays all collections in the current database.</p><p>Deletes a specific collection.</p><h2><strong>4. Document Management (CRUD Operations)</strong></h2><ul><li>Insert a single document:\n</li></ul><div><pre><code>  db.myCollection.insertOne name: , age: 25, city: </code></pre></div><ul><li>Insert multiple documents:\n</li></ul><div><pre><code>  db.myCollection.insertMany name: , age: 30, city: ,\n       name: , age: 35, city: </code></pre></div><h3><strong>Reading Documents (Querying)</strong></h3><ul><li>Find all documents in a collection:\n</li></ul><ul><li>Find a specific document:\n</li></ul><div><pre><code>  db.myCollection.findOne name: </code></pre></div><ul><li>Find documents with specific conditions:\n</li></ul><div><pre><code>  db.myCollection.find age: : 30 </code></pre></div><p>Finds all documents where  is greater than 30.</p><ul><li>Projection (return only specific fields):\n</li></ul><div><pre><code>  db.myCollection.find,  name: 1, _id: 0 </code></pre></div><p>Returns only the  field and excludes .</p><ul><li>Update a single document:\n</li></ul><div><pre><code>  db.myCollection.updateOne name: ,\n      :  age: 26 </code></pre></div><ul><li>Update multiple documents:\n</li></ul><div><pre><code>  db.myCollection.updateMany city: ,\n      :  country: </code></pre></div><div><pre><code>  db.myCollection.replaceOne name: ,\n       name: , age: 27, city: </code></pre></div><ul><li>Delete a single document:\n</li></ul><div><pre><code>  db.myCollection.deleteOne name: </code></pre></div><ul><li>Delete multiple documents:\n</li></ul><div><pre><code>  db.myCollection.deleteMany city: </code></pre></div><h2><strong>5. Indexing and Performance Optimization</strong></h2><ul><li>Create an index on a field:\n</li></ul><div><pre><code>  db.myCollection.createIndex name: 1 </code></pre></div><div><pre><code>  db.myCollection.createIndex name: 1, age: </code></pre></div><div><pre><code>  db.myCollection.createIndex email: 1 ,  unique: </code></pre></div><div><pre><code>db.myCollection.getIndexes</code></pre></div><div><pre><code>db.myCollection.dropIndex</code></pre></div><h3><strong>Using Aggregation Pipelines</strong></h3><ul><li>Count documents in a collection:\n</li></ul><div><pre><code>  db.myCollection.countDocuments</code></pre></div><ul><li>Group and count by a field:\n</li></ul><div><pre><code>  db.myCollection.aggregate:  _id: , count: : 1 </code></pre></div><div><pre><code>  db.myCollection.aggregate:  age: </code></pre></div><h2><strong>7. User and Role Management</strong></h2><div><pre><code>db.createUser\n    user: ,\n    : ,\n    roles:  role: , db: </code></pre></div><div><pre><code>mongodump  myDatabase  /backup/\n</code></pre></div><div><pre><code>mongorestore  myDatabase /backup/myDatabase/\n</code></pre></div><h2><strong>9. Replica Sets and Sharding</strong></h2><h3><strong>Checking Replica Set Status</strong></h3><h3><strong>Adding a Member to a Replica Set</strong></h3><div><pre><code>rs.add</code></pre></div><div><pre><code>sh.enableSharding</code></pre></div><h2><strong>10. Server Administration and Monitoring</strong></h2><h3><strong>Checking MongoDB Server Status</strong></h3><h3><strong>Checking Collection Stats</strong></h3><p>Edit  and add:</p><div><pre><code></code></pre></div><div><pre><code>systemctl restart mongod\n</code></pre></div><p>This guide provides a categorized reference for essential MongoDB commands. Whether you are working with collections, queries, indexes, or replication, these commands will help you efficiently manage MongoDB databases. Keep this guide handy as a quick reference! 🚀</p>","contentLength":3130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Must-Have VS Code Extensions for Front-End Developers","url":"https://dev.to/purushoth_26/must-have-vs-code-extensions-for-front-end-developers-5e53","date":1739736791,"author":"Purushothaman M","guid":770,"unread":true,"content":"<p>Visual Studio Code (VS Code) is one of the most popular code editors for front-end developers. With the right extensions, you can boost productivity, improve code quality, and streamline your workflow. In this blog, I'll share some of the best VS Code extensions that every front-end developer should have!</p><h2>\n  \n  \n  1. Code Formatting &amp; Productivity\n</h2><p>🔹 <strong>Prettier – Code formatter</strong></p><p>Prettier helps maintain consistent code formatting across your project. It automatically formats your HTML, CSS, JavaScript, and more.</p><p>This extension helps you catch JavaScript and TypeScript errors early by enforcing coding standards and best practices.</p><p>Automatically renames paired HTML/XML tags when you edit one of them, saving time when working with complex structures.</p><p>Lets you peek into CSS classes and IDs directly from your HTML files, making navigation easier.</p><p>🔹 <strong>Tailwind CSS IntelliSense</strong></p><p>If you're using Tailwind CSS, this extension provides class name autocompletion and linting to speed up your workflow.</p><p>Launches a local development server with live reload, making it easier to preview changes instantly in the browser.</p><p>Allows you to preview HTML files directly inside VS Code without opening a browser.</p><h2>\n  \n  \n  4. Version Control &amp; Collaboration\n</h2><p>Supercharges Git inside VS Code, showing who made changes, commit history, and blame annotations.</p><p>Collaborate in real-time with teammates by sharing your workspace.</p><h2>\n  \n  \n  5. Icons &amp; UI Enhancements\n</h2><p>Adds beautiful icons to your VS Code file explorer, improving navigation.</p><p>Colorizes indentation levels, making it easier to read nested code.</p><p>These VS Code extensions can greatly improve your front-end development workflow. Whether it's formatting, debugging, styling, or collaboration, these tools help you write better code, faster.</p><p>What are your favorite VS Code extensions? Drop them in the comments! 🚀</p>","contentLength":1843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Darc Athletics: Revolutionizing Vogue, Exercise, as well as Functionality","url":"https://dev.to/seo_worker_7c30fe1726aaed/darc-athletics-revolutionizing-vogue-exercise-as-well-as-functionality-3563","date":1739734929,"author":"seo worker","guid":764,"unread":true,"content":"<p>Inside the ever-evolving playing field of athletics, trend, as well as physical fitness, advancement can be key. Enter&nbsp;Darc Athletics, a brand name that has become symbolic of cutting-edge style, high-performance specific sport have on, plus a smooth combination of type as well as functionality. Darc Athletics is not just a brand name; it's a motion this redefines how some athletes as well as physical fitness enthusiasts strategy its equipment, empowering them to push restrictions and achieve its goals.</p><p>The Birth of Darc Sports\nDarc Athletics has been launched using a very simple but powerful vision: to develop apparel as well as equipment this improves performance with no inhibiting about style. Discerning the space looking for physical fitness as well as athletics equipment this serves each beauty as well as operation, the brand attempted to style products that encourage self-assurance as well as generate results. Whether you're also a reliable patient, the fitness center fanatic, as well as an individual just simply establishing its physical fitness process, Darc Athletics offers a little something with regard to everyone.</p><p>Fashion Meets Functionality\nOne of many talked about top features of Darc Athletics can be its ability to combine trend together with functionality. Much more any time specific sport have on has been limited to boring colorings as well as uninspired designs. Darc Athletics offers increased the sport by way of introducing clean, modern layouts that seem to be practically many people perform. Coming from daring, eye-catching habits so that you can smart, timeless items, the brand presents numerous selections for every single taste.</p><p>Yet it's not only concerning looks. Darc Athletics prioritizes performance by making use of innovative textiles as well as technologies. The apparel was made to be mesh, moisture-wicking, and durable, being sure that you stay comfy as well as aimed for the duration of sometimes the most powerful workouts. Whether you're also lifting weights, owning a race, as well as just practising meditation, Darc Athletics equipment can be built to maneuver together with you, providing the aid and flexibility you actually need.</p><p>Fitness Redefined\nDarc Athletics can be not just the garments company; it's a physical fitness revolution. The manufacturer appreciates this physical fitness isn't a one-size-fits-all journey. For this reason they have a diverse range of items tailor-made so that you can sports as well as activities. Coming from compression setting have on this improves muscle retrieval so that you can light in weight, sweat-resistant activewear, Darc Athletics offers considerately created every bit to satisfy the involves of assorted physical fitness disciplines.</p><p>The manufacturer additionally emphasizes inclusivity, offering sizes and styles this appeal to all of shape types. Darc Athletics believes that everybody deserves to think comfortable as well as stimulated for their physical fitness process, along with their inclusive strategy shows this commitment.</p><p>Innovation at Its Core\nJust what pieces Darc Athletics away from each other can be the persistent focus on innovation. The manufacturer regularly spends throughout analysis as well as progress to live in front of the curve. Coming from combining eco friendly components within their solutions so that you can using the most recent breakthroughs throughout materials technological innovation, Darc Athletics can be dedicated to delivering high-quality, eco-conscious equipment it doesn't skimp about performance.</p><p>A person with their talked about new developments can be the utilization of&nbsp;compression setting technological innovation&nbsp;for their apparel. Meant to increase the circulation of blood reducing muscle weariness, this element makes Darc Athletics popular among the some athletes as well as physical fitness enthusiasts alike. Also, its goods are frequently built with UV defense, odour weight, as well as temperatures legislation, driving them to ideal for each indoor and outdoor activities.</p><p>A Community of Champions\nDarc Athletics can be not just a brand name; it's a community. As a result of societal marketing, functions, as well as collaborations, the brand has produced the faithful following of physical fitness enthusiasts whom share a desire for performance as well as style. Darc Athletics on a regular basis engages with its market, offering physical fitness guidelines, workout sessions, as well as motivational content material so that you can encourage people to reach its whole potential.</p><p>The manufacturer additionally collaborates together with qualified some athletes as well as physical fitness influencers, presenting real-life tales of persistency as well as success. Most of these partners but not only high light the effectiveness of Darc Athletics solutions but will also enhance this brand's resolve forpersistance to assisting some athletes at most level.</p><p>Sustainability and Social Responsibility\nWith an era the place sustainability is much more crucial than ever, Darc Athletics can be making plans to lessen the enviromentally friendly footprint. The manufacturer can be devoted to using eco-friendly components as well as honorable manufacturing practices. Simply by showing priority for sustainability, Darc Athletics isn't just bringing about the far healthier entire world but will also placing an example with regard to additional manufacturers inside the industry.</p><p>In addition, Darc Athletics can be committed to offering to this community. As a result of numerous projects as well as partners, the brand helps underprivileged some athletes as well as helps bring about physical fitness programs throughout underserved areas. This particular resolve forpersistance to societal duty additional solidifies Darc Athletics while a brand name this likes you not just profits.</p><p>The Future of Darc Sports\nWhile Darc Athletics is growing, the brand shows not any indications of halting down. Using plans to flourish the product as well as accomplish new markets, Darc Athletics can be positioned to be a world wide boss throughout athletics trend as well as fitness. Your brand's unwavering perseverance so that you can advancement, inclusivity, as well as sustainability assures that it's going to stay the main point on the market for several years so that you can come.</p><p>Conclusion\nDarc Athletics can be not just a brand name; it's a lifestyle. Simply by flawlessly working trend, physical fitness, and satisfaction, Darc Athletics has established a distinctive niche market inside the specific sport have on market. Whether you're also an elite patient as well as a not so formal gym-goer, Darc Athletics enables you actually to look good, feel good, as well as conduct your best. While the brand is constantly on the innovate as well as encourage, a very important factor is obvious: Darc Athletics can be used to alter this game.</p><p>Consequently, when lovely increase your physical fitness process, you need to accept this [Darc Athletics] revolution. After all, wonder is not just the goal—it's the easiest way of life.</p>","contentLength":7103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Cloud Path Week 6 - Getting Started with AWS CDK","url":"https://dev.to/aws-builders/aws-cloud-path-week-6-getting-started-with-aws-cdk-1171","date":1739734016,"author":"Elizabeth Adeotun Adegbaju","guid":763,"unread":true,"content":"<p>This week, we made a big leap into <strong>Infrastructure as Code (IaC)</strong> by introducing the ! 🚀 Instead of manually configuring resources through the AWS Management Console or CLI, we began defining infrastructure in code, making deployments more scalable and repeatable.</p><h3>\n  \n  \n  Key Takeaways from Week 6:\n</h3><p> – We set up AWS CDK in our accounts to prepare for deployments.<strong>✅ Initializing a CDK Project</strong> – We ran  to scaffold our first CDK application.<strong>✅ Understanding AWS CloudFormation</strong> – We explored how AWS CDK synthesizes CloudFormation templates under the hood, bridging the gap between  and  infrastructure management.</p><p>This session laid the foundation for managing AWS resources efficiently using code!</p><p><strong>📺 Missed the session? Watch the recording here:</strong></p><h3>\n  \n  \n  What’s Next: Week 7 Preview\n</h3><p>Next week, we’ll dive deeper into AWS CDK, focusing on:</p><p>💡 Tip: If you’re following along, make sure you’ve installed AWS CDK and bootstrapped your environment. This will help you jump right into hands-on deployments next week!</p><p>Stay tuned—Week 7 is where we start building real infrastructure with AWS CDK! 🎯</p>","contentLength":1114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Configuring a Lambda Function in a VPC to access external API via NAT Gateway and Store in DynamoDB using AWS PrivateLink","url":"https://dev.to/chinmay13/configuring-a-lambda-function-in-a-vpc-to-access-external-api-via-nat-gateway-and-store-in-dynamodb-1b29","date":1739733764,"author":"Chinmay Tonape","guid":762,"unread":true,"content":"<p>AWS Lambda functions running inside a Virtual Private Cloud (VPC) provide enhanced security, network isolation, and direct access to other AWS services within the VPC. By placing Lambda in a VPC, we ensure it can securely communicate with private resources like databases while also accessing external APIs via a NAT Gateway. This setup is crucial for workloads that require controlled outbound internet access and private service interactions.</p><p>In this blog, we will configure a Lambda function inside a VPC that fetches Chuck Norris jokes from chucknorris.io via a NAT Gateway and stores them in a DynamoDB table using AWS PrivateLink.</p><h3>\n  \n  \n  Step 1: Create a VPC with Public and Private Subnets\n</h3><p>Create a new VPC with two public subnets and two private subnets. The public subnets will host the NAT Gateway, and the private subnets will be used for Lambda.\nA NAT Gateway is required to allow private subnets to access the internet while keeping them secure.</p><h3>\n  \n  \n  Step 2: Create a DynamoDB Table\n</h3><p>We need a DynamoDB table to store the Chuck Norris jokes.</p><h3>\n  \n  \n  Step 3: Create a VPC Interface Endpoint for DynamoDB\n</h3><p>Instead of routing traffic over the internet, we will use AWS PrivateLink to securely access DynamoDB from private subnets.\nWe will create and attach a security group to VPC endpoint which will contain or lambda function.</p><h3>\n  \n  \n  Step 4: Create IAM Role for Lambda\n</h3><p>The Lambda function requires an IAM role with permissions to access DynamoDB and VPC resources.\nIt will need acess to create ENIs for accessing the services via PrivateLink</p><h3>\n  \n  \n  Step 5: Create a Lambda Function\n</h3><p>The Lambda function will:</p><ol><li>Fetch a joke from chucknorris.io</li><li>Store the joke in the DynamoDB table</li></ol><p>(Note: when lambda function gets called from function URL, it causes duplicate hits to lambda as browser requests favicons, the code will gnore the favicon requests)</p><div><pre><code></code></pre></div><p>Create a lambda layer for requsts library and create lambda function with function URL</p><h3>\n  \n  \n  Step 6: Create cloudwatch log group for logging\n</h3><p>Follow these steps to execute the Terraform configuration:</p><p>Upon successful completion, Terraform will provide relevant outputs.</p><p>Lambda Invocation using Function URL</p><p>Remember to stop AWS components to avoid large bills.</p><p>This architecture provides a secure way to access DynamoDB using PrivateLink while allowing the Lambda function to communicate with external APIs through a NAT Gateway.</p>","contentLength":2381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The process of creating an effective Application Security Programm: Strategies, techniques and tools for the best outcomes","url":"https://dev.to/lynxfelony1/the-process-of-creating-an-effective-application-security-programm-strategies-techniques-and-5a2","date":1739733659,"author":"Smart Mohr","guid":761,"unread":true,"content":"<p>AppSec is a multifaceted and robust strategy that goes far beyond vulnerability scanning and remediation. The constantly evolving threat landscape, coupled with the rapid pace of technological advancement and the growing intricacy of software architectures, requires a comprehensive, proactive strategy that seamlessly integrates security into all phases of the development process. development security platform This comprehensive guide provides essential elements, best practices and cutting-edge technology used to build the highly effective AppSec program. It empowers companies to enhance their software assets, decrease risks and foster a security-first culture. </p><p>At the core of a successful AppSec program is a fundamental shift in thinking that sees security as an integral aspect of the process of development, rather than an afterthought or a separate task. This paradigm shift requires close collaboration between security, developers operations, and others. It breaks down silos and creates a sense of sharing responsibility, and encourages an approach that is collaborative to the security of the applications they develop, deploy and maintain. In embracing an DevSecOps approach, organizations can integrate security into the fabric of their development processes making sure security considerations are taken into consideration from the very first stages of ideation and design up to deployment and ongoing maintenance. </p><p>A key element of this collaboration is the creation of clear security guidelines, standards, and guidelines which provide a structure for secure coding practices threat modeling, and vulnerability management. These guidelines must be based on industry best practices, such as the OWASP top 10 list, NIST guidelines, as well as the CWE. They should be able to take into account the specific requirements and risk that an application's as well as the context of business. The policies can be codified and made accessible to all parties and organizations will be able to use a common, uniform security approach across their entire portfolio of applications. </p><p>To implement these guidelines and make them practical for developers, it's important to invest in thorough security training and education programs. These programs should provide developers with knowledge and skills to write secure codes as well as identify vulnerabilities and implement best practices for security throughout the process of development. The training should cover a variety of subjects, such as secure coding and common attack vectors as well as threat modeling and security-based architectural design principles. Through fostering a culture of constant learning and equipping developers with the equipment and tools they need to integrate security into their daily work, companies can establish a strong base for an efficient AppSec program. </p><p>In addition to training organizations should also set up solid security testing and validation procedures to discover and address vulnerabilities before they can be exploited by malicious actors. This requires a multilayered method that combines static and dynamic analyses techniques and manual code reviews as well as penetration testing. Static Application Security Testing (SAST) tools are able to examine the source code of a program and to discover potential vulnerabilities, such as SQL injection cross-site scripting (XSS), and buffer overflows in the early stages of the development process. Dynamic Application Security Testing (DAST) tools on the other hand are able to simulate attacks on running applications, identifying vulnerabilities that may not be detectable using static analysis on its own. </p><p>The automated testing tools can be extremely helpful in the detection of security holes, but they're not the only solution. Manual penetration testing and code reviews by skilled security professionals are equally important to uncover more complicated, business logic-related vulnerabilities which automated tools are unable to detect. Combining automated testing with manual validation, organizations can obtain a full understanding of their security posture. They can also prioritize remediation strategies based on the level of vulnerability and the impact it has on. </p><p>To enhance the efficiency of the effectiveness of an AppSec program, businesses should look into leveraging advanced technologies like artificial intelligence (AI) and machine learning (ML) to augment their security testing capabilities and vulnerability management. AI-powered tools are able to analyze huge quantities of application and code data, and identify patterns and abnormalities that could signal security concerns. These tools also help improve their detection and prevention of new threats by learning from past vulnerabilities and attack patterns. </p><p>One of the most promising applications of AI within AppSec is the use of code property graphs (CPGs) that can facilitate more precise and effective vulnerability detection and remediation. CPGs are a comprehensive, conceptual representation of an application's codebase. appsec with agentic AI They capture not just the syntactic architecture of the code, but also the complex relationships and dependencies between different components. Utilizing the power of CPGs AI-driven tools are able to perform deep, context-aware analysis of an application's security position by identifying weaknesses that might be overlooked by static analysis techniques. </p><p>CPGs can automate vulnerability remediation applying AI-powered techniques to code transformation and repair. By understanding the semantic structure of the code, as well as the nature of the vulnerabilities, AI algorithms can generate specific, context-specific fixes that address the root cause of the issue instead of only treating the symptoms. This approach not only accelerates the process of remediation but also minimizes the chance of introducing new weaknesses or breaking existing functionality. </p><p>Another crucial aspect of an effective AppSec program is the integration of security testing and validation into the ongoing integration and continuous deployment (CI/CD) process. Automating security checks and integrating them into the build-and-deployment process allows organizations to detect vulnerabilities early on and prevent them from affecting production environments. This shift-left approach to security allows for more efficient feedback loops, which reduces the amount of effort and time required to detect and correct issues. </p><p>For organizations to achieve this level, they have to invest in the proper tools and infrastructure that can assist their AppSec programs. It is not just the tools that should be utilized for security testing and testing, but also the frameworks and platforms that allow integration and automation. Containerization technology like Docker and Kubernetes play a significant role in this regard, since they provide a reproducible and uniform environment for security testing as well as separating vulnerable components. </p><p>Effective collaboration and communication tools are as crucial as technical tooling for creating an environment of safety and enabling teams to work effectively together. Issue tracking tools like Jira or GitLab will help teams focus on and manage weaknesses, while chat and messaging tools like Slack or Microsoft Teams can facilitate real-time communication and knowledge sharing between security professionals as well as development teams. </p><p>The performance of an AppSec program isn't solely dependent on the technologies and tools used as well as the people who help to implement the program. multi-agent approach to application security A strong, secure environment requires the leadership's support in clear communication, as well as an effort to continuously improve. By instilling a sense of shared responsibility for security, encouraging open dialogue and collaboration, and providing the resources and support needed organisations can establish a climate where security is not just a checkbox but an integral element of the development process. </p><p>In order to ensure the effectiveness of their AppSec program, organizations must concentrate on establishing relevant metrics and key performance indicators (KPIs) to track their progress and find areas for improvement. These metrics should be able to span all phases of the application lifecycle starting from the number of vulnerabilities discovered in the development phase through to the time required to fix issues and the overall security posture of production applications. These indicators can be used to illustrate the benefits of AppSec investment, spot trends and patterns and aid organizations in making informed decisions about where they should focus their efforts. </p><p>In addition, organizations should engage in constant education and training activities to keep up with the constantly evolving threat landscape and emerging best practices. This could include attending industry conferences, taking part in online-based training programs as well as collaborating with external security experts and researchers to stay on top of the latest technologies and trends. By fostering an ongoing training culture, organizations will ensure their AppSec programs remain adaptable and resistant to the new challenges and threats. </p><p>It is important to realize that app security is a continual process that requires ongoing investment and dedication. As new technologies are developed and the development process evolves companies must constantly review and revise their AppSec strategies to ensure that they remain efficient and aligned to their business objectives. By adopting a continuous improvement mindset, encouraging collaboration and communication, as well as leveraging advanced technologies such CPGs and AI companies can develop an efficient and flexible AppSec programme that will not only safeguard their software assets but also help them innovate within an ever-changing digital world. <a href=\"https://ismg.events/roundtable-event/denver-appsec/\" rel=\"noopener noreferrer\">multi-agent approach to application security</a></p>","contentLength":10010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden Vulnerabilities in E-commerce APIs (And How to Secure Them)","url":"https://dev.to/martin-security/the-hidden-vulnerabilities-in-e-commerce-apis-and-how-to-secure-them-1kb9","date":1739733358,"author":"Martin Alonso","guid":760,"unread":true,"content":"<p>E-commerce APIs are the backbone of online businesses, powering everything from payment gateways to inventory management. But as APIs grow in complexity, so do their security risks. Attackers are constantly finding new ways to exploit APIs—often through overlooked vulnerabilities.  </p><p>This article will dive deep into  that threaten e-commerce APIs and provide <strong>developer-centric security strategies</strong> to mitigate them.  </p><h2>\n  \n  \n  Common Attack Vectors Targeting E-commerce APIs\n</h2><h3>\n  \n  \n  1️⃣ Broken Object Level Authorization (BOLA) – The #1 API Security Threat\n</h3><p> Most e-commerce platforms expose API endpoints that allow users to retrieve data (e.g., order history, customer details). If authorization isn't properly enforced, an attacker can manipulate request parameters to access </p><p> A  was breached when attackers exploited an API that allowed access to any order ID without proper authorization checks. The breach exposed thousands of customer transactions.  </p><ul><li>Always <strong>verify object-level access</strong> using user sessions or OAuth scopes.\n</li><li>Implement <strong>role-based access control (RBAC)</strong> and <strong>attribute-based access control (ABAC)</strong> for stricter permissions.\n</li><li>Regularly audit API endpoints for <strong>IDOR (Insecure Direct Object References).</strong></li></ul><h3>\n  \n  \n  2️⃣ Mass Assignment – How Attackers Modify Data They Shouldn’t\n</h3><p> Many APIs accept JSON payloads to update records. If an API allows unintended fields (e.g.,  or ), an attacker can escalate privileges or modify restricted data.  </p><p>If  isn’t validated, an attacker could promote themselves to admin!  </p><ul><li>Implement  for accepted parameters instead of blacklisting.\n</li><li>Use DTOs (Data Transfer Objects) to control what fields can be modified.\n</li><li>Apply strict  at both the frontend and backend layers.\n</li></ul><h3>\n  \n  \n  3️⃣ API Rate Limiting &amp; DDoS Protection – The Overlooked Risk\n</h3><p> Attackers exploit APIs by sending a high volume of requests, leading to  (DoS) or automated brute-force attacks.  </p><p> A payment API without rate limits was exploited using an , allowing fraudsters to test stolen credit card details at scale.  </p><ul><li>Implement  using API gateways like Cloudflare, AWS API Gateway, or Kong.\n</li><li>Use <strong>reCAPTCHA or device fingerprinting</strong> to detect and block bot-driven attacks.\n</li><li>Apply  or IP-based throttling for repeated failed requests.\n</li></ul><h3>\n  \n  \n  4️⃣ API Keys &amp; Authentication – Why Hardcoding is a Security Disaster\n</h3><p> Developers often <strong>accidentally expose API keys</strong> in public repositories, making it easy for attackers to <strong>gain access to critical services</strong>.  </p><ul><li>In 2023, <strong>several Fortune 500 companies</strong> were impacted when their API keys were , allowing attackers to access private customer data.\n</li><li>In another case, an attacker found <strong>hardcoded AWS credentials</strong> inside a mobile app's API request logs, leading to unauthorized access.\n</li></ul><ul><li>NEVER hardcode API keys—use <strong>environment variables or vaults</strong> like HashiCorp Vault or AWS Secrets Manager.\n</li><li>Rotate API keys regularly and <strong>scope permissions to the bare minimum.</strong></li><li>Implement <strong>HMAC (Hash-based Message Authentication Code) signing</strong> for additional security.\n</li></ul><h2>\n  \n  \n  Advanced API Security Best Practices\n</h2><p>Now that we’ve covered common attack vectors, let’s focus on  to fortify your e-commerce APIs.  </p><h3>\n  \n  \n  ✅ 1. Implement OAuth 2.0 and JWT for Secure Authentication\n</h3><ul><li>Use <strong>OAuth 2.0 with short-lived access tokens</strong> and refresh tokens.\n</li><li>Sign JWTs with a  and validate expiration times.\n</li><li>Store refresh tokens securely—<strong>NEVER expose them to the frontend.</strong></li></ul><h3>\n  \n  \n  ✅ 2. Enforce Zero-Trust API Access\n</h3><ul><li>Require  for every API request, even internal ones.\n</li><li>Use  for sensitive API communications.\n</li><li>Block unused API endpoints—don’t leave outdated versions exposed.\n</li></ul><h3>\n  \n  \n  ✅ 3. Monitor &amp; Log API Activity for Threat Detection\n</h3><ul><li>Deploy  tools like Datadog, Graylog, or ELK Stack.\n</li><li>Log API errors, authentication failures, and suspicious behavior.\n</li><li>Set up alerts for <strong>abnormal request patterns</strong> or <strong>high-failure login attempts.</strong></li></ul><h3>\n  \n  \n  ✅ 4. Implement Web Application Firewalls (WAFs) and API Security Tools\n</h3><ul><li>Use , AWS WAF, or Azure WAF to block malicious API requests.\n</li><li>Deploy <strong>runtime security monitoring</strong> with solutions like Tornix Cyber’s  to detect and prevent unauthorized API calls.\n</li></ul><h2>\n  \n  \n  How Tornix Cyber Can Help Secure Your E-commerce APIs\n</h2><p>While best practices can reduce risks, real-world API security requires <strong>continuous monitoring, intelligent threat detection, and proactive defense.</strong></p><p> to detect suspicious activity in real time.<strong>Automated vulnerability scanning</strong> to identify misconfigurations and insecure endpoints. to prevent automated bot-driven API attacks.<strong>Fraud detection mechanisms</strong> to identify unauthorized transactions and credential stuffing attempts.  </p><p>E-commerce businesses using <strong>Tornix Cyber's API security engine</strong> have seen a <strong>60% drop in fraudulent API requests</strong> and <strong>zero API-related breaches</strong> in the last 12 months.  </p><p>🚀 <strong>Want to protect your e-commerce APIs?</strong> Start a security audit with </p><p>API security isn’t just about <strong>checking a few security boxes</strong>—it’s about . Attackers evolve, and so should your security practices.  </p><p>By implementing <strong>robust authentication, monitoring, rate limiting, and proactive security tools</strong>, you can <strong>stay ahead of API threats</strong> and <strong>keep your e-commerce business safe</strong>.  </p><p>🔹 <strong>What’s your biggest API security challenge? Drop a comment below!</strong> 👇  </p>","contentLength":5234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introduction to Artificial Intelligence: Neural Networks and Intelligent Agents","url":"https://dev.to/devshiba/introduction-to-artificial-intelligence-neural-networks-and-intelligent-agents-4nna","date":1739733226,"author":"DevShiba","guid":759,"unread":true,"content":"<h2>\n  \n  \n  Introduction to Artificial Intelligence\n</h2><p>Artificial Intelligence (AI) is a branch of computer science that seeks to create systems capable of performing tasks that typically require human intelligence. Over time, AI has grown to encompass a vast set of subfields, including machine learning, deep learning, robotics, multi-agent systems, evolutionary computation, and more. Today, these technologies power solutions in computer vision, natural language processing, speech recognition, autonomous vehicles, and even creative fields like art and music generation.</p><h2>\n  \n  \n  Early Foundations: From Turing to Symbolic AI\n</h2><ul><li><strong>Alan Turing and the Turing Test</strong>: Often referred to as the “father of AI,” Alan Turing laid the groundwork for thinking about machine intelligence in the 1940s and 1950s. His Turing Test proposes that a machine can be considered intelligent if it can fool a human interrogator into believing it is human, solely through conversation.\n</li><li>: Early AI systems focused on symbolic reasoning, using rule-based systems (or “expert systems”) that relied on logic and explicitly defined knowledge. These systems excelled at tasks like medical diagnosis (e.g., MYCIN) and tax preparation, but struggled with uncertainty and tasks requiring large-scale data processing.</li></ul><h2>\n  \n  \n  Agent-Based Systems and Rule-Based Decision Making\n</h2><p>Modern AI often conceptualizes intelligent entities as : programs or robots that sense the environment through  and act upon it using . A simple approach to controlling an agent is with a —essentially a list of “if [condition], then [action]” statements. Although straightforward, these rule-based methods can scale up to surprisingly complex systems:</p><ol><li>: Respond directly to environmental inputs without maintaining an internal state (like a cleaning robot that immediately reacts to a dirty spot).</li><li>: Maintain a representation of the world to reason about unobserved aspects or predict future states.</li><li>: Choose actions that help them achieve explicit objectives.</li><li>: Aim to maximize a certain utility function, balancing trade-offs and probabilities of success.</li></ol><p>Such agents can also incorporate  or  to handle probabilistic decision-making—useful when the environment is uncertain (as discussed in some of the PDFs).</p><h2>\n  \n  \n  Neural Networks: The Core of Modern Machine Learning\n</h2><h3>\n  \n  \n  Basic Feed-Forward Networks (FNNs)\n</h3><p>A <strong>feed-forward neural network</strong> is the foundational architecture in deep learning. Information flows in one direction—from input layer to output layer—without cycles. These networks are trained using , which adjusts the  and  of each neuron to minimize an error metric. Although simple, feed-forward networks can tackle tasks like basic classification and regression.</p><h3>\n  \n  \n  Convolutional Neural Networks (CNNs)\n</h3><p>Convolutional Neural Networks specialize in  (and increasingly audio or text), using  to detect patterns such as edges, shapes, and textures. CNNs are behind many breakthroughs in :</p><ul><li> (e.g., classifying dog vs. cat).</li><li> (e.g., bounding boxes in autonomous driving).</li><li> (e.g., labeling each pixel in medical imaging).</li></ul><h3>\n  \n  \n  Recurrent Neural Networks (RNNs) and LSTMs\n</h3><p>RNNs introduce the concept of  in the network, allowing information to persist over multiple time steps—ideal for  such as time series or natural language.  </p><ul><li><strong>Long Short-Term Memory (LSTM)</strong> networks are a specialized type of RNN that mitigate the <strong>vanishing/exploding gradient</strong> problem, enabling them to learn long-range dependencies (e.g., entire sentences or paragraphs).</li></ul><h3>\n  \n  \n  Autoencoders (AEs) and Variational Autoencoders (VAEs)\n</h3><ul><li> compress data into a  and then reconstruct the input from that latent code. They are commonly used for , , or .\n</li><li><strong>Variational Autoencoders (VAEs)</strong> extend this idea by enforcing a probabilistic structure on the latent space, enabling controlled data generation (e.g., generating new handwritten digits after training on MNIST).</li></ul><h3>\n  \n  \n  Restricted Boltzmann Machines (RBMs), DBNs, and DBMs\n</h3><ul><li><strong>Restricted Boltzmann Machines (RBMs)</strong>: Energy-based models with a visible and a hidden layer, often used for feature extraction or as building blocks in deeper networks.\n</li><li><strong>Deep Belief Networks (DBNs)</strong>: Stacks of RBMs that can be  layer by layer, then fine-tuned with backpropagation.\n</li><li><strong>Deep Boltzmann Machines (DBMs)</strong>: A deeper extension of RBMs where multiple hidden layers can capture more complex patterns.</li></ul><h3>\n  \n  \n  Capsule Networks (CapsNets)\n</h3><p>Proposed by Geoffrey Hinton,  aim to preserve  in the data by grouping neurons into “capsules.” They tackle limitations of CNNs in recognizing objects from different perspectives, though they can be more computationally demanding.</p><h3>\n  \n  \n  Attention and Transformers\n</h3><p>An increasingly important development is the  architecture, which relies on  to process input sequences (text, for instance) in parallel rather than sequentially. Transformers underpin modern <strong>Large Language Models (LLMs)</strong> such as GPT and BERT, enabling superior performance on tasks like translation, text generation, and summarization.</p><h2>\n  \n  \n  Evolutionary Computation: Genetic Algorithms\n</h2><p>Inspired by Darwinian evolution,  simulate the process of natural selection. They maintain a  of candidate solutions—each represented by a “chromosome” of parameters—that evolve via  (random alterations),  (combining traits from two parents), and  (fitter solutions have higher probability to reproduce).  </p><ul><li>: Optimization in engineering (e.g., designing more efficient aircraft components), robotics (e.g., evolving control strategies), and even game strategy development.</li></ul><h2>\n  \n  \n  Advanced Generative Models: GANs and Diffusion Models\n</h2><h3>\n  \n  \n  Generative Adversarial Networks (GANs)\n</h3><p>A  consists of two networks—a  that creates synthetic data and a  that tries to distinguish between real and fake data. The two engage in a : the generator strives to fool the discriminator, and the discriminator strives to catch the fakes.  </p><ul><li>: Image synthesis (e.g., creating realistic human faces that do not exist), data augmentation (generating synthetic data to train other models), style transfer, and more.</li></ul><p> are a newer class of generative models that learn to reverse a  that gradually destroys data structure. By iteratively denoising random noise, they can generate high-fidelity images, often rivaling or surpassing GANs in terms of detail and diversity.</p><h2>\n  \n  \n  Large Language Models (LLMs)\n</h2><p>Modern AI has also been propelled by <strong>Large Language Models (LLMs)</strong>, which can handle huge amounts of text data and learn intricate linguistic patterns.  </p><ul><li>: LLMs break text into smaller units called tokens, which can be words, subwords, or even single characters, then process them in sequences.\n</li><li>: Each token is converted into a high-dimensional vector (embedding) that represents semantic meaning.\n</li><li>: LLMs typically use self-attention to capture long-range dependencies in text, making them exceptionally good at tasks like question answering, summarization, translation, and creative writing.</li></ul><h2>\n  \n  \n  Putting It All Together: The AI Ecosystem\n</h2><ol><li>: Sensors, cameras, and microphones collect data that feed into AI models (e.g., CNNs for images, RNNs or Transformers for audio).</li><li>: Decision trees, Bayesian networks, and rule-based systems provide interpretable logic, while deep learning models handle high-dimensional data.</li><li>: Neural networks—whether feed-forward, recurrent, or convolutional—are trained via large datasets. Advanced methods like GANs and diffusion models handle generative tasks.</li><li>: Agents (robotic or software) use actuators or APIs to interact with the environment, often guided by evolutionary algorithms or reinforcement learning strategies.</li><li>: As AI continues to expand, considerations like fairness, transparency, accountability, and safety become paramount.</li></ol><p>Artificial Intelligence today stands at the intersection of multiple paradigms—from classic rule-based expert systems to powerful neural architectures and evolutionary approaches. By combining agent-based designs, advanced neural network models (CNNs, RNNs, Transformers, GANs, etc.), and evolutionary computation, we can tackle an ever-growing list of complex real-world problems. The chart of neural networks you referenced illustrates just how diverse the deep learning landscape has become, reminding us that AI is both a mature field with decades of research behind it and a rapidly advancing frontier with breakthroughs still to come.</p><p><strong>References &amp; Further Reading</strong></p><ul><li>IA1 through IA6: Introductory lectures covering AI history, neural networks, genetic algorithms, agent-based systems, GANs, LLMs, and more.\n</li><li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … &amp; Bengio, Y. (2014). <em>Generative Adversarial Nets</em>. Advances in Neural Information Processing Systems.\n</li><li>Hinton, G. E., Osindero, S., &amp; Teh, Y. W. (2006). <em>A fast learning algorithm for deep belief nets</em>. Neural Computation.\n</li><li>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). . Nature.\n</li></ul>","contentLength":8896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Entra Connect Sync Architecture: A Deep Dive - Part 3","url":"https://dev.to/neontiger12/understanding-entra-connect-sync-architecture-a-deep-dive-part-3-oc2","date":1739733189,"author":"Merényi Mónika","guid":758,"unread":true,"content":"<p>Continuing our journey to better understand Entra Connect Sync let's see everything in action!</p><p> created in <a href=\"https://dev.to/neontiger12/deploying-and-configuring-a-hybrid-identity-lab-using-bicep-part-1-active-directory-setup-and-2eo7\">this post</a> , login.</p><p>Make sure that the Microsoft Azure AD Sync service is running:</p><p><strong>Start the Syncrhonization Service</strong></p><p><strong>Check what steps are included in the Delta Sync</strong></p><p>To make it clean and easy to follow, let's clear the previous Operations if you have one:</p><p>Now you can manually start a Delta Sync with PowerShell:</p><p>_Import-Module ADSync\nStart-ADSyncSyncCycle -PolicyType Delta _</p><p>Delta Sync only syncronizes the changed data, while the Initial Sync syncronizes all data from the connected data sources.</p><p>In the Operation Tab you can see the steps involved in the Delta Syc:</p><ol><li>Delta Import from the local Active Directory - CD to CS </li><li>Delta Import from Entra ID (shown as AAD) - CD to CS</li><li>Delta Synchronization from Entra ID (shown as AAD) - CS to MV</li><li>Delta Synchronization from Active Directory - CS to MW</li><li>Export to Entra ID (shown as AAD) - MW to CS</li><li>Export to Active Directory - MW to CS</li></ol><p>! Disable the sync scheduler now to make sure it won't automatically start a Delta Sync after you create a new user:</p><p><em>Set-ADSyncScheduler -SyncCycleEnabled $false</em></p><p>Active Directory Users and Computers</p><p>Users » right click » New » User</p><p><strong>Check if the object exists in the Connector Space</strong></p><p>Connector Space tab » Right click on your CS connected to Active Directory » Search</p><p>Click Search and order the list by the Object type</p><p>No \"New User\" object here.</p><p>To start a Delta Import right click on the Connector Space connected to the AD » Run » Delta Import</p><p>Check the Synchronization Statistics: 1 added item, our new user is replicated to the Connector Space.</p><p>This process of bringing in a new object from a connected data source into the Connector Space is called  (from the perspective of the Connector Space)</p><p>If you search again in the Connector Space you will find the replica of the New User there.</p><p>Check the object in the Metaverse. Since it hasn't been synchronized to the Metaverse yet, it should not be visible.</p><p>Metaverse Search Tab » Search</p><p>Our new user is not here.</p><p><strong>Run a Delta Import from Entra ID (shows as AAD).</strong>\nNo updates from Entra ID.</p><p><strong>Now let's run a Delta Synchronization on the local AD Connector Space</strong> to project our new user object to the Metaverse.</p><p>Right click on the Connector Space connected to the local AD » Run » Delta Syncrhonization</p><p>You can see the Synchronization Statistics: there is one new projection.</p><p>During the Inbound Synchronization step, if the object does not yet exist in the metaverse, it is  into the metaverse.</p><p> refers to the creation of a new object in the metaverse <strong>when there is no existing matching object</strong>.</p><p>If you search the Metaverse now you will find our new object.</p><p><strong>Do the Delta Syncronization on the Connector Space connected to the Entra ID (AAD).</strong>\nThere will be no changes comming from Entra ID.</p><p>Now check the object in the Connector Space connected to Entra ID (AAD). Search the Connector Space and organize the result by the Object Type.</p><p>By clicking the Properties of the new user (with the Display name blank) you can see it is flagged as \"\".</p><p>**Now let's do an Export on the Connector Space connected to Entra ID (AAD) **to export the newly created object to the cloud.</p><p>Right click on the Connector Space connected to the AAD » Run » Export</p><p>After in finish successfully you can see there is 1 added item.</p><p>Check if you can find the new object in the Connector Space connected to Entra ID (AAD). </p><p>You can see there is a new item with the Display name field blank.\n(if you don't see the Display Name column click on  and add it)</p><p>We are waiting for confirmation from Entra ID that the export was successful. Since this hasn't happened yet, the status shows \"Awaiting Export Confirmation.\"</p><p>Check the properties of the object:</p><p>If you check your users in Entra ID you can already see the newly added user:</p><p><strong>Let's get the confirmation from Entra ID (AAD)</strong>\nRun a Full Import on the Connector Space connected to Entra ID (AAD)</p><p>Check the new user again, now the object will show it's display name:</p><p>Even if you do a Delta Sync with PowerShell you still see the new object as Awaiting Export Confirmation after the Delta Sync successfully finished.</p><p>To try this create a new user like \"Delta New User\" and run a Delta Sync manually ( <em>Start-ADSyncSyncCycle -PolicyType Delta</em> ).\nCheck the object in the Connector Space connected to the Entra ID (AAD). It shows as Awaiting Export Confirmation.</p><p>The \"Awaiting Export Confirmation\" status means that the Sync Engine has successfully exported the new user object to Entra ID but has not yet received confirmation that Entra ID processed the change.</p><p>The Sync process (exporting the change from AD to Entra ID) is , meaning changes are sent to Entra ID, but the system waits for confirmation that they have been processed. Once Entra ID acknowledges the export, the status will update, and the object will be fully synchronized. \nUntil then, the object remains in this state, indicating the process is not yet complete. <p>\nConfirmation will occur during the next Sync Cycle or can be triggered manually by initiating an Import (full or delta) from Entra ID, as demonstrated in the example.</p></p>","contentLength":5098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why I Hate WordPress—But Why It’s Still Great and Necessary","url":"https://dev.to/itamartati/why-i-hate-wordpress-but-why-its-still-great-and-necessary-1lbg","date":1739732807,"author":"Itamar Tati","guid":757,"unread":true,"content":"<p>WordPress powers over 40% of the web, making it one of the most widely used content management systems (CMS) in existence. Yet, among software engineers—especially those who specialize in backend development and scalable web applications—WordPress has a reputation for being inefficient, bloated, and frustrating to work with.  </p><p>I hate WordPress. I’ve spent countless hours dealing with messy codebases, troubleshooting plugin conflicts, and optimizing slow sites. But here’s the thing: <strong>WordPress is still great and necessary because customers want it.</strong> No matter how much I dislike working with it, businesses and clients continue to demand WordPress solutions, and for good reason.  </p><h2>\n  \n  \n  1. Why Engineers Hate WordPress\n</h2><h3>\n  \n  \n  Spaghetti Code and Legacy Baggage\n</h3><p>WordPress started in 2003 as a simple blogging platform and evolved into a full-fledged CMS. Unfortunately, its core codebase carries a lot of legacy design choices, leading to:  </p><ul><li><strong>Global functions and variables:</strong> Instead of modern encapsulation, WordPress relies on global functions, making it easy to introduce conflicts.\n</li><li><strong>Mixing PHP, HTML, and Business Logic:</strong> Many themes and plugins contain PHP logic mixed directly into templates, violating software design best practices.\n</li><li><strong>Backwards Compatibility at All Costs:</strong> WordPress prioritizes compatibility with old plugins and themes, often at the expense of performance, security, and maintainability.\n</li></ul><p>Plugins are a double-edged sword: they make WordPress flexible, but they also create major headaches:  </p><ul><li><strong>Bloated and Inefficient Plugins:</strong> Many plugins load unnecessary scripts, slowing down websites.\n</li><li> Poorly maintained plugins are one of the leading causes of WordPress hacks.\n</li><li><strong>Compatibility Nightmares:</strong> Plugins frequently conflict with each other, leading to endless debugging.\n</li></ul><p>WordPress is not built with performance in mind:  </p><ul><li> WordPress stores everything in a MySQL database, which leads to inefficiencies as a site scales.\n</li><li> Many themes include unnecessary CSS and JavaScript, hurting page speed.\n</li><li> High-traffic sites require extensive optimization (e.g., caching, CDNs) to perform well.\n</li></ul><p>For non-developers, WordPress is easy to customize. But for engineers, it’s frustrating:  </p><ul><li><strong>Modifying Core Behavior Requires Workarounds:</strong> Hooks and filters feel more like hacks than clean abstractions.\n</li><li><strong>Gutenberg Editor Conflicts:</strong> The React-based block editor adds complexity.\n</li><li><strong>Lack of a Modern Development Workflow:</strong> Unlike frameworks like Next.js or Spring Boot, WordPress lacks standardized build processes, dependency management, and testable architecture.\n</li></ul><p>WordPress is a frequent hacking target:  </p><ul><li><strong>Frequent Vulnerabilities in Plugins and Themes</strong></li><li><strong>Brute Force Attacks on Login Pages</strong></li><li><strong>Lack of Default Security Best Practices</strong></li></ul><h2>\n  \n  \n  2. Why WordPress Is Still Great and Necessary\n</h2><p>Despite all these flaws, <strong>WordPress remains the best option for many businesses and non-technical users.</strong> Here’s why:  </p><p>Most clients don’t care about clean code, scalable architecture, or software best practices. They just want a site that looks good, works well, and is easy to manage. WordPress delivers on this.  </p><h3>\n  \n  \n  2. Rapid Development and Low Cost\n</h3><p>For businesses that don’t have the budget for a custom-built application, WordPress provides a cheap and effective solution. They can get an e-commerce store, a blog, or a portfolio site up and running within hours.  </p><h3>\n  \n  \n  3. Huge Ecosystem and Community Support\n</h3><p>With thousands of themes and plugins available, WordPress allows businesses to add functionality without hiring developers. And if something goes wrong, there are countless tutorials and forums for support.  </p><h3>\n  \n  \n  4. SEO and Marketing Advantages\n</h3><p>WordPress has excellent SEO plugins (like Yoast) and marketing integrations, making it easy for businesses to optimize their sites for search engines without needing technical expertise.  </p><h3>\n  \n  \n  5. It’s Familiar and User-Friendly\n</h3><p>Many business owners and content managers already know how to use WordPress. Switching to a custom-built CMS or headless system often requires retraining, which they don’t want.  </p><h2>\n  \n  \n  3. Better Alternatives Exist—But Clients Don’t Care\n</h2><p>For engineers building modern web applications, there are far better alternatives:  </p><ul><li> Next.js, Astro, or Hugo offer faster and more secure solutions.\n</li><li> Django, FastAPI, and Spring Boot provide scalable, maintainable backends.\n</li><li> Strapi, Sanity, and Contentful provide structured content management without WordPress bloat.\n</li></ul><p>But at the end of the day, most clients <strong>don’t want to hear about better alternatives</strong>—they just want a WordPress site that works.  </p><p>I hate WordPress. Many software engineers hate WordPress. But it’s still one of the most widely used and necessary tools in web development because it gives non-technical users exactly what they need: a simple, flexible, and cost-effective solution.  </p><p>As engineers, we can complain about WordPress all we want, but as long as businesses keep demanding it, we’ll continue to work with it—whether we like it or not.  </p>","contentLength":5010,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The RAIL Model: Making Web Apps Feel Lightning Fast","url":"https://dev.to/rijultp/the-rail-model-making-web-apps-feel-lightning-fast-o7p","date":1739732674,"author":"Rijul Rajesh","guid":756,"unread":true,"content":"<p>When it comes to web performance, speed isn’t the only thing that matters—how fast a website  to users is just as important. This is where Google's  comes in. Designed to improve user experience, RAIL helps developers build web apps that respond quickly and run smoothly. Let’s break down what RAIL stands for and how it can help improve your website.</p><p>RAIL stands for <strong>Response, Animation, Idle, and Load</strong>—four key aspects of web performance. The model provides clear goals to ensure a smooth and engaging user experience:</p><ul><li> A website should react to user interactions (like taps and clicks)  so it feels instant.</li><li> Smooth animations, scrolling, and transitions should run at <strong>60 frames per second (FPS)</strong>, meaning each frame has just  to render.</li><li> Use  efficiently to handle background tasks and prepare for upcoming interactions.</li><li> A web page should be <strong>interactive within 5 seconds</strong> (on mid-range devices with slow internet) to keep users engaged.</li></ul><p>Following these principles ensures your web apps feel fast and responsive, improving user experience and reducing frustration.</p><h2>\n  \n  \n  Breaking Down RAIL in Action\n</h2><p>When a user clicks or taps, the page should react immediately. A delay longer than  feels sluggish.</p><ul><li>Keep event handlers simple and quick.</li><li>Delay heavy operations until later.</li><li>Use  for non-urgent updates.</li></ul><h3>\n  \n  \n  Animation (16ms per frame)\n</h3><p>Animations, transitions, and scrolling should feel smooth at . Since one second has 1000ms, this means you only get .</p><ul><li>Use CSS animations for better performance.</li><li>Reduce JavaScript execution during animations.</li><li>Avoid excessive layout changes (also called layout thrashing).</li></ul><p>Idle moments are opportunities to process background tasks <strong>without delaying the next interaction</strong>.</p><ul><li>Use  to schedule non-critical work.</li><li>Split long operations into smaller chunks.</li><li>Prefetch resources that might be needed soon.</li></ul><h3>\n  \n  \n  Load (≤5s for interactive state)\n</h3><p>Users won’t wait long for a page to load. If your site isn’t interactive within , many users will leave.</p><ul><li>Optimize images and assets with compression and lazy loading.</li><li>Minimize JavaScript execution time by removing unnecessary code.</li><li>Load critical resources first using  and .</li></ul><p>By following RAIL, you’re focusing on how fast your website  rather than just raw loading speed. A site that feels quick keeps users engaged and improves conversions and retention.</p><p>RAIL isn’t just theory—it’s a practical approach backed by . Whether you're working on an online store, a web app, or a personal blog, applying RAIL guidelines can make your site feel  to use.</p><p>I came across RAIL when I was developing <a href=\"https://hexmos.com/liveapi?utm_source=dev.to&amp;utm_medium=article&amp;utm_campaign=npmlibs&amp;utm_content=rtp\">LiveAPI</a>.</p><p>LiveAPI is a Super-Convenient tool for generating API documentations within seconds, all we need to do is connect our git repository and LiveAPI will generate the Interactive API Docs like magic!</p><p>Excited about using RAIL? Let me know your thoughts in the comments!</p>","contentLength":2838,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harnessing the Power of DeepSeek on Azure with Power Automate","url":"https://dev.to/seenakhan/harnessing-the-power-of-deepseek-on-azure-with-power-automate-jdl","date":1739732505,"author":"Seena Khan","guid":755,"unread":true,"content":"<p>In today's rapidly evolving technological landscape, integrating advanced AI models into business workflows is more crucial than ever. One such powerful model is DeepSeek-R1, known for its exceptional reasoning capabilities. By deploying DeepSeek on Azure and integrating it with Power Automate, businesses can automate complex tasks, enhance decision-making, and streamline operations. This blog will guide you through the process of deploying DeepSeek on Azure and using it in Power Automate, enabling you to leverage cutting-edge AI technology effortlessly.</p><p>DeepSeek-R1 stands out due to its robust reasoning abilities, making it ideal for tasks that require logical decision-making and problem-solving. With its open-source framework, DeepSeek offers flexibility and cost-effectiveness, allowing businesses to customize the model for their specific needs. Additionally, deploying DeepSeek on Azure ensures scalability, security, and compliance, backed by Microsoft's reliable infrastructure</p><h4>\n  \n  \n  Step-by-Step Guide to Deploy DeepSeek on Azure\n</h4><ul><li>Login into Azure portal. Then select  service.  Select +Create and then select .</li></ul><ul><li><p>Enter the following details:</p><ul><li>Azure Subscription : All resources in an Azure subscription are billed together. </li><li>Azure Resource Group: A resource group is a collection of resources that share the same life cycle, permissions, and policies. </li><li>Workspace Name: Unique name that matches the constraints for naming on Azure.</li><li>Region: Choose the region closest to you and your customers. As this workspace we are going to deploy Deepseek R1 so please select East US2.</li><li>Storage Account: A storage account is used as the default datastore for the workspace. You may create a new Azure Storage resource or select an existing one in your subscription. Learn more: here)</li><li>Key vault: A key vault is used to store secrets and other sensitive information that is needed by the workspace. You may create a new Azure Key Vault resource or select an existing one in your subscription. </li><li>Application Insights: The workspace uses Azure Application Insights to store monitoring information about your deployed models. You may create a new Azure Application Insights resource or select an existing one in your subscription. </li><li>Container Registry: A container registry is used to register docker images used in training and deployments. To minimize costs, a new Azure Container Registry resource is created only after you build your first image. Alternatively, you may choose to create the resource now or select an existing one in your subscription. Here I have selected .</li></ul></li></ul><ul><li>After entering all the details please select Review+Create.</li><li>Once the validation done successfully please select Create.</li><li>After successfully deployed please select .</li></ul><ul><li>Select  to open the <strong>Azure AI Machine Learning Studio</strong>.</li></ul><ul><li>Select , then search for Deepseek R1 model and select Deepseek R1 from the list.</li></ul><ul><li>Enter the Deployment name and then select .</li></ul><ul><li>After successfully deployed the model, please copy the following details and keep it in a notepad.\n\n<ul><li>deepseek-r1: Chat Completion</li></ul></li></ul><h4>\n  \n  \n  Integrating DeepSeek with Power Automate\n</h4><ul><li>Login into Power Automate.</li><li>Select , then select  from +New Flow dropdown.</li></ul><ul><li>Enter the name of the flow, then select Manually trigger the flow and Create.</li></ul><ul><li><p>Select Manually trigger the flow and from the left pane under parameters Select +Add Input, then select Text.</p></li></ul><ul><li>Add an Action, search HTTP and then select HTTP.</li></ul><ul><li><p>Enter the following details:</p><ul><li>URI : Enter the copied <strong>deepseek-r1: Chat Completion</strong> URI from Deepseek R1 deployment.</li><li>Headers: content-type   : application/json\n      authorization  : KEY copied from Deepseek R1 deployment \n                       model.</li></ul></li></ul><div><pre><code>{\n  \"model\": \"deepseek-reasoner\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": @{triggerBody()?['text']}\n    }\n  ],\n  \"stream\": false\n}\n</code></pre></div><p>For content you can give the prompt from the manually trigger the flow action.</p><ul><li>Add a compose action and select add dynamic expression, then enter the below formula:\n</li></ul><div><pre><code>body('HTTP')?['choices'][0]['message/content']\n</code></pre></div><ul><li>Add a Send an Email action, then enter the following details as per the below picture reference:</li></ul><p>I have received an email which gives the answer i have asked in the prompt at the testing time.</p><p>Successfully completed a small example of deploying Deepseek R1 model in Azure and effectively use it in Power Automate.</p><p>Hope you enjoy the session.</p><p>Please leave a comment below if you have any further questions.</p><p>Happy Sharing !!!\nKeep Learning | Spread Knowledge | Stay blessed |</p>","contentLength":4547,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"love calulator project","url":"https://dev.to/yuvrajmunjal9/love-calulator-project-41hk","date":1739731962,"author":"YUVRAJ","guid":754,"unread":true,"content":"<p>Check out this Pen I made! Love calculator</p>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trending Mobile App Development Frameworks on 2025","url":"https://dev.to/resource_bunk_1077cab07da/trending-mobile-app-development-frameworks-on-2025-3i5g","date":1739731884,"author":"Resource Bunk","guid":740,"unread":true,"content":"<p>Imagine building a groundbreaking mobile app that not only runs seamlessly across devices but also accelerates your time to market—this isn’t just a dream for 2025, it’s within your grasp. Today’s evolving digital landscape demands that developers and businesses alike harness the best tools available. In this article, we’ll explore the <a href=\"https://www.techugo.com/blog/explore-the-top-10-mobile-app-development-frameworks-to-power-your-app-in-2025/\" rel=\"noopener noreferrer\">top mobile app development frameworks 2025</a> providing actionable insights to help you choose the right technology for your next project. For additional background, check out <a href=\"https://www.c-metric.com/blog/top-mobile-app-development-frameworks-2025/\" rel=\"noopener noreferrer\">this comprehensive guide on mobile frameworks</a>.</p><h2>\n  \n  \n  Why Mobile App Development Frameworks 2025 Matter\n</h2><p>In a world where smartphones and tablets are integral to our daily lives, the framework you choose to build your app can make all the difference. Mobile app development frameworks are the building blocks that allow you to:</p><ul><li> Maintain a single codebase, minimizing the need for separate development teams.\n</li><li> Deliver uniform user experiences across Android, iOS, and even desktop platforms. See insights on <a href=\"https://en.wikipedia.org/wiki/Cross-platform_software\" rel=\"noopener noreferrer\">cross-platform strategies</a>.</li><li> Easily integrate emerging technologies like <a href=\"https://www.ibm.com/cloud/learn/what-is-artificial-intelligence\" rel=\"noopener noreferrer\">AI</a>, <a href=\"https://www.microsoft.com/en-us/internet-of-things\" rel=\"noopener noreferrer\">IoT</a>, and <a href=\"https://developer.oculus.com/\" rel=\"noopener noreferrer\">AR/VR</a> to stay ahead of the curve.</li></ul><p>By embracing modern frameworks, you not only future-proof your project but also empower yourself to innovate faster and more efficiently. Additional industry insights can be found at <a href=\"https://www.forbes.com/technology/\" rel=\"noopener noreferrer\">Forbes Technology Council</a> and <a href=\"https://techcrunch.com/\" rel=\"noopener noreferrer\">TechCrunch</a>.</p><h2>\n  \n  \n  How to Choose the Right Framework\n</h2><p>Before diving into the details, ask yourself these questions:</p><p>With these questions in mind, let’s explore the most promising mobile app development frameworks for 2025.</p><h2>\n  \n  \n  Top Mobile App Development Frameworks 2025\n</h2><p><a href=\"https://flutter.dev\" rel=\"noopener noreferrer\">Flutter</a> (developed by Google) continues to lead the pack in mobile app development frameworks 2025. It uses the <a href=\"https://dart.dev\" rel=\"noopener noreferrer\">Dart programming language</a> to offer a unified, expressive, and efficient way to build natively compiled applications for mobile, web, and desktop.  </p><ul><li> Customize your UI effortlessly.</li><li> Directly compiles to native code for smooth, responsive apps.</li></ul><p><p>\nIf you’re looking to build visually stunning apps with rapid iterations, Flutter should be your go-to framework. Experiment with its hot reload feature to iterate designs quickly. Additional Flutter resources can be found on </p><a href=\"https://flutterawesome.com\" rel=\"noopener noreferrer\">Flutter Awesome</a>.</p><p><a href=\"https://reactnative.dev\" rel=\"noopener noreferrer\">React Native</a> (maintained by Facebook) empowers developers to build mobile apps using JavaScript and <a href=\"https://reactjs.org\" rel=\"noopener noreferrer\">React</a>. Its ability to use native components means your app feels truly native on both iOS and Android.  </p><ul><li> Code once, deploy everywhere.</li><li> Enhance productivity by seeing changes in real time.</li></ul><p><p>\nFor those who already have JavaScript expertise and want to create high-performance apps quickly, React Native is an excellent choice. Consider building a small prototype to evaluate its capabilities for your project. More tutorials are available at </p><a href=\"https://www.reactnative.express\" rel=\"noopener noreferrer\">React Native Express</a>.</p><p><a href=\"https://en.wikipedia.org/wiki/Xamarin\" rel=\"noopener noreferrer\">Xamarin</a>, Microsoft’s evolution to <a href=\"https://dotnet.microsoft.com/apps/maui\" rel=\"noopener noreferrer\">.NET MAUI</a> integrates seamlessly with Visual Studio. It allows developers to write C# code that runs on Android, iOS, macOS, and Windows.</p><ul><li> Streamline development across platforms.</li><li> Enjoy near-native speed with access to platform-specific APIs.</li></ul><p><p>\nIf you’re invested in the Microsoft ecosystem or have a team proficient in C#, .NET MAUI offers a powerful environment to build cross-platform apps efficiently. For more information, check out the </p><a href=\"https://docs.microsoft.com/en-us/dotnet/maui/\" rel=\"noopener noreferrer\">.NET MAUI documentation</a>.</p><p><a href=\"https://ionicframework.com\" rel=\"noopener noreferrer\">Ionic</a> uses web technologies—HTML, CSS, and JavaScript—to build hybrid mobile apps that run on multiple platforms. It’s a favorite for projects where rapid development and cost efficiency are priorities. (<a href=\"https://ionicframework.com/blog\" rel=\"noopener noreferrer\">Learn more on Ionic’s official blog</a>)</p><ul><li> Speed up development with ready-made elements.</li><li> Ideal for projects with tight budgets and timelines.</li></ul><p><p>\nFor startups and projects that need to launch quickly without compromising on design, Ionic offers a compelling solution. Leverage its extensive plugin library to integrate native device features seamlessly. Additional resources are available on </p><a href=\"https://ionicacademy.com\" rel=\"noopener noreferrer\">Ionic Academy</a>.</p><p><a href=\"https://cordova.apache.org\" rel=\"noopener noreferrer\">Apache Cordova</a> allows you to create mobile apps using standard web technologies. It packages your web code as a native app, offering a cost-effective path to cross-platform deployment. (<a href=\"https://cordova.apache.org/docs/en/latest/\" rel=\"noopener noreferrer\">Cordova documentation</a>)</p><p><strong>Why Choose Apache Cordova?</strong></p><ul><li> Use your existing web skills to build mobile apps.</li><li><strong>Extensive Plugin Ecosystem:</strong> Easily access device features like the camera, GPS, and accelerometer.</li><li> Supports multiple platforms with minimal configuration.</li></ul><p><p>\nIf you’re transitioning from web development and want to enter mobile app development without a steep learning curve, Apache Cordova is a great starting point. Test your app on real devices using </p><a href=\"https://www.lambdatest.com\" rel=\"noopener noreferrer\">cloud-based testing tools</a>.</p><p><a href=\"https://nativescript.org\" rel=\"noopener noreferrer\">NativeScript</a> provides direct access to native APIs using JavaScript or TypeScript, ensuring that your mobile app has full native performance without writing platform-specific code. (<a href=\"https://docs.nativescript.org\" rel=\"noopener noreferrer\">NativeScript documentation</a>)</p><ul><li> No need for wrapper libraries—use native APIs directly.</li><li> Supports Angular and Vue for added productivity. (<a href=\"https://nativescript-vue.org\" rel=\"noopener noreferrer\">NativeScript-Vue</a>)</li><li> Enjoy the benefits of native UI components and system integrations.</li></ul><p><p>\nFor developers who desire the flexibility of JavaScript with the power of native performance, NativeScript is a standout option. Explore its integration with your preferred framework on </p><a href=\"https://github.com/NativeScript/NativeScript\" rel=\"noopener noreferrer\">NativeScript’s GitHub</a>.</p><p><a href=\"https://developer.apple.com/xcode/swiftui/\" rel=\"noopener noreferrer\">SwiftUI</a> is Apple’s modern framework for building native iOS apps using a declarative syntax. It allows you to write less code while achieving more expressive and dynamic UIs. (<a href=\"https://developer.apple.com/tutorials/swiftui\" rel=\"noopener noreferrer\">Apple SwiftUI Tutorials</a>)</p><ul><li> Write concise code that’s easy to read and maintain.</li><li> Works naturally with other Apple development tools.</li><li> Leverage state-driven updates to keep your UI responsive.</li></ul><p><p>\nIf your primary audience is on iOS and you want to create apps that leverage the latest Apple design trends, SwiftUI is the ideal choice. Use its preview feature to rapidly iterate on your design and optimize user experience.</p></p><p><a href=\"https://kotlinlang.org\" rel=\"noopener noreferrer\">Kotlin</a> has become the language of choice for Android app development due to its concise syntax and robust performance. It’s fully interoperable with Java and has strong support from Google. (<a href=\"https://kotlinlang.org/docs/home.html\" rel=\"noopener noreferrer\">Kotlin Documentation</a>)</p><ul><li><strong>Modern Language Features:</strong> Reduce boilerplate code with Kotlin’s concise syntax.</li><li> Built-in null safety features minimize runtime errors.</li><li> Optimized for Android, ensuring smooth and fast applications.</li></ul><p><p>\nFor Android-focused projects, Kotlin offers a modern alternative to Java. If you’re starting a new Android project or migrating an existing one, consider Kotlin to take advantage of its productivity and performance benefits.</p></p><p><a href=\"https://tauri.app\" rel=\"noopener noreferrer\">Tauri</a> is gaining traction as a lightweight framework for building cross-platform apps with <a href=\"https://www.rust-lang.org\" rel=\"noopener noreferrer\">Rust</a> and <a href=\"https://www.typescriptlang.org\" rel=\"noopener noreferrer\">TypeScript</a>. Though primarily known for desktop apps, its mobile capabilities are emerging, offering a promising future for developers seeking efficiency and low overhead. (<a href=\"https://github.com/tauri-apps/tauri\" rel=\"noopener noreferrer\">Tauri on GitHub</a>)</p><p><p>\nKeep an eye on Tauri if you’re interested in a cutting-edge framework that prioritizes performance and security. Experiment with small projects to understand its potential and limitations in mobile contexts.</p></p><p>The mobile app development frameworks 2025 landscape is rich with options—each tailored to different needs and skill sets. Whether you’re a developer striving for rapid prototyping with <a href=\"https://flutter.dev\" rel=\"noopener noreferrer\">Flutter</a>, <a href=\"https://reactnative.dev\" rel=\"noopener noreferrer\">React Native</a>, or an enterprise aiming for robust performance with <a href=\"https://dotnet.microsoft.com/apps/maui\" rel=\"noopener noreferrer\">.NET MAUI</a> or <a href=\"https://kotlinlang.org\" rel=\"noopener noreferrer\">Kotlin</a>, there’s a framework that fits your vision.</p><h3>\n  \n  \n  Action Steps to Get Started\n</h3><ol><li><p><strong>Assess Your Requirements:</strong><p>\nList the core features and performance metrics your app must meet. For more guidance, see </p><a href=\"https://www.smashingmagazine.com/2018/02/mobile-app-design-checklist/\" rel=\"noopener noreferrer\">this checklist</a>.</p></li><li><p><strong>Evaluate Your Team’s Expertise:</strong><p>\nChoose a framework that aligns with the programming languages your team knows best. Explore language-specific resources like </p><a href=\"https://developer.mozilla.org\" rel=\"noopener noreferrer\">MDN Web Docs</a> for JavaScript and <a href=\"https://docs.microsoft.com/en-us/learn/\" rel=\"noopener noreferrer\">Microsoft Learn</a> for C#.</p></li><li><p><p>\nBuild a small prototype using your top framework choices to compare performance, ease of use, and scalability. (</p><a href=\"https://uxdesign.cc/prototyping-101-how-to-create-a-prototype-in-5-steps-94d7953d5e3e\" rel=\"noopener noreferrer\">Learn how to prototype fast</a>).</p></li><li><p><strong>Engage with the Community:</strong><p>\nTap into online forums, webinars, and documentation to stay updated on best practices and emerging trends. Resources like </p><a href=\"https://stackoverflow.com\" rel=\"noopener noreferrer\">Stack Overflow</a> and <a href=\"https://dev.to\">Dev.to</a> are invaluable.</p></li><li><p><p>\nSelect a framework that not only meets your current needs but also offers the flexibility to integrate emerging technologies like </p><a href=\"https://ai.google/\" rel=\"noopener noreferrer\">AI</a>, <a href=\"https://www.ibm.com/internet-of-things\" rel=\"noopener noreferrer\">IoT</a>, and <a href=\"https://developers.google.com/ar\" rel=\"noopener noreferrer\">AR/VR</a>.</p></li></ol><p>Your journey in mobile app development is an opportunity to push boundaries and create experiences that resonate with users. With the right mobile app development frameworks 2025, you can streamline your process, cut down on costs, and deliver apps that not only meet today’s standards but also pave the way for tomorrow’s innovations. The tools are in your hands—now is the time to build something extraordinary.</p><p>Embrace these frameworks, experiment boldly, and let your creativity lead the way to success in 2025 and beyond.</p><h2>\n  \n  \n  Free Resources &amp; Giveaways\n</h2><p>As you embark on your development journey, check out these free giveaways and cheat sheets that have been downloaded by hundreds of developers:</p><h2>\n  \n  \n  Additional Reading &amp; External Resources\n</h2><p>Your journey in mobile app development is supported by a wealth of resources and free tools. Embrace these frameworks, explore the linked resources, and download the free cheat sheets to boost your productivity. The future of mobile app development is bright—seize the opportunity and build something extraordinary in 2025 and beyond!</p><h3>\n  \n  \n  Earn $100 Fast: AI + Notion Templates\n</h3><p>Do you want to make extra money quickly? This guide shows you how to create and sell Notion templates step by step. Perfect for beginners or anyone looking for an easy way to start earning online.</p><ul><li> Follow a simple process to create templates people want and will buy.</li><li> Learn to use tools like ChatGPT to design and improve templates.</li><li> More people are using Notion every day, and they need templates to save time and stay organized.</li></ul><ul><li> Ready-made prompts to spark ideas and create templates faster.</li><li> Stay on track as you work.</li></ul><ul><li> Learn everything from idea to sale.</li><li><strong>How to Find Popular Ideas:</strong> Research trends and needs.</li><li> Tips for improving templates with AI tools.</li><li><strong>Making Templates User-Friendly:</strong> Simple tips for better design.</li><li> Advice on sharing and selling on platforms like Gumroad or Etsy.</li><li> Solutions for issues like low sales or tricky designs.</li></ul><ul><li>Anyone who wants to make extra money online.</li><li>People who love using Notion and want to share their ideas.</li><li>Creators looking for a simple way to start selling digital products.</li></ul><h3>\n  \n  \n  💰 <strong>Want to Earn 40% Commission?</strong></h3><p>Join our affiliate program and start making money by promoting ! Earn 40% on every sale you refer.  </p><p>You'll get on average around 5$ per sell and for bundled products it will be around 40$ per sale. (So just share it and make money with worrying about product creation and maintanence)</p>","contentLength":10572,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meet Checkmate.so — The Open-Source Tool to Keep Your Servers in Check!","url":"https://dev.to/satwikloka/meet-checkmateso-the-open-source-tool-to-keep-your-servers-in-check-37cp","date":1739731798,"author":"Satwikloka","guid":739,"unread":true,"content":"<p>Think of  as your friendly assistant that never sleeps. It keeps track of your servers , giving you real-time insights and alerts so you can prevent issues before they even happen. And the best part? , meaning you can tweak it, contribute to it, and make it work exactly the way you want.  </p><h2>\n  \n  \n  Why You’ll Love Checkmate.so\n</h2><p>✅  — Get a clear, real-time view of your server’s health, from CPU usage to memory and network stats. — No complicated dashboards or overwhelming graphs. Just a clean, simple interface that tells you what you need to know. — If something goes wrong, you’ll know ASAP. Set up notifications so you can act fast before small problems turn into big ones. — Whether you’re running one server or a whole fleet, Checkmate.so scales with you. — You can customize it, improve it, and even contribute to making it better for everyone. No hidden fees, no limitations — just pure, community-driven innovation.  </p><h2>\n  \n  \n  How to Get Started (It’s Super Simple!)\n</h2><ol><li> — The website has everything you need to get started.\n</li><li> — The documentation is beginner-friendly and walks you through installation step by step.\n</li><li> — Get it up and running in no time by following the easy instructions.\n</li><li> — Sit back and let Checkmate.so do its thing! You’ll get alerts and insights without lifting a finger.\n</li></ol><p>Let’s be real — server management shouldn’t be a nightmare. With , you can stop worrying about unexpected crashes and performance issues. It’s <strong>simple, smart, completely free, and open-source</strong>.  </p><p>🚀  Give it a try today and make your life easier. Your servers (and your sanity) will thank you!  </p>","contentLength":1634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making an Effective Application Security Program: Strategies, methods and tools for the best outcomes","url":"https://dev.to/lynxfelony1/making-an-effective-application-security-program-strategies-methods-and-tools-for-the-best-jlm","date":1739731716,"author":"Smart Mohr","guid":738,"unread":true,"content":"<p>AppSec is a multifaceted and robust method that goes beyond the simple vulnerability scan and remediation. A holistic, proactive approach is required to incorporate security into every phase of development. The constantly evolving threat landscape as well as the growing complexity of software architectures is driving the need for an active, holistic approach. <a href=\"https://www.youtube.com/watch?v=vZ5sLwtJmcU\" rel=\"noopener noreferrer\">https://www.youtube.com/watch?v=vZ5sLwtJmcU</a> This comprehensive guide explores the fundamental elements, best practices, and the latest technologies that make up the highly efficient AppSec program, which allows companies to fortify their software assets, mitigate risk, and create a culture of security first development. </p><p>At the core of a successful AppSec program lies an important shift in perspective that views security as an integral part of the development process, rather than a thoughtless or separate project. This paradigm shift requires close collaboration between security, developers operations, and the rest of the personnel. It helps break down the silos that hinder communication, creates a sense sharing responsibility, and encourages an approach that is collaborative to the security of applications that are created, deployed or manage. DevSecOps lets organizations integrate security into their development processes. This will ensure that security is taken care of throughout the entire process beginning with ideation, development, and deployment up to ongoing maintenance. </p><p>This collaborative approach relies on the development of security standards and guidelines that provide a structure for secure the coding process, threat modeling, and vulnerability management. These guidelines should be based on industry best practices, like the OWASP Top Ten, NIST guidelines, and the CWE (Common Weakness Enumeration) in addition to taking into account the unique needs and risk profiles of each organization's particular applications as well as the context of business. By codifying these policies and making available to all parties, organizations can provide a consistent and standard approach to security across their entire application portfolio. </p><p>To implement these guidelines and make them practical for development teams, it is important to invest in thorough security training and education programs. The goal of these initiatives is to equip developers with the knowledge and skills necessary to create secure code, recognize the potential weaknesses, and follow best practices for security during the process of development. Training should cover a wide array of subjects including secure coding methods and the most common attack vectors, to threat modelling and security architecture design principles. Organizations can build a solid base for AppSec through fostering an environment that encourages constant learning, and by providing developers the tools and resources they require to integrate security into their work. </p><p>Security testing is a must for organizations. and verification processes in addition to training to find and fix weaknesses prior to exploiting them. This is a multi-layered process which includes both static and dynamic analysis techniques, as well as manual penetration testing and code review. Early in the development cycle, Static Application Security Testing tools (SAST) are a great tool to identify vulnerabilities such as SQL Injection, Cross-Site Scripting (XSS) and buffer overflows. Dynamic Application Security Testing tools (DAST) are on the other hand, can be used for simulated attacks against running applications to identify vulnerabilities that might not be identified through static analysis. </p><p>The automated testing tools are extremely useful in finding weaknesses, but they're not a solution. manual penetration testing performed by security professionals is essential to discover the business logic-related weaknesses that automated tools might overlook. By combining automated testing with manual validation, businesses can obtain a more complete view of their application security posture and prioritize remediation efforts based on the impact and severity of identified vulnerabilities. </p><p>In order to further increase the effectiveness of an AppSec program, organizations must take into consideration leveraging advanced technology such as artificial intelligence (AI) and machine learning (ML) to boost their security testing capabilities and vulnerability management. AI-powered tools are able to look over large amounts of data from applications and code and spot patterns and anomalies that may signal security concerns. These tools also learn from past vulnerabilities and attack techniques, continuously improving their abilities to identify and stop emerging threats. </p><p>Code property graphs are an exciting AI application that is currently in AppSec. They can be used to identify and correct vulnerabilities more quickly and effectively. CPGs are a comprehensive, symbolic representation of an application's codebase, capturing not only the syntactic structure of the code, but additionally the intricate relationships and dependencies between various components. AI-driven tools that utilize CPGs can perform a deep, context-aware analysis of the security posture of an application, and identify vulnerabilities which may have been missed by conventional static analyses. </p><p>CPGs can be used to automate vulnerability remediation by applying AI-powered techniques to repair and transformation of code. Through understanding the semantic structure of the code as well as the characteristics of the weaknesses, AI algorithms can generate targeted, context-specific fixes that address the root cause of the problem instead of simply treating symptoms. This process will not only speed up treatment but also lowers the chance of breaking functionality or creating new weaknesses. </p><p>Integration of security testing and validating in the continuous integration/continuous deployment (CI/CD) pipeline is another key element of an effective AppSec. Automating security checks, and integrating them into the build-and-deployment process allows companies to identify security vulnerabilities early, and keep the spread of vulnerabilities to production environments. This shift-left security approach allows quicker feedback loops and reduces the amount of effort and time required to detect and correct problems. </p><p>To reach this level, they should put money into the right tools and infrastructure that can enable their AppSec programs. Not only should these tools be utilized for security testing as well as the platforms and frameworks which allow integration and automation. Containerization technologies like Docker and Kubernetes are able to play an important part in this, creating a reliable, consistent environment to run security tests as well as separating the components that could be vulnerable. </p><p>Effective collaboration tools and communication are just as important as the technical tools for establishing the right environment for safety and enabling teams to work effectively in tandem. Issue tracking tools, such as Jira or GitLab will help teams focus on and manage security vulnerabilities. Chat and messaging tools like Slack or Microsoft Teams can facilitate real-time communication and knowledge sharing between security professionals and development teams. </p><p>Ultimately, the success of an AppSec program does not rely only on the tools and technologies employed, but also on the employees and processes that work to support them. To build a culture of security, you require the commitment of leaders, clear communication and a dedication to continuous improvement. Companies can create an environment that makes security more than a box to check, but rather an integral element of development by fostering a sense of responsibility, encouraging dialogue and collaboration, providing resources and support and promoting a belief that security is an obligation shared by all. </p><p>To ensure that their AppSec programs to be effective over the long term companies must establish relevant metrics and key performance indicators (KPIs). These KPIs help them keep track of their progress and pinpoint improvement areas. The metrics must cover the whole lifecycle of the application, from the number and types of vulnerabilities that are discovered in the development phase through to the time needed for fixing issues to the overall security level. autonomous agents for appsec These indicators can be used to show the benefits of AppSec investment, spot patterns and trends as well as assist companies in making data-driven choices about the areas they should concentrate on their efforts. </p><p>To stay current with the constantly changing threat landscape and new best practices, organizations must continue to pursue learning and education. This may include attending industry-related conferences, participating in online training programs, and collaborating with outside security experts and researchers to stay abreast of the most recent developments and techniques. Through the cultivation of a constant culture of learning, companies can ensure that their AppSec applications are able to adapt and remain resistant to the new challenges and threats. </p><p>It is also crucial to recognize that application security isn't a one-time event and is an ongoing process that requires a constant dedication and investments. Organizations must constantly reassess their AppSec strategy to ensure that it remains effective and aligned with their goals for business as new technologies and development practices emerge. By adopting a continuous improvement mindset, encouraging collaboration and communication, and using advanced technologies like CPGs and AI, organizations can create an efficient and flexible AppSec program that will not only protect their software assets, but also let them innovate in a constantly changing digital landscape. <a href=\"https://www.youtube.com/watch?v=WoBFcU47soU\" rel=\"noopener noreferrer\">autonomous agents for appsec</a></p>","contentLength":9893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Newly Launched 30+ Github Repositories for Your Next Project.","url":"https://dev.to/gittech/newly-launched-20-github-repositories-for-your-next-project-4ah5","date":1739731373,"author":"Gittech","guid":737,"unread":true,"content":"<h4>\n  \n  \n  1. Funtrace: A C/C++ function call tracer for x86/Linux\n</h4><h4>\n  \n  \n  2. Bloodflowtrixi.jl – 1D and 2D blood flow models for arterial circulation\n</h4><h4>\n  \n  \n  3. Turn Entire YouTube Playlists to Refined Text Books (In Any Language)\n</h4><h4>\n  \n  \n  4. Get Started with Vibe Coding\n</h4><h4>\n  \n  \n  5. gh-ssh-add – secure GitHub repo access on the server\n</h4><h4>\n  \n  \n  6. OmniParser V2 – A simple screen parsing tool towards pure vision based GUI agent\n</h4><h4>\n  \n  \n  7. Schemesh: Fusion between Unix shell and Lisp REPL\n</h4><h4>\n  \n  \n  8. Simple collaborative filtering in pure PostgreSQL\n</h4><h4>\n  \n  \n  9. macOS \"Steam Play\"-like integration with Windows games and CrossOver\n</h4><h4>\n  \n  \n  10. Jill – a functional programming language for the Nand2Tetris platform\n</h4><h4>\n  \n  \n  11. HackerBuzz an cross-platform HN client built using React Native\n</h4><h4>\n  \n  \n  12. I created a media player with AI-generated subtitles for Windows\n</h4><h4>\n  \n  \n  13. A transient feed aggregator built with Gleam\n</h4><h4>\n  \n  \n  14. CSSGrid – small HTML and CSS-grid layout engine (Nim)\n</h4><h4>\n  \n  \n  15. Deepseek R1 Distill 8B Q40 on 4 x Raspberry Pi 5, 6.43 tok/s (eval 11.68 tok/s)\n</h4><h4>\n  \n  \n  16. Distributed-Llama: Connect home devices into a cluster for LLM inference\n</h4><h4>\n  \n  \n  17. Lunatik v3.6 Released\n</h4><h4>\n  \n  \n  18. Cr: Exec: The improved exec lib inspired from Crystal\n</h4><h4>\n  \n  \n  19. Kind 0.27.0: containerd v2 and Kubernetes 1.32\n</h4><h4>\n  \n  \n  20. Open source lists of proxy IP addresses used by bots\n</h4><h4>\n  \n  \n  21. Kernighan's Law: You are not smart enough to debug(2020)\n</h4><h4>\n  \n  \n  22. Block AI bots crawlers on Nginx\n</h4><h4>\n  \n  \n  23. Synergetica – A Modern, End-to-End Genetic Circuit Design Desktop App\n</h4><h4>\n  \n  \n  24. Akvorado: Flow Collector, Enricher and Visualizer\n</h4><h4>\n  \n  \n  25. Elixir Bumblebee: Pre-Trained Neural Network Models in Axon\n</h4><h4>\n  \n  \n  26. A Go tool to turn codebases into structured Markdown for LLMs\n</h4><h4>\n  \n  \n  27. Are LLMs able to play the card game Set?\n</h4><h4>\n  \n  \n  28. OCI Registry as Storage\n</h4><h4>\n  \n  \n  29. Kreuzberg – Modern async Python library for document text extraction\n</h4><h4>\n  \n  \n  30. Mynav: Workspace and Session Management TUI\n</h4><h4>\n  \n  \n  31. Blueprint: Reverse engineer server configuration (2013)\n</h4><h4>\n  \n  \n  32. Duty – TypeScript Workflow Orchestration\n</h4><h4>\n  \n  \n  34. oEmbedPy is oEmbed client library for Python 3.x\n</h4><h4>\n  \n  \n  35. A 3D-printable flatbed knitting machine\n</h4><h4>\n  \n  \n  36. Rust Without a Borrow Checker\n</h4><h4>\n  \n  \n  37. Kindle_download_helper\n</h4><h4>\n  \n  \n  28. Cybersecurity links; a resource dump I'm maintaining\n</h4><h2>\n  \n  \n  📌 Additional Resources / Cheat Sheets\n</h2><ul><li><p><strong>Stealth Tracerouting with 0trace – The Ultimate Cheat Sheet!</strong> - <a href=\"https://0x7bshop.gumroad.com/l/hmrtuc?layout=profile\" rel=\"noopener noreferrer\">Download Here</a></p></li></ul><ul><li><strong>File Compression in Terminal with the Ultimate 7‑Zip Cheat Sheet! 🚀</strong> - <a href=\"https://0x7bshop.gumroad.com/l/urfoda?layout=profile\" rel=\"noopener noreferrer\">Download Here</a></li></ul><h3>\n  \n  \n  🕵️‍♂️ Forensics &amp; Network Analysis\n</h3><ul><li><p><strong>Stealth Network Sniffing with This Ultimate 'Above' Tool Cheat Sheet!</strong> - <a href=\"https://0x7bshop.gumroad.com/l/nkvkqd?layout=profile\" rel=\"noopener noreferrer\">Download Here</a></p></li><li><p><strong>Advanced Forensic Format (AFF) Toolkit's Ultimate Cheat Sheet</strong> - <a href=\"https://0x7bshop.gumroad.com/l/dmhqn?layout=profile\" rel=\"noopener noreferrer\">Download Here</a></p></li></ul><h3>\n  \n  \n  Earn $100 Fast: AI + Notion Templates\n</h3><p>Do you want to make extra money quickly? This guide shows you how to create and sell Notion templates step by step. Perfect for beginners or anyone looking for an easy way to start earning online.</p><ul><li> Follow a simple process to create templates people want and will buy.</li><li> Learn to use tools like ChatGPT to design and improve templates.</li><li> More people are using Notion every day, and they need templates to save time and stay organized.</li></ul><ul><li> Ready-made prompts to spark ideas and create templates faster.</li><li> Stay on track as you work.</li></ul><ul><li> Learn everything from idea to sale.</li><li><strong>How to Find Popular Ideas:</strong> Research trends and needs.</li><li> Tips for improving templates with AI tools.</li><li><strong>Making Templates User-Friendly:</strong> Simple tips for better design.</li><li> Advice on sharing and selling on platforms like Gumroad or Etsy.</li><li> Solutions for issues like low sales or tricky designs.</li></ul><ul><li>Anyone who wants to make extra money online.</li><li>People who love using Notion and want to share their ideas.</li><li>Creators looking for a simple way to start selling digital products.</li></ul><h3>\n  \n  \n  💰 <strong>Want to Earn 40% Commission?</strong></h3><p>Join our affiliate program and start making money by promoting ! Earn 40% on every sale you refer.  </p><p>You'll on average around 5$ per sell and for bundled products it will be around 40$ per sale. (So just share it and make money with worrying about product creation and maintanence)</p>","contentLength":4250,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"126/365 | ¥10M Job Challenge - Why remote work?","url":"https://dev.to/kameken100/126365-y10m-job-challenge-why-remote-work-37dn","date":1739731085,"author":"転職カメ","guid":736,"unread":true,"content":"<p>The main reason is that, as everyone knows, Tokyo’s packed commuter trains are exhausting—you feel drained even before you start work. For those who live farther away, it’s not just physically tiring but also mentally stressful. Remote work doesn’t necessarily mean working fewer hours; in fact, it might even mean working more. But being able to save two hours of commuting and prep time is definitely worth it. Honestly, if remote work is no longer an option in the future, I’d probably lose interest.</p><p>Another key reason is that working in an office makes it easy to get interrupted, which constantly disrupts my train of thought. Frequent interruptions aren’t great for the company either. While I do get a lot of messages throughout the day, I still find it much more efficient to work in a quiet home environment rather than dealing with people interrupting me or background noise. Especially with my own computer setup, it’s just way more comfortable.</p><p>I won’t deny that achieving my personal goals is challenging, but I believe in market value rather than relying on the judgment of a single company or a small group of people.</p>","contentLength":1147,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative and Predictive AI in Application Security: A Comprehensive Guide","url":"https://dev.to/lynxfelony1/generative-and-predictive-ai-in-application-security-a-comprehensive-guide-171p","date":1739730772,"author":"Smart Mohr","guid":735,"unread":true,"content":"<p>Machine intelligence is redefining the field of application security by allowing smarter weakness identification, automated testing, and even autonomous malicious activity detection. This write-up provides an comprehensive discussion on how machine learning and AI-driven solutions function in AppSec, written for cybersecurity experts and executives alike. We’ll examine the evolution of AI in AppSec, its current strengths, challenges, the rise of “agentic” AI, and forthcoming directions. Let’s start our exploration through the history, present, and prospects of artificially intelligent application security. </p><p>History and Development of AI in AppSec </p><p>Early Automated Security Testing \nLong before artificial intelligence became a hot subject, infosec experts sought to streamline security flaw identification. In the late 1980s, Professor Barton Miller’s trailblazing work on fuzz testing demonstrated the impact of automation. His 1988 university effort randomly generated inputs to crash UNIX programs — “fuzzing” uncovered that roughly a quarter to a third of utility programs could be crashed with random data. This straightforward black-box approach paved the foundation for future security testing methods. By the 1990s and early 2000s, practitioners employed automation scripts and scanning applications to find typical flaws. Early source code review tools functioned like advanced grep, scanning code for risky functions or hard-coded credentials. Even though these pattern-matching methods were beneficial, they often yielded many false positives, because any code matching a pattern was flagged regardless of context. </p><p>Progression of AI-Based AppSec \nDuring the following years, academic research and commercial platforms grew, transitioning from rigid rules to intelligent reasoning. Machine learning gradually infiltrated into AppSec. Early adoptions included neural networks for anomaly detection in network traffic, and probabilistic models for spam or phishing — not strictly application security, but predictive of the trend. Meanwhile, code scanning tools improved with data flow tracing and CFG-based checks to observe how inputs moved through an app. </p><p>A major concept that arose was the Code Property Graph (CPG), combining structural, execution order, and data flow into a single graph. This approach facilitated more meaningful vulnerability detection and later won an IEEE “Test of Time” award. By capturing program logic as nodes and edges, security tools could pinpoint intricate flaws beyond simple signature references. </p><p>In 2016, DARPA’s Cyber Grand Challenge proved fully automated hacking systems — capable to find, exploit, and patch software flaws in real time, minus human involvement. The winning system, “Mayhem,” combined advanced analysis, symbolic execution, and certain AI planning to go head to head against human hackers. This event was a notable moment in self-governing cyber security. </p><p>Major Breakthroughs in AI for Vulnerability Detection \nWith the growth of better learning models and more datasets, machine learning for security has accelerated. Major corporations and smaller companies alike have reached milestones. One notable leap involves machine learning models predicting software vulnerabilities and exploits. An example is the Exploit Prediction Scoring System (EPSS), which uses thousands of features to estimate which flaws will face exploitation in the wild. This approach enables infosec practitioners prioritize the highest-risk weaknesses. </p><p>In code analysis, deep learning networks have been supplied with massive codebases to identify insecure structures. Microsoft, Big Tech, and other entities have indicated that generative LLMs (Large Language Models) enhance security tasks by automating code audits. securing code with AI For example, Google’s security team used LLMs to develop randomized input sets for open-source projects, increasing coverage and spotting more flaws with less manual intervention. </p><p>Current AI Capabilities in AppSec </p><p>Today’s application security leverages AI in two primary categories: generative AI, producing new outputs (like tests, code, or exploits), and predictive AI, analyzing data to detect or forecast vulnerabilities. These capabilities span every aspect of the security lifecycle, from code inspection to dynamic scanning. </p><p>AI-Generated Tests and Attacks \nGenerative AI produces new data, such as attacks or snippets that expose vulnerabilities. This is visible in AI-driven fuzzing. Conventional fuzzing uses random or mutational data, in contrast generative models can generate more targeted tests. Google’s OSS-Fuzz team experimented with large language models to auto-generate fuzz coverage for open-source repositories, boosting vulnerability discovery. </p><p>Likewise, generative AI can assist in crafting exploit PoC payloads. Researchers carefully demonstrate that AI empower the creation of proof-of-concept code once a vulnerability is disclosed. On the adversarial side, penetration testers may use generative AI to simulate threat actors. Defensively, teams use automatic PoC generation to better test defenses and create patches. </p><p>Predictive AI for Vulnerability Detection and Risk Assessment \nPredictive AI scrutinizes code bases to locate likely bugs. Rather than fixed rules or signatures, a model can infer from thousands of vulnerable vs. safe code examples, spotting patterns that a rule-based system might miss. This approach helps indicate suspicious logic and predict the severity of newly found issues. </p><p>Prioritizing flaws is a second predictive AI benefit. The Exploit Prediction Scoring System is one illustration where a machine learning model scores security flaws by the chance they’ll be exploited in the wild. This helps security programs zero in on the top fraction of vulnerabilities that carry the greatest risk. Some modern AppSec toolchains feed source code changes and historical bug data into ML models, predicting which areas of an system are especially vulnerable to new flaws. </p><p>AI-Driven Automation in SAST, DAST, and IAST \nClassic static scanners, DAST tools, and interactive application security testing (IAST) are increasingly empowering with AI to upgrade performance and precision. </p><p>SAST analyzes source files for security vulnerabilities without running, but often triggers a torrent of incorrect alerts if it cannot interpret usage. AI assists by triaging notices and removing those that aren’t truly exploitable, through smart control flow analysis. Tools for example Qwiet AI and others use a Code Property Graph combined with machine intelligence to evaluate exploit paths, drastically reducing the noise. </p><p>DAST scans the live application, sending attack payloads and analyzing the reactions. AI advances DAST by allowing dynamic scanning and adaptive testing strategies. The agent can interpret multi-step workflows, modern app flows, and RESTful calls more effectively, increasing coverage and reducing missed vulnerabilities. </p><p>IAST, which instruments the application at runtime to observe function calls and data flows, can produce volumes of telemetry. An AI model can interpret that instrumentation results, identifying risky flows where user input reaches a critical sensitive API unfiltered. By combining IAST with ML, irrelevant alerts get filtered out, and only valid risks are highlighted. </p><p>Methods of Program Inspection: Grep, Signatures, and CPG \nToday’s code scanning engines usually combine several methodologies, each with its pros/cons: </p><p>Grepping (Pattern Matching): The most rudimentary method, searching for tokens or known patterns (e.g., suspicious functions). Fast but highly prone to false positives and missed issues due to no semantic understanding. </p><p>Signatures (Rules/Heuristics): Heuristic scanning where experts create patterns for known flaws. It’s useful for established bug classes but not as flexible for new or novel bug types. </p><p>Code Property Graphs (CPG): A more modern context-aware approach, unifying AST, CFG, and DFG into one graphical model. Tools process the graph for risky data paths. Combined with ML, it can discover unknown patterns and reduce noise via flow-based context. </p><p>In practice, providers combine these strategies. They still use rules for known issues, but they supplement them with CPG-based analysis for deeper insight and ML for ranking results. </p><p>Securing Containers &amp; Addressing Supply Chain Threats \nAs companies embraced containerized architectures, container and open-source library security became critical. AI helps here, too: </p><p>Container Security: AI-driven image scanners scrutinize container images for known CVEs, misconfigurations, or sensitive credentials. Some solutions determine whether vulnerabilities are active at execution, diminishing the alert noise. Meanwhile, machine learning-based monitoring at runtime can flag unusual container behavior (e.g., unexpected network calls), catching intrusions that static tools might miss. </p><p>Supply Chain Risks: With millions of open-source libraries in public registries, human vetting is impossible. AI can study package documentation for malicious indicators, detecting typosquatting. Machine learning models can also rate the likelihood a certain dependency might be compromised, factoring in usage patterns. This allows teams to focus on the most suspicious supply chain elements. In parallel, AI can watch for anomalies in build pipelines, confirming that only approved code and dependencies go live. </p><p>Challenges and Limitations </p><p>Although AI introduces powerful capabilities to application security, it’s not a cure-all. Teams must understand the limitations, such as misclassifications, feasibility checks, algorithmic skew, and handling zero-day threats. </p><p>Accuracy Issues in AI Detection \nAll automated security testing deals with false positives (flagging harmless code) and false negatives (missing real vulnerabilities). AI can reduce the spurious flags by adding reachability checks, yet it introduces new sources of error. A model might spuriously claim issues or, if not trained properly, ignore a serious bug. Hence, human supervision often remains essential to confirm accurate diagnoses. </p><p>Reachability and Exploitability Analysis \nEven if AI detects a vulnerable code path, that doesn’t guarantee malicious actors can actually exploit it. Assessing real-world exploitability is challenging. Some frameworks attempt symbolic execution to prove or disprove exploit feasibility. However, full-blown runtime proofs remain rare in commercial solutions. Consequently, many AI-driven findings still demand human judgment to classify them urgent. </p><p>Data Skew and Misclassifications \nAI algorithms train from historical data. If that data is dominated by certain vulnerability types, or lacks cases of novel threats, the AI could fail to recognize them. Additionally, a system might under-prioritize certain platforms if the training set indicated those are less apt to be exploited. Ongoing updates, inclusive data sets, and model audits are critical to lessen this issue. </p><p>Coping with Emerging Exploits \nMachine learning excels with patterns it has processed before. A entirely new vulnerability type can slip past AI if it doesn’t match existing knowledge. Attackers also use adversarial AI to trick defensive systems. Hence, AI-based solutions must evolve constantly. Some developers adopt anomaly detection or unsupervised clustering to catch abnormal behavior that signature-based approaches might miss. Yet, even these unsupervised methods can miss cleverly disguised zero-days or produce false alarms. </p><p>Emergence of Autonomous AI Agents </p><p>A modern-day term in the AI community is agentic AI — intelligent programs that don’t just generate answers, but can pursue tasks autonomously. In AppSec, this implies AI that can manage multi-step operations, adapt to real-time conditions, and make decisions with minimal manual oversight. </p><p>Understanding Agentic Intelligence \nAgentic AI programs are given high-level objectives like “find vulnerabilities in this software,” and then they plan how to do so: collecting data, running tools, and adjusting strategies according to findings. Ramifications are significant: we move from AI as a utility to AI as an independent actor. </p><p>Offensive vs. Defensive AI Agents \nOffensive (Red Team) Usage: Agentic AI can launch penetration tests autonomously. Security firms like FireCompass provide an AI that enumerates vulnerabilities, crafts exploit strategies, and demonstrates compromise — all on its own. In parallel, open-source “PentestGPT” or comparable solutions use LLM-driven logic to chain scans for multi-stage penetrations. </p><p>Defensive (Blue Team) Usage: On the protective side, AI agents can oversee networks and proactively respond to suspicious events (e.g., isolating a compromised host, updating firewall rules, or analyzing logs). Some security orchestration platforms are integrating “agentic playbooks” where the AI makes decisions dynamically, rather than just following static workflows. </p><p>Self-Directed Security Assessments \nFully agentic pentesting is the holy grail for many security professionals. Tools that comprehensively enumerate vulnerabilities, craft attack sequences, and evidence them without human oversight are emerging as a reality. Notable achievements from DARPA’s Cyber Grand Challenge and new self-operating systems signal that multi-step attacks can be chained by machines. </p><p>intelligent security testing Risks in Autonomous Security \nWith great autonomy arrives danger. An agentic AI might inadvertently cause damage in a critical infrastructure, or an hacker might manipulate the AI model to mount destructive actions. Careful guardrails, sandboxing, and manual gating for risky tasks are critical. Nonetheless, agentic AI represents the next evolution in cyber defense. </p><p>autonomous agents for appsec Where AI in Application Security is Headed </p><p>AI’s influence in AppSec will only expand. We expect major changes in the next 1–3 years and decade scale, with innovative compliance concerns and adversarial considerations. </p><p>Short-Range Projections \nOver the next handful of years, companies will integrate AI-assisted coding and security more frequently. Developer tools will include AppSec evaluations driven by ML processes to highlight potential issues in real time. Intelligent test generation will become standard. Continuous security testing with self-directed scanning will supplement annual or quarterly pen tests. Expect improvements in alert precision as feedback loops refine machine intelligence models. </p><p>Cybercriminals will also exploit generative AI for social engineering, so defensive systems must adapt. We’ll see malicious messages that are nearly perfect, demanding new AI-based detection to fight machine-written lures. </p><p>check security features Regulators and governance bodies may lay down frameworks for ethical AI usage in cybersecurity. For example, rules might call for that companies log AI outputs to ensure accountability. </p><p>Extended Horizon for AI Security \nIn the 5–10 year range, AI may reshape the SDLC entirely, possibly leading to: </p><p>AI-augmented development: Humans collaborate with AI that writes the majority of code, inherently embedding safe coding as it goes. </p><p>Automated vulnerability remediation: Tools that go beyond detect flaws but also fix them autonomously, verifying the safety of each fix. </p><p>Proactive, continuous defense: Automated watchers scanning apps around the clock, predicting attacks, deploying security controls on-the-fly, and dueling adversarial AI in real-time. </p><p>Secure-by-design architectures: AI-driven threat modeling ensuring software are built with minimal attack surfaces from the outset. </p><p>We also predict that AI itself will be strictly overseen, with compliance rules for AI usage in critical industries. This might mandate traceable AI and regular checks of ML models. </p><p>AI in Compliance and Governance \nAs AI assumes a core role in AppSec, compliance frameworks will expand. We may see: </p><p>AI-powered compliance checks: Automated verification to ensure controls (e.g., PCI DSS, SOC 2) are met continuously. </p><p>Governance of AI models: Requirements that companies track training data, prove model fairness, and document AI-driven findings for regulators. </p><p>Incident response oversight: If an AI agent initiates a defensive action, which party is liable? Defining liability for AI actions is a thorny issue that policymakers will tackle. </p><p>Moral Dimensions and Threats of AI Usage \nApart from compliance, there are social questions. Using AI for insider threat detection can lead to privacy concerns. Relying solely on AI for safety-focused decisions can be risky if the AI is biased. check this out Meanwhile, malicious operators use AI to mask malicious code. Data poisoning and prompt injection can corrupt defensive AI systems. </p><p>Adversarial AI represents a heightened threat, where bad agents specifically target ML models or use LLMs to evade detection. Ensuring the security of ML code will be an key facet of AppSec in the future. </p><p>Machine intelligence strategies are fundamentally altering software defense. We’ve reviewed the evolutionary path, current best practices, hurdles, agentic AI implications, and future prospects. The overarching theme is that AI functions as a mighty ally for AppSec professionals, helping accelerate flaw discovery, rank the biggest threats, and streamline laborious processes. </p><p>Yet, it’s no panacea. False positives, training data skews, and zero-day weaknesses still demand human expertise. The arms race between attackers and security teams continues; AI is merely the most recent arena for that conflict. Organizations that incorporate AI responsibly — integrating it with team knowledge, robust governance, and regular model refreshes — are best prepared to prevail in the ever-shifting landscape of application security. </p><p>Ultimately, the promise of AI is a more secure software ecosystem, where vulnerabilities are caught early and addressed swiftly, and where protectors can counter the agility of cyber criminals head-on. With continued research, partnerships, and evolution in AI capabilities, that scenario could be closer than we think. <a href=\"https://www.youtube.com/watch?v=vZ5sLwtJmcU\" rel=\"noopener noreferrer\">securing code with AI</a></p>","contentLength":18243,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rotating Chess Game in Octagon","url":"https://dev.to/dan52242644dan/rotating-chess-game-in-octagon-5epd","date":1739730720,"author":"Dan","guid":734,"unread":true,"content":"<p>Check out this Pen I made!</p>","contentLength":26,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Say Goodbye to If-Else Chaos – Feature Flags for Everyone","url":"https://dev.to/arseniydev/say-goodbye-to-if-else-chaos-feature-flags-for-everyone-4p2p","date":1739730453,"author":"Arseniy","guid":733,"unread":true,"content":"<p>If you’ve ever tried rolling out a new feature in a React app, you know the drill. You add a bunch of  statements, check environment variables, or maybe even maintain separate branches for different versions. It’s messy, error-prone, and worst of all—it doesn’t scale.  </p><p>Meanwhile, big tech companies use feature flags to make these problems disappear. They roll out new features gradually, run A/B tests effortlessly, and toggle features on or off without redeploying. But if you're a startup or a solo developer, implementing feature flags yourself feels like overkill.  </p><p>That’s exactly why we built —to bring powerful, enterprise-level feature management to . And the best part? <strong>You don’t need if-else statements, and it works with just a single import.</strong></p><h3><strong>One Import. No If-Else. Infinite Flexibility.</strong></h3><p>Imagine this: instead of writing convoluted logic like this—</p><div><pre><code></code></pre></div><p>—you just <strong>import and call one function</strong>:</p><div><pre><code></code></pre></div><p>Boom. No environment variables, no conditional rendering, no spaghetti code. Just clean, maintainable feature toggling that works right out of the box.  </p><h3><strong>Feature Flags Without the Overhead</strong></h3><p>With , you can:  </p><p>✅ <strong>Roll out features gradually</strong> – Control what percentage of users see a new feature.<strong>A/B test different versions</strong> – Let users experience multiple UI variations without extra work.<strong>Avoid breaking production</strong> – Instantly disable a buggy feature without redeploying. – Override versions manually or let the system decide dynamically.  </p><p>All of this happens automatically behind the scenes. No need to refactor your app, write complex logic, or set up a backend just for feature management.  </p><h3><strong>Flexible Yet Stupidly Simple</strong></h3><p>Need global settings? No problem. You can tweak  to work exactly the way you want:</p><div><pre><code></code></pre></div><p>Or if you just want to force a feature for testing, that’s just one line:</p><div><pre><code></code></pre></div><h3><strong>Feature Flags Are No Longer Just for Big Tech</strong></h3><p>For too long, feature flags and A/B testing have been seen as tools reserved for large enterprises with dedicated DevOps teams. But that’s changing. <strong>With FlagSwitch React, you get all the power of feature flags with none of the complexity.</strong></p><p>No more conditional logic. No more painful rollouts. No more guessing what works.  </p><p>Just <strong>import, define, and deploy</strong>—it’s that easy.  </p><p>Ready to ship features like the pros? Install now:</p><div><pre><code>npm flagswitch-react\n</code></pre></div><p>🚀 <strong>Try it out today and take control of your React app!</strong></p>","contentLength":2346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Put Paid Access to Your GitHub Code Without Writing a Line of Code 🔥","url":"https://dev.to/abubakersiddique761/put-paid-access-to-your-github-code-without-writing-a-line-of-code-4p0c","date":1739730342,"author":"Abubaker Siddique","guid":732,"unread":true,"content":"<p>This article shows you how to let people who pay get into your <a href=\"https://github.com/\" rel=\"noopener noreferrer\">GitHub</a> code area without using code. You can use three services to do this. The services are <a href=\"https://github.com/\" rel=\"noopener noreferrer\">GitHub</a>, <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a>, and <a href=\"https://zapier.com/\" rel=\"noopener noreferrer\">Zapier</a>.<p>\nLearn more about each service:  </p></p><p>This article explains the method in five main parts. The parts are as follows:  </p><p>In addition, the article shows one extra option that uses other tools. The method uses a no-code approach. This method does not require you to write any code. It is a simple way to manage access to your repository by connecting the three tools in an automated way. For further reading on no-code automation, check out <a href=\"https://www.nocode.tech/\" rel=\"noopener noreferrer\">No-Code Tools Explained</a>.</p><h2>\n  \n  \n  Step 1. Set Up a Private GitHub Repository\n</h2><p>The first part is to create a <a href=\"https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository\" rel=\"noopener noreferrer\">GitHub repository</a> that only paid users can see. Follow these steps:  </p><p>This method keeps your code away from public view. It makes sure that only the people you add can see the content.</p><h2>\n  \n  \n  Step 2. Set Up a GitHub Organization and Team\n</h2><p>The next part is to set up an <a href=\"https://docs.github.com/en/organizations\" rel=\"noopener noreferrer\">organization</a> in GitHub. An organization is a group that holds one or more repositories. To do this:  </p><ul><li>Once the organization is created, set up a <a href=\"https://docs.github.com/en/organizations/organizing-members-into-teams\" rel=\"noopener noreferrer\">Team</a> within the organization.\n</li><li>In the team, add the repository that you created earlier.\n</li><li>Choose the proper access level for the team. This access level can let users <a href=\"https://docs.github.com/en/organizations/managing-peoples-access-to-your-organization-with-roles/about-roles-in-an-organization\" rel=\"noopener noreferrer\">read or write</a>.\n</li></ul><p>By setting up an organization and a team, you create a way to manage who sees the code. Only the people in the team can access the repository.</p><h2>\n  \n  \n  Step 3. Set Up Gumroad for Selling Access\n</h2><p>The third part is to use <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a> to sell access to your repository. Gumroad is a tool that lets you sell digital items. The steps are as follows:</p><ul><li>Create a new product. You can name the product in a way that shows it gives access to your GitHub code. See <a href=\"https://help.gumroad.com/hc/en-us/articles/360013997974\" rel=\"noopener noreferrer\">Gumroad Product Setup</a> for details.\n</li><li>Set a price for the product. For more tips on pricing digital products, check out <a href=\"https://www.shopify.com/blog/price-your-products\" rel=\"noopener noreferrer\">Pricing Strategies</a>.\n</li><li>Publish the product so that buyers can see it on <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a>.\n</li></ul><p>This step gives you a place to sell access. People who want the code must buy the product from Gumroad. The sale on Gumroad triggers the rest of the process. For more on digital product sales, see <a href=\"https://help.gumroad.com/\" rel=\"noopener noreferrer\">Digital Sales on Gumroad</a>.</p><h2>\n  \n  \n  Step 4. Use Zapier to Join Paid Users to Your GitHub Team\n</h2><p>The fourth part is to use <a href=\"https://zapier.com/\" rel=\"noopener noreferrer\">Zapier</a> to connect Gumroad with GitHub. Zapier is a tool that helps one service talk to another. The goal here is to have Zapier add a buyer to your GitHub organization automatically. Do this as follows:</p><ul><li>Create a new workflow (called a <a href=\"https://zapier.com/blog/what-is-a-zap/\" rel=\"noopener noreferrer\">Zap</a>).\n</li><li>Set the trigger for the Zap as a new sale in <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a>. This means that whenever a buyer pays for the product, the Zap will start. For a guide, visit <a href=\"https://zapier.com/help/create/customize/trigger-options\" rel=\"noopener noreferrer\">Zapier Triggers</a>.\n</li></ul><p>When a sale happens on Gumroad, Zapier makes sure that the buyer receives an invite to join your GitHub team. This process happens automatically. It does not require any manual work on your part.</p><h2>\n  \n  \n  Step 5. Approve Users Automatically\n</h2><p>After a buyer receives an invite to join your GitHub organization, they must accept the invite. When they do this, they become part of the team that has access to your code. Follow these steps:</p><ul><li>Once the buyer accepts the invite, they become a member of your team.\n</li><li>As a member, the buyer has the access that you set in the team settings.\n</li></ul><p>This step completes the process. The buyer pays, the sale is noted, and the buyer gets access to your code automatically. You do not have to write any code to manage this access.</p><h2>\n  \n  \n  Extra Option: Alternative No-Code Options\n</h2><p>There is an extra option if you want to use different tools. Two other services can be used instead of Zapier and GitHub Teams:</p><p>These extra options show that you have choices. You can choose the setup that best fits your needs.</p><p>Would you like help setting this up step by step? 🚀</p><h2>\n  \n  \n  How the Process Works in Simple Steps\n</h2><p>To review, here is how the process works in simple steps:</p><p>This process uses simple steps. Each step is clear. You do not need to learn new languages or write any lines of code. You use three tools that work well together.</p><h2>\n  \n  \n  Reasons to Use This Method\n</h2><p>There are reasons to use this method. You may want to charge people for the chance to see your code. You may want to keep the code away from public view. You may want to use a method that does not require writing any code. The steps in this article let you do that. For more on the benefits of no-code solutions, visit <a href=\"https://www.nocode.tech/\" rel=\"noopener noreferrer\">No-Code Revolution</a>.</p><p>The method gives you a way to manage access. It uses a mix of services that talk to each other. It keeps the process simple. You focus on your work and let the system handle the access.</p><h2>\n  \n  \n  Detailed Look at Each Tool\n</h2><p>GitHub is the place where you store your code. By making the repository <a href=\"https://docs.github.com/en/github/setting-up-and-managing-your-github-user-account/managing-your-repository-settings\" rel=\"noopener noreferrer\">private</a>, you limit who can see it. GitHub lets you set up an organization and teams. This feature makes it easy to control access. You choose who sees the code by adding them to the team. More details are available at the <a href=\"https://docs.github.com/\" rel=\"noopener noreferrer\">GitHub Docs</a>.</p><p>Gumroad is a service for selling digital items. With Gumroad, you create a product that gives access to your code. Gumroad manages payments and notes each sale. This record is used to trigger the next steps. Learn more at the <a href=\"https://help.gumroad.com/\" rel=\"noopener noreferrer\">Gumroad Help Center</a>.</p><p>Zapier is the tool that connects the services. It listens for a sale on <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a>. When a sale happens, Zapier tells <a href=\"https://github.com/\" rel=\"noopener noreferrer\">GitHub</a> to send an invite. Zapier works in the background. It makes the process smooth and automatic. Check out <a href=\"https://zapier.com/apps\" rel=\"noopener noreferrer\">Zapier’s Integrations</a> for further exploration.</p><p>Take your time with each step. Make sure that the settings are correct. Check that the <a href=\"https://docs.github.com/en/organizations/organizing-members-into-teams\" rel=\"noopener noreferrer\">GitHub team</a> has the right access. Verify that <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a> is recording sales. Confirm that <a href=\"https://zapier.com/\" rel=\"noopener noreferrer\">Zapier</a> is triggering the GitHub invites. For troubleshooting, see <a href=\"https://zapier.com/help/\" rel=\"noopener noreferrer\">Zapier Troubleshooting</a>.</p><p>Once all the steps are done, test the system. Make a small sale and see if the buyer gets the invite. If there are issues, check the settings in each service. Adjust as needed.</p><h2>\n  \n  \n  Common Issues and Their Fixes\n</h2><p>Sometimes, the system does not work as expected. Here are some common issues and what to do:</p><ul><li>: If <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a> does not record a sale, review the product settings on Gumroad. Ensure that the price and description are correct. For more on Gumroad settings, see <a href=\"https://help.gumroad.com/\" rel=\"noopener noreferrer\">Gumroad Product Management</a>.\n</li><li>: If <a href=\"https://zapier.com/\" rel=\"noopener noreferrer\">Zapier</a> does not send the invite, recheck the Zap settings. Make sure the trigger and action are set correctly. Consult <a href=\"https://zapier.com/help/\" rel=\"noopener noreferrer\">Zapier’s Help Center</a> for further assistance.</li></ul><p>These checks help keep the system running smoothly. When you work through these issues, you learn more about each tool.</p><h2>\n  \n  \n  Best Practices for the Setup\n</h2><p>To ensure the system works well, follow these best practices:</p><ul><li>: Whenever you make a change, test the system with a small sale. This test shows that the change does not break the process.\n</li><li>: Maintain a record of sales and invites. This record helps you see if the system works as expected. For more on record keeping, see <a href=\"https://www.shopify.com/blog/sales-records\" rel=\"noopener noreferrer\">Digital Sales Record Keeping</a>.\n</li><li>: Spend time learning the basics of <a href=\"https://docs.github.com/\" rel=\"noopener noreferrer\">GitHub</a>, <a href=\"https://help.gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a>, and <a href=\"https://zapier.com/learn/\" rel=\"noopener noreferrer\">Zapier</a>. Familiarity with the tools helps in case problems arise.</li></ul><p>Following these practices makes the system stable. The steps are clear and each part works on its own.</p><p>The method in this article gives you a way to manage who sees your code on <a href=\"https://github.com/\" rel=\"noopener noreferrer\">GitHub</a>. By using <a href=\"https://github.com/\" rel=\"noopener noreferrer\">GitHub</a>, <a href=\"https://gumroad.com/\" rel=\"noopener noreferrer\">Gumroad</a>, and <a href=\"https://zapier.com/\" rel=\"noopener noreferrer\">Zapier</a>, you create a system that does not need any code. Each tool plays a part in the system. GitHub holds your code, Gumroad manages the sale, and Zapier connects the two.</p><p>This setup helps you offer paid access in a simple way. It is a method that uses a no-code approach. The steps are clear, and the process is easy to follow. You can set this up with a few accounts and careful checks. For more on building no-code systems, check out <a href=\"https://www.nocode.tech/\" rel=\"noopener noreferrer\">No-Code Movement</a>.</p><p>By following the instructions here, you control access to your GitHub repository. You do not need to learn new code. The system handles the work for you. With the right steps and careful testing, you have a way to share your code only with those who pay.</p><p>The method is set up to work well for anyone who wants a simple system. It shows how to use available tools without any need for extra code. Follow the steps and check each setting. You will have a system that keeps your code safe and gives paid users the access they need.</p><p>Take the time to set this up properly. The effort now leads to a working system later. Enjoy the process of linking the tools. Let each step work as planned. With the right steps, you open the door to your code only for those who pay.</p><p>This guide is complete. Follow each step. Adjust as needed. The system is yours to keep and control.<p>\nFor more insights on secure code sharing, visit </p><a href=\"https://docs.github.com/en/code-security\" rel=\"noopener noreferrer\">GitHub Security</a>.</p><h2>\n  \n  \n  Additional Resources and Tools\n</h2><p>For further learning and insights on related topics, explore these resources:</p><p><em>Happy automating and best of luck in setting up your system!</em></p><h3>\n  \n  \n  Earn $100 Fast: AI + Notion Templates\n</h3><p>Do you want to make extra money quickly? This guide shows you how to create and sell Notion templates step by step. Perfect for beginners or anyone looking for an easy way to start earning online.</p><ul><li> Follow a simple process to create templates people want and will buy.</li><li> Learn to use tools like ChatGPT to design and improve templates.</li><li> More people are using Notion every day, and they need templates to save time and stay organized.</li></ul><ul><li> Ready-made prompts to spark ideas and create templates faster.</li><li> Stay on track as you work.</li></ul><ul><li> Learn everything from idea to sale.</li><li><strong>How to Find Popular Ideas:</strong> Research trends and needs.</li><li> Tips for improving templates with AI tools.</li><li><strong>Making Templates User-Friendly:</strong> Simple tips for better design.</li><li> Advice on sharing and selling on platforms like Gumroad or Etsy.</li><li> Solutions for issues like low sales or tricky designs.</li></ul><ul><li>Anyone who wants to make extra money online.</li><li>People who love using Notion and want to share their ideas.</li><li>Creators looking for a simple way to start selling digital products.</li></ul><h3>\n  \n  \n  💰 <strong>Want to Earn 40% Commission?</strong></h3><p>Join our affiliate program and start making money by promoting ! Earn 40% on every sale you refer.  </p><p>You'll on average around 5$ per sell and for bundled products it will be around 40$ per sale. (So just share it and make money with worrying about product creation and maintanence)</p>","contentLength":10150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trending 20+ NextJS Resource Curated for Your Next Project.","url":"https://dev.to/dev-resources/trending-20-nextjs-resource-curated-for-your-next-project-pim","date":1739730321,"author":"Dev Resources","guid":731,"unread":true,"content":"<h2><strong>1. Newly Curated 40+ Developer Fixes || Try it now :)</strong></h2><p>It's an free Giveaway                                                                               ... <a href=\"http://dev-resources.site/topic/nextjs/2281199/newly-curated-40-developer-fixes-try-it-now--14fe\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DNewly%2520Curated%252040%2B%2520Developer%2520Fixes%2520%7C%7C%2520Try%2520it%2520now%2520%3A\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>2. typing tests were boring, so i built TypeTheLyrics.</strong></h2><p>I've always wanted to type along with my favourite songs, so I built TypeTheLyrics, an app that lets... <a href=\"http://dev-resources.site/topic/nextjs/2282021/typing-tests-were-boring-so-i-built-typethelyrics-4o9j\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fn66jjwqc3rgdrl9tm7zt.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>3. 🚀 Ensuring Unique Slugs in Next.js 15 with Prisma &amp; Slugify</strong></h2><p>Creating SEO-friendly slugs is essential for URLs in Next.js applications. However, ensuring they... <a href=\"http://dev-resources.site/topic/nextjs/2282279/ensuring-unique-slugs-in-nextjs-15-with-prisma-slugify-4agc\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fdpq7wf2jz6prcrwjsobe.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>4. Is the MERN Stack Dying? What’s Next for Full-Stack Development?</strong></h2><p>The MERN stack — MongoDB, Express.js, React, and Node.js — has been a favorite among full-stack... <a href=\"http://dev-resources.site/topic/nextjs/2281997/is-the-mern-stack-dying-whats-next-for-full-stack-development-35pg\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fuw0s608gq0znygjxrg4c.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>5. Expo + Next.js + React Native + TailwindCSS Boilerplate</strong></h2><p>A starter template integrating Expo, Next.js, and React Native for cross-platform development (Web,... <a href=\"http://dev-resources.site/topic/nextjs/2282363/-expo-nextjs-react-native-tailwindcss-boilerplate-1hcn\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DExpo%2520%2B%2520Next.js%2520%2B%2520React%2520Native%2520%2B%2520TailwindCSS%2520Boilerplate%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2282363%2F-expo-nextjs-react-native-tailwindcss-boilerplate-1hcn\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><p>Hey DEV Community! 👋    I'm excited to share my new portfolio with you all! 🚀 As a Frontend Lead... <a href=\"http://dev-resources.site/topic/nextjs/2282163/balaji-udayagiri-14o3\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmgc5rr6kmoqy74a54i91.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>7. Building a Modern Portfolio with Next.js, MDX, and AWS</strong></h2><p>👉 Check out my portfolio website here or here and let me know what you think!  (yes I have two... <a href=\"http://dev-resources.site/topic/nextjs/2281275/building-a-modern-portfolio-with-nextjs-mdx-and-aws-3h\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DBuilding%2520a%2520Modern%2520Portfolio%2520with%2520Next.js%2C%2520MDX%2C%2520and%2520AWS%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2281275%2Fbuilding-a-modern-portfolio-with-nextjs-mdx-and-aws-3h\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>8. 🚀 Optimizing Database Queries in Next.js 15 with cache and Prisma</strong></h2><p>Fetching data efficiently in a Next.js application is crucial for performance. With Next.js 15 and... <a href=\"http://dev-resources.site/topic/nextjs/2281638/optimizing-database-queries-in-nextjs-15-with-cache-and-prisma-5ehe\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fod8vnzghmzl6uqa7yh89.jpeg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>9. Deploying a Next.js Application with Dokku</strong></h2><p>Introduction   In this article, we will deploy a Next.js application using Dokku... <a href=\"http://dev-resources.site/topic/nextjs/2280831/deploy-a-nextjs-application-with-dokku-2on7\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DDeploying%2520a%2520Next.js%2520Application%2520with%2520Dokku%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2280831%2Fdeploy-a-nextjs-application-with-dokku-2on7\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>10. What is Emotion CSS? A Beginner’s Guide</strong></h2><p>CSS in JavaScript (CSS-in-JS) has become a popular approach for styling modern web applications, and... <a href=\"http://dev-resources.site/topic/nextjs/2280775/what-is-emotion-css-a-beginners-guide-4c60\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fy7o4saegxo7zlz0amza5.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>11. How set Google Tag Manager (GTM) on Nextjs applications with differents themes and IDs</strong></h2><p>This week, I needed to solve a problem with Google Tag Manager (GTM).  The problem was that my... <a href=\"http://dev-resources.site/topic/nextjs/2229510/how-set-google-tag-manager-gtm-on-nextjs-applications-with-differents-themes-and-ids-3jk2\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8nqvej8ipqx2wu83gdxp.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>12. Event Handling &amp; Conditional Rendering in React</strong></h2><p>This article is part of the \"Mastering React with Next.js: A Developer's Guide\" series. In our... <a href=\"http://dev-resources.site/topic/nextjs/2272487/event-handling-conditional-rendering-in-react-2aee\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F85i96hfj6c91rhbvdoc1.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>13. The Ultimate Guide to Server Components, Server Actions, Route Handlers, and Suspense in Next.js App Router</strong></h2><p>Overview   With the introduction of Next.js App Router, developers have gained access to... <a href=\"http://dev-resources.site/topic/nextjs/2275494/the-ultimate-guide-to-server-components-server-actions-route-handlers-and-suspense-in-nextjs-46ah\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fd3y4g7t0wzohahdbyvno.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>14. Build a data-intensive Next.js app with Tinybird and Cursor</strong></h2><p>Building a Next.js app is easy these days. With generative AI webdev platforms like v0, Lovable, and... <a href=\"http://dev-resources.site/topic/nextjs/2276478/build-a-data-intensive-nextjs-app-with-tinybird-and-cursor-2h8\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F7ihltjb9gsw4rg9kg7iq.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>15. Authentication in Next.js: Clerk vs. Auth.js vs. Custom Auth – A Comprehensive Guide 🔐</strong></h2><p>In today’s fast‑paced web development landscape, robust user authentication isn’t just a security... <a href=\"http://dev-resources.site/topic/nextjs/2276311/authentication-in-nextjs-clerk-vs-authjs-vs-custom-auth-a-comprehensive-guide-5fnk\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fnclwgzdv8ht5gs2vxdsq.jpeg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>16. Why Tailwind CSS Sometimes Leaves You Scratching Your Head</strong></h2><p>Hey there! 👋  If you use Tailwind CSS regularly, you’re probably a fan of its utility-first approach.... <a href=\"http://dev-resources.site/topic/nextjs/2278465/why-tailwind-css-sometimes-leaves-you-scratching-your-head-329d\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DWhy%2520Tailwind%2520CSS%2520Sometimes%2520Leaves%2520You%2520Scratching%2520Your%2520Head%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2278465%2Fwhy-tailwind-css-sometimes-leaves-you-scratching-your-head-329d\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>17. Stop using framework specific  component. Use this instead...</strong></h2><p>Pretty much every JS framwork today has a special &lt;Link&gt; component which wraps HTML &lt;a&gt;... <a href=\"http://dev-resources.site/topic/nextjs/2274132/stop-using-framework-specific-component-use-this-instead-3d7l\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DStop%2520using%2520framework%2520specific%2520%3CLink%3E%2520component.%2520Use%2520this%2520instead...%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2274132%2Fstop-using-framework-specific-component-use-this-instead-3d7l\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>18. Building a free Text to Speech Generator</strong></h2><p>Watch Now!  🚀 What You'll Learn: 🛠️ Set up Next.js to build a responsive and interactive web... <a href=\"http://dev-resources.site/topic/nextjs/2276165/building-a-free-text-to-speech-generator-22e\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fzu4b23nnw0rbwodnxhx5.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>19. How to Change Swipe Direction in Swiper.js (Horizontal &amp; Vertical Guide)</strong></h2><p>When I first started using Swiper.js in my React projects, I was playing a “guess the swipe... <a href=\"http://dev-resources.site/topic/nextjs/2275771/how-to-change-swipe-direction-in-swiperjs-horizontal-vertical-guide-49j5\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F0mjch0pt5dx58zagrdhm.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>20. NextJs Server Actions</strong></h2><p>In this article, we will explore Next.js Server Actions, which were introduced in Next.js 14 and are... <a href=\"http://dev-resources.site/topic/nextjs/2268699/nextjs-server-actions-3oc9\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2hk5vl3szj3km7qayu7l.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>21. Learn server actions in Nextjs</strong></h2><p>NextJs Server Actions       Khalid Kakar ・ Feb 13              ... <a href=\"http://dev-resources.site/topic/nextjs/2275166/learn-this-the-server-actions-in-nextjs-237h\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DLearn%2520server%2520actions%2520in%2520Nextjs%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2275166%2Flearn-this-the-server-actions-in-nextjs-237h\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>22. 🚀 My Journey in Frontend Development &amp; Building with the Community</strong></h2><p>Hey Dev.to! 👋 I'm excited to share my journey as a frontend developer and how I keep up with the... <a href=\"http://dev-resources.site/topic/nextjs/2274963/my-journey-in-frontend-development-building-with-the-community-31h3\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fap580mj68khk05l5oc7j.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>23. 5+ Best static site generators 2025</strong></h2><p>Statichunt - Explore 3500+ curated static themes, including Astro, Hugo, Jekyll, and more.          ... <a href=\"http://dev-resources.site/topic/nextjs/2274901/5-best-static-site-generators-2025-1236\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F50t3sqsie4hlvi2wqqpo.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>24. Using Clerk Authentication Webhooks with Next.js || Sync Clerk data to your Database</strong></h2><p>Clerk is a powerful authentication service that simplifies user management in modern applications.... <a href=\"http://dev-resources.site/topic/nextjs/2274921/using-clerk-authentication-webhooks-with-nextjs-sync-clerk-data-to-your-database-2pni\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fnfydw7wusq8hboi5fb9f.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>25. 5 Design Patterns for Building Scalable Next.js Applications</strong></h2><p>Next.js is a powerful React framework that simplifies building fast, scalable, and SEO-friendly web... <a href=\"http://dev-resources.site/topic/nextjs/2271529/5-design-patterns-for-building-scalable-nextjs-applications-1c80\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9a3wwv9gga11y440b6s7.jpg\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>26. Self-Hosting a Multiplayer WebXR App with Needle Engine on Vercel</strong></h2><p>Disclaimer   This is an English translation of my original Japanese post on Qiita... <a href=\"http://dev-resources.site/topic/nextjs/2272386/self-hosting-a-multiplayer-webxr-app-with-needle-engine-on-vercel-385c\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DSelf-Hosting%2520a%2520Multiplayer%2520WebXR%2520App%2520with%2520Needle%2520Engine%2520on%2520Vercel%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2272386%2Fself-hosting-a-multiplayer-webxr-app-with-needle-engine-on-vercel-385c\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>27. IntraPaste: A Modern Intranet-First Content Sharing Solution with Next.js and MinIO</strong></h2><p>🚀 Introducing IntraPaste   Ever struggled with sharing files and text snippets across... <a href=\"http://dev-resources.site/topic/nextjs/2272895/intrapaste-a-modern-intranet-first-content-sharing-solution-with-nextjs-and-minio-320g\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DIntraPaste%3A%2520A%2520Modern%2520Intranet-First%2520Content%2520Sharing%2520Solution%2520with%2520Next.js%2520and%2520MinIO%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2272895%2Fintrapaste-a-modern-intranet-first-content-sharing-solution-with-nextjs-and-minio-320g\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>28. How i got 500 stars on my first Open Source project</strong></h2><p>I had never shared my work online before the last three months, this taught me a lot and was a great... <a href=\"http://dev-resources.site/topic/nextjs/2281358/how-i-got-500-stars-on-my-first-open-source-project-3inb\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjeg672d56kw7a4clcszy.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><h2><strong>29. Supercharge Your Payload CMS App's Middleware with saveToJWT</strong></h2><p>Payload CMS offers a ton of flexibility, and one feature I recently rediscovered is the saveToJWT... <a href=\"http://dev-resources.site/topic/nextjs/2272012/supercharge-your-payload-cms-apps-middleware-with-savetojwt-5375\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Funconventional-online-tools.site%2Fapi%2Fgenerate-image%3Ftitle%3DSupercharge%2520Your%2520Payload%2520CMS%2520App%27s%2520Middleware%2520with%2520saveToJWT%26url%3Dhttp%3A%2F%2Fdev-resources.site%2Ftopic%2Fnextjs%2F2272012%2Fsupercharge-your-payload-cms-apps-middleware-with-savetojwt-5375\" alt=\"Thumbnail\" width=\"800\" height=\"122\"></a></p><h2><strong>30. FingerTyping.com: A Free, Open-Source, Secure Typing Application Built with Next.js, Tailwind CSS, and TypeScript</strong></h2><p>I’m excited to introduce FingerTyping.com—a  fully free, open-source, and secure typing tutor that I... <a href=\"http://dev-resources.site/topic/nextjs/2271277/fingertypingcom-a-free-open-source-secure-typing-tutor-built-with-nextjs-tailwind-css-and-13lb\" rel=\"noopener noreferrer\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fr3nlqfhech5x1phdu7uz.png\" alt=\"Thumbnail\" width=\"800\" height=\"336\"></a></p><div><div><div>\n        Next.js gives you hybrid static and server rendering, TypeScript support, smart bundling, route pre-fetching, and more. No config needed.\n      </div></div></div><h2>\n  \n  \n  Additional Technical Cheat Sheets\n</h2><p>For further technical insights and quick references, explore these cheat sheets:</p><h3>\n  \n  \n  Earn $100 Fast: AI + Notion Templates\n</h3><p>Do you want to make extra money quickly? This guide shows you how to create and sell Notion templates step by step. Perfect for beginners or anyone looking for an easy way to start earning online.</p><ul><li> Follow a simple process to create templates people want and will buy.</li><li> Learn to use tools like ChatGPT to design and improve templates.</li><li> More people are using Notion every day, and they need templates to save time and stay organized.</li></ul><ul><li> Ready-made prompts to spark ideas and create templates faster.</li><li> Stay on track as you work.</li></ul><ul><li> Learn everything from idea to sale.</li><li><strong>How to Find Popular Ideas:</strong> Research trends and needs.</li><li> Tips for improving templates with AI tools.</li><li><strong>Making Templates User-Friendly:</strong> Simple tips for better design.</li><li> Advice on sharing and selling on platforms like Gumroad or Etsy.</li><li> Solutions for issues like low sales or tricky designs.</li></ul><ul><li>Anyone who wants to make extra money online.</li><li>People who love using Notion and want to share their ideas.</li><li>Creators looking for a simple way to start selling digital products.</li></ul><h3>\n  \n  \n  💰 <strong>Want to Earn 40% Commission?</strong></h3><p>Join our affiliate program and start making money by promoting ! Earn 40% on every sale you refer.  </p><p>You'll on average around 5$ per sell and for bundled products it will be around 40$ per sale. (So just share it and make money with worrying about product creation and maintanence)</p>","contentLength":6526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The process of creating an effective Application Security Program: Strategies, Practices and tools for the best results","url":"https://dev.to/lynxfelony1/the-process-of-creating-an-effective-application-security-program-strategies-practices-and-tools-3c1o","date":1739730235,"author":"Smart Mohr","guid":730,"unread":true,"content":"<p>AppSec is a multifaceted and robust method that goes beyond simple vulnerability scanning and remediation. A systematic, comprehensive approach is required to incorporate security into every phase of development. The ever-changing threat landscape as well as the growing complexity of software architectures have prompted the need for an active, comprehensive approach. This comprehensive guide explores the fundamental elements, best practices, and cutting-edge technology used to build a highly-effective AppSec programme. It empowers organizations to enhance their software assets, mitigate the risk of attacks and create a security-first culture. </p><p>The underlying principle of a successful AppSec program is an essential shift in mentality which sees security as an integral aspect of the development process rather than an afterthought or separate undertaking. This paradigm shift requires a close collaboration between developers, security, operations, and other personnel. It eliminates silos that hinder communication, creates a sense sharing responsibility, and encourages collaboration in the security of the applications they develop, deploy or maintain. In embracing the DevSecOps approach, organizations can integrate security into the structure of their development processes to ensure that security considerations are considered from the initial designs and ideas until deployment and ongoing maintenance. </p><p>Central to this collaborative approach is the development of clear security policies that include standards, guidelines, and policies that provide a framework to secure coding practices, vulnerability modeling, and threat management. These policies should be based on industry-standard practices like the OWASP top ten, NIST guidelines and the CWE. They should also take into consideration the distinct requirements and risk profiles of an organization's applications and business context. By codifying these policies and making available to all stakeholders, companies can guarantee a consistent, standard approach to security across their entire portfolio of applications. </p><p>can apolication security use ai In order to implement these policies and make them actionable for development teams, it's vital to invest in extensive security education and training programs. These programs should be designed to equip developers with the knowledge and skills necessary to create secure code, recognize possible vulnerabilities, and implement security best practices during the process of development. Training should cover a range of aspects, including secure coding and common attack vectors, in addition to threat modeling and safe architectural design principles. By promoting a culture that encourages constant learning and equipping developers with the tools and resources they need to incorporate security into their daily work, companies can establish a strong base for an efficient AppSec program. </p><p>Organizations should implement security testing and verification methods along with training to identify and fix vulnerabilities prior to exploiting them. This requires a multi-layered strategy that incorporates static and dynamic analysis techniques along with manual code reviews as well as penetration testing. Static Application Security Testing (SAST) tools are able to analyse the source code to identify vulnerability areas that could be vulnerable, including SQL injection, cross-site scripting (XSS) as well as buffer overflows at the beginning of the development process. Dynamic Application Security Testing (DAST) tools are, however, can be used to simulate attacks on running applications, identifying vulnerabilities that are not detectable with static analysis by itself. </p><p>Although these automated tools are essential in identifying vulnerabilities that could be exploited at large scale, they're not a silver bullet. Manual penetration testing conducted by security experts is crucial to discover the business logic-related vulnerabilities that automated tools could miss. Combining automated testing with manual verification allows companies to have a thorough understanding of the application security posture. They can also prioritize remediation activities based on severity and impact of vulnerabilities. </p><p>Businesses should take advantage of the latest technology like artificial intelligence and machine learning to improve their capabilities in security testing and vulnerability assessment. AI-powered tools are able to analyze huge amounts of code and data, identifying patterns and irregularities that could indicate security concerns. These tools also help improve their ability to detect and prevent new threats through learning from the previous vulnerabilities and attacks patterns. </p><p>A particularly exciting application of AI in AppSec is using code property graphs (CPGs) to enable more accurate and efficient vulnerability identification and remediation. CPGs provide a rich, semantic representation of an application's codebase, capturing not just the syntactic structure of the code but as well the intricate relationships and dependencies between different components. Through the use of CPGs AI-driven tools, they can provide a thorough, context-aware analysis of an application's security position, identifying vulnerabilities that may be overlooked by static analysis methods. </p><p>Additionally, CPGs can enable automated vulnerability remediation with the use of AI-powered repair and transformation techniques. AI algorithms are able to generate context-specific, targeted fixes by analyzing the semantic structure and the nature of vulnerabilities that are identified. This allows them to address the root cause of an issue, rather than just treating its symptoms. This strategy not only speed up the remediation process but decreases the possibility of introducing new vulnerabilities or breaking existing functionality. </p><p>Integration of security testing and validating into the continuous integration/continuous deployment (CI/CD) pipeline is a key component of a successful AppSec. Automating security checks, and including them in the build-and-deployment process allows organizations to spot security vulnerabilities early, and keep them from reaching production environments. This shift-left security approach allows rapid feedback loops that speed up the amount of time and effort required to discover and rectify problems. </p><p>In order to achieve this level of integration enterprises must invest in right tooling and infrastructure to support their AppSec program. This does not only include the security tools but also the underlying platforms and frameworks which allow seamless automation and integration. Containerization technology like Docker and Kubernetes play a significant role in this regard because they offer a reliable and uniform setting for testing security and isolating vulnerable components. </p><p>Effective collaboration tools and communication are just as important as technology tools to create an environment of safety, and making it easier for teams to work with each other. Jira and GitLab are systems for tracking issues that help teams to manage and prioritize security vulnerabilities. Chat and messaging tools like Slack and Microsoft Teams facilitate real-time knowledge sharing and exchange between security professionals. </p><p>The effectiveness of an AppSec program is not solely on the tools and technology employed, but also the employees and processes that work to support them. A strong, secure culture requires leadership commitment, clear communication, and an ongoing commitment to improvement. Companies can create an environment that makes security more than a box to mark, but an integral component of the development process by encouraging a sense of responsibility by encouraging dialogue and collaboration by providing support and resources and promoting a belief that security is an obligation shared by all. </p><p>To ensure that their AppSec programs to remain effective over the long term Organizations must set up meaningful metrics and key-performance indicators (KPIs). These KPIs will help them track their progress and help them identify improvements areas. These metrics should be able to span all phases of the application lifecycle starting from the number of vulnerabilities discovered during the development phase to the time it takes to correct the issues and the security level of production applications. These indicators can be used to demonstrate the benefits of AppSec investment, spot trends and patterns and assist organizations in making data-driven choices regarding where to focus on their efforts. </p><p>To keep up with the ever-changing threat landscape as well as new best practices, organizations must continue to pursue learning and education. This could include attending industry conferences, taking part in online-based training programs and collaborating with outside security experts and researchers to stay abreast of the latest trends and techniques. Through the cultivation of a constant education culture, organizations can assure that their AppSec programs remain adaptable and capable of coping with new challenges and threats. </p><p>It is important to realize that security of applications is a constant process that requires constant investment and dedication. It is essential for organizations to constantly review their AppSec strategy to ensure it is effective and aligned to their objectives as new technologies and development practices emerge. Through adopting a continuous improvement mindset, promoting collaboration and communications, and leveraging advanced technologies such CPGs and AI businesses can design an efficient and flexible AppSec program that will not only secure their software assets, but also allow them to be innovative in a rapidly changing digital landscape.<a href=\"https://www.linkedin.com/posts/qwiet_free-webinar-revolutionizing-appsec-with-activity-7255233180742348801-b2oV\" rel=\"noopener noreferrer\">can apolication security use ai</a></p>","contentLength":9831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The art of creating an effective application security Program: Strategies, Methods and tools for optimal results","url":"https://dev.to/lynxfelony1/the-art-of-creating-an-effective-application-security-program-strategies-methods-and-tools-for-23bp","date":1739730234,"author":"Smart Mohr","guid":729,"unread":true,"content":"<p>Navigating the complexities of contemporary software development requires a robust, multifaceted approach to application security (AppSec) which goes beyond just vulnerability scanning and remediation. A holistic, proactive approach is required to incorporate security seamlessly into all phases of development. The constantly evolving threat landscape and the ever-growing complexity of software architectures have prompted the need for a proactive, comprehensive approach. This comprehensive guide explains the most important components, best practices, and the latest technologies that make up an extremely effective AppSec program that allows organizations to secure their software assets, limit threats, and promote an environment of security-first development. </p><p>The underlying principle of a successful AppSec program lies a fundamental shift in mindset that views security as a crucial part of the process of development, rather than an afterthought or a separate endeavor. This paradigm shift necessitates an intensive collaboration between security teams, developers, and operations personnel, removing silos and instilling a feeling of accountability for the security of applications they develop, deploy and maintain. DevSecOps allows organizations to incorporate security into their process of development. This means that security is taken care of throughout the process beginning with ideation, design, and implementation, until ongoing maintenance. </p><p>The key to this approach is the establishment of clear security guidelines standards, guidelines, and standards which provide a structure to secure coding practices, vulnerability modeling, and threat management. These policies should be based upon industry best practices such as the OWASP top ten, NIST guidelines as well as the CWE. multi-agent approach to application security They should also take into consideration the distinct requirements and risk specific to an organization's application and the business context. These policies could be codified and made accessible to all stakeholders in order for organizations to have a uniform, standardized security process across their whole range of applications. </p><p>It is crucial to fund security training and education programs that will aid in the implementation and operation of these policies. The goal of these initiatives is to provide developers with the know-how and expertise required to create secure code, recognize vulnerable areas, and apply best practices for security throughout the development process. The training should cover a broad range of topics including secure coding methods and the most common attack vectors, to threat modelling and principles of secure architecture design. Businesses can establish a solid foundation for AppSec by fostering an environment that promotes continual learning and providing developers with the resources and tools that they need to incorporate security into their daily work. </p><p>Alongside training organisations must also put in place secure security testing and verification procedures to discover and address vulnerabilities before they can be exploited by criminals. This requires a multi-layered approach that includes static and dynamic techniques for analysis along with manual code reviews as well as penetration testing. Early in the development cycle, Static Application Security Testing tools (SAST) can be used to identify vulnerabilities such as SQL Injection, cross-site scripting (XSS) and buffer overflows. Dynamic Application Security Testing (DAST) tools on the other hand are able to simulate attacks on running software, and identify vulnerabilities that may not be detectable by static analysis alone. </p><p>Although these automated tools are vital to identify potential vulnerabilities at large scale, they're not a silver bullet. Manual penetration tests and code reviews conducted by experienced security professionals are also critical in identifying more complex business logic-related weaknesses that automated tools might miss. Combining automated testing and manual validation enables organizations to obtain a full understanding of the application security posture. It also allows them to prioritize remediation strategies based on the severity and impact of vulnerabilities. </p><p>autonomous agents for appsec Enterprises must make use of modern technologies like artificial intelligence and machine learning to enhance their capabilities in security testing and vulnerability assessments. AI-powered tools are able look over large amounts of application and code data and spot patterns and anomalies that could signal security problems. They also learn from past vulnerabilities and attack patterns, constantly improving their ability to detect and stop new security threats. </p><p>One of the most promising applications of AI in AppSec is using code property graphs (CPGs) to provide more accurate and efficient vulnerability identification and remediation. CPGs offer a rich, conceptual representation of an application's codebase. They capture not just the syntactic structure of the code, but also the complex relationships and dependencies between different components. <a href=\"https://ismg.events/roundtable-event/denver-appsec/\" rel=\"noopener noreferrer\">https://ismg.events/roundtable-event/denver-appsec/</a> AI-driven tools that utilize CPGs can provide an in-depth, contextual analysis of the security stance of an application, and identify security holes that could have been missed by conventional static analysis. </p><p>CPGs can be used to automate vulnerability remediation making use of AI-powered methods to perform repairs and transformations to code. AI algorithms can create targeted, context-specific fixes by analyzing the semantics and characteristics of the vulnerabilities identified. This permits them to tackle the root causes of an issue, rather than dealing with its symptoms. This technique will not only speed up treatment but also lowers the possibility of breaking functionality, or introducing new security vulnerabilities. </p><p>Another important aspect of an effective AppSec program is the incorporation of security testing and validation into the integration and continuous deployment (CI/CD) process. Through automated security checks and integrating them in the build and deployment process, companies can spot vulnerabilities early and prevent them from entering production environments. multi-agent approach to application security This shift-left approach for security allows faster feedback loops, reducing the amount of time and effort needed to detect and correct problems. </p><p>To achieve this level of integration, enterprises must invest in right tooling and infrastructure for their AppSec program. This is not just the security testing tools but also the platforms and frameworks that facilitate seamless automation and integration. Containerization technologies like Docker and Kubernetes play a significant role in this regard, since they offer a reliable and reliable environment for security testing as well as isolating vulnerable components. </p><p>In addition to the technical tools effective communication and collaboration platforms can be crucial in fostering an environment of security and helping teams across functional lines to work together effectively. Issue tracking tools such as Jira or GitLab can assist teams to prioritize and manage the risks, while chat and messaging tools such as Slack or Microsoft Teams can facilitate real-time collaboration and sharing of information between security specialists and development teams. </p><p>The effectiveness of the success of an AppSec program depends not only on the tools and techniques employed, but also on the individuals and processes that help them. To establish a culture that promotes security, you must have leadership commitment, clear communication and the commitment to continual improvement. Organisations can help create an environment that makes security more than a box to check, but rather an integral part of development by encouraging a shared sense of accountability, encouraging dialogue and collaboration as well as providing support and resources and encouraging a sense that security is an obligation shared by all. </p><p>In order to ensure the effectiveness of their AppSec program, companies must concentrate on establishing relevant measures and key performance indicators (KPIs) to track their progress and pinpoint areas of improvement. These indicators should cover the entire application lifecycle starting from the number of vulnerabilities discovered during the development phase through to the duration required to address issues and the overall security status of applications in production. agentic ai in appsec These metrics can be used to illustrate the benefits of AppSec investments, detect patterns and trends as well as assist companies in making informed decisions regarding where to focus their efforts. </p><p>To keep up with the constantly changing threat landscape and the latest best practices, companies should be engaged in ongoing education and training. This may include attending industry conferences, taking part in online courses for training, and collaborating with outside security experts and researchers to stay on top of the latest developments and techniques. In fostering a culture that encourages continuing learning, organizations will assure that their AppSec program is able to adapt and resilient in the face of new challenges and threats. </p><p>It is vital to remember that security of applications is a process that requires constant commitment and investment. As new technologies emerge and the development process evolves and change, companies need to constantly review and update their AppSec strategies to ensure that they remain efficient and in line with their objectives. By embracing a mindset of continuous improvement, fostering collaboration and communication, and leveraging the power of advanced technologies such as AI and CPGs, companies can develop a robust and adaptable AppSec program which not only safeguards their software assets but also enables them to develop with confidence in an increasingly complex and challenging digital world.<a href=\"https://ismg.events/roundtable-event/denver-appsec/\" rel=\"noopener noreferrer\">multi-agent approach to application security</a></p>","contentLength":10128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Object Oriented Programming for Interview:)","url":"https://dev.to/madgan95/object-oriented-programming-for-interview-cb2","date":1739727965,"author":"Madhav Ganesan","guid":712,"unread":true,"content":"<p>It is a type of programming paradigm which uses the concept of objects to solve real world problems.</p><p>C++, Java, Python, Javascript, Ruby, Swift, Objective-c, C#</p><ul><li>It is also called as . </li><li>It is an user defined data type used to create an instance of a class. It consists of  and .</li><li>(Note: Classes do not use memory. They merely serve as a template from which items are made)</li></ul><ul><li>It is an instance of a class. </li><li>Objects have been made with class as a blueprint.</li></ul><ul><li>This shows important things to the user and hides the internal details. </li><li>It refers to providing only necessary information about the data to the outside world, hiding the background details or implementation.</li><li>Abstraction is about <strong>simplifying the interface</strong></li></ul><p> We know the important features of the phone such as battery,sim,design and os. But we may not know about how it works like memory allocation operation etc</p><ul><li>It is principle in which all important information is contained inside an object. </li><li>It protects the internal state of an object by keeping its data members private. Encapsulation hides the internal implementation details of a class from external code.</li><li>Encapsulation is concerned with  and  by restricting access to the internal state</li></ul><ul><li>It is a property of OOPS in which a class can derive properties and characteristics from another class.</li></ul><p>\nChild class derived directly from the base class</p><p>\nChild class derived from multiple base classes.</p><p>\nChild class derived from the class which is also derived from another base class.</p><p><strong>Hierarchical Inheritance:</strong>\nMultiple child classes derived from a single base class.</p><p>\nInheritance consisting of multiple inheritance types of the above specified.</p><p>It means having many forms.</p><h2>\n  \n  \n  Compile-time Polymorphism (Method Overloading):\n</h2><ul><li>This occurs when multiple methods in the same class have the same name but different parameters.\n</li></ul><div><pre><code></code></pre></div><h2>\n  \n  \n  Runtime Polymorphism (Method Overriding):\n</h2><ul><li>This occurs when a subclass provides a specific implementation of a method that is already defined in its superclass. </li><li>This is typically achieved using virtual functions in C++.\n</li></ul><div><pre><code></code></pre></div><h2>\n  \n  \n  Dynamic Binding (late binding/dynamic dispatch)\n</h2><ul><li>It is the process of determining the method or function to be executed at runtime, rather than at compile time. </li></ul><ul><li>It refers to the ability of a programming language to determine the data type of a variable at runtime, rather than at compile time.</li></ul><p>Dynamic loading is a mechanism where a program loads a library or module into memory only when it is explicitly requested or required. This helps reduce memory usage and improve performance by loading only the necessary components.</p><p>An abstract class in C++ is a class that cannot be instantiated on its own and is designed to be inherited by other classes. It contains at least one pure virtual function.</p><div><pre><code></code></pre></div><p>An interface in C++ is essentially an abstract class that contains only pure virtual functions</p><div><pre><code></code></pre></div><p>Default Access Control:\nstruct: Members of a struct are public by default.<p>\nclass: Members of a class are private by default.</p></p><p>It is used for simple data structures where data encapsulation is not a primary concern\nit is used for more complex objects where encapsulation, data hiding, and member functions are typically required.</p><p>It is a special member function of a class that is automatically called when an object of the class is created. It initializes the object.</p><div><pre><code></code></pre></div><p>It is a special member function of a class that is automatically called when an object of the class is destroyed. It cleans up resources that the object may have acquired during its lifetime.\n(Note: No, you cannot overload the destructor in C++)</p><p>\nIf you enjoyed this post, don’t forget to follow me on social media for more updates and insights:</p>","contentLength":3641,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Run Google Chrome as Administrator on Windows 11! {6+} Best Ways","url":"https://dev.to/winsides/run-google-chrome-as-administrator-on-windows-11-6-best-ways-56ib","date":1739727300,"author":"Vigneshwaran Vijayakumar","guid":711,"unread":true,"content":"<p><a href=\"https://www.google.com/intl/en_in/chrome/\" rel=\"noopener noreferrer\">Google Chrome</a> is one of the most widely used web browsers,  ,  , and an <strong>extensive range of features</strong>. Chrome is a <strong>cross platform application</strong> and so you can use chrome on  ,  , and . Chrome browser offers you fast performance and ability to load web pages quickly and also have the best integrated security features like automatic updates and safe browsing technology and more. You can run <strong>Google Chrome as Administrator</strong> to take advantage of its amazing features, such as  , <strong>installing extensions and apps</strong> , browsing in  ,  , and efficiently . In this tutorial, I’ve shared the 6 essential ways to open chrome browser with elevated rights on your windows 11 pc.</p><h2>\n  \n  \n  Multiple Ways to Open and Run Google Chrome Web Browser as Administrator on Windows 11\n</h2><ol><li>Quick Way to Run Google Chrome as Administrator on Windows 11 using the .</li><li>Use the  and open Chrome Web Browser as Admin on Windows 11.</li><li>Create a New Task to Run Google Chrome Web Browser with Administrative Privileges using the .</li><li>Always Open Chrome Browser as Admin on Windows 11 by creating a <strong>Dedicated Desktop Shortcut</strong>.</li><li>Open Google Chrome with Elevated Privileges using the .</li><li>Run  with Elevated Permissions and open Google Chrome on Windows 11. </li><li>Open  with Administrator Rights and open Chrome Browser.</li></ol><blockquote><p> : Running <strong>applications as an administrator grants elevated privileges</strong> that can significantly affect your system. Please use caution when doing so, as it can lead to <strong>Unintended System Modifications</strong> ,  ,  ,  ,  , etc.</p></blockquote><h2>\n  \n  \n  1. How to Open Google Chrome Browser as Administrator on Windows 11 using the Start Menu?\n</h2><ul><li>Open the  , you can also use the key combination WinKey + S. Search for .</li><li>Once you find the application,  on that and click .</li></ul><ul><li>The System (  ) will confirm and open Google Chrome Web Browser with Administrative Privileges.</li></ul><h2>\n  \n  \n  2. Use the Run Command and Open Chrome as Admin on Windows 11\n</h2><ul><li>Open the  Command. The easy way to access the Run Command is to use the Keyboard Shortcut, WinKey + R.</li><li>Type the command <code>C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe</code> and press CTRL + SHIFT + ENTER. This will prompt the system to execute the command with Administrative Privileges. This is the  to access the Chrome Executable file, however, if you had chose to install it on a different location or if you using  , then, the file path will change. </li></ul><ul><li>The  will confirm and open this application with administrative privileges.</li></ul><h2>\n  \n  \n  3. Create a New Task to Open Google Chrome Web Browser with Administrative Privileges on Windows 11\n</h2><ul><li>Right-click on the  and click . You can also use the shortcut CTRL + SHIFT + ESC. This combination will open Task Manager directly.</li></ul><ul><li>In the Task Manager, under  , click on .  dialog will pop up now. </li></ul><ul><li>Type the command <code>C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe</code> and enable the checkbox “ <strong>Create this Task with Administrative Privileges</strong> “. Finally, click .</li></ul><h2>\n  \n  \n  4. Always Open Google Chrome Web Brower with Elevated Privileges by creating a Dedicated Desktop Shortcut\n</h2><ul><li> on the Empty space of the Desktop, and click .</li></ul><ul><li>The  dialog box will open now. In “ <strong>Type the Location of the Item</strong> “, enter the command <code>C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe</code> and click .</li></ul><ul><li>The system will assign the shortcut name . Finally, click .</li></ul><ul><li><strong>Google Chrome Web Browser Desktop Shortcut</strong> will be created.</li></ul><ul><li>Right-click on the Shortcut and click on . </li></ul><ul><li><strong>Chrome Browser Shortcut Properties</strong> will open now. Click .</li></ul><ul><li>In  , make sure to enable the checkbox “  “. Finally, click  , and then .</li></ul><ul><li>This option allows you to run this shortcut as an administrator, while protecting your computer from unathorized activity.</li></ul><h2>\n  \n  \n  5. Open Chrome Browser with Elevated Privileges using the File Explorer\n</h2><ul><li>Open File Explorer using the Shortcut WinKey + E.</li><li>In the File Explorer, enter the following path to access the Google Chrome Installation Folder. <code>C:\\Program Files\\Google\\Chrome\\Application</code></li></ul><ul><li>Once in the Google Chrome Folder, locate Google Chrome Application,  on that and click .</li></ul><h2>\n  \n  \n  6. Open Command Prompt with Elevated Permissions and Run Google Chrome Web Browser on Windows 11\n</h2><ul><li>Open Run Command using the shortcut WinKey + R. </li><li>Type  and press CTRL + SHIFT + ENTER.</li></ul><ul><li>This will prompt the system to execute this command with Administrative Privileges. In the PowerShell, execute the following command. <code>start \"\" \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"</code></li></ul><h2>\n  \n  \n  7. Open Windows PowerShell with Elevated Privileges and Run Google Chrome\n</h2><ul><li>Open Run Command using the shortcut WinKey + R. </li><li>Type , and press CTRL + SHIFT + ENTER. This will prompt the system to execute this command with Administrative Privileges.</li></ul><ul><li>In the PowerShell, execute the following command. <code>Start-Process \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" -Verb RunAs</code></li></ul><h2>\n  \n  \n  Kindly Be Cautious while Running an App as Administrator on Windows 11\n</h2><p>While running <strong>Google Chrome Web Browser as Administrator on Windows 11</strong> grants it  , which can <strong>potentially bypass system security restrictions</strong>. Please proceed with care and only run trusted applications in this mode. At <a href=\"https://winsides.com\" rel=\"noopener noreferrer\">Winsides.com</a>, we advise our users to use Administrator privileges only when absolutely necessary to perform tasks requiring elevated rights. Misuse of administrative privileges may lead to  , <strong>Data Loss, System Instability, Security Concerns</strong> ,  , .</p><p>I hope the above tutorial helps you run and open the Google Chrome browser as an Administrator with elevated rights on your Windows 11 PC. If you have any questions about this topic, feel free to leave a comment below, and I will assist you in resolving your issue. Find more interesting tutorials on our homepage: <a href=\"https://winsides.com/\" rel=\"noopener noreferrer\">WinSides.com</a></p>","contentLength":5600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Types of Errors in Python","url":"https://dev.to/sandhya_kolikonda_/types-of-errors-in-python-1ccn","date":1739726560,"author":"Sandhya Kolikonda","guid":710,"unread":true,"content":"<p>In python programming we get three types of errors. they are</p><div><pre><code>           1.Compile Time Error\n           2.Logical Error\n           3.Run Time Error\n</code></pre></div><p>Let us discuss in briefly:\n*\nThese errors are occurs during compilation Time. and these errors occurs due to the syntax's are not followed during program Development.</p><ul><li>These errors are solved by programmers During Development Time.</li></ul><p>\nThese errors occurs during runtime or execution time. and these errors occurs due to wrong representation of Logic.</p><ul><li>These errors are solved by programmers during Development Time.</li></ul><p>\nThese errors are occurs during Runtime or execution time. and these errors occurs due to wrong inputs or invalid inputs entered by end users or application users.</p><ul><li><p>When Runtime errors occurs, by default all the languages gives Technical error messages, which are understandable by programmers but not by end users. this is not a recommended process in real time.</p></li><li><p>According to industry standards it is recommended to display always user friendly error messages for making the application robust by using exception handling.</p></li></ul><p>Note:\nWhen the end-user enter the valid input to the project it gives valid result. and when the end-user enter the invalid input to the project then the project displays user-friendly error message by using exception handling. </p>","contentLength":1303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ETL with DLTHUB","url":"https://dev.to/grokker_f9bf83d79cb9beb6f/etl-with-dlthub-3i25","date":1739726216,"author":"Dinesh","guid":709,"unread":true,"content":"<p>Just wrapped up the Data Engineering Zoomcamp workshop on dlt! Learned how to build data pipelines the easy way - extracted NYC Taxi data using dlt's REST client, handled pagination like a pro, and loaded it all into DuckDB. Love how dlt makes ETL feel like a breeze!</p>","contentLength":267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Setup a GitHub Self-Hosted Runner on an Ubuntu VM: A DevOps Guide","url":"https://dev.to/codehirise/setup-a-github-self-hosted-runner-on-an-ubuntu-vm-a-devops-guide-3aim","date":1739725419,"author":"CodeHiRise","guid":708,"unread":true,"content":"<p>In this article, we will walk you through the steps for setting up GitHub Self-hosted Runners in Ubuntu virtual machine as a service for persistence runner installation.</p><p>The primary reason organizations to adopt GitHub self-hosted runners is to securely access internal, private networks such as databases, services, or virtual machines without exposing them to the public internet. By keeping workflows within their controlled infrastructure, they eliminate risks like accidental data leaks or compliance violations.</p><p>Additionally, self-hosted runners safeguarding sensitive code, credentials, or artifacts since runners are hosted on organization network.</p><ol><li>An&nbsp;Ubuntu Virtual machine with root access with internet connectivity.</li><li>Basic familiarity with Linux commands and GitHub Actions.</li></ol><p>First go to your desired GitHub repository and go to Settings &gt; Actions &gt; Runners &gt; New self-hosted runner.</p><p>When you click New self hosted runner button you will get below page with all required steps.</p><p>It outlines downloading and setting up action runner but in our case we will take some extra steps to make sure our setup is more secure and configure runner as a systemd service.</p><h2>\n  \n  \n  Step 1: Create a Dedicated System User for GitHub action runner\n</h2><p>we will create new system user user in our ubuntu vm.</p><div><pre><code>useradd  /usr/sbin/nologin gh-action-runner-user\n</code></pre></div><ul><li>: Creates a non-interactive service account.</li><li><code>--shell /usr/sbin/nologin</code>: Blocks SSH/login access.</li></ul><h2><strong>Step 2: Download the GitHub Runner</strong></h2><p>Run below command to create runners installation directory in /opt directory.</p><div><pre><code> /opt/gh-runner\n /opt\n</code></pre></div><h4>\n  \n  \n  Use the&nbsp;/opt&nbsp;directory for installation of add-on application software package and avoid using user's home directory\n</h4><p>Run below command to change ownership of the directory to our current owner for downloading and extracting files.</p><div><pre><code>codehirise:codehirise gh-runner/\ngh-runner\n</code></pre></div><p>then run below to extract action runner.</p><div><pre><code>curl  actions-runner-linux-x64-2.322.0.tar.gz  https://github.com/actions/runner/releases/download/v2.322.0/actions-runner-linux-x64-2.322.0.tar.gz\n</code></pre></div><p>optionally run below command to verify downloaded tar file.</p><div><pre><code> | shasum  256 </code></pre></div><p>Next run below command to extact all actions runner tar.gz file.</p><div><pre><code>xzf ./actions-runner-linux-x64-2.322.0.tar.gz\n</code></pre></div><h2><strong>Step 3: Configure the Runner</strong></h2><p>After downloading run below command to configure GitHub action runner.</p><div><pre><code>./config.sh  https://github.com/codehirise/github-actions-private-runner-demo  YOUR_TOKEN\n</code></pre></div><h4>\n  \n  \n  Replace YOUR_TOKEN with the token displayed in GitHub add new self hosted action runner page.\n</h4><p>Here you will be prompled for some input like runner name and lables. You can enter them as you see fit.</p><p>It will create some additional files like  which we will be using to configure action runner as a systemd service.</p><p>Before that make sure to change the ownership of the directory to our Linux service user which is . \nRun below command</p><div><pre><code>gh-action-runner-user:gh-action-runner-user /opt/gh-runner/ </code></pre></div><h2><strong>Step 4: Configure the Runner Systemd Service</strong></h2><p>Next run below command to install systemd service for github action runner. This will install action runner as a systemd service and set user as .</p><div><pre><code> ./svc.sh gh-action-runner-user\n</code></pre></div><p>Next you can run status command to check status</p><p>and run start command to start github action runner</p><p>to check github action run related logs simply run</p><div><pre><code>journalctl  actions.runner.codehirise-github-actions-private-runner-demo.gh-private-runner-demo.service\n</code></pre></div><h4>\n  \n  \n  Note that actions.runner.codehirise-github-actions-private-runner-demo.gh-private-runner-demo.service is the service name which is displayed when running  sudo ./svc.sh status command.\n</h4><p>It will show connected to GitHub which indicate that we have successfully setup our GitHub action runner as a service.\nif you now visit runners page you can see our brand new github action self hosted runner is ready and waiting to pickup jobs.</p><p>Action runner systemd service comes enabled by default so it will automatically start after a system reboot.</p><h2><strong>Step 5: Using selfhosted runner in Github action workflow</strong></h2><p>To use this runner in workflow , set  in your github actions workflow file to </p><p>Example workflow file below.</p><div><pre><code></code></pre></div><p>in action workflow we can see job ran on the runner we setup.</p><p>In our runner logs we can see job completed successfully </p><p>Self-hosted GitHub runners on Ubuntu VMs put you in control—locking down sensitive workflows and integrating seamlessly with private infrastructure. With this setup, your pipelines stay fast, secure, and entirely within your environment. Will this be your team’s next step toward DevOps adventure? 🚀</p>","contentLength":4514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop Icons from Moving When Your Page Loads","url":"https://dev.to/documendous/stop-icons-from-moving-when-your-page-loads-5o","date":1739725403,"author":"Documendous","guid":707,"unread":true,"content":"<p>Have you ever seen your icons flicker or move when your page loads? This happens because the browser loads the icon font a little later, making things shift around. Here’s how to keep the icons in place and make them appear smoothly.</p><p>Material Icons and Symbols use , which don’t load right away. Instead, the browser:</p><ul><li>Leaves a blank space or shows temporary text where the icon should be.</li><li>Loads the font in the background.</li><li>Puts the icon in place once the font is ready, causing a sudden change.</li></ul><p>Some people use  to hide icons until they load, but this makes them  and causes layout changes. Instead, using  keeps the space reserved so the page looks smoother.</p><h2>\n  \n  \n  The Fix: Use  Instead of </h2><p>Change your CSS to use  so that the icons are invisible at first but don’t move things around:</p><div><pre><code></code></pre></div><h3><strong>2. Update Your JavaScript</strong></h3><p>Once the page loads, add a class to make the icons visible:</p><div><pre><code></code></pre></div><p>This makes the icons fade in smoothly instead of appearing suddenly.</p><div><table><tbody><tr><td>The icon disappears completely, which can cause things to move.</td></tr><tr><td>The icon stays in place but is invisible at first, preventing movement.</td></tr></tbody></table></div><p>Using  means your icons won’t suddenly pop in and push things around.</p><h2>\n  \n  \n  Bonus Tip: Load the Font Faster\n</h2><p>To make sure icons appear even faster, you can  the font so the browser gets it earlier:</p><div><pre><code></code></pre></div><p>This helps the browser load the font before it’s needed, reducing the delay.</p><p>By using  instead of , you can stop your icons from shifting around when they load. This small trick makes your page look better and feel smoother.</p><p>Try it out and see the difference!</p>","contentLength":1533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Say Goodbye to Media Queries: Print Fully Styled HTML Content","url":"https://dev.to/raheelshan/say-goodbye-to-media-queries-print-fully-styled-html-content-3be7","date":1739725303,"author":"Raheel Shan","guid":706,"unread":true,"content":"<p>Printing HTML content with all styles intact has always been a challenging task for me. Media queries often complicate things when trying to achieve a clean and consistent print layout. However, I recently discovered a simple solution that bypasses the need for media queries altogether. Here’s how I did it.</p><h2>\n  \n  \n  The Solution: Using jsPDF to Print HTML as an Image\n</h2><p>I used jsPDF, a JavaScript library, to convert HTML content into an image. This image is then set into a hidden <a href=\"https://dev.to/raheelshan/say-goodbye-to-media-queries-print-fully-styled-html-content-3be7\"></a> element, which is displayed only during print. The idea is straightforward:</p><p>The HTML body has two main elements:</p><ul><li>A  containing the main content with id root-container.\n</li><li>A hidden  element with id printed-image outside of root-container where the image generated by jsPDF will be stored.\n</li></ul><pre><code>&lt;body&gt;\n    &lt;div id=\"root-container\"&gt;\n        &lt;!-- html content --&gt;\n        &lt;div id=\"content-to-print\"&gt;&lt;/div&gt;\n        &lt;!-- other content --&gt;\n    &lt;/div&gt;    \n    &lt;img src=\"\" alt=\"\" id=\"printed-image\" /&gt;\n&lt;/body&gt;\n</code></pre><ul><li>Extract the HTML content you want to print and use jsPDF to convert it into an image.</li><li>Save the generated image into the hidden  element (#printed-image).\n</li></ul><pre><code>// generate image on document ready to be saved in image tag\n$(document).ready(function() {\n    generateImage()\n})\n\n// function to be called on button click\nfunction printImage() {\n    window.print();\n}\n\n// logic to convert content to image\nfunction generateImage() {\n\n    const style = document.createElement('style');\n    document.head.appendChild(style);\n    style.sheet?.insertRule('body &gt; div:last-child img { display: inline-block; }');\n\n    html2canvas(document.getElementById('content-to-print'), {\n        useCORS: true, // To handle cross-origin images\n        scale: 3,\n        backgroundColor: '#FFF', // Do not force a background color in html2canvas\n        scrollX: 0,\n        scrollY: 0,\n        windowWidth: document.documentElement.scrollWidth,\n        windowHeight: document.documentElement.scrollHeight\n    }).then(function(canvas) {\n        const {\n            jsPDF\n        } = window.jspdf;\n        const imgData = canvas.toDataURL('image/png');\n        const pdf = new jsPDF('p', 'mm', 'a4'); // 'p' for portrait, 'mm' for millimeters, 'a4' for A4 size\n        const imgWidth = 210; // A4 width in mm (210mm)\n        const pageHeight = 285; // A4 height in mm (297mm)\n        const imgHeight = canvas.height * imgWidth / canvas.width;\n        const heightLeft = imgHeight;\n        let position = -7;\n\n        // Add a white background to the PDF page\n        pdf.setFillColor(255, 255, 255); // RGB for white\n        pdf.rect(0, 0, imgWidth, pageHeight, 'F'); // Fill the background\n\n        pdf.addImage(imgData, 'PNG', 0, position, imgWidth, imgHeight);\n\n        $('#printed-image').prop('src', imgData)\n    });\n}\n</code></pre><p>Add print-specific styles to hide the main content and display only the generated image when the print button is pressed.</p><pre><code>/* Default styles */\n#printed-image {\n    display: none;\n}\n\n/* Print-specific styles */\n@media print {\n    #root-container {\n        display: none !important;\n    }\n\n    #printed-image {\n        display: block !important; /* Force image to display only during print */\n        margin: auto;             /* Optional: center the image */\n        max-width: 100%;          /* Ensure image fits the page */\n    }\n}\n</code></pre><ul><li>When the document is ready html content will be converted to image</li><li>This image is placed inside the hidden  element.</li><li>During printing, the main content (#root-container) is hidden, and the image (#printed-image) is displayed.</li></ul><p>By converting the HTML content into an image</p><ul><li>Media Queries Become Obsolete: Styles are baked into the image, so there’s no need to tweak styles for different screen sizes or layouts.</li><li>Consistent Print Output: The image ensures that the print result matches what you see on the screen, regardless of the printer or browser quirks.</li></ul>","contentLength":3849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Quantum Computing Revolution: Opportunities and Challenges in Cryptography","url":"https://dev.to/brio97/the-quantum-computing-revolution-opportunities-and-challenges-in-cryptography-5h6i","date":1739725282,"author":"Brian K Mutai","guid":705,"unread":true,"content":"<p>Quantum computing has emerged as a transformative field, promising breakthroughs in industries such as finance, healthcare, and artificial intelligence. However, one of its most profound implications lies in the realm of cybersecurity and cryptography. As quantum computers continue to advance, they pose an existential threat to classical encryption systems that secure global communications, financial transactions, and personal data. This looming challenge has prompted a race toward developing post-quantum cryptographic solutions that can withstand the power of quantum attacks.\n**</p><h2>\n  \n  \n  Understanding Quantum Computing\n</h2><p>Unlike classical computers, which process information using binary bits (0s and 1s), quantum computers leverage quantum bits, or qubits. Qubits can exist in a state of superposition, meaning they can be both 0 and 1 simultaneously. Additionally, they exhibit entanglement, a phenomenon where the state of one qubit is directly linked to another, regardless of distance. These properties enable quantum computers to perform calculations at an exponential rate compared to classical computers.</p><p>Potential applications of quantum computing span across various domains. In finance, they could optimize complex risk models and trading algorithms. In pharmaceuticals, they could revolutionize drug discovery by simulating molecular interactions more efficiently than traditional methods. Despite these exciting prospects, their impact on cybersecurity remains one of the most pressing concerns.\n**</p><h2>\n  \n  \n  The Cryptographic Threat of Quantum Computing\n</h2><p>Modern encryption techniques, such as RSA (Rivest-Shamir-Adleman) and ECC (Elliptic Curve Cryptography), rely on the difficulty of certain mathematical problems, such as prime factorization and discrete logarithms. These problems are computationally infeasible for classical computers, providing strong security guarantees. However, quantum computers equipped with Shor’s algorithm can solve these problems exponentially faster, rendering RSA and ECC encryption obsolete.</p><p>If a sufficiently powerful quantum computer were built today, it could break widely used encryption protocols, exposing confidential government communications, banking systems, and personal data to potential cyber threats. The implications of such a breakthrough would be catastrophic, leading to massive security breaches and loss of privacy on an unprecedented scale.\n**</p><h2>\n  \n  \n  The Race Toward Post-Quantum Cryptography\n</h2><p>To counteract the quantum threat, researchers are actively developing post-quantum cryptographic algorithms—encryption techniques designed to resist attacks from both classical and quantum computers. The National Institute of Standards and Technology (NIST) has been leading an initiative to standardize post-quantum cryptography (PQC), with several promising candidates emerging from their ongoing evaluation.</p><p>Some of the most notable post-quantum cryptographic approaches include:</p><p>-<strong>Lattice-based cryptography</strong> – Relies on complex geometric \n   problems that even quantum computers struggle to solve <p>\n   efficiently. Examples include CRYSTALS-Kyber and CRYSTALS- </p>\n   Dilithium. – Based on the difficulty of decoding \n   random linear codes, a problem that has remained unsolved for <p>\n   decades. McEliece encryption is a notable example.</p>\n-<strong>Multivariate polynomial cryptography</strong> – Uses systems of \n   polynomial equations over finite fields, which are hard to solve <p>\n   even with quantum algorithms.</p>\n- – Uses cryptographic hash functions \n   for digital signatures and is considered one of the most <p>\n   reliable alternatives to current methods.</p></p><h2>\n  \n  \n  The Urgency of Transitioning to Post-Quantum Security\n</h2><p>Although large-scale quantum computers capable of breaking encryption do not yet exist, the transition to post-quantum cryptographic systems must begin now. The process of integrating new encryption standards across industries and governments is complex and time-consuming. Companies that delay adoption risk falling victim to the \"harvest now, decrypt later\" strategy, where adversaries collect encrypted data today in anticipation of decrypting it once quantum computers become viable.</p><p>Organizations, financial institutions, and governments must start implementing hybrid cryptographic approaches that combine classical encryption with quantum-resistant algorithms. This ensures a gradual and secure transition as quantum technology continues to progress.\n**</p><h2>\n  \n  \n  Collaboration and Future Outlook\n</h2><p>The development of quantum-safe encryption requires collaboration between governments, cybersecurity experts, and industry leaders. Governments must establish regulations and incentives to encourage early adoption of post-quantum cryptographic methods. Technology companies and research institutions must continue refining these new encryption techniques to ensure they are scalable, efficient, and resistant to future quantum attacks.</p><p>While quantum computing promises immense benefits, it also presents one of the most significant security challenges of our time. The race to secure digital infrastructure against quantum threats is already underway, and proactive measures taken today will define the future of cybersecurity in the quantum era.</p><p>Quantum computing and cryptography are evolving rapidly, what are your thoughts on the future of digital security? Leave a comment below and share your perspective!</p><p>If you enjoyed this article, check out more of my work on my portfolio: <a href=\"https://moderndevspace.netlify.app\" rel=\"noopener noreferrer\">moderndevspace</a> 🚀</p>","contentLength":5479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semantic Kernel – Parte 03: Embeddings y Retrieval-Augmented Generation (RAG)","url":"https://dev.to/isaacojeda/semantic-kernel-parte-03-embeddings-y-retrieval-augmented-generation-rag-4fjn","date":1739725231,"author":"Isaac Ojeda","guid":704,"unread":true,"content":"<p>En esta tercera parte de nuestra serie sobre , nos adentramos en la integración de  y <strong>Retrieval-Augmented Generation (RAG)</strong> para mejorar la generación de contenido y la recuperación de información. Utilizando herramientas como  para la creación de embeddings y  como vector store, podemos construir sistemas más inteligentes capaces de generar respuestas más precisas basadas en datos externos y actualizados, en lugar de depender únicamente de los datos con los que fue entrenado el modelo. A través de esta configuración, aprovechamos la sinergia entre el poder de los modelos de lenguaje y la capacidad de búsqueda semántica para proporcionar soluciones más efectivas a las consultas de los usuarios. En este artículo, exploraremos cómo implementar estas tecnologías en , con ejemplos prácticos para configurar los servicios y gestionar los embeddings de manera eficiente.</p><p>Los  son representaciones numéricas de texto en un espacio vectorial de alta dimensión. Estas representaciones permiten medir la similitud semántica entre palabras, frases o documentos completos. En el contexto de , los embeddings se utilizan para mejorar la búsqueda de información y la generación de respuestas basadas en conocimiento almacenado.</p><h3><strong>Características clave de los Embeddings:</strong></h3><ul><li>Capturan el significado semántico del texto.</li><li>Permiten encontrar contenido relacionado aunque no se utilicen exactamente las mismas palabras.</li><li>Se almacenan en bases de datos especializadas llamadas  (como ).</li><li>Se generan con modelos de lenguaje avanzados, como los proporcionados por  u .</li></ul><h3><strong>¿Qué es RAG (Retrieval-Augmented Generation)?</strong></h3><p><strong>Retrieval-Augmented Generation (RAG)</strong> es una técnica que combina la recuperación de información con la generación de respuestas mediante un modelo de lenguaje. En lugar de confiar solo en los datos con los que fue entrenado el modelo, RAG permite buscar información relevante en bases de conocimiento externas y utilizarla para generar respuestas más precisas y actualizadas.</p><h4><strong>Funcionamiento de RAG en Semantic Kernel:</strong></h4><ol><li>: Un usuario hace una pregunta o solicitud.</li><li>:\n\n<ul><li>Se generan embeddings de la consulta.</li><li>Se comparan con los embeddings almacenados en el vector store (Qdrant) para recuperar información relevante.</li></ul></li><li>:\n\n<ul><li>La información recuperada se pasa como contexto al modelo de lenguaje.</li><li>Se genera una respuesta más precisa y fundamentada en los datos encontrados.</li></ul></li></ol><h3><strong>Beneficios de RAG en Semantic Kernel:</strong></h3><ul><li><strong>Mejor comprensión de consultas</strong>: Permite obtener respuestas relevantes incluso si el usuario no usa palabras exactas.</li><li>: Se pueden agregar nuevos datos al sistema sin necesidad de reentrenar el modelo de IA.</li><li><strong>Optimización del procesamiento</strong>: Reduce el costo computacional al recuperar solo la información relevante en lugar de analizar todo el corpus de datos.</li></ul><h2><strong>Configurando Semantic Kernel con Embeddings y un Vector Store</strong></h2><p>En esta sección, configuraremos  para trabajar con  y un , utilizando  para orquestar los servicios.</p><p>Los servicios principales que vamos a configurar son:</p><ul><li>: Modelo generador de texto.</li><li>: Modelo generador de embeddings.\n</li><li>: Almacenamiento y recuperación de embeddings.\n</li><li>: Orquestación y despliegue de los servicios.</li></ul><p>Aspire nos permite definir y gestionar los servicios necesarios en un solo archivo de configuración. En este caso, configuramos  como nuestro vector store y  y  como proveedor de embeddings.</p><div><pre><code></code></pre></div><ul><li> se configura como un servicio persistente.\n\n<ul><li>Se hace de esta forma para que no sea tan lento de inicializar en cada inicio de Aspire.</li></ul></li><li> se configura con almacenamiento de datos y una interfaz web.\n\n<ul><li> Se añaden los modelos  para generación de texto y  para embeddings.\n</li></ul></li><li>Por último se agrega nuestra API en ASP.NET Core.\n<h3><strong>Configuración de Semantic Kernel con Aspire</strong></h3></li></ul><p>Aspire se encargará de inyectar las  en la API, según los nombres definidos en la configuración.</p><p>Para hacer que  utilice correctamente los servicios orquestados por Aspire, debemos procesar estas cadenas de conexión. Como lo hemos hecho en ejemplos anteriores, parsearemos la información recibida y estructuraremos mejor nuestra configuración.</p><p>Para lograr esto, <strong>separaremos la configuración de dependencias en un archivo específico</strong>, lo que nos permitirá mantener un código más limpio y modular.</p><h4><strong>Creando  para gestionar las dependencias</strong></h4><p>En este archivo, crearemos el método de extensión , el cual:</p><ul><li>Obtiene las cadenas de conexión inyectadas por Aspire.\n</li><li>Extrae los detalles necesarios de  (para generación de texto y embeddings).\n</li><li>Extrae la información de conexión a  (Vector Store).\n</li><li>Registra los servicios de  con las dependencias configuradas.\n</li><li>Agrega un servicio de  () que permite realizar búsquedas en una colección de vectores, el cual usaremos más adelante.</li></ul><p>Aquí está el código en :</p><div><pre><code></code></pre></div><ol><li><strong>Obtenemos los detalles de conexión</strong><ul><li><code>GetModelDetailsFromConnectionString</code> extrae el modelo y endpoint desde las cadenas de conexión de  (chat y embeddings).</li><li><code>GetQdrantDetailsFromConnectionString</code> extrae el  de Qdrant.</li></ul></li><li><strong>Registramos los servicios de Semantic Kernel</strong><ul><li> para generación de texto con .</li><li><code>AddOllamaTextEmbeddingGeneration</code> para generación de .</li><li> para almacenar y recuperar datos desde .</li></ul></li><li><strong>Configuramos un servicio de búsqueda semántica ()</strong><ul><li>Se obtiene la colección de  asociada a .</li><li>Si la colección no existe, se  en Qdrant.</li><li>Se inicializa un objeto <code>VectorStoreTextSearch&lt;BlogPost&gt;</code>, permitiendo realizar  en la colección.</li></ul></li></ol><h2><strong>Almacenando Embeddings en Qdrant</strong></h2><p>Para almacenar datos en Qdrant, primero debemos definir nuestro modelo de datos que será indexado en la base de datos vectorial.<p>\nEn este ejemplo, queremos guardar una serie de posts y realizar búsquedas semánticas sobre su contenido.</p></p><blockquote><p>Advertencia ⚠️: Las búsquedas vectoriales que estamos viendo aquí están en , por lo que podrían cambiar en el futuro. Actualmente, <strong>no se recomienda su uso en producción</strong>, a menos que estés dispuesto a evolucionar junto con el framework.\nAnteriormente, se utilizaban los <strong>Memory de Semantic Kernel</strong> para búsquedas similares, pero estos están en proceso de ser considerados , por lo que su uso a largo plazo podría no ser viable.</p></blockquote><h3><strong>Definición del modelo </strong></h3><p>El modelo  representará las entradas del blog en nuestra base de datos vectorial.</p><div><pre><code></code></pre></div><ul><li>: Identificador único del post. Se marca como clave primaria con  y como enlace de resultados de búsqueda con .</li><li>: Almacena el título del post. Usado como filtro con <code>[VectorStoreRecordData(IsFilterable = true)]</code> y como nombre principal en los resultados de búsqueda con .</li><li>: Contiene el contenido del post. Es indexado para búsquedas de texto completo con <code>[VectorStoreRecordData(IsFullTextSearchable = true)]</code> y su valor es mostrado en los resultados con .</li><li>: Representación vectorial de . Almacenado como un vector de 768 dimensiones usando <code>[VectorStoreRecordVector]</code>, con  como función de distancia y  para indexación eficiente.</li><li>: Etiquetas relacionadas con el post. Pueden ser usadas como filtros en consultas con <code>[VectorStoreRecordData(IsFilterable = true)]</code>.\n### <strong>Creando un endpoint para almacenar  en Qdrant</strong></li></ul><p>Para guardar nuevos , crearemos un endpoint que recibirá los datos del post, generará su embedding y lo almacenará en la base de datos vectorial.</p><p>Nuestro servicio ya tiene configurados <code>ITextEmbeddingGenerationService</code> (para generar embeddings) e  (para interactuar con Qdrant).</p><div><pre><code></code></pre></div><ol><li><ul><li>Recibe los servicios <code>ITextEmbeddingGenerationService</code> y  a través del constructor.</li></ul></li><li><ul><li>Obtiene la colección  de Qdrant.</li><li>Si la colección no existe, la crea con <code>CreateCollectionIfNotExistsAsync()</code>.</li><li>Genera el embedding del  usando <code>_embeddingService.GenerateEmbeddingAsync()</code>.</li><li>Crea un  con los datos proporcionados y el embedding generado.</li><li>Guarda el post en Qdrant con , que inserta o actualiza el registro.</li></ul></li></ol><blockquote><p>: Para ver cómo este handler se integra con Minimal APIs, revisa el código fuente del proyecto. Para mantener este artículo conciso, omitiremos detalles específicos.</p></blockquote><h4><strong>Probando el endpoint con </strong></h4><p>Para poblar la base de datos vectorial, podemos hacer una solicitud HTTP de prueba.</p><p>Ejemplo en <code>SemanticKernelLearning03.ApiService.http</code>:</p><div><pre><code>@SemanticKernelLearning03.ApiService_HostAddress = https://localhost:7391\n\nPOST {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts\nContent-Type: application/json\n\n{\n  \"Title\": \"Introducción a ASP.NET Core\",\n  \"Description\": \"Una guía completa sobre cómo comenzar con ASP.NET Core y sus características principales.\",\n  \"Tags\": [\"ASP.NET Core\", \"C#\", \"Web Development\"]\n}\n</code></pre></div><h3><strong>Verificando los datos en Qdrant</strong></h3><p>Si ejecutamos nuestra solución con Aspire y realizamos la solicitud anterior, podemos inspeccionar los datos en el dashboard de Qdrant y verificar que la colección y los registros existen.</p><h4><strong>Consideraciones sobre el tamaño de los vectores</strong></h4><ul><li>Este ejemplo usa , que genera vectores de .</li><li>Qdrant debe estar configurado para aceptar este tamaño de vector.</li><li>Si usas modelos de OpenAI u otros proveedores, verifica el tamaño del vector antes de insertarlo en Qdrant.</li></ul><h2><strong>Recuperando Información desde Qdrant</strong></h2><p>Para demostrar cómo realizar búsquedas semánticas en los vectores almacenados en Qdrant, podemos hacerlo de distintas maneras: usando el vector directamente o utilizando una abstracción que  nos proporciona, llamada .</p><p> tiene distintas implementaciones, como  (que realiza búsquedas en Bing), pero en nuestro caso usaremos , que está diseñado específicamente para realizar búsquedas en bases de datos vectoriales.</p><p>Esta clase necesita conocer:</p><ul><li> que estamos usando.</li><li><strong>El nombre de la colección en Qdrant</strong> donde haremos la búsqueda.</li></ul><p>El proceso realmente es sencillo y podríamos hacerlo manualmente construyendo consultas directamente contra la base de datos vectorial. Sin embargo, en este caso, aprovecharemos  para simplificar el proceso.</p><h3><strong>Implementación del Endpoint </strong></h3><p>Para realizar la búsqueda semántica, crearemos un nuevo endpoint llamado , que permitirá encontrar los  basados en una consulta en lenguaje natural.</p><div><pre><code></code></pre></div><ol><li><ul><li>Lo recibimos en el constructor con <code>[FromKeyedServices(BlogPost.VectorName)]</code>. Esto asegura que estamos inyectando la instancia correcta configurada para nuestra colección de  en Qdrant.\n\n<ul><li>Esto lo hemos configurado en  anteriormente.</li></ul></li></ul></li><li><strong>Realizamos la búsqueda semántica</strong><ul><li>Llamamos a <code>GetTextSearchResultsAsync(...);</code>, donde:\n\n<ul><li> es el término de búsqueda ingresado.</li><li> indica que queremos los <strong>dos resultados más relevantes</strong>.</li><li> significa que no saltaremos ningún resultado.</li></ul></li></ul></li><li><strong>Procesamos los resultados</strong><ul><li>Iteramos sobre  para extraer:\n\n<ul><li>: Contiene el  almacenado como string, lo convertimos a .</li><li>: Corresponde al título del blog post.</li><li>: Contiene la descripción del blog post.</li></ul></li></ul></li><li><strong>Devolvemos la lista de resultados</strong><ul><li>Cada  representa un  basado en la búsqueda semántica.</li></ul></li></ol><p>Una vez que el endpoint  está registrado, podemos comenzar a realizar búsquedas semánticas para encontrar los posts más relevantes según el texto ingresado.</p><p>Podemos hacer una búsqueda con la siguiente solicitud:</p><div><pre><code>### Busquedas Semanticas\n@Query=arquitectura de software\nGET {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts?Query={{Query}}\n</code></pre></div><p>Si hay blog posts relevantes en la base de datos, la API devolverá un listado de los más coincidentes. Un ejemplo de respuesta podría ser:</p><div><pre><code></code></pre></div><p>Si revisamos el repositorio, podemos notar que hay posts de distintos temas (por ejemplo, <strong>Machine Learning, Bases de Datos, DevOps</strong>), pero la búsqueda regresó los más relevantes según el texto ingresado. Esto demuestra que el motor de búsqueda semántico está funcionando correctamente.</p><h4><strong>Personalizando la Búsqueda</strong></h4><p>Podemos ajustar la consulta cambiando:</p><ul><li><strong>La cantidad de resultados ()</strong>: Actualmente devuelve , pero podríamos aumentar este valor si queremos más opciones.</li><li>: Si cambiamos el modelo de embeddings usado para almacenar los vectores, los resultados pueden variar en precisión y relevancia.</li></ul><h2><strong>Integración con RAG (Retrieval-Augmented Generation)</strong></h2><p>En esta sección, vamos a implementar un nuevo endpoint que va más allá de una búsqueda semántica. En lugar de solo buscar contenido relevante, vamos a generar texto de manera dinámica a partir de una consulta proporcionada por el usuario. Lo interesante aquí es que el contexto para la generación de la respuesta será el contenido de los  que coincidan con la consulta del usuario.</p><p>Si el usuario hace una pregunta como:</p><div><pre><code>cuál es el objetivo principal de clean architecture?\n</code></pre></div><p>En lugar de simplemente devolver una lista de posts relacionados, el modelo de Semantic Kernel buscará los posts más relevantes relacionados con la consulta, y usará esos resultados como contexto para generar una respuesta detallada. El modelo no solo devolverá contenido estático, sino que también incorporará la información más relevante y actualizada disponible en los posts, permitiendo una respuesta más precisa y contextualizada.</p><p>Esta técnica es útil porque, por lo general, los modelos de lenguaje (LLMs) se entrenan con datos estáticos que no contienen información actualizada o específica del dominio. Al integrar la <strong>Recuperación de Información (RAG)</strong>, aprovechamos los datos disponibles en tiempo real para mejorar la relevancia y exactitud de las respuestas generadas.</p><p>En el post anterior, vimos cómo crear  o , y cómo Semantic Kernel se encarga de invocar automáticamente esos plugins cuando es necesario. Ahora, vamos a utilizar un plugin de búsqueda vectorial para recuperar los posts relevantes y luego construir un  que permita al modelo generar una respuesta coherente utilizando esos posts como contexto.</p><p>El endpoint que vamos a implementar es el siguiente:</p><div><pre><code></code></pre></div><ul><li>: Esta clase maneja la lógica de la consulta, invoca el plugin de búsqueda, crea el prompt con el contexto adecuado y genera la respuesta. Recibe un objeto  y una instancia de  que se utiliza para realizar la búsqueda de contenido relevante en los posts del blog.\n</li><li>: Este es el plugin de búsqueda que se crea utilizando el método <strong><code>CreateWithGetTextSearchResults</code></strong>. Este plugin se encarga de realizar la búsqueda semántica en los posts y recuperar los resultados más relevantes. El plugin se añade al  para que pueda ser invocado cuando sea necesario.</li><li>: Es el template que se utilizará para generar el texto. En este caso, se utiliza  para permitir la inserción de los resultados de la búsqueda dentro del prompt. La idea es que el modelo genere una respuesta usando solamente los contenidos encontrados en la búsqueda. El template también incluye instrucciones para que el modelo devuelva el nombre y el valor de los posts utilizados como referencia.\n</li></ul><div><pre><code>{{#with (SearchPlugin-GetTextSearchResults query)}}  \n    {{#each this}}  \n    Name: {{Name}}  \n    Value: {{Value}}  \n    -----------------\n    {{/each}}  \n{{/with}}  \n</code></pre></div><ul><li>Al final, se agrega la pregunta del usuario para que el modelo la utilice como parte de la respuesta:\n</li></ul><ul><li>: Este objeto contiene los argumentos que se pasan al modelo, en este caso, la consulta realizada por el usuario.</li><li>: Este método invoca el modelo utilizando el template y los argumentos proporcionados, lo que genera una respuesta basada en los contenidos recuperados.</li></ul><ol><li><strong>El usuario hace una consulta</strong> (por ejemplo, \"¿Qué es Clean Architecture?\").</li><li>El  busca los posts relevantes utilizando el <strong>plugin de búsqueda vectorial</strong>.</li><li>El  es generado e incluye los resultados de la búsqueda.</li><li>El  genera una respuesta utilizando los posts recuperados como contexto.</li><li>La  se devuelve al usuario, incluyendo la referencia a los contenidos utilizados.</li></ol><p>Este enfoque permite mejorar la calidad de las respuestas generadas por el modelo, ya que se aprovecha la información más actualizada y relevante disponible en el dominio. Además, al usar el contenido de los posts, el modelo tiene un contexto más específico y adaptado a las necesidades del usuario, lo que mejora la relevancia y precisión de la respuesta.</p><p>Una vez que hemos configurado y creado nuestro endpoint, es hora de probarlo. Para ello, podemos hacer una solicitud HTTP como la siguiente, que ilustrará cómo interactuar con el servicio de generación de texto mediante la integración de RAG.</p><p>Ejemplo de solicitud HTTP:</p><div><pre><code>###\n@Question=cuál es el objetivo principal de clean architecture?\nGET {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts/qa?Query={{Question}}\n</code></pre></div><p>Cuando se ejecuta esta solicitud, obtendremos una respuesta estructurada que incluye no solo la respuesta generada por el modelo, sino también la referencia al contenido utilizado para generar esa respuesta.</p><div><pre><code>El&nbsp;objetivo&nbsp;principal&nbsp;de&nbsp;Clean&nbsp;Architecture&nbsp;es&nbsp;promover&nbsp;la&nbsp;separación&nbsp;de&nbsp;preocupaciones&nbsp;y&nbsp;la&nbsp;modularidad&nbsp;en&nbsp;el&nbsp;diseño&nbsp;de&nbsp;software,&nbsp;lo&nbsp;que&nbsp;permite&nbsp;a&nbsp;las&nbsp;aplicaciones&nbsp;ser&nbsp;más&nbsp;mantenibles&nbsp;y&nbsp;escalables&nbsp;a&nbsp;lo&nbsp;largo&nbsp;del&nbsp;tiempo.\n\nReference:&nbsp;Clean&nbsp;Architecture:&nbsp;Diseño&nbsp;de&nbsp;Software&nbsp;Modular&nbsp;y&nbsp;Escalable.\n</code></pre></div><p>En este caso, la respuesta del endpoint es completamente dinámica y se genera utilizando los resultados de búsqueda obtenidos de los posts del blog relacionados con la pregunta. Como puedes observar, no solo obtenemos un resultado que responde a la consulta, sino que también proporcionamos el nombre del contenido utilizado como referencia, lo cual permite al usuario conocer la fuente de la información generada.</p><p>En el ejemplo, configuramos el  para que el modelo de lenguaje solo utilizara los contenidos relevantes encontrados a través de la búsqueda de texto vectorial. De esta forma, garantizamos que la generación del modelo sea específica y relevante para el contexto proporcionado por los posts del blog.</p><p>Este template establece claramente la regla para que el modelo solo use los resultados de búsqueda como contexto, lo cual optimiza la relevancia de las respuestas generadas.</p><p><strong>Escalabilidad y Flexibilidad del Prompt</strong></p><p>El prompt puede ser adaptado para ser más específico y mejorar la calidad de las respuestas generadas, por ejemplo, agregando instrucciones más detalladas sobre el formato de la respuesta o proporcionando una estructura más precisa para guiar al modelo. La flexibilidad del sistema permite ajustar el nivel de detalle según la necesidad del caso de uso.</p><div><pre><code>Enlista los puntos más importantes al querer optimizar consultas SQL\n</code></pre></div><div><pre><code>¡Claro! A continuación, te presento los puntos más importantes para optimizar consultas SQL:\n\n1. **Índices**: Los índices ayudan a acelerar la búsqueda de datos en una base de datos. Deben ser creados en las columnas que se utilizan en la cláusula WHERE o en la cláusula ORDER BY.\n2. **Normalización y desnormalización de datos**: La normalización ayuda a reducir la cantidad de datos duplicados en una base de datos, mientras que la desnormalización puede mejorar el rendimiento en ciertas situaciones.\n.\n.\n.\n\nReferencia: Optimización de Consultas SQL en Aplicaciones .NET\n</code></pre></div><p>Como vemos, el modelo genera una lista de los puntos más importantes y proporciona la referencia adecuada para la información. Este tipo de integración con Semantic Kernel y RAG hace que sea posible tener respuestas altamente relevantes y específicas basadas en contenido actual y relacionado.</p><p>A lo largo de este post, hemos aprendido a configurar y utilizar Semantic Kernel para aprovechar el poder de los embeddings y la técnica de Retrieval-Augmented Generation (RAG). Desde la generación y almacenamiento de vectores en Qdrant, hasta la realización de búsquedas semánticas y la generación de respuestas contextualizadas mediante prompts personalizados, hemos visto cómo combinar distintas tecnologías para crear soluciones de IA avanzadas y especializadas.</p><p>Esta integración no solo mejora la precisión y relevancia de las respuestas, sino que también demuestra el potencial de unir funciones semánticas, RAG y bases de datos vectoriales para abordar desafíos reales en el manejo de información. Con estas herramientas, podrás desarrollar aplicaciones que se adaptan al contexto de tus datos, ofreciendo respuestas dinámicas y actualizadas.</p><p>¡El camino hacia aplicaciones inteligentes y contextualmente precisas está abierto! Sigue experimentando y optimizando estas técnicas para llevar tus proyectos al siguiente nivel.</p>","contentLength":19827,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Java Reflection","url":"https://dev.to/lahiru_rajapakshe_8634adb/java-reflection-141b","date":1739724547,"author":"Lahiru Rajapakshe","guid":685,"unread":true,"content":"<p>Java Reflection is a powerful feature in Java that allows programs to examine and modify the runtime behavior of applications. It enables developers to inspect classes, methods, fields, and constructors dynamically, even if their names are unknown at compile-time. Reflection is part of the  package.</p><h2>\n  \n  \n  Where is Reflection Used?\n</h2><p>Reflection is commonly used in the following scenarios:</p><ul><li>: Used in popular frameworks like Spring, Hibernate, and JUnit to dynamically load and manage components.</li><li>: Frameworks like Spring use reflection to inject dependencies dynamically.</li><li>: Reflection allows access to private fields and methods in unit testing.</li><li><strong>Serialization and Deserialization</strong>: Used in Java Serialization APIs to dynamically inspect object fields.</li><li>: Enables the creation of proxy classes at runtime.</li><li>: Libraries use reflection to process custom annotations at runtime.</li><li>: Used in Java ClassLoader mechanisms to dynamically load and instantiate classes.</li></ul><p>Reflection provides flexibility by allowing:</p><ul><li>: Load and instantiate classes dynamically at runtime.</li><li><strong>Inspection of Class Members</strong>: Retrieve details about methods, fields, and constructors of a class.</li><li>: Call methods dynamically, even private ones.</li><li><strong>Modification of Field Values</strong>: Access and modify private fields.</li><li>: Extract and process annotations at runtime.</li></ul><h2>\n  \n  \n  Disadvantages of Reflection\n</h2><p>While reflection is powerful, it comes with some drawbacks:</p><ul><li>: Reflection operations are slower compared to normal method calls due to additional processing.</li><li>: Bypassing encapsulation can lead to security vulnerabilities.</li><li>: Reflection code can be harder to read and maintain.</li><li><strong>Potential for Runtime Errors</strong>: Issues like  and  can occur if the class structure changes.</li></ul><p>Despite its drawbacks, reflection offers several benefits:</p><ul><li>: Enables working with classes and objects that are unknown at compile-time.</li><li>: Allows the creation of extensible applications.</li><li><strong>Useful for Debugging and Testing</strong>: Enables unit tests to inspect and modify private members.</li><li><strong>Supports Framework Development</strong>: Provides capabilities necessary for building frameworks and libraries.</li></ul><h2>\n  \n  \n  Example of Java Reflection\n</h2><p>The following example demonstrates the use of reflection to inspect a class and invoke its methods dynamically:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Key Classes and Methods in Java Reflection\n</h2><p>Reflection is primarily facilitated by the following classes in :</p><ul><li><code>forName(String className)</code>: Loads a class dynamically.</li><li>: Retrieves all declared fields of a class.</li><li>: Retrieves all declared methods of a class.</li><li><code>getDeclaredConstructors()</code>: Retrieves all constructors of a class.</li></ul><ul><li>: Allows access to private fields.</li><li>: Gets the value of a field.</li><li><code>set(Object obj, Object value)</code>: Sets the value of a field.</li></ul><ul><li><code>invoke(Object obj, Object... args)</code>: Invokes a method dynamically.</li></ul><ul><li><code>newInstance(Object... initargs)</code>: Creates a new instance using a constructor.</li></ul><h2>\n  \n  \n  Best Practices for Using Reflection\n</h2><ul><li>: Use reflection only when necessary, as it has performance and security implications.</li><li>: Reduce overhead by caching method and field lookups.</li><li><strong>Use Access Control Properly</strong>: Be mindful of accessing private members to avoid breaking encapsulation.</li><li><strong>Handle Exceptions Properly</strong>: Always catch and handle exceptions like , , and <code>InvocationTargetException</code>.</li></ul><p>Java Reflection is a powerful feature that enables dynamic inspection and modification of class structures at runtime. While it provides significant flexibility, it should be used carefully due to its performance impact and security risks. Understanding when and how to use reflection effectively is essential for building dynamic and flexible Java applications.</p>","contentLength":3549,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starting My Web Dev Journey!!","url":"https://dev.to/toshal_kirange_0dd326864c/starting-my-web-dev-journey-4mn4","date":1739723860,"author":"Toshal Kirange","guid":684,"unread":true,"content":"<p>Hey everybody!\nso i am a 1st year cse student and just starting web development to prepare for intercollege hackathons and many more.super excited to learn,build and connect with experienced webdevelopers.</p><p>If you're also into hackathons or learning web development, let’s grow together</p>","contentLength":286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Idempotency: A Guide to Reliable System Design","url":"https://dev.to/leapcell/understanding-idempotency-a-guide-to-reliable-system-design-18e3","date":1739723474,"author":"Leapcell","guid":683,"unread":true,"content":"<p>Idempotency is a concept in mathematics and computer science, commonly found in abstract algebra.</p><p>In programming, an idempotent operation is characterized by the fact that performing it multiple times has the same effect as performing it once. An idempotent function or method is one that can be executed repeatedly with the same parameters while yielding the same result. These functions do not alter the system state, and repeated executions do not cause any unintended changes. For example, functions like  and  are idempotent.</p><p>In simple terms: No matter how many times an operation is executed, its effect or return result remains the same.</p><ul><li>When a frontend form is submitted multiple times, the backend should only generate one result.</li><li>When initiating a payment request, the user's account should only be deducted once. Even in cases of network retries or system bugs causing multiple submissions, only one deduction should occur.</li><li>When sending a message, it should only be sent once. If the same SMS is sent multiple times, it can frustrate users.</li><li>When creating a business order, each request should result in the creation of only one order, avoiding duplicate records.</li></ul><p>There are many such scenarios where idempotency is essential.</p><h2>\n  \n  \n  Techniques for Implementing Idempotency\n</h2><p>A query operation, whether executed once or multiple times, returns the same result as long as the underlying data remains unchanged. This makes  queries naturally idempotent.</p><p>Deletion operations are also idempotent. Whether a deletion is executed once or multiple times, the data will be removed. (However, the return value may differ: if the data is absent, the deletion returns 0; if multiple records exist, multiple rows will be affected.)</p><h3>\n  \n  \n  Unique Indexing to Prevent Dirty Data\n</h3><p>For example, in a financial system where each user should have only one financial account, how can we prevent the creation of multiple accounts for a single user?</p><p>By adding a  on the user ID field in the financial account table, only one request will succeed when attempting to create an account. Any subsequent requests will throw a <strong>unique constraint violation error</strong>, such as <code>org.springframework.dao.DuplicateKeyException</code>. The system can then simply query the database again to confirm if the data already exists and return the appropriate result.</p><h3>\n  \n  \n  Token Mechanism to Prevent Duplicate Form Submissions\n</h3><p> The data on a page should only be submitted once.</p><p> Duplicate clicks, network retries, or re-submissions triggered by Nginx retries may cause the same data to be processed multiple times.</p><ul><li><strong>In a clustered environment:</strong> Use a token combined with Redis.</li><li><strong>In a single JVM environment:</strong> Use a token combined with Redis or a token stored in JVM memory.</li></ul><ol><li>Before data submission, request a token from the service, which is stored in Redis or JVM memory with a validity period.</li><li>Upon submission, the backend verifies the token, deletes it, and generates a new token for the next request.</li></ol><ul><li>Must be requested before submission.</li><li>Can act as a rate-limiting mechanism.</li></ul><p> Redis should use a  to verify the token. If deletion succeeds, the token is considered valid. Using  can cause concurrency issues and is not recommended.</p><p>Acquiring a lock when retrieving data:</p><div><pre><code></code></pre></div><p> The  field must be a  or a ; otherwise, the query will lock the entire table, which can lead to performance issues.</p><p>Pessimistic locking is generally used with , and the data remains locked for the duration of the transaction. The longer the lock is held, the greater the potential impact on performance, so use it based on specific requirements.</p><p>Optimistic locking only locks data at the moment of an update, avoiding unnecessary locking at other times. Compared to pessimistic locking, it offers .</p><p>There are multiple ways to implement optimistic locking, such as using a  or <strong>condition-based constraints</strong>:</p><div><pre><code></code></pre></div><ol><li><strong>Condition-based approach:</strong></li></ol><div><pre><code></code></pre></div><ul><li>The condition <code>avai_amount - subAmount &gt;= 0</code> ensures safe updates without requiring a version number.</li><li>This is ideal for inventory models where stock is deducted and rolled back as needed.</li><li>It provides better performance.</li></ul><ul><li><strong>Use primary keys or unique indexes</strong> in  queries to avoid table locks. The queries above should be modified as follows:\n</li></ul><div><pre><code></code></pre></div><p>When inserting data in a , it may be difficult to enforce a globally unique index (e.g., due to the lack of a universal unique key across distributed nodes).</p><p>In such cases, a  can be implemented using  to manage concurrent data insertion or updates. The system acquires a lock, performs the operation, and then releases the lock. This approach helps <strong>prevent concurrent writes</strong> and is a common solution in distributed systems.</p><p>For backend systems with  or scheduled job tasks,  can be ensured by checking whether the operation has already been executed before proceeding.</p><ol><li>First, query the database for critical data to determine whether the operation has already been executed.</li><li>If it has not been executed, proceed with processing.</li></ol><p> Avoid using this approach in high-concurrency scenarios.</p><h3>\n  \n  \n  State Machine Idempotency\n</h3><p>In business applications involving  or , state transitions must be carefully managed.</p><ul><li>Business records usually have a , and transitions occur based on predefined .</li><li>If a request attempts to transition from an  to an already , it should be rejected.</li><li>This ensures  in state transitions.</li></ul><p> For , where states evolve over time, a strong understanding of state machines is essential to designing robust business systems.</p><h2>\n  \n  \n  Ensuring Idempotency in API Calls\n</h2><p>For example, a bank providing a  requires merchants to include two fields in their request:</p><ul><li>: The origin of the request.</li><li>: A unique sequence number.</li></ul><p>By enforcing a  on  in the database, duplicate payments can be prevented, ensuring that <strong>only one request is processed in concurrent scenarios</strong>.</p><p>To support idempotency in external APIs:</p><ol><li> and .</li><li><strong>Enforce a unique constraint</strong> on these fields in the service provider’s system.</li><li><strong>Before processing a request, check if it has already been handled.</strong><ul><li>If it has been processed, return the existing result.</li><li>If it has not been processed, proceed with execution.</li></ul></li></ol><p> Always query whether a request has been processed before inserting new records. Directly inserting data without a check can result in errors, even though the operation may have already succeeded.</p><p>Idempotency is a fundamental principle of good software design. It is a critical consideration when designing , especially in industries like <strong>third-party payment platforms, banking, and fintech</strong>, where accuracy is paramount. Issues like <strong>duplicate deductions or multiple payouts</strong> can be extremely difficult to rectify and significantly impact user experience.</p><p>By implementing , developers can build robust, fault-tolerant systems that ensure <strong>consistency, correctness, and a seamless user experience</strong>.</p><p><a href=\"https://leapcell.io/?lc_t=d_idempotency\" rel=\"noopener noreferrer\">Leapcell</a> is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:</p><ul><li>Develop with Node.js, Python, Go, or Rust.</li></ul><p><strong>Deploy unlimited projects for free</strong></p><ul><li>pay only for usage — no requests, no charges.</li></ul><p><strong>Unbeatable Cost Efficiency</strong></p><ul><li>Pay-as-you-go with no idle charges.</li><li>Example: $25 supports 6.94M requests at a 60ms average response time.</li></ul><p><strong>Streamlined Developer Experience</strong></p><ul><li>Intuitive UI for effortless setup.</li><li>Fully automated CI/CD pipelines and GitOps integration.</li><li>Real-time metrics and logging for actionable insights.</li></ul><p><strong>Effortless Scalability and High Performance</strong></p><ul><li>Auto-scaling to handle high concurrency with ease.</li><li>Zero operational overhead — just focus on building.</li></ul>","contentLength":7372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Secure your React app with Firebase Auth","url":"https://dev.to/debajit13/secure-your-react-app-with-firebase-auth-4a6o","date":1739723046,"author":"Debajit Mallick","guid":682,"unread":true,"content":"<p>Authentication is one of the most important features that needs to be implemented in most of the apps. Firebase Authentication makes it easy to manage user authentication in your React applications. In this blog, we will set up Firebase authentication in a , using <strong>Email/Password Authentication</strong> and . Once you understand those 2 you can also try with other sign-in methods as they are mostly the same. To demonstrate, we will integrate this authentication setup into a simple .</p><h2>\n  \n  \n  Step 1: Setting Up a React Project with Vite\n</h2><p>First, create a new React project using Vite:</p><div><pre><code>npm create vite@latest todo-app  react\ntodo-app\nnpm </code></pre></div><h2>\n  \n  \n  Step 2: Setting Up Firebase\n</h2><ol><li>Click  and follow the setup steps.</li><li>In the  section, enable:\n\n<ul><li><strong>Email/Password Authentication</strong></li></ul></li><li>In , under the  tab, find your Firebase SDK configuration. Copy the  object.</li></ol><h2>\n  \n  \n  Step 3: Configuring Firebase in React\n</h2><p>Create a new file  and add the following:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 4: Creating Authentication Components\n</h2><p>Create a  file:</p><div><pre><code></code></pre></div><p>Create a  file:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 5: Protecting the Todo App\n</h2><p>Now, integrate authentication into the Todo App. Create a  file:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Step 6: Creating the Todo App\n</h2><p>Create <code>src/components/TodoApp.js</code>:</p><div><pre><code></code></pre></div><p>Now, as you have successfully integrated  into a  with <strong>Email/Password Authentication</strong> and . You can now protect your app's features based on user authentication! You can take the project a step further and implement role-based authentication, and public and protected routes based on user authentication. If you like this blog and want to learn more about  and , you can follow me on <a href=\"https://dev.to/debajit13/\">Dev.to</a>.</p>","contentLength":1567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Ever wondered why some systems never fail while others crash at the worst moments?","url":"https://dev.to/oleksandr_kashytskyi_a630/ever-wondered-why-some-systems-never-fail-while-others-crash-at-the-worst-moments-discover-1lb9","date":1739723018,"author":"Oleksandr Kashytskyi","guid":681,"unread":true,"content":"<h2>Reliability in Data-Intensive Applications</h2><h3>Oleksandr Kashytskyi ・ Feb 16</h3>","contentLength":73,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reliability in Data-Intensive Applications","url":"https://dev.to/oleksandr_kashytskyi_a630/reliability-in-data-intensive-applications-23l6","date":1739722872,"author":"Oleksandr Kashytskyi","guid":680,"unread":true,"content":"<ol><li>\nTypes of Faults in Data-Intensive Systems\n\n<ul></ul></li><li>\nVisualizing Reliability in Systems\n\n<ul></ul></li></ol><p>Data-intensive applications differ from compute-intensive ones by relying heavily on data storage, processing, and retrieval rather than raw computational power. These applications are typically built from standard building blocks, such as databases, caches, messaging systems, and distributed storage.</p><p>Beyond databases, maintaining a data-intensive system requires a suite of other tools to ensure reliability, performance, and fault tolerance.\nWhat is Reliability?</p><p>A system is considered reliable if it:</p><ul><li>Performs its intended function correctly as expected by the user.</li><li>Can tolerate user mistakes without severe failures.</li><li>Maintains good enough performance for the required use case.</li><li>Prevents unauthorized access to sensitive data.</li></ul><p>Reliability is closely related to  — the system’s ability to continue functioning despite faults.</p><ul><li><p>A  occurs when a component stops working (e.g., a database node crashes).</p></li><li><p>A  happens when the system as a whole can no longer function correctly.</p></li></ul><h2>\n  \n  \n  Types of Faults in Data-Intensive Systems </h2><p>Hardware failures include disk crashes, memory corruption, and power outages.</p><p>Modern distributed systems can tolerate hardware faults through redundancy and failover mechanisms (RAID for storage, replication for databases e.t.c.).</p><p>Software errors are trickier to handle than hardware faults. They can be caused by:</p><ul><li>Crashes due to bad input or unhandled edge cases.</li><li>A runaway process consuming all system resources.</li><li>Failures in external services that the system depends on.</li><li>, where a small failure triggers larger system-wide outages.</li></ul><p>To mitigate software errors:</p><ul><li>Implement robust error handling and graceful degradation.</li><li>Use circuit breakers and retry mechanisms.</li><li>Employ canary releases and feature flags to minimize blast radius.</li></ul><p>Studies show that only 10-25% of outages are due to server or network faults, meaning human errors are a major contributor to system failures.</p><p>Strategies to reduce human-induced faults:</p><ol><li> – Make critical operations harder to break.</li><li><strong>Decouple risky operations</strong> – Separate places where humans interact most.</li><li> – Include unit, integration, and system-level tests.</li><li> – Provide rollback mechanisms and automated recovery.</li><li><strong>Detailed monitoring and alerting</strong> – Detect anomalies early.</li><li><strong>Training and process improvement</strong> – Foster good management practices and continuous learning.</li></ol><h2>\n  \n  \n  Visualizing Reliability in Systems </h2><p>A well-architected system uses fault isolation to prevent one failing component from bringing down the entire system.</p><ul><li>Load balancer ensures traffic is distributed evenly.</li><li>Circuit breakers prevent overload from failed services.</li><li>Caching layers reduce direct dependencies on databases.</li></ul><p>A good monitoring and alerting system is essential:</p><ul><li>Logs, metrics, and tracing should be unified for quick debugging.</li><li>Real-time dashboards help detect anomalies.</li><li>Automated alerts ensure rapid response to incidents.</li></ul><p>Reliability is a key aspect of data-intensive applications. Achieving it requires:</p><ul><li><strong>Understanding and mitigating different types of faults</strong> (hardware, software, and human errors).</li><li><strong>Designing systems with resilience in mind</strong> (e.g., fault isolation, circuit breakers, failover strategies).</li><li><strong>Implementing strong observability tools</strong> (Sentry, AWS CloudWatch) to detect and resolve issues quickly.</li></ul><p>By following these principles, data-intensive applications can achieve high availability, fault tolerance, and consistent performance, ensuring a smooth experience for users.</p>","contentLength":3482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Cloud-Based APIs Are Making GPT and Other AI Models More Accessible for Businesses and Developers Worldwide","url":"https://dev.to/marufhossain/how-cloud-based-apis-are-making-gpt-and-other-ai-models-more-accessible-for-businesses-and-g0p","date":1739720676,"author":"Maruf Hossain","guid":679,"unread":true,"content":"<p>Artificial intelligence is no longer a tool only for tech giants. Cloud computing makes AI models like GPT available to businesses, startups, and developers across the world. Cloud-based APIs allow companies to integrate AI into their services without building complex systems from scratch. This transformation helps businesses improve customer interactions, automate tasks, and increase efficiency.  </p><h3><strong>APIs Bridge the Gap Between AI and Businesses</strong></h3><p>Many companies want to use AI, but building an AI model from the ground up requires time, money, and expertise. Cloud-based APIs solve this problem by offering ready-to-use AI models like GPT. Developers connect their applications to these APIs and instantly access AI-powered features.  </p><p>Businesses integrate AI into chatbots, virtual assistants, and customer support systems. Instead of training AI models themselves, they rely on cloud-based solutions that provide instant responses and updates.  </p><h3><strong>Cloud-Based APIs Save Time and Resources</strong></h3><p>Training an AI model demands powerful hardware and extensive datasets. Small businesses cannot afford these resources. Cloud APIs eliminate this problem by offering pre-trained AI models that perform complex tasks without extra effort.  </p><p>A company that wants AI-powered customer service does not need to build its own chatbot. With cloud-based AI services, businesses simply connect their platforms to GPT and start using AI-generated responses immediately. This setup reduces costs and allows businesses to focus on their core services.  </p><h3><strong>Developers Gain More Flexibility and Scalability</strong></h3><p>Cloud computing enables developers to scale their AI usage based on demand. A small startup can begin with minimal AI integration and expand as customer interactions grow. Cloud APIs adjust to business needs, ensuring smooth performance even during peak usage times.  </p><p>A mobile app using GPT-based AI for text generation can start with a basic plan. As more users adopt the app, developers increase API usage without modifying their infrastructure. This flexibility helps businesses grow without worrying about technical limitations.  </p><h3><strong>AI Integration Becomes Easier Across Multiple Platforms</strong></h3><p>Cloud-based AI services work across different platforms. Businesses use AI-powered chatbots on websites, mobile apps, and messaging services. Since AI operates through cloud servers, users get seamless interactions on any device.  </p><p>A company using AI for customer support can offer the same AI-driven experience on its website, Facebook Messenger, and mobile app. This consistency improves customer satisfaction and strengthens brand trust.  </p><h3><strong>Cloud-Based AI Improves Data Processing and Insights</strong></h3><p>Businesses rely on AI to analyze customer behavior, automate reports, and provide insights. Cloud-based AI models process data faster, allowing companies to make informed decisions quickly.  </p><p>For example, an e-commerce store using AI for product recommendations can analyze customer preferences in real time. GPT-powered AI suggests personalized products, increasing sales and customer engagement. Businesses gain a competitive edge by offering smarter services.  </p><h3><strong>Security and Reliability in AI-Powered APIs</strong></h3><p>Companies using cloud-based AI need reliable security. Cloud providers offer encryption and data protection, ensuring that AI models process customer interactions safely. Businesses trust cloud services to handle sensitive information without risks.  </p><p>Reliability is another key factor. AI-powered chatbots and virtual assistants need to be available at all times. Cloud computing ensures these services run smoothly without downtime. Even if one server experiences an issue, others take over to maintain seamless performance.  </p><h3><strong>Businesses Compare Cloud and AI Models for Best Results</strong></h3><p>Companies exploring AI often compare different models and services. Many want to understand how <a href=\"https://www.clickittech.com/ai/claude-vs-gpt/?utm_source=backlinks&amp;utm_id=referral\" rel=\"noopener noreferrer\">cloude vs gpt</a> performs when integrated into cloud environments. Businesses analyzing these comparisons can decide which AI solutions fit their needs best.  </p><h3><strong>Future of AI-Powered Cloud Services</strong></h3><p>Cloud-based APIs will continue to expand AI accessibility. More businesses will integrate AI into customer interactions, automation, and content creation. Cloud providers will improve AI models, offering faster processing and smarter solutions.  </p><p>Small businesses and developers will benefit the most from these advancements. With cloud-powered AI, companies can offer high-tech services without large investments. The future of AI belongs to businesses that embrace cloud-based solutions, making AI a key part of their operations.  </p>","contentLength":4560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🔥Best alternative to HTMX","url":"https://dev.to/hmpljs/best-alternative-to-htmx-35j7","date":1739720088,"author":"Anthony Max","guid":666,"unread":true,"content":"<p>Hello everyone! In this article we will consider such a javascript module as <a href=\"https://github.com/hmpl-language/hmpl\" rel=\"noopener noreferrer\">HMPL</a> and how it can replace <a href=\"https://github.com/bigskysoftware/htmx\" rel=\"noopener noreferrer\">HTMX</a> in a project. Also consider their differences, advantages and disadvantages.</p><p>When further comparing the two modules, it is worth considering that one is a template language, while the other is a set of tools for working with HTML, implemented through attributes and more.</p><p>Let's start with the general concept for the two modules.</p><h2>\n  \n  \n  🌐 About server-side rendering and client-side rendering\n</h2><p>The HMPL module is similar in concept to HTMX. We can also take HTML from the server via API, thus replacing modern frameworks and libraries for creating UI. Let's take a small example illustrating the work of HMPL and HTMX, as well as a framework such as Vue.js:</p><div><pre><code></code></pre></div><p><em>Size: 226 bytes (4KB on disk)</em></p><div><pre><code></code></pre></div><p><em>Size: 209 bytes (4KB on disk)</em></p><div><pre><code>Click!Clicks: 0</code></pre></div><p>Using a simple clicker as an example, we can see (with some caveats regarding server-side and client-side data, as well as html and js markup, but that's not the main idea) that we get the same interface, although the file sizes on the client will be completely different. This is precisely the main advantage of the approach to creating a ready-made, or template UI component on the server side, so that the site user can load it faster while preserving the result.</p><p>Now, let's remember how large applications today, well, or at least earlier (when server-side rendering was not so popular), could be obtained using frameworks and libraries. The same SPA (Single Page Application) generates all content via javascript, when in html we have literally one line, but the joke is that with 10 kilobyte html we get a javascript file of several tens of megabytes. Such a site, the first time users visit it, can take a long time to load.</p><p>For example, if a potential client wants to quickly order flowers, he will not wait 10-15 seconds for the delivery store website to load, he will go to another website where the website will load faster.</p><p>There are many more practical examples of how websites work that can influence the sales funnel. But the point is that the main thing is the speed and convenience of the interface, and here there are already differences in approaches. But it is better to do this in a separate article. Here we also consider a comparison of HMPL and HTMX.</p><h2>\n  \n  \n  👀 Why use HMPL and what are its advantages over HTMX?\n</h2><p>In this section I will try to tell you about several main reasons why you may choose HMPL instead of HTMX in some cases. Also, if you want to support the projects, you can give a star to all two modules! Thank you ❤️!</p><p><strong>1. With a similar idea of ​​reducing code, the two modules differ in concepts.</strong> In the case of HTMX, on the one hand, we get a convenient tool for working with an existing DOM, but on the other, all this happens through HTML and is updated literally in real time. With great difficulty, through non-standard solutions, we can work more or less through javascript, and in fact, working with javascript is almost completely absent. In the case of HMPL, on the one hand, we can easily work with javascript; generate a custom RequestInit, create thousands of separate DOM nodes with the same server UI support as on HTMX, but on the other - all the work is done with code, which is not always convenient when you want to create projects faster. Let's take an example of code:</p><div><pre><code></code></pre></div><div><pre><code>Enter your email: </code></pre></div><p>This example clearly shows that HTMX is more about maximizing the speed and shortening of code, while HMPL is something combined between HTMX and a modern framework or library for creating UI. We can say that we get a somewhat similar result, but taking into account that we can customize the request to the server. This is very important, because customization of the request in conjunction with fetch and work in javascript will allow you to work with a microfrontend, or in conjunction with another framework, or even with tests.</p><p><strong>2. The HMPL syntax is also an advantage in its own way, because the request objects are not tied to any tag.</strong> When rendering, they are replaced with comments that do not clutter the DOM with unnecessary tags. Example syntax:</p><div><pre><code>some text {{ src: \"/api/getSomeText\" }} some text</code></pre></div><div><pre><code>some text  some text</code></pre></div><p>In some cases, it is not possible to assign an attribute to achieve the minimum file size with just short tags like p or s. Sometimes, you will have to use the template tag in the same table, and it, in turn, takes up a lot of characters in the file. In the hmpl syntax, there are always single curly brackets and then an object.</p><p><strong>3. HMPL is built entirely on  requests, which were introduced as a standard in 2015.</strong> HTMX, for IE13 support, uses  by default, which was introduced in 1999 in IE and 2000 in Mozilla Gecko with Netscape Navigator. The  function allows you to use modern javascript features in browsers, such as , signals, and more.</p><p>And, there are still a fair number of advantages, like a separate .hmpl file extension when working with webpack and others, but in my opinion, those that I highlighted are the most important. Webpack config example:</p><div><pre><code></code></pre></div><p>Also, hmpl has a number of disadvantages that I would like to talk about:</p><p><strong>1. HMPL does not yet support WebSockets, which may complicate the implementation of code into the project.</strong> In HTMX, this support is present.</p><p><strong>2. Since fetch is used, the layout will not be supported in some older browser versions.</strong></p><p><strong>3. HMPL is a new module, it can sometimes have bugs. HTMX, on the contrary, is tested due to its widespread use.</strong></p><p>The HMPL template language can replace HTMX in cases where flexible request customization is required, as well as direct work with nodes via JavaScript; If you, for example, want to create a cycle of 1000 identical nodes, and at the same time have the advantages of a server-oriented UI, then it will also suit the task. If the goal is to completely minimize work with JavaScript, or to use an established tested module and a simple connection to the server with a minimum amount of HTML code, then HTMX is good here.</p><p><strong>Thank you all for reading this article!</strong></p>","contentLength":6042,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Here's a step-by-step guide to create a Linux Virtual Machine on Azure, SSH into it, and install NGINX:","url":"https://dev.to/chifum/heres-a-step-by-step-guide-to-create-a-linux-virtual-machine-on-azure-ssh-into-it-and-install-5a0n","date":1739719861,"author":"chifum","guid":665,"unread":true,"content":"<p>This article will walk you through creating a Linux Virtual Machine, connecting to it via SSH, and installing NGINX on it.</p><ul><li>Log in using your Azure account credentials.</li></ul><h4>\n  \n  \n  2. <strong>Search for Virtual Machines</strong></h4><ul><li>In the Azure portal, type  in the search bar and select  from the results.</li></ul><ul><li>On the  page, click on the  button to start creating a virtual machine.</li></ul><h4>\n  \n  \n  4. <strong>Select Azure Virtual Machine</strong></h4><ul><li>Choose the  option to proceed with setting up your virtual machine.</li></ul><ul><li>Under , go to the  field.</li><li>Click on  and provide a unique name for the resource group. You can use a special name or the name of your project to make it easily identifiable.</li><li>Click  after entering the name to save it. </li></ul><h4>\n  \n  \n  6. </h4><ul><li>Under the  section, provide a name for your virtual machine.\n</li><li>Choose a name that reflects the purpose of the VM or your project. For example:  or . </li></ul><ul><li>In the  dropdown menu, select the region where you want your virtual machine to be hosted.</li><li>Choose a location close to you or your target users to reduce latency and improve performance. For example: , , or .</li></ul><h4>\n  \n  \n  8. <strong>Select Availability Zones</strong></h4><ul><li>In the  section, choose the  based on how highly available you want your virtual machine to be:\n\n<ul><li>: Select these if you want your VM to be .</li><li><strong>Zone 1, Zone 2, and Zone 3</strong>: Select these if you want your VM to be .\n<em>(Keep in mind that higher availability increases the cost.)</em></li></ul></li></ul><p>Choose the option that best aligns with your budget and project requirements. <a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fj5epkhi3r4jdy8cymy5m.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fj5epkhi3r4jdy8cymy5m.png\" alt=\"Image description\" width=\"800\" height=\"373\"></a></p><h4>\n  \n  \n  10. <strong>Set Administrator Account</strong></h4><ul><li>Under the  section:\n\n<ul><li>For , select .\n</li><li>Enter a  and a  of your choice.\n(Ensure the password is strong and meets Azure's security requirements.)</li></ul></li></ul><p>Using a password for authentication allows you to access your virtual machine directly without needing an SSH public key.<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1i6mywp31c050ga6nyn1.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F1i6mywp31c050ga6nyn1.png\" alt=\"Image description\" width=\"800\" height=\"373\"></a></p><h4>\n  \n  \n  11. <strong>Create a Username and Password</strong></h4><ul><li>Under the  section:\n\n<ul><li>Select  to allow web traffic.\n</li><li>Select  to enable secure remote access to your virtual machine.</li></ul></li></ul><p>This configuration ensures that you can access your VM via SSH and view the web page hosted on it.</p><h4>\n  \n  \n  13. </h4><ul><li>Navigate to the  tab.</li><li>Locate the  option and set it to .</li></ul><h4>\n  \n  \n  16. <strong>Create the Virtual Machine</strong></h4><h4>\n  \n  \n  18. <strong>Increase Idle Timeout for the IP Address</strong></h4><h4>\n  \n  \n  19. <strong>Connect to the Linux Virtual Machine via SSH</strong></h4><ul><li><p>Open the  application on your Windows laptop.</p></li><li><p>Type the following SSH command, replacing  and  with your VM's details:</p></li></ul><div><pre><code>  ssh mylinuxvm@123.45.67.89\n</code></pre></div><p> There should be no spaces between the VM name and the IP address.</p><ul><li>When prompted, type  to confirm that you trust the remote host and want to continue.</li></ul><ul><li><p>After successfully connecting to the Linux VM, you will see a message instructing that administrative commands require  privileges.</p></li><li><p>To switch to the root user, type the following command and press :</p></li></ul><ul><li>Now that you are connected as a root user, you can install NGINX by typing the following command:\n</li></ul><ul><li>Press  to execute the command.</li></ul><h4>\n  \n  \n  24. <strong>Follow Installation Prompts</strong></h4><h4>\n  \n  \n  25. <strong>Verify NGINX Installation</strong></h4><ul><li>Go to your Azure portal and navigate to your virtual machine's  page.</li><li>Locate the  of your virtual machine.</li></ul><ul><li>Open a web browser on your local machine.</li><li>Paste the  into the browser’s address bar and press .</li></ul>","contentLength":3088,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dicas Rápidas: Como Ativar o Spring Boot DevTools no IntelliJ IDEA","url":"https://dev.to/diegobrandao/dicas-rapidas-como-ativar-o-spring-boot-devtools-no-intellij-idea-1agg","date":1739719739,"author":"Diego de Sousa Brandão","guid":664,"unread":true,"content":"<p>O Spring Boot DevTools é uma ferramenta essencial para acelerar o desenvolvimento de aplicações Spring Boot, permitindo recarregamento automático da aplicação sempre que alterações no código são feitas. Neste guia rápido, vamos mostrar como ativá-lo corretamente no IntelliJ IDEA.</p><p>Passo 1: Adicionar a Dependência do DevTools</p><p>Para utilizar o DevTools, primeiro é necessário adicioná-lo ao projeto. Se você usa Maven, adicione a seguinte dependência no pom.xml:</p><div><pre><code>        &lt;dependency&gt;\n            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n            &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;\n            &lt;scope&gt;runtime&lt;/scope&gt;\n            &lt;optional&gt;true&lt;/optional&gt;\n        &lt;/dependency&gt;\n</code></pre></div><p>Se você utiliza Gradle, adicione a seguinte linha no build.gradle:</p><div><pre><code>dependencies {\n    developmentOnly 'org.springframework.boot:spring-boot-devtools'\n}\n</code></pre></div><p>Isso garantirá que o DevTools seja incluído apenas no ambiente de desenvolvimento.</p><p>Passo 2: Habilitar a Compilação Automática no IntelliJ IDEA</p><p>Para que o DevTools funcione corretamente, é necessário ativar a compilação automática no IntelliJ IDEA:</p><p>Acesse File &gt; Settings &gt; Build, Execution, Deployment &gt; Compiler.</p><p>Marque a opção \"Build project automatically\".</p><p>Permitir auto-make mesmo quando a aplicação está em execução:\nVá em File &gt; Settings &gt; Advanced Settings.</p><p>Marque a opção Allow auto-make to start even if developed application is currently running. </p><p><em>Não se esqueça de clicar em \"Apply\" e \"Ok\".</em></p><p><strong>Por ultimo uma dica final, ele demora de meio segundo até 2 segundos para refletir a mudança, caso queira ver na mesma hora, utilize o comando (CTRL + F9).</strong></p><p>Essas configurações permitirão que o IntelliJ compile automaticamente as alterações, acionando o reinício automático da aplicação pelo DevTools.</p>","contentLength":1784,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TypeScript Utility Types Every React Developer Should Know","url":"https://dev.to/byte-sized-news/typescript-utility-types-every-react-developer-should-know-40g5","date":1739719632,"author":"Ishan Bagchi","guid":663,"unread":true,"content":"<p>As React developers, we've all been in those moments where managing props, state, or any dynamic data structures becomes slightly... \"messy.\" Enter TypeScript utility types—your productivity's unsung heroes! They’re the Swiss Army knife of TypeScript, designed to make our lives easier by enabling powerful transformations on existing types. Let me show you how these utilities can make your React code cleaner, safer, and, yes, more enjoyable to write.</p><h2>\n  \n  \n  1. <strong>: Make Everything Optional</strong></h2><p>Picture this: You’re building a component, and you’d like to create a draft version of its props object where not all fields are required.  has your back.</p><div><pre><code></code></pre></div><p>By wrapping  with , we’ve made all its properties optional. This is particularly useful for scenarios like form initialization, mocking, or default props.</p><h2>\n  \n  \n  2. <strong>: Cherry-Pick What You Need</strong></h2><p>Sometimes, you’re working with a large type but only care about a subset of its properties. Enter  to save the day.</p><div><pre><code></code></pre></div><p>Here, we created a component that only needs the  and  fields from the  interface. This keeps our prop definitions clean and avoids unnecessary data leakage.</p><h2>\n  \n  \n  3. <strong>: Exclude What You Don’t Need</strong></h2><p>What if you want the opposite of ? Maybe you have a type but need to exclude a specific property. That’s where  shines.</p><div><pre><code>Close</code></pre></div><p>With , we’ve excluded the  property. This utility is super handy when dealing with inherited props where certain fields aren’t relevant for a specific component.</p><h2>\n  \n  \n  4. <strong>: A Type-Safe Key-Value Map</strong></h2><p>Imagine you’re building a theme system, and you need a key-value pair for different color tokens.  makes this simple and type-safe.</p><div><pre><code></code></pre></div><p>With , you define both the keys and their value types. It’s great for mapping enums or predefined constants.</p><h2>\n  \n  \n  5. <strong>: Enforce Every Property</strong></h2><p>We’ve all had that moment where optional properties suddenly need to become required. That’s where  steps in.</p><div><pre><code></code></pre></div><p> transforms all optional properties into mandatory ones, ensuring you’ll never miss a critical field.</p><p>These utility types are more than just tools; they’re the cheat codes to cleaner, more maintainable React code. They reduce boilerplate, minimize type errors, and make your life as a developer a little less stressful.</p><p>Next time you find yourself typing out repetitive or verbose type definitions, pause for a moment and ask: <em>Can a utility type simplify this?</em> Chances are, it can.</p><p>What’s your favorite TypeScript utility type? Let me know in the comments—or better yet, tweet it out! Let’s share the knowledge and keep our React codebases as elegant as they deserve to be.</p>","contentLength":2567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WebSocket vs. MQTT: Which One Should You Use in Your IoT Project?","url":"https://dev.to/atakanatici/websocket-vs-mqtt-which-one-should-you-use-in-your-iot-project-4dbn","date":1739718977,"author":"Atakan ATICI","guid":662,"unread":true,"content":"<p><strong>WebSocket vs. MQTT: Which One Should You Use in Your IoT Project?</strong></p><p>In the world of IoT, efficient and reliable communication between devices is crucial. Choosing the right protocol can make a significant difference in performance, scalability, and power consumption. Two popular protocols for IoT communication are  and . But which one should you use? Let's break it down.</p><h2>\n  \n  \n  🚀 WebSocket: Real-Time, Low-Latency Communication\n</h2><p>WebSocket is a full-duplex communication protocol that allows for continuous data exchange between a client and a server. It is widely used in applications that require real-time updates, such as:</p><ul><li> displaying sensor data</li><li><strong>Remote control applications</strong> where instant response is needed</li><li> and interactive systems</li></ul><h3>\n  \n  \n  🔹 Key Advantages of WebSocket\n</h3><ul><li> Provides an always-open connection for instant data transfer.</li><li><strong>Efficient for bidirectional communication:</strong> Ideal for real-time applications.</li><li><strong>Works well in high-bandwidth environments:</strong> When network conditions are stable, WebSocket performs exceptionally.</li></ul><ul><li><strong>Higher power consumption:</strong> Requires a continuous connection, which may drain battery-powered IoT devices.</li><li><strong>Not optimized for intermittent connections:</strong> If devices frequently disconnect, maintaining WebSocket connections can be challenging.</li><li> Unlike MQTT, WebSocket does not have built-in message persistence.</li></ul><h2>\n  \n  \n  🌍 MQTT: Lightweight and Reliable IoT Messaging\n</h2><p>MQTT (Message Queuing Telemetry Transport) is a lightweight messaging protocol designed for low-power devices and unreliable networks. It is commonly used in:</p><ul><li> that send periodic updates</li><li><strong>Smart home automation systems</strong></li><li> where power efficiency is critical</li></ul><ul><li> Designed for constrained networks with minimal data overhead.</li><li> Ideal for battery-powered IoT devices.</li><li><strong>Supports Quality of Service (QoS) levels:</strong> Ensures message delivery even in unstable network conditions.</li><li> MQTT brokers can store and forward messages for offline clients.</li></ul><ul><li><strong>Higher latency than WebSocket:</strong> Due to message queuing and broker-based architecture.</li><li><strong>Not designed for bidirectional real-time communication:</strong> Best suited for event-driven messaging rather than instant interactions.</li><li> Adds an extra layer to the architecture, whereas WebSocket can communicate directly with servers.</li></ul><h2>\n  \n  \n  🔥 When to Use WebSocket vs. MQTT?\n</h2><div><table><tbody><tr><td>Real-time interaction &amp; updates</td><td>Periodic sensor data, IoT automation</td></tr><tr><td>Slightly higher due to message queuing</td></tr><tr><td>Broker-based, publish/subscribe</td></tr><tr><td>Dashboards, control apps, chat</td><td>Smart homes, telemetry, low-power IoT</td></tr></tbody></table></div><h2>\n  \n  \n  🏆 Conclusion: Choosing the Right Protocol\n</h2><ul><li>Use  if your IoT project requires real-time, bidirectional communication with minimal latency.</li><li>Use  if your devices need low-power, intermittent connections with reliable message delivery.</li><li>In some cases, a  combining both protocols can be beneficial, such as using WebSocket for instant updates and MQTT for background data transmission.</li></ul><p>Each protocol has its strengths and weaknesses, and the best choice depends on the specific needs of your IoT application. If your goal is , WebSocket is the way to go. If <strong>energy efficiency and message reliability</strong> are your priorities, MQTT will serve you better.</p><p>What protocol are you using in your IoT projects? Share your experience! 🚀</p>","contentLength":3193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Invariant Violation: __fbBatchedBridgeConfig is not set, cannot invoke native modules","url":"https://dev.to/laf523/invariant-violation-fbbatchedbridgeconfig-is-not-set-cannot-invoke-native-modules-443a","date":1739718957,"author":"李奥飞","guid":661,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Getting Started with Flutter Modular: My Journey to a More Organized Codebase","url":"https://dev.to/thinnakrit/getting-started-with-flutter-modular-my-journey-to-a-more-organized-codebase-3pbb","date":1739718872,"author":"Thinnakrit","guid":660,"unread":true,"content":"<p>Hey there, Flutter devs! 👋 Ever found yourself drowning in a spaghetti mess of routes and dependencies? Yeah, me too. When my Flutter project started growing, managing navigation and dependencies became a nightmare. That’s when I stumbled upon Flutter Modular, and let me tell you—it was a game changer! 🚀</p><p>In this article, I'll share my experience with Flutter Modular, why I love it, and how you can get started with it too. Let’s dive in!</p><ol><li>Why Did I Start Using It?</li><li>Setting Up Flutter Modular</li><li>Breaking Down Modules and Routes</li><li>Smooth Navigation Between Screens</li></ol><h2>\n  \n  \n  1. What is Flutter Modular?\n</h2><p>Think of Flutter Modular as your app’s superpower that keeps things tidy. It helps you organize your project by modularizing routes and dependencies. Instead of cluttering your main.dart with a ton of navigation logic, Flutter Modular keeps it clean and structured.</p><h2>\n  \n  \n  2. Why Did I Start Using It?\n</h2><p>Okay, confession time—I used to hardcode my routes and manually pass dependencies everywhere. It worked for small projects, but as my app grew, maintaining it felt like playing Jenga with my sanity. 🤯</p><p>Then, I came across Flutter Modular, and suddenly, everything clicked:</p><ul><li>No more scattered route management 🎯</li><li>Dependencies are injected where needed 🔥</li><li>My project became way easier to scale 📈</li></ul><h2>\n  \n  \n  3. Setting Up Flutter Modular\n</h2><div><pre><code></code></pre></div><h2>\n  \n  \n  Boom! You’re ready to roll. 🎉\n</h2><h2>\n  \n  \n  4. Breaking Down Modules and Routes\n</h2><p>In Flutter Modular, you create modules to structure your app. Here's how I structured mine:</p><div><pre><code></code></pre></div><p>This makes it super easy to add or update routes without touching your main widget. Love it! ❤️</p><h2>\n  \n  \n  5. Smooth Navigation Between Screens\n</h2><p>Navigation used to be a mess, but Modular makes it a breeze:</p><div><pre><code></code></pre></div><p>And when I want to go back:</p><p>So clean and simple, right? No more <code>Navigator.of(context).push(...)</code> everywhere! 😍</p><p>Switching to Flutter Modular completely changed the way I structure my Flutter apps. If you're struggling with messy navigation and dependencies, I highly recommend giving it a try. It keeps your project modular, scalable, and easy to maintain.</p><p>I’d love to hear about your experience too—have you tried Flutter Modular? Let’s chat in the comments! 👇</p><p>If you found this helpful and want to stay updated with more Flutter tips, feel free to follow me:</p>","contentLength":2298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Use Tailwind CSS with the Clamp Function for Responsive Designs","url":"https://dev.to/rowsanali/how-to-use-tailwind-css-with-the-clamp-function-for-responsive-designs-2pn","date":1739718290,"author":"Rowsan Ali","guid":659,"unread":true,"content":"<p>In modern web development, creating responsive designs that adapt seamlessly to different screen sizes is crucial. Tailwind CSS, a utility-first CSS framework, has gained immense popularity for its simplicity and flexibility in building responsive UIs. One of the most powerful tools in CSS for responsive design is the  function, which allows you to set a value that scales between a minimum and maximum size based on the viewport width.</p><p>In this blog post, we’ll explore how to combine Tailwind CSS with the  function to create fluid, responsive designs. We’ll cover the basics of , how to integrate it with Tailwind CSS, and provide practical code examples to help you get started.</p><h2>\n  \n  \n  What is the  Function?\n</h2><p>The  function is a CSS function that allows you to define a value that scales between a minimum and maximum value based on the viewport size. It takes three parameters:</p><div><pre><code></code></pre></div><ul><li>: The smallest value the property can take.</li><li>: The value that scales with the viewport.</li><li>: The largest value the property can take.</li></ul><p>For example, if you want a font size to scale between 16px and 24px based on the viewport width, you can use:</p><div><pre><code></code></pre></div><p>This ensures the font size is never smaller than 16px, never larger than 24px, and scales fluidly in between.</p><h2>\n  \n  \n  Why Use  with Tailwind CSS?\n</h2><p>Tailwind CSS is known for its utility classes that make it easy to apply styles directly in your HTML. However, Tailwind’s default responsive utilities (like , , etc.) rely on breakpoints, which can result in abrupt changes in design at specific screen sizes.</p><p>By combining Tailwind CSS with the  function, you can achieve , where elements scale smoothly across all screen sizes without the need for multiple breakpoints.</p><h2>\n  \n  \n  How to Use  in Tailwind CSS\n</h2><p>Tailwind CSS doesn’t provide built-in support for the  function out of the box. However, you can easily integrate it using custom utilities or inline styles. Let’s explore both approaches.</p><h3>\n  \n  \n  1. Using Custom Utilities\n</h3><p>You can extend Tailwind’s default configuration to include custom utilities for . Here’s how:</p><h4>\n  \n  \n  Step 1: Extend Tailwind’s Configuration\n</h4><p>In your  file, add a custom utility for :</p><div><pre><code></code></pre></div><p>In this example, we’ve added a custom  utility for  and . You can now use these utilities in your HTML.</p><h4>\n  \n  \n  Step 2: Apply the Custom Utility\n</h4><div><pre><code>This text scales fluidly with the viewport.This div has fluid padding.</code></pre></div><p>If you prefer not to extend Tailwind’s configuration, you can use inline styles with the  function directly in your HTML:</p><div><pre><code>\n  This text scales fluidly with the viewport.\n\n  This div has fluid padding.\n</code></pre></div><p>This approach is quick and doesn’t require any configuration changes.</p><p>Let’s look at some practical examples of using  with Tailwind CSS.</p><h3>\n  \n  \n  Example 1: Fluid Typography\n</h3><p>Fluid typography ensures that your text scales smoothly across different screen sizes. Here’s how you can achieve it:</p><div><pre><code>\n  This text scales fluidly with the viewport.\n</code></pre></div><p>Alternatively, using a custom utility:</p><div><pre><code>\n  This text scales fluidly with the viewport.\n</code></pre></div><p>Fluid spacing is useful for padding, margins, and gaps that adapt to the viewport size:</p><div><pre><code>\n  This div has fluid padding.\n</code></pre></div><p>Or with a custom utility:</p><div><pre><code>\n  This div has fluid padding.\n</code></pre></div><h3>\n  \n  \n  Example 3: Fluid Container Width\n</h3><p>You can use  to create a container that scales between a minimum and maximum width:</p><div><pre><code>\n  This container scales fluidly with the viewport.\n</code></pre></div><h2>\n  \n  \n  Benefits of Using  with Tailwind CSS\n</h2><ol><li>: Elements scale fluidly across all screen sizes, eliminating abrupt changes at breakpoints.</li><li>: You don’t need to define multiple breakpoints for different screen sizes.</li><li>: Fluid designs are more visually appealing and user-friendly.</li></ol><h2>\n  \n  \n  Limitations and Considerations\n</h2><ul><li>: The  function is supported in all modern browsers but may require fallbacks for older browsers.</li><li>: Overusing  can make your CSS harder to maintain. Use it judiciously for key elements like typography and spacing.</li></ul><p>Combining Tailwind CSS with the  function is a powerful way to create fluid, responsive designs that adapt seamlessly to any screen size. Whether you choose to extend Tailwind’s configuration or use inline styles,  can help you achieve a more polished and professional look for your web projects.</p><p>By leveraging the examples and techniques in this blog post, you can start incorporating fluid responsiveness into your designs today. Happy coding!</p>","contentLength":4313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oracle Linux on ARM - Whats the harm ?","url":"https://dev.to/nabhaas/oracle-linux-on-arm-whats-the-harm--3e0e","date":1739718141,"author":"Abhilash Kumar Bhattaram","guid":658,"unread":true,"content":"<h2>\n  \n  \n  Build Applications for the future\n</h2><p>ARM is getting much ahead in terms of computing is leveraring much ahead of Intel x86_64  , it's just a matter of time where developers would start shifting to ARM based environments for hosting the applications.</p><p>Access to ARM Linux is not hard , it's available at <a href=\"https://yum.oracle.com/oracle-linux-isos.html\" rel=\"noopener noreferrer\">https://yum.oracle.com/oracle-linux-isos.html</a> , you just need the required hardware to set it up. For starters Apple Mac / iMac enviuronments have ARM hardware and running a Virtualbox with OEL 9 Linux would be a good starting point. Windows 11 on ARM is also available and certified. </p><ul><li>Easy Deployment: Oracle Linux for Arm images can be deployed directly from the OCI console on AmpereOne and Ampere Altra Arm-based compute services.</li><li>Frequent Updates: The images are regularly updated to ensure users have access to the latest software.</li><li>Preinstalled OCI Utilities: These utilities simplify and accelerate deployment and configuration.</li><li>Optimized Performance: The Oracle Linux yum server is mirrored inside OCI regions, enabling faster downloads of Oracle software, bug fixes, and security updates.</li><li>No Additional Costs: Since all traffic remains within OCI, there’s no internet bandwidth consumption and no extra network charges for updates.</li></ul><p>This ensures a secure, efficient, and cost-effective experience for OCI users running Oracle Linux on Arm-based compute services.</p><h2>\n  \n  \n  Oracle Database Support for ARM\n</h2><p>Oracle Linux serves as the core development and runtime platform at Oracle and is the foundation for Oracle Database in production environments.</p><p>Below is the download link for Oracle Database on ARM </p><ul><li>Optimized for Oracle Database: Oracle Linux is engineered to support and run Oracle Database efficiently.</li><li>Certified for Arm Architecture: Oracle Database on Arm, powered by Oracle Linux, is fully certified for both cloud and on-premises deployments.</li><li>Consistent Performance &amp; Scalability: Enables businesses to build and run complex applications with predictable performance and seamless scalability.</li></ul><p>This ensures a robust, high-performance, and future-ready infrastructure for enterprises leveraging Oracle technologies.</p><h2>\n  \n  \n  Ksplice Zero Downtime patching\n</h2><p>Oracle Ksplice enables zero-downtime security updates by applying patches to the kernel and key user-space libraries (such as glibc and OpenSSL) without requiring a reboot or stopping applications.</p><ul><li>Seamless Updates: Ensures continuous system operation while applying critical security patches.</li><li>Available on Arm: Supported on the Unbreakable Enterprise Kernel (UEK) for Oracle Linux on Arm-based platforms.</li><li>Included with Oracle Linux Premier Support: Available at no additional cost for OCI subscribers.</li></ul><p>This makes Ksplice an essential tool for organizations requiring high availability, security, and minimal downtime in their Oracle Cloud or on-premises environments.</p><h2>\n  \n  \n  OS management made easy with OS Management Hub\n</h2><p>Oracle OS Management Hub is a managed service in OCI that streamlines the management and monitoring of updates and patches for Oracle Linux systems across private data centers, OCI, and supported third-party clouds.</p><ul><li>Centralized Management: A single intuitive console for managing updates and patches across distributed environments.</li><li>Flexible Patching: Execute updates immediately or automate patching policies with scheduled rollouts based on best practices.</li><li>Enhanced Security: Integrated with Oracle Ksplice for zero-downtime security updates.</li><li>Lifecycle Management: Supports staged rollouts of security errata and other updates, ensuring controlled deployments.</li><li>No Additional Cost: Available with Oracle Linux Support (Basic and Premier) and OCI Compute subscriptions at no extra charge.</li></ul><p>This makes OS Management Hub an essential tool for enterprises seeking efficient, automated, and secure Linux system management across hybrid and multi-cloud environments.</p><p>Oracle Linux for Arm provides a comprehensive developer toolset, including the GCC compiler, to build and optimize code for 64-bit Arm platforms.</p><ul><li>Available in Oracle Linux 9 and 8, allowing multiple versions of user-space components to be updated independently of the core OS.</li><li>Developers can safely use the latest versions of Python, PHP, Node.js, nginx, and more without disrupting existing applications.</li><li>Extra Packages for Enterprise Linux (EPEL) are built and digitally signed by Oracle, ensuring trusted and secure software distribution.</li></ul><p>With Oracle Linux for Arm, developers can leverage a modern, secure, and flexible environment to build and deploy applications efficiently on Oracle Cloud Infrastructure and beyond.</p><h2>\n  \n  \n  How do you know you are running on ARM\n</h2><p>uname -m would show as aarch64</p><div><pre><code>[root@ab8lab1 ~]# uname -m\naarch64\n\n</code></pre></div><h2>\n  \n  \n  Would yum repo work on ARM\n</h2><p>Yes for sure , you could see yum repo list with .aarch64 which is showing you that these are ARM processor packages</p><div><pre><code>[root@ab8lab1 ~]# yum list all | grep oracle\noracle-backgrounds.noarch                                                                90.2-1.0.4.el9                              @anaconda                  \noracle-epel-release-el9.aarch64                                                          1.0-1.el9                                   @ol9_baseos_latest         \noracle-indexhtml.noarch                                                                  9-4.0.1.el9_2                               @anaconda                  \noracle-logos.aarch64                                                                     90.2-1.0.4.el9                              @anaconda                  \noraclelinux-release.aarch64                                                              9:9.5-1.0.5.el9                             @anaconda                  \noraclelinux-release-el9.aarch64                                                          1.0-18.el9                                  @anaconda                  \nNetworkManager-config-connectivity-oracle.noarch                                         1:1.48.10-5.0.1.el9_5                       ol9_baseos_latest          \nmono-data-oracle.aarch64                                                                 6.12.0-5.el9                                ol9_developer_EPEL         \nnagios-plugins-oracle.aarch64                                                            2.4.9-1.el9                                 ol9_developer_EPEL         \noracle-bookmarks.noarch                                                                  28-22.0.1.el9                               ol9_distro_builder         \noracle-bookmarks.src                                                                     28-22.0.1.el9                               ol9_distro_builder         \noracle-epel-release-el9.src                                                              1.0-1.el9                                   ol9_baseos_latest          \noracle-indexhtml.src                                                                     9-4.0.1.el9_2                               ol9_baseos_latest          \noracle-indexhtml.src                                                                     9-4.0.1.el9_2                               ol9_u2_baseos_base         \noracle-indexhtml.src                                                                     9-4.0.1.el9_2                               ol9_u3_baseos_base         \noracle-indexhtml.src                                                                     9-4.0.1.el9_2                               ol9_u4_baseos_base         \noracle-indexhtml.src                                                                     9-4.0.1.el9_2                               ol9_u5_baseos_base         \noracle-linux-manager-client-release-el9.noarch                                           1.0-2.el9                                   ol9_baseos_latest          \noracle-linux-manager-client-release-el9.src                                              1.0-2.el9                                   ol9_baseos_latest          \noracle-logos.src                                                                         90.2-1.0.4.el9                              ol9_baseos_latest          \noracle-logos.src                                                                         90.2-1.0.4.el9                              ol9_u3_baseos_base         \noracle-logos.src                                                                         90.2-1.0.4.el9                              ol9_u4_baseos_base         \noracle-logos.src                                                                         90.2-1.0.4.el9                              ol9_u5_baseos_base         \noracle-logos-httpd.noarch                                                                90.2-1.0.4.el9                              ol9_baseos_latest          \noracle-logos-httpd.noarch                                                                90.2-1.0.4.el9                              ol9_u3_baseos_base         \noracle-logos-httpd.noarch                                                                90.2-1.0.4.el9                              ol9_u4_baseos_base         \noracle-logos-httpd.noarch                                                                90.2-1.0.4.el9                              ol9_u5_baseos_base         \noracle-logos-ipa.noarch                                                                  90.2-1.0.4.el9                              ol9_baseos_latest          \noracle-logos-ipa.noarch                                                                  90.2-1.0.4.el9                              ol9_u3_baseos_base         \noracle-logos-ipa.noarch                                                                  90.2-1.0.4.el9                              ol9_u4_baseos_base         \noracle-logos-ipa.noarch                                                                  90.2-1.0.4.el9                              ol9_u5_baseos_base         \noracle-ocne-release-el9.aarch64                                                          1.0-5.el9                                   ol9_baseos_latest          \noracle-ocne-release-el9.src                                                              1.0-5.el9                                   ol9_baseos_latest          \noracle-olcne-release-el9.aarch64                                                         1.0-4.el9                                   ol9_baseos_latest          \noracle-olcne-release-el9.src                                                             1.0-4.el9                                   ol9_baseos_latest          \noracleasm-support.aarch64                                                                3.0.0-7.el9                                 ol9_addons                 \noracleasm-support.src                                                                    3.0.0-7.el9                                 ol9_addons                 \noraclelinux-developer-release-el9.aarch64                                                1.0-2.el9                                   ol9_baseos_latest          \noraclelinux-developer-release-el9.src                                                    1.0-2.el9                                   ol9_baseos_latest          \noraclelinux-release.src                                                                  9:9.5-1.0.5.el9                             ol9_baseos_latest          \noraclelinux-release.src                                                                  9:9.5-1.0.5.el9                             ol9_u5_baseos_base         \noraclelinux-release.src                                                                  9:9.5-1.0.5.el9                             ol9_codeready_builder      \noraclelinux-release-el9.src                                                              1.0-18.el9                                  ol9_baseos_latest          \noraclelinux-release-el9.src                                                              1.0-18.el9                                  ol9_u5_baseos_base         \noraclelinux-sb-certs.noarch                                                              9:9.5-1.0.5.el9                             ol9_codeready_builder      \npcp-oracle-conf.aarch64                                                                  1-6.0.3.el9                                 ol9_addons                 \npcp-oracle-conf.src                                                                      1-6.0.3.el9                                 ol9_addons                 \npcp-pmda-oracle.aarch64                                                                  6.2.2-7.el9_5                               ol9_appstream              \npython-cx-oracle.src                                                                     8.3.0-7.el9                                 ol9_developer_EPEL         \npython-oracledb.src                                                                      1.2.2-1.el9                                 ol9_developer_EPEL         \npython3-cx-oracle.aarch64                                                                8.3.0-7.el9                                 ol9_developer_EPEL         \npython3-oracledb.aarch64                                                                 1.2.2-1.el9                                 ol9_developer_EPEL         \ntuned-profiles-oracle.noarch                                                             2.24.0-2.0.1.el9_5                          ol9_appstream              \n[root@ab8lab1 ~]# \n\n</code></pre></div><h2>\n  \n  \n  How to install an ARM yum package\n</h2><p>It's no different from any other Linux Install package , you can see a sample install steps below.</p><div><pre><code>[root@ab8lab1 ~]# curl -o oracle-epel-release-el9.aarch64 https://yum.oracle.com/repo/OracleLinux/OL9/appstream/aarch64/getPackage/oracle-epel-release-el9.aarch64\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100    10  100    10    0     0      4      0  0:00:02  0:00:02 --:--:--     4\n\n\n[root@ab8lab1 ~]# ls -l \ntotal 8\n-rw-------. 1 root root 1510 Feb 16 15:46 anaconda-ks.cfg\n-rw-r--r--. 1 root root   10 Feb 16 18:45 oracle-epel-release-el9.aarch64\n[root@ab8lab1 ~]# \n\n\n\n[root@ab8lab1 ~]# dnf -y install oracle-epel-release-el9.aarch64\nLast metadata expiration check: 0:52:32 ago on Sun 16 Feb 2025 05:56:12 PM IST.\nDependencies resolved.\n====================================================================================================================================================================================================================================================\n Package                                                           Architecture                                      Version                                                      Repository                                                   Size\n====================================================================================================================================================================================================================================================\nInstalling:\n oracle-epel-release-el9                                           aarch64                                           1.0-1.el9                                                    ol9_baseos_latest                                            14 k\nInstalling dependencies:\n yum-utils                                                         noarch                                            4.3.0-16.0.1.el9                                             ol9_baseos_latest                                            53 k\n\nTransaction Summary\n====================================================================================================================================================================================================================================================\nInstall  2 Packages\n\nTotal download size: 67 k\nInstalled size: 41 k\nDownloading Packages:\n(1/2): oracle-epel-release-el9-1.0-1.el9.aarch64.rpm                                                                                                                                                                3.8 kB/s |  14 kB     00:03    \n(2/2): yum-utils-4.3.0-16.0.1.el9.noarch.rpm                                                                                                                                                                         13 kB/s |  53 kB     00:04    \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nTotal                                                                                                                                                                                                                17 kB/s |  67 kB     00:04     \nRunning transaction check\nTransaction check succeeded.\nRunning transaction test\nTransaction test succeeded.\nRunning transaction\n  Preparing        :                                                                                                                                                                                                                            1/1 \n  Installing       : yum-utils-4.3.0-16.0.1.el9.noarch                                                                                                                                                                                          1/2 \n  Installing       : oracle-epel-release-el9-1.0-1.el9.aarch64                                                                                                                                                                                  2/2 \n  Running scriptlet: oracle-epel-release-el9-1.0-1.el9.aarch64                                                                                                                                                                                  2/2 \n  Verifying        : oracle-epel-release-el9-1.0-1.el9.aarch64                                                                                                                                                                                  1/2 \n  Verifying        : yum-utils-4.3.0-16.0.1.el9.noarch                                                                                                                                                                                          2/2 \n\nInstalled:\n  oracle-epel-release-el9-1.0-1.el9.aarch64                                                                                    yum-utils-4.3.0-16.0.1.el9.noarch                                                                                   \n\nComplete!\n[root@ab8lab1 ~]# \n\n</code></pre></div><p>Hope this blog gives you some insight on the ARM to get you started.</p>","contentLength":18869,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Untitled","url":"https://dev.to/sanant_sharma_d7450b9163d/untitled-59h1","date":1739718131,"author":"sanant sharma","guid":657,"unread":true,"content":"<p>Check out this Pen I made!</p>","contentLength":26,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Các bước vẽ Collabarotion Diagram","url":"https://dev.to/hcmute_project_988df1c63c/cac-buoc-ve-collabarotion-diagram-30fo","date":1739716362,"author":"HCMUTE Project","guid":656,"unread":true,"content":"<blockquote><p>Sau khi tìm hiểu về các component trong Collaboration Diagram, tiếp theo chúng ta sẽ tìm hiểu về các bước để vẽ Collaboration Diagram. \nTạo một Collaboration Diagram hiệu quả bao gồm một số bước quan trọng, mỗi bước giúp đảm bảo biểu diễn rõ ràng và toàn diện về các tương tác trong hệ thống. Dưới đây là hướng dẫn chi tiết từng bước:</p></blockquote><p>Bắt đầu bằng cách xác định ranh giới của quy trình hoặc hệ thống mà bạn muốn biểu diễn. Điểm bắt đầu và điểm kết thúc ở đâu? Bước này giúp thiết lập nội dung sẽ có trong biểu đồ.</p><p>Xác định tất cả các đối tượng (hoặc lớp) sẽ xuất hiện trong biểu đồ. Đối tượng có thể là các thành phần hệ thống, tác nhân tham gia vào quy trình hoặc thực thể dữ liệu. Hãy đảm bảo đầy đủ nhưng vẫn có liên quan.</p><h4>\n  \n  \n  3. Xác định mối quan hệ giữa các đối tượng\n</h4><p>Sau khi có danh sách đối tượng, hãy xác định cách chúng tương tác với nhau. Chúng có gửi tin nhắn không? Có hợp tác trong một nhiệm vụ cụ thể không? Bước này rất quan trọng để hiểu được động lực hoạt động của hệ thống.</p><h4>\n  \n  \n  4. Phác thảo sơ đồ ban đầu\n</h4><p>Bắt đầu với một bản phác thảo sơ đồ. Đặt các đối tượng vào vị trí và vẽ các đường kết nối để chỉ ra sự tương tác. Sử dụng các ký hiệu UML tiêu chuẩn để thể hiện các loại tương tác và mối quan hệ khác nhau.</p><h4>\n  \n  \n  5. Gán số thứ tự cho các tương tác\n</h4><p>Số thứ tự rất quan trọng trong biểu đồ cộng tác vì chúng thể hiện trình tự của các tương tác. Hãy gán số cho từng tương tác để phản ánh đúng luồng xử lý của quy trình.</p><h4>\n  \n  \n  6. Bổ sung chi tiết cho các tương tác\n</h4><p>Đối với mỗi tương tác, hãy thêm các chi tiết cần thiết như điều kiện xảy ra, thông điệp được truyền đi và phản hồi (nếu có). Thông tin này giúp biểu đồ có chiều sâu hơn.</p><h4>\n  \n  \n  7. Xác minh luồng tương tác\n</h4><p>Xem xét lại biểu đồ để đảm bảo trình tự các tương tác hợp lý và phản ánh chính xác quy trình. Có thể cần tham khảo ý kiến từ các thành viên nhóm hoặc bên liên quan để đảm bảo tính chính xác.</p><h4>\n  \n  \n  8. Chỉnh sửa và hoàn thiện biểu đồ\n</h4><p>Dựa trên phản hồi và các hiểu biết bổ sung, hãy tinh chỉnh biểu đồ. Điều chỉnh bố cục để dễ đọc hơn và đảm bảo tất cả các yếu tố được gán nhãn và sắp xếp chính xác.</p><p>Cuối cùng, xem lại biểu đồ để thực hiện các điều chỉnh cuối cùng. Khi đã hoàn tất, hãy chia sẻ với những người liên quan, bao gồm thành viên nhóm, quản lý dự án hoặc khách hàng, tùy vào mục đích của biểu đồ.</p>","contentLength":3089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Power of Promises: Separating UI and Business Logic in React","url":"https://dev.to/anton_akhatov/the-power-of-promises-separating-ui-and-business-logic-in-react-55h0","date":1739716143,"author":"Anton Akhatov","guid":645,"unread":true,"content":"<p>One of the key principles in modern frontend development is keeping UI and business logic separate. When working with async operations, the real power lies in Promises. Properly structuring Promises ensures better modularity, testability, and separation of concerns, making components easier to manage.</p><h3><strong>The Problem: Mixing UI and Business Logic</strong></h3><p>A common anti-pattern is placing both business logic and side effects (such as notifications) inside a custom hook:</p><div><pre><code></code></pre></div><p>Here, the toast notifications are inside the hook, making it harder to control from the UI and less flexible for reuse.</p><h3><strong>A Better Approach: Keeping API Handling Separate</strong></h3><p>Instead of handling toasts inside the hook, we can structure our code so that business logic remains separate, making the API logic reusable without being tied to UI concerns.</p><h4><strong>Step 1: Define the Business Logic</strong></h4><div><pre><code></code></pre></div><h4><strong>Step 2: Use a Custom Hook to Manage API Calls (optional)</strong></h4><div><pre><code></code></pre></div><p>This keeps the API logic self-contained without enforcing any specific UI behavior.</p><h4><strong>Step 3: Handle UI Logic in the Component with </strong></h4><div><pre><code></code></pre></div><p>✅ <strong>Clear Separation of Responsibilities</strong> – The API logic is independent of UI concerns, allowing for better maintainability and scalability.\n✅ <strong>Decoupled UI and Business Logic</strong> – The component only handles rendering and user interactions, while the business logic stays separate.\n✅  – The API logic and UI behavior can be tested independently, improving reliability.\n✅  – The API logic can be used across different components without modification, making it modular and flexible.\n✅  – Changes to API handling or UI behavior can be made independently without affecting each other.</p><p>By properly structuring our async operations and leveraging Promises, we ensure that our React applications remain clean, modular, and easier to maintain. How do you handle async operations in your projects? Let’s discuss!</p>","contentLength":1837,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Ensuring Unique Slugs in Next.js 15 with Prisma & Slugify","url":"https://dev.to/saiful7778/ensuring-unique-slugs-in-nextjs-15-with-prisma-slugify-4agc","date":1739715880,"author":"Saiful Islam","guid":644,"unread":true,"content":"<p>Creating  slugs is essential for URLs in . However, ensuring they remain  can be tricky—especially when dealing with  like blog posts.</p><p>In this guide, we’ll explore an <strong>efficient recursive approach</strong> to generate unique slugs using  and  in ! 🔥</p><h2>\n  \n  \n  🌟 Why Unique Slugs Matter?\n</h2><p>✅  – Clean, readable URLs improve search rankings\n✅  – Easy to share &amp; understand\n✅  – Avoid duplicate slugs for different posts</p><h2>\n  \n  \n  🛠 Implementing Recursive Slug Generation\n</h2><p>The following function:\n✔ Converts a title into a slug<p>\n✔ Checks if the slug already exists in the database</p>\n✔ Recursively appends a suffix if the slug is taken</p><div><pre><code></code></pre></div><p>1️⃣ Converts the input string into a lowercase, URL-friendly slug\n2️⃣ Checks the database if a record with the same slug exists<p>\n3️⃣ If found, recursively calls itself with a numeric suffix (e.g., </p>, , ...)\n4️⃣ Returns the first available unique slug</p><h2>\n  \n  \n  📌 Example Usage in Next.js 15\n</h2><p>You can use this function when creating new posts dynamically:</p><div><pre><code></code></pre></div><p>✅ Calling <code>createPost(\"My Next.js Guide\", \"Some content\")</code>\n🚀 Generates , or if taken, , , etc.</p><p>🔥 <strong>Handles Duplicates Automatically</strong> – Ensures slugs remain unique\n⚡  – Clean and structured URLs\n🛠 <strong>Works Seamlessly with Prisma</strong> – No extra logic needed in API routes</p><p>Slugs are a  part of a Next.js app’s . With this recursive approach, you ensure that every post, page, or product gets a , automatically.</p><p>💡 Would you use this approach in your project? Let’s discuss in the comments! 🚀</p>","contentLength":1510,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"20 Must-Know React.js Techniques for Beginners in 2025","url":"https://dev.to/sovannaro/20-must-know-reactjs-techniques-for-beginners-in-2025-2ln4","date":1739715696,"author":"SOVANNARO","guid":643,"unread":true,"content":"<p>Welcome to the exciting world of React.js! Whether you're just starting out or looking to deepen your understanding, mastering these 20 techniques will set you on a path to becoming a proficient React developer. Let's dive in and explore the essential tips and tricks that will make your journey both enjoyable and productive.</p><p>JSX is a syntax extension for JavaScript that looks similar to XML or HTML. It's a fundamental part of React, allowing you to write HTML-like code within JavaScript. Embrace JSX as it makes your code more readable and easier to write.</p><div><pre><code>Hello, world!</code></pre></div><h2>\n  \n  \n  2. <strong>Component-Based Architecture</strong></h2><p>React is all about components. Break down your UI into reusable pieces to manage and build your application more efficiently. Think of components as building blocks that can be assembled in various ways.</p><div><pre><code>Hello, </code></pre></div><h2>\n  \n  \n  3. <strong>Functional vs. Class Components</strong></h2><p>Start with functional components, as they are simpler and easier to understand. Once you're comfortable, explore class components to grasp the full spectrum of React's capabilities.</p><div><pre><code>Hello, !Hello, !</code></pre></div><p>Understanding state is crucial. Use the  hook in functional components to manage local state. Remember, state should be kept as simple as possible to avoid complexity.</p><div><pre><code>You clicked  times\n        Click me\n      </code></pre></div><p>Pass data between components using props. While it's straightforward, be mindful of prop drilling—passing props through multiple layers of components—which can make your code harder to maintain.</p><div><pre><code></code></pre></div><p>When multiple components need to share state, lift it up to their closest common ancestor. This technique helps in maintaining a single source of truth for your data.</p><div><pre><code>IncrementCount: </code></pre></div><h2>\n  \n  \n  7. <strong>Hooks: Your New Best Friends</strong></h2><p>Hooks like , , and custom hooks are game-changers. They allow you to use state and other React features in functional components, making your code more concise and reusable.</p><div><pre><code>You clicked  times\n        Click me\n      </code></pre></div><h2>\n  \n  \n  8. <strong>Context API for Global State</strong></h2><p>For managing global state, the Context API is a lifesaver. It allows you to pass data through the component tree without having to pass props down manually at every level.</p><div><pre><code></code></pre></div><p>Render components conditionally based on state or props. Use JavaScript's logical operators to control what gets displayed, making your UI dynamic and responsive.</p><div><pre><code>Welcome back!Please sign up.</code></pre></div><p>When rendering lists, always provide a unique  prop to each element. This helps React identify which items have changed, are added, or are removed, optimizing the rendering process.</p><div><pre><code></code></pre></div><p>Handling events in React is straightforward. Define your event handlers as functions and pass them as props to your components. Remember to use synthetic events provided by React.</p><div><pre><code>Click me</code></pre></div><p>Use controlled components for form inputs. By keeping the form data in the component's state, you have a single source of truth, making it easier to manage and validate input.</p><div><pre><code></code></pre></div><p>Error boundaries catch JavaScript errors anywhere in their child component tree, log those errors, and display a fallback UI instead of the component tree that crashed. They improve the user experience by preventing the entire app from crashing.</p><div><pre><code>Something went wrong.</code></pre></div><h2>\n  \n  \n  14. <strong>Code Splitting and Lazy Loading</strong></h2><p>Improve your app's performance with code splitting and lazy loading. Use React's  and  to load components only when they are needed, reducing the initial load time.</p><div><pre><code>Loading...</code></pre></div><h2>\n  \n  \n  15. <strong>Higher-Order Components (HOC)</strong></h2><p>HOCs are a pattern for reusing component logic. They take a component and return a new component with enhanced functionality, promoting code reuse and separation of concerns.</p><div><pre><code></code></pre></div><p>Create custom hooks to encapsulate and reuse stateful logic. They allow you to extract component logic into reusable functions, making your components cleaner and more focused.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  17. <strong>React Router for Navigation</strong></h2><p>For single-page applications, React Router is essential. It allows you to define routes and navigate between different views, providing a seamless user experience.</p><div><pre><code></code></pre></div><p>Explore different styling options like CSS Modules, styled-components, or even plain CSS. Choose the one that best fits your project's needs and your team's preferences.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  19. <strong>Testing with Jest and React Testing Library</strong></h2><p>Write tests to ensure your components work as expected. Jest and React Testing Library are popular choices that integrate well with React, helping you catch bugs early.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  20. </h2><p>Optimize your app's performance by using techniques like memoization with  and , and avoiding unnecessary re-renders. Profile your app using React's built-in tools to identify bottlenecks.</p><div><pre><code></code></pre></div><p>Great! You can encourage readers to follow your GitHub account by including a call-to-action in your blog post. Here's how you can integrate it into the conclusion of your blog:</p><p>Congratulations on taking the first steps into the world of React.js! By mastering these 20 techniques, you'll be well on your way to building robust and efficient web applications. Embrace the learning journey, experiment with new ideas, and most importantly, have fun coding!</p><p>If you found this guide helpful, I'd love for you to follow me on GitHub at <a href=\"https://github.com/sovannaro\" rel=\"noopener noreferrer\">sovannaro</a> for more coding tips, projects, and updates. Let's connect and grow together in the React community!</p>","contentLength":5164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a Better Way to Track Domain Prices: My Journey Creating TLDSpy API","url":"https://dev.to/arnauddsj/building-a-better-way-to-track-domain-prices-my-journey-creating-tldspy-api-42ff","date":1739715480,"author":"Arnaud DE SAINT JEAN","guid":642,"unread":true,"content":"<p>I wanted to share a side project I've been working on, something born out of my own frustration and, hopefully, something that might be useful to others here in the dev community.</p><p><strong>The Backstory: A Domain Portfolio Manager Needs Accurate Data</strong></p><p>I'm building a domain portfolio manager (it's still a work in progress!). As part of that project, I needed a reliable source of domain pricing data, specifically for comparing prices across different registrars. I was surprised at how difficult it was to find something that met my needs. Scraping was a nightmare (constantly breaking!), existing APIs were either incomplete, provided wrong data, or didn't offer the specific endpoints I was looking for.</p><p>So, like any indie hacker, I decided to build my own. That's how TLDSpy API came to life. I've now decided to make this stand-alone and provide this as a service.</p><p>TLDSpy API is designed to provide accurate and comprehensive domain pricing and information. Here's a quick overview of what it offers:</p><ul><li> Compare registration, renewal, and transfer prices across multiple registrars.</li><li> Access WHOIS, DNS, SSL, and domain status data.</li><li> Since February 2025 for historical insights.</li></ul><p><strong>A Little Technical Dive: Getting Started</strong></p><p>The API is pretty straightforward to use. Here's an example of how you can find the cheapest registrars for a  domain using :</p><div><pre><code>curl  GET </code></pre></div><p>This returns a JSON response like this:</p><div><pre><code></code></pre></div><p>Here's another example to compare registrar prices for multiple TLDs:</p><div><pre><code>curl  GET </code></pre></div><p>This call would give you a JSON response like this:</p><div><pre><code></code></pre></div><p>For those needing to perform bulk lookups, the API offers a batch domain lookup endpoint. Here's an example of how to check WHOIS and SSL information for multiple domains:</p><div><pre><code>curl  POST </code></pre></div><div><pre><code>{\n  \"results\": {\n    \"example.com\": {\n      \"domain\": \"example.com\",\n      \"whois\": {\n        \"domain\": \"example.com\",\n        \"registrar\": \"Namecheap, Inc.\",\n        \"registrar_key\": \"namecheap\",\n        \"registrar_iana_id\": \"1068\",\n        \"registrar_url\": \"https://www.namecheap.com\",\n        \"creation_date\": \"1995-03-16T05:00:00Z\",\n        \"expiry_date\": \"2025-03-17T05:00:00Z\",\n        \"last_updated\": \"2024-02-28T14:48:12Z\",\n        \"status\": [\n          \"clientTransferProhibited\"\n        ],\n        \"nameservers\": [\n          \"ns1.example.com\",\n          \"ns2.example.com\"\n        ],\n        \"source_library\": \"whois-go\",\n        \"is_available\": false\n      },\n      \"dns\": {\n        \"a_records\": [\n          \"93.184.216.34\"\n        ],\n        \"aaaa_records\": [\n          \"2606:2800:220:1:248:1893:25c8:1946\"\n        ],\n        \"mx_records\": [\n          \"10 mail.example.com\"\n        ],\n        \"txt_records\": [\n          \"v=spf1 -all\"\n        ],\n        \"ns_records\": [\n          \"ns1.example.com\",\n          \"ns2.example.com\"\n        ],\n        \"cname_records\": [\n          \"www.example.com\"\n        ],\n        \"dnssec\": {\n          \"enabled\": true,\n          \"ds_records\": [\n            \"12345 13 2 ABCDEF123456789...\"\n          ]\n        }\n      },\n      \"ssl\": {\n        \"valid\": true,\n        \"issuer\": \"DigiCert Inc\",\n        \"subject\": \"example.com\",\n        \"version\": \"3\",\n        \"serial_number\": \"0123456789ABCDEF\",\n        \"not_before\": \"2024-01-01T00:00:00Z\",\n        \"not_after\": \"2025-01-01T23:59:59Z\",\n        \"san_list\": [\n          \"example.com\",\n          \"www.example.com\"\n        ]\n      },\n      \"status\": {\n        \"available\": true,\n        \"status_code\": 200,\n        \"response_time_ms\": 245,\n        \"protocol\": \"https\",\n        \"checked_at\": \"2024-01-15T10:30:00Z\",\n        \"error\": null\n      },\n      \"available\": false,\n      \"errors\": null\n    },\n    \"example.net\": {\n      \"domain\": \"example.net\",\n      \"whois\": {\n        \"domain\": \"example.net\",\n        \"registrar\": \"Namecheap, Inc.\",\n        \"registrar_key\": \"namecheap\",\n        \"registrar_iana_id\": \"1068\",\n        \"registrar_url\": \"https://www.namecheap.com\",\n        \"creation_date\": \"1997-01-29T05:00:00Z\",\n        \"expiry_date\": \"2025-01-30T05:00:00Z\",\n        \"last_updated\": \"2024-02-28T14:48:12Z\",\n        \"status\": [\n          \"clientTransferProhibited\"\n        ],\n        \"nameservers\": [\n          \"ns1.example.net\",\n          \"ns2.example.net\"\n        ],\n        \"source_library\": \"whois-go\",\n        \"is_available\": false\n      },\n      \"dns\": {\n        \"a_records\": [\n          \"93.184.216.34\"\n        ],\n        \"aaaa_records\": [\n          \"2606:2800:220:1:248:1893:25c8:1946\"\n        ],\n        \"mx_records\": [\n          \"10 mail.example.net\"\n        ],\n        \"txt_records\": [\n          \"v=spf1 -all\"\n        ],\n        \"ns_records\": [\n          \"ns1.example.net\",\n          \"ns2.example.net\"\n        ],\n        \"cname_records\": [\n          \"www.example.net\"\n        ],\n        \"dnssec\": {\n          \"enabled\": true,\n          \"ds_records\": [\n            \"12345 13 2 ABCDEF123456789...\"\n          ]\n        }\n      },\n      \"ssl\": {\n        \"valid\": true,\n        \"issuer\": \"DigiCert Inc\",\n        \"subject\": \"example.net\",\n        \"version\": \"3\",\n        \"serial_number\": \"0123456789ABCDEF\",\n        \"not_before\": \"2024-01-01T00:00:00Z\",\n        \"not_after\": \"2025-01-01T23:59:59Z\",\n        \"san_list\": [\n          \"example.net\",\n          \"www.example.net\"\n        ]\n      },\n      \"status\": {\n        \"available\": true,\n        \"status_code\": 200,\n        \"response_time_ms\": 245,\n        \"protocol\": \"https\",\n        \"checked_at\": \"2024-01-15T10:30:00Z\",\n        \"error\": null\n      },\n      \"available\": false,\n      \"errors\": null\n    }\n  },\n  \"stats\": {\n    \"total_domains\": 2,\n    \"successful\": 2,\n    \"failed\": 0,\n    \"time_elapsed\": \"1.234s\"\n  }\n}\n</code></pre></div><p>You can find more detailed documentation, including endpoints for WHOIS, DNS lookups, and more examples in multiple languages (Python, JavaScript, Go, Ruby) here: <a href=\"https://api.tldspy.com/docs\" rel=\"noopener noreferrer\">https://api.tldspy.com/docs</a></p><p><strong>Why I'm Sharing (and Asking for Feedback)</strong></p><p>I've reached a point where I think TLDSpy API could be valuable to others, especially those working on domain-related projects, tools, or even just trying to make informed decisions about domain registration.</p><p>I'd love to get your feedback on this:</p><ul><li><strong>Is this something you'd find useful in your projects?</strong></li><li><strong>Are there any features or endpoints you'd like to see added?</strong></li><li><strong>Any thoughts on the pricing structure?</strong></li></ul><p>I'm currently offering a free tier with access to several popular TLDs. You can get started here: <a href=\"https://tldspy.com\" rel=\"noopener noreferrer\">https://tldspy.com</a> No credit card required.</p><p>I'm really eager to hear your thoughts and suggestions. Thanks for taking the time to read about my project!</p>","contentLength":6440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning React Js","url":"https://dev.to/ali007depug/learning-react-js-1p94","date":1739715237,"author":"Ali AbdElbagi","guid":641,"unread":true,"content":"<p>I am going to share my React learning journey with you.</p><p>i need some advice from people who went in this path before me</p><p>i have good knowledge in HTML,CSS,JS,SASS,TAILWIND</p>","contentLength":167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Memoization in React","url":"https://dev.to/anuraggharat/memoization-in-react-379i","date":1739714952,"author":"Anurag Gharat","guid":640,"unread":true,"content":"<p>Lately, I have been working on some old React applications. During this time I learnt and implemented some techniques to optimize and improve the performance of a React application. One such technique is Memoization.</p><p>Here in this blog, I will talk about how to apply Memoization in a React application. We will see how the  function can be used to avoid unnecessary re-renders. We will also check the  and  hooks and how to use them alongside the  higher-order function. In the end, we will discuss practical use cases on when to apply memoization and when to skip.</p><blockquote><p>Memoization in software engineering is a technique of caching the results of expensive operations and reusing them whenever the same operation is called. Using this technique, redundant time-consuming operations are skipped, thus improving performance.</p></blockquote><p>Before we move on to the actual topic, let’s revise some basics first. </p><p>React is a JavaScript library used for building user interfaces. It follows a component-based architecture, where the UI is divided into small, reusable components that function independently based on their state. </p><p>A state can be thought of as the local memory of a component, responsible for holding its dynamic data. React ensures the UI stays up-to-date by re-rendering components whenever the state of that component changes. In essence, any change in a component’s state triggers a re-render to reflect the updated change on the UI.</p><p>A Component in React will re-render on two occasions</p><ol><li>If the state of that component changes.</li><li>If the Parent component of that component re-renders.</li></ol><p>For example, if we have Component A, which is a parent of Component B, then every time Component A re-renders, B will automatically re-render itself since it is the child of that Component. See the image below for a better understanding.   </p><p>By triggering a component re-render, React maintains the sync between all the components and keeps the UI updated with the latest changes. But this very own re-rendering property of React also is responsible for slowing down the application. You see, re-rendering a component means re-creating the entire Component along with the variables and functions inside it from scratch. This process is not always simple and inexpensive. For example, look at the below image</p><p>When Component A re-renders, all components within its branch(child components) will also be re-rendered. This means that even if a component didn't undergo state change, it will still be re-rendered again since its parent got re-rendered. This at times is unnecessary and inefficient. So as per the above image, if Component A re-renders, Component B, C, and D will be re-rendered.</p><p>If these components involve resource-intensive operations such as data fetching, initializing large objects or arrays, or running computationally expensive functions, those operations will be executed again. This situation becomes even more problematic if Component X triggers a re-render. In such a case, not only will the entire Branch A(children of A) re-render, but so will Branch B(children of B), causing significant performance issues.</p><p>Hence, the inherent behavior of React, which involves re-rendering, can also contribute to performance degradation. Now that we have identified the problem, let's explore how we can address it effectively.</p><p>Enough talk, let’s start writing some code. In the below code, I have written a simple Parent component with a  state, a variable displaying the  value, and a  to update the count. Inside the Parent component, I have added a simple Child component that takes no props.</p><div><pre><code>This is child component\n          count is </code></pre></div><p>After running the above code in the browser we can see the below output in the browser's console.</p><p>Try clicking the  to change the count. If you see the console again, you will observe that not only the Parent component is re-rendered but also the Child component.</p><p>This is the unnecessary re-rendering of the Child component problem that we discussed earlier.</p><p>Component rendering can be a heavy task that might affect the overall performance of an application. To avoid this unnecessary re-rendering of components, we can use the  higher-order function.</p><p> is a higher-order component provided by React to create a memoized version of a component. A memoized component makes a shallow comparison of the previous and new props. If the props are different, the components re-render while if the props are the same the component re-rendering is skipped. If props are absent, the component does not re-render on the Parent component re-render. Let’s see this in action by modifying the previous non-performant code.</p><div><pre><code>This is child component\n          count is </code></pre></div><p>Now re-run the application and open the console. The Parent and Child component will be re-rendered on the initial render. After that every time you click on the , only and only the Parent Component will re-render and rendering of the Child component will be skipped.</p><p>Well, that's how easy it is to memoize a component. Let's try some more scenarios with the same example and see how memoization performs in those cases.</p><p>In the previous code, I added a new variable called  of type  and passed it as a  to the Child component.</p><div><pre><code>\n          count is </code></pre></div><p>Now, if you rerun the application and press the count button, the Child component will not re-render. This is because we have memoized it. Every time the Parent component re-renders, the memoized Child component does the prop check. Since the user prop does not change, the Child component does not re-render.</p><p>Now, I will change the type of  variable to an object and run the same scenario.</p><div><pre><code>\n          count is </code></pre></div><p>Now after the initial render, if I click on the count button, the Child component starts re-rendering again! The memoization fails in this case.</p><p>But why? We memoized the function and the value of the  variable also does not change so why does the Child component re-render? Well, this is because  does a shallow comparison of the props that are passed to the component. Props of types String, Number, and Boolean are passed by value whereas props of type Arrays, and Objects are passed by reference.</p><div><pre><code></code></pre></div><p>If you do not understand the above code, I suggest you read about how Objects and Arrays work in JavaScript. </p><p>So what's happening in our React code is the user variable is of type Object. In the initial render, it is initialized with an initial reference. When the  state is updated in the Parent Component, the Parent component re-renders and hence the  object is initialized again with a new reference. So now the memoized Child component does a shallow check i.e. it checks the reference of the object rather than the actual object values. Since the reference has changed after the initial render it assumes that the variable has been updated and hence the Child component gets rendered again!</p><p>Hence, to avoid re-rendering, we need to preserve the reference to the user object between the renders. In such scenarios, the  hook can be helpful. </p><p>The  hook is used to memoize a value and preserve its reference between component re-renders. This hook takes an array of dependencies as a second parameter and recalculates the value if any of the dependencies change. In our case, we can use this hook to memoize the user object. Let's update the code with the  hook.</p><div><pre><code>\n          count is </code></pre></div><p>Now the  object will not re-initialized and lose its reference between renders and therefore the Child component will not re-render whenever the Parent Component re-renders.</p><p>The  hook can also be used to memoize a returned value from a heavy function. The value is saved locally and only re-calculated if any of its dependencies change. Take a look at the following example</p><div><pre><code>Count is Add CountAge is Increment AgeGet days </code></pre></div><p>You can read more about the  hook <a href=\"https://react.dev/reference/react/useMemo\" rel=\"noopener noreferrer\">here.</a>.</p><p>Now let's pass one more prop to the Child component, this time a function.</p><div><pre><code>Logout User\n          count is </code></pre></div><p>Now if you check in the browser, the Child will start again re-rendering on the  state change.</p><p>The reason behind re-rendering is similar to what we discussed in the previous example. Similar to objects, functions are also passed by references. Hence, when the Parent component re-renders, the reference of the function is changed and hence the memoized child Component re-renders again. </p><p>To avoid this, we can use the  React hook which is similar to the  hook. The  hook is used to optimize callback functions passed to child components. The only difference is  hook is used to memoize functions whereas the  hook is used to memoize values. You can read more about the  hook <a href=\"https://react.dev/reference/react/useCallback\" rel=\"noopener noreferrer\">here</a>.</p><div><pre><code>Logout User\n          count is </code></pre></div><p>In the above code, we used the  hook to avoid the re-rendering of the Child Component by preserving the function reference. Now every time, the Parent component re-renders, the  function will not lose its reference and hence the Child component will not re-render.</p><p>As you can see, only the Parent component gets re-rendered when the  state changes.</p><p>With that, we have completed all the scenarios that you might encounter while trying to implement Memoization in your React app. While Memoization is great for improving performance, if not used correctly it can negatively affect performance.</p><p>Let's understand this by an example. I have used the same code from our previous example where we are memoizing the  and  props and passing them to the memoized Child component.</p><div><pre><code>Logout User\n          count is </code></pre></div><p>As you can see, we had to add quite a few extra lines of code and made the code complex just to memoize a simple Component. Let's look at the un-memoized version of the above code.</p><div><pre><code>Logout User\n          count is </code></pre></div><p>By simplifying our code, we reduced unnecessary complexity and eliminated a few extra lines. In software engineering, every line of code contributes to memory usage and can impact application performance, even if marginally. In our example, the Child component is simple and does not involve heavy computations. As a result, rendering this component has minimal impact on the application's overall performance. In such scenarios, adding an extra layer of complexity through memoization is unnecessary and does not justify the effort, as it offers no meaningful performance gains.</p><p>When I think about optimizing anything, I always follow the following principle.</p><blockquote><p>\"The cost of optimization should not outweigh the actual benefit from optimization\".</p></blockquote><p>This means, that while performing any sort of optimization, always make sure that the code you write doesn’t introduce unnecessary complexity or maintenance overhead that outweighs the benefits of the optimization itself. So next time you think about optimizing something, think if the performance boost after optimizing is worth the effort and extra code that you write.</p><p>So then when should we actually consider Memoization? </p><p>Here is a very good example for it.</p><div><pre><code>\n           count is </code></pre></div><p>In the above piece of code, we have an  component that has a list of  and a  state. Inside the App component, we are rendering the list using the  component and passing the  and  props. The  component calls a function to get details of a post. </p><p>In this case, memoizing is essential because we have a long list of posts. So every time the  changes and the  component re-renders, all these  components will also render and the function  will be called again. If you see the below picture, you can notice that all the  components get re-rendered again.</p><div><pre><code>\n           count is </code></pre></div><p>Observe the optimizations in the output below.</p><p>As you can see, on the  change only the  component re-rendered and all the  components skipped re-rendering. This is the perfect example where Memoizing will help improve the performance!</p><p>Before we end this blog, let's look at some best practices for React Memoization</p><ol><li>Skip memoizing Static Component or Components whose props rarely change.</li><li>Use  judiciously. If the props for your components change regularly, Memoizing is not needed there.</li><li>Use the  and  hooks for memoizing non-primitive types of props like Objects, Arrays, or Functions. </li><li>Use the React profiler tool to monitor the performance of your Application.</li><li>Always evaluate the efforts for Optimization against the benefits.</li><li>The  function only performs shallow comparison of props, if you want to perform deep comparison, use a custom comparator function. A custom comparator function is passed as a second parameter to the  function in cases where custom comparison between old and new props is required. If this function returns , the Memoized Component will re-render and will skip re-rendering if the function returns .\n</li></ol><div><pre><code>const MemoizedPost = React.memo(PostComponent, (prevProps, nextProps) =&gt; {\n  if (deepCompare(prevProps, nextProps)) {\n    return true;\n  } else {\n    return false\n  }\n})\n</code></pre></div><p>That's all we had for Memoization in React. In this blog, we understood how Memoization works in React with the help of the ,  and  hook. We saw a few scenarios on handling Memoization as well as a few examples of when to use Memoization and when to skip. In the end, we saw a few best practices when it comes to Memoization!</p><p>I hope you liked this blog and learned something new! I share such development-related tips on my Twitter as well. Happy coding!</p>","contentLength":13122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 10 VS Code Extensions Every Web Developer Should Have in 2025","url":"https://dev.to/osarobo_osabuohien_414a2c/top-10-vs-code-extensions-every-web-developer-should-have-in-2025-1am","date":1739714872,"author":"Osarobo Osabuohien","guid":639,"unread":true,"content":"<p>Visual Studio Code (VS Code) is one of the most popular code editors, and the right extensions can supercharge your productivity. Whether you’re a front-end, back-end, or full-stack developer, these must-have VS Code extensions for 2025 will enhance your workflow.</p><ol><li>GitHub Copilot – AI Code Assistant</li></ol><p>GitHub Copilot is like having an AI-powered pair programmer. It suggests entire lines or blocks of code based on your context, making development faster and more efficient. It supports most programming languages and helps implement secure coding practices effortlessly.</p><p>• AI-powered code suggestions in real time.\n• Works with multiple programming languages.<p>\n• Reduces repetitive coding tasks.</p></p><ol><li>Live Server – Instant Browser Preview</li></ol><p>Live Server launches a local development server with hot reloading, meaning any HTML, CSS, or JavaScript changes you make are instantly reflected in the browser.</p><p>• No need to manually refresh your browser.\n• Works with static and dynamic pages.<p>\n• Great for testing and debugging frontend code.</p></p><ol><li>Prettier – Code Formatter</li></ol><p>Prettier ensures your code remains clean and consistent by automatically formatting it based on your chosen style guide.</p><p>• Supports JavaScript, CSS, HTML, and more.\n• Eliminates style inconsistencies across teams.<p>\n• Can format code on save.</p></p><ol><li>Tabnine – AI Autocomplete</li></ol><p>Tabnine is an AI-powered autocompletion tool that speeds up coding by predicting entire lines or blocks based on context. Unlike GitHub Copilot, it also works offline.</p><p>• AI-driven code predictions.\n• Supports multiple languages and frameworks.<p>\n• Works offline for improved security.</p></p><ol><li>IntelliCode – Smart Code Suggestions</li></ol><p>Microsoft’s IntelliCode offers context-aware suggestions, making it easier to follow best practices while coding. It learns from your coding style and adapts over time.</p><p>• Smart IntelliSense for better code completions.\n• Supports Python, JavaScript, TypeScript, and more.<p>\n• Improves productivity by reducing keystrokes.</p></p><ol><li>Bracket Pair Colorizer 2 – Code Readability Boost</li></ol><p>Bracket Pair Colorizer makes it easier to identify matching brackets in complex code by assigning each pair a different color.</p><p>• Reduces debugging time for deeply nested code.\n• Customizable colors and styles.<p>\n• Works for JavaScript, Python, C++, and more.</p></p><ol><li>Error Lens – Real-Time Debugging</li></ol><p>Error Lens makes debugging easier by displaying inline error messages instead of making you switch to the “Problems” panel.</p><p>• Errors and warnings appear directly in your code.\n• Saves time by eliminating the need to check the terminal.<p>\n• Highly customizable appearance.</p></p><ol><li>GitLens – Advanced Git Integration</li></ol><p>GitLens extends VS Code’s built-in Git functionality, giving you deeper insights into commit history, changes, and authorship.</p><p>• See who last modified a line of code.\n• Easily navigate through commit history.<p>\n• Useful for team collaboration.</p></p><ol><li>REST Client – API Testing Inside VS Code</li></ol><p>REST Client allows you to test APIs directly in VS Code without switching to tools like Postman.</p><p>• Supports REST, GraphQL, OAuth, and more.\n• Easily send HTTP requests and view responses.<p>\n• Great for backend and full-stack developers.</p></p><ol><li>ES7+ React/Redux/React-Native Snippets</li></ol><p>If you’re a React developer, this extension is a must. It provides handy snippets for quickly creating components, hooks, and Redux boilerplate code.</p><p>• Speeds up React development with pre-made snippets.\n• Reduces typing errors in JSX and Redux.<p>\n• Covers React, Redux, and React-Native.</p></p><p>These VS Code extensions can significantly enhance your coding experience in 2025. Whether you’re looking for AI-powered assistance, better debugging tools, or faster frontend development, these tools will make your workflow more efficient.</p>","contentLength":3749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simple Swiper.js Example","url":"https://dev.to/preetha_vaishnavi_2b82358/simple-swiperjs-example-24j2","date":1739714225,"author":"preetha vaishnavi","guid":638,"unread":true,"content":"<p>Check out this Pen I made!</p>","contentLength":26,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a Successful Career in Cybersecurity","url":"https://dev.to/abdelaziz_moustakim_45a4c/building-a-successful-career-in-cybersecurity-5eif","date":1739714161,"author":"ABDELAZIZ MOUSTAKIM","guid":637,"unread":true,"content":"<p>Imagine walking into a job market where companies are practically begging for skilled professionals. No layoffs, no uncertainty, just high-paying roles waiting for the right talent. Sounds too good to be true? Well, welcome to cybersecurity.</p><p>This industry is on fire. The demand for cybersecurity pros has been skyrocketing for years, and here’s the kicker, it’s not slowing down. The unemployment rate in this field has been 0% since 2016, and by 2025, there will be a 3.5 million job gap waiting to be filled. That’s not just job security, that’s job certainty.</p><p>Why? Because cyber threats aren’t going anywhere. Every day, companies are fighting an invisible war against hackers, ransomware, and data breaches. And they need people like you to protect them. Whether you’re a tech geek, a problem solver, or just someone looking for a career that pays well, cybersecurity is the career you’ve been searching for.</p><p>By the end of this article, you’ll know exactly how to start and grow your cybersecurity career. Let’s get started.</p><h2>\n  \n  \n  Choosing Your Cybersecurity Path\n</h2><p>Alright, so you’re sold on cybersecurity. Great choice. But now comes the big question: Which path should you take?</p><p>Cybersecurity isn’t just one job like so many people think it is. It’s an entire ecosystem of roles, each with different skill sets, responsibilities, and career trajectories. The good news? The industry is booming, with 3.5 million unfilled jobs projected by 2025, so no matter which path you choose, there’s a demand for it. The challenge is picking the right fit for your skills, interests, and long-term goals.</p><h2>\n  \n  \n  Defending Systems: The Frontline of Cybersecurity\n</h2><p>If you love analyzing threats, preventing attacks, and protecting systems, a defensive cybersecurity role might be perfect for you. These professionals focus on monitoring networks, detecting vulnerabilities, and responding to cyber incidents.</p><p>Cybersecurity Analysts: Monitor networks, detect threats, and respond to security incidents.\nSOC Analysts: Work in a Security Operations Center, analyzing alerts and investigating suspicious activity.<p>\nIncident Responders: Act as first responders when cyberattacks occur, containing and mitigating threats.</p>\nThreat Intelligence Analysts: Gather and analyze data to predict and prevent future cyber threats.</p><h2>\n  \n  \n  Ethical Hacking and Offensive Security\n</h2><p>For those who enjoy breaking into systems, finding vulnerabilities, and thinking like a hacker, offensive security provides an exciting challenge. These professionals test security measures by simulating real-world cyberattacks.</p><p>Penetration Testers (Ethical Hackers): Simulate cyberattacks to uncover security flaws before criminals do.\nOffensive Security Operators: Conduct advanced security assessments using real-world hacking techniques.<p>\nExploit Developers: Design and develop custom exploits to test system vulnerabilities.</p>\nBug Bounty Hunters: Independently search for security flaws in applications and earn rewards for their discoveries.<p>\nGovernance, Risk, and Compliance (GRC): Cybersecurity Without Coding</p>\nNot all cybersecurity roles require deep technical skills. If you’re interested in policy, regulations, and risk management, GRC could be a great fit. These roles focus on ensuring organizations meet security standards and legal requirements.</p><p>Security Auditors: Conduct assessments to verify compliance with security frameworks.\nCompliance Analysts: Ensure businesses meet regulatory requirements like GDPR and ISO 27001.<p>\nRisk Analysts: Identify and evaluate cybersecurity risks to help organizations mitigate threats.</p>\nCybersecurity Consultants: Advise companies on security strategies and best practices.</p><h2>\n  \n  \n  Cybersecurity Engineering: Building Secure Systems\n</h2><p>If you’re interested in creating security tools and infrastructures, cybersecurity engineering might be the right path. These roles focus on designing, implementing, and maintaining security systems.</p><p>Security Engineers: Develop security solutions such as firewalls and intrusion detection systems.\nCloud Security Engineers: Secure cloud environments like AWS, Azure, and Google Cloud.<p>\nDevSecOps Engineers: Integrate security into the software development lifecycle.</p>\nCryptographers: Develop encryption methods to protect sensitive data.</p><h2>\n  \n  \n  AI and Cybersecurity: The Future of Defense\n</h2><p>With artificial intelligence playing a growing role in cybersecurity, new career paths are emerging that blend AI and security together. If you’re interested in automation and machine learning, this field is worth exploring.</p><p>AI Security Specialists: Protect AI systems from adversarial attacks.\nMachine Learning Engineers in Cybersecurity: Use AI to detect and prevent cyber threats.<p>\nCyber Threat Hunters: Proactively search for hidden threats using AI-powered tools.</p></p><h2>\n  \n  \n  How to Get Into Cyber Security?\n</h2><p>**Get Educated but Stay Practical\n**A degree in computer science, IT, or cybersecurity can help, but it’s not the only way in. As a matter of fact, many high-paying cybersecurity professionals don’t have a degree. What really matters is hands-on experience. If you’re in school, take on internships or personal projects. If you’re learning online, try cybersecurity competitions, hackathons, or Capture the Flag (CTF) challenges to sharpen your skills.</p><p>**Get Certified\n**Certifications help you prove your skills to employers. Start with beginner-friendly ones like CompTIA Security+ if you’re new. As you gain experience, aim for advanced certs like CISSP or CEH. These can boost your credibility and job prospects.</p><p>**Build a Strong Network\n**Cybersecurity is a small world, and who you know can open doors. Join online forums, attend security conferences, and connect with professionals on LinkedIn. Contributing to open-source projects or writing about security topics can also help you stand out.</p><p>**Show Off Your Skills\n**Create a portfolio showcasing your cybersecurity projects, reports on vulnerabilities you’ve found, or write-ups on CTF challenges you’ve solved. A strong online presence, whether through GitHub, LinkedIn, or a personal website, can attract job opportunities.</p><p>**Never Stop Learning\n**Cyber threats evolve constantly, and so should your knowledge. Keep up with the latest trends by following cybersecurity blogs, taking online courses, and experimenting with new tools. The best cybersecurity professionals are the ones who never stop improving.</p><p>**Helpful Tips from Industry Professionals\n**Success in cybersecurity goes beyond technical skills. Start by securing your own digital footprint, employers often check candidates’ online presence. Strong passwords, two-factor authentication, and a clean professional profile can make a difference.</p><blockquote><p>Certifications help you stand out, but they aren’t enough on their own. Networking is just as important. Engage with professionals, join cybersecurity communities, and attend events to open new opportunities.\nStay updated on new threats and technologies, and diversify your skills by learning programming, networking, and cloud security. Finally, take initiative. Participate in Capture The Flag competitions, contribute to open-source projects, and always push yourself to improve. Passion and continuous learning will set you apart in this competitive field.</p></blockquote><p>Important Note:\nThe information in this article is based on my own research and may not be entirely accurate. While I’ve done my best to ensure the accuracy of the data, there may be errors or updates that I have overlooked. I’m a student who enjoys writing on topics related to software engineering and cybersecurity. I have a lot to offer, and I’m confident that I will make a significant impact in the field. I encourage readers to verify the information independently and make any necessary adjustments. If you have any questions, suggestions, or corrections, please don’t hesitate to reach out and talk to me. I welcome feedback and am more than happy to make revisions if needed.</p>","contentLength":7988,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Kicking Off My 6-Month DevOps Learning Journey","url":"https://dev.to/princemalikk/kicking-off-my-6-month-devops-learning-journey-32fb","date":1739714066,"author":"Prince Malik","guid":636,"unread":true,"content":"<p>🚀 Kicking Off My 6-Month DevOps Learning Journey\nHey everyone! 👋</p><p>I’m thrilled to share that I’ve started my 6-month DevOps learning journey! 🎯 Each week, I’ll be documenting my progress, sharing key learnings, and diving into hands-on experiences.</p><p>I’ll be posting updates here on Dev.to, as well as on Hashnode, LinkedIn and GitHub, to make this journey more interactive and engaging. My goal is to explore DevOps tools, automation, and best practices while building a strong community of learners.</p><p>🔹 If you’re also learning DevOps or working in this field, let’s connect! 🚀\n🔹 Drop your thoughts, suggestions, or experiences in the comments! 💡</p><p>Stay tuned for my weekly updates, and let’s learn together!</p>","contentLength":734,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Preparing for Senior PHP Symfony Developer role at Clubee","url":"https://dev.to/gurachek/preparing-for-senior-php-symfony-developer-role-at-clubee-2can","date":1739713730,"author":"Valera Gurachek","guid":627,"unread":true,"content":"<p>Please take a look and let's discuss the preparation process.</p>","contentLength":61,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux User Management Basics","url":"https://dev.to/madhushan/linux-user-management-basics-me0","date":1739711853,"author":"madhushan  Herath","guid":626,"unread":true,"content":"<p>If you're using Linux, knowing how to manage users is super important, especially if you're running a server or sharing your system with others. It helps you keep things organized and secure. Let’s go over the basics of handling users in Linux!</p><p>Adding a new user is easy! If you're on Ubuntu/Debian, use this:</p><p>This sets up a new user with a home directory and some basic settings.</p><p>For other Linux versions, try:</p><p>The  flag makes sure the home directory is created.</p><h2>\n  \n  \n  2. Setting and Changing User Passwords\n</h2><p>After adding a user, they’ll need a password:</p><p>If you need to change your own password:</p><p>To remove a user but keep their files:</p><p>If you want to remove the user and their home directory:</p><div><pre><code>deluser  username\n</code></pre></div><p>For RedHat-based systems:</p><p>Groups help organize users with similar permissions. To add a user to a group:</p><div><pre><code>usermod  groupname username\n</code></pre></div><p>To see which groups a user is in:</p><p>To remove a user from a group:</p><div><pre><code>gpasswd  username groupname\n</code></pre></div><h2>\n  \n  \n  5. Understanding Linux File Permissions\n</h2><p>Files and folders have permissions that control who can read, write, or run them. To check permissions:</p><p>You'll see something like this:</p><div><pre><code>-rw-r--r-- 1 user group 1234 Jan 1 12:00 file.txt\n</code></pre></div><ul><li>The first character () means it's a file ( means directory).</li><li>The next three () are for the owner.</li><li>The next three () are for the group.</li><li>The last three () are for others.</li></ul><p>To switch to another user:</p><p>Or use  to run commands as another user:</p>","contentLength":1394,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Essential Technologies for Software Engineers Beyond Core Development","url":"https://dev.to/kalana250/essential-technologies-for-software-engineers-beyond-core-development-1mig","date":1739711752,"author":"Kalana Heshan","guid":625,"unread":true,"content":"<p>Software engineering is a dynamic and ever-evolving field that demands continuous learning and adaptation. While mastering programming languages and frameworks is crucial, the modern software engineer must explore and integrate a broad set of technologies to enhance efficiency, security, scalability, and innovation. This article delves into key technologies beyond traditional software development that can significantly benefit engineers in their careers.</p><h2>\n  \n  \n  1. <strong>Cloud Computing: The Backbone of Modern Applications</strong></h2><p>Cloud computing has transformed how applications are developed, deployed, and maintained. Instead of relying on traditional on-premise servers, businesses now leverage cloud platforms to achieve scalability, cost-efficiency, and high availability.</p><ul><li><strong>Amazon Web Services (AWS)</strong> – The industry leader offering services for computing, storage, networking, and AI-driven applications.</li><li> – A cloud platform popular in enterprise environments, seamlessly integrated with Microsoft products like Office 365 and Active Directory.</li><li><strong>Google Cloud Platform (GCP)</strong> – Preferred for data analytics, AI-powered applications, and container orchestration with Kubernetes.</li></ul><h3><strong>Why Cloud Computing Matters:</strong></h3><ul><li> Cloud platforms allow developers to scale applications dynamically based on demand.</li><li> Pay-as-you-go pricing ensures businesses only pay for the resources they use.</li><li><strong>Disaster Recovery &amp; High Availability:</strong> Cloud infrastructure provides redundancy, ensuring that applications remain available even during failures.</li></ul><h2>\n  \n  \n  2. <strong>DevOps &amp; CI/CD: Streamlining Software Development</strong></h2><p>DevOps combines development and operations to improve collaboration, automate workflows, and enhance software delivery speed. Continuous Integration and Continuous Deployment (CI/CD) pipelines further automate the testing and deployment process.</p><ul><li> – Containerization tools that enable software to run consistently across environments.</li><li><strong>Jenkins, GitHub Actions, GitLab CI/CD</strong> – Tools for automating software builds, testing, and deployments.</li><li> – Infrastructure as Code (IaC) tools that automate provisioning and configuration management.</li></ul><h3><strong>Why DevOps &amp; CI/CD Matter:</strong></h3><ul><li><strong>Faster Software Releases:</strong> Automated deployment pipelines reduce manual errors and accelerate time-to-market.</li><li> Monitoring and logging tools ensure application performance remains optimal.</li><li> Encourages close coordination between developers and operations teams.</li></ul><h2>\n  \n  \n  3. <strong>Cybersecurity: Protecting Digital Assets</strong></h2><p>With cyber threats becoming more sophisticated, software engineers must integrate security measures from the initial development stages to prevent data breaches and system vulnerabilities.</p><ul><li> – A security framework identifying the most critical web application security risks.</li><li> – Tools for penetration testing and ethical hacking.</li><li> – Authentication and authorization protocols to secure applications.</li></ul><h3><strong>Why Cybersecurity Matters:</strong></h3><ul><li> Secure applications prevent sensitive user information from being exposed.</li><li> Adhering to standards like GDPR and HIPAA ensures legal protection.</li><li> Secure coding practices reduce attack surfaces and vulnerabilities.</li></ul><h2>\n  \n  \n  4. <strong>AI &amp; Machine Learning: Powering Intelligent Applications</strong></h2><p>Artificial Intelligence (AI) and Machine Learning (ML) are revolutionizing industries by enabling automation, data-driven decision-making, and advanced user interactions.</p><ul><li> – Frameworks for developing machine learning models.</li><li> – A beginner-friendly toolkit for AI-powered data analysis.</li><li> – Platforms driving innovations in natural language processing (NLP) and AI-driven automation.</li></ul><h3><strong>Why AI &amp; Machine Learning Matter:</strong></h3><ul><li> AI-powered solutions reduce human effort in repetitive tasks.</li><li> Machine learning enhances decision-making by analyzing vast datasets.</li><li><strong>Enhanced User Experience:</strong> AI-driven chatbots, recommendation engines, and predictive analytics improve software applications.</li></ul><h2>\n  \n  \n  5. <strong>Blockchain &amp; Web3: Building Decentralized Applications</strong></h2><p>Blockchain technology and Web3 innovations are reshaping finance, supply chains, and digital ownership, making them valuable areas of expertise for software engineers.</p><ul><li> – Smart contract development on decentralized networks.</li><li> – Enterprise blockchain framework for business applications.</li><li> – Distributed file storage system enhancing data security and decentralization.</li></ul><ul><li> Blockchain ensures data immutability and prevents fraud.</li><li> Reduces reliance on intermediaries in transactions.</li><li> Automates processes with self-executing agreements.</li></ul><h2>\n  \n  \n  6. <strong>Software Architecture &amp; System Design: Building Scalable Applications</strong></h2><p>As software applications grow, designing them for scalability, maintainability, and efficiency is critical.</p><h3><strong>Key Technologies &amp; Concepts:</strong></h3><ul><li><strong>Microservices &amp; Monolithic Architectures</strong> – Choosing the right structure for an application.</li><li> – Efficient data communication between services.</li><li><strong>Event-Driven Architecture (Kafka, RabbitMQ)</strong> – Enhancing real-time data processing.</li></ul><h3><strong>Why Software Architecture Matters:</strong></h3><ul><li><strong>Performance Optimization:</strong> Ensures applications run efficiently under heavy loads.</li><li> Reduces the risk of system-wide failures.</li><li> Enables applications to grow seamlessly as demand increases.</li></ul><h2>\n  \n  \n  7. <strong>Internet of Things (IoT): Connecting the Physical and Digital Worlds</strong></h2><p>IoT technology enables real-time data exchange between smart devices, revolutionizing industries such as healthcare, manufacturing, and logistics.</p><ul><li> – Hardware platforms for prototyping IoT devices.</li><li> – Communication and data processing frameworks.</li><li><strong>AWS IoT &amp; Google Cloud IoT</strong> – Cloud services for managing IoT networks.</li></ul><ul><li> IoT applications enhance efficiency in various sectors.</li><li> Enables smart systems in homes, cities, and industries.</li><li> Reduces equipment failures with AI-driven insights.</li></ul><p>The field of software engineering extends beyond programming languages. Mastering emerging technologies such as <strong>cloud computing, DevOps, cybersecurity, AI, blockchain, system design, IoT, and AR/VR</strong> can make software engineers more versatile and valuable. By staying updated with these advancements, engineers can design secure, scalable, and innovative applications that drive the future of technology.</p>","contentLength":6043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Glue vs AWS Lambda: Comparativa Serverless para Ingeniería de Datos en AWS","url":"https://dev.to/jlarizar/aws-glue-vs-aws-lambda-comparativa-serverless-para-ingenieria-de-datos-en-aws-108f","date":1739711580,"author":"Jose Luis Ariza","guid":624,"unread":true,"content":"<p>El mundo Cloud ha revolucionado la forma en que las empresas gestionan y analizan sus datos. <strong>Amazon Web Services (AWS)</strong> ofrece diversas herramientas  que permiten a los ingenieros de datos trabajar sin preocuparse por la infraestructura subyacente. Entre estas herramientas,  destacan por su versatilidad y eficiencia. Aunque ambos servicios comparten el principio de la ejecución sin servidores, tienen objetivos y funcionalidades distintas. En este artículo, exploraremos las diferencias clave, sus ventajas y limitaciones, y cuándo elegir cada uno.</p><p>AWS Glue es un servicio administrado de integración de datos que facilita la preparación, transformación y carga de datos (ETL). Su función principal es conectar diversas fuentes de datos, organizarlas y prepararlas para su posterior análisis. AWS Glue es especialmente útil en escenarios donde se manejan grandes volúmenes de información y se necesita automatizar procesos repetitivos.</p><p><strong>Principales componentes de AWS Glue</strong></p><blockquote><ul><li> Repositorio centralizado que almacena los metadatos y define la estructura de las fuentes de datos.</li><li> Programas que examinan las fuentes de datos, detectan su estructura y actualizan automáticamente el catálogo.</li><li> Procesos que ejecutan las transformaciones de datos, programados en Python o Scala.</li><li> Reglas que activan la ejecución de trabajos según un cronograma o la ocurrencia de ciertos eventos.</li><li> Entornos interactivos para escribir y probar el código ETL.</li></ul></blockquote><blockquote><ul><li><strong>Automatización del Proceso ETL:</strong> Reduce significativamente el tiempo necesario para preparar datos gracias a la detección automática de esquemas.</li><li> Se conecta de manera sencilla con otros servicios de AWS, como Amazon S3, Amazon Redshift y AWS Athena.</li><li> Ajusta la capacidad de procesamiento según el volumen de datos.</li><li> Elimina la necesidad de gestionar infraestructura.</li></ul></blockquote><blockquote><ul><li> Al trabajar con un entorno distribuido basado en Apache Spark, el inicio de los trabajos puede tardar algunos minutos.</li><li><strong>Soporte de Lenguajes Limitado:</strong> Solo admite Python y Scala, lo que puede ser un inconveniente si se utilizan otros lenguajes en el ecosistema.</li><li> Para trabajos esporádicos o de poca carga, el costo puede resultar elevado en comparación con otras alternativas.</li></ul></blockquote><p><strong>Ejemplo Diagrama de Arquitectura - Pipeline Glue</strong></p><p>AWS Lambda es un servicio serverless que permite ejecutar código en respuesta a eventos específicos sin la necesidad de aprovisionar ni gestionar servidores. Su uso es ideal para aplicaciones que requieren respuestas rápidas a eventos en tiempo real, como la carga de archivos en S3 o el procesamiento de mensajes de una cola.</p><p><strong>Principales componentes de AWS Lambda</strong></p><blockquote><ul><li> Fragmentos de código que se ejecutan al activarse un evento.</li><li> Eventos que inician la ejecución de las funciones, como cambios en bases de datos, flujos de eventos o solicitudes a través de API Gateway.</li><li> Elementos reutilizables que permiten compartir bibliotecas y configuraciones entre funciones.</li><li> Recibe y procesa eventos desde diversas fuentes, facilitando la construcción de arquitecturas basadas en eventos.</li></ul></blockquote><blockquote><p><strong>Ejecución Basada en Eventos:</strong> Se activa automáticamente al detectarse un evento relevante, eliminando la necesidad de supervisión constante.<strong>Soporte para Múltiples Lenguajes:</strong> Compatible con Python, Node.js, Java, Go, Ruby y otros. Escala horizontalmente para manejar picos de demanda sin intervención manual. Se paga únicamente por el tiempo de ejecución y el número de invocaciones.</p></blockquote><blockquote><ul><li><strong>Tiempo Máximo de Ejecución:</strong> Las funciones no pueden superar los 15 minutos de ejecución, lo que limita su aplicación en procesos extensos.</li><li> La memoria y el tiempo de procesamiento tienen límites que podrían afectar cargas intensivas.</li><li> Al tratarse de un servicio sin estado, se necesita recurrir a otras herramientas, como DynamoDB, para almacenar información entre invocaciones.</li></ul></blockquote><p><strong>Ejemplo Diagrama de Arquitectura - Pipeline Lambda</strong></p><p><strong>Comparativa entre AWS Glue y AWS Lambda</strong></p><p>Aunque ambos servicios pertenecen al ecosistema serverless de AWS, tienen aplicaciones distintas. La siguiente tabla resume las diferencias más relevantes:</p><p>AWS Glue es la mejor opción cuando se necesita:</p><ul><li>Realizar transformaciones complejas y procesamiento por lotes.</li><li>Gestionar y organizar metadatos para análisis posteriores.</li><li>Automatizar tareas de integración de datos en proyectos de Big Data.</li><li>Trabajar con datos almacenados en Amazon S3, Redshift o Data Lakes.</li></ul><p>\nUna empresa que procesa información de ventas históricas para generar reportes mensuales podría usar AWS Glue para consolidar, limpiar y transformar estos datos de manera eficiente.</p><p>AWS Lambda es más adecuado cuando:</p><ul><li>Se requieren respuestas inmediatas a eventos en tiempo real.</li><li>Se necesita construir microservicios ligeros y altamente escalables.</li><li>Se desea automatizar tareas basadas en eventos sin preocuparse por la infraestructura.</li><li>Se implementan flujos de trabajo orquestados con Step Functions.</li></ul><p>\nUna aplicación que notifica en tiempo real a los clientes cada vez que se realiza una compra puede usar Lambda para procesar los eventos generados por las transacciones.</p><p>AWS Glue y AWS Lambda son herramientas poderosas en el ecosistema de AWS, pero están orientadas a tareas diferentes. Mientras AWS Glue se destaca en el procesamiento y preparación de datos a gran escala, AWS Lambda es ideal para ejecutar tareas rápidas en respuesta a eventos. La elección entre uno u otro dependerá de los requisitos específicos de tu proyecto, el volumen de datos y la frecuencia de las operaciones. Si tu objetivo es transformar y organizar datos en procesos ETL, AWS Glue es la opción indicada. Por otro lado, si buscas una ejecución inmediata basada en eventos, AWS Lambda será tu mejor elección.</p>","contentLength":5629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Week 12 Recap of #100DaysOfCode","url":"https://dev.to/lymah/week-12-recap-of-100daysofcode-21h7","date":1739710478,"author":"Lymah","guid":623,"unread":true,"content":"<h2>\n  \n  \n  Firebase Integration, API Enhancements &amp; DSA Learning\n</h2><p>This week, I focused on enhancing my Expense Tracker App by integrating Firebase for receipt storage and authentication, improving API functionality, and diving deeper into data structures and algorithms (DSA).</p><h2>\n  \n  \n  Key Developments in the Expense Tracker App\n</h2><p>🔹 Integrated Firebase for Receipt Storage\nI implemented a workflow where users can upload receipts from the frontend, which are then processed by the backend and stored securely in Firebase Storage.</p><p>🔹 Enhanced API &amp; Report Generation</p><ul><li>Improved API handlers to ensure efficient communication between the frontend and backend.</li><li>Worked on report generation to help users analyze their expenses better.</li><li><p>Faced a form handling issue while testing report generation but successfully troubleshot and fixed it.\n🔹 Implemented Firebase Authentication</p></li><li><p>Set up Firebase Authentication for user registration and login.</p></li><li><p>Developed authentication middleware to manage secure access.</p></li><li><p>Organized and refined API routes for better structure and efficiency.</p></li></ul><p>## Learning &amp; Problem-Solving</p><p>Aside from working on the app, I spent time improving my DSA skills and solving LeetCode problems:</p><ul><li>Insert Delete Get Random – Practiced hashmaps and randomized data structures.</li><li>Product of Array Except Self – Strengthened understanding of prefix sum techniques.</li></ul><p>With the major backend integrations complete, my focus will now be on:</p><ul><li>Finalizing app testing and fixing any edge cases.</li><li>Optimizing performance and deployment readiness.</li><li>Continuing DSA practice to sharpen my problem-solving skills.</li></ul><p>Week 12 was packed with important features, debugging, and learning! 🚀 Excited for the next phase of this journey!</p>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GIT haqida ma'lumot","url":"https://dev.to/sunnat_qayumov/git-8a","date":1739710429,"author":"Sunnat Qayumov","guid":605,"unread":true,"content":"<p> bu loyihalar saqlash uchun va dasturchilar birga loyihalar ustida jamoaviy ishlashida kerak bo'ladigan platforma(<code>bulutli texnologiya - cloud technology</code>).</p><p> - bu dasturiy ta'minot versiyalarni nazorat qiluvchi tizim('version control system`). Undan dasturchilar kodni kuzatish, boshqarish va jamoa bilan ishlash uchun foydalanadilar.</p><p><strong>Git o'zi qanday ishlaydi?</strong> kodni  (repo) deb nomlanuvchi markaziy joyda saqlaydi. Dasturchilar ushbu repodan nusxa olib, unda ishlashlari va keyin o‘z o‘zgarishlarini qayta yuklashlari mumkin. Git quyidagi asosiy operatsiyalarni bajarishga imkon beradi:</p><ul><li> -&gt; yangi Git repository yaratish.</li><li> -&gt; mavjud repository’ni nusxalash.</li><li> -&gt; o‘zgarishlarni kuzatish uchun fayllarni qo‘shish.</li><li><code>Git commit -m \"File name\"</code> -&gt; komentariya qoldirish.</li><li> -&gt; lokal commitlarni serverga ulash.</li><li> -&gt; serverdagi so‘nggi o‘zgarishlarni olish.</li><li> -&gt; ikkita kod tarmog‘ini birlashtirish.</li><li> -&gt; ortiqcha fayllarni olib tashlash. </li></ul>","contentLength":931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Getting Started with Linux: Key Lessons from the Linux Foundation","url":"https://dev.to/madhushan/getting-started-with-linux-key-lessons-from-the-linux-foundation-1g62","date":1739709768,"author":"madhushan  Herath","guid":622,"unread":true,"content":"<p>Linux is an awesome, open-source operating system that powers everything from servers to smartphones. If you're new to Linux and want to get started, the Linux Foundation has some great courses to help you out. Here are some key takeaways from a beginner-friendly Linux Foundation course.</p><ol><li>Understanding Linux Basics</li></ol><ul><li><p>Linux is based on Unix principles, which focus on keeping things simple and efficient. You'll get to know:</p></li><li><p>The Linux file system structure, including folders like /home, /var, /etc, and /bin</p></li><li><p>Handy command-line commands to move around and manage files</p></li><li><p>How the Linux kernel works and talks to your hardware and apps</p></li><li><p>The different Linux distributions and what makes them unique</p></li></ul><ol><li>Mastering the Command Line</li></ol><p>The command line is where all the magic happens in Linux. Here are some must-know commands:</p><ul><li>ls – Lists files and folders (use ls -l for extra details)</li><li>cd – Moves between directories (cd .. takes you up a level)</li><li>cp – Copies files (cp file1 file2 to make a copy)</li><li>mv – Moves or renames files (mv oldname newname to rename a file)</li><li>rm – Deletes files (rm -r removes folders too)</li><li>grep – Searches for words in files (grep 'keyword' filename)</li><li>cat, less, and more – Show the contents of files quickly</li></ul><ol><li>Managing Users and Permissions</li></ol><p>To keep things secure, Linux has a solid user and permission system. You'll learn about:</p><ul><li>Adding and managing users (adduser, passwd, usermod, deluser)</li><li>Changing file permissions (chmod, chown, ls -l to check permissions)</li><li>Managing groups (usermod -aG groupname username to add users to groups)</li><li>How permission values like rwxr-xr-- and numbers like chmod 755 work</li></ul><ol><li>Installing and Managing Software</li></ol><p>Installing software on Linux is pretty straightforward. Here’s what you’ll use:</p><ul><li>apt (for Debian/Ubuntu-based systems) – Example: apt install package_name</li><li>yum or dnf (for RHEL/CentOS-based systems) – Used for software management</li><li>snap and flatpak – Universal package managers for different distros</li><li>How to build and install software from source if needed</li></ul><ol><li>System Monitoring and Process Management</li></ol><p>Keeping your Linux system running smoothly means monitoring it. Some useful commands include:</p><ul><li>top and htop – Show what’s running on your system in real time</li><li>ps – Check running processes (ps aux gives detailed info)</li><li>kill – Stop processes (kill -9 PID for force quitting)</li><li>df and du – Check disk space (df -h makes it human-readable)</li><li>free -m – See how much memory is being used</li><li>uptime – Find out how long your system has been running</li></ul><p>Want to automate boring tasks? Shell scripting makes life easier. You'll get into:</p><ul><li>Writing simple Bash scripts using #!/bin/bash</li><li>Using loops (for, while) and conditions (if, else)</li><li>Setting up automated tasks with cron (crontab -e to edit cron jobs)</li><li>Working with variables and user inputs (read command) to make scripts interactive</li></ul><ol><li>Networking and Security Basics</li></ol><ul><li>Linux is a networking powerhouse. You'll learn how to:</li><li>Check network connectivity (ping, netstat, ip a for IP details)</li><li>Set up firewalls (iptables, ufw for managing access rules)</li><li>Securely connect to remote systems with SSH (ssh user@hostname, scp for file transfers)</li><li>View active connections (netstat -tulnp shows open ports and services)</li><li>Strengthen security with tools like fail2ban and SELinux</li></ul>","contentLength":3204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From a Curious Student to Meeting Satya Nadella: My Journey with the Azure Developer Community","url":"https://dev.to/vivek0712/from-a-curious-student-to-meeting-satya-nadella-my-journey-with-the-azure-developer-community-39g3","date":1739709743,"author":"Vivek0712","guid":621,"unread":true,"content":"<p>In late 2020, I was just a curious college student with a thirst for knowledge about cloud technologies and artificial intelligence. My journey began with a decision that many would call impulsive but turned out to be life-changing. I traveled over 500 kilometers just to attend a Microsoft Azure event. That one step of courage and curiosity marked the beginning of an extraordinary adventure.</p><h2>\n  \n  \n  🚀 The First Leap of Faith\n</h2><p>I still remember the excitement mixed with nervousness as I entered the event venue. I knew no one there, but the warmth of the Azure Developer Community made me feel at home. I met developers, architects, and enthusiasts who were more than happy to share their knowledge. It was this openness that fueled my passion for cloud technologies.</p><h2>\n  \n  \n  🌱 Community: The Catalyst for Growth\n</h2><p>The Azure Developer Community officially began in late 2020 and quickly became my launchpad for growth. It didn't just teach me technical skills; it taught me the essence of collaboration and continuous learning. I actively sought mentorship, participated in discussions, and never hesitated to ask questions. Over time, my efforts bore fruit. I was invited to speak at small meetups, and soon, these meetups turned into large-scale events and conferences.</p><p>Fast forward to today: I've spoken, hosted, or organized more than 300 events across the globe. These events range from intimate bootcamps to large-scale conferences with thousands of attendees. I even had the privilege of winning the AI ODC Challenge in both 2024 and 2025, which opened doors to new opportunities and collaborations.</p><h2>\n  \n  \n  🤝 Meeting the Visionary: Satya Nadella\n</h2><p>One of the most surreal moments of my journey was meeting Satya Nadella, the CEO of Microsoft, not once but twice. The moment I shook hands with him, I felt the weight of years of learning and hard work crystallizing into a single, unforgettable experience. Getting photographed with him was a dream come true, and the picture was later featured in Pune Chaak's keynote session during the Microsoft AI Tour 2024 — a moment of recognition that made me realize the impact of my work and the power of community-driven growth.</p><h2>\n  \n  \n  ❤️ Giving Back to the Community\n</h2><p>The community that once nurtured me is now the one I contribute to passionately. I’ve mentored aspiring developers, organized countless bootcamps, and supported the growth of the Azure Developer Community in Tamil Nadu, which has now grown to over 5,000 members in record time.</p><h2>\n  \n  \n  🗣️ Talks and Sessions: A Chronicle of Knowledge Sharing\n</h2><p>Throughout my journey, I've delivered numerous talks and conducted hands-on sessions. Here are some notable ones related to Azure and the Azure Developer Community:</p><ol><li><p><strong>Image Data Analytics using Python</strong> – Mepco Schlenk Engineering College  </p></li><li><p><strong>Deep Dive into Microsoft Azure Automated Machine Learning</strong> – Global Azure Datafest 2020  </p></li><li><p><strong>Machine Learning Pipelines in AzureML</strong> – Azure Developer Community Tamil Nadu  </p></li><li><p><strong>Demo on Azure Cognitive Services</strong> – Azure ML Series  </p></li><li><p><strong>Training &amp; Deploying ML Models in Azure</strong> – Azure Developer Community Mumbai  </p></li><li><p><strong>Exploring Azure Cognitive Services</strong> – Azure Developer Community Raipur  </p></li><li><p><strong>Getting started with Azure AutoML</strong> – Azure Developer Community Chandigarh  </p></li><li><p><strong>Deploying Python Apps to AKS</strong> – Azure Developer Community Tamil Nadu  </p></li></ol><ol><li><strong>Attendance System with Face Mask Detection using Azure Face API</strong><em>(Published: Feb 15, 2024)</em></li><li><strong>Predict NYC Taxi Fares | End-to-end Azure MLOps with GitHub Actions</strong><em>(Published: Feb 14, 2024)</em></li><li><strong>Build a Production LLM-Powered QC Visual Inspection System Using Azure AI Foundry</strong><em>(Published: Feb 13, 2024)</em></li><li><strong>Analyse Customer Sentiment using Azure Data Factory pipeline</strong><em>(Published: Feb 12, 2024)</em></li><li><strong>Electronics-Datasheet-GPT: A Step-by-Step Guide to Building Your Own Datasheet Q&amp;A AI Agent</strong><em>(Published: Feb 11, 2024)</em></li><li><strong>Experimenting with DeepSeek-R1 Model in VS Code Hosted via GitHub Models</strong><em>(Published: Feb 10, 2024)</em></li><li><strong>How AWS GenAI boosted my day-to-day Productivity</strong><em>(Published: May 31, 2024)</em></li><li><strong>What If Amazon Prime A/B Tests IPL Broadcasting?</strong><em>(Published: Feb 21, 2024)</em></li><li><strong>Speed-Dataing for hackers!</strong></li><li><strong>Transforming Madurai | Tech, Community and more...</strong><em>(Published: Apr 17, 2023)</em></li><li><strong>AWS Community Day Hyderabad 2023 | Experience with Vivek</strong><em>(Published: Mar 25, 2023)</em></li><li><strong>Blur Personal Identifiable Information (PII) in Text, Images and Videos</strong><em>(Published: Jun 10, 2022)</em></li><li><strong>Adding Chaos to ML Compute Targets</strong><em>(Published: Feb 15, 2022)</em><em>(Azure Architecture Series)</em></li><li><strong>Introducing Chaos Engineering to Machine Learning deployments</strong><em>(Published: Feb 12, 2022)</em><em>(Azure Architecture Series)</em></li><li><strong>How FORMULA 1 insights are powered by AWS</strong><em>(Published: Oct 20, 2021)</em></li><li><strong>Hosting Python Packages in Azure DevOps</strong><em>(Published: Aug 31, 2021)</em></li><li><strong>Training Machine Learning models in AzureML</strong><em>(Published: Aug 22, 2021)</em><em>(Azure Machine Learning Series)</em></li><li><strong>Getting started with Azure Machine Learning</strong><em>(Published: Aug 20, 2021)</em><em>(Azure Machine Learning Series)</em></li><li><strong>Introduction to Machine Learning</strong><em>(Published: Aug 19, 2021)</em><em>(Azure Machine Learning Series)</em></li><li><strong>Fine-tuning the performance of the DeepRacer model</strong><em>(Published: Jul 31, 2021)</em><em>(AWS Community Builders - DeepRacer)</em></li><li><strong>Create, Train, Race your first AWS DeepRacer Model</strong><em>(Published: Jul 24, 2021)</em><em>(AWS Community Builders - DeepRacer)</em></li><li><strong>Detecting and Visualizing Twitter Sentiment during COVID-19 Pandemic using AWS Comprehend and Quicksight</strong><em>(Published: Jul 19, 2021)</em></li><li><strong>Creating COVID19 stats newsletter using Azure Function Apps</strong><em>(Published: Apr 14, 2021)</em></li><li><strong>Literature Text Translation &amp; Audio Synthesis using Microsoft Azure Cognitive Services</strong></li></ol><p> All posts are published on .</p><h2>\n  \n  \n  🎓 Certifications: The Foundation of My Technical Knowledge\n</h2><p>My passion for continuous learning is reflected in the certifications I've earned:</p><ul><li>Microsoft Certified Azure Developer Associate (AZ-204)</li><li>Microsoft Certified Azure Data Scientist Associate (DP-100)</li><li>Microsoft Certified Azure AI Engineer Associate (AI-100)</li><li>Microsoft Certified Azure Data Engineer Associate (DP-200 &amp; DP-201) </li><li>Microsoft Certified Azure Fundamentals (AZ-900)</li></ul><h2>\n  \n  \n  🌐 VickyBytes: A New Chapter\n</h2><p>As I reflect on this journey, I’m thrilled to share that I'm working closely with Microsoft's leadership to build a new platform called <a href=\"https://vickybytes.com\" rel=\"noopener noreferrer\">VickyBytes</a>. This initiative aims to provide developers with curated, high-quality content to stay ahead in the ever-evolving tech landscape. The idea is simple yet powerful: leverage the strength of community to empower developers worldwide.</p><h2>\n  \n  \n  👨‍💼 My Professional Journey: A Snapshot\n</h2><ul><li><em>(YourStory Tech30 startup winner; Raised $3.5M!)</em></li><li><em>(Youngest ever!), AWS Hero of the Year (APJ 2024)</em></li><li> recipient at AWS Community Leaders Summit 2022</li><li> Azure Developer Community TN </li><li> AWS User Group Madurai <em>(Nominated as Best UG 2022, 2023 &amp; 2024)</em></li><li><strong>Guinness World Record Holder</strong></li><li> Recent paper at IIM-B &amp; holder of 2 AI patents</li><li>; Participant &amp; Jury</li><li><strong>Mentor | Blogger | Content Creator</strong></li><li><strong>Movie Buff | Go-Kart Racer | Guitar Enthusiast | Professional Reels Sender</strong></li></ul><h2>\n  \n  \n  🌟 Lessons from the Journey\n</h2><ol><li> Growth accelerates when like-minded people collaborate.</li><li> Showing up consistently to learn, share, and contribute can open unimaginable doors.</li><li> The more you give to the community, the more you grow.</li></ol><p>I owe my success to the Azure Developer Community and the incredible people who believed in me when I was just a student with dreams. To every mentor, attendee, and fellow enthusiast:  for being part of my story.</p><h2>\n  \n  \n  Photo Gallery (Random moments clicked!)\n</h2><p>This journey from being a curious student to becoming one of India's recognized AI figures has been nothing short of magical. And if there's one message I'd like to share, it’s this: <strong>take that first step, no matter how uncertain it seems. The community will catch you, guide you, and help you soar.</strong></p><p><strong>#AzureCommunity #AIInnovation #CommunityFirst #VickyBytes</strong></p>","contentLength":7719,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Docker Tips: Essential Tips and Tricks for Developers","url":"https://dev.to/d_thiranjaya_6d3ec4552111/docker-tips-essential-tips-and-tricks-for-developers-1hl8","date":1739708844,"author":"Pawani Madushika","guid":600,"unread":true,"content":"<h2>\n  \n  \n  Advanced Docker Tips for Modern Development (2025)\n</h2><p>In the rapidly evolving world of software development, staying ahead of the curve is crucial. Docker, a renowned containerization platform, has revolutionized the way we build, deploy, and manage applications. To optimize your Docker development workflow, this comprehensive guide delves into advanced techniques, best practices, and performance optimization strategies that experienced developers may not be aware of.</p><h2>\n  \n  \n  Latest Advanced Techniques\n</h2><h3>\n  \n  \n  1. Podman: Unleashing Container Management Potential\n</h3><p>Traditional Docker commands can be replaced with the lightweight and sandboxed  utility. This alternative offers enhanced security, reduced resource consumption, and simplified container management:</p><div><pre><code># Create a container using podman\npodman create --name my-container my-image\n</code></pre></div><h3>\n  \n  \n  2. BuildKit: Accelerating Image Building\n</h3><p>BuildKit accelerates image building by leveraging incremental builds, caching, and parallel execution. By integrating BuildKit into your Dockerfile, you can significantly reduce build times:</p><div><pre><code># Dockerfile using BuildKit\nFROM my-base-image as build-stage\nRUN apk add ... # Use BuildKit syntax\n</code></pre></div><h3>\n  \n  \n  3. Swarm Mode: Distributed Container Orchestration\n</h3><p>Swarm mode enables seamless scaling of containerized applications across multiple hosts. It provides advanced load balancing, service discovery, and automated container placement:</p><div><pre><code># Initialize a Swarm cluster\ndocker swarm init\n</code></pre></div><h3>\n  \n  \n  1. Multi-Stage Builds for Optimized Images\n</h3><p>Multi-stage builds allow you to create smaller and more efficient images by separating build steps into distinct stages. This minimizes the transfer of unnecessary dependencies:</p><div><pre><code># Multi-stage Dockerfile\nFROM node:lts AS build\nWORKDIR /usr/src/app\nCOPY . .\nRUN npm install --production\n\nFROM nginx:latest\nCOPY --from=build /usr/src/app /usr/share/nginx/html\n</code></pre></div><h3>\n  \n  \n  2. Performance Monitoring with Prometheus and Grafana\n</h3><p>Prometheus and Grafana provide comprehensive monitoring and visualization for Docker environments. They enable real-time tracking of container metrics, performance bottlenecks, and resource usage:</p><div><pre><code># Docker Compose configuration for monitoring\nversion: '3'\nservices:\nprometheus:\nimage: prom/prometheus\nvolumes:\n- ./prometheus.yml:/etc/prometheus/prometheus.yml\ngrafana:\nimage: grafana/grafana\n</code></pre></div><h2>\n  \n  \n  Modern Development Workflow\n</h2><h3>\n  \n  \n  1. CI/CD Automation with GitHub Actions\n</h3><p>GitHub Actions integrates seamlessly with Docker to automate build, test, and deployment processes. This facilitates continuous integration and delivery practices:</p><div><pre><code># GitHub Actions workflow\nname: Docker Build and Deploy\non: [push]\njobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- uses: actions/checkout@v3\n- uses: docker/build-push-action@v3\nwith:\npush: true\n</code></pre></div><h3>\n  \n  \n  2. Remote Container Debugging for Enhanced Efficiency\n</h3><p>Remote debugging allows you to troubleshoot running containers from your local IDE. This simplifies issue identification and resolution:</p><div><pre><code># Enable remote debugging in Docker Compose\nversion: '3'\nservices:\nmy-app:\nimage: my-image\ncommand: [\"/usr/local/bin/debug\", \"--host=0.0.0.0\", \"--port=5678\", \"--wait\", \"true\"]\n</code></pre></div><h3>\n  \n  \n  1. Docker Compose v2: Simplified Configuration and Automation\n</h3><p>Docker Compose v2 introduces a simplified syntax, improved performance, and support for multiple Dockerfiles. It streamlines container orchestration:</p><div><pre><code># Docker Compose v2 configuration\nversion: '2'\nservices:\ndb:\nimage: postgres:latest\nweb:\nimage: my-web-app\ndepends_on:\n- db\n</code></pre></div><h3>\n  \n  \n  2. Docker Desktop for Mac and Windows\n</h3><p>Docker Desktop offers a local development environment for Docker on Mac and Windows. It includes pre-configured services, simplified container management, and easy access to Docker resources.</p><h3>\n  \n  \n  3. Docker Hub: Centralized Image Management\n</h3><p>Docker Hub serves as a central repository for storing and sharing Docker images. It provides access to official images, community contributions, and automated builds.</p><ul><li>Leverage advanced techniques like Podman, BuildKit, and Swarm Mode to enhance your Docker development workflow.</li><li>Implement performance optimization strategies using multi-stage builds and monitoring tools.</li><li>Utilize modern best practices such as remote container debugging and CI/CD automation.</li><li>Explore the latest tools and resources to improve your Docker development experience.</li></ul><p>By implementing these advanced techniques, you can optimize your Docker development process, enhance performance, and stay ahead of the evolving landscape of modern software development.</p>","contentLength":4515,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microinteractions: The Secret Sauce to Enhancing User Experience","url":"https://dev.to/okoye_ndidiamaka_5e3b7d30/microinteractions-the-secret-sauce-to-enhancing-user-experience-220f","date":1739708731,"author":"Okoye Ndidiamaka","guid":604,"unread":true,"content":"<p>In the world of web and app design, it's often the little things that leave the biggest impressions. Microinteractions are those subtle, engaging animations or design elements that guide users, provide feedback, and add personality to your digital product. Think of the gentle bounce of a button after it's clicked or the satisfying swipe animation when archiving an email. These can be minor moments, but they can change a good user experience into an unforgettable one.</p><p>Why Microinteractions Matter</p><p>Microinteractions are not just about aesthetics; they serve very real functional purposes. Here's why they're so important:</p><p>Provide Feedback: They tell users what happened with their action, whether it was a button click or a process that is complete.</p><p>Guide the User: Subtle animations can draw attention to important elements or actions. For example, a blinking cursor in a search bar encourages users to start typing.</p><p>Enhance Engagement: When designed well, microinteractions are pleasurable moments that make users want to come back for more.</p><p>Strengthen Branding: Custom animations unique to your brand can create a memorable identity that stands out.</p><p>Key Elements of Microinteractions</p><p>Every microinteraction is composed of four major parts:</p><p>Trigger: What starts the microinteraction? It can be an explicit user action-like clicking or swiping-or an implicit system event, such as a timer or condition being met.</p><p>Rules: These determine how the microinteraction works. For example, what happens when a button is clicked?</p><p>Feedback: The visual, auditory, or tactile response users get, such as a click sound or a color change.</p><p>Loops and Modes: These are the conditions that determine if the microinteraction repeats or evolves over time.</p><p>Practical Tips for Designing Effective Microinteractions</p><p>Here’s how to implement microinteractions that enhance user experience:</p><ol><li><p>Keep It Simple\nMicrointeractions should be subtle and straightforward. Overloading users with flashy animations can make your design look cluttered and confuse users. Stick to minimal, functional designs.</p></li><li><p>Prioritize Functionality\nEvery animation should have a purpose. Never add microinteractions just because they look good. For instance, a loading spinner tells the user that something is happening, while an animation of a toggle switch explains how it works.</p></li><li><p>Be Consistent\nConsistency in microinteractions is important to create a cohesive user experience. Align animations with your brand identity and ensure they behave predictably across different platforms and devices.</p></li><li><p>Design for Context\nConsider the user's intention and the context of the interaction. For example, an animated behavior may be perfectly fine for a gaming application, but it may look like it doesn't belong if it's part of financial software.</p></li><li><p>Employ the Right Tools\nFigma, Adobe After Effects, or Lottie are some tools that make designing and prototyping microinteractions a breeze. Experiment with these to bring your designs to life.</p></li></ol><p>Real-World Examples of Microinteractions</p><p>Facebook Reactions: Hover on the \"Like\" button and a row of emojis pops out, making it fun and interactive for users to express themselves.</p><p>LinkedIn Endorsements: The animation of skill endorsement is so subtle, yet interactive, sans overwhelming the user.</p><p>Heart Animation by Instagram: Users can get instant feedback when they double-tap a post, which results in an animated heart popping up. </p><p>Common Mistakes to Avoid  While\n micro-interactions are powerful, this is how they can become counter-productive if not implemented well:</p><p>Overcomplicating the Animations: Too-long or flashy animations irritate the users.</p><p>Disregarding Accessibility: Make sure microinteractions are accessible for all users, whether they have visual or motor impairments.</p><p>Overloading the Interface: Too many animations can make performance worse and confuse users.</p><p>The Future of Microinteractions</p><p>As technology evolves, microinteractions will also get more intelligent. With AI and machine learning, in the future, designs may automatically adapt microinteractions based on user behavior, thus creating even more personalized experiences.</p><p>While micro in nature, microinteractions make great impacts on user experiences. By paying attention to minute details, you will also be able to create not just functional but delightful interfaces. So, go live with microinteractions in all your projects and watch your UX soar high.</p><p>What's your favorite microinteraction in the apps or websites that you use? Share it with me in the comments below!</p>","contentLength":4508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Balaji Udayagiri","url":"https://dev.to/balaji_udayagiri_aa6f571e/balaji-udayagiri-14o3","date":1739707687,"author":"Balaji Udayagiri","guid":599,"unread":true,"content":"<p>I'm excited to share my  with you all! 🚀 with 4+ years of experience, I’ve worked on UI/UX, component libraries, AI-driven interfaces, and more.  </p><h3><strong>🔹 What’s in my portfolio?</strong></h3><ul><li>⚡ <strong>Built with Next.js &amp; Tailwind CSS</strong></li><li>🎨 <strong>Showcasing my best projects &amp; UI work</strong></li><li>🚀 <strong>Optimized for performance &amp; accessibility</strong></li><li>🛠️ <strong>Dark mode &amp; interactive animations</strong></li></ul><p>What do you think?<p>\nWould love your feedback, suggestions, or just a quick \"Hi\"! 😊  </p></p><p>🚀 <strong>If you’re a frontend developer, what’s the one thing you love most about building UI?</strong> Let’s chat in the comments! 👇  </p>","contentLength":565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Appointment Form","url":"https://dev.to/preetha_vaishnavi_2b82358/appointment-form-1kdl","date":1739707526,"author":"preetha vaishnavi","guid":598,"unread":true,"content":"<p>Check out this Pen I made!</p>","contentLength":26,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a Slack Clone with Next.js and TailwindCSS - Part Three","url":"https://dev.to/tropicolx/building-a-slack-clone-with-nextjs-and-tailwindcss-part-three-4db6","date":1739707428,"author":"Oluwabusayo Jacobs","guid":597,"unread":true,"content":"<p>In the previous part, we added real-time messaging, rich text formatting, reactions, and file uploads to our Slack clone using the <a href=\"https://getstream.io/chat/sdk/react/\" rel=\"noopener noreferrer\">Stream React Chat SDK</a>.</p><p>In this final part of our series, we’ll add video-calling capabilities to our Slack clone using the <a href=\"https://getstream.io/video/sdk/react/\" rel=\"noopener noreferrer\">Stream React Video and Audio SDK</a>. By the end of this part, users will be able to initiate and join video calls, similar to the \"Huddle\" feature in Slack.</p><h2>\n  \n  \n  Creating a Call Using the Huddle Button\n</h2><p>First, we need a way for users to start and stop video calls easily. Let's create a button for this purpose, which we'll call the .</p><p>Go to the  directory, and create a  file with the following code:</p><div><pre><code></code></pre></div><p>In the component above, we update the current state of the call, including creating a new call, updating the call details, joining the call, or leaving the call using the following functions:</p><ul><li><p>: This function creates a new call by setting the required data, including channel details and workspace members. If there's an existing call, it first leaves that call.</p></li><li><p>: This function toggles the state between joining and leaving the call, depending on whether the user is already in an active call.</p></li></ul><p>We also use the  and  hooks to keep track of the current call state. The button updates its appearance and behavior based on whether a call is ongoing, available, or inactive.</p><p>Next, let's integrate the  into the channel page.</p><p>Navigate to <code>/client/[workspaceId]/[channelId]/page.tsx</code> and update it as follows:</p><div><pre><code></code></pre></div><p>Here, we integrate the  component with the channel toolbar. We also wrap it with the  component to manage the call state.</p><h2><strong>Implementing a Custom Hook for Huddles</strong></h2><p>With the huddle button in place, let's move on to creating a custom hook for our huddle feature. This hook will help manage different aspects of a call, such as toggling the microphone, camera, and screen sharing. It will also track the current state of the call, like whether the user is joining, speaking, or sharing their screen.</p><p>Create a new file in the  directory called  and add the following code:</p><div><pre><code></code></pre></div><ul><li><p>: The  hook uses several hooks from <code>@stream-io/video-react-sdk</code> to manage different aspects of a call. It tracks the current call state, such as the , whether the user is muted, if screen sharing is active, and if the user has permission to perform specific actions.</p></li><li><p><strong>Toggling Microphone, Camera, and Screen Share</strong>: The , , and  functions allow users to control their microphone, camera, and screen sharing during calls.</p></li><li><p>: The  variable helps determine whether the spotlight layout should be used. This typically happens if one participant shares their screen or is pinned.</p></li><li><p>: The  function allows the user to leave the call. If the user has the required permission, they can also end the call for all participants.</p></li></ul><h2><strong>Building and Showing a Call</strong></h2><p>So far, users can create and join calls, but now we want to add the UI for the calls themselves. In this section, we'll integrate the huddle UI components, allowing users to manage video calls within a workspace, similar to Slack's huddle feature.</p><h3><strong>Creating the Call Control Button</strong></h3><p>The first component we need is a call control button. This button will toggle features during a call, like muting the microphone, turning the camera on or off, and more.</p><p>Create a new  file in the  directory, and add the following code:</p><div><pre><code></code></pre></div><p>The  component adjusts its styles based on whether it is  or not. The button changes its background color and hover behavior to indicate the active state, helping users visually understand the button's current state.</p><h3><strong>Customizing the Participant View</strong></h3><p>The main component we’ll use in our video layouts is Stream’s <a href=\"https://getstream.io/video/docs/react/ui-components/core/participant-view/\" rel=\"noopener noreferrer\"></a> component. This component displays the participant’s video, plays their audio, shows their information, and provides action buttons for controls like pinning or muting.</p><p>However, the  default UI doesn't match our design, so we'll override it by creating our own custom UI using the  prop.</p><p>Create a new file in the  directory called  and add the following code:</p><div><pre><code></code></pre></div><p>The  is divided into several components:</p><ul><li><p>: The  component shows details for each participant, such as their name and audio status, making it easy to identify who is currently speaking or pinned.</p></li><li><p><strong>Speech Indicator and Speech Ring</strong>: The  component gives a clear visual cue to users when a participant is speaking. We also included a speech ring effect highlighting the participant's video when speaking.</p></li><li><p>: The <code>DefaultScreenShareOverlay</code> is used when a participant is sharing their screen, providing a visual indication to other participants.</p></li><li><p>: The menu allows participants to perform actions such as pinning or muting themselves or another participant.</p></li></ul><p>With the main call controls and participant view in place, it's time to create the huddle interface. This UI will contain the video feed layout and call controls and ensure the interface responds dynamically to different situations, like toggling between the sidebar and modal views.</p><p>Create a new file in the  directory called  and add the following code:</p><div><pre><code></code></pre></div><p>In the code snippet above:</p><ul><li><p>: We use the  and  to arrange participants based on the context (e.g., sidebar vs. full screen).</p></li><li><p>: We use the  component to toggle the microphone, camera, and screen sharing.</p></li><li><p>: We use the  and  props to handle when the huddle is open in another window or embedded in the sidebar.</p></li></ul><p>Now that we've developed the core user interface for our video calls, it's time to integrate the functionality into a single cohesive component. The huddle component will be responsible for managing the different states of a video call, such as when a user is joining, leaving, or responding to an invitation.</p><p>Create a new file in the  directory called  and add the following code:</p><div><pre><code></code></pre></div><ul><li><p>: The  from the  hook is used to determine the appropriate UI to display. For example, if the call is ringing and wasn't initiated by the current user (), an invitation message, and options to join or decline the call are shown.</p></li><li><p><strong>Joining and Declining a Call</strong>: The  and  functions handle the respective actions. The  function invokes the  method, while the  function invokes  with the option to reject.</p></li><li><p>: Depending on the call state, the component renders different UI sections, including:</p><ul><li>The invitation view, when the call is  but not initiated by the user.</li><li>The call UI when the user has  or is .</li></ul></li></ul><p>The huddle modal component will give an expanded view of the video call interface. It provides a more immersive experience for users when compared to the sidebar view.</p><p>Create a new file in the  directory called  and add the following code:</p><div><pre><code></code></pre></div><p>The component accepts an  prop to control when to display the modal. The modal also uses fixed positioning and includes styles that provide a centered, full-screen overlay.</p><p>With the  and  now created, we can add them to our app.</p><p>Normally, we could place our  component in the . However, the  is a child component in our  file, and layout files persist across routes and maintain the same state.</p><p>This feature would be unsuitable for our needs as we want the current call state to be able to reset and effects to re-run each time a user navigates between channels.</p><p>So instead, we'll use a template file. <a href=\"https://nextjs.org/docs/app/building-your-application/routing/layouts-and-templates#templates\" rel=\"noopener noreferrer\">Templates</a> in Next.js are similar to layouts in that they wrap a child layout or page. However, unlike layouts, they create a new instance of their children each time the user navigates.</p><p>This behavior makes templates perfect for our huddle feature, as it allows us to reset the state and re-synchronize effects seamlessly when the channel or call data changes.</p><p>In the  directory, create a new  file with the following code:</p><div><pre><code></code></pre></div><ul><li><p>: This hook allows us to get the active call. We use it along with the  from our app context to determine which call to display.</p></li><li><p>: We use the  hook to decide the currently active call, giving priority to matching calls from both the current call and channel call.</p></li><li><p>: The  function from  is used to render the Huddle and HuddleModal components to specific parts of the DOM—the sidebar and body. This allows us to dynamically position the components and ensure they integrate smoothly with the UI layout.</p></li><li><p>: The  wrapper is used to pass the active call to our Huddle components, allowing them to manage the video and audio streams effectively.</p></li></ul><p>Before wrapping things up, let's add some important styles to our  file to ensure our huddle interface looks just right:</p><div><pre><code></code></pre></div><p>With this, our huddles are fully functional, allowing users to collaborate seamlessly during video calls!</p><p>We've covered a lot in this three-part series, from setting up the foundation of our Slack clone with a database, authentication, and initial pages, to adding real-time messaging and video calling.</p>","contentLength":8520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hacking Go-TFE and Fetching All Workspaces in Terraform Enterprise: A Journey Through Pagination","url":"https://dev.to/muhammetberdi_jepbarov/hacking-go-tfe-and-fetching-all-workspaces-in-terraform-enterprise-a-journey-through-pagination-4lel","date":1739707176,"author":"Muhammetberdi Jepbarov","guid":596,"unread":true,"content":"<p>Terraform Enterprise (TFE) is a powerful platform for organizations using Terraform at scale. It provides collaboration, governance, and self-service workflows for infrastructure automation. As organizations grow, managing infrastructure becomes increasingly complex, often requiring the ability to programmatically interact with TFE using its robust API.</p><p>Recently, I found myself in a situation where I needed to fetch  for a given organization in TFE. Workspaces, as you may know, are the fundamental units in Terraform where runs occur. Each workspace holds the Terraform state file, which tracks infrastructure resources. For automation and reporting, I needed to pull in the full list of workspaces, but I quickly ran into a seemingly simple yet frustrating problem: .</p><h3>\n  \n  \n  The Goal: Get All Workspaces\n</h3><p>Our goal was clear — we wanted to retrieve every single workspace associated with a particular organization, without worrying about pagination limits. Whether for generating reports, validating configurations, or orchestrating CI/CD workflows, having a full view of all workspaces was crucial.</p><p>The  library by HashiCorp provides an elegant way to interact with TFE’s API. The most straightforward way to list workspaces looks something like this:</p><div><pre><code></code></pre></div><p>However, there was a catch.</p><h3>\n  \n  \n  The Pain Point: Pagination Woes\n</h3><p>When we ran this code, we noticed something odd — it only returned . After digging into the documentation and the source code of the  library, it became clear that the API defaults to a . Even when we explicitly set the page size to the maximum limit of 100, it still only gave us one page of results:</p><div><pre><code></code></pre></div><p>The issue was evident: . This means that if your organization has more than 100 workspaces (which is not uncommon for larger teams), you have to make multiple requests to get all of them.</p><p>But we didn’t want to be bound by pagination. We wanted all the workspaces in one go — a clean, consolidated list.</p><h3>\n  \n  \n  The Solution: Handling Pagination Manually\n</h3><p>To work around the pagination limit, the only solution was to  through the results until all workspaces were fetched. We crafted the following solution:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Breaking Down the Solution\n</h3><p>Let’s walk through the logic step by step:</p><ol><li>: We start with an empty slice to hold all workspaces and set the page number to 1.</li><li>: Inside the  loop, we make requests using the , setting both the page number and the maximum page size.</li><li>: After each request, we append the returned workspaces to our  slice.</li><li>: If the number of workspaces in the current response is less than the page size (meaning we’ve reached the last page), we break out of the loop.</li><li>: Finally, we return the full list of workspaces.</li></ol><p>Fetching all workspaces might seem like a trivial task at first glance, but it highlights a crucial aspect of working with APIs — understanding and handling pagination correctly.</p><p>When dealing with APIs that support pagination:</p><ul><li>Always check default page sizes.</li><li>Understand the max page size allowed.</li><li>Implement proper loop termination to avoid infinite loops.</li><li>Consider API rate limits when making multiple requests.</li></ul><p>In our case, the solution was not only about fetching data but about ensuring the stability and reliability of our automation pipelines.</p><p>In the end, handling pagination in TFE’s API required us to step back, understand the underlying mechanics, and build a robust way to gather all the data we needed. The final solution is now part of our infrastructure tooling, allowing us to work seamlessly with Terraform Enterprise’s workspaces.</p><p>So next time you hit a wall with an API and pagination, remember — it’s not a bug; it’s a feature. Master it, and your automation game will only grow stronger.</p>","contentLength":3683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a Slack Clone with Next.js and TailwindCSS - Part Two","url":"https://dev.to/tropicolx/building-a-slack-clone-with-nextjs-and-tailwindcss-part-two-11cl","date":1739707085,"author":"Oluwabusayo Jacobs","guid":595,"unread":true,"content":"<p>In part one of this series, we built the basics of our Slack clone by setting up user authentication, workspace creation, and designing a responsive layout for our workspace hub.</p><p>In this second part, we'll bring our Slack clone to life by adding real-time messaging with <a href=\"https://getstream.io/chat/sdk/react/\" rel=\"noopener noreferrer\">Steam React Chat SDK</a>. We'll add features like rich text, file sharing, images, and emoji reactions.</p><p>By the end of this part, users will be able to communicate with each other, making our app a functional chat platform.</p><h2><strong>Adding More Channels To Your Workspace</strong></h2><p>Currently, users can only have one channel in a workspace, which is the channel added during the workspace creation process. Before adding the messaging feature to our app, let's enable users to create additional channels within a workspace.</p><p>To add more channels, we'll create a pop-up modal that appears when users click an '' button in the sidebar.</p><h3><strong>Creating the Channel API Route</strong></h3><p>First, we need an API route to handle channel creation. Create a  directory inside the existing <code>/api/workspaces/[workspaceId]</code> directory, then add a  file with the following code:</p><div><pre><code></code></pre></div><ul><li><p><strong>Authentication and Validation</strong>: We check if the user is authenticated and if they belong to the workspace.</p></li><li><p>: Only users with an '' role can create new channels.</p></li><li><p>: We ensure that no other channel in the workspace has the same name.</p></li><li><p>: If all checks pass, the channel is created and saved in the database.</p></li></ul><h3><strong>Creating the Add Channel Modal</strong></h3><p>Next, let's create a modal for adding new channels. In the  directory, create a file called  with the following code:</p><div><pre><code></code></pre></div><p>Let’s break down some of the component's key features:</p><ul><li><p>We use the  regular expression to ensure that each channel name is unique within the workspace by comparing it against existing channel names.</p></li><li><p>: We use the  state to show a loading spinner () while the channel creation is ongoing.</p></li><li><p><strong>Navigation to New Channel</strong>: After successfully creating a channel, we redirect users to the new channel page. The modal is also closed by resetting the input fields and calling the  function.</p></li></ul><h3><strong>Adding the 'Add Channel' Button to the Sidebar</strong></h3><p>Next, let's add the  to the  file:</p><div><pre><code></code></pre></div><p>In , we add a  hook to manage the modal's open state, and an ““ button that shows the modal if the current user is the workspace owner. This button is placed below the channel list for easy access.</p><p>With this setup, users can now create new channels to help organize conversations within the workspace.</p><h2><strong>Building the Chat Interface</strong></h2><p>Now that users can create multiple channels, let's start working on our main chat interface. First, we'll be building the loading state for our chat UI, then the main chat interface, and finally, we'll customize different aspects of the chat, like the message input, date separator, and more.</p><h3><strong>Creating a Channel Loading Indicator</strong></h3><p>To let users know the channel chat is loading, we will create a loading indicator that provides a visual cue while fetching data. Stream already provides a default loading UI, but we want a custom one to match our application's design.</p><p>Navigate to the  directory and create a new file called  with the following code:</p><div><pre><code></code></pre></div><p>The component shows a skeleton screen, which gives users a visual hint that content is loading.</p><p>Next, let's build the main chat interface so users can send messages and see their conversation history.</p><p>Go to the  folder, create a new file named , and add the following code:</p><div><pre><code></code></pre></div><p>The  component accepts the  data as a prop and uses the <a href=\"https://getstream.io/chat/docs/sdk/react/components/core-components/channel/\" rel=\"noopener noreferrer\"></a> component from  to manage chat sessions. Here are its key components:</p><ul><li><p>: This displays the conversation history within the current channel.</p></li><li><p>: This component allows users to type and send messages. The  is rendered using React Portals, which helps position the input field in a different part of the DOM to match the layout we want for our Slack clone.</p></li><li><p>: The  component also accepts our custom  component as a prop to override the default loading UI.</p></li></ul><h3><strong>Integrating the Channel Chat Component</strong></h3><p>Next, we need to integrate the  component into our channel page. Go to the <code>/client/[workspaceId]/[channelId]/page.tsx</code> file and update it as follows:</p><div><pre><code></code></pre></div><ul><li><p>We check if the channel is still loading using the  state. If it is, we display the  component.</p></li><li><p>Once the channel data is loaded, we display the  component, which provides the main chat interface for users to interact with.</p></li></ul><p>Finally, let’s add some styling to customize the look of our chat UI. Navigate to the  directory and update the  file with the following code:</p><div><pre><code></code></pre></div><p>And with that, users can now send messages. However, the current UI still looks far from what we want, so in the following sections, we'll add custom components to enhance it.</p><h2><strong>Adding a Custom Date Separator</strong></h2><p>To help users follow conversations more easily, we'll add custom date separators that indicate when messages are from different days.</p><p>Go to the  folder, create a new file called , and add the following code:</p><div><pre><code></code></pre></div><p>This component shows a separator to help users see when messages are from different days. Using the  function, we provide labels like \"\", \"\", or a formatted date with an ordinal suffix.</p><p>Next, let’s add the  to the  component to make conversations more readable:</p><div><pre><code></code></pre></div><h2><strong>Creating a Custom Emoji Picker</strong></h2><p>In this section, we'll create a custom emoji picker for our Slack clone using the  library. While Stream already provides an <a href=\"https://getstream.io/chat/docs/sdk/react/guides/customization/emoji_picker/\" rel=\"noopener noreferrer\"></a> using the same library, we want to build a more flexible version that better suits our chat components and integrates seamlessly into our clone.</p><p>Firstly, we need to install the necessary packages for the emoji picker. These include:</p><ul><li><p>: This library provides the emoji picker component.</p></li><li><p>: This package is specifically for using the emoji picker in React apps.</p></li><li><p>: This contains all the data needed for the emojis.</p></li></ul><p>Run the following command in your terminal to install the packages:</p><div><pre><code>npm emoji-mart @emoji-mart/react @emoji-mart/data\n</code></pre></div><p>Next, go to your  folder, create a new file called , and add the following code:</p><div><pre><code></code></pre></div><ul><li><p>: We use  to display the emoji picker and  to get all the emoji data. We also use  from  to handle the positioning of our emoji picker.</p></li><li><p>: The component accepts several props, such as  for the button that triggers the picker,  to handle emoji selection, and optional styling classes for customization.</p></li><li><p>: The  hook positions the emoji picker correctly relative to the button.</p></li><li><p>: We use the  state to show or hide the picker. We also handle clicks outside the picker to close it.</p></li></ul><h2><strong>Implementing a Custom Message Input</strong></h2><p>In this section, we'll implement a custom message input for our Slack clone. This new input will allow users to easily add rich formatting, such as bold or italics, and even upload files and add emojis, creating a more dynamic chatting experience.</p><p>To achieve this, we'll use <a href=\"https://docs.slatejs.org/\" rel=\"noopener noreferrer\"></a>, which is a robust framework for building rich text editors. We'll also use <a href=\"https://www.npmjs.com/package/is-hotkey\" rel=\"noopener noreferrer\"></a> to define keyboard shortcuts for formatting text.</p><p>First, let's install the necessary libraries. Open your terminal and run the following commands:</p><div><pre><code></code></pre></div><p>Next, we'll create our custom input component, which will act as the primary input container for our chat.</p><p>Navigate to the  directory, create a new file named , and add the following code:</p><div><pre><code></code></pre></div><p>There’s a lot going on here, so let’s break it down:</p><ul><li><p>: We use Slate to create a rich text editor that supports multiple formatting options, like bold, italics, underline, and strikethrough.</p></li><li><p>: The  and  functions convert the editor's content to markdown format, allowing us to maintain rich formatting in text.</p></li><li><p>: Functions like , , and  help manage file uploads, previews, and removal, making the chat input more versatile.</p></li><li><p>: The buttons for formatting text (bold, italic, etc.) call the  function to add or remove specific text styles.</p></li><li><p>: The  library binds hotkeys like  for bold,  for italics, and so on, making the editor more user-friendly.</p></li><li><p>: The  function is responsible for sending the message by serializing the editor's content and then using Stream's  function.</p></li></ul><p>Next, let's integrate the  with our channel chat interface.</p><p>Open the  file and update it with the following code:</p><div><pre><code></code></pre></div><p>In the code above, we import the  component and pass it as the  prop for the  component to override the default UI.</p><p>Next, let’s add some styling to support the rich text formatting features, ensuring elements like  blocks and other inline styles look polished.</p><p>Open your  file, and include the following styles:</p><div><pre><code></code></pre></div><p>While the chat interface is now visually improved with a customized message input, the message UI still needs work to match the look of the rest of the app.</p><h2><strong>Creating a Custom Message UI</strong></h2><p>In this section, we'll create a custom message UI to match the look and feel of our Slack clone. This custom message component will display user messages in a clean interface with the ability to send reactions and view attachments.</p><p>To get started, navigate to the  directory, create a new file named , and add the following code:</p><div><pre><code></code></pre></div><p>In the  component:</p><ul><li><p> We use the  hook to get information about the current message displayed, such as the message content and its author.</p></li><li><p> Using , we calculate the number of reactions and whether the user has reacted to the message or not. Users can add or remove reactions by clicking on the reaction buttons.</p></li><li><p> The message can contain attachments such as images or files. We provide download and preview options for attachments.</p></li><li><p> We added a button to send reactions using our custom .</p></li></ul><p>Now, let's integrate our new  component into our . Navigate to <code>components/ChannelChat.tsx</code> and update it to use :</p><div><pre><code></code></pre></div><p>In , we update the  to use our custom  component. This change allows our newly defined custom message UI to display each message.</p><p>And that’s it! We now have a fully customized chat experience similar to Slack.</p><p>In this part, we made our Slack clone more interactive by implementing core messaging features using <a href=\"https://getstream.io/chat/sdk/react/\" rel=\"noopener noreferrer\">Stream React Chat SDK</a>. We added custom components to further style and enhance the user interface with features like rich text formatting, emojis, and file sharing.</p><p>In this series's next and final part, we will integrate a video calling feature using <a href=\"https://getstream.io/video/sdk/react/\" rel=\"noopener noreferrer\">Stream React Video and Audio SDK</a>. This feature will allow users to transition between text and video conversations, making the app more versatile and interactive.</p>","contentLength":10020,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🐞Debugging Life: The Software Engineering Lessons You Can’t Ignore","url":"https://dev.to/hassanshahzadaheer/debugging-life-the-software-engineering-lessons-you-cant-ignore-4n79","date":1739707082,"author":"Hassan Shahzad Aheer","guid":594,"unread":true,"content":"<p>In software engineering, every line of code has a purpose. A well-structured program runs smoothly, while a messy one leads to endless bugs and frustration. Life works the same way. The mistakes we make, the shortcuts we take, and the mindset we carry shape the outcomes we get. Here are some key lessons from software engineering that apply directly to life.</p><h3>&nbsp;The Blueprint Matters – Planning Before Execution\n</h3><p>In programming, we don’t just start coding randomly. We . We define the , break the problem into smaller parts, and structure the code accordingly. If we skip this, we end up with messy code that’s hard to fix later.</p><p>If we go through life without a clear , we act randomly and waste energy. Imagine starting a new habit, a career, or even a relationship without knowing where it leads. Without direction, you’re just throwing code into an empty file, hoping it runs.</p><p>🪴  Before jumping into anything, ask yourself, <em>“What is my goal? Why am I doing this?”</em></p><h3> Quick Fixes Create Bigger Problems Later\n</h3><p>Sometimes, developers rush to fix a bug without thinking about the . The code works for now, but it breaks something else in the future. This is called , and eventually, it slows everything down.</p><p>Many people look for quick fixes—crash diets instead of healthy habits, avoiding hard conversations instead of fixing relationships, or chasing shortcuts instead of real skills. But these  create .</p><p>🪴  Don’t patch problems—. Invest time in doing things right the first time.</p><h3>&nbsp;Overengineering – The Trap of Perfectionism\n</h3><p>A common mistake in coding is —making things so complicated that it becomes impossible to maintain. Sometimes, a  is better than an overly complex one.</p><p>Overthinking and overplanning often lead to . Instead of making a simple start, we keep adding unnecessary steps, waiting for perfection, and never actually doing anything.</p><p>🪴  Don’t wait for perfect conditions— and improve along the way.</p><h3> Stay Focused – Avoiding Distractions\n</h3><p>In programming, every function must have a clear . If it’s unclear, the program doesn’t know what to do with it, leading to .</p><p>If your <strong>values, boundaries, and priorities</strong> are undefined, you’ll constantly feel lost. People will , and you’ll struggle to make decisions.</p><p>🪴  Define what matters to you.  Know when to say  and when to say .</p><h3> Debugging is Essential—But So is Learning from Mistakes\n</h3><p>Every coder knows that debugging is part of the job. We don’t cry over errors—we fix them, learn, and improve.</p><p>Mistakes are unavoidable. The problem isn’t —it’s . If we repeat the same mistakes without reflection, we stay stuck.</p><p>🪴  See mistakes as . Ask, <em>“What went wrong? What can I do better next time?”</em></p><h3> Distractions Are Like Memory Leaks\n</h3><p>In software, memory leaks happen when the program keeps holding onto unnecessary data, .</p><p>Distractions—social media, negativity, unnecessary worries—. You feel exhausted, but nothing productive happens.</p><p>🪴  Free up space in your mind. <strong>Focus on what truly matters.</strong></p><h3> Refactoring Makes Everything Better\n</h3><p>Refactoring is about —making it simpler, more efficient, and easier to maintain.</p><p>Sometimes, we hold onto old habits, outdated beliefs, or toxic relationships. Just like messy code, they slow us down.</p><p>🪴  Regularly . Let go of things that no longer serve you and .</p><h3>\n  \n  \n  Final Thoughts: Code Your Life Wisely\n</h3><p>Life and software engineering share the same rule: <strong>Structure leads to stability</strong>. Whether you’re writing code or shaping your future, small, intentional decisions lead to big results.</p><p>💬 <strong>Which lesson resonated with you the most?</strong> Share your thoughts in the comments! 🔥</p>","contentLength":3623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Introducing My Personal Tech Blog: CodePanda.online 📝","url":"https://dev.to/sheraz4194/introducing-my-personal-tech-blog-codepandaonline-5pk","date":1739706440,"author":"Sheraz Manzoor","guid":593,"unread":true,"content":"<p>I’m super excited to share that I’ve launched my personal blog website — CodePanda.online! It’s a space where I’ll be sharing my knowledge, experiences, and helpful tips for web developers and tech enthusiasts.</p><h2>\n  \n  \n  💻 What to Expect on CodePanda:\n</h2><p>✅ Hands-on tutorials for JavaScript, TypeScript, React, Next.js, and more.\n✅ Practical Tips &amp; Tricks – Quick solutions to common coding challenges.<p>\n✅ Insights from real-world projects I’ve worked on.</p>\n✅ In-Depth Tutorials – Step-by-step guides on JavaScript, TypeScript, React, Next.js, and more.<p>\n✅ Snippets and solutions you can use in your own apps.</p></p><p>I created CodePanda to document my journey as a developer and help others by sharing what I learn along the way. Whether you're just starting out or already deep into your coding career, I hope my posts make your development life a little easier.</p><p>I’d love for you to stop by, explore, and let me know what topics you’d like me to cover next! 🚀🔥</p>","contentLength":982,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"11 Must Read Software Design and Architecture Books for Developers","url":"https://dev.to/somadevtoo/10-must-read-software-design-and-architecture-books-for-developers-347m","date":1739705886,"author":"Soma","guid":592,"unread":true,"content":"<p>Disclosure: This post includes affiliate links; I may receive compensation if you purchase products or services from the different links provided in this article.<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Far0k5j7a5n0jljjpohp2.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Far0k5j7a5n0jljjpohp2.png\" alt=\"best book to learn software architecture and design\" width=\"800\" height=\"600\"></a></p><p>Hello friends, <strong>System design and Software design</strong> are two important topic for any tech interviews and also two important skills for Software developers.</p><p>Without knowing how to design System you cannot create new software and it will also be difficult to learn and understand existing software and system.</p><p>That's why big technical companies like  pays special attention to System design skill and test candidates thoroughly.</p><p>Whether you are a beginner or experienced developer, you can read these books as you will definitely find valuable stuff. </p><p>I have read them and even though I have been doing Software development for more than 15 years I have learned a lot.</p><p><a href=\"https://dev.to/somadevtoo/15-system-design-tradeoffs-for-software-developer-interviews-613\">System design</a>** is a complex process and you need to know a lot of stuff to actually design a system which can withstand test of time on production.</p><p>Software architecture is another field where you are expected to learn a lot of things. It's simply impossible to become a software architect by reading few books but if you have experience then and hunger to learn then these books can be gold mine.</p><p>These books allows you to learn from other people's experience. You can read these books to find what challenges they face when they design a real world system like Spotify, Google, or Amazon and how they overcome. </p><p>Each story is a journey in itself and you will learn a thing or two by reading and then relating with your own experience. </p><p>I love to read books and they are my primary source of learning, along with online courses nowadays. In this article, you will find few books which I have gone through in recent time to improve my knowledge about System Design and Software Architecture.</p><p>If you love these books, give high five and if you got any other excellent book to share, feel free to drop a comment. </p><p>P.S. Keep reading until the end. I have a free bonus for you.</p><h2>\n  \n  \n  11 Best System Architecture and System Design Books for Developers\n</h2><p>Here are my favorite 10 books on system design, software architecture, and software development. These are really good books and it's not just I am recommending it, you go reddit or hacker news, everywhere these books are recommend. They are also recommended to me by many people in past. </p><p>I love head first books, they are simplest books you can read on programming and software development so when the release a book on Software Architecture, I immediately ordered a copy.</p><p>It teaches you key principles to design software in a fun and engaging way, the Head First way. You will appreciate their simple to do examples and exercises in this complex topic.</p><p>While I loved this book, I know many folks who really hate anything Head First, for them its like a comic book and not worth their time. </p><p>If you are one of them I am sorry but this book definitely is a must, valuable insights, advices, and examples different topics that you need to consider when designing and architecting applications.</p><p>If you are a senior developer and want to become tech lead or software architect, you should definitely read this book. </p><p>This book was recommended to me by my tech lead but when I started reading this book, I literally fall asleep, it was too complex for me to understand, but I didn't give up and slowly I found that this is one heck of book for anyone who want to work on real systems. </p><p>In this fantastic book on system design, Martin Kleppmann will help you understand the pros and cons of all the different technologies that are used for storing and processing data. </p><p>It is a book that is written in a lucid style and presents a very broad overview of data storage systems.</p><p>You will get a very good grasp of fundamental concepts, algorithms, as well as practical applications of various technologies.&nbsp;</p><p>This is also one of the most popular book when it comes to learn Software design and System Design and I highly recommend this book to all kind of software developers .&nbsp;</p><p>The book is also good for beginners and experienced, developers and software architects and anyone who wants to be better at software design in 2024.&nbsp;</p><p>This book was recommended to me by one of my colleague who cracked Google interview, when I asked how did he prepared, he shared few books and courses apart from grinding leetcode and this was one of them.</p><p>As the title suggests, this is the perfect book for everyone who is preparing for a system interview. Trust me, this book is the finest on the internet right now.&nbsp; This book is created by Alex Xu who has gone through the same process.&nbsp;</p><p>You will get access to a number of drawings and diagrams that will assist you in gaining an understanding of the real system. You will be able to understand what the recruiters are looking for in your answers to questions.</p><p>Alex also have a companion&nbsp;, where you will not only find all the content of this book and the second part of System Design Interview Book\nbut also new content, deep dive into popular system questions like how to design YouTube and <a href=\"https://javarevisited.substack.com/p/messaging-app-system-design-in-5\" rel=\"noopener noreferrer\">WhatsApp</a> as well as&nbsp; proven System design framework to solve Software design problem.</p><p>In short, if you read this book, you will be able to confidently answer questions on your next system design interview.</p><p>This is also <strong>one of the most recommend System design interview books on Reddit, Quora</strong>, Hacker News, Twitter, and other online platforms and its obvious from the number of reviews this book have on Amazon.&nbsp;</p><p>If you are a senior developer who want to expand your knowledge about Software Engineering by learning from those who have done that in companies like Google then this book is for you.</p><p>This book is an interesting read on intricacies of developing and maintaining a sustainable and healthy codebase, emphasizing the distinction between programming and software engineering.</p><p>Drawing on their experiences at Google, the authors provide a detailed look at the practices employed by some of the world's leading software engineers to navigate the challenges of evolving codebases in response to changing requirements and demands.</p><p>This <a href=\"https://www.amazon.com/Software-Engineering-Google-Lessons-Programming/dp/1492082791?tag=javamysqlanta-20\" rel=\"noopener noreferrer\">software design book</a> also delves into Google's unique engineering culture, processes, and tools, shedding light on how these elements contribute to the effectiveness of their engineering organization.</p><p>Throughout the book, three fundamental principles are highlighted: the impact of time on software sustainability, the influence of scale on software practices within an organization, and the trade-offs engineers must consider when making design and development decisions.</p><p>With a focus on practical insights and real-world examples, this book serves as a valuable resource for software engineers seeking to enhance their understanding of software engineering principles and practices.&nbsp;&nbsp;</p><p>While this book <a href=\"https://www.amazon.com/Software-Engineering-Google-Lessons-Programming/dp/1492082791?tag=javamysqlanta-20\" rel=\"noopener noreferrer\">book</a> is not exclusively focused on System design it has many valuable lessons on trade-offs developers must consider when making design and development decisions, which is quite important for senior developers and software architects.</p><p>If you are looking a Software Architecture book to start with then this book is for you. Along with <a href=\"https://www.amazon.com/dp/1098134354?tag=javamysqlanta-20\" rel=\"noopener noreferrer\"><strong>Head First Software Architecture</strong></a> this is best book I have read for senior developers who want to become software architect.</p><p>System design, Software Design, and Software architecture are closely related. When you read this book, you will learn everything there is to know about Software structure and Design.&nbsp;&nbsp;</p><p>You will also be able to make your code smoother and integrate agile methodology into your solutions.&nbsp;</p><p>This book is created by Robert C. Martin, fondly known as Uncle Bob which is also author of famous <a href=\"http://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882?tag=javamysqlanta-20\" rel=\"noopener noreferrer\"></a> and <a href=\"https://www.amazon.com/Clean-Coder-Conduct-Professional-Programmers/dp/0137081073?tag=javamysqlanta-20\" rel=\"noopener noreferrer\"></a>. This is the third in the series and many people read all three to become a better software developer.&nbsp;</p><p>This book focuses on the different principles related to system analysis and design as well as the different challenges that system designers face while developing a solution in the real world.&nbsp;</p><p>In addition, you will learn how to run unit tests and use a test-driven strategy for improving the efficiency of your systems.&nbsp;&nbsp;</p><p>According to industry experts, this is one of the most important systems design textbooks that is available on the market today. That is because this book will make you realize what has changed in the industry as well as why a change was necessary in the first place.&nbsp;</p><p>Also, this book has sections that are dedicated to object-oriented coding and agile methodology, which are two of the most used methodologies these days. You will also learn how systems function, as well as about cloud computing and mobile applications.&nbsp;</p><p>The 12th edition of System Analysis and Design by Scott Tilley is the latest book in this series and you should read that to get the update knowledge.</p><p>If you are preparing for System design interview then you can also checkout&nbsp;&nbsp;by Sandeep Kaul (Tech Lead @ Facebook) for better preparation.</p><p>This is another interesting book that explains what patterns are important as well as how to apply these patterns to your designs. It focuses on object-oriented principles that are the basis of these patterns.</p><p>Using this book, you will be able to vastly improve your knowledge of software development. You will get to know about important design principles that can be useful for solving software design principles.&nbsp;</p><p>The 2nd edition of this book is now available and you should read that to learn design pattern with Java 8 and beyond. Most importantly how to use Lambdas and Stream to implement classic design pattern sin Java.&nbsp;</p><p>And, if you need an online course to go along this book,&nbsp; course by Dmitri Nestruk on Udemy is best as it shows the modern implementation of classic design patterns in Java.&nbsp;</p><p>This is another book you can read to prepare for FAANG System Design interview. This book not just cover essential System design concepts which every software architect should know but also cover many popular System design questions and coding problems.&nbsp;</p><p>Created by&nbsp;,&nbsp; a Google Software Engineer,&nbsp; this is also one of the best selling book on System design on Amazon.&nbsp; </p><p>The best thing about this book is that it walk you through key components which are used to build any system like below:</p><ul></ul><p>This book also includes real interview questions based on hundreds of interviews conducted at big tech companies like Google and Meta, and their detailed solutions. I highly recommend this book to anyone preparing for technical interviews.</p><p>You can also combine this with the  or  System design course for better preparation.&nbsp;</p><p>This is another highly recommended book on Software architecture. Written by Mark Richards and Neal Ford stands as an invaluable guide for developers aspiring to transition into the role of a software architect, a position consistently ranked among the top 10 best jobs in salary surveys globally.&nbsp;</p><p>This first-of-its-kind book offers a comprehensive overview of software architecture, covering a wide array of topics such as architectural characteristics, patterns, component determination, diagramming, evolutionary architecture, and more.&nbsp;</p><p>Written by hands-on practitioners with extensive experience in teaching software architecture classes, Mark Richards and Neal Ford focus on universal architecture principles applicable across various technology stacks.&nbsp;</p><p>The book delves into critical aspects like architecture patterns, component identification, soft skills, modern engineering practices, and treating architecture as an engineering discipline.&nbsp;</p><p>With a modern perspective that incorporates innovations from the past decade, this book equips both aspiring and existing architects with the necessary tools and insights to navigate the complexities of software architecture, making it an indispensable resource in the field.</p><p>I highly recommend this book to any senior developer who also want to become a software architect.&nbsp;</p><p>This is a great book that will give you a very good understanding of system analysis and design, as the title suggests. You will get access to extensive descriptions, as well as practical projects that reflect real-life situations.&nbsp;</p><p>The 6th Edition of this book is also made up of a number of case studies and many examples along with deep explanations for all the case studies and examples.&nbsp;</p><p>This book will definitely help you in achieving your goals as a design student. You will get a solid grasp of all the principles related to system analysis and design.&nbsp;</p><p>This is another Software design book that will teach you all about the skills, ideas, and techniques that are connected with system analysis and design. Trust me when I say that this is the perfect and most comprehensive system design book for you.&nbsp;</p><p>This System Design and Analysis book is written by Joshep Valacich and Joey George and also made up of a section on agile methods, which will ensure that this is a perfect student learning system for system designers.&nbsp;</p><p>The writing style is very simple but informative, which means that it will be accessible to everyone, including those whose mother tongue is not English.&nbsp;&nbsp;</p><p>There are multiple edition of these book to make sure you choose the latest edition, the 9th Edition and if you need a Udemy course to go with this book I recommend&nbsp;&nbsp;course on Udemy.&nbsp;</p><p>I have found that by reading books and watching course is the best way to learn System design.&nbsp;</p><p>That's all about the&nbsp;<strong>best Software architecture and System Design Books</strong>. You can read these books to learn essential Software Architecture design concepts and how to architect a system. </p><p>If you are learning software architecture for System design interview then <a href=\"https://amzn.to/3nU2Mbp\" rel=\"noopener noreferrer\"><strong>The System Design Interview Insider Guide book</strong></a> by Alex Xu also cover common Software design problems and shows you how to solve them which can be really beneficial for interviews.&nbsp;</p><p><strong>Thank you and all the best for your System Design interview and learning journey!!</strong></p>","contentLength":13855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/asadbukhari/-2mmi","date":1739705821,"author":"Asad Bukhari","guid":591,"unread":true,"content":"<h2>The Future of Login Systems: Biometric Authentication</h2>","contentLength":53,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devto"]}